With timestamps:

00:00 - hey this is anthony tavelos your cloud
00:02 - instructor at exam pro
00:05 - bringing you a complete study course for
00:07 - the google cloud associate cloud
00:10 - engineer made available to you here on
00:13 - free code camp and so this course is
00:16 - designed to help you pass and achieve
00:19 - google issued certification the way
00:21 - we're going to do that
00:23 - is to go through lots of lecture content
00:26 - follow alongs and using my cheat sheets
00:29 - on the day of the exam so you pass
00:32 - and you can take that certification and
00:34 - put it on your resume or linkedin
00:37 - so you can get that cloud job or
00:39 - promotion that you've been looking for
00:42 - and so a bit about me is that i have 18
00:45 - years industry experience
00:47 - seven of it specializing in cloud and
00:50 - four years of that as a cloud trainer i
00:53 - previously been a cloud and devops
00:56 - engineer and i've also published
00:58 - multiple cloud courses and i'm a huge
01:01 - fan of the cartoon looney tunes as well
01:05 - as a coffee connoisseur and so i wanted
01:07 - to take a moment to thank viewers like
01:10 - you
01:10 - because you make these free courses
01:12 - possible and so if you're looking for
01:15 - more ways of supporting more free
01:17 - courses just like this one
01:19 - the best way is to buy the extra study
01:22 - material at
01:25 - co example.com in particular for this
01:27 - certification you can find it at gcp
01:31 - hyphen ace there you can get study notes
01:34 - flash cards quizlets
01:37 - downloadable lectures which are the
01:39 - slides to all the lecture videos
01:42 - downloadable cheat sheets which by the
01:45 - way are free if you just go sign up
01:48 - practice exams
01:50 - and you can also ask questions and get
01:52 - learning support and if you want to keep
01:55 - up to date with new courses i'm working
01:57 - on
01:58 - the best way is to follow me on twitter
02:01 - at antony's cloud
02:02 - and i'd love to hear from you if you
02:04 - passed your exam
02:06 - and also i'd love to hear on what you'd
02:08 - like to see next
02:11 - [Music]
02:15 - welcome back
02:16 - in this lesson i wanted to quickly go
02:18 - over how to access the course resources
02:22 - now the resources in this course are
02:24 - designed to accompany the lessons and
02:27 - help you understand not just the theory
02:30 - but to help with the demo lessons that
02:32 - really drive home the component of
02:35 - hands-on learning
02:36 - these will include study notes lesson
02:38 - files scripts as well as resources that
02:42 - are used in the demo lessons
02:44 - these files can be found in a github
02:46 - repository that i will be including
02:49 - below that are always kept up-to-date
02:52 - and it is through these files that you
02:55 - will be able to follow along and
02:56 - complete the demos on your own to really
02:59 - cement the knowledge learned
03:01 - it's a fairly simple process but varies
03:04 - through the different operating systems
03:07 - i'll be going through this demo to show
03:09 - you how to obtain access through the
03:12 - three major operating systems being
03:14 - windows mac os and ubuntu linux
03:18 - so i'm first going to begin with windows
03:21 - and the first step would be to open up
03:22 - the web browser
03:24 - and browse to this url which i will
03:26 - include in the notes below
03:31 - and this is the course github repository
03:33 - which will house all the course files
03:36 - that i have mentioned before
03:38 - keeping the course up to date will mean
03:40 - that files may need to be changed and so
03:42 - as i update them they will always be
03:45 - reflected
03:46 - and uploaded here in the repo
03:49 - so getting back to it there are two ways
03:51 - to access this repository so the easiest
03:54 - way to obtain a copy of these files will
03:57 - be to click on the clone or download
03:59 - button and click on download zip
04:03 - once the file has been downloaded you
04:05 - can then open it up by clicking on it
04:08 - here
04:09 - and here are the files here in downloads
04:12 - and this will give you a snapshot of all
04:14 - the files and folders as you see them
04:18 - from this repository
04:20 - now although this may seem like the
04:21 - simple way to go this is not the
04:24 - recommended method to download as if any
04:27 - files have changed you will not be up to
04:30 - date with the latest files and will only
04:32 - be current from the date at which you've
04:34 - downloaded them now the way that is
04:36 - recommended is using a source control
04:39 - system called git and so the easiest way
04:42 - to install it would be to go to this url
04:47 - https
04:49 - colon forward slash forward slash
04:52 - git dash scm.com
04:56 - and this will bring you to the git
04:57 - website where you can download the
04:59 - necessary software for windows or any
05:02 - other supported operating system
05:04 - and so i'm going to download it here
05:07 - and this should download the latest
05:09 - version of git
05:11 - for windows and it took a few seconds
05:13 - there but it is done
05:15 - and no need to worry about whether or
05:17 - not you've got the proper version
05:19 - usually when you click that download
05:21 - button it will download the latest
05:23 - version for your operating system
05:26 - so i'm going to go over here and open
05:28 - this up
05:31 - you'll get a prompt where you would just
05:33 - say yes
05:35 - and we're going to go ahead and accept
05:37 - all the defaults here this is where it's
05:39 - going to install it let's hit next
05:43 - these are all the components that
05:44 - they're going to be installed let's
05:46 - click on next
05:49 - and again we're going to go through
05:51 - everything
05:52 - with all the defaults
05:55 - and once we've reached
05:58 - installing all the defaults it's gonna
06:00 - take a couple minutes to install
06:03 - and again it took a minute or so
06:05 - we're going to just click on next
06:07 - and it's going to ask if you want to
06:09 - view the release notes and we don't
06:12 - really need those so
06:14 - we can click on ok
06:16 - and simply close that
06:19 - and we're just going to go over and see
06:21 - if git is installed
06:24 - we're going to run the command prompt
06:30 - and i'm going to just zoom in here so we
06:31 - can see a little better
06:33 - and there we go and we are just going to
06:35 - type in git
06:36 - and as you can see it's been installed
06:38 - and so now that we've installed git we
06:41 - want to be able to pull down all the
06:43 - folders and the files within them from
06:45 - the repository to our local system and
06:48 - so i'm just going to clear the screen
06:50 - here
06:51 - and we're going to do a cd to make sure
06:53 - that i'm in my home directory
06:55 - and then we're going to make a directory
06:57 - called repos
06:59 - and in order to do that we're going to
07:01 - do mkdir
07:04 - space
07:05 - repos and then we're going to move into
07:07 - that directory
07:08 - so cd
07:10 - space repos and so again here we want to
07:13 - clone those files that are in the
07:14 - repository to our local system
07:17 - so in order to do that we're going to
07:19 - use the command git clone so get space
07:22 - clone
07:24 - and then we're going to need our
07:25 - location of the git repository so let's
07:28 - go back to the browser
07:30 - and we're going to go over here to clone
07:32 - or download
07:33 - and here you will see clone with https
07:37 - so make sure that this says https and
07:40 - you can simply click on this button
07:42 - which will copy this to the clipboard
07:45 - and then we'll move back to our command
07:47 - prompt
07:48 - and paste that in
07:50 - and once that's pasted just hit enter
07:53 - and it will clone your repository into
07:56 - the repos directory and so just to
07:58 - verify that we've cloned all the
08:00 - necessary files
08:02 - we're going to cd into the master
08:04 - directory that we had just cloned
08:10 - and we're going to do a dir
08:13 - and there you have it all of the files
08:16 - are cloned exactly as it is here in the
08:19 - repository
08:21 - now just as a note in order to keep
08:23 - these files up to date we need to run a
08:26 - different command which would be a git
08:29 - pull
08:30 - and this can be run at any time in order
08:33 - to pull down any files or folders that
08:35 - have been updated since you did the
08:38 - first pull which in this case
08:40 - would be cloning of the repository
08:43 - again this will provide you with the
08:45 - latest and most up-to-date files at any
08:48 - given moment in time and in this case
08:51 - since nothing has changed i have been
08:53 - prompted with a message stating that i'm
08:56 - up to date if nothing is changed you
08:59 - will always be prompted with this
09:01 - message
09:02 - if there was
09:03 - it will pull your changes down to your
09:05 - synced local copy and the process for
09:08 - windows is completed and is similar in
09:11 - mac os and i'll move over to my mac os
09:14 - virtual machine and log in
09:16 - and once you've logged in just going to
09:19 - go over here to the terminal and i'm
09:20 - just going to cd
09:22 - to make sure i'm in my home directory
09:24 - then i'm going to do exactly what we did
09:26 - in windows so i'm going to run the
09:28 - command mk
09:30 - dir
09:31 - space repos
09:33 - and create the repos directory and i'm
09:35 - going to move in to the repos directory
09:39 - and then i'm going to run git
09:42 - now for those of you who do not have get
09:44 - installed you will be prompted with this
09:46 - message to install it and you can go
09:48 - ahead and just install you'll be
09:50 - prompted with this license agreement you
09:52 - can just hit agree
09:55 - and depending on your internet
09:57 - connection this will take a few minutes
09:59 - to download and install so as this is
10:02 - going to take a few minutes i'm going to
10:04 - pause the video here and come back when
10:06 - it's finished installing
10:08 - okay and the software was successfully
10:10 - installed
10:11 - so just to do a double check i'm going
10:14 - to run git
10:15 - and as you can see it's been installed
10:17 - so now that we have git installed we
10:19 - want to clone all the directories and
10:22 - the files from the github repository to
10:24 - our local repos folder so i'm going to
10:27 - open up my browser and i'm going to
10:29 - paste my github repository url right
10:32 - here
10:34 - and
10:35 - you'll see the clone button over here so
10:38 - we're going to click on this button
10:40 - and here we can download zip but like i
10:43 - said we're not going to be doing that
10:45 - we're going to go over here
10:47 - and copy this url for the github
10:50 - repository again make sure it says https
10:54 - and we're going to copy this to our
10:56 - clipboard
10:58 - and we're going to go back to our
10:59 - terminal
11:01 - and we are going to
11:03 - run the command git space
11:06 - clone
11:08 - and we're going to paste in our url
11:12 - and as you can see here i've cloned the
11:15 - repository and all the files and folders
11:17 - within it and so as is my best practice
11:20 - i always like to verify that the files
11:22 - have been properly cloned and so i'm
11:25 - going to run the command ls
11:27 - just to make sure and go into the master
11:30 - directory
11:32 - and do a double check and as you can see
11:34 - the clone was successful as all the
11:36 - files and folders are here and again to
11:38 - download any updates to any files or
11:41 - directories we can simply run the
11:43 - command git space poll and because we've
11:47 - already cloned it it's already up to
11:48 - date and so the process is going to be
11:51 - extremely similar on linux so i'm going
11:53 - to simply move over to my linux machine
11:56 - and log in
11:58 - i'm going to open up a terminal
12:00 - and i'm going to make my terminal a
12:01 - little bit bigger for better viewing and
12:04 - so like the other operating systems i
12:06 - want to clone all the files and
12:08 - directories from the github repository
12:12 - to my machine and so i'm going to cd
12:14 - here to make sure i'm in my home
12:16 - directory and like we did before we want
12:18 - to create a directory called repos so
12:21 - i'm going to run the command mkdir space
12:25 - repos and we're going to
12:28 - create the repos directory we're now
12:30 - going to move into the repos directory
12:33 - and here we're going to run the git
12:35 - command
12:36 - and because git is not installed on my
12:38 - machine i've been prompted with the
12:40 - command in order to install it so i'm
12:42 - going to run that now so the command is
12:45 - sudo
12:46 - space
12:47 - apt
12:48 - space
12:50 - install
12:51 - space
12:53 - get
12:54 - and i'm going to enter in my password
12:57 - and install it
13:04 - and just to verify i'm going to run the
13:06 - command git and i can see here it's been
13:08 - installed so now i'm going to go over
13:10 - here to my browser and i'm going to
13:12 - paste in the url to my repository and
13:15 - over here we'll have the same clone
13:16 - button and when i click on it i can get
13:19 - the url for the github repository in
13:21 - order to clone it again make sure before
13:24 - you clone that this says https
13:27 - if it doesn't say https you'll have the
13:30 - option of clicking on a button that will
13:32 - allow you to do so once it says https
13:35 - then you can simply copy this url to
13:37 - your clipboard by clicking on the button
13:40 - and then move over back to the terminal
13:42 - and we are going to clone this
13:44 - repository by typing in the get space
13:49 - clone command
13:50 - along with the url of the repository
13:53 - and when we hit enter it'll clone it
13:56 - right down to our directory so i'm just
13:58 - going to move into the master directory
14:00 - just to verify that the files are there
14:04 - and again they're all here so again if
14:06 - you're looking to update your repository
14:09 - with any new updated changes you can
14:12 - simply run
14:13 - the get space pull command
14:16 - to update those files
14:18 - and so that's the linux setup so you
14:20 - have a local copy of the lesson files
14:23 - now there's just one more thing that i
14:25 - highly recommend you do and to
14:27 - demonstrate it i'm going to move back
14:29 - over to my windows virtual machine now
14:31 - i'm going to open up the web browser
14:33 - again
14:35 - open up a new tab
14:37 - and i'm going to browse to this url
14:42 - https colon forward slash forward slash
14:46 - code.visualstudio.com
14:47 - [Music]
14:50 - and i'll make sure that the url is in
14:53 - the text below there is a version of
14:55 - this code editor available for windows
14:58 - mac os and linux you can simply click on
15:01 - this drop down and you'll find the link
15:03 - to download it for your operating system
15:05 - but in most cases it should
15:07 - automatically show the correct version
15:10 - so just go ahead and click on download
15:15 - and it should start downloading
15:17 - automatically and you should be able to
15:19 - run it right away
15:27 - now the reason behind me asking you to
15:29 - install this utility is for editing code
15:32 - of different sorts
15:34 - whether you're adjusting yaml or python
15:36 - documents for deployment manager
15:39 - or even managing scripts
15:41 - a code editor will give you the ease of
15:43 - use when it comes to managing editing
15:47 - and even syntactical highlighting of
15:49 - code as shown here below
15:51 - it will highlight the code to make it
15:53 - easier to understand now if you have
15:55 - your own editor that you would prefer to
15:57 - use go ahead and use that but for those
16:00 - that don't my recommendation will be to
16:02 - use visual studio code so to install
16:05 - visual studio code we're just going to
16:07 - accept this license agreement
16:09 - and then we're going to click on next
16:11 - and we're just going to follow all the
16:13 - defaults
16:20 - to install it
16:22 - it's going to take a minute or two
16:24 - and for those running windows you want
16:26 - to make sure that this box is checked
16:28 - off so that you can launch it right away
16:31 - let's hit finish
16:35 - another recommendation would be to go
16:36 - over here to the task bar so you can pin
16:39 - it in place so that it's easier to find
16:42 - and so now you have access to all the
16:45 - resources that's needed for this course
16:47 - but with that that's everything that i
16:49 - wanted to cover for this lesson so you
16:51 - can now mark this lesson as complete and
16:53 - let's move on to the next one
16:55 - [Music]
16:59 - welcome back and in this lesson i wanted
17:02 - to discuss the various certifications
17:05 - available for google cloud as this
17:07 - number keeps on growing and i am looking
17:10 - to keep this lesson as up to date as
17:12 - possible so with that being said let's
17:14 - dive in
17:16 - now google cloud has released a slew of
17:18 - certifications in many different areas
17:21 - of expertise as well as different
17:23 - experience levels
17:25 - now there are two levels of difficulty
17:27 - when it comes to the google cloud
17:29 - certifications starting off with the
17:31 - associate level we see that there is
17:33 - only the one certification which is the
17:36 - cloud engineer the associate level
17:38 - certification is focused on the
17:41 - fundamental skills of deploying
17:44 - monitoring and maintaining projects on
17:46 - google cloud this is a great starting
17:49 - point for those completely new to cloud
17:52 - and google recommends the associate
17:54 - cloud engineer as the starting point to
17:57 - undergoing your certification journey
18:00 - this was google cloud's very first
18:02 - certification and to me was the entry
18:05 - point of wanting to learn more as an
18:07 - engineer in cloud in my personal opinion
18:10 - no matter your role this certification
18:13 - will cover the general knowledge that is
18:15 - needed to know about starting on google
18:18 - cloud and the services within it which
18:21 - is why i labeled it here as the
18:24 - foundational level course i also
18:26 - consider this the stepping stone
18:28 - into any other professional level
18:30 - certifications which also happens to be
18:33 - a recommended path by google with a
18:36 - great course and some dedication i truly
18:39 - believe that anyone with even a basic
18:41 - skill level in it should be able to
18:44 - achieve this associate level
18:46 - certification now it is recommended from
18:48 - google themselves that prior to taking
18:51 - this exam that you should have over six
18:54 - months experience building on google
18:56 - cloud for those of you with more of an
18:59 - advanced background in google cloud or
19:01 - even other public clouds this
19:04 - certification should be an easy pass as
19:06 - it covers the basics that you should be
19:08 - familiar with adding a google twist to
19:10 - it at the time of this lesson this exam
19:13 - is two hours long and the cost is 125 us
19:17 - dollars the exam is a total of 50
19:20 - questions which consists of both
19:22 - multiple choice and multiple answer
19:25 - questions each of the questions contain
19:27 - three to four line questions with single
19:30 - line answers that by the time you finish
19:33 - this course you should have the
19:35 - confidence to identify the incorrect
19:37 - answers
19:38 - and be able to select the right answers
19:40 - without a hitch moving into the
19:42 - professional level certifications there
19:44 - are seven certifications that cover a
19:47 - variety of areas of specialty depending
19:50 - on your role you might want to take one
19:52 - or maybe several of these certifications
19:55 - to help you gain more knowledge in
19:57 - google cloud or if you love educating
20:00 - yourself and you're really loving your
20:02 - journey in gcp you will probably want to
20:05 - consider pursuing them all in my
20:07 - personal opinion the best entry point
20:10 - into the professional level would be the
20:12 - cloud architect it is a natural step up
20:16 - from the associate cloud engineer and it
20:18 - builds on top of what is learned through
20:21 - that certification with a more detailed
20:24 - and more thorough understanding of cloud
20:27 - architecture that is needed for any
20:29 - other certification there is some
20:31 - overlap from the cloud engineer which is
20:34 - why in my opinion doing this
20:36 - certification right after makes sense it
20:39 - also brings with it the ability to
20:41 - design develop and manage secure
20:45 - scalable and highly available dynamic
20:47 - solutions it is a much harder exam and
20:50 - goes into great depth on services
20:53 - available the professional cloud
20:55 - architect is a great primer for any
20:58 - other professional level certification
21:00 - and can be really helpful to solidify
21:03 - the learning that is needed in any other
21:06 - technical role i find it the most common
21:08 - path that many take who look to learn
21:11 - google cloud which is why i personally
21:14 - recommend it to them and at the time of
21:16 - this lesson it also holds the highest
21:18 - return on investment
21:20 - due to the highest average wage over any
21:23 - other current cloud certification in the
21:26 - market google recommends over three
21:28 - years of industry experience including
21:31 - one year on google cloud before
21:34 - attempting these exams with regards to
21:36 - the exams in the professional tier they
21:39 - are much harder than the associate level
21:41 - and at the time of this course is two
21:43 - hours long and the cost is 200 us
21:47 - dollars these exams are a total of 50
21:49 - questions which consists of both
21:52 - multiple choice and multiple answer
21:54 - questions it's the same amount of
21:56 - questions with the same amount of time
21:59 - but it does feel much harder each of the
22:02 - questions contain four to five line
22:04 - questions with one to three line answers
22:07 - it's definitely not a walk in the park
22:10 - and will take some good concentration
22:12 - and detailed knowledge on google cloud
22:14 - to solidify a pass after completing the
22:17 - cloud architect certification depending
22:20 - on your role my suggestion would be to
22:22 - pursue the areas that interest you the
22:24 - most to make your journey more enjoyable
22:27 - for me at the time i took the security
22:30 - engineer out as i am a big fan of
22:32 - security and i knew that i would really
22:34 - enjoy the learning and make it more fun
22:37 - for me this is also a great
22:38 - certification for those who are looking
22:41 - to excel their cloud security knowledge
22:44 - on top of any other security
22:46 - certifications such as the security plus
22:49 - or cissp
22:51 - now others may be huge fans of
22:53 - networking or hold other networking
22:55 - certifications such as the ccna and so
22:58 - obtaining the network engineer
23:00 - certification might be more up your
23:03 - alley and give you a better
23:04 - understanding in cloud networking now if
23:07 - you're in the data space you might want
23:09 - to move into the data engineer exam as
23:12 - well as taking on the machine learning
23:14 - engineer exam to really get some deeper
23:17 - knowledge in the areas of big data
23:20 - machine learning and artificial
23:22 - intelligence on google cloud now i know
23:24 - that there are many that love devops me
23:27 - being one of them and really want to dig
23:29 - deeper and understand sre and so they
23:32 - end up tackling the cloud developer and
23:35 - cloud devops engineer certifications so
23:38 - the bottom line is whatever brings you
23:40 - joy in the area of your choosing start
23:43 - with that and move on to do the rest all
23:46 - the professional certifications are
23:48 - valuable but do remember that they are
23:50 - hard and need preparation for study last
23:53 - but not least is the collaboration
23:56 - engineer certification and this
23:58 - certification focuses on google's core
24:01 - cloud-based collaboration tools that are
24:04 - available in g suite or what is now
24:06 - known as google workspaces such as gmail
24:10 - drive hangouts docs and sheets now the
24:13 - professional level collaboration
24:15 - engineers certification dives into more
24:18 - advanced areas of g suite such as mail
24:20 - routing identity management and
24:23 - automation of it all using tools
24:25 - scripting and apis
24:27 - this certification is great for those
24:30 - looking to build their skill set as an
24:32 - administrator of these tools but gives
24:35 - very little knowledge of google cloud
24:37 - itself so before i move on there is one
24:40 - more certification that i wanted to
24:42 - cover that doesn't fall under the
24:44 - associate or professional certification
24:47 - levels and this is the google cloud
24:49 - certified fellow program
24:51 - now this is by far
24:53 - one of the hardest certifications to
24:55 - obtain as there are very few certified
24:58 - fellows at the time of recording this
25:00 - lesson it is even harder than the
25:02 - professional level certifications and
25:04 - this is due to the sheer level of
25:06 - competency with hybrid multi-cloud
25:09 - architectures using google cloud anthos
25:12 - google's recommended experience
25:14 - is over 10 years with a year of
25:17 - designing enterprise solutions with
25:19 - anthos then a four-step process begins
25:22 - first step is to receive a certified
25:25 - fellow invitation from google and once
25:27 - you've received that invitation then you
25:30 - need to submit an application with some
25:32 - work samples that you've done
25:34 - showing google your competency in hybrid
25:37 - multi-cloud once that is done the third
25:40 - step is a series of technical hands-on
25:43 - labs that must be completed and is a
25:46 - qualifying assessment that must be
25:47 - passed in order to continue and after
25:50 - all that the last step is a panel
25:52 - interview done with google experts in
25:55 - order to assess your competency of
25:57 - designing hybrid and multi-cloud
25:59 - solutions with anthos so as you can see
26:02 - here this is a very difficult and highly
26:05 - involved certification process
26:07 - to achieve the title of certified fellow
26:10 - this is definitely not for the faint of
26:12 - heart but can distinguish yourself as a
26:15 - technical leader in anthos and a hybrid
26:18 - multi-cloud expert in your industry now
26:21 - i get asked many times whether or not
26:23 - certifications hold any value are they
26:25 - easy to get are they worth more than the
26:28 - paperwork that they're printed on and
26:30 - does it show that people really know how
26:32 - to use google cloud and my answer is
26:35 - always yes as the certifications hold
26:37 - benefits beyond just the certification
26:40 - itself and here's why targeting yourself
26:43 - for a certification gives you a
26:45 - milestone for learning something new
26:47 - with this new milestone it allows you to
26:49 - put together a study plan in order to
26:52 - achieve the necessary knowledge needed
26:54 - to not only pass the exam but the skills
26:57 - needed to progress in your everyday
26:59 - technical role this new knowledge helps
27:02 - keep your skills up to date therefore
27:05 - making you current instead of becoming a
27:07 - relic now having these up-to-date skills
27:10 - will also help advance your career
27:13 - throughout my career in cloud i have
27:15 - always managed to get my foot in the
27:17 - door with various interviews due to my
27:20 - certifications it gave me the
27:22 - opportunity to shine in front of the
27:24 - interviewer while being able to
27:27 - confidently display my skills in cloud
27:29 - it also allowed me to land the jobs that
27:31 - i sought after as well as carve out the
27:34 - career path that i truly wanted on top
27:37 - of landing the jobs that i wanted i was
27:39 - able to achieve a higher salary due to
27:42 - the certifications i had i have doubled
27:45 - and tripled my salary since i first
27:47 - started in cloud all due to my
27:49 - certifications and i've known others
27:52 - that have obtained up to five times
27:54 - their salary because of their
27:56 - certifications now this was not just
27:58 - from achieving the certification to put
28:00 - on my resume and up on social media but
28:03 - from the knowledge gained through the
28:05 - process and of course i personally feel
28:08 - that having your skills constantly up to
28:11 - date
28:12 - advancing your career and getting the
28:14 - salary that you want keeps you motivated
28:17 - to not only get more certifications but
28:20 - continue the learning process i am and
28:23 - always have been a huge proponent of
28:26 - lifelong learning and as i always say
28:29 - when you continue learning you continue
28:31 - to grow so in short google cloud
28:34 - certifications are a great way to grow
28:37 - and so that about covers everything that
28:39 - i wanted to discuss in this lesson so
28:41 - you can now mark this lesson as complete
28:43 - and i'll see you in the next one
28:45 - [Music]
28:50 - welcome back
28:51 - and in this lesson i'm going to be
28:53 - talking about the fictitious
28:54 - organization called bow tie inc that i
28:57 - will be using throughout the course
29:00 - now while going through the
29:01 - architectures and demos in this course
29:04 - together i wanted to tie them to a real
29:07 - world situation
29:09 - so that the theory and practical
29:11 - examples are easy to understand
29:14 - tying it to a scenario is an easy way to
29:16 - do this as well it makes things a lot
29:19 - more fun
29:21 - so the scenario again
29:23 - that i will be using is based on bow tie
29:26 - ink
29:27 - so before we get started with the course
29:29 - i'd like to quickly run through the
29:31 - scenario
29:32 - and don't worry it's going to be very
29:34 - high level and i will keep it brief
29:38 - so bow tie ink is a bow tie
29:40 - manufacturing company that designs and
29:42 - manufactures bow ties within their own
29:45 - factories
29:46 - they also hold a few retail locations
29:49 - where they sell their bow ties as well
29:52 - as wholesale to other thai and men's
29:55 - fashion boutiques and department stores
29:57 - across the globe
29:58 - being in the fashion business
30:00 - they mainly deal with commerce security
30:04 - and big data sets
30:06 - bow tie inc is a global company and they
30:09 - are headquartered in montreal canada
30:12 - they employ about 300 people globally
30:16 - with a hundred of them being in sales
30:18 - alone to support both the brick and
30:20 - mortar stores and wholesale branches
30:24 - there are many different departments to
30:25 - the company that make it work such as
30:28 - in-store staff
30:30 - i.t
30:31 - marketing for both in-store and online
30:33 - sales
30:34 - manufacturing finance and more
30:38 - the types of employees that work in bow
30:40 - tie inc vary greatly due to the various
30:43 - departments and consists of many people
30:46 - such as sales for both in-store and
30:48 - wholesale managers that run the stores
30:52 - and sewers that work in the
30:54 - manufacturing plant
30:55 - and many more that work in these various
30:58 - departments
30:59 - the business has both offices
31:01 - and brick and mortar stores
31:03 - in montreal
31:05 - london and los angeles
31:08 - now due to the thrifty mindset of
31:10 - management concentrating all their
31:12 - efforts on commerce and almost none in
31:15 - technical infrastructure has caused
31:17 - years of technical debt
31:20 - and is now a complete disaster
31:23 - within the brick and mortar location
31:25 - there contains two racks with a few
31:28 - servers and some networking equipment
31:30 - the global inventory of bow ties are
31:34 - updated upon sales in both stores
31:37 - and wholesale as well as new stock that
31:40 - has been manufactured from the factory
31:43 - there are point-of-sale systems in each
31:45 - store or office location
31:47 - these systems are all connected to each
31:50 - other over a vpn connection in order to
31:54 - keep updates of the inventory fresh
31:57 - all office and store infrastructure are
31:59 - connected to each other and the montreal
32:01 - headquarters
32:03 - and the point of sale systems and kiosk
32:05 - systems are backed up to tape in the
32:08 - montreal headquarters as well and like i
32:10 - said before
32:12 - management is extremely thrifty but they
32:15 - have finally come to the realization
32:17 - that they need to start spending money
32:19 - on the technical infrastructure in order
32:22 - to scale so diving into a quick overview
32:25 - of exactly what the architecture looks
32:28 - like the head office is located in
32:30 - montreal canada
32:32 - it has its main database for the crm and
32:35 - point-of-sale systems
32:37 - as well as holding the responsibility of
32:40 - housing the equipment for the tape
32:42 - backups the tapes are then taken off
32:45 - site within montreal by a third-party
32:48 - company for storage
32:49 - the company has two major offices one in
32:52 - london covering the eu
32:55 - and the other in the west coast us
32:57 - in los angeles these major offices
33:00 - are also retail locations that consume
33:03 - i.t services from the headquarters in
33:06 - montreal again being in the fashion
33:08 - business bowtie inc employs a large
33:11 - amount of sales people and the managers
33:13 - that support them
33:15 - these employees operate the
33:16 - point-of-sale systems so we're
33:18 - constantly looking to have the website
33:21 - sales and the inventory updated at all
33:24 - times each salesperson has access to
33:27 - email and files for updated forecasts on
33:31 - various new bowtie designs
33:34 - most sales people communicate over a
33:36 - voice over ip phone and chat programs
33:40 - through their mobile phone the managers
33:42 - also manually look at inventory on
33:45 - what's been sold
33:46 - versus what's in stock
33:48 - to predict the sales for stores in
33:51 - upcoming weeks
33:52 - this will give manufacturing a head
33:54 - start to making more bow ties for future
33:57 - sales
33:58 - now whatever implementations that we
34:00 - discuss throughout this course
34:02 - we'll need to support the day-to-day
34:04 - operations of the sales people and the
34:07 - managers
34:08 - and because of the different time zones
34:10 - in play
34:11 - the back-end infrastructure needs to be
34:13 - available
34:14 - 24 hours a day seven days a week
34:18 - any downtime will impact updated
34:21 - inventory for both online sales as well
34:25 - as store sales at any given time
34:28 - now let's talk about the current
34:29 - problems that the business is facing
34:32 - most locations hold on premise hardware
34:36 - that is out of date and also out of
34:38 - warranty
34:39 - the business looked at extending this
34:41 - warranty but became very costly as well
34:45 - management is on the fence about whether
34:47 - to buy new on-premise hardware or just
34:50 - move to the cloud they were told that
34:53 - google cloud is the way to go when it
34:55 - comes to the retail space and so are
34:57 - open to suggestions
34:59 - yet still very weary now when it comes
35:02 - to performance there seems to be a major
35:05 - lag from the vpn connecting from store
35:08 - to store
35:09 - as well as the head office that's
35:12 - responsible for proper inventory
35:15 - thus slowing down the point of sale
35:17 - systems and to top it all off
35:20 - backups taking an exorbitant amount of
35:23 - time
35:24 - is consuming a lot of bandwidth with the
35:26 - current vpn connection now bowtie inc
35:29 - has always struggled with the lack of
35:31 - highly available systems and scalability
35:35 - due to cost of new hardware this is
35:37 - causing extreme stress for online
35:40 - e-commerce whenever a new marketing
35:42 - campaign is launched
35:44 - as the systems are unable to keep up
35:46 - with the demand
35:48 - looking at the forecast for the next two
35:50 - quarters the business is looking to open
35:52 - up more stores in the eu
35:55 - as well as in the us
35:57 - and with the current database in place
35:59 - providing very inefficient high
36:01 - availability or scalability
36:04 - there is a major threat of the main
36:06 - database going down
36:08 - now when it comes to assessing the
36:09 - backups the tape backups have become
36:12 - very slow especially backing up from
36:15 - london and the off-site storage costs
36:18 - continuously go up every year
36:21 - the backups are consuming a lot of
36:23 - bandwidth and are starting to become the
36:25 - major pain point
36:26 - for connection issues between locations
36:29 - on top of all these issues the small it
36:32 - staff that is employed have outdated i.t
36:35 - skills and so there is a lot of manual
36:38 - intervention that needs to be done
36:40 - to top it all off all the running around
36:43 - that is necessary to keep the outdated
36:45 - infrastructure alive
36:47 - management is also now pushing to open
36:50 - new stores to supply bow ties globally
36:53 - given the ever-growing demand
36:55 - as well as being able to supply the
36:57 - demand of bow ties online through their
37:00 - e-commerce store
37:02 - now these are some realistic yet common
37:04 - scenarios that come up in reality for a
37:07 - lot of businesses that are not using
37:09 - cloud computing
37:11 - and throughout the course we will dive
37:14 - into how google cloud can help ease the
37:16 - pain
37:17 - of these current ongoing issues
37:20 - now at a high level with what the
37:22 - business wants to achieve and what the
37:24 - favorable results are
37:26 - they are all interrelated issues so
37:29 - bowtie inc requires a reliable and
37:31 - stable connection between all the
37:33 - locations of the stores and offices
37:36 - so that sales
37:38 - inventory and point-of-sale systems are
37:41 - quick and up-to-date at all times
37:44 - this will also allow all staff in these
37:46 - locations to work a lot more efficiently
37:50 - with a stable and reliable connection in
37:52 - place
37:53 - backups should be able to run smoothly
37:56 - and also eliminate the cost of off-site
37:59 - backup
38:00 - not to mention the manpower and
38:02 - infrastructure involved to get the job
38:04 - done
38:05 - while scaling up offices and stores due
38:08 - to increase in demand
38:10 - the business should be able to deploy
38:11 - stores in new regions using pay as you
38:14 - go billing while also meeting the
38:16 - requirements and regulations when it
38:19 - comes to gpdr and pci
38:23 - this would also give the business
38:25 - flexibility of having a disaster
38:28 - recovery strategy in place
38:30 - in case there was a failure of the main
38:32 - database in montreal now as mentioned
38:35 - before the business is extremely thrifty
38:38 - especially when it comes to spend on it
38:40 - infrastructure
38:41 - and so the goal is to have the costs as
38:44 - low as possible yet having the
38:46 - flexibility of scaling up when needed
38:50 - especially when new marketing campaigns
38:52 - are launched during high demand sales
38:55 - periods
38:56 - this would also give bowtie inc the
38:58 - flexibility of analyzing sales ahead of
39:01 - time
39:02 - using real-time analytics and catering
39:06 - to exactly what the customer is
39:08 - demanding
39:09 - thus making inventory a lot more
39:12 - accurate and reducing costs in
39:14 - manufacturing items that end up going on
39:16 - sale and costing the company money in
39:19 - the end
39:20 - finally when it comes to people
39:22 - supporting infrastructure automation is
39:25 - key
39:26 - removing manual steps and a lot of the
39:28 - processes can reduce the amount of
39:30 - manpower needed to keep the
39:33 - infrastructure alive
39:34 - and especially will reduce downtime when
39:37 - disaster arises
39:39 - putting automation in place will also
39:41 - reduce the amount of tedious tasks that
39:44 - all departments have on their plate
39:47 - so that they can focus on more important
39:49 - business needs
39:50 - now that's the scenario at a high level
39:53 - i wanted to really emphasize that this
39:56 - is a typical type of scenario that you
39:58 - will face as a cloud engineer and a
40:00 - cloud architect
40:02 - the key to this scenario is the fact
40:04 - that there are areas that are lacking in
40:06 - detail
40:08 - and areas that are fully comprehensible
40:10 - and this will trigger knowing when and
40:13 - where to ask relevant questions
40:16 - especially in your day-to-day role as an
40:19 - engineer
40:20 - it will allow you to fill the gaps so
40:22 - that you're able to figure out what
40:24 - services you will need and what type of
40:26 - architecture to use
40:28 - this is also extremely helpful when it
40:31 - comes to the exam
40:32 - as in the exam you will be faced with
40:35 - questions that pertain to real life
40:37 - scenarios that will test you in a
40:39 - similar manner knowing what services and
40:42 - architecture to use based on the
40:44 - information given
40:46 - will always give you the keys to the
40:48 - door with the right answer and lastly
40:51 - when it comes to the demos
40:53 - this scenario used throughout the course
40:55 - will help put things in perspective
40:58 - as we will come to resolve a lot of
41:00 - these common issues
41:02 - real world scenarios can give you a
41:04 - better perspective on learning as it is
41:07 - tied to something that makes it easy to
41:09 - comprehend
41:11 - and again bow tie inc is the scenario
41:13 - that i will be using throughout the
41:15 - course to help you grasp these concepts
41:18 - so that's all i have to cover this
41:20 - scenario so you can now mark this lesson
41:23 - as complete and let's move on to the
41:25 - next one
41:26 - [Music]
41:29 - hey this is anthony cevallos and what i
41:32 - wanted to show you here is where you can
41:35 - access the practice exam on the exam pro
41:38 - platform
41:40 - so once you've signed up for your
41:41 - account you can head on over to the
41:44 - course
41:45 - and you can scroll down to the bottom of
41:47 - the curriculum list and you will see the
41:50 - practice exams here at the bottom
41:52 - now just as a quick note
41:54 - you should generally not attempt the
41:56 - practice exam unless you have completed
42:00 - all the lecture content including the
42:02 - follow alongs as once you start to see
42:05 - those questions you will get an urge to
42:08 - start remembering these questions
42:10 - and so i always recommend to use the
42:13 - practice exam as a serious attempt
42:17 - and not just a way to get to the final
42:19 - exam at a faster pace taking your time
42:22 - with the course
42:23 - will allow you to really prevail through
42:26 - these practice exams and allow you for a
42:29 - way better pass rate on the final exam
42:32 - looking here we can see two practice
42:35 - exams with 50 questions each and so i
42:38 - wanted to take a moment here and dive
42:41 - into the practice exam
42:43 - and show you what some of these
42:44 - questions will look like and so clicking
42:47 - into one of these exams we can get right
42:49 - into it and so as you can see i've
42:52 - already started on practice exam one and
42:55 - so i'm going to click into that right
42:57 - now and as you can see the exam is
43:00 - always timed and in this case will be
43:02 - 120 minutes for this specific exam there
43:06 - are 50 questions for this practice exam
43:09 - and you will see the breakdown in the
43:11 - very beginning
43:12 - of the types of questions you will be
43:14 - asked now for the google cloud exams at
43:17 - the associate level they are usually
43:20 - structured in a common format
43:22 - they generally start with one or two
43:25 - lines of sentences which will typically
43:27 - represent a scenario followed by the
43:30 - question itself this question tends to
43:33 - be brief and to the point immediately
43:36 - following that you will be presented
43:39 - with a number of answers
43:41 - usually four or five in nature and can
43:44 - sometimes be very very technical as they
43:47 - are designed for engineers like asking
43:50 - about which gcloud commands to use to
43:53 - execute in a given scenario as well as
43:57 - theoretical questions that can deal with
43:59 - let's say best practices or questions
44:02 - about the specific services themselves
44:05 - now these answers will come in two
44:08 - different styles either multi-choice or
44:11 - multi-select the multi-choice is usually
44:14 - about identifying the correct answer
44:17 - from a group of incorrect or less
44:20 - correct answers whereas the multi-select
44:23 - will be about choosing multiple correct
44:26 - solutions to identify the answer as well
44:30 - for this associate exam the overall
44:32 - structure is pretty simple in nature and
44:36 - typically will be either right or wrong
44:39 - now sometimes these questions can get
44:41 - tricky where there are multiple possible
44:44 - answers
44:45 - and you will have to select the most
44:47 - suitable ones
44:49 - now although most of these types of
44:51 - questions
44:52 - usually show up in the professional exam
44:55 - they can sometimes peek their heads into
44:58 - the associate and so a great tactic that
45:01 - i always like to use is to immediately
45:04 - identify what matters in the question
45:07 - itself
45:08 - and then to start ruling out any of the
45:10 - answers that are wrong and this will
45:13 - allow you to answer the question a lot
45:15 - more quickly and efficiently as it will
45:18 - bring the more correct answer to the
45:21 - surface as well as making the answer a
45:24 - lot more obvious
45:26 - and making the entire question less
45:29 - complex so for instance with this
45:31 - question here you are immediately asked
45:34 - about google's recommended practices
45:37 - when it comes to using cloud storage as
45:40 - backup for disaster recovery and this
45:43 - would be for a specific storage type and
45:46 - so quickly looking at the answers you
45:48 - can see that standard storage and near
45:51 - line storage will not be part of the
45:53 - answer and so that will leave cold line
45:56 - storage or archive storage as the two
45:59 - possible choices
46:00 - for the answer of this question and so
46:03 - these are the typical techniques that i
46:06 - always like to use
46:07 - for these exams and so provided that
46:10 - you've gone through all the course
46:12 - content you will be able to answer these
46:14 - technical questions with ease and
46:17 - following the techniques i've just given
46:20 - and applying them to each question
46:22 - can really help you in not only this
46:25 - practice exam but for the final exam
46:28 - landing you a passing grade getting you
46:31 - certified
46:33 - [Music]
46:38 - welcome back and in this section i
46:40 - wanted to really hone in on the basics
46:43 - of cloud computing the characteristics
46:46 - that make it what it is
46:48 - the different types of computing
46:50 - and how they differ from each other as
46:52 - well as the types of service models now
46:55 - in this lesson i wanted to dive into the
46:58 - definition of cloud computing and the
47:01 - essential characteristics that define it
47:04 - now for some advanced folk watching this
47:06 - this may be a review
47:08 - and for others
47:09 - this may fulfill a better understanding
47:12 - on what is cloud now cloud is a term
47:15 - that is thrown around a lot these days
47:17 - yet holds a different definition or
47:20 - understanding to each and every
47:22 - individual
47:24 - you could probably ask 10 people on
47:27 - their definition of cloud and chances
47:29 - are
47:30 - everyone would have their own take on it
47:34 - many see cloud as this abstract thing in
47:37 - the sky
47:38 - where files and emails are stored but
47:41 - it's so much more than that
47:43 - now the true definition of it can be put
47:46 - in very simple terms
47:48 - and can be applied to any public cloud
47:51 - being google cloud
47:53 - aws
47:54 - and azure
47:56 - moving on to the definition
47:58 - cloud computing is the delivery of a
48:01 - shared pool of on-demand computing
48:04 - services over the public internet
48:07 - that can be rapidly provisioned
48:09 - and released
48:10 - with minimal management effort or
48:13 - service provider interaction
48:15 - these computing services
48:17 - consist of things like servers
48:20 - storage
48:21 - networking and databases they can be
48:24 - quickly provisioned and accessed from
48:26 - your local computer
48:28 - over an internet connection now coupled
48:30 - with this definition are five essential
48:33 - characteristics that define the cloud
48:35 - model that i would like to go over with
48:38 - you and i believe that it would hold
48:40 - massive benefits to understanding when
48:43 - speaking to cloud this information can
48:46 - be found in the white paper published by
48:49 - the national institute of standards and
48:52 - technology i will include a link to this
48:54 - publication in the lesson notes for your
48:57 - review now these essential
48:59 - characteristics are as follows the first
49:02 - one
49:03 - is on-demand self-service
49:06 - and this can be defined as being able to
49:08 - provision resources automatically
49:11 - without requiring human interaction on
49:14 - the provider's end
49:15 - so in the end you will never need to
49:17 - call up or interact with the service
49:20 - provider in order to get resources
49:22 - provisioned for you
49:24 - as well you have the flexibility of
49:26 - being able to provision and de-provision
49:29 - these resources
49:31 - whenever you need them and at any given
49:33 - time of the day
49:35 - the second characteristic is broad
49:37 - network access
49:39 - now this simply means that cloud
49:41 - computing resources are available over
49:43 - the network and can be accessed by many
49:46 - different customer platforms such as
49:49 - mobile phones
49:50 - tablets or computers
49:53 - in other words
49:54 - cloud services are available over a
49:56 - network moving into the third
49:59 - is resource pooling
50:02 - so the provider's computing resources
50:04 - are pooled together to support a
50:06 - multi-tenant model that allows multiple
50:10 - customers to share the same applications
50:13 - or the same physical infrastructure
50:16 - while retaining privacy and security
50:18 - over their information
50:20 - this includes things like processing
50:22 - power
50:23 - memory storage and networking
50:26 - it's similar to people living in an
50:28 - apartment building sharing the same
50:31 - building infrastructure like power and
50:33 - water yet they still have their own
50:35 - apartments and privacy within that
50:38 - infrastructure
50:39 - this also creates a sense of location
50:42 - independence in that the customer
50:44 - generally has no control or knowledge
50:48 - over the exact location of the provided
50:51 - resources
50:52 - but they may be able to specify location
50:55 - at a higher level of abstraction so in
50:58 - the end the customer does not really
51:00 - have the option of choosing exactly
51:02 - which server server rack or data center
51:05 - for that matter
51:07 - of where the provided resources are
51:09 - coming from
51:10 - they will only be able to have the
51:12 - option to choose things like regions or
51:16 - sections within that region
51:19 - the fourth essential characteristic is
51:21 - rapid elasticity
51:24 - this to me
51:25 - is the key factor of what makes cloud
51:27 - computing so great and so agile
51:31 - capabilities can be elastically
51:33 - provisioned and released
51:35 - in some cases automatically to scale
51:38 - rapidly outwards and inwards in response
51:42 - with demand
51:43 - to the consumer the capabilities
51:46 - available for provisioning often appear
51:48 - to be unlimited
51:50 - and can be provisioned in any quantity
51:53 - at any time and touching on the fifth
51:56 - and last characteristic cloud systems
51:59 - automatically control and optimize
52:02 - resource usage by leveraging a metering
52:05 - capability
52:06 - resource usage can be monitored
52:09 - controlled and reported providing
52:12 - transparency for both the provider and
52:15 - consumer of the service
52:17 - now what this means is that cloud
52:19 - computing resource usage is metered and
52:22 - you can pay accordingly for what you've
52:25 - used
52:26 - resource utilization
52:27 - can be optimized by leveraging
52:30 - pay-per-use capabilities
52:33 - and this means that cloud resource usage
52:35 - whether they are instances that are
52:37 - running
52:38 - cloud storage or bandwidth it all gets
52:42 - monitored measured and reported by the
52:44 - cloud service provider the cost model is
52:47 - based on
52:48 - pay for what you use and so the payment
52:51 - is based on the actual consumption
52:54 - by the customer
52:56 - so knowing these key characteristics of
52:58 - cloud computing along with their
53:00 - benefits
53:01 - i personally find can really give you a
53:04 - leg up on the exam
53:06 - as well as speaking to others in your
53:08 - day-to-day role
53:10 - as more and more companies start moving
53:12 - to cloud i hope this lesson has
53:15 - explained to you on what is cloud
53:17 - computing and the benefits it provides
53:20 - so that's all i have for this lesson
53:22 - so you can now mark this lesson as
53:24 - complete and let's move on to the next
53:27 - one
53:31 - welcome back
53:32 - in this lesson i wanted to go over the
53:34 - four common cloud deployment models and
53:37 - distinguish the differences between
53:40 - public cloud
53:41 - multi-cloud
53:42 - private cloud and hybrid cloud
53:44 - deployment models
53:46 - this is a common subject that comes up a
53:48 - fair amount in the exam
53:50 - as well as a common theme in any
53:52 - organization moving to cloud knowing the
53:55 - distinctions between them can be
53:57 - critical to the types of architecture
54:00 - and services that you would use for the
54:03 - specific scenario you are given
54:06 - as well as being able to speak to the
54:08 - different types of deployment models as
54:11 - an engineer in the field getting back to
54:13 - the deployment models let's start with
54:15 - the public cloud model which we touched
54:18 - on a bit in our last lesson
54:20 - now the public cloud is defined as
54:23 - computing services offered by
54:25 - third-party providers
54:27 - over the public internet
54:29 - making them available to anyone who
54:32 - wants to use or purchase them so this
54:34 - means that google cloud will fall under
54:37 - this category as a public cloud
54:41 - there are also other vendors that fall
54:43 - under this category such as aws and
54:47 - azure
54:48 - so again public cloud is a cloud that is
54:51 - offered over the public internet
54:54 - now public clouds can also be connected
54:57 - and used together within a single
55:00 - environment for various use cases
55:03 - this cloud deployment model is called
55:06 - multi-cloud
55:07 - now a multi-cloud implementation can be
55:10 - extremely effective if architected in
55:13 - the right way
55:15 - one implementation that is an effective
55:17 - use of multi-cloud is when it is used
55:20 - for disaster recovery this is where your
55:23 - architecture would be replicated across
55:26 - the different public clouds in case one
55:29 - were to go down
55:31 - another could pick up the slack what
55:33 - drives many cases of a multi-cloud
55:35 - deployment
55:36 - is to prevent vendor lock-in where you
55:39 - are locked into a particular cloud
55:41 - provider's infrastructure and unable to
55:44 - move due to the vendor-specific feature
55:47 - set the main downfall to this type of
55:49 - architecture is that the infrastructure
55:52 - of the public cloud that you're using
55:54 - cannot be fully utilized
55:56 - as each cloud vendor has their own
55:59 - proprietary resources that will only
56:02 - work in their specific infrastructure in
56:05 - other words
56:06 - in order to replicate the environment it
56:08 - needs to be the same within each cloud
56:12 - this removes each cloud's unique
56:14 - features which is what makes them so
56:17 - special and the resources so compelling
56:20 - so sometimes finding the right strategy
56:23 - can be tricky depending on the scenario
56:27 - now the next deployment model i wanted
56:29 - to touch on is private cloud private
56:32 - cloud refers to your architecture that
56:34 - exists on premise
56:37 - and restricted to the business itself
56:39 - with no public access
56:42 - yet it still carries the same five
56:44 - characteristics that we discussed with
56:47 - regards to what defines cloud each of
56:50 - the major cloud providers shown here
56:52 - all have their own flavor of private
56:54 - cloud that can be implemented on site
56:57 - google cloud has anthos
57:00 - aws
57:01 - has aws outposts
57:03 - and azures is azure stack
57:06 - they show the same characteristic
57:09 - and leverage similar technologies that
57:12 - can be found in the vendor's public
57:14 - cloud yet can be installed on your own
57:18 - on-premise infrastructure please be
57:20 - aware
57:21 - any organizations may have a vmware
57:24 - implementation which holds cloud-like
57:27 - features yet this is not considered a
57:30 - private cloud
57:31 - true private cloud will always meet the
57:34 - characteristics that make up cloud now
57:37 - it is possible to use private cloud with
57:40 - public cloud
57:41 - and this implementation is called hybrid
57:44 - cloud
57:45 - so hybrid cloud is when you are using
57:48 - public cloud in conjunction with private
57:51 - cloud as a single system a common
57:54 - architecture used is due to compliance
57:57 - where one cloud could help organizations
58:00 - achieve specific governance
58:03 - risk management and compliance
58:05 - regulations while the other cloud could
58:08 - take over the rest
58:10 - now i'd really like to make an important
58:12 - distinction here
58:13 - if your on-premise infrastructure is
58:16 - connected to public cloud this is not
58:19 - considered hybrid cloud this is what's
58:22 - known as hybrid environment or a hybrid
58:25 - network as the on-premises
58:28 - infrastructure holds no private cloud
58:31 - characteristics true hybrid cloud allows
58:34 - you to use the exact same interface and
58:37 - tooling as what's available in the
58:40 - public cloud so being aware of this can
58:42 - avoid a lot of confusion down the road
58:45 - so to sum up everything that we
58:47 - discussed when it comes to public cloud
58:50 - this is when one cloud provided by one
58:53 - vendor that is available over the public
58:56 - internet
58:57 - multi-cloud is two or more public clouds
59:01 - that are connected together to be used
59:03 - as a single system a private cloud
59:07 - is considered an on-premises cloud that
59:10 - follows the five characteristics of
59:12 - cloud
59:13 - and is restricted to the one
59:16 - organization with no accessibility to
59:19 - the public and finally hybrid cloud
59:22 - is private cloud connected to a public
59:25 - cloud
59:26 - and being used as a single environment
59:29 - again as a note
59:31 - on-premises architecture connected to
59:33 - public cloud is considered a hybrid
59:36 - environment and not hybrid cloud
59:40 - the distinction between the two
59:42 - are very different and should be
59:44 - observed carefully as gotchas may come
59:47 - up in both the exam
59:49 - and in your role as an engineer so these
59:52 - are all the different cloud deployment
59:53 - models which will help you distinguish
59:56 - on what type of architecture you will be
59:59 - using in any scenario that you are given
60:02 - and so this is all i wanted to cover
60:04 - when it comes to cloud deployment models
60:06 - so you can now mark this lesson as
60:08 - complete
60:09 - and let's move on to the next one
60:15 - welcome back
60:16 - so to finish up the nist definition of
60:19 - cloud computing i wanted to touch on
60:22 - cloud service models which is commonly
60:25 - referred to as zas
60:27 - now this model is usually called zas or
60:31 - xaas
60:32 - standing for anything as a service it
60:35 - includes all the services in a cloud
60:38 - that customers can consume and x can be
60:41 - changed to associate with the specific
60:44 - service
60:45 - so in order to describe the cloud
60:47 - service models i needed to touch on some
60:50 - concepts that you may or may not be
60:52 - familiar with this will make
60:54 - understanding the service models a
60:56 - little bit easier as i go through the
60:58 - course and describe the services
61:01 - available and how they relate to the
61:03 - model this lesson will make so much
61:05 - sense by the end
61:07 - it'll make the services in cloud easier
61:09 - to both describe and define
61:12 - now when it comes to deploying an
61:14 - application they are deployed in an
61:16 - infrastructure stack
61:18 - like the one you see here
61:20 - now a stack is a collection of needed
61:23 - infrastructure that the application
61:26 - needs to run on it is layered and each
61:29 - layer builds on top of the one previous
61:31 - to it to create what it is that you see
61:34 - here now as you can see at the top this
61:37 - is a traditional on-premises
61:40 - infrastructure stack that was typically
61:42 - used pre-cloud now in this traditional
61:45 - model
61:46 - all the components are managed by the
61:48 - customer the purchasing of the data
61:50 - center and all the network and storage
61:53 - involved the physical servers the
61:56 - virtualization the licensing for the
61:59 - operating systems the staff that's
62:01 - needed to put it all together
62:03 - including racking stacking cabling
62:06 - physical security was also something
62:08 - that needed to be taken into
62:10 - consideration in other words for the
62:12 - organization to put this together by
62:15 - themselves they were looking at huge
62:18 - costs
62:18 - now the advantages to this is that it
62:21 - allowed for major flexibility as the
62:24 - organization is able to tune this any
62:27 - way they want to satisfy the application
62:30 - compliance standards
62:32 - basically anything that they wanted now
62:34 - when talking about the cloud service
62:36 - model concepts
62:37 - parts are always managed by you and
62:40 - parts are managed by the vendor now
62:42 - another concept i wanted to touch on is
62:45 - that unit of consumption is how the
62:47 - vendor prices what they are serving to
62:50 - their customer now just before cloud
62:52 - became big in the market there was a
62:55 - model where the data center was hosted
62:58 - for you so a vendor would come along and
63:00 - they would take care of everything with
63:02 - regards to the data center
63:04 - the racks
63:05 - the power to the racks the air
63:07 - conditioning
63:09 - the networking cables out of the
63:10 - building and even the physical security
63:13 - and so the unit of consumption here was
63:16 - the rack space within the data center so
63:18 - the vendor would charge you for the rack
63:20 - space and in turn they would take care
63:23 - of all the necessities within the data
63:25 - center now this is less flexible than
63:28 - the traditional on-premises model but
63:31 - the data center is abstracted for you so
63:34 - throughout this lesson i wanted to
63:36 - introduce a concept that might make
63:38 - things easier to grasp which is the
63:41 - pizza as a service so now the
63:43 - traditional on-premises model
63:45 - is where you would buy everything and
63:48 - make the pizza at home
63:49 - now as we go on in the lesson
63:52 - less flexibility will be available
63:54 - because more layers will be abstracted
63:57 - so the next service model that i wanted
63:59 - to introduce is infrastructure as a
64:02 - service
64:03 - or i as for short this is where all the
64:06 - layers from the data center up to
64:08 - virtualization is taken care of by the
64:11 - vendor this is the most basic model
64:14 - which is essentially your virtual
64:16 - machines in a cloud data center
64:18 - you set up configure
64:21 - and manage instances that run in the
64:23 - data center infrastructure and you put
64:26 - whatever you want on them on google
64:28 - cloud google compute engine would
64:30 - satisfy this model and so the unit of
64:33 - consumption here would be the operating
64:35 - system as you would manage all the
64:38 - operating system updates and everything
64:40 - that you decide to put on that instance
64:43 - but as you can see here you are still
64:45 - responsible for the container the run
64:48 - time the data and the application layers
64:51 - now bringing up the pizza as a service
64:53 - model is would be you picking up the
64:57 - pizza and you cooking it at home moving
64:59 - on to platform as a service or paz for
65:03 - short this is a model that is geared
65:05 - more towards developers and with pass
65:08 - the cloud provider provides a computing
65:11 - platform typically
65:13 - including the operating system the
65:15 - programming language execution
65:17 - environment the database and the web
65:20 - server now typically with pass you never
65:23 - have to worry about the operating system
65:25 - updates or managing the runtime and
65:28 - middleware and so the unit of
65:29 - consumption here would be the runtime
65:32 - now the runtime layer would be the layer
65:34 - you would consume as you would be
65:36 - running your code in the supplied
65:38 - runtime environment that the cloud
65:41 - vendor provides for you the provider
65:43 - manages the hardware and software
65:46 - infrastructure and you just use the
65:48 - service this is usually the layer on top
65:51 - of is and so all the layers between the
65:54 - data center and runtime is taken care of
65:57 - by the vendor a great example of this
66:00 - for google cloud is google app engine
66:03 - which we will be diving into a little
66:05 - bit later getting back to the pizza as a
66:08 - service model
66:09 - pass would fall under the pizza being
66:11 - delivered right to your door
66:13 - now with the past model explained i want
66:16 - to move into the last model which is sas
66:19 - which stands for software as a service
66:22 - now with sas all the layers are taken
66:24 - care of by the vendor so users are
66:27 - provided access to application software
66:30 - and cloud providers manage the
66:32 - infrastructure and platforms that run
66:34 - the applications g suite and microsoft's
66:37 - office 365 are great examples of this
66:41 - model now sas doesn't offer much
66:43 - flexibility but the trade-off is that
66:46 - the vendor actually takes care of all
66:48 - these layers so again the unit of
66:50 - consumption here is the application
66:53 - itself and of course getting to the
66:55 - pizza as a service model sas
66:58 - is pretty much dining in the restaurant
67:01 - enjoying your pizza now to summarize
67:03 - when you have a data center on site you
67:06 - manage everything
67:08 - when it's infrastructure as a service
67:10 - part of that stack is abstracted by the
67:13 - cloud vendor with platform as a service
67:16 - you're responsible for the application
67:19 - and data
67:20 - everything else is abstracted by the
67:22 - vendor with software as a service again
67:25 - using the pizza as a service analogy on
67:28 - premise you buy everything and you make
67:30 - the pizza at home infrastructure as a
67:33 - service
67:34 - you pick up the pizza and you cook it at
67:36 - home when it comes to platform as a
67:38 - service the pizza is delivered
67:41 - and of course software as a service is
67:43 - dining in the restaurant now there will
67:46 - be some other service models coming up
67:48 - in this course such as function as a
67:50 - service and containers as a service and
67:53 - don't worry i'll be getting into those
67:55 - later but i just wanted to give you a
67:57 - heads up so now for some of you this may
68:00 - have been a lot of information to take
68:02 - in but trust me
68:04 - knowing these models will give you a
68:06 - better understanding of the services
68:09 - provided in google cloud as well as any
68:12 - other cloud vendor so that's all i
68:14 - wanted to cover in this lesson so you
68:16 - can now mark this lesson as complete and
68:18 - let's move on to the next one
68:24 - welcome back in this lesson i wanted to
68:27 - discuss google cloud global
68:29 - infrastructure how data centers are
68:31 - connected how traffic flows when a
68:34 - request is done
68:35 - along with the overall structure of how
68:38 - google cloud geographic locations are
68:41 - divided for better availability
68:44 - durability and latency
68:47 - now google holds a highly provisioned
68:50 - low latency network where your traffic
68:52 - stays on google's private backbone for
68:55 - most of its journey ensuring high
68:58 - performance and a user experience that
69:01 - is always above the norm google cloud
69:03 - has been designed to serve users all
69:06 - around the world by designing their
69:08 - infrastructure with redundant cloud
69:11 - regions connected with high bandwidth
69:14 - fiber cables as well as subsea cables
69:17 - connecting different continents
69:19 - currently google has invested in 13
69:22 - subsea cables connecting these
69:25 - continents at points of presence as you
69:28 - see here in this diagram hundreds of
69:30 - thousands of miles of fiber cables have
69:33 - also been laid to connect points of
69:36 - presence for direct connectivity
69:39 - privacy and reduced latency
69:42 - just to give you an idea of what a
69:44 - subsea cable run might look like i have
69:47 - included a diagram of how dedicated
69:50 - google is to their customers as there is
69:53 - so much that goes into running these
69:55 - cables that connect continents as you
69:58 - can see here this is the north virginia
70:00 - region being connected to the belgium
70:03 - region from the u.s over to europe a
70:07 - cable is run from the north virginia
70:09 - data center as well as having a point of
70:12 - presence in place
70:14 - going through a landing station before
70:16 - going deep into the sea on the other
70:19 - side the landing station on the french
70:22 - west coast
70:23 - picks up the other side of the cable and
70:25 - brings it over to the data center in the
70:28 - belgium region and this is a typical
70:30 - subsea cable run for google so
70:33 - continents are connected for maximum
70:35 - global connectivity
70:37 - now at the time of recording this video
70:40 - google cloud footprint spans 24 regions
70:44 - 73 zones and over
70:47 - 144 points of presence across more than
70:52 - 200 countries and territories worldwide
70:55 - and as you can see here the white dots
70:58 - on the map are regions that are
71:00 - currently being built to expand their
71:02 - network for wider connectivity now to
71:06 - show you how a request is routed through
71:08 - google's network i thought i would
71:10 - demonstrate this by using tony bowtie
71:14 - now tony makes a request to his database
71:17 - in google cloud and google responds to
71:20 - tony's request
71:21 - from a pop or edge network location that
71:25 - will provide the lowest latency this
71:27 - point of presence is where isps can
71:30 - connect to google's network google's
71:33 - edge network receives tony's request and
71:36 - passes it to the nearest google data
71:38 - center over its private fiber network
71:41 - the data center generates a response
71:44 - that's optimized to provide the best
71:46 - experience for tony at that given moment
71:49 - in time the app or browser that tony is
71:51 - using retrieves the requested content
71:54 - with a response back from various google
71:57 - locations including the google data
72:00 - centers edge pops and edge nodes
72:03 - whichever is providing the lowest
72:05 - latency this data path happens in a
72:08 - matter of seconds and due to google's
72:11 - global infrastructure it travels
72:13 - securely and with the least amount of
72:16 - latency possible
72:17 - no matter the geographic location that
72:20 - the request is coming from
72:22 - now i wanted to take a moment to break
72:24 - down how the geographic areas are broken
72:28 - out and organized in google cloud
72:31 - we start off with the geographic
72:33 - location such as the united states of
72:36 - america and it's broken down into
72:38 - multi-region into regions and finally
72:41 - zones and so to start off with i wanted
72:44 - to talk about zones now a zone is a
72:47 - deployment area for google cloud
72:49 - resources within a region a zone is the
72:52 - smallest entity in google's global
72:55 - network you can think of it as a single
72:57 - failure domain within a region now as a
73:00 - best practice resources should always be
73:03 - deployed
73:04 - in zones that are closest to your users
73:07 - for optimal latency
73:09 - now next up we have a region
73:12 - and regions are independent geographic
73:15 - areas that are subdivided into zones so
73:18 - you can think of a region as a
73:20 - collection of zones and having a region
73:23 - with multiple zones is designed for
73:26 - fault tolerance and high availability
73:29 - the intercommunication between zones
73:32 - within a region is under five
73:34 - milliseconds so rest assured that your
73:37 - data is always traveling at optimal
73:40 - speeds
73:41 - now moving on into a multi-region
73:44 - now multi-regions are large geographic
73:47 - areas that contain two or more regions
73:51 - and this allows google services to
73:53 - maximize redundancy and distribution
73:57 - within and across regions
73:59 - and this is for google redundancy or
74:02 - high availability having your data
74:04 - spread across multiple regions
74:07 - always reassures that your data is
74:10 - constantly available
74:12 - and so that covers all the concepts that
74:14 - i wanted to go over when it comes to
74:17 - geography and regions within google
74:20 - cloud
74:21 - note that the geography and regions
74:23 - concepts are fundamental not only for
74:26 - the exam but for your day-to-day role in
74:30 - google cloud
74:31 - so just as a recap a zone is a
74:34 - deployment area for google cloud
74:37 - resources within a region a zone is the
74:40 - smallest entity of google's global
74:42 - infrastructure now a region is an
74:45 - independent geographic area that are
74:48 - subdivided into zones and finally when
74:51 - it comes to multi-region
74:53 - multi-regions are large geographic areas
74:57 - that contains two or more regions
75:00 - again
75:01 - these are all fundamental concepts that
75:03 - you should know for the exam and for
75:06 - your day-to-day role in google cloud and
75:09 - so that's all i had for this lesson so
75:12 - you can now mark this lesson as complete
75:14 - and let's move on to the next one
75:17 - [Music]
75:21 - welcome back
75:22 - this lesson is going to be an overview
75:24 - of all the compute service options that
75:27 - are available in google cloud
75:29 - how they differ from each other
75:31 - and where they fall under the cloud
75:33 - service model again this lesson is just
75:36 - an overview of the compute options as we
75:39 - will be diving deeper into each compute
75:41 - option
75:42 - later on in this course so google cloud
75:45 - gives you so many options when it comes
75:47 - to compute services ones that offer
75:50 - complete control and flexibility others
75:53 - that offer flexible container technology
75:55 - managed application platform and
75:58 - serverless environments and so when we
76:00 - take all of these compute options and we
76:03 - look at it from a service model
76:05 - perspective you can see that there's so
76:07 - much flexibility starting here on the
76:09 - left with infrastructure as a service
76:12 - giving you the most optimal flexibility
76:15 - moving all the way over to the right
76:17 - where we have function as a service
76:20 - offering less flexibility but the upside
76:23 - being less that you have to manage and
76:25 - we'll be going through these compute
76:27 - options starting on the left here with
76:30 - infrastructure as a service we have
76:32 - compute engine now compute engine is
76:35 - google's staple infrastructure the
76:37 - service product that offers virtual
76:39 - machines or vms called instances these
76:43 - instances can be deployed in any region
76:46 - or zone that you choose you also have
76:48 - the option of deciding what operating
76:50 - system you want on it as well as the
76:53 - software so you have the option of
76:55 - installing different types of flavors of
76:57 - linux or windows and the software to go
77:00 - with it google also gives you the
77:02 - options of creating these instances
77:04 - using public or private images
77:07 - so if you or your company have a private
77:09 - image that you'd like to use you can use
77:12 - this to create your instances google
77:14 - also gives you the option to use public
77:17 - images to create instances and are
77:19 - available when you launch compute engine
77:21 - as well there are also pre-configured
77:24 - images and software packages available
77:27 - in the google cloud marketplace and we
77:29 - will be diving a little bit deeper into
77:31 - the google cloud marketplace in another
77:34 - lesson just know that there are slew of
77:36 - images out there that's available to
77:39 - create instances giving you the ease to
77:42 - deploy now when it comes to compute
77:44 - engine and you're managing multiple
77:46 - instances these are done using instance
77:49 - groups
77:50 - and when you're looking at adding or
77:52 - removing capacity for those compute
77:54 - engine instances automatically you would
77:57 - use auto scaling in conjunction with
77:59 - those instance groups compute engine
78:01 - also gives you the option of attaching
78:04 - and detaching disks as you need them as
78:07 - well google cloud storage can be used in
78:10 - conjunction with compute engine as
78:12 - another storage option and when
78:14 - connecting directly to compute engine
78:17 - google gives you the option of using ssh
78:20 - to securely connect to it so moving on
78:22 - to the next compute service option
78:24 - we have google kubernetes engine also
78:27 - known as gke
78:29 - now gke
78:31 - is google's flagship container
78:33 - orchestration system
78:35 - for automating
78:36 - deploying
78:37 - scaling and managing containers
78:40 - gke is also built on the same open
78:44 - source kubernetes project that was
78:46 - introduced by google to the public back
78:49 - in 2014
78:51 - now before google made kubernetes a
78:53 - managed service there was many that
78:55 - decided to build kubernetes on premise
78:58 - in their data centers and because it is
79:01 - built on the same platform
79:03 - gke
79:04 - offers the flexibility of integrating
79:07 - with these on-premise kubernetes
79:09 - deployments now under the hood gke uses
79:12 - compute engine instances as nodes in a
79:16 - cluster and as a quick note a cluster is
79:19 - a group of nodes or compute engine
79:21 - instances and again we'll be going over
79:24 - all this in much greater detail in a
79:27 - different lesson so if you haven't
79:29 - already figured it out google kubernetes
79:32 - engine is considered container as a
79:34 - service now the next compute service
79:36 - option that i wanted to go over
79:39 - that falls under platform as a service
79:41 - is app engine
79:43 - now app engine is a fully managed
79:46 - serverless platform for developing and
79:48 - hosting web applications at scale now
79:52 - with app engine google handles most of
79:54 - the management of the resources for you
79:57 - for example if your application requires
79:59 - more computing resources because traffic
80:02 - to your website increases google
80:04 - automatically scales the system to
80:06 - provide these resources if the system
80:09 - software needs a security update as well
80:12 - that's handled for you too and so all
80:14 - you need to really take care of is your
80:17 - application
80:18 - and you can build your application in
80:20 - your favorite language go java.net and
80:24 - many others
80:25 - and you can use both pre-configured
80:27 - runtimes or use custom runtimes to allow
80:31 - you to write the code in any language
80:34 - app engine also allows you to connect
80:36 - with google cloud storage products and
80:38 - databases seamlessly app engine also
80:41 - offers the flexibility of connecting
80:44 - with third-party databases as well as
80:47 - other cloud providers and third-party
80:49 - vendors app engine also integrates with
80:52 - a well-known security product in google
80:54 - cloud called web security scanner as to
80:57 - identify security vulnerabilities and so
81:00 - that covers app engine in a nutshell
81:02 - moving on to the next compute service
81:04 - option
81:05 - we have cloud functions and cloud
81:08 - functions fall under function as a
81:10 - service this is a serverless execution
81:13 - environment
81:14 - for building and connecting cloud
81:16 - services with cloud functions you write
81:19 - simple single purpose functions that are
81:22 - attached to events
81:24 - that are produced from your
81:26 - infrastructure and services in google
81:28 - cloud your function is triggered when an
81:31 - event being watched is fired your code
81:34 - then executes in a fully managed
81:36 - environment there is no need to
81:38 - provision any infrastructure or worry
81:41 - about managing any servers and cloud
81:43 - functions can be written using
81:45 - javascript python 3
81:48 - go or java runtimes so you can take your
81:51 - function and run it in any of these
81:54 - standard environments which makes it
81:56 - extremely portable now cloud functions
81:59 - are a good choice for use cases that
82:01 - include the following
82:03 - data processing or etl operations such
82:06 - as video transcoding and iot streaming
82:09 - data web hooks that respond to http
82:12 - triggers
82:13 - lightweight apis that compose loosely
82:16 - coupled logic into applications as well
82:19 - as mobile back-end functions
82:21 - again cloud functions are considered
82:23 - function as a service and so that covers
82:26 - cloud functions
82:28 - now moving to the far right of the
82:29 - screen on the other side of the arrow we
82:32 - have our last compute service option
82:34 - which is cloud run now cloud run is a
82:38 - fully managed compute platform for
82:40 - deploying and scaling containerized
82:42 - applications quickly and securely
82:46 - cloudrun was built on an open standard
82:48 - called k native and this enabled the
82:50 - portability of any applications that
82:53 - were built on it cloudrun also abstracts
82:56 - away all the infrastructure management
82:59 - by automatically scaling up and down
83:01 - almost instantaneously depending on the
83:04 - traffic now cloud run was google's
83:07 - response to abstracting all the
83:09 - infrastructure that was designed to run
83:12 - containers and so this is known as
83:14 - serverless for containers cloudrun has
83:17 - massive flexibility as you can write it
83:20 - in any language
83:22 - any library using any binary this
83:25 - compute service is considered a function
83:28 - as a service now at the time of
83:30 - recording this video i have not heard of
83:32 - cloud cloudrun being in the exam but
83:35 - since it is a compute service option i
83:38 - felt the need for cloudrun to have an
83:40 - honorable mention and so these are all
83:43 - the compute service options that are
83:45 - available on google cloud and we will be
83:48 - diving deeper into each one of these
83:51 - later on in this course
83:53 - again this is just an overview of all
83:55 - the compute service options that are
83:57 - available on the google cloud platform
84:00 - and so that's all i wanted to cover for
84:02 - this lesson
84:03 - so you can now mark this lesson as
84:05 - complete and let's move on to the next
84:07 - one
84:08 - [Music]
84:12 - welcome back
84:13 - now in the last lesson i covered all the
84:16 - different options for compute services
84:19 - in this lesson we're going to cover the
84:21 - options that are available that couple
84:23 - well with these compute services by
84:26 - diving deeper into the different storage
84:28 - types and the different databases
84:31 - available on google cloud again this is
84:34 - strictly an overview as i will be diving
84:37 - deeper into these services later on in
84:40 - the course
84:41 - now when it comes to storage options
84:43 - there are three services that are
84:45 - readily available to you in google cloud
84:48 - each of them have their own specific use
84:50 - case that i will be diving into in just
84:53 - a second
84:54 - the first one i wanted to go over is
84:56 - cloud storage
84:57 - now with cloud storage this is google's
85:00 - consistent scalable
85:02 - large capacity and highly durable object
85:06 - storage
85:07 - so when i refer to object storage this
85:09 - is not the type of storage that you
85:11 - would attach to your instance and store
85:14 - your operating system on i'm talking
85:16 - about managing data as objects such as
85:20 - documents or pictures and shouldn't be
85:22 - confused with block storage which
85:24 - manages data at a more granular level
85:27 - such as an operating system not to worry
85:30 - if you fully don't grasp the concept of
85:32 - object storage i will be going into
85:34 - further detail with that
85:36 - later on in the cloud storage lesson
85:38 - cloud storage has 11 9's durability and
85:42 - what i mean by durability is basically
85:45 - loss of files so just to give you a
85:47 - better picture on cloud storage
85:49 - durability if you store 1 million files
85:53 - statistically google would lose one file
85:56 - every 659
85:59 - 000 years and you are about over 400
86:02 - times more likely to get hit by a meteor
86:05 - than to actually lose a file so as you
86:08 - can see cloud storage is a very good
86:10 - place to be storing your files another
86:13 - great feature on cloud storage is the
86:15 - unlimited storage that it has with no
86:18 - minimum object size so feel free to
86:21 - continuously put files in cloud storage
86:24 - now when it comes to use cases cloud
86:26 - storage is fantastic for content
86:29 - delivery data lakes and backups and to
86:32 - make cloud storage even more flexible it
86:35 - is available in different storage
86:37 - classes and availability which i will be
86:40 - going over in just a second now when it
86:42 - comes to these different storage classes
86:45 - there are four different classes that
86:47 - you can choose from the first one is the
86:49 - standard storage class and this storage
86:52 - class offers the maximum availability
86:54 - with your data with absolutely no
86:57 - limitations this is great for storage
87:00 - that you access all the time the next
87:02 - storage class is near line and this is
87:05 - low-cost archival storage so this
87:08 - storage class is cheaper than standard
87:11 - and is designed for storage that only
87:13 - needs to be accessed less than once a
87:15 - month and if you're looking for an even
87:17 - more cost effective solution cloud
87:20 - storage has cold line storage class
87:22 - which is an even lower cost archival
87:25 - storage solution this storage class is
87:27 - designed for storage that only needs to
87:30 - be accessed less than once every quarter
87:33 - and just when you thought that the
87:34 - prices couldn't get lower than cold line
87:37 - cloud storage has offered another
87:39 - storage class called archive and this is
87:42 - the lowest cost archival storage which
87:45 - offers storage at a fraction of a penny
87:48 - per gigabyte but is designed for
87:50 - archival or backup use that is accessed
87:53 - less than once a year now when it comes
87:55 - to cloud storage availability there are
87:58 - three options that are available
88:01 - there is region dual region and
88:03 - multi-region region is designed to store
88:06 - your data in one single region dual
88:09 - region is exactly how it sounds which is
88:11 - a pair of regions now in multiregion
88:14 - cloud storage stores your data over a
88:16 - large geographic area consisting of many
88:20 - different regions across that same
88:22 - selected geographic area and so that
88:25 - about covers cloud storage as a storage
88:28 - option the next storage option that i
88:29 - wanted to talk about is file store
88:32 - now file store is a fully managed nfs
88:36 - file server from google cloud that is
88:38 - nfs version 3 compliant you can store
88:41 - data from running applications from
88:44 - multiple vm instances and kubernetes
88:47 - clusters
88:48 - accessing the data at the same time file
88:52 - store is a great option for when you're
88:53 - thinking about accessing data from let's
88:56 - say an instance group and you need
88:58 - multiple instances to access the same
89:01 - data and moving on to the last storage
89:04 - option we have persistent disks
89:07 - now with persistent disks this is
89:09 - durable block storage for instances now
89:12 - as i explained before block storage is
89:15 - different than object storage
89:17 - if you remember previously i explained
89:20 - that object storage is designed to store
89:23 - objects such as data or photos or videos
89:27 - whereas block storage is raw storage
89:29 - capacity that is used in drives that are
89:33 - connected to an operating system in this
89:35 - case persistent disks are doing just
89:38 - that persistent disks come in two
89:40 - options
89:42 - the first one is the standard option
89:44 - which gives you regular standard storage
89:47 - at a reasonable price and the other
89:49 - option is solid state or ssd
89:53 - which gives you lower latency
89:55 - higher iops and is just all around
89:57 - faster than your standard persistent
89:59 - disk both of these options are available
90:02 - in zonal and regional options depending
90:05 - on what you need for your specific
90:08 - workload so now that i've covered all
90:10 - three storage options i wanted to touch
90:12 - into the database options that are
90:15 - available on google cloud these database
90:17 - options come in both the sql and nosql
90:21 - flavors depending on your use case now
90:24 - getting into the options themselves i
90:26 - wanted to start off going into a little
90:28 - bit of detail with the sql relational
90:31 - options so the first option is cloud sql
90:35 - and cloud sql is a fully managed
90:37 - database service that is offered in
90:39 - postgres mysql and sql server flavors
90:43 - cloud sql also has the option of being
90:46 - highly available across zones now moving
90:49 - into cloud spanner this is a scalable
90:51 - relational database service that's
90:54 - highly available not only across zones
90:56 - but across regions and if need be
90:59 - available globally cloud spanner is
91:02 - designed to support transactions strong
91:04 - consistency and synchronous replication
91:07 - moving into the nosql options there are
91:10 - four available services that google
91:12 - cloud offers moving into the first one
91:15 - is bigtable
91:17 - and bigtable is a fully managed scalable
91:20 - nosql database that has high throughput
91:24 - and low latency bigtable also comes with
91:27 - the flexibility of doing cluster
91:29 - resizing without any downtime the next
91:32 - nosql option available is datastore and
91:35 - this is google cloud's fast fully
91:38 - managed
91:39 - serverless
91:40 - nosql document database datastore is
91:43 - designed for mobile web
91:46 - and internet of things applications
91:48 - datastore has the capabilities of doing
91:51 - multi-region replication
91:53 - as well as acid transactions for those
91:56 - of you who don't know i will be covering
91:58 - acid transactions in a later lesson next
92:01 - up for nosql options is firestore and
92:05 - this is a nosql real-time database
92:08 - and is optimized for offline use if
92:11 - you're looking to store data in a
92:13 - database in real time firestore is your
92:16 - option and like bigtable you can resize
92:19 - the cluster in firestore without any
92:21 - downtime and the last nosql option is
92:24 - memorystore and this is google cloud's
92:27 - highly available
92:29 - in memory service for redis and
92:31 - memcached this is a fully managed
92:34 - service and so google cloud takes care
92:36 - of everything for you now i know this
92:38 - has been a short lesson on storage and
92:41 - database options but a necessary
92:43 - overview nonetheless of what's to come
92:46 - and so that's about all i wanted to
92:47 - cover in this lesson so you can now mark
92:50 - this lesson as complete and let's move
92:52 - on to the next one
92:54 - [Music]
92:58 - welcome back now while there are some
93:00 - services in gcp that take care of
93:02 - networking for you
93:04 - there are still others like compute
93:06 - engine that give you a bit more
93:08 - flexibility in the type of networking
93:11 - you'd like to establish
93:13 - this lesson will go over these
93:15 - networking services at a high level and
93:18 - provide you with strictly an overview to
93:21 - give you an idea on what's available for
93:24 - any particular type of scenario when it
93:27 - comes to connecting and scaling your
93:29 - network traffic i will be going into
93:32 - further details
93:34 - on these networking services in later
93:36 - lessons now i wanted to start off with
93:39 - some core networking features for your
93:41 - resources and how to govern specific
93:43 - traffic
93:44 - traveling to and from your network this
93:47 - is where networks firewalls and routes
93:50 - come into play so first i wanted to
93:52 - start off with virtual private cloud
93:55 - also known as vpc now vpc
93:59 - manages networking functionality for
94:02 - your google cloud resources
94:04 - this is a virtualized network within
94:06 - google cloud so you can picture it as
94:09 - your virtualized data center vpc is a
94:12 - core networking service
94:15 - and is also a global resource that spans
94:18 - throughout all the different regions
94:21 - available in google cloud each vpc
94:24 - contains a default network as well
94:27 - additional networks can be created in
94:29 - your project but networks cannot be
94:32 - shared between projects
94:34 - and i'll be going into further depth on
94:36 - vpc in a later lesson so now that we've
94:39 - covered vpc i wanted to get into
94:42 - firewall rules and routes now firewall
94:45 - rules segment your networks with a
94:48 - global distributive firewall to restrict
94:51 - access to resources so this governs
94:54 - traffic coming into instances on a
94:56 - network each default network has a
94:59 - default set of firewall rules that have
95:01 - already been established but don't fret
95:04 - you can create your own rules and set
95:06 - them accordingly depending on your
95:08 - workload now when it comes to routes
95:11 - this specifies how traffic should be
95:14 - routed within your vpc to get a little
95:17 - bit more granular routes specify how
95:20 - packets leaving an instance should be
95:22 - directed so it's a basic way of defining
95:25 - which way your traffic is going to
95:27 - travel moving on to the next concept i
95:30 - wanted to cover a little bit about low
95:32 - balancing and how it distributes
95:34 - workloads across multiple instances
95:37 - now we have two different types of load
95:39 - balancing and both these types of load
95:42 - balancing can be broken down to even a
95:44 - more granular level now when it comes to
95:47 - http or https low balancing this is the
95:51 - type of load balancing that covers
95:53 - worldwide auto scaling and load
95:55 - balancing over multiple regions or even
95:58 - a single region on a single global ip
96:01 - https load balancing distributes traffic
96:05 - across various regions and make sure
96:08 - that the traffic is routed to the
96:09 - closest region or in case there's
96:12 - failures amongst instances or in
96:14 - instances being bombarded with traffic
96:17 - http and https load balancing can route
96:21 - the traffic to a healthy instance in the
96:24 - next closest region another great
96:26 - feature of this load balancing is that
96:28 - it can distribute traffic based on
96:30 - content type now when it comes to
96:33 - network load balancing this is a
96:35 - regional load balancer and supports any
96:38 - and all ports
96:39 - it distributes traffic among server
96:42 - instances in the same region
96:44 - based on incoming ip protocol data such
96:47 - as address port and protocol now when it
96:50 - comes to networking
96:52 - dns plays a big part and because dns
96:55 - plays a big part in networking google
96:58 - has made this service 100 available
97:03 - on top of giving any dns queries the
97:06 - absolute lowest latency with google
97:08 - cloud dns you can publish and maintain
97:11 - dns records by using the same
97:14 - infrastructure that google uses and you
97:16 - can work with your managed zones and dns
97:19 - records such as mx records tax records
97:23 - cname records and a records and you can
97:26 - do this all through the cli
97:28 - the api
97:29 - or the sdk now some of the advanced
97:32 - connectivity options that are available
97:35 - in google cloud are cloudvpn and direct
97:38 - interconnect now cloudvpn connects your
97:41 - existing network whether it be
97:43 - on-premise or in another location
97:46 - to your vbc network through an ipsec
97:49 - connection
97:50 - the traffic is encrypted and travels
97:53 - between the two networks over the public
97:55 - internet now when it comes to direct
97:58 - interconnect this connectivity option
98:00 - allows you to connect your existing
98:02 - network to your vpc network using a
98:06 - highly available
98:07 - low latency connection this connectivity
98:10 - option does not traverse the public
98:13 - internet and merely connects to google's
98:16 - backbone and this is what gives it the
98:18 - highly available low latency connection
98:21 - a couple of other advanced connectivity
98:23 - options is direct and carrier peering
98:26 - these connections allow your traffic to
98:29 - flow through google's edge network
98:31 - locations and pairing can be done
98:33 - directly or it can be done through a
98:36 - third-party carrier and so although this
98:38 - is a very short lesson i will be going
98:41 - into greater depth on all these concepts
98:44 - in later lessons in the course so that's
98:47 - all i had to cover for this lesson so
98:49 - you can now mark this lesson as complete
98:52 - and let's move on to the next one
98:58 - welcome back in this lesson we're going
99:00 - to learn about how resources and
99:03 - entities
99:04 - are organized within google cloud and
99:06 - how permissions are inherited through
99:09 - this approach knowing this structure
99:11 - is a fundamental concept that you should
99:14 - know while working in gcp at any
99:17 - capacity so before defining what the
99:20 - resource hierarchy is i'd like to take a
99:23 - little bit of time to define what is a
99:26 - resource now in the context of google
99:29 - cloud a resource can refer to the
99:32 - service level resources that are used to
99:35 - process your workloads such as compute
99:38 - instance vms
99:40 - cloud storage buckets
99:42 - and even cloud sql databases as well as
99:45 - the account level resources that sit
99:48 - above the services
99:50 - such as the organization itself
99:53 - the folders
99:54 - and the projects of course which we will
99:57 - be getting into a little bit deeper in
99:59 - just a minute
100:01 - the resource hierarchy is google's way
100:03 - to configure and grant access
100:05 - to the various cloud resources for your
100:08 - company within google cloud both at the
100:12 - service level
100:13 - and at the account level
100:16 - the resource hierarchy in google cloud
100:18 - can truly define the granular
100:21 - permissions needed for when you need to
100:23 - configure permissions to everyone in the
100:25 - organization that actually makes sense
100:29 - so now that we covered what is a
100:30 - resource i wanted to start digging into
100:34 - the resource hierarchy and the structure
100:36 - itself now google cloud resources are
100:39 - organized hierarchically using a
100:42 - parent-child relationship this hierarchy
100:45 - is designed to map an organization's
100:48 - operational structure to google cloud
100:51 - and to manage access control and
100:54 - permissions for groups of related
100:57 - resources
100:58 - so overall
100:59 - resource hierarchy will give
101:01 - organizations better management of
101:03 - permissions and access control
101:06 - the accessibility of these resources or
101:09 - policies are controlled by identity and
101:12 - access management also known as iam a
101:16 - big component of gcp which we will be
101:18 - digging into a little bit later on in
101:21 - this course and so when an iam policy is
101:24 - set on a parent the child will inherit
101:27 - this policy respectively access control
101:30 - policies and configuration settings on a
101:33 - parent resource
101:35 - are always inherited by the child also
101:38 - please note that each child object can
101:41 - only have exactly one parent
101:44 - and that these policies are again
101:46 - controlled by iam so now to understand a
101:49 - little bit more about how the gcp
101:52 - resource hierarchy works
101:54 - i wanted to dig into the layers that
101:56 - support this hierarchy
101:58 - so this is a diagram of exactly what the
102:01 - resource hierarchy looks like in all of
102:04 - its awesomeness
102:06 - including the billing account along with
102:08 - the payments profile but we're not going
102:10 - to get into that right now i'll actually
102:12 - be covering that in a later lesson so
102:15 - more on that later
102:17 - so building the structure from the top
102:19 - down we start off with the domain or
102:22 - cloud level and as you can see here the
102:24 - domain of bowtieinc.co
102:27 - is at the top
102:28 - this is the primary identity of your
102:31 - organization at the domain level this is
102:34 - where you manage your users in your
102:36 - organizations
102:38 - so users policies and these are linked
102:41 - to g suite or cloud identity accounts
102:45 - now underneath the domain level we have
102:47 - the organization level and this is
102:50 - integrated very closely with the domain
102:53 - so with the organization level this
102:55 - represents an organization and is the
102:58 - root node of the gcp resource hierarchy
103:02 - it is associated with exactly one domain
103:05 - here we have the domain set as bowtie
103:08 - inc
103:09 - all entities or resources
103:11 - belong to and are grouped under the
103:13 - organization
103:15 - all controlled policies applied to the
103:17 - organization
103:19 - are inherited by all other entities and
103:22 - resources underneath it so any folders
103:25 - projects or resources will get those
103:28 - policies that are applied from the
103:30 - organization layer now i know that we
103:32 - haven't dug into roles as of yet
103:35 - but the one thing that i did want to
103:37 - point out is that when an organization
103:39 - is created
103:41 - an organization admin role is created
103:44 - and this is to allow full access to edit
103:47 - any or all resources
103:49 - now moving on to the folders layer this
103:52 - is an additional grouping mechanism and
103:54 - isolation boundary between each project
103:58 - in essence
103:59 - it's a grouping of other folders
104:02 - projects and resources so if you have
104:05 - different departments and teams within a
104:07 - company
104:08 - this is a great way to organize it now a
104:11 - couple of caveats when it comes to
104:13 - folders
104:14 - the first one is you must have an
104:16 - organization node and the second one is
104:19 - while a folder can contain multiple
104:21 - folders or resources
104:24 - a folder or resource can have exactly
104:27 - one parent
104:28 - now moving into the projects layer this
104:31 - is a core organizational component of
104:33 - google cloud as projects are required to
104:37 - use service level resources
104:40 - these projects are the base level
104:42 - organizing entity in gcp
104:45 - and parent all service level resources
104:48 - just as a note
104:50 - any given resource can only exist in one
104:53 - project and not multiple projects at the
104:56 - same time and moving on to the last
104:58 - layer we have the resources layer and
105:01 - this is any service level resource
105:04 - created in google cloud
105:06 - everything from compute engine instances
105:09 - to cloud storage buckets to cloud sql
105:12 - databases apis users
105:15 - all these service level resources that
105:17 - we create in google cloud fall under
105:19 - this layer now giving the hierarchy a
105:21 - little bit more context i want to touch
105:24 - on labels for just a second
105:26 - labels help categorize resources by
105:29 - using a key value pair and you can
105:31 - attach them to any resource
105:34 - and so what labels help you do is to
105:36 - break down and organize costs when it
105:39 - comes to billing now to give you some
105:41 - more structure with regards to the
105:43 - hierarchy
105:44 - under the domain level
105:46 - everything underneath this is considered
105:48 - a resource
105:50 - and to break it down even further
105:52 - everything you see from the organization
105:54 - layer to the projects layer is
105:57 - considered an account level resource
106:00 - everything in the resource layer is
106:02 - considered a service level resource and
106:05 - so this is how the google cloud resource
106:08 - hierarchy is split up and organized and
106:11 - so before i finish off this lesson i
106:14 - wanted to give you a quick run-through
106:15 - on how policies can be applied at a
106:18 - hierarchical level
106:20 - so i thought i'd bring in tony bowtie
106:22 - for a quick demo
106:24 - so just to give you an example
106:26 - tony bowtie is part of department b and
106:29 - tony's manager lark
106:31 - decides to set a policy on department
106:34 - b's folder and this policy grants
106:38 - project owner role to tony at
106:40 - bowtieinc.co so tony will have the
106:43 - project owner role for project x and for
106:46 - project y at the same time
106:49 - lark assigns laura at bowtieinc.co
106:53 - cloud storage admin role on project x
106:57 - and thus she will only be able to manage
107:00 - cloud storage buckets in that project
107:03 - this hierarchy and permission
107:05 - inheritance comes up quite a bit not
107:08 - only in the exam but is something that
107:10 - should be carefully examined when
107:12 - applying permissions
107:14 - anywhere within the hierarchy in your
107:16 - day-to-day role as an engineer
107:19 - applying permissions or policies to
107:22 - resources
107:23 - with existing policies
107:25 - may not end up getting you the desired
107:27 - results you're looking for and may have
107:30 - a chance to be overlooked now i hope
107:32 - these diagrams have given you some good
107:35 - contacts with regards to resource
107:37 - hierarchy its structure and the
107:40 - permissions applied down the chain now
107:43 - that's all i have for this lesson on
107:45 - resource hierarchy so you can now mark
107:47 - this lesson as complete and let's move
107:49 - on to the next one
107:51 - [Music]
107:55 - welcome back
107:56 - in this lesson i will be covering a few
107:58 - different topics that i will touch on
108:01 - when creating a new google cloud account
108:03 - i will be covering going over the free
108:06 - tier and the always free options the
108:08 - differences between them and a demo
108:11 - showing how you can create your own free
108:13 - tier account as well i'll also be going
108:16 - into what you will need in order to
108:18 - fulfill this demo so for the remainder
108:21 - of this course all the demos will run
108:24 - under the free tier now when i built
108:26 - this course i built it with budget in
108:28 - mind and having viewed on ways where i
108:31 - can keep the price to a minimum while
108:34 - still keeping the demos extremely useful
108:37 - and so the free tier falls within all
108:39 - these guidelines and will help you learn
108:42 - without the high ticket price and so
108:44 - getting into a quick overview of the
108:46 - differences between the free tier and
108:48 - the always free option i have broken
108:51 - them down here with their most
108:53 - significant differences in the free tier
108:55 - google cloud offers you a 12 month free
108:58 - trial with a 300
109:00 - u.s credit this type of account ends
109:03 - when the credit is used or after the 12
109:06 - months whichever happens first and so
109:09 - for those of you who are looking at
109:10 - taking advantage of this on a business
109:13 - level unfortunately the free tier only
109:16 - applies to a personal account and cannot
109:19 - be attached to a business account now
109:22 - moving over to the always free option
109:24 - the always free option isn't a special
109:26 - program but it's a regular part of your
109:29 - google cloud account it provides you
109:31 - limited access to many of the google
109:34 - cloud resources free of charge and once
109:37 - these limits have been hit
109:38 - then you are charged at the regular per
109:41 - second billing rate
109:42 - and i will show you a little bit later
109:44 - how to monitor these credits so that you
109:46 - don't go over using this in conjunction
109:49 - with the free tier account is not
109:51 - possible you have to have an upgraded
109:53 - billing account which can also include a
109:56 - business account now there are a bunch
109:58 - more stipulations in this program and i
110:01 - will include a link to both of them in
110:03 - the lesson text below for later viewing
110:06 - at your convenience
110:07 - now lastly before we get into the demo
110:10 - i wanted to go through a quick
110:11 - run-through of exactly what's needed to
110:14 - open up your free tier account
110:17 - so we're going to start off with a fresh
110:19 - new gmail address so that it doesn't
110:21 - conflict with any current gmail address
110:24 - that you may have you're gonna need a
110:25 - credit card for verification and this is
110:28 - for google to make sure that you're an
110:30 - actual human being and not a robot and
110:32 - you won't be charged unless you go above
110:35 - the 300 credit limit as well i highly
110:38 - recommend going into a private browsing
110:40 - session so whether you're using chrome
110:43 - you would use an incognito session if
110:45 - you're using firefox you would use
110:47 - private browsing and in microsoft edge
110:50 - you would be using the in private mode
110:53 - and so in order to start with this free
110:54 - trial you can head on over to the url
110:57 - listed here and i'll also include this
111:00 - in the lesson text so head on over to
111:02 - this url and i'll see you there in just
111:04 - a second
111:06 - okay so here we are at the free trial
111:08 - url i'm here in google chrome in an
111:11 - incognito session and so we're not going
111:14 - to sign up we're going to go over here
111:15 - to create account you can just click on
111:17 - create account
111:18 - for myself because as i mentioned
111:20 - earlier you're not able to create a free
111:23 - trial account with your business
111:26 - so i'm going to click on for myself
111:28 - and it's going to bring you to this page
111:29 - where it says create your google account
111:32 - and you're going to go to create a new
111:33 - gmail address instead
111:36 - and now you're going to fill in all the
111:37 - necessary information that's needed in
111:39 - order to open up this new gmail account
111:48 - once you're finished typing your
111:49 - password you can hit next
111:52 - and now i got prompted for six digit
111:54 - verification code that i have to plug in
111:56 - but in order to do that google needs my
111:59 - telephone number so i'm gonna type that
112:01 - in now and just to let you know this
112:03 - verification is done to let google know
112:06 - that you're not a bot and you're a real
112:08 - human and google just sent me a
112:10 - verification code
112:11 - and this is a one-time verification code
112:14 - that i'm going to plug in
112:17 - and i'm going to hit verify
112:19 - and you can plug in the necessary
112:21 - information here for recovery email
112:23 - address your birthday and gender and
112:25 - this is so that google can authenticate
112:28 - you in case you accidentally misplace
112:31 - your password
112:32 - and then just hit next and here google
112:34 - gives you a little bit more information
112:36 - on what your number can be used for and
112:38 - so i'm going to go ahead and skip it
112:41 - and of course we're going to read
112:42 - through the terms of service and the
112:44 - privacy policy
112:45 - click on agree
112:50 - and as you can see we're almost there
112:52 - it shows here that we're
112:54 - signing up for the free trial i'm in
112:57 - canada so
112:59 - depending on your country this may
113:00 - change of course i read the terms of
113:02 - service and i'm going to agree to it and
113:05 - i don't really want any updates so you
113:06 - can probably skip that and just hit
113:09 - continue
113:12 - and so this is all the necessary
113:14 - information that needs to be filled out
113:15 - for billing and so here under account
113:18 - type be sure to click on individual as
113:20 - opposed to business and again fill in
113:22 - all the necessary information with
113:24 - regards to your address and your credit
113:27 - card details and once you fill that in
113:29 - you can click on start my free trial
113:34 - and once you've entered in all that
113:36 - information you should be brought to
113:38 - this page with a prompt
113:40 - asking you exactly what you need with
113:43 - regards to google cloud and you can just
113:45 - hit skip here
113:47 - and i'm going to zoom in here just see a
113:49 - little better and so here you're left
113:51 - with a checklist where you can go
113:53 - through all the different resources and
113:55 - it even gives you a checklist to go
113:57 - through but other than that we're in
114:00 - and so just to verify that we're signed
114:02 - up for a free tier account i'm going to
114:04 - go over to billing and i'm going to see
114:06 - here that i have my free trial credit
114:08 - and it says 411 dollars and due to the
114:11 - fact that my currency is in canadian
114:13 - dollars it's been converted from us
114:15 - dollars and so we'll be going through
114:17 - billing in a later lesson
114:19 - but right now we are actually logged in
114:23 - and so that's all i wanted to cover for
114:25 - this lesson on how to sign up for your
114:27 - free trial account
114:28 - so you can now mark this lesson as
114:30 - complete and you can join me in the next
114:32 - one
114:33 - where we will secure the account
114:35 - using a method called two-step
114:37 - verification
114:39 - [Music]
114:43 - welcome back
114:44 - so in the last lesson we went ahead and
114:46 - created a brand new gcp account in this
114:50 - lesson we'll be discussing how to secure
114:53 - that gcp account by following some best
114:56 - practices
114:57 - whenever any account is created in
114:59 - google cloud and this can be applied
115:02 - with regards to personal accounts as
115:04 - well as the super admin account as it's
115:07 - always good to keep safety as a priority
115:10 - this lesson may be a refresher for those
115:13 - who are a bit more advanced as for
115:16 - everyone else these steps could help you
115:18 - from an attack on your account i'd first
115:21 - like to run you through a scenario of
115:23 - the outcome on both secure and
115:25 - non-secure accounts
115:27 - as well as the different options that
115:29 - reside in google cloud
115:31 - when it comes to locking down your
115:33 - account i'll then run through a hands-on
115:35 - demo in the console
115:37 - to show you how you can apply it
115:39 - yourself
115:40 - so in this specific scenario a username
115:43 - and password is used to secure the
115:45 - account
115:47 - here lark a trouble causing manager
115:50 - looks over the shoulder of tony bowtie
115:53 - while he plugs in his username and
115:55 - password
115:56 - so that he can later access his account
115:59 - to wreak havoc on tony's reputation as
116:02 - tony leaves for coffee
116:04 - lark decides to log in and send a
116:06 - company-wide email from tony's account
116:10 - to change an already made decision about
116:13 - next season's store opening in rome
116:15 - italy that would not look good for tony
116:19 - it was that easy for lark to steal
116:21 - tony's password and in a real life
116:24 - scenario it would be that easy for
116:26 - someone to steal your password now when
116:29 - someone steals your password they could
116:31 - do even more devious things than what
116:33 - lark did not just sending out harmful
116:36 - emails they could lock you out of your
116:38 - account or even delete emails or
116:40 - documents this is where two-step
116:43 - verification comes in this can help keep
116:46 - bad people out
116:47 - even if they have your password two-step
116:50 - verification is an extra layer of
116:53 - security most people only have one layer
116:56 - to protect their account which is their
116:58 - password with two-step verification
117:01 - if a bad person hacks through your
117:03 - password they'll still need your phone
117:06 - or security key to get into your account
117:10 - so how two-step verification works is
117:13 - that sign-in will require something you
117:16 - know
117:17 - and something that you have
117:19 - the first one is to protect your account
117:22 - with something you know which will be
117:23 - your password and the second is
117:26 - something that you have
117:28 - which is your phone or security key
117:31 - so whenever you sign into google you'll
117:33 - enter your password as usual
117:36 - then a code will be sent to your phone
117:39 - via text
117:40 - voice call or google's mobile app or if
117:44 - you have a security key you can insert
117:46 - it into your computer's usb port
117:49 - codes can be sent in a text message or
117:52 - through a voice call depending on the
117:55 - setting you choose
117:56 - you can set up google authenticator or
117:59 - another app that creates a one-time
118:02 - verification code which is great for
118:04 - when you're offline you would then enter
118:07 - the verification code on the sign in
118:09 - screen to help verify that it is you
118:12 - another way for verification is using
118:14 - google prompts and this can help protect
118:17 - against sim swap or other phone number
118:20 - based hacks google prompts are push
118:23 - notifications you'll receive on android
118:26 - phones that are signed into your google
118:28 - account or iphones with the gmail app or
118:31 - google app that's signed into your
118:33 - google account now you can actually skip
118:36 - a second step on trusted devices
118:39 - if you don't want to provide a second
118:40 - verification step each time you sign in
118:44 - on your computer or your phone you can
118:46 - check the box next to don't ask again on
118:49 - this computer and this is a great added
118:51 - feature if you are the only user on this
118:54 - device
118:55 - this feature is not recommended if this
118:58 - device is being used by multiple users
119:01 - security keys are another way to help
119:03 - protect your google account from
119:05 - phishing attacks when a hacker tries to
119:08 - trick you into giving them your password
119:11 - or other personal information now a
119:13 - physical security key is a small device
119:16 - that you can buy to help prove it's you
119:19 - signing in when google needs to make
119:21 - sure that it's you
119:23 - you can simply connect your key to your
119:25 - computer and verify that it's you and
119:28 - when you have no other way to verify
119:30 - your account you have the option of
119:33 - using backup codes and these are
119:35 - one-time use codes that you can print or
119:38 - download and these are multiple sets of
119:40 - eight-digit codes that you can keep in a
119:43 - safe place in case you have no other
119:45 - options for verification i personally
119:48 - have found use in using these backup
119:50 - codes as i have used them in past when
119:53 - my phone died
119:55 - so ever since lark's last email
119:58 - tony not only changed his password
120:01 - but added a two-step verification to his
120:04 - account so that only he would have
120:06 - access and would never have to worry
120:09 - again about others looking over his
120:11 - shoulder to gain access to his account
120:14 - as tony leaves for coffee
120:16 - lark tries to log in again but is
120:18 - unsuccessful due to the two-step
120:21 - verification in place tony has clearly
120:24 - outsmarted the bad man in this scenario
120:27 - and lark will have to look for another
120:29 - way to foil tony's plan to bring
120:32 - greatness to bow ties across the globe
120:35 - and this is a sure difference between
120:38 - having a secure account and a not so
120:40 - secure account and so now that i've gone
120:42 - through the theory of the two-step
120:44 - verification process i'm going to dive
120:47 - into the console and implement it with
120:49 - the hands-on demo just be aware that you
120:52 - can also do this through the gmail
120:55 - console but we're going to go ahead and
120:57 - do it through the google cloud console
120:59 - using the url you see here so whenever
121:02 - you're ready feel free to join me in the
121:04 - console
121:06 - and so here we are back in the console
121:08 - and over here on the top right hand
121:10 - corner you will find a user icon and you
121:13 - can simply click on it
121:14 - and click over to your google account
121:17 - now i'm just going to zoom in for better
121:19 - viewing
121:20 - and so in order to enable two-step
121:22 - verification we're gonna go over here to
121:24 - the menu on the left and click on
121:27 - security and under signing into google
121:29 - you will find two-step verification
121:32 - currently it's off as well as using my
121:34 - phone to sign in is off so i'm going to
121:37 - click on this bar here for two-step
121:38 - verification
121:40 - and i definitely want to add an extra
121:41 - layer of security and i definitely want
121:44 - to keep the bad guys out so i'm going to
121:46 - go ahead and click on the get started
121:48 - button
121:49 - it'll ask me for my password
121:52 - and because i've entered my phone number
121:54 - when i first signed up for the account
121:56 - it actually shows up here this is i
121:58 - antony which is my iphone and so now i
122:01 - can get a two-step verification here on
122:04 - my iphone and again this is going to be
122:06 - a google prompt as it shows here but if
122:08 - i wanted to change it to something else
122:11 - i can simply click on show more options
122:14 - and here we have a security key as well
122:17 - as text message or voice call i highly
122:19 - recommend the google prompt as it's
122:21 - super easy to use with absolutely no
122:24 - fuss and so as i always like to verify
122:27 - what i've done i'm going to click on
122:29 - this try it now button and so because i
122:31 - wanted to show you exactly what a live
122:34 - google prompt looks like i'm going to
122:36 - bring up my phone here on the screen so
122:38 - that you can take a look
122:41 - and it actually sent me a google prompt
122:44 - to my phone and i'm just going to go
122:46 - ahead and open up my gmail app so i can
122:48 - verify that it is indeed me that wants
122:51 - to log in which i will accept
122:57 - and so once i've accepted the google
122:58 - prompt another window will pop up asking
123:01 - me about a backup option and so i'll
123:04 - simply need my phone number
123:05 - and i can either get a text message or a
123:08 - phone call and again you have other
123:10 - options as well so you can use the
123:12 - one-time backup codes which we discussed
123:14 - earlier and you can print or download
123:16 - them but i usually like to use a text
123:19 - message and so i'm going to use that
123:22 - i'm going to send it to my phone
123:24 - and so just to verify it
123:28 - i'm gonna now plug in the one-time code
123:30 - that was sent to me
123:35 - and then just hit next
123:37 - so the second step is the google prompt
123:40 - it's my default and my backup options if
123:43 - i can't get google prompt is a voice or
123:46 - text message and again this is for my
123:48 - account antony gcloud ace at gmail.com
123:51 - sending it to my i antony device so turn
123:55 - on two-step verification absolutely
123:59 - and so there you have it there is
124:01 - two-step verification enabled and if i
124:03 - wanted to change the available steps i
124:06 - can do so here i can also edit it i can
124:09 - edit my phone number and i can also set
124:12 - up any backup codes in case i need it in
124:15 - my personal opinion two-step
124:17 - verification
124:19 - is a must-have on any account best
124:22 - practice is to always do it for your
124:24 - super admin account which would be my
124:27 - gmail account that i am currently signed
124:29 - up with but i find is a necessity for
124:32 - any other users and always make it a
124:35 - policy for people to add two-step
124:37 - verification to their accounts i highly
124:40 - recommend that you make it your best
124:42 - practice to do this in your role as an
124:45 - engineer in any environment at any
124:48 - organization again two-step verification
124:51 - will allow to keep you safe your users
124:54 - safe and your environment safe from any
124:57 - malicious activities that could happen
124:59 - at any time and that's all i have for
125:01 - this lesson on two-step verification and
125:04 - securing your account
125:06 - so you can now mark this lesson as
125:08 - complete and let's move on to the next
125:10 - one
125:11 - [Music]
125:15 - welcome back
125:17 - now there are many different ways in
125:18 - which you can interact with google cloud
125:21 - services and resources this lesson is an
125:25 - overview of the gcp console and how you
125:28 - can interact with it using the graphical
125:31 - user interface and so for this hands-on
125:33 - demo
125:34 - i will be diving into how to navigate
125:36 - through the gcp console and point out
125:39 - some functions and features that you may
125:41 - find helpful so with that being said
125:43 - let's dive in
125:46 - and so here we are back in the console
125:48 - up here you can see the free trial
125:50 - status and then i still have 410 credit
125:54 - again this is canadian dollars so i
125:56 - guess consider me lucky so i'm going to
125:58 - go ahead over here and dismiss this
126:01 - don't activate it because otherwise this
126:03 - will kill your free trial status and you
126:06 - don't want to do that so i'm just going
126:08 - to hit dismiss so over here on the main
126:10 - page you have a bunch of cards here that
126:13 - will give you the status of your
126:14 - environment as well as the status of
126:17 - what's happening within google cloud
126:19 - with these cards you can customize them
126:22 - by hitting this button over here
126:24 - customize and you can turn them on or
126:26 - off and you can go ahead and move these
126:28 - around if you'd like
126:34 - and i'm going to put this up here as
126:36 - well i'm going to turn on my billing so
126:38 - i can keep track of exactly what my
126:41 - spend is i don't really need my get
126:43 - starting card so i'm going to turn that
126:45 - off as well as the documentation i'm
126:47 - going to turn that off as well
126:50 - and the apis is always nice to have
126:54 - as well up here on the project info this
126:56 - reflects the current project which is my
126:59 - first project
127:00 - and the project name here is the same
127:04 - the project id is showing and the
127:06 - project number and i'm going to dive
127:08 - deeper into that in another lesson also
127:10 - note that your cards will reflect
127:12 - exactly what it is that you're
127:14 - interacting with and so the more
127:16 - resources that you dive into the cards
127:18 - will end up showing up here
127:21 - and you can add them and turn them off
127:23 - at will so i'm going to go up here and
127:24 - click on done because i'm satisfied with
127:27 - the way that things look
127:29 - here on my home page and over here to
127:31 - your left i wanted to focus on
127:33 - all the services that are available in
127:36 - their own specific topics so for
127:39 - instance all of compute
127:41 - you will find app engine compute engine
127:43 - kubernetes and so on so note that
127:46 - anything compute related you'll find
127:48 - them all grouped together also another
127:51 - great feature is that you can pin
127:53 - exactly what it is that you use often so
127:56 - if i am a big user of app engine i can
127:59 - pin this and it will move its way up to
128:01 - the top this way it saves me the time
128:04 - from having to go and look for it every
128:06 - time i need it and if i'm using it
128:08 - constantly it's great to have a shortcut
128:11 - to unpin it i simply go back to the pin
128:14 - and click on it again as well if i'd
128:16 - like to move the menu out of the way to
128:18 - get more screen real estate i can simply
128:20 - click on this hamburger button here
128:22 - and make it disappear and to bring it
128:24 - back i can just click on that again and
128:26 - i'll bring it back again now i know that
128:28 - there's a lot of resources here to go
128:30 - through so if you're looking for
128:32 - something specific you can always go up
128:35 - to the search bar right here
128:37 - and simply type it in so if i'm looking
128:39 - for let's say cloud sql i can simply
128:42 - type in sql
128:45 - and i can find it right here
128:47 - i can find the api and if anything
128:50 - associated with the word sql if i'm
128:53 - looking for cloud sql specifically i can
128:56 - simply type in
128:57 - cloud sql
129:00 - and here it is
129:04 - another thing to note is that if you
129:06 - want to go back to your homepage you can
129:09 - simply go up to the left hand corner
129:11 - here and click on the google cloud
129:13 - platform logo and it'll bring you right
129:16 - back and right here under the google
129:18 - cloud platform logo you'll see another
129:20 - set of tabs we have dashboard we also
129:23 - have activity and this will show all the
129:25 - latest activity that's been done
129:28 - and because this is a brand new account
129:30 - i don't have much here now because this
129:32 - is my first time in activity this is
129:34 - going to take some time to index
129:36 - and in the meantime i wanted to show you
129:38 - filters
129:40 - if this were a long list to go through
129:42 - where activity has been happening for
129:44 - months i can filter through these
129:46 - activities either by user or by
129:49 - categories or by resource type as well
129:52 - as the date i can also combine these to
129:55 - search for something really granular and
129:57 - beside the activity tab we have
129:59 - recommendations which is based on the
130:02 - recommender service and this service
130:04 - provides recommendations and insights
130:07 - for using resources on google cloud
130:10 - these recommendations and insights are
130:12 - on a per product or per service basis
130:15 - and they are based on machine learning
130:17 - and current resource usage a great
130:19 - example of a recommendation is vm
130:22 - instance right sizing so if the
130:24 - recommender service detects that a vm
130:26 - instance is underutilized it will
130:29 - recommend changing the machine size so
130:31 - that i can save some money and because
130:33 - this is a fresh new account and i
130:36 - haven't used any resources this is why
130:38 - there is no recommendations for me so
130:40 - going back to the home page
130:42 - i want to touch on this projects menu
130:44 - for a second and as you can see here i
130:46 - can select a project now if i had many
130:50 - different projects i can simply search
130:52 - from each different one and so to cover
130:54 - the last part of the console i wanted to
130:56 - touch on this menu on the top right hand
130:59 - corner here so clicking on this present
131:01 - icon will reveal my free trial status
131:04 - which i dismissed earlier next to the
131:06 - present we have a cloud shell icon and
131:09 - this is where you can activate and bring
131:11 - up the cloud shell which i will be
131:13 - diving into deeper in a later lesson and
131:16 - right next to it is the help button in
131:18 - case you need a shortcut to any
131:20 - documentations or tutorials as well some
131:22 - keyboard shortcuts
131:24 - may help you be a little bit more
131:26 - efficient and you can always click on
131:28 - this and it'll show you exactly what you
131:30 - need to know and so i'm going to close
131:32 - this
131:33 - and to move over to the next part in the
131:35 - menu this is the notifications so any
131:38 - activities that happen you will be
131:40 - notified here and you can simply click
131:42 - on the bell and it'll show you a bunch
131:44 - of different notifications for either
131:46 - resources that are created or any other
131:49 - activities that may have happened now
131:52 - moving on over
131:54 - three buttons over here is the settings
131:56 - and utilities button
131:58 - and over here you will find the
131:59 - preferences
132:02 - and under communication you will find
132:04 - product notifications and updates and
132:06 - offers and you can turn them off or on
132:09 - depending on whether or not you want to
132:11 - receive these notifications as well you
132:13 - have your language and region and you
132:15 - can personalize the cloud console as to
132:18 - whether or not you want to allow google
132:20 - to track your activity and this is great
132:22 - for when you want recommendations so i'm
132:24 - going to keep that checked off getting
132:26 - back to some other options you will find
132:28 - a link to downloads as well as cloud
132:31 - partners and the terms of service
132:33 - privacy and project settings and so to
132:36 - cover the last topic i wanted to touch
132:38 - on is the actual google account button
132:41 - and here you can add other user accounts
132:43 - for when you log into the console with a
132:45 - different user as well as go straight to
132:48 - your google account and of course if
132:50 - you're using a computer that's used by
132:51 - multiple users you can sign out here as
132:54 - well and so that's just a quick
132:56 - run-through of the console and so feel
132:58 - free to poke around and get familiar
133:01 - with exactly what's available in the
133:02 - console so that it's a lot easier for
133:05 - you to use and allow you to become more
133:07 - efficient and so that's all i have for
133:09 - this lesson so you can now mark this
133:11 - lesson as complete and let's move on to
133:14 - the next one
133:19 - welcome back
133:20 - in this lesson i'm going to be going
133:23 - through a breakdown of cloud billing and
133:25 - an overview of the various resources
133:28 - that's involved with billing billing is
133:30 - important to know
133:32 - and i'll be diving into the concepts
133:34 - around billing and billing interaction
133:37 - over the next few lessons
133:39 - as well i'll be getting into another
133:41 - demo going through the details on how to
133:44 - create
133:46 - edit
133:46 - and delete a cloud billing account
133:49 - now earlier on in the course i went over
133:51 - the resource hierarchy and how google
133:54 - cloud resources are broken down starting
133:58 - from the domain level down to their
134:00 - resource level
134:02 - this lesson will focus strictly on the
134:05 - billing account
134:06 - and payments profile
134:08 - and the breakdown are concepts that are
134:10 - comprised within them
134:12 - so getting right into it let's start
134:15 - with the cloud billing account a cloud
134:17 - billing account is a cloud level
134:19 - resource managed in the cloud console
134:22 - this defines who pays for a given set of
134:26 - google cloud resources billing tracks
134:28 - all of the costs incurred by your google
134:31 - cloud usage as well it is connected to a
134:35 - google payments profile which includes a
134:38 - payment method defining on how you pay
134:41 - for your charges a cloud billing account
134:44 - can be linked to one or more projects
134:47 - and not to any one project specifically
134:50 - cloud billing also has billing specific
134:53 - roles and permissions to control
134:55 - accessing and modifying billing related
134:59 - functions that are established by
135:01 - identity and access management cloud
135:04 - billing is offered in two different
135:06 - account types there is the self-service
135:09 - or online account or you can also choose
135:12 - from the invoiced or offline payments
135:14 - when it comes to the self-service option
135:17 - the payment method is usually a credit
135:19 - or debit card and costs are charged
135:21 - automatically to the specific payment
135:24 - method connected to the cloud billing
135:26 - account and when you need access to your
135:28 - invoices you can simply go to the cloud
135:31 - console and view them online now when it
135:34 - comes to the invoice account first you
135:36 - must be eligible for invoice billing
135:39 - once you are made eligible the payment
135:41 - method used can be check or wire
135:44 - transfer your invoices are sent by mail
135:47 - or electronically as well they're also
135:50 - available in the cloud console as well
135:53 - as the payment receipts
135:55 - now another cool feature of billing
135:56 - account is sub-accounts and these are
136:00 - intended for resellers so if you are a
136:02 - reseller you can use subaccounts to
136:05 - represent your customers and make it
136:07 - easy for chargebacks cloud billing
136:10 - subaccounts allow you to group charges
136:12 - from projects together on a separate
136:15 - section of your invoice and is linked
136:17 - back to the master cloud billing account
136:20 - on which your charges appear
136:22 - sub-accounts are designed to allow for
136:24 - customer separation and management so
136:27 - when it comes to ownership of a cloud
136:30 - billing account it is limited to a
136:32 - single organization
136:34 - it is possible though for a cloud
136:36 - billing account to pay for projects that
136:39 - belong to an organization that is
136:41 - different than the organization that
136:43 - owns the cloud billing account now one
136:46 - thing to note is that if you have a
136:48 - project that is not linked to a billing
136:51 - account you will have limited use of
136:53 - products and services available for your
136:56 - project that is projects that are not
136:59 - linked to a billing account cannot use
137:01 - google cloud services that aren't free
137:05 - and so now that we've gone through an
137:07 - overview of the billing account let's
137:09 - take a quick step into the payments
137:11 - profile now the payments profile is a
137:15 - google level resource managed at
137:17 - payments.google.com
137:19 - the payments profile
137:21 - processes payments for all google
137:23 - services and not just for google cloud
137:27 - it connects to all of your google
137:29 - services such as google ads as well as
137:32 - google cloud it stores information like
137:35 - your name address and who is responsible
137:38 - for the profile it stores your various
137:40 - payment methods like credit cards debit
137:43 - cards and bank accounts the payments
137:46 - profile
137:47 - functions as a single pane of glass
137:50 - where you can view invoices payment
137:52 - history and so on it also controls who
137:56 - can view and receive invoices for your
137:59 - various cloud billing accounts and
138:01 - products
138:02 - now one thing to note about payments
138:04 - profile is that there are two different
138:07 - types of payment profiles the first one
138:11 - is individual and that's when you're
138:13 - using your account for your own personal
138:15 - payments if you register your payments
138:18 - profile as an individual then only you
138:21 - can manage the profile you won't be able
138:23 - to add or remove users or change
138:26 - permissions on the profile now if you
138:29 - choose a business profile type you're
138:31 - paying on behalf of a business or
138:33 - organization a business profile gives
138:36 - you the flexibility to add other users
138:40 - to the google payments profile you
138:42 - manage so that more than one person can
138:45 - access or manage a payments profile all
138:48 - users added to a business profile
138:51 - can then see the payment information on
138:53 - that profile another thing to note is
138:56 - that once the profile type has been
138:58 - selected it cannot be changed afterwards
139:02 - and so now that we've quickly gone
139:04 - through an overview of all the concepts
139:06 - when it comes to billing i am now going
139:08 - to run through a short demo where i will
139:11 - create a new billing account edit that
139:14 - billing account and show you how to
139:16 - close a billing account so whenever
139:18 - you're ready join me in the console and
139:21 - so here i am back in the console and so
139:23 - the first thing i want to do is i want
139:25 - to make sure that i have the proper
139:26 - permissions in order to create and edit
139:29 - a new billing account so what i'm going
139:31 - to do is go over here to the hamburger
139:33 - menu up here in the top left hand corner
139:36 - and click on it
139:38 - and go over to i am an admin and over to
139:41 - iam
139:45 - now don't worry i'm not going to get
139:47 - really deep into this i will be going
139:49 - over this in a later section where i'll
139:52 - go through iam and roles but i wanted to
139:56 - give you a sense of exactly what you
139:58 - need with regards to permissions so now
140:01 - that i'm here i'm going to be looking
140:03 - for a role that has to do with billing
140:05 - so i'm simply going to go over here on
140:07 - the left hand menu and click on roles
140:10 - and you'll have a slew of roles coming
140:13 - up
140:14 - and what you can do is filter through
140:16 - them just by simply typing in billing
140:19 - into the filter table here at the top
140:22 - and as you can see here
140:24 - there is billing account administrator
140:26 - billing account creator and so on and so
140:29 - forth and just to give you a quick
140:31 - overview on these roles and so for the
140:33 - billing account administrator this is a
140:35 - role that lets you manage billing
140:37 - accounts but not create them so if you
140:40 - need to set budget alerts or manage
140:42 - payment methods you can use this role
140:44 - the billing account creator allows you
140:46 - to create new self-serve online billing
140:49 - accounts the billing account user allows
140:51 - you to link projects to billing accounts
140:54 - the billing account viewer allows you to
140:56 - view billing account cost information
140:58 - and transactions and lastly the project
141:01 - billing manager allows you to link or
141:05 - unlink the project to and from a billing
141:08 - account so as you can see these roles
141:10 - allow you to get pretty granular when it
141:13 - comes to billing so i'm going to go back
141:15 - over to the left hand menu over on iam
141:18 - and click on there and i want to be able
141:20 - to check my specific role and what
141:23 - permissions that i have or i will need
141:26 - in order to create a new billing account
141:28 - and so if i click on this pencil it'll
141:31 - show me exactly
141:32 - what my role is and what it does and as
141:36 - it says here i have full access to all
141:38 - resources which means that i am pretty
141:41 - much good to go so i'm going to cancel
141:43 - out here
141:46 - and i'm going to exit i am an admin
141:50 - so i'm going to click on the navigation
141:51 - menu
141:53 - and go over to billing
141:55 - and so this billing account is tied to
141:58 - the current project and because it's the
142:00 - only billing account it's the one that
142:02 - shows up and so what i want to do is i
142:05 - want to find out a little bit more
142:06 - information with regards to this billing
142:08 - account so i'm going to move down the
142:10 - menu and click on account management
142:13 - here i can see the billing account which
142:15 - is my billing account i can rename it if
142:18 - i'd like
142:19 - and i can also see the projects that are
142:22 - linked to this billing account so now
142:24 - that we've viewed all the information
142:26 - with regards to the my billing account
142:28 - i'm going to simply click on this menu
142:30 - over here
142:31 - and click on the arrow and go to manage
142:34 - billing accounts and here it will bring
142:36 - me to all my billing accounts and
142:39 - because i only have one is shown here my
142:42 - billing account but if i had more than
142:44 - one they would show up here and so now
142:47 - in order for me to create this new
142:48 - billing account i'm going to simply
142:50 - click on create account
142:53 - and i will be prompted with a name a
142:56 - country and a currency for my new
142:58 - billing account and i'm actually going
143:00 - to rename this billing account and i'm
143:02 - going to rename it to gcloud
143:05 - ace
143:06 - dash billing
143:09 - i'm going to leave my country as canada
143:11 - and my currency in canadian dollars and
143:13 - i'm going to simply hit continue
143:17 - and it's giving me the choice in my
143:19 - payments profile
143:20 - and because i want to use the same
143:22 - payments profile i'm just going to
143:24 - simply leave everything as is but for
143:27 - demonstration purposes
143:29 - over here you can click on the payments
143:31 - profile
143:32 - and the little arrow right beside the
143:34 - current profile will give me the option
143:36 - to create a new payments profile
143:40 - and we're going to leave that as is
143:42 - under customer info i have the option of
143:45 - changing my address and i can click on
143:47 - this pencil icon and change it as well i
143:50 - can go to payment methods and click on
143:53 - the current payment method with that
143:55 - little arrow
143:56 - and add a new credit or debit card and
144:00 - as i said before we're going to keep
144:01 - things the way they are and just hit
144:03 - submit and enable billing
144:06 - now as you can see here i got a prompt
144:08 - saying that a confirmation email will be
144:10 - sent within 48 hours now usually when
144:13 - you're setting up a brand new billing
144:15 - profile with an already created payments
144:17 - profile you'll definitely get a
144:19 - confirmation email in less than 48 hours
144:23 - now in order for me to finish up this
144:24 - demo i'm gonna wait until the new
144:27 - billing account shows up and continue
144:29 - with the demo from then and so here i am
144:31 - back in the billing console and it only
144:34 - took about 20 minutes and the gcloud ace
144:36 - billing account has shown up and so with
144:39 - part of this demo what i wanted to show
144:42 - is how you can take a project and attach
144:45 - it to a different billing account and so
144:47 - currently my only project is attached to
144:50 - the my billing account so now if i
144:53 - wanted to change my first project to my
144:56 - gcloud ace dash billing account
144:59 - i can simply go over here to actions
145:02 - click on the hamburger menu
145:04 - and go to change billing
145:07 - here i'll be prompted to choose a
145:08 - billing account and i can choose g cloud
145:11 - a stash billing
145:13 - and then click on set account
145:17 - and there it is my first project is now
145:20 - linked to g cloud a stash billing so if
145:23 - i go back over to my billing accounts
145:26 - you can see here that my billing account
145:28 - currently has zero projects and g cloud
145:32 - a stash billing has one project now just
145:35 - as a quick note and i really want to
145:37 - emphasize this
145:39 - is that if you're changing a billing
145:41 - account for a project
145:43 - and you are a regular user
145:45 - you will need the role of the billing
145:47 - account administrator
145:49 - as well as the project owner role
145:52 - so these two together will allow a
145:55 - regular user to change a billing account
145:58 - for a project
145:59 - and so now what i want to do is i want
146:02 - to take the gcloud a stash billing and i
146:05 - want to close that account
146:07 - but before i do that i need to unlink
146:10 - this project and bring it back to
146:12 - another billing account which in this
146:14 - case would be my billing account so i'm
146:17 - going to go back up here to the menu
146:18 - click on my projects and we're going to
146:21 - do the exact same thing that we did
146:23 - before
146:24 - under actions i'm going to click on the
146:26 - hamburger menu and change billing
146:28 - i'm going to get the prompt again and
146:30 - under billing account i'm going to
146:32 - choose my billing account and then click
146:34 - on set account
146:36 - so as you can see the project has been
146:38 - moved to a different billing account i'm
146:40 - going to go back to my billing accounts
146:43 - and as you can see here the project is
146:45 - back to my billing account and so now
146:47 - that the project is unlinked from the
146:49 - gcloud a stash billing account i can now
146:52 - go ahead and close out that account now
146:54 - in order to do that i'm going to click
146:56 - on gcloud a stash billing i'm going to
146:59 - go down here on the hand menu all the
147:02 - way to the bottom to account management
147:04 - click on there and at the top here you
147:07 - will see close billing account i'm going
147:10 - to simply click on that and i'll get a
147:12 - prompt that i've spent zero dollars and
147:15 - is linked to zero projects
147:17 - now if i did have a project that was
147:19 - linked to this billing account i would
147:21 - have to unlink the project before i was
147:23 - able to close this billing account so as
147:26 - a failsafe i'm being asked to type close
147:29 - in order to close this billing account
147:31 - so i'm going to go ahead and do that now
147:33 - and click on close billing account just
147:36 - as a note google gives me the option to
147:39 - reopen this billing account in case i
147:41 - did this by mistake and i really needed
147:43 - it
147:44 - i can reopen this billing account so now
147:47 - moving back over to billing you'll see
147:49 - here
147:50 - that i'm left with my single billing
147:52 - account called my billing account with
147:55 - the one project that's linked to it and
147:57 - so that covers my demo on creating
148:00 - editing and closing a new billing
148:02 - account as well as linking and unlinking
148:05 - a project to and from a different
148:08 - billing account so i hope you found this
148:10 - useful
148:11 - and you can now mark this lesson as
148:12 - complete and let's move on to the next
148:15 - one
148:16 - [Music]
148:20 - welcome back
148:21 - in this lesson i'm going to be going
148:23 - over controlling costs in google cloud
148:26 - along with budget alerts
148:28 - i will be touching on all the available
148:30 - discounts the number of ways to control
148:33 - costs
148:34 - and go over budget alerts to get a more
148:36 - granular and programmatic approach so
148:39 - starting off i wanted to touch on
148:41 - committed use discounts now committed
148:44 - use discounts provide discounted prices
148:47 - in exchange for your commitment to use a
148:50 - minimum level of resources for a
148:53 - specified term the discounts are
148:55 - flexible cover a wide range of resources
148:58 - and are ideal for workloads with
149:01 - predictable resource needs when you
149:03 - purchase google cloud committed use
149:05 - discounts you commit to a consistent
149:08 - amount of usage for a one or three year
149:11 - period there are two commitment types
149:13 - available
149:14 - and as you can see here they are spend
149:17 - based and resource based commitment
149:19 - types
149:20 - and unlike most other providers the
149:23 - commitment fee is billed monthly so
149:26 - going over the specific commitment types
149:28 - i wanted to start off with spend based
149:30 - commitment now for spend based
149:32 - commitment you commit to a consistent
149:35 - amount of usage measured in dollars per
149:38 - hour
149:39 - of equivalent on-demand spend for a one
149:42 - or three year term in exchange you
149:45 - receive a discounted rate on the
149:47 - applicable usage your commitment covers
149:50 - so you can purchase committed use
149:51 - discounts from any cloud billing account
149:54 - and the discount applies to any eligible
149:57 - usage in projects paid for by that cloud
150:00 - billing account any overage is charged
150:03 - at the on-demand rate spend based
150:06 - commitments can give you a 25 discount
150:10 - off on-demand pricing for a one-year
150:12 - commitment and up to a 52 discount off
150:16 - of on-demand pricing for a three-year
150:19 - commitment now spend-based commitments
150:22 - are restricted to specific resources
150:25 - which is cloud sql database instances
150:28 - and google cloud vmware engine
150:31 - and this commitment
150:32 - applies to the cpu and memory usage for
150:36 - these available resources now the other
150:38 - committed use discount is the
150:41 - resource-based commitment
150:43 - so this discount is for a commitment to
150:45 - spend a minimum amount for compute
150:48 - engine resources in a particular region
150:51 - resource-based commitments are ideal for
150:54 - predictable workloads when it comes to
150:57 - your vms
150:58 - when you purchase a committed use
151:00 - contract you purchase compute resources
151:03 - such as vcpus
151:06 - memory
151:07 - gpus
151:08 - and local ssds and you purchase these at
151:12 - a discounted price in return for
151:14 - committing to paying for those resources
151:17 - for one or three years the discount is
151:20 - up to 57 percent for most resources like
151:24 - machine types or gpus the discount is up
151:28 - to 70 percent for memory optimized
151:30 - machine types and you can purchase a
151:33 - committed use contract for a single
151:35 - project or purchase multiple contracts
151:38 - which you can share across many project
151:41 - by enabling shared discounts and sharing
151:44 - your committed use discounts across all
151:47 - your projects reduces the overhead of
151:50 - managing discounts on a per project
151:53 - basis
151:54 - and maximizes your savings by pooling
151:57 - all of your discounts across your
151:59 - project's resource usage if you have
152:02 - multiple projects that share the same
152:04 - cloud billing account you can enable
152:07 - committed use discount sharing so all of
152:10 - your projects within that cloud billing
152:12 - account share all of your committed use
152:15 - discount contracts and so your sustained
152:18 - use discounts are also pooled at the
152:21 - same time so touching on sustained use
152:24 - discounts
152:26 - these are automatic discounts for
152:28 - running specific compute engine
152:30 - resources a significant portion of the
152:33 - billing month
152:34 - sustained use discounts apply to the
152:37 - general purpose
152:38 - compute and memory optimize machine
152:41 - types as well as sole tenant nodes and
152:44 - gpus again sustained use discounts are
152:48 - applied automatically to usage within a
152:51 - project separately for each region so
152:54 - there's no action required on your part
152:56 - to enable these discounts so for example
152:59 - when you're running one of these
153:00 - resources for more than let's say 25
153:03 - percent of the month
153:05 - compute engine automatically gives you a
153:08 - discount for every incremental minute
153:11 - that you use for that instance now
153:14 - sustained use discounts automatically
153:16 - apply to vms created by both google
153:19 - kubernetes engine and compute engine but
153:22 - unfortunately do not apply to vms
153:25 - created using the app engine flexible
153:28 - environment as well as data flow and e2
153:32 - machine types
153:33 - now to take advantage of the full
153:34 - discount you would create your vm
153:37 - instances on the first day of the month
153:39 - as discounts reset at the beginning of
153:42 - each month and so the following table
153:45 - shows the discount you get at each usage
153:48 - level of a vm instance these discounts
153:51 - apply for all machine types but don't
153:54 - apply to preemptable instances and so
153:57 - sustained use discounts can save you up
154:00 - to a maximum of a 30 percent discount so
154:04 - another great way to calculate savings
154:06 - in google cloud is by using the gcp
154:09 - pricing calculator this is a quick way
154:12 - to get an estimate of what your usage
154:15 - will cost on google cloud so the gcp
154:18 - pricing calculator can help you identify
154:21 - the pricing for the resources that you
154:23 - plan to use in your future architecture
154:27 - so that you are able to calculate how
154:29 - much your architecture will cost you
154:32 - this calculator holds the pricing for
154:35 - almost all resources encapsulated within
154:38 - gcp and so you can get a pretty good
154:41 - idea of what your architecture will cost
154:44 - you without having to find out the hard
154:46 - way this calculator can be found at the
154:49 - url shown here and i will include this
154:52 - in the lesson text below now moving
154:54 - right along to cloud billing budgets so
154:57 - budgets enable you to track your actual
155:00 - spend
155:01 - against your plan spend
155:03 - after you've set a budget amount you set
155:05 - budget alert threshold rules that are
155:07 - used to trigger email notifications and
155:10 - budget alert emails help you stay
155:13 - informed about how your spend is
155:15 - tracking against your budget this
155:17 - example here is a diagram of a budget
155:21 - alert notification and is the default
155:23 - functionality for any budget alert
155:26 - notifications
155:28 - now to get a little bit more granular
155:30 - you can define the scope of the budget
155:33 - so for example you can scope the budget
155:36 - to apply to the spend of an entire cloud
155:40 - billing account or get more granular to
155:42 - one or more projects
155:44 - and even down to a specific product you
155:47 - can set the budget amount to a total
155:50 - that you specify
155:51 - or base the budget amount on the
155:53 - previous month's spend when costs exceed
155:56 - a percentage of your budget based on the
155:58 - rules that you set by default alert
156:01 - emails are sent to billing account
156:03 - administrators and billing account users
156:06 - on the target cloud billing account and
156:08 - again this is the default behavior of a
156:11 - budget email notification
156:14 - now as said before the default behavior
156:16 - of a budget is to send alert emails to
156:19 - billing account administrators and
156:21 - billing account users on the target
156:24 - cloud billing account when the budget
156:26 - alert threshold rules trigger an email
156:29 - notification now these email recipients
156:32 - can be customized by using cloud
156:34 - monitoring to specify other people in
156:37 - your organization to receive these
156:39 - budget alert emails a great example of
156:42 - this would be a project manager or a
156:45 - director knowing how much spend has been
156:48 - used up in your budget and the last
156:50 - concept i wanted to touch on when it
156:52 - comes to cloud billing budgets is that
156:55 - you can also use pub sub for
156:57 - programmatic notifications to automate
157:00 - your cost control response based on the
157:03 - budget notification
157:04 - you can also use pub sub in conjunction
157:07 - with billing budgets to automate cost
157:09 - management tasks and this will provide a
157:12 - real-time status of the cloud billing
157:15 - budget and allow you to do things like
157:18 - send notifications to slack or disable
157:21 - billing to stop usage as well as
157:23 - selectively control usage when budget
157:26 - has been met and so these are all the
157:28 - concepts that i wanted to cover when it
157:31 - came to cloud billing budgets now i know
157:33 - this lesson may have been a bit dry and
157:36 - not the most exciting service to dive
157:38 - into but it is very important to know
157:40 - both for the exam
157:42 - and for your role as an engineer when it
157:45 - comes to cutting costs in environments
157:48 - where your business owners deem
157:50 - necessary and so that's all i had for
157:52 - this lesson
157:53 - so you can now mark this lesson as
157:55 - complete and please join me in the next
157:57 - one where i dive into the console and do
158:00 - some hands-on demos when it comes to
158:02 - committed use discounts
158:04 - budget alerts and editing budget alerts
158:07 - as well as adding a little bit of
158:09 - automation into the budgeting alerts
158:12 - [Music]
158:16 - welcome back in the last lesson i went
158:18 - over a few ways to do cost management
158:22 - and the behaviors of budget alerts
158:24 - in this lesson i will be doing a demo to
158:28 - show you committed use discounts and
158:30 - reservations along with how to create
158:33 - budget alerts
158:34 - and as well how to edit them so with
158:36 - that being said let's dive in so now i'm
158:39 - going to start off with committed use
158:41 - discounts in order to get there i'm
158:44 - going to find it in compute engine so
158:46 - i'm going to simply go up here on the
158:48 - top left hand corner back to the
158:50 - navigation menu
158:52 - i'm going to go down to compute engine
158:55 - and i'm going to go over here to
158:56 - committed use discounts
158:59 - and as we discussed earlier these
159:01 - commitments for compute engine are
159:03 - resource based and as you can see here
159:06 - we have hardware commitments and
159:07 - reservations now reservations i will get
159:10 - into just a little bit later but with
159:13 - regards to hardware commitments we're
159:15 - going to get into that right now and as
159:17 - expected i have no current commitments
159:20 - so i'm going to go up to purchase
159:21 - commitment and so i need to start off
159:24 - with
159:24 - finding a name for this commitment and
159:26 - so i'm going to name this commitment
159:28 - demo dash commitment
159:34 - it's going to ask me for a region i'm
159:36 - going to keep it in us central one with
159:38 - the commitment type here is where i can
159:40 - select the type of machine that i'm
159:42 - looking for so i can go into general
159:45 - purpose and 1 and 2
159:47 - and 2d e2 as well as memory optimize
159:51 - and compute optimized and so i'm going
159:53 - to keep it at general purpose and one
159:56 - again the duration one or three years
159:59 - and we get down to cores i can have as
160:02 - many vcpus as i'd like
160:04 - so if i needed 10 i can do that and i'll
160:07 - get a pop-up here on the right showing
160:09 - me the estimated monthly total as well
160:11 - as an hourly rate for this specific vm
160:15 - with 10 cores i can also select the
160:17 - duration for three years and as expected
160:20 - i'll get a higher savings because i'm
160:22 - giving a bigger commitment so bring it
160:24 - back down to one year and let's put the
160:27 - memory up to 64 gigabytes here i can add
160:31 - gpus and i have quite a few to choose
160:34 - from as well as local ssds and here with
160:37 - the local ssds i can choose as many
160:40 - disks as i'd like as long as it's within
160:42 - my quota and each disk size is going to
160:45 - be 375 gigabytes so if you're looking
160:48 - into committed use discounts and using
160:51 - local ssds please keep that in mind
160:54 - again the reservation can be added here
160:56 - and i'll be getting into that in just a
160:58 - second and now i don't want to actually
161:00 - purchase it but i did want to show you
161:03 - exactly what a committed use discount
161:05 - would look like and how you would apply
161:07 - it again here on the right hand side it
161:10 - shows me the details of the estimated
161:13 - monthly total and the hourly rate so i'm
161:16 - going to go over here and hit cancel
161:18 - and if i were to have applied it
161:20 - the commitment would show up here in
161:22 - this table and give me all the specified
161:24 - configurations
161:26 - of that instance right here now touching
161:28 - on reservations
161:30 - reservations is when you reserve the vm
161:33 - instances you need
161:35 - so when the reservation has been placed
161:37 - the reservation ensures that those
161:40 - resources are always available for you
161:43 - as some of you might know when you go to
161:45 - spin up a new compute engine vm
161:47 - especially when it comes to auto scaling
161:50 - instance groups the instances can
161:52 - sometimes be delayed or unavailable now
161:55 - the thing with reservations is that a vm
161:58 - instance can only use a reservation if
162:01 - its properties exactly match the
162:04 - properties of the reservation which is
162:06 - why it's such a great pairing with
162:09 - committed use discounts
162:10 - so if you're looking to make a
162:12 - resource-based commitment and you always
162:14 - want your instance available you can
162:16 - simply create a reservation attach it to
162:19 - the commitment and you will never have
162:21 - to worry about having the resources to
162:24 - satisfy your workload as they will
162:26 - always be there so again going into
162:28 - create reservation it'll show me here
162:31 - the name the description i can choose to
162:34 - use the reservation automatically or
162:36 - select a specific reservation the region
162:39 - and zone
162:40 - number of instances and here i can
162:43 - specify the machine type or specify an
162:46 - instance template and again this is
162:48 - another use case where if you need
162:50 - compute engine instances spun up due to
162:53 - auto scaling this is where reservations
162:56 - would apply so getting back to machine
162:58 - type i can choose from vcpus
163:02 - as well as the memory i can customize it
163:05 - i can add as many local ssds as my
163:08 - quotas will allow me and i can select my
163:10 - interface type and i'm going to cancel
163:13 - out of here now when it comes to
163:15 - committed use discounts and reservations
163:17 - as it pertains to the exam
163:20 - i have not seen it but since this is an
163:22 - option to save money i wanted to make
163:25 - sure that i included it in this lesson
163:28 - as this could be a great option for use
163:30 - in your environment so now that we
163:32 - covered resource-based committed use
163:34 - discounts i wanted to move into spend
163:37 - based commitments and so where you would
163:39 - find that would be over in billing
163:42 - so again i'm going to go up to the
163:43 - navigation menu in the top left hand
163:46 - corner and go into billing
163:49 - now you'd think that you would find it
163:51 - here under commitments but only when you
163:53 - have purchased a commitment will it
163:56 - actually show up here but as you can see
163:58 - here it's prompting us to go to the
164:00 - billing overview page
164:03 - so going back to the overview page
164:06 - you'll find it
164:07 - down here on the right and so i can now
164:10 - purchase a commitment and as we
164:12 - discussed before
164:13 - a spend based commitment can be used for
164:16 - either cloud sql or for vmware engine
164:19 - i select my billing account
164:22 - the commitment name the period either
164:24 - one year or three years and it also
164:27 - shows me the discount which could help
164:29 - sway my decision as well as the region
164:32 - as well as the hourly on-demand
164:34 - commitment now you're probably wondering
164:36 - what this is
164:38 - and as explained here this commitment is
164:40 - based on the on-demand price
164:43 - and once this is all filled out the
164:45 - commitment summary will be populated and
164:48 - after you agree to all the terms and
164:49 - services you can simply hit purchase
164:52 - but i'm going to cancel out of here
164:54 - and so that is an overview for the spend
164:57 - based commitment and again these
164:59 - committed use discounts i have not seen
165:02 - on the exam
165:03 - but i do think that it's good to know
165:05 - for your day-to-day environment if
165:08 - you're looking to save money and really
165:10 - break down costs
165:12 - so now that i've covered committed use
165:14 - discounts and reservations
165:16 - i wanted to move over to budgets and
165:18 - budget alerts and because i'm already on
165:21 - the billing page all i need to do is go
165:23 - over here to the left hand menu and
165:25 - click on budgets and alerts now setting
165:28 - up a budget for yourself for this course
165:31 - would be a great idea especially for
165:34 - those who are cost conscious on how much
165:37 - you're spending with regards to your
165:39 - cloud usage and so we're to go ahead and
165:41 - create a new budget right now so let's
165:43 - go up here to the top to create budget
165:46 - and i'm going to be brought to a new
165:48 - window where i can put in the name of
165:50 - the budget and i'm going to call this
165:52 - ace
165:54 - dash budget and because i want to
165:57 - monitor all projects and all products
165:59 - i'm going to leave this as is but if you
166:02 - did have multiple projects you could get
166:04 - a little bit more granular and the same
166:07 - thing with products
166:13 - so i'm going to go ahead and leave it as
166:14 - is and just click on next now under
166:17 - budget type
166:18 - i can select from either a specified
166:20 - amount or the last month's spend and so
166:23 - for this demo i'm going to keep it at
166:25 - specified amount
166:27 - and because i want to be really
166:28 - conscious about how much i spend in this
166:31 - course i'm going to put in 10
166:33 - for my target amount i'm going to
166:35 - include the credits and cost and then
166:37 - i'm going to click on next now these
166:40 - threshold rules are where billing
166:42 - administrators will be emailed when a
166:45 - certain percent of the budget is hit
166:48 - so if my spend happens to hit five
166:50 - dollars
166:51 - because i am a billing administrator i
166:54 - will be sent an email telling me that my
166:56 - spend has hit five dollars i also have
166:59 - the option of changing these percentages
167:02 - so if i decided to change it to forty
167:03 - percent
167:05 - now my amount goes to four dollars and
167:07 - this is done automatically so no need to
167:10 - do any calculations
167:12 - but i'm going to keep this here at 50
167:14 - percent
167:15 - and vice versa if i wanted to change the
167:18 - amount
167:19 - the percentage of budget will actually
167:21 - change now with the trigger i actually
167:23 - have the option of selecting forecasted
167:27 - or actual and so i'm going to keep it on
167:29 - actual and if i want i can add more
167:31 - threshold rules now i'm going to leave
167:33 - everything as is and just click on
167:35 - finish
167:36 - and now as you can see here i have a
167:38 - budget name of ace budget now because
167:41 - the budget name doesn't have to be
167:43 - globally unique in your environment you
167:46 - can name your budget exactly the same
167:48 - and again it'll give me all the specific
167:50 - configurations that i filled out shows
167:52 - me how much credits i've used and that's
167:55 - it and that's how you would create a
167:57 - budget alert now if i needed to edit it
168:00 - i can always go back to ace budget and
168:03 - here i can edit it but i'm not going to
168:05 - touch it and i'm just going to hit
168:06 - cancel
168:07 - and so the last thing i wanted to show
168:09 - you before we end this lesson is how to
168:11 - create another budget but being able to
168:14 - send out the trigger alert emails to
168:17 - different users
168:18 - and so in order to do that i'm going to
168:20 - go back up here to create budget i'm
168:22 - going to name this to ace dash
168:26 - budget
168:27 - dash
168:28 - users i'm going to leave the rest as is
168:32 - i'm going to click on next again i'm
168:34 - going to leave the budget type the way
168:35 - it is the target amount i'm going to put
168:38 - ten dollars
168:39 - leave the include credits and cost and
168:41 - just click on next
168:43 - and so here i'm going to leave the
168:44 - threshold rules the way they are and
168:46 - right here under manage notifications
168:49 - i'm going to click off link monitoring
168:51 - email notification channels to this
168:53 - budget now because the email
168:56 - notification channel needs cloud
168:58 - monitoring in order to work i am
169:00 - prompted here to select a workspace
169:03 - which is needed by cloud monitoring so
169:05 - because i have none i'm going to go
169:07 - ahead and create one and so clicking on
169:10 - managing monitoring workspaces will
169:12 - bring you to the documentation but in
169:14 - order for me to get a workspace created
169:17 - i need to go to cloud monitoring now
169:20 - workspace is the top level container
169:23 - that is used to organize and control
169:25 - access to your monitoring notification
169:28 - channels in order for your notification
169:30 - channels to work they must belong to a
169:33 - monitoring workspace so you need to
169:36 - create at least one workspace before
169:38 - adding monitoring
169:40 - notification channels and don't worry
169:42 - we'll be getting into greater depth with
169:45 - regards to monitoring in a later section
169:48 - in this course so i'm going to go ahead
169:50 - and cancel this
169:51 - and i'm going to go up to the navigation
169:53 - menu
169:54 - click on there
169:56 - and scroll down to monitoring
170:01 - and then overview and this may take a
170:03 - minute to start up
170:05 - as the apis are being enabled and the
170:07 - default workspace for cloud monitoring
170:10 - is being built
170:12 - okay and now that the monitoring api has
170:14 - been enabled we are now in monitoring
170:17 - the workspace that was created is my
170:19 - first project so now that we have our
170:21 - monitoring workspace created i need to
170:24 - add the emails to the users that i want
170:27 - the alerts to be sent out to and added
170:29 - to the notification channel so in order
170:31 - to do that i'm going to go over here to
170:34 - alerting and up here at the top i'm
170:36 - going to click on edit notification
170:38 - channels
170:39 - and here as you can see are many
170:41 - notification channels that you can
170:42 - enable by simply clicking on add new
170:45 - over here on the right so now what i'm
170:48 - looking for is under email i'm going to
170:51 - click on add new now here i can add the
170:53 - new email address and so for me i'm
170:56 - going to add antony
170:58 - at antonyt.com
171:00 - and you can add whatever email address
171:02 - you'd like
171:03 - and under display name i'm going to add
171:06 - billing admin
171:08 - notification
171:12 - and just click on save
171:14 - and as you can see my email has been
171:17 - added to the notification channel and so
171:19 - this is all i needed to do in order to
171:22 - move on to the next step and so now that
171:24 - i've covered creating my monitoring
171:26 - workspace as well as adding another
171:29 - email to my email notification channels
171:32 - i can now go back to billing and finish
171:35 - off my budget alert
171:39 - let's go over here to budgets and alerts
171:43 - create budget
171:44 - and we're gonna go through the same
171:46 - steps
171:47 - call this billing
171:50 - alert
171:53 - users
171:55 - leave everything else as is and click on
171:58 - next
171:59 - i'm just going to change the target
172:00 - amount to 10
172:03 - click on next
172:04 - i'm going to leave everything here as is
172:07 - and i'm going to go back to click on
172:09 - link monitoring email notification
172:11 - channels to this budget now if you
172:13 - notice when i click on select workspace
172:16 - my first project shows up and here it
172:19 - will ask me for my notification channels
172:22 - and because i've already set it up i can
172:24 - simply click on it and you'll see the
172:27 - billing admin notification channel
172:29 - and so if i didn't have this set up i
172:32 - can always go to manage notification
172:34 - channels and it'll bring me back to the
172:36 - screen which you saw earlier
172:38 - and so now that that's set up i can
172:40 - simply click on finish
172:42 - and so now that i have a
172:44 - regular budget alert i also have another
172:47 - budget alert that can go to a different
172:49 - email so if you have a project manager
172:52 - or a director that you want to send
172:54 - budget alerts to this is how you would
172:57 - do it and so that about covers this demo
173:00 - on committed use discounts reservations
173:03 - budgets and budget alerts and so that's
173:06 - all i wanted to cover for this lesson
173:08 - so you can now mark this lesson as
173:10 - complete
173:11 - and let's move on to the next one
173:13 - [Music]
173:17 - welcome back
173:18 - in this short lesson i will be covering
173:21 - the exporting of your billing data so
173:24 - that you're able to analyze that data
173:26 - and understand your spend at a more
173:28 - granular level
173:30 - i will also be going through a short
173:32 - demo where i will show you how to enable
173:35 - the export billing feature and bring it
173:38 - into bigquery to be analyzed
173:41 - now cloud billing export to bigquery
173:44 - enables you to export granular google
173:48 - cloud billing data such as usage
173:51 - cost details and pricing data
173:54 - automatically to a bigquery data set
173:56 - that you specify then you can access
174:00 - your cloud billing data from bigquery
174:02 - for detailed analysis
174:04 - or use a tool like data studio to
174:07 - visualize your data just a quick note
174:10 - here that billing export is not
174:12 - retroactive and this should be taken
174:15 - into consideration when planning for
174:18 - analysis on this data and so there are
174:20 - two types of cloud billing data that you
174:23 - can export
174:24 - there's the daily cost detail data
174:28 - and the pricing data and these can be
174:30 - selected right within the console
174:32 - depending on your use case and so now
174:35 - that we've gone through exactly what
174:37 - billing export is i wanted to get into a
174:40 - demo and show you how to export your
174:43 - cloud billing data to bigquery and go
174:46 - through all the necessary steps to get
174:49 - it enabled so when you're ready join me
174:51 - in the console and so here we are back
174:53 - in the console and so in order to enable
174:56 - billing export i'm going to be going to
174:58 - the billing page so i'm going to move up
175:01 - to the top left hand corner to the
175:03 - navigation menu
175:05 - and click on billing
175:08 - here in the left hand menu you'll see
175:10 - billing export and you can just click on
175:12 - there
175:13 - and so for those just coming to billing
175:15 - export for the first time there's a
175:17 - quick summary of exactly what the
175:19 - bigquery export is used for and as we
175:22 - discussed earlier there is an option for
175:24 - the daily cost detail and for pricing
175:28 - and i'm going to use the daily cost
175:30 - detail in this demo and export that data
175:33 - to bigquery so the first step i'm going
175:35 - to do
175:36 - is to click on edit settings and it's
175:38 - going to bring me to a new page where it
175:41 - will ask me for my project and this is
175:43 - where my billing data is going to be
175:45 - stored but as you can see here i'm
175:47 - getting a prompt that says you need to
175:49 - create a bigquery data set first now the
175:52 - bigquery data set that is asking for is
175:55 - where the billing data is going to be
175:57 - stored so in order to move forward with
175:59 - my billing export i need to go to
176:01 - bigquery and set up a data set so i'm
176:04 - going to simply click on this button
176:05 - here that says go to bigquery
176:09 - and it's going to bring me to the
176:10 - bigquery page where i'll be prompted
176:13 - with a big welcome note you can just
176:15 - click on done and over here in the right
176:17 - hand side where it says create data set
176:20 - i'm just going to click on there and i'm
176:22 - going to create my new data set and so
176:24 - for my data set id i'm going to call
176:27 - this
176:28 - billing
176:29 - export
176:31 - and just as a note with the data set id
176:33 - you can't use any characters like
176:35 - hyphens commas or periods and therefore
176:38 - i capitalize the b and the e now with
176:41 - the data location the default location
176:44 - is the us multi region but i can simply
176:47 - click on the drop down and have an
176:49 - option to store my data in a different
176:52 - location
176:53 - but i'm going to keep it at default i
176:55 - have the option of expiring this table
176:58 - in either a certain amount of days or to
177:00 - never expire as well when it comes to
177:02 - encryption i'm going to leave it as
177:05 - google manage key as opposed to a
177:07 - customer manage key and i'll get into
177:10 - encryption and key management a little
177:12 - later on in this course i'm going to go
177:14 - ahead and move right down to the bottom
177:16 - and click on create data set
177:18 - and now my data set has been created i
177:21 - can now see it over here on the left
177:23 - hand side menu where subtle poet 28400
177:27 - is the id for my project if i simply
177:30 - click on the arrow beside it it'll show
177:32 - my billing export data set because
177:35 - there's nothing in it nothing is showing
177:37 - and so now that the data set is set up i
177:40 - can now go back to the billing export
177:42 - page and finish setting up my billing
177:44 - export so with that being said i'm going
177:47 - to go back up to the navigation menu
177:49 - head over to billing
177:52 - and go to billing export under daily
177:54 - cost detail i'm going to click on edit
177:56 - settings and because i have a data set
177:59 - already set up and since it's the only
178:01 - one it has been propagated in my billing
178:04 - export data set field if i had more data
178:06 - sets then i would be able to select them
178:08 - here as well so i'm going to leave the
178:10 - data set at billing export
178:12 - and simply click on save
178:15 - and so now that billing export has been
178:17 - enabled i'll be able to check on my
178:19 - billing as it is updated each day as it
178:22 - says here and to go right to the data
178:24 - set i can simply click on this hot link
178:27 - and it'll bring me right to bigquery and
178:29 - so there is one last step that still
178:31 - needs to be done to enable the billing
178:34 - export to work and that is to enable the
178:37 - bigquery data transfer service api so in
178:41 - order to do that we need to go back to
178:43 - the navigation menu
178:45 - go into apis and services
178:48 - into the dashboard
178:51 - and now i'm going to do a search for the
178:53 - bigquery data transfer service and i'm
178:55 - going to simply go up here to the top
178:57 - search bar
178:58 - and simply type in bigquery
179:02 - and here it is bigquery data transfer
179:04 - api i'm going to simply click on that
179:07 - and hit enable
179:09 - and this might take a minute and you may
179:11 - be asked to create credentials over here
179:13 - on the top right and you can simply
179:15 - ignore that as they are not currently
179:18 - needed and so now that the bigquery data
179:20 - transfer service api has been enabled
179:23 - i'm now able to go over to bigquery and
179:26 - take a look at my billing export data
179:28 - without any issues now it's going to
179:30 - take time to propagate but by the time i
179:33 - come here tomorrow the data will be
179:35 - fully propagated and i'll be able to
179:37 - query the data as i see fit and so
179:40 - although this is a short demo this is
179:43 - necessary to know for the exam
179:45 - as well being an engineer and looking to
179:48 - query your billing data you will now
179:50 - have the knowledge in order to take the
179:52 - steps necessary that will allow you to
179:55 - do so and so that's all i have for this
179:57 - lesson and demo on export billing data
180:00 - so you can now mark this lesson as
180:02 - complete
180:03 - and let's move on to the next one
180:10 - welcome back
180:11 - in this hands-on demo i'm going to go
180:13 - over apis in google cloud
180:17 - now the google cloud platform is pretty
180:19 - much run on apis
180:22 - whether it's in the console or the sdk
180:25 - under the hood it's hitting the apis now
180:29 - some of you may be wondering what is an
180:31 - api
180:32 - well this is an acronym standing for
180:35 - application programming interface and
180:38 - it's a standard used amongst the
180:40 - programming community in this specific
180:42 - context it is the programming interface
180:46 - for google cloud services
180:48 - and as i said before both the cloud sdk
180:51 - and the console are using apis under the
180:54 - hood and it provides similar
180:57 - functionality now when using the apis
180:59 - directly it allows you to enable
181:02 - automation in your workflow by using the
181:05 - software libraries that you use for your
181:08 - favorite programming language now as
181:10 - seen in previous lessons
181:12 - to use a cloud api
181:14 - you must enable it first so if i went to
181:17 - compute engine or when i was enabling
181:19 - monitoring
181:22 - i had to enable the api so no matter the
181:24 - service you're requesting here in google
181:26 - cloud and some of them may be even
181:28 - linked together it always has to be
181:30 - enabled in order to use it now getting a
181:33 - little bit more granular when using an
181:35 - api you need to have a project so when
181:38 - you enable the api you enable it for
181:41 - your project
181:42 - using the permissions on the project and
181:45 - permissions on the api to enable it now
181:48 - since this is a demo i want to go over
181:51 - to the navigation menu and go straight
181:53 - into apis and services
181:56 - and so here is the dashboard of the apis
181:58 - and services you can see the traffic
182:01 - here the errors and the latency with
182:04 - regards to these apis as well up here it
182:08 - has a time frame for the median latency
182:10 - that you can select for a more granular
182:12 - search now when it comes to what is
182:15 - enabled already you can see list here of
182:18 - the apis that are enabled and since we
182:21 - haven't done much there's only a few
182:23 - apis that are enabled now this hands-on
182:26 - demo is not meant to go into depth with
182:29 - apis but is merely an overview so that
182:32 - you understand what the apis are used
182:35 - for in context with google cloud if
182:38 - you'd like to go more in depth with
182:40 - regards to apis and possibly get
182:42 - certified in it the apogee certification
182:46 - with its corresponding lessons would be
182:48 - a great way to get a little bit more
182:50 - understanding but for this demo we're
182:52 - going to stick with this overview and so
182:54 - in order to search for more apis that
182:57 - need to be enabled or if you're looking
182:59 - for something specific
183:00 - you can come up here to enable apis and
183:03 - services
183:04 - or you can do a quick search on the
183:06 - search bar at the top of the page but
183:08 - just as a quick glance i'm going to go
183:10 - into enable apis and services and so you
183:14 - will be brought to a new page where you
183:16 - will see the api library on the left you
183:19 - will see a menu where the apis are
183:21 - categorized and all the apis that are
183:25 - available when it comes to google cloud
183:28 - and other google services so as you saw
183:30 - before when i needed to enable the api
183:34 - for bigquery
183:35 - i would simply type in bigquery
183:38 - and i can go to the api and since the
183:41 - api is enabled there's nothing for me to
183:44 - do but if i needed to enable it i could
183:47 - do that right there and just as a quick
183:49 - note when going to a service that's
183:52 - available in the console the api
183:54 - automatically gets enabled when you go
183:57 - and use it for the first time and so
183:59 - again this is just a quick overview of
184:02 - apis and the api library with regards to
184:06 - google cloud a short yet important demo
184:09 - to understand the under workings of the
184:12 - cloud sdk and the console so just
184:15 - remember that when using any service in
184:17 - google cloud
184:18 - again you must enable the api in order
184:22 - to start using it and so that about
184:24 - wraps up this demo for cloud apis so you
184:27 - can now mark this lesson as complete and
184:29 - let's move on to the next one
184:31 - [Music]
184:35 - welcome back
184:36 - in this demo i'll be creating and
184:39 - setting up a new gmail user as an admin
184:42 - user for use moving ahead in this course
184:46 - as well as following google's best
184:48 - practices we need a user that has lesser
184:51 - privileges than the user account that we
184:53 - set up previously and i'll be going
184:55 - through a full demo to show you how to
184:58 - configure it
185:00 - now in a google cloud setup that uses a
185:03 - g suite or cloud identity account a
185:06 - super administrator account is created
185:09 - to administer the domain this super
185:11 - admin account has irrevocable
185:14 - administrative permissions
185:16 - that should not be used for day-to-day
185:18 - administration this means that no
185:21 - permissions can be taken away from this
185:24 - account and has the power to grant
185:27 - organization admin role
185:29 - or any other role for that matter and
185:31 - recover accounts at the domain level
185:33 - which makes this account extremely
185:36 - powerful now since i do not have a
185:38 - domain setup or using a g suite or cloud
185:42 - identity account i don't need to worry
185:44 - about a super admin account in this
185:47 - specific environment as gmail accounts
185:50 - are standalone accounts that are meant
185:52 - to be personal and hold no organization
185:55 - and usually start at the project level
185:58 - and so to explain it in a bit more
186:00 - detail
186:01 - i have a diagram here showing the two
186:04 - different accounts i will be using
186:06 - and the structure behind it
186:09 - now as we discussed before billing
186:11 - accounts have the option of paying for
186:14 - projects in a different organization so
186:16 - when creating new projects using the two
186:19 - different gmail accounts they were
186:21 - created without any organization and so
186:25 - each account is standalone and can
186:27 - create their own projects now what makes
186:29 - them different is that the antony gcloud
186:33 - ace account owns the billing account and
186:36 - is set as a billing account
186:38 - administrator and the tony bowtie ace
186:41 - account is a billing account user that
186:44 - is able to link projects to that billing
186:46 - account but does not hold full access to
186:49 - billing
186:51 - so in the spirit of sticking to the
186:53 - principle of lease privilege
186:55 - i will be using the tony bowtie ace
186:58 - account that i had created earlier with
187:00 - lesser privileges on billing it will
187:03 - still give me all the permissions i need
187:05 - to create edit and delete resources
187:09 - without all the powerful permissions
187:11 - needed for billing i will be assigning
187:14 - this new gmail user the billing account
187:17 - user role and it will allow you to
187:19 - achieve everything you need to build for
187:22 - the remainder of the course
187:25 - so just as a review i will be using a
187:28 - new google account that i have created
187:30 - or if you'd like you can use a
187:32 - pre-existing google account and as
187:35 - always i recommend enabling two-step
187:38 - verification on your account
187:40 - as this user will hold some powerful
187:43 - permissions to access a ton of different
187:46 - resources in google cloud
187:49 - so now that we've gone over the details
187:52 - of the what and why for setting up this
187:54 - second account let's head into the demo
187:57 - and get things started so whenever
187:59 - you're ready join me over in the console
188:02 - and so here i am back in the console and
188:05 - so before switching over to my new user
188:08 - i need to assign the specific roles that
188:10 - i will need for that user which is the
188:13 - billing account user role so to assign
188:16 - this role to my new user i need to head
188:18 - over to billing so i'm going to go back
188:20 - up here to the left-hand corner
188:22 - and click on the navigation menu
188:25 - and go to billing
188:27 - again in the left-hand menu i'm going to
188:29 - move down to account management and
188:31 - click on there and over here under my
188:34 - billing account you will see that i have
188:36 - permissions assigned to one member of
188:39 - the billing account administrator
188:42 - and as expected i am seeing anthony g
188:45 - cloud ace
188:46 - gmail.com and so i want to add another
188:49 - member to my billing account so i'm
188:52 - going to simply click on add members and
188:54 - here i will enter in my new second user
188:57 - which is tony bowtie ace
189:01 - gmail.com
189:07 - and under select a role i'm going to
189:09 - move down to billing and over to billing
189:11 - account user and as you can see here
189:14 - this role billing account user will
189:17 - allow permissions to associate projects
189:19 - with billing accounts which is exactly
189:22 - what i want to do
189:23 - and so i'm going to simply click on that
189:25 - and simply click on save
189:29 - and so now that i've assigned my second
189:30 - user the proper permissions that i
189:32 - needed i am now going to log out
189:35 - and log in as my new user by simply
189:38 - going up to the right hand corner in the
189:40 - icon clicking on the icon and going to
189:43 - add account by adding the account i'll
189:45 - be able to switch back and forth between
189:48 - the different users and i would only
189:50 - recommend this if you are the sole user
189:52 - of your computer if you are on a
189:54 - computer that has multiple users simply
189:57 - sign out and sign back in again with
189:59 - your different user
190:01 - and here i'm asked for the email which
190:03 - would be tony bowtie ace
190:06 - gmail.com
190:08 - i'm gonna plug in my password
190:12 - and it's going to ask me for my two-step
190:14 - verification
190:21 - i'm going to click on yes
190:24 - and i should be in
190:28 - and because it's my first time logging
190:30 - into google cloud with this user i get a
190:32 - prompt asking me to agree to the terms
190:35 - of service i'm going to agree to them
190:37 - and simply click on agree and continue
190:40 - and so now i'm going to move back up to
190:42 - overview and as you can see here i don't
190:44 - have the permissions to view costs for
190:47 - this billing account and so all the
190:49 - permissions assigned for the billing
190:51 - account administrator which is antony g
190:54 - cloud ace is not applied to tony bowtie
190:57 - ace and therefore things like budgets
191:00 - and alerts even billing exports i do not
191:03 - have access to
191:05 - so moving forward in the course if you
191:07 - need to access anything in billing that
191:10 - you currently don't have access to like
191:13 - budgets and alerts you can simply switch
191:15 - over to your other account and take care
191:17 - of any necessary changes but what i do
191:20 - have access to is if i go up here to my
191:23 - billing account click on the drop down
191:25 - menu and click on manage billing
191:28 - accounts but as you can see here i do
191:30 - have access to view all the billing
191:32 - accounts along with the projects that
191:35 - are linked to them now because these
191:37 - gmail accounts are standalone accounts
191:39 - this project here that is owned by
191:42 - antony gcloud ace i do not have access
191:45 - to in order to access the project i
191:48 - would have to have permissions assigned
191:51 - to me directly in order for me to
191:53 - actually view the project or possibly
191:56 - creating any resources within that
191:58 - project now if i go back to my home page
192:01 - i can see here that i have no projects
192:04 - available and therefore no resources
192:07 - within my environment and so to kick it
192:10 - off i'm going to create a new project
192:13 - and so under project name i am going to
192:15 - call this
192:16 - project tony
192:18 - and you can name your project whatever
192:20 - you'd like
192:21 - under location i don't have any
192:23 - organization
192:25 - and so therefore i'm just going to click
192:27 - on create
192:28 - and this may take a minute to
192:30 - create and here we are with my first
192:34 - project
192:35 - named project tony as well as my
192:37 - notification came up saying that my
192:39 - project has been created and so now that
192:42 - this project has been created it should
192:44 - be linked to my billing account so in
192:46 - order to verify this i'm going to go
192:49 - over into billing
192:53 - and under the drop down
192:55 - i'm going to click on manage billing
192:56 - accounts
192:58 - and as you can see here the number of
193:00 - projects has gone from one to two and if
193:03 - i click on the menu up here under my
193:05 - projects you can see that project tony
193:08 - is a project that is linked to my
193:11 - billing account i also have the
193:12 - permissions to either disable billing or
193:15 - change billing for this specific project
193:18 - yet in order to change billing i will
193:21 - have to have another billing account but
193:23 - there are no other billing accounts
193:25 - available
193:26 - and so moving forward i will only have
193:29 - this one billing account and so any
193:31 - projects i decide to create will be
193:34 - linked to this billing account and so
193:36 - this is a great example of trimming down
193:39 - the permissions needed for different
193:41 - users and even though this is not a
193:44 - domain owned account but a personal
193:46 - account it's always recommended to
193:49 - practice the principle of lease
193:51 - privilege whenever you come across
193:53 - assigning permissions to any user now as
193:56 - i said before any billing related tasks
193:59 - that you decide to do moving forward
194:02 - you can simply switch over to your other
194:04 - user and do the necessary changes and so
194:08 - that's all i have for this lesson
194:10 - so you can now mark this lesson as
194:12 - complete
194:13 - and let's move on to the next one
194:15 - [Music]
194:19 - welcome back
194:21 - in this short lesson i'm going to be
194:23 - covering an overview of the cloud sdk
194:27 - and the command line interface as it is
194:29 - an essential component of interacting
194:32 - with google cloud for the exam you will
194:35 - need to get familiar with the command
194:37 - line and the commands needed in order to
194:40 - create
194:41 - modify and delete resources this is also
194:44 - an extremely valuable tool for your tool
194:48 - belt in the world of being a cloud
194:50 - engineer as i have found that is a very
194:53 - common and easy way to implement small
194:56 - operations within google cloud as well
194:59 - as automating the complex ones so what
195:02 - exactly is the cloud sdk
195:05 - well the cloud sdk is a set of command
195:08 - line tools
195:10 - that allows you to manage resources
195:12 - through the terminal in google cloud and
195:15 - includes commands such as gcloud
195:18 - gsutil bq and cubectl using these
195:23 - commands
195:24 - allow you to manage resources such as
195:26 - compute engine
195:27 - cloud storage bigquery kubernetes and so
195:31 - many other resources these tools can be
195:35 - run interactively or through automated
195:38 - scripts giving you the power and
195:40 - flexibility that you need to get the job
195:43 - done the cloud sdk is so powerful that
195:47 - you can do everything that the console
195:49 - can do yet has more options than the
195:52 - console you can use it for
195:53 - infrastructure as code autocompletion
195:56 - helps you finish all of your command
195:58 - line statements and for those of you who
196:00 - run windows the cloud sdk has got you
196:04 - covered with availability for powershell
196:08 - now in order to access google cloud
196:10 - platform you will usually have to
196:13 - authorize google cloud sdk tools so to
196:17 - grant authorization to cloud sdk tools
196:20 - you can either use a user account or a
196:24 - service account now a user account is a
196:26 - google account that allows end users to
196:30 - authenticate directly to your
196:32 - application for most common use cases on
196:35 - a single machine using a user account is
196:38 - best practice now going the route of a
196:40 - service account this is a google account
196:43 - that is associated with your gcp project
196:47 - and not a specific user a service
196:49 - account can be used by providing a
196:52 - service account key to your application
196:54 - and is recommended to script cloud sdk
196:58 - tools for use on multiple machines now
197:01 - having installed the cloud sdk it comes
197:04 - with some built-in commands that allow
197:07 - you to configure different options using
197:09 - gcloud init this initializes and
197:12 - authorizes access and performs other
197:16 - common cloud sdk setup steps using some
197:19 - optional commands
197:21 - gcloud auth login authorizes your access
197:25 - for gcloud with google user credentials
197:28 - and sets the current account as active
197:31 - gcloud config
197:33 - is another optional configuration that
197:35 - allows you to configure accounts and
197:38 - projects as well gcloud components
197:41 - allow you to install
197:43 - update and delete
197:45 - optional components of the sdk that give
197:48 - you more flexibility with different
197:51 - resources now after having installed the
197:54 - cloud sdk almost all gcloud commands
197:57 - will follow a specific format shown here
198:01 - is an example of this format and is
198:03 - broken down through component
198:05 - entity
198:06 - operation positional arguments and flags
198:10 - and i'll be going through some specific
198:12 - examples in the demonstration a little
198:15 - bit later on and so that's all i wanted
198:17 - to cover in this overview of the cloud
198:20 - sdk and the cli so you can now mark this
198:24 - lesson as complete and you can join me
198:26 - in the next one where i go ahead and
198:28 - demonstrate installing the cloud sdk
198:31 - [Music]
198:36 - back in this demonstration i will show
198:39 - you how to download install and
198:41 - configure the cloud sdk and i will be
198:44 - using the quick start guide that lies in
198:47 - the cloud sdk documentation which holds
198:51 - all the steps for installing the cloud
198:53 - sdk on different operating systems and i
198:57 - will make sure to include it in the
198:58 - lesson text below this demo will show
199:01 - you how to install the cloud sdk
199:04 - on each of the most common operating
199:06 - systems
199:08 - windows
199:09 - mac os and ubuntu linux all you need to
199:12 - do is follow the process on each of the
199:15 - pages and you should be well on your way
199:18 - so with that being said let's get this
199:20 - demo started and bring the cloud sdk to
199:23 - life by getting it all installed and
199:26 - configured for your specific operating
199:28 - system
199:30 - so as i explained before i'm gonna go
199:32 - ahead and install the cloud sdk
199:36 - on each of the three different operating
199:38 - systems
199:39 - windows mac os and ubuntu linux and i
199:43 - will be installing it with the help of
199:45 - the quick start guide that you see here
199:47 - and as i said before i'll be including
199:50 - this link in the lesson text and so to
199:52 - kick off this demo i wanted to start by
199:55 - installing the cloud sdk on windows so
199:58 - i'm going to move over to my windows
200:00 - virtual machine and i'm going to open up
200:02 - a browser and i'm going to paste in the
200:05 - link for the quick start guide
200:07 - and you can click on either link for the
200:10 - quick start for windows and each quick
200:12 - start page will give me the instructions
200:15 - of exactly what i need to do for each
200:18 - operating system so now it says that we
200:20 - need to have a project created which i
200:23 - did in the last lesson which is project
200:25 - tony so next i'm going to download the
200:28 - cloud sdk installer
200:30 - so i'm going to click on there
200:33 - and i'll see a prompt in the bottom left
200:35 - hand corner that the installer has been
200:37 - downloaded i'm going to click on it to
200:39 - open the file and i'm going to be
200:41 - prompted to go through this wizard and
200:43 - so i'm just going to click on next
200:45 - i'm going to agree to the terms of the
200:47 - agreement it's going to be for just me
200:49 - anthony and my destination folder i'll
200:52 - keep it as is and here's all the
200:54 - components that it's going to install
200:56 - i'm going to keep the beta commands
200:58 - unchecked as i don't really need them
201:00 - and if i need them later then i can
201:02 - install that component for those who are
201:04 - more experienced or even a bit curious
201:07 - you could click on the beta commands and
201:09 - take it for a test drive but i'm going
201:11 - to keep it off and i'm going to click
201:13 - install and depending on the power of
201:15 - your machine
201:16 - it should take anywhere from two to five
201:18 - minutes to install and the google cloud
201:21 - sdk has been installed and so i'm just
201:24 - going to click on next and as shown here
201:26 - in the documentation you want to make
201:28 - sure that you have all your options
201:30 - checked off is to create a start menu
201:32 - shortcut a desktop shortcut you want to
201:35 - start the google cloud sdk shell and
201:38 - lastly you want to run gcloud init in
201:41 - order to initialize and configure the
201:43 - cloud sdk now i'm going to click on
201:46 - finish to exit the setup and i'm going
201:48 - to get a command shell that pops up and
201:50 - i'm just going to zoom in for better
201:52 - viewing
201:53 - and so it says here my current
201:54 - configuration has been set to default so
201:57 - when it comes to configuration this is
202:00 - all about selecting the active account
202:03 - and so my current active account is
202:05 - going to be set as the default account
202:07 - it also needed to do a diagnostic check
202:09 - just to make sure that it can connect to
202:12 - the internet so that it's able to verify
202:14 - the account and so now the prompt is
202:16 - saying you must log in to continue would
202:18 - you like to log in yes
202:21 - you can just click on y and then enter
202:24 - and it's going to prompt me with a new
202:26 - browser window where i need to log in
202:29 - using my current account so that i can
202:32 - authorize the cloud sdk so i'm going to
202:34 - log in with my tony bowtie ace account
202:37 - click on next
202:38 - type in my password
202:41 - again it's going to ask me for my
202:43 - two-step verification
202:46 - and i'm going to get a prompt saying
202:48 - that the google sdk wants to access my
202:50 - google account
202:51 - i'm going to click on allow
202:54 - and success you are now authenticated
202:56 - with the google cloud sdk
202:59 - and if i go back to my terminal i am
203:01 - prompted to enter some values
203:03 - so that i can properly configure the
203:06 - google cloud sdk so i'm going to pick a
203:09 - cloud project to use
203:10 - and i'm going to use project tony that i
203:13 - created earlier so i'm going to enter 1
203:15 - and hit enter
203:18 - and again whatever project that you've
203:20 - created use that one for your default
203:23 - configuration and it states here that my
203:26 - current project has been set to project
203:28 - tony and again this configuration is
203:30 - called default
203:32 - so if i have a second configuration that
203:34 - i wanted to use i can call it a
203:36 - different configuration but other than
203:38 - that my google cloud sdk is configured
203:42 - and ready to use so just to make sure
203:44 - that it's working i'm going to run a
203:46 - couple commands i'm going to run the
203:48 - gcloud
203:50 - help
203:52 - command
203:53 - and as you can see it's given me a list
203:56 - of a bunch of different commands that i
203:58 - can run and to exit you can just hit
204:00 - ctrl c i'm going to run gcloud
204:04 - config list
204:07 - and this will give me
204:08 - my properties in my active configuration
204:11 - so my account is tony bowtie ace
204:14 - gmail.com i've disabled usage reporting
204:17 - and my project is project tony and my
204:20 - active configuration is set as default
204:23 - now don't worry i'm going to be covering
204:26 - all these commands in the next lesson
204:28 - and i'm going to be going into detail on
204:31 - how you can configure and add other
204:34 - users
204:35 - within your cloud sdk configuration so
204:38 - as we go deeper into the course i'm
204:40 - going to be using a lot more command
204:42 - line just so you can get familiar with
204:44 - the syntax and become a bit more
204:47 - comfortable with it so now that i've
204:49 - installed the cloud sdk on windows the
204:52 - process will be a little bit different
204:54 - when it comes to installation on the
204:56 - other operating systems but will be very
204:59 - similar when it comes to the
205:01 - configuration so now let's head over to
205:04 - mac os and install the cloud sdk there
205:08 - and so here we are in mac os and so the
205:11 - first thing i want to do is i want to
205:13 - open up a web browser and i want to go
205:15 - to the cloud sdk quick start page so i'm
205:18 - just going to paste in the url here
205:22 - and we're looking for the quick start
205:23 - for mac os and so you can either click
205:26 - on the menu from the left hand side or
205:28 - the menu here on the main page
205:31 - and so like i said before
205:32 - this installation is going to be a
205:34 - little bit different than what it was in
205:37 - windows and so there's a few steps here
205:39 - to follow and so the first step asks us
205:41 - if we have a project already created
205:44 - which we've already done and is project
205:46 - tony and so the next step tells us that
205:49 - the cloud sdk
205:50 - requires python and so we want to check
205:53 - our system to see if we have a supported
205:56 - version so in order to check our version
205:59 - we're going to use this command here
206:00 - python minus v
206:02 - and i'm going to copy that to my
206:03 - clipboard
206:05 - and then open up a terminal and i'm
206:07 - going to zoom in for better viewing and
206:09 - so i'm going to paste the command in
206:11 - here
206:12 - and simply click on enter and as you can
206:14 - see here i'm running python 2.7
206:19 - but the starred note here says that the
206:21 - cloud sdk will soon move to python 3 and
206:25 - so in order to avoid having to upgrade
206:28 - later you'd want to check your version
206:30 - for python 3 and so you can use a
206:32 - similar command by typing in python 3
206:37 - space minus capital v
206:39 - and as you can see i'm running version
206:42 - 3.7.3
206:44 - and so moving back to the guide i can
206:46 - see here that it is a supportive version
206:49 - if you do not have a supportive version
206:52 - i will include a link on how to upgrade
206:54 - your version
206:55 - in the lesson text below and so now that
206:58 - i've finished off this step let's move
207:00 - on to the next one
207:02 - where i can download the archive file
207:05 - for the google cloud sdk again most
207:08 - machines will run the 64-bit package so
207:12 - if you do have the latest operating
207:14 - system for mac os
207:16 - you should be good to go so i'm going to
207:18 - click on this package
207:20 - and it'll start downloading for me and
207:22 - once it's finished you can click on
207:24 - downloads and click on the file itself
207:27 - and it should extract itself in the same
207:30 - folder with all the files and folders
207:32 - within it and so just as another quick
207:34 - note google prefers that you keep the
207:37 - google cloud sdk in your home directory
207:40 - and so following the guide i'm going to
207:42 - do exactly that and so the easiest way
207:45 - to move the folder into your home
207:47 - directory is to simply drag and drop it
207:51 - into the home folder
207:53 - on the left hand menu it should be
207:55 - marked with a little house icon and
207:58 - nested under favorites i can now move
208:00 - into my home folder and confirm that it
208:03 - is indeed in here and so now moving to
208:06 - the last step which shows as optional
208:09 - the guide asks us to install a script to
208:12 - add cloud sdk tools to our path now i
208:16 - highly recommend that you install this
208:18 - script so that you can add the tools for
208:22 - command completion and i will get into
208:24 - command completion a little bit later on
208:27 - in the next couple of lessons and so
208:29 - here is the command that i need to run
208:32 - so i'm going to copy that to my
208:33 - clipboard again and i'm going to move
208:35 - back over to my terminal i'm going to
208:37 - clear my screen and so to make sure i'm
208:40 - in my home directory where the cloud sdk
208:43 - folder is i'm going to simply type ls
208:47 - and so for those who don't know
208:49 - ls is a linux command that will list all
208:52 - your files and folders in your current
208:54 - path and as you can see here the google
208:57 - cloud sdk is in my path and therefore i
209:01 - can run that script so i'm going to
209:03 - paste it in here
209:05 - and i'm going to hit enter
209:07 - and so a prompt comes up asking me
209:09 - whether or not i want to disable usage
209:11 - reporting and because i want to help
209:13 - improve the google cloud sdk i'm going
209:16 - to type in y for yes and hit enter and
209:19 - so as i was explaining before
209:22 - the cloud sdk tools will be installed in
209:25 - my path and so this is the step that
209:27 - takes care of it and so i'm going to
209:29 - type y
209:30 - and enter
209:32 - for yes to continue and usually the path
209:35 - that comes up is the right one unless
209:37 - you've changed it otherwise so i'm going
209:39 - to leave this blank and just hit enter
209:42 - and that's it i've installed the tools
209:44 - so now in order for me to run gcloud
209:46 - init i have to start a new shell as it
209:49 - says here for the changes to take effect
209:52 - so i'm going to go up here to the top
209:54 - left hand menu click on terminal and
209:56 - quit terminal and so now i can restart
209:59 - the terminal
210:00 - again i'm going to zoom in for better
210:02 - viewing
210:03 - and now i'm able to run gcloud init in
210:06 - order to initialize the installation
210:12 - again the prompt to do the diagnostic
210:14 - tests and i can see i have no network
210:16 - issues but it shows me that i have to
210:18 - login to continue i would like to log in
210:21 - so i'm going to type y for yes and hit
210:24 - enter
210:27 - and so a new browser has popped open
210:30 - prompting me to enter my email and
210:33 - password and so i'm going to do that now
210:39 - i'm going to authorize my account with
210:41 - two-step verification
210:44 - i'm not going to save this password and
210:47 - yes i want to allow the google cloud sdk
210:50 - to access my google account
210:53 - so i'm going to click on allow
210:57 - and it shows that i've been
210:58 - authenticated so now i'm going to move
211:00 - back to my terminal and so just as a
211:02 - note before we move forward in case you
211:05 - don't get a browser pop-up for you to
211:07 - log into your google account you can
211:10 - simply highlight this url copy it into
211:13 - your browser and it should prompt you
211:15 - just the same so moving right ahead it
211:18 - shows that i'm logged in as
211:19 - tonybowtieace
211:21 - gmail.com which is exactly what i wanted
211:24 - and it's asking me to pick a cloud
211:26 - project to use now i want to use project
211:28 - tony so i'm going to type in 1 and enter
211:32 - and that's it the cloud sdk has been
211:34 - configured and just to double check i'm
211:36 - going to run the gcloud
211:39 - config list command to show me my
211:41 - configuration and as you can see here my
211:44 - account is tonybowties
211:46 - gmail.com my disable usage reporting is
211:49 - equal to false
211:50 - and my project is project tony and again
211:53 - my active configuration is set as
211:56 - default and so that about covers the
211:58 - cloud sdk install for mac os and so
212:02 - finally i'm going to move over to ubuntu
212:05 - linux and configure the cloud sdk there
212:09 - and so here we are in ubuntu and like i
212:11 - did in the other operating systems i'm
212:13 - going to open up the browser and i'm
212:15 - going to paste in the url for the quick
212:17 - start guide
212:19 - and so we want to click on the quick
212:21 - start for debian and ubuntu and so again
212:24 - you have your choice from either
212:26 - clicking on the link on the left hand
212:27 - menu or the one here in the main menu
212:30 - and so following the guide
212:32 - it is telling us that when it comes to
212:35 - an ubuntu release it is recommended that
212:38 - the sdk should be installed on an ubuntu
212:41 - release that has not reached end of life
212:44 - the guide also asks to create a project
212:47 - if we don't have one already which we
212:49 - have already done
212:50 - and so now we can continue on with the
212:52 - steps and so since we are not installing
212:55 - it inside a docker image we're gonna go
212:57 - ahead and use the commands right here
212:59 - now you can copy all the commands at
213:01 - once
213:02 - by copying this to the clipboard but my
213:05 - recommendation is to install each one
213:08 - one by one so i'm going to copy this
213:11 - and i'm going to open up my terminal i'm
213:14 - going to zoom in for better viewing and
213:16 - i'm going to paste that command in and
213:18 - click on enter it's going to prompt me
213:20 - for my password
213:21 - and it didn't come up with any errors so
213:24 - that means it was successfully executed
213:26 - and so i'm going to move on to the next
213:28 - command
213:30 - i'm going to copy this
213:32 - go back over to my terminal
213:35 - and paste it in
213:37 - now for those of you who do not have
213:39 - curl installed you will be prompted to
213:41 - install it and given the command to run
213:44 - it so i'm going to copy and paste this
213:46 - command
213:47 - and click on enter
213:50 - i'm going to type in y for yes to
213:52 - continue and it's going to install it
213:55 - after a couple of minutes okay now that
213:57 - curl has been installed i'm able to run
214:00 - that command again i'm going to clear
214:02 - the screen first
214:04 - and that executed with no errors as well
214:07 - and so now moving on to the last command
214:09 - this command will download and install
214:11 - the google cloud sdk
214:18 - i am prompted to install some packages
214:21 - and so i'm going to type y for yes to
214:23 - continue so now it's going to download
214:26 - and install the necessary packages
214:29 - needed for the google cloud sdk and
214:32 - depending on the speed of your internet
214:34 - and the speed of your machine this could
214:36 - take anywhere from two to five minutes
214:38 - okay and the google cloud sdk has been
214:41 - installed
214:42 - and so now that the cloud sdk has been
214:44 - installed we can now initialize the
214:47 - configuration
214:48 - so i'm going to type in gcloud init
214:52 - again the prompt with the network
214:54 - diagnostics i'm going to type y for yes
214:57 - to log in
214:59 - and i'm going to get the prompt for my
215:01 - email and password
215:04 - i'm going to take care of my two-step
215:06 - verification and i'm going to allow the
215:08 - google cloud sdk to access my google
215:11 - account and success i am now
215:13 - authenticated and moving back to the
215:15 - terminal just to verify it and again i'm
215:18 - going to pick project tony as the cloud
215:21 - project to use
215:24 - and the cloud sdk has been configured as
215:27 - always i'm going to do a double check by
215:29 - running a gcloud config list
215:36 - and as expected the same details has
215:39 - come up and so this is a quick run
215:41 - through on all three operating systems
215:44 - windows mac os and ubuntu linux on how
215:48 - to install the google cloud sdk and this
215:52 - will help you get started with becoming
215:54 - more familiar and more comfortable using
215:58 - the command line interface and so that
216:00 - about wraps up for this lesson
216:02 - so you can now mark this lesson as
216:04 - complete
216:05 - and let's move on to the next one
216:07 - [Music]
216:11 - welcome back in the last demo we went
216:14 - through a complete install of the cloud
216:16 - sdk
216:17 - and configured our admin account to be
216:20 - used within it in this demonstration i
216:22 - will be walking through how to manage
216:25 - the cloud sdk and this will involve how
216:28 - to utilize it and how to customize it to
216:31 - your environment as well as configuring
216:33 - our other user account so that we are
216:36 - able to apply switching configurations
216:39 - from one user to another and so i will
216:42 - be going through initializing and
216:44 - authorization
216:45 - configurations and properties installing
216:48 - and removing components as well as a
216:51 - full run through of the gcloud
216:53 - interactive shell so let's kick off this
216:55 - demo by diving into a pre-configured
216:58 - terminal with the sdk installed and
217:01 - configured with my second user tony
217:04 - bowtie ace gmail.com
217:07 - and so here i am in the mac os terminal
217:10 - and just be aware that it doesn't matter
217:12 - which operating system you're running as
217:15 - long as the sdk is installed and you
217:17 - have your user configured and so as you
217:19 - saw in the last lesson after you install
217:22 - the cloud sdk the next step is typically
217:26 - to initialize the cloud sdk
217:29 - by running the gcloud init command and
217:31 - this is to perform the initial setup
217:34 - tasks as well as authorizing the cloud
217:37 - sdk to use your user account credentials
217:40 - so that it can access google cloud and
217:42 - so in short it sets up a cloud sdk
217:45 - configuration and sets a base set of
217:48 - properties and this usually covers the
217:51 - active account the current project and
217:53 - if the api is enabled the default google
217:56 - compute engine region and zone now as a
217:59 - note if you're in a remote terminal
218:02 - session with no access to a browser you
218:04 - can still run the gcloud init command
218:07 - but adding a flag of dash dash console
218:11 - dash only
218:12 - and this will prevent the command from
218:14 - launching a browser-based authorization
218:17 - like you saw when setting up your last
218:19 - user so now even though i have a user
218:22 - already set up i can still run gcloud
218:24 - init and it will give me a couple
218:27 - different options to choose from so i
218:29 - can re-initialize this configuration
218:32 - with some new settings or i can create a
218:35 - new configuration now for this demo
218:37 - since we already have two users and to
218:41 - demonstrate how to switch between
218:42 - different users i want to create a new
218:45 - configuration with my very first user so
218:48 - i'm going to type in 2 and hit enter and
218:51 - it's going to ask me for a configuration
218:53 - name now it asks me for a configuration
218:56 - name because when setting up your first
218:58 - configuration it's set as default and
219:01 - because i know that this user account
219:04 - has full access to billing as well as
219:06 - administration privileges i'm going to
219:09 - call this configuration master and i'm
219:11 - going to hit enter
219:12 - it did the necessary network checks and
219:15 - now it's asking me for which account i
219:17 - want to use this configuration for now
219:19 - if tony bowtie ace had access to two
219:23 - different google cloud accounts i would
219:25 - be able to add a different configuration
219:27 - here and so because i'm going to log in
219:30 - with a new account i'm going to put in
219:32 - two
219:33 - and hit enter
219:36 - and so again it brought me to my browser
219:38 - window and i'm going to log in using
219:41 - another account
219:42 - and so here you can type in the first
219:44 - account that you created and for me it
219:47 - was antony gcloud ace gmail.com
219:51 - i hit next and i'm going to enter my
219:54 - password
219:56 - it's going to ask me for my two-step
219:57 - verification
219:59 - and i don't want to save this password
220:01 - and i'm going to allow the google cloud
220:03 - sdk to access my google account and i am
220:07 - now authenticated so moving back to the
220:09 - console you can see here that i am
220:11 - currently logged in and it's asking me
220:14 - to pick a cloud project to use now since
220:16 - i only have one project in that google
220:19 - cloud account which is subtle poet i'm
220:22 - going to choose one
220:24 - and since i have the compute engine api
220:26 - enabled i am now able to configure a
220:29 - default compute region and zone and so
220:32 - i'm going to hit y for yes to configure
220:35 - it and as you can see there are 74
220:37 - different options to choose from and if
220:40 - you scroll up a little bit you should be
220:42 - able to find the zone that you're
220:43 - looking for and so for this course we
220:46 - are going to be using us central one
220:49 - dash a and so this is number eight so
220:51 - i'm going to scroll back down
220:53 - and type in eight
220:56 - and so now my master configuration has
220:58 - been configured with my antony g cloud
221:02 - ace account using us central 1a as the
221:05 - compute engine zone now touching back on
221:08 - authorization if i didn't want to set up
221:11 - a whole configuration i can simply type
221:14 - in gcloud
221:15 - auth login
221:17 - and this will allow me to authorize just
221:20 - the user account only so gcloud init
221:23 - would authorize access and perform the
221:26 - cloud sdk setup steps and gcloud auth
221:29 - login will authorize the access only now
221:32 - as i mentioned in a previous lesson you
221:35 - can use a service account for
221:36 - authorization to the cloud sdk tools and
221:39 - this would be great for a compute
221:41 - instance or an application but would
221:44 - need a service account key file in order
221:46 - to authorize it and so moving back to
221:49 - our user accounts when running the cloud
221:51 - sdk you can only have one active account
221:55 - at any given time and so to check my
221:58 - active account i can type in the command
222:00 - gcloud auth list
222:03 - and this will give me a list of all the
222:05 - accounts that have been authorized and
222:07 - so whenever you run a gcloud init it
222:10 - will use that account as the active
222:13 - account and as you can see here the
222:15 - antony gcloud ace gmail.com has a star
222:19 - beside it and this is marked as the
222:21 - active account and so in essence the
222:24 - account with the star beside it is the
222:26 - active account and so i'm looking to
222:29 - change my active account back to tony
222:31 - bowtie ace and in order for me to do
222:34 - that the command is conveniently shown
222:37 - here and so i'm going to go ahead and
222:38 - run that
222:40 - and the account would be the user shown
222:43 - above and so when i do a gcloud auth
222:46 - list
222:47 - i can see that my active account is now
222:49 - back to tony bowtie bowtieace gmail.com
222:53 - now if you wanted to switch the account
222:55 - on a per command basis you can always do
222:58 - that using the flag dash dash account
223:01 - after the command and put in the user
223:03 - account that you want to use and so
223:05 - let's say i wanted to revoke credentials
223:07 - from an account that i don't need
223:09 - anymore i can simply use the command
223:12 - gcloud auth revoke
223:14 - followed by the username and it will
223:16 - revoke the credentials for that account
223:19 - and so doing this
223:20 - would remove your credentials and any
223:22 - access tokens for any specific account
223:26 - that you choose that's currently on your
223:28 - computer and so if we're looking for
223:30 - that specific account we can always use
223:33 - the gcloud info command and it will give
223:36 - us the path for the user config
223:38 - directory and it is this directory that
223:41 - holds your encrypted credentials and
223:43 - access tokens
223:45 - alongside with your active
223:46 - configurations and any other
223:48 - configurations as well now as you can
223:50 - see here running the gcloud info command
223:53 - will also give you some other
223:55 - information everything from the account
223:58 - the project the current properties and
224:01 - where the logs can be found so now
224:04 - moving on to configurations
224:06 - a configuration is a named set of gcloud
224:09 - cli properties
224:11 - and it works kind of like a profile and
224:13 - so earlier on i demonstrated how to set
224:16 - up another configuration through gcloud
224:19 - init so now if i run a gcloud config
224:23 - list command it would give me all the
224:25 - information of the active configuration
224:28 - so as you can see here my user has
224:31 - changed but my configuration has stayed
224:33 - the same now as seen previously in a
224:36 - different lesson tony bow tie ace does
224:39 - not have access to the project subtle
224:42 - poet this project belongs to antony g
224:45 - cloud ace and the configuration was set
224:48 - for that account now if tony bowtie ace
224:51 - did have access to the subtle poet
224:53 - project then i could use this
224:55 - configuration but it doesn't and so i
224:58 - want to switch back to my other
225:00 - configuration and how i would do this is
225:03 - type in the command
225:04 - gcloud config configurations
225:08 - activate and the configuration that i
225:10 - set up for tony bowtie ace is the
225:13 - default configuration
225:15 - and so now that it has been activated i
225:18 - can now run a gcloud config list and as
225:22 - you can see here the configuration is
225:24 - back to default setup during the
225:26 - initialization process for tony bowtie
225:29 - ace now if i wanted to create multiple
225:32 - configurations for the same user account
225:35 - i can simply type in the command gcloud
225:38 - config configurations
225:41 - create
225:42 - but if i wanted to just view the
225:44 - configuration properties i can always
225:47 - type in the command gcloud config
225:49 - configurations describe
225:52 - and as you can see after the describe i
225:55 - needed the configuration name to
225:57 - complete the command and so i'm going to
225:59 - do that now
226:02 - and i've been given all the properties
226:04 - for this configuration now another thing
226:07 - that i wanted to share when it comes to
226:09 - properties is that you can change the
226:11 - project or the compute region and zone
226:14 - by simply typing in the command
226:17 - gcloud config set now if i wanted to
226:20 - change the project i can simply type in
226:22 - project and the project name if it was
226:25 - for the compute instance i can simply
226:28 - type in compute
226:29 - forward slash zone for the specific zone
226:32 - and just as a note only the properties
226:35 - that are not in the core property
226:37 - section are the ones that can be set as
226:40 - well when you are setting the properties
226:42 - this only applies to the active
226:45 - configuration if you want to change the
226:47 - configuration of one that is not active
226:50 - then you'd have to switch to it and run
226:53 - the gcloud config set command and so
226:55 - moving on i wanted to touch on
226:57 - components which are the installable
227:00 - parts of the sdk and when you install
227:02 - the sdk the components gcloud bq gsutil
227:08 - and the core libraries are installed by
227:11 - default now you probably saw a list of
227:13 - components when you ran the gcloud init
227:16 - command and so to see all the components
227:19 - again you can simply type in the gcloud
227:22 - components
227:23 - list command
227:25 - and if you scroll up you're able to see
227:28 - all the components that are available
227:30 - that you can install at your convenience
227:33 - and so if i wanted to install the
227:35 - cubectl component i can type in the
227:37 - command gcloud components install
227:41 - cubectl and a prompt will come up asking
227:44 - me if i want to continue with this i
227:46 - want to say yes and now it will go
227:48 - through the process of installing these
227:50 - components
227:52 - and so just to verify if i run the
227:54 - command gcloud components list you can
227:58 - see here that i have the cube ctl
228:01 - component installed now if i wanted to
228:03 - remove that component i can simply type
228:06 - in
228:06 - gcloud components
228:08 - remove
228:10 - and then the component that i want to
228:12 - remove
228:13 - which is cubectl i'm going to be
228:15 - prompted if i want to do this i'm going
228:17 - to say yes and it's going to go through
228:19 - the stages of removing this component
228:23 - and it's been successfully uninstalled
228:25 - and so if you're working with a resource
228:27 - that you need a component for you can
228:29 - simply install or uninstall it using the
228:33 - gcloud components command and so one
228:35 - last thing about components before we
228:37 - move on
228:38 - is that you can update your components
228:40 - to make sure you have the latest version
228:42 - and so in order to update all of your
228:44 - installed components you would simply
228:47 - run the command gcloud components update
228:50 - and so before i go ahead and finish off
228:52 - this demonstration i wanted to touch on
228:55 - the gcloud interactive shell the gcloud
228:57 - interactive shell provides a richer
229:00 - shell experience simplifying commands
229:03 - and documentation discovery with as you
229:06 - type autocompletion and help text
229:09 - snippets below it produces suggestions
229:12 - and autocompletion for gcloud bq gsutil
229:17 - and cubectl command line tools as well
229:20 - as any command that has a man page sub
229:24 - commands and flags can be completed
229:27 - along with online help as you type the
229:29 - command and because this is part of the
229:32 - beta component i need to install it and
229:35 - so i'm going to run the command gcloud
229:38 - components install beta and i want to
229:41 - hit yes to continue and this will go
229:43 - ahead and kick off the installation of
229:46 - the gcloud beta commands
229:48 - and so now that it's installed i'm going
229:50 - to simply clear the screen and so now in
229:53 - order to run the gcloud interactive
229:55 - shell i need to run the command gcloud
229:59 - beta
230:00 - interactive
230:01 - and so now for every command that i type
230:04 - i will get auto suggestions that will
230:06 - help me with my commands and so to see
230:09 - it in all of its glory i'm going to
230:11 - start typing
230:12 - and as you can see it's giving me the
230:14 - option between g cloud or gsutil and i
230:17 - can use the arrow to choose either one
230:20 - and below it it'll also show me the
230:22 - different flags that i can use for these
230:25 - specific commands and how to structure
230:27 - them and so for now i'm going to run
230:29 - gsutil version
230:31 - minus l and as you can see here it's
230:34 - giving me all the information about this
230:36 - command and what it can do and so i'm
230:38 - going to hit enter
230:40 - and as you can see my gsutil version is
230:43 - 4.52
230:45 - and along with the version number i'm
230:47 - also given all the specific information
230:50 - with regards to this gsutil version and
230:53 - this can be used with absolutely any
230:55 - command used on the google cloud
230:58 - platform and so i'm going to go ahead
231:00 - and do that again but running a
231:01 - different command so i'm just going to
231:03 - first clear the screen and i'm going to
231:05 - type gcloud
231:07 - compute
231:08 - instances and as you can see the snippet
231:11 - on the bottom of the screen is showing
231:14 - me not only the command and how it's
231:16 - structured but also the url for the
231:19 - documentation so continuing on gcloud
231:22 - compute instances i'm going to do a list
231:25 - and i'm going to filter it by using the
231:28 - flag dash dash filter and i'm going to
231:31 - filter the us east one a zone and i'm
231:34 - going to hit enter
231:37 - and as expected there are no instances
231:40 - in us east 1a and as you've just
231:43 - experienced this is a great tool and i
231:46 - highly recommend that you use it
231:47 - whenever you can now i know this is a
231:50 - lot to take in
231:51 - and a lot of these commands will not
231:53 - show up on the exam but again getting
231:56 - comfortable with the command line and
231:58 - the sdk will help you on your path to
232:01 - becoming a cloud engineer as well it
232:04 - will help you get really comfortable
232:06 - with the command line and before you
232:08 - know it you'll be running commands in
232:10 - the command line and prefer it over
232:13 - using the console and so that's all i
232:15 - have for this demo on managing the cloud
232:18 - sdk so you can now mark this lesson as
232:21 - complete and let's move on to the next
232:23 - one
232:28 - welcome back in this demonstration i'm
232:31 - going to be talking about the always
232:33 - available browser-based shell called
232:36 - cloud shell cloud shell is a virtual
232:38 - machine that is loaded with development
232:40 - tools and offers a persistent five
232:44 - gigabyte home directory that runs on
232:46 - google cloud cloud shell is what
232:48 - provides you command line access to your
232:52 - google cloud resources within the
232:54 - console cloud shell also comes with a
232:57 - built-in code editor that i will be
232:59 - diving into and allows you to browse
233:02 - file directories as well as view and
233:05 - edit files while still accessing the
233:07 - cloud shell the code editor is available
233:10 - by default with every cloud shell
233:13 - instance and is based on the open source
233:16 - editor thea
233:18 - now cloud shell is available from
233:20 - anywhere in the console by merely
233:22 - clicking on the icon showed here in the
233:25 - picture
233:26 - and is positioned in the top right hand
233:28 - corner of the console in the blue
233:30 - toolbar so let's get started with the
233:33 - cloud shell by getting our hands dirty
233:36 - and jumping right into it
233:38 - and so here we are back in the console
233:40 - and i am logged in as tony bowtie ace
233:43 - gmail.com and as you can see up here in
233:46 - the right hand corner
233:48 - as mentioned earlier you will find the
233:50 - cloud shell logo and so to open it up
233:53 - you simply click on it and it'll
233:55 - activate the cloud shell here at the
233:57 - bottom and because it's my first time
233:59 - using cloud shell i'll get this prompt
234:02 - quickly explaining an overview of what
234:04 - cloud shell is and i'm going to simply
234:06 - hit continue
234:09 - and i'm going to make the terminal a
234:10 - little bit bigger by dragging this line
234:13 - up to the middle of the screen and so
234:15 - when you start cloud shell it provisions
234:17 - an e2 small
234:19 - google compute engine instance running a
234:22 - debian-based linux operating system now
234:24 - this is an ephemeral pre-configured vm
234:27 - and the environment you work with is a
234:30 - docker container running on that vm
234:33 - cloud shell instances are provisioned on
234:35 - a per user per session basis the
234:38 - instance persists while your cloud shell
234:40 - session is active and after an hour of
234:43 - inactivity your session terminates and
234:46 - the vm is discarded you can also
234:48 - customize your environment automatically
234:51 - on boot time and it will allow you to
234:53 - have your preferred tools when cloud
234:56 - shell boots up so when your cloud shell
234:58 - instance is provision it's provisioned
235:01 - with 5 gigabytes of free persistent disk
235:04 - storage and it's mounted at your home
235:06 - directory on the virtual machine
235:09 - instance and you can check your disk
235:11 - storage by simply typing in the command
235:14 - df minus h and here where it shows dev
235:17 - disk by id google home part one it shows
235:21 - here the size as 4.8 gigabytes and this
235:25 - would be the persistent disk storage
235:27 - that's mounted on your home directory
235:30 - now if you've noticed it shows here that
235:32 - i'm logged in as tony bowtie ace at
235:35 - cloud shell and that my project id is
235:38 - set at project tony so the great thing
235:40 - about cloud shell
235:42 - is that you're automatically
235:43 - authenticated as the google account
235:46 - you're logged in with so here you can
235:48 - see i'm logged in as tony bowtie ace and
235:51 - so picture it like running gcloud auth
235:54 - login and specifying your google account
235:57 - but without having to actually do it now
235:59 - when the cloud shell is started the
236:01 - active project in the console is
236:04 - propagated to your gcloud configuration
236:07 - inside cloud shell so as you can see
236:09 - here my project is set at project tony
236:13 - now if i wanted to change it to a
236:14 - different project i could simply use the
236:16 - command stated up here
236:18 - gcloud config set project along with the
236:21 - project id and this will change me to a
236:24 - different project now behind the scenes
236:26 - cloud shell is globally distributed
236:29 - across multiple regions so when you
236:31 - first connect to cloud shell you'll be
236:34 - automatically assigned to the closest
236:36 - available region and thus avoiding
236:39 - any unnecessary latency you do not have
236:42 - the option to choose your own region and
236:44 - so cloud shell does that for you by
236:46 - optimizing it to migrate to a closer
236:49 - region whenever it can so if you're ever
236:51 - curious where your cloud shell session
236:54 - is currently active
236:55 - you can simply type in this command
236:58 - curl metadata slash compute metadata
237:02 - slash version one slash instance slash
237:05 - zone
237:06 - and this will give me the zone where my
237:08 - instance is located and as shown here it
237:11 - is in us east 1b now as you've probably
237:14 - been seeing every time i highlight
237:16 - something that there is a picture of
237:18 - scissors coming up the cloud shell has
237:20 - some automated and available tools that
237:23 - are built in and so one of those
237:25 - available tools is that whenever i
237:27 - highlight something it will
237:29 - automatically copy it to the clipboard
237:31 - for me cloud shell also has a bunch of
237:34 - very powerful pre-installed tools that
237:36 - come with it such as the cloud sdk bash
237:40 - vim helm
237:42 - git
237:43 - docker and more as well cloud shell has
237:46 - support for a lot of major different
237:48 - programming languages like java go
237:52 - python node.js
237:54 - ruby and net core for those who run
237:57 - windows now if you're looking for an
237:59 - available tool that is not pre-installed
238:02 - you can actually customize your
238:04 - environment when your instance boots up
238:07 - and automatically run a script that will
238:09 - install the tool of your choice and the
238:12 - script runs as root and you can install
238:14 - any package that you please and so in
238:17 - order for this environment customization
238:19 - to work there needs to be a file labeled
238:22 - as dot customize underscore environment
238:25 - now if we do an ls here you can see that
238:28 - all we have is the readme dash cloud
238:30 - shell text file if we do ls space minus
238:34 - al to show all the hidden files as well
238:37 - you can see that the dot customize
238:40 - underscore environment file does not
238:42 - exist and this is because we need to
238:44 - create it ourselves and so for this
238:47 - example i want terraform installed as an
238:50 - available tool when my instance boots up
238:53 - and so i have to create this file so i'm
238:56 - going to do so by using the touch
238:58 - command and then the name of the file
239:01 - dot customize
239:02 - underscore environment hit enter and if
239:06 - i clear the screen and do another ls
239:09 - space minus al i can see that my dot
239:12 - customize underscore environment file
239:15 - has been created and so now i'm going to
239:17 - need the script to install terraform
239:20 - which means i would have to edit it and
239:22 - so another great feature of cloud shell
239:25 - is that it comes with a code editor and
239:27 - i can do it one of two ways i can either
239:29 - come up here and click on the open
239:32 - editor button which will open up a new
239:34 - tab or i can simply use the edit command
239:37 - with the file name and i'm going to do
239:39 - just that so edit
239:43 - dot customize underscore environment and
239:46 - i'm just going to hit enter and as you
239:48 - can see i got a prompt saying that it's
239:51 - unable to load the code editor and this
239:53 - is because when using code editor you
239:56 - need cookies enabled on your browser and
239:59 - because i am using a private browser
240:01 - session cookies are disabled and because
240:04 - my cloud shell environment persists i'm
240:07 - going to open up a regular browser
240:08 - window and i'm going to continue where i
240:11 - left off and so here i am back with a
240:13 - new browser window again logged in as
240:16 - tony bowtie ace and so just to show you
240:18 - the persistence that happens in cloud
240:20 - shell i'm going to run the command ls
240:23 - space minus al and as you can see here
240:26 - the customize environment is still here
240:29 - and so again i wanted to install
240:31 - terraform as an extra tool to have in my
240:34 - environment and so i'm going to open up
240:36 - the editor by typing in edit dot
240:39 - customize underscore environment and i'm
240:42 - going to hit enter and here is the
240:44 - editor that popped up
240:46 - as you can see here it's built with
240:47 - eclipse thea and this is an open source
240:50 - code editor that you can download from
240:52 - eclipse and this is what the editor is
240:55 - built on now this menu here on the left
240:57 - i can make it a little bit bigger and
241:00 - because the only viewable file on my
241:02 - persistent disk is the readme cloud
241:05 - shell dot text file i'm not able to see
241:08 - my dot customize underscore environment
241:11 - so in order to open it and edit it i'm
241:13 - going to go to the menu at the top of
241:15 - the editor and click on file open
241:18 - and here i'll be able to select the file
241:20 - that i need so i'm going to select
241:22 - customize environment and click on open
241:24 - and so i'm going to paste in my script
241:27 - to install terraform and i'm just going
241:29 - to paste in my script from my clipboard
241:32 - and i'll be including the script in the
241:34 - github repo for those of you who use
241:36 - terraform and i'm going to move over to
241:38 - the menu on the left click on file and
241:41 - then hit save and so now in order for me
241:44 - to allow this to work
241:46 - the customize environment needs to be
241:48 - loaded into my cloud shell so i'm going
241:50 - to have to restart it and so in order to
241:53 - accomplish this i'm going to move over
241:55 - to the menu on the right i'm going to
241:57 - click on the icon with the three dots
242:00 - and click on restart and you'll be
242:02 - presented with a prompt it's saying that
242:04 - it will immediately terminate my session
242:06 - and then a new vm will be provisioned
242:08 - for me and you'll also be presented with
242:11 - an optional response from google telling
242:14 - them why you're restarting the vm and
242:16 - this is merely for statistical purposes
242:19 - so i'm going to click on restart and i'm
242:21 - going to wait till a new cloud shell is
242:23 - provisioned and my new cloud shell is
242:25 - provisioned and up and running and so i
242:27 - want to double check to see if terraform
242:29 - has been installed so i'm going to go
242:31 - over here to the open terminal button on
242:33 - the right hand side toolbar and i'm
242:35 - going to move back to my terminal and
242:37 - i'm going to simply run the command
242:39 - terraform dash dash version
242:42 - and so it looks like terraform has been
242:44 - installed and as you can see i'm running
242:46 - version.12 but it says my terraform
242:49 - version is out of date and that the
242:51 - latest version is dot 13. and so because
242:54 - i really want to be up to date with
242:56 - terraform i want to be able to go into
242:59 - my customize environment file and edit
243:01 - my version of terraform so that when my
243:04 - cloud shell is initiated
243:07 - terraform.13 can be installed and so i'm
243:09 - going to simply type in the command edit
243:12 - dot customize underscore environment and
243:15 - i'm back to my editor and i'm going to
243:17 - change the terraform version from dot 12
243:20 - to dot 13 and then go over here to the
243:23 - left-hand menu click on file and then
243:26 - save and now i'm going to restart my
243:29 - machine again
243:31 - and come back when it's fully
243:32 - provisioned and i'm back again my
243:35 - machine has been provisioned and i'm
243:37 - going to go back to my terminal by
243:39 - clicking on the open terminal button and
243:41 - so i'm going to type in the command
243:43 - terraform dash dash version and as you
243:46 - can see i'm at version dot 13 and i'm
243:49 - going to run a simple terraform command
243:51 - to see if it's working and as you can
243:54 - see i am successful in running terraform
243:56 - on cloud shell now customizing the
243:59 - environment is not on the exam but it is
244:02 - such an amazing feature that i wanted to
244:04 - highlight it for you with a real world
244:07 - example like terraform in case you're
244:09 - away from your computer
244:11 - and you're logged into a browser and you
244:13 - need some special tools to use in cloud
244:16 - shell this is the best way to do it now
244:19 - as i mentioned before the cloud sdk is
244:22 - pre-installed on this and so everything
244:25 - that i've showed you in the last lesson
244:27 - with regards to cloud sdk can be done in
244:30 - the cloud shell as well so if i run the
244:32 - command gcloud
244:34 - beta interactive i'd be able to bring up
244:38 - the interactive cloud shell and i'll be
244:40 - able to run the same commands so now if
244:42 - i go ahead and run the command
244:44 - gcloud components list i'll be able to
244:48 - see all the components installed and as
244:50 - you can see with the cloud shell there
244:52 - are more components installed than
244:54 - what's installed on the default
244:56 - installation of the sdk i can also run
245:00 - the gcloud config list command to see
245:03 - all the properties in my active
245:04 - configuration and so this goes to show
245:07 - you that the sdk installation that's on
245:10 - cloud shell is just as capable as the
245:13 - one that you've installed on your
245:15 - computer the only difference here is
245:17 - that the sdk along with all the other
245:20 - tools that come installed in cloud shell
245:22 - is updated every week and so you can
245:25 - always depend that they're up to date
245:27 - and so moving on to a few more features
245:30 - of cloud shell i wanted to point out the
245:32 - obvious ones
245:33 - up here in the cloud shell toolbar right
245:36 - beside the open terminal i can open
245:38 - brand new tabs opening up different
245:41 - projects
245:42 - or even the same project
245:44 - but just a different terminal and moving
245:46 - over to the right hand menu of cloud
245:48 - shell this keyboard icon can send key
245:52 - combinations that you would normally not
245:54 - have access to moving on to the gear
245:57 - icon with this you're able to change
245:59 - your preferences
246:01 - and looking at the first item on the
246:02 - list when it comes to color themes you
246:05 - can go from a dark theme to a light
246:07 - theme or if you prefer a different color
246:10 - in my case i prefer the dark theme
246:13 - as well you have the options of changing
246:16 - your text size we can go to largest
246:21 - but i think we'll just keep things back
246:23 - down to medium and as well
246:25 - we have
246:26 - the different fonts the copy settings
246:29 - from which i showed you earlier as well
246:31 - as keyboard preferences you also have
246:33 - the option of showing your scroll bar
246:35 - now moving on to this icon right beside
246:38 - the gear
246:39 - is the web preview button and so the web
246:41 - preview button is designed so that you
246:44 - can run any web application that listens
246:47 - to http requests on the cloud shell and
246:51 - be able to view it in a new web browser
246:53 - tab when running these web applications
246:56 - web preview also supports applications
246:59 - run in app engine now mind you these
247:01 - ports are only available to the secure
247:04 - cloud shell proxy service which
247:06 - restricts access over https to your user
247:10 - account only and so to demonstrate this
247:13 - feature i am going to run a simple http
247:17 - server running a hello world page so
247:20 - first i'm going to clear my screen and
247:22 - then i'm going to exit the interactive
247:24 - shell and again i'm going to paste in
247:26 - for my clipboard a simple script that
247:29 - will run my simple http server and as
247:32 - you can see it's running on port 8080
247:35 - and now i'm able to click on the web
247:37 - preview button and i'm able to preview
247:39 - it on port 8080
247:41 - and a new web browser tab will open up
247:44 - and here i'll see my hello world page
247:47 - now this is just a simple example and so
247:49 - i'm sure that many of you can find great
247:51 - use for this and so i'm going to stop
247:53 - this http server now by hitting ctrl c
247:57 - and just as a quick note web preview can
248:00 - also run on a different port anywhere
248:02 - from port 2000
248:04 - all the way up to 65 000. now moving on
248:07 - to the rest of the features
248:09 - hitting on the more button here with the
248:11 - three dots starting from the top we
248:14 - covered restart earlier when we had to
248:16 - restart our cloud shell you're able to
248:18 - both upload and download a file within
248:22 - cloud shell when the demands are needed
248:24 - as well if i have a misconfigured
248:26 - configuration i can boot into safe mode
248:29 - and fix the issue instead of having to
248:32 - start from scratch again moving on to
248:34 - boost cloud shell also known as boost
248:37 - mode is a feature that increases your
248:39 - cloud shell vm from the default e2 small
248:43 - to an e2 medium so in essence a memory
248:46 - bump from 2 gigabytes to 4 gigabytes and
248:50 - once it's activated all your sessions
248:52 - will be boosted for the next 24 hours
248:55 - and just as a quick note
248:57 - enabling boost mode restarts your cloud
249:00 - shell and immediately terminates your
249:02 - session but don't worry the data in your
249:04 - home directory will persist but any of
249:07 - the processes that you are running will
249:09 - be lost now when it comes to usage quota
249:12 - cloud shell has a 50 hour weekly usage
249:16 - limit so if you reach your usage limit
249:18 - you'll need to wait until your quota is
249:20 - reset before you can use cloud shell
249:23 - again so it's always good to keep your
249:25 - eyes on this in case you're a heavy user
249:28 - of cloud shell
249:30 - and moving back to the menu again you
249:32 - have your usage statistics which
249:35 - collects statistics on commands that
249:37 - come pre-installed in the vm and you can
249:39 - turn them on or off
249:41 - and as well help for cloud shell is
249:43 - available here as well if you wanted to
249:46 - give feedback to the google cloud team
249:49 - with regards to cloud shell this is the
249:51 - place to do it and so one last thing
249:53 - about cloud shell before we end this
249:55 - demo is that if you do not access cloud
249:58 - shell for 120 days your home disk will
250:02 - be deleted now don't worry you'll
250:04 - receive an email notification before its
250:07 - deletion and if you just log in and
250:10 - start up a session you'll prevent it
250:12 - being removed now moving ahead in this
250:14 - course i will be using cloud shell quite
250:17 - a bit and so feel free to use either
250:20 - cloud shell or the cloud sdk installed
250:23 - on your computer or feel free to follow
250:25 - along with me in the cloud shell within
250:28 - your google cloud environment and so if
250:30 - you are following along please make sure
250:32 - that you keep an eye on your quota and
250:35 - so i hope this demonstration has given
250:37 - you some really good insight as to what
250:40 - you can do with cloud shell and its
250:42 - limitations and so that's pretty much
250:44 - all i wanted to cover in this
250:46 - demonstration of cloud shell so you can
250:48 - now mark this as complete and let's move
250:51 - on to the next one
250:52 - [Music]
251:00 - welcome back in this lesson and
251:02 - demonstration i am going to go over
251:05 - limits and quotas and how they affect
251:07 - your cloud usage within google cloud i'm
251:11 - going to quickly go over some theory
251:13 - followed by a demonstration on where to
251:16 - find the quotas and how to edit them
251:18 - accordingly so google cloud enforces
251:21 - quotas on resource usage for project
251:24 - owners
251:25 - setting a hard limit on how much of a
251:28 - particular google cloud resource your
251:31 - project can use and so there are two
251:34 - types of resource usage that google
251:37 - limits with quota the first one is rate
251:40 - quota such as api requests per day this
251:44 - quota resets after a specified time such
251:47 - as a minute or a day the second one is
251:51 - allocation quota
251:53 - an example is the number of virtual
251:55 - machines or load balancers used by your
251:57 - project
251:59 - and this quota does not reset over time
252:02 - but must be explicitly released when you
252:05 - no longer want to use the resource
252:08 - for example by deleting a gke cluster
252:12 - now quotas are enforced for a variety of
252:15 - reasons
252:16 - for example they protect other google
252:19 - cloud users by preventing unforeseen
252:22 - usage spikes
252:24 - quotas also help with resource
252:26 - management so you can set your own
252:28 - limits on service usage within your
252:31 - quota while developing and testing your
252:33 - applications
252:35 - each quota limit is expressed in terms
252:38 - of a particular countable resource from
252:41 - requests per day to an api to the number
252:45 - of load balancers used by your
252:47 - application not all projects have the
252:50 - same quotas for the same services and so
252:53 - using this free trial account you may
252:55 - have very limited quota compared to a
252:59 - higher quota on a regular account as
253:01 - well with your use of google cloud over
253:04 - time your quotas may increase
253:06 - accordingly and so you can also request
253:09 - more quota if you need it and set up
253:12 - monitoring and alerts
253:14 - and cloud monitoring to warn you about
253:16 - unusual quota usage behavior or when
253:19 - you're actually running out of quota now
253:22 - in addition to viewing basic quota
253:24 - information in the console
253:26 - google cloud lets you monitor quota
253:28 - usage
253:30 - limits and errors in greater depth using
253:33 - the cloud monitoring api and ui along
253:37 - with quota metrics appearing in the
253:40 - metrics explorer you can then use these
253:42 - metrics to create custom dashboards and
253:45 - alerts
253:46 - letting you monitor quota usage over
253:49 - time
253:50 - and receive alerts when for example
253:53 - you're near a quota limit only your
253:56 - services that support quota metrics are
253:58 - displayed and so popular supported
254:01 - services include compute engine
254:04 - data flow
254:05 - cloud spanner cloud monitoring and cloud
254:08 - logging common services that are not
254:10 - supported include app engine cloud
254:13 - storage and cloud sql now as a note be
254:17 - aware that quota limits are updated once
254:19 - a day
254:20 - and hence new limits may take up to 24
254:23 - hours to be reflected in the google
254:26 - cloud console if your project exceeds a
254:29 - particular quota while using a service
254:32 - the platform will return an error
254:35 - in general
254:36 - google cloud will return an http
254:40 - 429 error code if you're using http or
254:45 - rest to access the service
254:48 - or resource exhausted if you're using
254:51 - grpc if you're using cloud monitoring
254:54 - you can use it to identify the quota
254:56 - associated with the error
254:59 - and then create custom alerts upon
255:01 - getting a quota error and we will be
255:04 - going into greater depth with regards to
255:07 - monitoring later on in the course now
255:10 - there are two ways to view your current
255:12 - quota limits in the google cloud console
255:15 - the first is using the quotas page which
255:18 - gives you a list of all of your
255:20 - project's quota usage and limits the
255:23 - second is using the api dashboard which
255:27 - gives you the quota information for a
255:29 - particular api
255:31 - including resource usage over time quota
255:35 - limits are also accessible
255:37 - programmatically through the service
255:39 - usage api and so let's head into the
255:43 - console where i will provide a
255:45 - demonstration
255:46 - on where to look for quotas
255:48 - and how to increase them when you need
255:50 - to
255:51 - and so here we are back in the console
255:54 - and so as i explained before there are
255:56 - two main ways to view your current quota
255:59 - limits
256:00 - in the console and so the first one is
256:02 - using the quotas page and so in order to
256:05 - get to the quotas page i need to go to
256:07 - iam so i'm going to do that now by going
256:10 - up to the navigation menu in the top
256:12 - left hand corner
256:14 - i'm going to go to i am and admin and
256:16 - over to quotas
256:19 - and so here i am shown all the quotas of
256:22 - the current apis that i have enabled as
256:25 - you can see here it shows me the service
256:28 - the limit name
256:30 - the quota status and the details in this
256:33 - panel here on the right hand side shows
256:36 - me a little bit more information with
256:38 - regards to the service and the quota
256:40 - itself and so let's say i wanted to
256:43 - increase my quota on the compute engine
256:46 - api
256:47 - within networks so i'm going to select
256:50 - this service and over here on the right
256:52 - hand panel i'm going to tick the box
256:55 - that says global and i'm going to go
256:57 - back over here to the top left
257:00 - and click on the edit quotas button and
257:02 - a panel will pop up and i am prompted to
257:05 - enter a new quota limit
257:07 - along with a description explaining to
257:10 - google why i need this quota limit
257:12 - increase and so once i've completed my
257:15 - request
257:16 - i can click on done and then submit
257:18 - request and like i said before once the
257:21 - request has been submitted it will go to
257:24 - somebody at google to evaluate the
257:26 - requests for approval and don't worry
257:29 - these quota limit increases are usually
257:32 - approved within two business days and
257:35 - can often times be sooner than that also
257:38 - a great way to enter multiple quota
257:40 - changes is to click on the selected apis
257:44 - let's do bigquery api
257:46 - and cloud data store api and so i've
257:49 - clicked off three and now i can go back
257:51 - up to the top and click on the edit
257:54 - quotas button and as you can see in the
257:56 - panel i have all three apis that i want
258:00 - to increase my quotas on so i can enter
258:03 - all my new limit requests for each api
258:06 - and then i can submit it as a bulk
258:09 - request with all my new quota limit
258:11 - changes and so doing it this way would
258:14 - increase the efficiency
258:16 - instead of increasing the quotas for
258:18 - each service one by one and because i'm
258:21 - not going to submit any quota changes
258:24 - i'm going to close this panel and so
258:26 - again using the quotas page will give
258:29 - you a list of all your project quota
258:31 - usage and its limits and allow you to
258:34 - request changes accordingly and so now
258:37 - moving on to the second way which you
258:39 - can view your current quota limits i'm
258:41 - going to go to the api dashboard which
258:44 - will give me a more granular view
258:46 - including the resource usage over time
258:49 - so to get there i'm going to go back up
258:51 - to the left hand side to the navigation
258:54 - menu i'm going to go to apis and
258:56 - services and click on dashboard
259:00 - and here i will see all the names of the
259:02 - apis and i'm going to click on compute
259:05 - engine api for this demonstration
259:08 - and over here on the left hand menu you
259:11 - will see quotas
259:12 - and in here as i said before
259:15 - you can get some really granular data
259:18 - with regards to queries read requests
259:21 - list requests and a whole bunch of other
259:23 - requests i'm going to drill down into
259:26 - queries here and i can see my queries
259:28 - per day per 100 seconds per user and per
259:32 - 100 seconds and i can see here that my
259:35 - queries per 100 seconds is at a limit of
259:38 - 2 000 so if i wanted to increase that
259:41 - limit i can simply click on the pencil
259:43 - icon
259:44 - and a panel on the right hand side will
259:47 - prompt me to enter a new quota limit but
259:50 - i currently see that my quota limit is
259:52 - at its maximum and that i need to apply
259:55 - for a higher quota so when i click on
259:58 - the link it will bring me back to my iam
260:01 - page where my services are filtered and
260:04 - i can easily find the service that i was
260:07 - looking at to raise my quota limit and i
260:10 - can increase the quota by checking off
260:12 - this box and clicking on the edit quotas
260:15 - button at the top of the page and so as
260:18 - you can see the quotas page as well as
260:21 - the api dashboard work in tandem so that
260:24 - you can get all the information you need
260:27 - with regards to quotas and limits and to
260:30 - edit them accordingly and so i hope this
260:33 - gave you a good idea and some great
260:36 - insight
260:37 - on how you can view and edit your quotas
260:40 - and quota limits according to the
260:42 - resources you use and so that about
260:45 - wraps up this brief yet important demo
260:48 - on limits and quotas so you can now mark
260:51 - this as complete and let's move on to
260:53 - the next section
260:54 - [Music]
260:58 - welcome back
261:00 - and in this section we're going to be
261:02 - going through in my opinion one of the
261:04 - most important services in google cloud
261:08 - identity and access management also
261:11 - known as iam for short and i'll be
261:14 - diving into identities roles and the
261:17 - architecture of policies that will give
261:19 - you a very good understanding of how
261:22 - permissions are granted and how policies
261:25 - are inherited so before i jump into i am
261:28 - i wanted to touch on the principle of
261:30 - least privilege just for a second now
261:33 - the principle of least privilege states
261:35 - that a user program or process
261:38 - should have access to the bare minimum
261:41 - privileges necessary
261:42 - or the exact resources it needs in order
261:46 - to perform its function so for example
261:49 - if lisa is performing a create function
261:52 - to a cloud storage bucket
261:54 - lisa should be restricted to create
261:56 - permissions only on exactly one cloud
261:59 - storage bucket
262:01 - she doesn't need read edit or even
262:04 - delete permissions on a cloud storage
262:06 - bucket to perform her job and so this is
262:09 - a great illustration of how this
262:11 - principle works
262:13 - and this is something that happens in
262:15 - not only google cloud but in every cloud
262:18 - environment as well as any on-premises
262:21 - environment so note that the principle
262:24 - of least privilege is something that i
262:26 - have previously and will continue to be
262:29 - talking about a lot in this course and
262:32 - this is a key term that comes up quite a
262:35 - bit
262:35 - in any major exam
262:37 - and is a rule that most apply in their
262:40 - working environment to avoid any
262:42 - unnecessary granted permissions a
262:45 - well-known and unsaid rule when it comes
262:48 - to security hence me wanting to touch on
262:51 - this for a brief moment so now with that
262:53 - out of the way i'd like to move on to
262:55 - identity and access management or i am
262:59 - for short so what is it really well with
263:02 - iam you manage access control by
263:05 - defining who the identity
263:08 - has what access which is the role for
263:12 - which resource and this also includes
263:14 - organizations
263:16 - folders and projects
263:18 - in iam permission to access a resource
263:21 - isn't granted directly to the end user
263:24 - instead permissions are grouped into
263:27 - roles and roles are then granted to
263:30 - authenticated members an iam policy
263:33 - defines and enforces what roles are
263:36 - granted to which members
263:38 - and this policy is attached to a
263:40 - resource so when an authenticated member
263:43 - attempts to access a resource iam checks
263:46 - the resources policy to determine
263:49 - whether the action is permitted and so
263:52 - with that being said i want to dive into
263:54 - the policy architecture breaking it down
263:57 - by means of components in this policy
263:59 - architecture will give you a better
264:01 - understanding of how policies are put
264:04 - together so now what is a policy a
264:07 - policy is a collection of bindings audit
264:10 - configuration and metadata now the
264:12 - binding specifies how access should be
264:15 - granted on resources
264:17 - and it binds one or more members with a
264:20 - single role and any contact specific
264:23 - conditions that change how and when the
264:26 - role is granted now the metadata
264:29 - includes additional information about
264:31 - the policy such as an etag and version
264:34 - to facilitate policy management and
264:37 - finally the audit config field specifies
264:40 - the configuration data of how access
264:43 - attempts should be audited and so now i
264:46 - wanted to take a moment to dive deeper
264:48 - into each component starting with member
264:52 - now when it comes to members this is an
264:54 - identity that can access a resource
264:57 - so the identity of a member is an email
265:00 - address associated with a user
265:03 - service account or google group or even
265:06 - a domain name associated with a g suite
265:09 - or cloud identity domains now when it
265:12 - comes to a google account this
265:14 - represents any person who interacts with
265:17 - google cloud any email address that is
265:20 - associated with a google account can be
265:23 - an identity including gmail.com or other
265:26 - domains now a service account is an
265:29 - account that belongs to your application
265:32 - instead of an individual end user
265:35 - so when you run your code that is hosted
265:37 - on gcp
265:38 - this is the identity you would specify
265:41 - to run your code a google group is a
265:44 - named collection of google accounts and
265:47 - can also include service accounts now
265:50 - the advantages of using google groups is
265:52 - that you can grant and change
265:54 - permissions
265:55 - for the collection of accounts all at
265:57 - once instead of changing access one by
266:00 - one google groups can help you manage
266:03 - users at scale and each member of a
266:05 - google group inherits the iam roles
266:08 - granted to that group the inheritance
266:11 - means that you can use a group's
266:13 - membership to manage users roles instead
266:16 - of granting iam roles to individual
266:19 - users moving on to g suite domains this
266:22 - represents your organization's internet
266:24 - domain name such as
266:26 - antonyt.com and when you add a user to
266:29 - your g suite domain a new google account
266:32 - is created for the user inside this
266:34 - virtual group such as antony
266:37 - antonyt.com a g suite domain in
266:40 - actuality represents a virtual group of
266:43 - all of the google accounts that have
266:45 - been created like google groups g suite
266:48 - domains cannot be used to establish
266:50 - identity but they simply enable
266:53 - permission management now a cloud
266:55 - identity domain is like a g suite domain
266:58 - but the difference is that domain users
267:00 - don't have access to g suite
267:03 - applications and features so a couple
267:05 - more members that i wanted to address
267:08 - is the all authenticated users and the
267:10 - all users members the all authenticated
267:13 - users is a special identifier that
267:15 - represents anyone who is authenticated
267:18 - with a google account or a service
267:20 - account users who are not authenticated
267:23 - such as anonymous visitors are not
267:25 - included and finally the all users
267:28 - member is a special identifier that
267:30 - represents anyone and everyone so any
267:34 - user who is on the internet including
267:36 - authenticated and unauthenticated users
267:40 - and this covers the slew of the
267:42 - different types of members now touching
267:44 - on the next component of policies is
267:47 - roles now diving into roles this is a
267:50 - named collection of permissions that
267:53 - grant access to perform actions on
267:56 - google cloud resources
267:58 - so at the heart of it permissions are
268:00 - what determines what operations are
268:03 - allowed on a resource they usually but
268:06 - not always correspond one-to-one with
268:09 - rest methods that is each google cloud
268:12 - service has an associated permission for
268:15 - each rest method that it has so to call
268:18 - a method the caller needs that
268:20 - permission now these permissions are not
268:22 - granted to the users directly but
268:25 - grouped together within the role you
268:27 - would then grant roles which contain one
268:30 - or more permissions
268:32 - you can also create a custom role by
268:34 - combining one or more of the available
268:37 - iam permissions and again permissions
268:40 - allow users to perform specific actions
268:44 - on google cloud resources so you will
268:46 - typically see a permission such as the
268:48 - one you see here
268:50 - compute.instances.list
268:53 - and within google cloud iam permissions
268:56 - are represented in this form
268:59 - service.resource.verb
269:01 - so just as a recap on roles this is a
269:04 - collection of permissions
269:06 - and you cannot grant a permission
269:08 - directly to the user but you grant a
269:11 - role to a user and all the permissions
269:14 - that the role contains so an example is
269:17 - shown here where the compute instances
269:20 - permissions are grouped together in a
269:22 - role now you can grant permissions by
269:24 - granting roles to a user a group or a
269:28 - service account so moving up into a more
269:31 - broader level there are three types of
269:33 - roles in iam
269:35 - there are the primitive roles the
269:37 - predefined roles
269:39 - and the custom roles
269:41 - with the primitive roles
269:43 - these are roles that existed prior to
269:46 - the introduction of iam
269:48 - and they consist of three specific roles
269:51 - owner editor and viewer and these roles
269:54 - are concentric which means that the
269:57 - owner role includes the permissions in
269:59 - the editor role and the editor role
270:01 - includes the permissions in the viewer
270:03 - role and you can apply primitive roles
270:06 - at the project or service resource
270:08 - levels by using the console the api and
270:12 - the gcloud tool just as a note you
270:15 - cannot grant the owner role to a member
270:18 - for a project using the iam api or the
270:22 - gcloud command line tool you can only
270:24 - add owners to a project using the cloud
270:27 - console as well
270:29 - google recommends avoiding these roles
270:31 - if possible due to the nature of how
270:34 - much access the permissions are given in
270:37 - these specific roles google recommends
270:40 - that you use pre-defined roles over
270:42 - primitive roles and so moving into
270:44 - predefined roles
270:46 - these are roles that give granular and
270:49 - finer-grained access control than the
270:51 - primitive roles to specific google cloud
270:54 - resources and prevent any unwanted
270:57 - access to other resources predefined
271:00 - roles are created and maintained by
271:03 - google their permissions are
271:05 - automatically updated as necessary when
271:08 - new features or services are added to
271:10 - google cloud now when it comes to custom
271:13 - roles these are user defined and allow
271:16 - you to bundle one or more supported
271:18 - permissions to meet your specific needs
271:22 - unlike predefined roles custom roles are
271:24 - not maintained by google so when new
271:27 - permissions features or services are
271:29 - added to google cloud your custom roles
271:32 - will not be updated automatically when
271:35 - you create a custom role you must choose
271:37 - an organization or project to create it
271:40 - in you can then grant the custom role on
271:43 - the organization or project as well as
271:46 - any resources within that organization
271:48 - or project and just as a note you cannot
271:51 - create custom roles at the folder level
271:54 - if you need to use a custom role within
271:56 - a folder define the custom role on the
271:59 - parent of that folder as well the custom
272:02 - roles user interface is only available
272:05 - to users who have permissions to create
272:08 - or manage custom roles by default only
272:12 - project owners can create new roles now
272:15 - there is one limitation that i wanted to
272:17 - point out
272:18 - and that is that some predefined roles
272:21 - contain permissions that are not
272:23 - permitted in custom roles so i highly
272:26 - recommend that you check whether you can
272:28 - use a specific permission when making a
272:30 - custom role custom roles also have a
272:34 - really cool feature that includes a
272:36 - launch stage which is stored in the
272:38 - stage property for the role the stage is
272:42 - informational and helps you keep track
272:44 - of how close each role is to being
272:47 - generally available and these launch
272:49 - stages are available in the stages shown
272:52 - here
272:53 - alpha which is in testing beta which is
272:56 - tested and awaiting approval and of
272:58 - course ga which is generally available
273:02 - and i'll be getting hands-on later with
273:04 - these roles in an upcoming demonstration
273:07 - so now moving on to the next component
273:09 - is conditions and so a condition is a
273:13 - logic expression and is used to define
273:16 - and enforce
273:17 - conditional
273:18 - attribute-based access control for
273:21 - google cloud resources conditions allow
273:23 - you to choose
273:25 - granting resource access to identities
273:28 - also known as members only if configured
273:31 - conditions are met for example this
273:34 - could be done to configure temporary
273:36 - access for users that are contractors
273:39 - and have been given specific access for
273:42 - a certain amount of time a condition
273:44 - could be put in place to remove the
273:47 - access they needed once the contract has
273:50 - ended conditions are specified in the
273:52 - role bindings of a resources im policy
273:56 - so when a condition exists the access
273:59 - request is only granted if the condition
274:02 - expression is true so now moving on to
274:04 - metadata this component carries both e
274:07 - tags and version so first touching on e
274:11 - when multiple systems try to write to
274:14 - the same im policy at the same time
274:17 - there is a risk that those systems might
274:20 - overwrite each other's changes and the
274:22 - risk exists because updating an im
274:25 - policy involves multiple operations so
274:29 - in order to help prevent this issue iam
274:32 - supports concurrency control through the
274:34 - use of an etag field in the policy the
274:38 - value of this field changes each time a
274:41 - policy is updated now when it comes to a
274:43 - version
274:44 - this is a version number that is added
274:46 - to determine features such as a
274:49 - condition
274:50 - and for future releases of new features
274:53 - it is also used to avoid breaking your
274:56 - existing integrations on new feature
274:58 - releases that rely on consistency in the
275:01 - policy structure
275:03 - also when new policy schema versions are
275:05 - introduced and lastly we have the
275:08 - auditconfig component and this is used
275:11 - in order to configure audit logging for
275:13 - the policy it determines which
275:15 - permission types are logged and what
275:18 - identities if any are exempted from
275:20 - logging and so to sum it up this is a
275:23 - policy in all its entirety each
275:26 - component as you can see plays a
275:28 - different part and i will be going
275:30 - through policies and how they are
275:32 - assembled in statements in a later
275:34 - lesson and so there is one more thing
275:36 - that i wanted to touch on before ending
275:39 - this lesson and that is the policy
275:41 - inheritance when it comes to resource
275:44 - hierarchy and so as explained in an
275:46 - earlier lesson you can set an im policy
275:49 - at any level in the resource hierarchy
275:52 - the organization level the folder level
275:55 - the project level or the resource level
275:57 - and resources inherit the policies of
276:01 - all their parent resources the effective
276:03 - policy for a resource
276:06 - is the union of the policy set on that
276:09 - resource and the policies inherited from
276:12 - higher up in the hierarchy and so again
276:14 - i wanted to reiterate that this policy
276:17 - inheritance is transitive in other words
276:20 - resources inherit policies from the
276:23 - project
276:24 - which inherit policies from folders
276:26 - which inherit policies from the
276:28 - organization therefore the organization
276:31 - level policies
276:33 - also apply at the resource level and so
276:36 - just a quick example if i apply a policy
276:39 - on project x
276:41 - on any resources within that project the
276:43 - effective policy is going to be a union
276:46 - of these policies as the resources will
276:49 - inherit the policy that is granted to
276:51 - project x so i hope this gave you a
276:54 - better understanding of how policies are
276:57 - granted as well as the course structure
277:00 - and so that's all i have for this lesson
277:02 - so you can now mark this lesson as
277:04 - complete and let's move on to the next
277:06 - one
277:07 - [Music]
277:11 - welcome back and in this lesson i wanted
277:13 - to build on the last lesson where we
277:16 - went through iam and policy architecture
277:19 - and dive deeper into policies and
277:21 - conditions when it comes to putting them
277:24 - together in policy statements as cloud
277:26 - engineers you should be able to read and
277:29 - decipher policy statements and
277:31 - understand how they're put together by
277:34 - using all the components that we
277:36 - discussed earlier so just as a refresher
277:39 - i wanted to go over the policy
277:40 - architecture again now as i discussed
277:43 - previously a policy is a collection of
277:45 - statements that define who has what type
277:48 - of access it is attached to a resource
277:51 - and is used to enforce access control
277:54 - whenever that resource is accessed now
277:57 - the binding within that policy binds one
278:00 - or more members with a single role and
278:03 - any context specific conditions so in
278:06 - other words the member roles and
278:08 - conditions are bound together using a
278:11 - binding combined with the metadata and
278:13 - audit config we have a policy so now
278:17 - taking all of this and putting it
278:18 - together in a policy statement shown
278:21 - here you can see the bindings which have
278:24 - the role the members and conditions the
278:28 - first member being tony beauties
278:30 - gmail.com holding the role of storage
278:33 - admin and the second member as
278:36 - larkfetterlogin at gmail.com
278:38 - holding the role of storage object
278:41 - viewer now because lark only needs to
278:43 - view the files for this project in cloud
278:46 - storage till the new year a condition
278:48 - has been applied that does not grant
278:51 - access for lark to view these files
278:53 - after january the 1st an e tag has been
278:56 - put in and the version is numbered 3 due
278:59 - to the condition which i will get into a
279:02 - little bit later this policy statement
279:05 - has been structured in json format and
279:08 - is a common format used in policy
279:10 - statements moving on we have the exact
279:13 - same policy statement but has been
279:15 - formatted in yaml as you can see the
279:18 - members roles and conditions in the
279:20 - bindings are exactly the same as well as
279:23 - the etag and version but due to the
279:26 - formatting it is much more condensed so
279:29 - as you can see policy statements can be
279:31 - written in both json or yaml depending
279:34 - on your preference my personal
279:36 - preference is to write my policy
279:39 - statements in yaml due to the shorter
279:41 - and cleaner format so i will be moving
279:44 - ahead in this course with more
279:46 - statements written in yaml when you are
279:48 - looking to query your projects for its
279:51 - granted policies
279:52 - an easy way to do this would be to query
279:55 - it from the command line as shown here
279:58 - here i've taken a screenshot from tony
280:00 - bowtie ace in the cloud shell and have
280:03 - used the command gcloud projects get
280:07 - dash iam
280:08 - policy with the project id and this
280:12 - brought up all the members and roles
280:14 - within the bindings
280:15 - as well as the etag and version for the
280:18 - policy that has been attached to this
280:21 - project and as you can see here i have
280:23 - no conditions in place for any of my
280:26 - bindings and so again using the command
280:29 - gcloud projects
280:31 - get dash iam dash policy along with the
280:35 - project id will bring up any policies
280:37 - that are attached to this resource and
280:40 - the resource being the project id if the
280:42 - resource were to be the folder id then
280:44 - you could use the command gcloud
280:47 - resource dash manager
280:49 - folders get dash iam-policy
280:53 - with the folder id and for organizations
280:56 - the command would be gcloud
280:58 - organizations get dash
281:00 - iam-policy along with the organization
281:03 - id now because we don't have any folders
281:06 - or organizations in our environment
281:09 - typing these commands in wouldn't bring
281:11 - up anything and just as a note using
281:13 - these commands in the cloud shell or in
281:15 - the sdk will bring up the policy
281:18 - statement formatted in yaml so now i
281:21 - wanted to just take a second to dive
281:23 - into policy versions now as i haven't
281:26 - covered versions in detail i wanted to
281:28 - quickly go over it and the reasons for
281:31 - each numbered version now version one of
281:34 - the i am syntax schema for policies
281:37 - supports binding one role to one or more
281:41 - members
281:42 - it does not support conditional role
281:44 - bindings and so usually with version 1
281:47 - you will not see any conditions version
281:50 - 2 is used for google's internal use and
281:53 - so querying policies
281:55 - usually you will not see a version 2.
281:58 - and finally with version 3 this
282:00 - introduces the condition field in the
282:03 - role binding which constrains the role
282:06 - binding via contact space and attributes
282:09 - based rules so just as a note if your
282:12 - request does not specify a policy
282:14 - version
282:15 - iam will assume that you want a version
282:18 - 1 policy and again if the policy does
282:21 - not contain any conditions
282:23 - then iam always returns a version one
282:26 - policy regardless of the version number
282:29 - in the request so moving on to some
282:32 - policy limitations each resource can
282:34 - only have one policy and this includes
282:38 - organizations folders and projects
282:41 - another limitation
282:42 - is that each iam policy can contain up
282:46 - to 1500 members
282:48 - and up to 250 of these members
282:51 - can be google groups now when making
282:54 - policy changes it will take up to seven
282:57 - minutes to fully propagate across the
282:59 - google cloud platform this does not
283:02 - happen instantaneously as iam is global
283:06 - as well there is a limit of 100
283:08 - conditional role bindings per policy now
283:11 - getting a little bit deeper into
283:13 - conditions
283:14 - these are attributes that are either
283:16 - based on resource or based on details
283:19 - about the request and this could vary
283:21 - from time stamp to originating or
283:24 - destination ip address now as you
283:27 - probably heard me use the term earlier
283:29 - conditional role bindings are another
283:32 - name for a policy that holds a condition
283:35 - within the binding conditional role
283:37 - bindings can be added to new or existing
283:40 - iam policies
283:42 - to further control access to google
283:44 - cloud resources so when it comes to
283:46 - resource attributes this would enable
283:49 - you to create conditions that evaluate
283:52 - the resource in the access request
283:55 - including the resource type the resource
283:57 - name and the google cloud service being
284:00 - used request attributes allow you to
284:03 - manage access based on days or hours of
284:06 - the week a conditional role binding can
284:09 - be used to grant time bounded access to
284:12 - a resource ensuring that a user can no
284:15 - longer access that resource after the
284:18 - specified expiry date and time and this
284:21 - sets temporary access to google cloud
284:24 - resources using conditional role
284:26 - bindings in iam policies by using the
284:29 - date time attributes shown here you can
284:32 - enforce time-based controls when
284:35 - accessing a given resource now showing
284:38 - another example of a time-based
284:40 - condition it is possible to get even
284:43 - more granular and scope the geographic
284:45 - region
284:46 - along with the day and time for access
284:49 - in this policy lark only has access
284:52 - during business hours to view any
284:55 - objects within cloud storage lark can
284:57 - only access these objects from monday to
285:00 - friday nine to five this policy can also
285:03 - be used as a great example for
285:06 - contractors coming into your business
285:09 - yet only needing access during business
285:11 - hours now an example of a resource-based
285:14 - condition shown here a group member has
285:18 - a condition tied to it where dev only
285:21 - access has been implemented any
285:23 - developers that are part of this group
285:25 - will only have access to vm resources
285:29 - within project cat bowties and tied to
285:32 - any resources that's name starts with
285:35 - the word development now some
285:36 - limitations when it comes to conditions
285:39 - is that conditions are limited to
285:41 - specific services
285:43 - primitive roles are unsupported and
285:46 - members cannot be of the all users or
285:49 - all authenticated users members
285:52 - conditions also hold a limit of 100
285:55 - conditional role bindings per policy as
285:58 - well as 20 role bindings for the same
286:01 - role and same member and so for the last
286:04 - part of the policy statements i wanted
286:07 - to touch on audit config logs and this
286:10 - specifies the audit configuration for a
286:12 - service the configuration determines
286:15 - which permission types are logged and
286:18 - what identities if any are exempted from
286:21 - logging and when specifying audit
286:23 - configs they must have one or more audit
286:26 - log configs now as shown here
286:29 - this policy enables data read data write
286:33 - and admin read logging on all services
286:37 - while exempting tony bowtie ace
286:40 - gmail.com
286:42 - from admin read logging on cloud storage
286:45 - and so that's pretty much all i wanted
286:47 - to cover in this lesson
286:49 - on policies policy statements
286:52 - and conditions and so i highly recommend
286:55 - as you come across more policy
286:57 - statements take the time to read through
287:00 - it and get to know exactly
287:02 - what the statement is referring to and
287:04 - what type of permissions that are given
287:07 - and this will help you not only in the
287:09 - exam but will also help you in reading
287:12 - and writing policy statements in future
287:15 - and so that's all i have for this lesson
287:17 - so you can now mark this lesson as
287:19 - complete and let's move on to the next
287:21 - one
287:22 - [Music]
287:26 - welcome back
287:27 - and in this demonstration i'm going to
287:30 - do a hands-on tour working with iam here
287:34 - in the google cloud console we're going
287:36 - to go through the available services in
287:38 - the iam console as well as touching on
287:41 - the command line in the cloud shell to
287:44 - show how policies can be both added and
287:46 - edited we're also going to be bringing
287:48 - in another new user to really bring this
287:51 - demo to life and to show you how to edit
287:54 - existing policies so with that being
287:57 - said let's dive in so if i go over here
288:00 - to my user icon in the top right hand
288:03 - corner i can see that i am logged in as
288:05 - tony bowtie ace
288:07 - gmail.com and as you can see at the top
288:10 - i'm here in project tony so now to get
288:13 - to iam i'm going to go over to the
288:15 - navigation menu
288:17 - and i'm going to go to i am in admin and
288:19 - over to iam now moving over here to the
288:22 - menu on the left i wanted to go through
288:24 - the different options that we have in
288:27 - iam so under iam itself this is where
288:30 - you would add or edit permissions
288:33 - with regards to members and roles for
288:36 - the policy added to your given project
288:39 - which in my case is project tony and
288:42 - i'll be coming back in just a bit to go
288:44 - greater in depth with regards to adding
288:47 - and editing the policy permissions
288:49 - moving on to identity and organization
288:52 - now although we haven't touched on cloud
288:54 - identity yet i will be covering this in
288:56 - high level detail in a different lesson
288:59 - but for now know that cloud identity is
289:02 - google cloud's identity as a service
289:04 - solution and it allows you to create and
289:07 - manage users and groups within google
289:09 - cloud now if i was signed into cloud
289:12 - identity i would have a whole bunch of
289:14 - options here but since this is a
289:16 - personal account i cannot create or
289:18 - manage any users as well i do not have a
289:21 - domain tied to any cloud identity
289:24 - account as well as any g suite account
289:27 - so just know that if you had cloud
289:29 - identity or g suite set up you would
289:32 - have a bunch of different options to
289:34 - choose from in order to help you manage
289:36 - your users and groups and here under
289:39 - organization policies
289:41 - i'm able to manage organization policies
289:44 - but since i am not an organization
289:46 - policy administrator and i don't have an
289:48 - organization there's not much that i can
289:51 - do here just know that when you have an
289:53 - organization set up you are able to come
289:56 - here in order to manage and edit your
289:59 - organization policies now moving under
290:02 - quotas we went over this in a little bit
290:04 - of detail in a previous lesson and again
290:08 - this is to edit any quotas for any of
290:10 - your services in case you need a limit
290:13 - increase moving on to service accounts
290:16 - i will be covering this topic in great
290:18 - depth in a later lesson and we'll be
290:21 - going through a hands-on demonstration
290:23 - as well now i know i haven't touched
290:25 - much on labels as of yet but know that
290:28 - labels are a key value pair that helps
290:30 - you organize and then filter your
290:33 - resources based on their labels these
290:36 - same labels are also forwarded to your
290:38 - billing system so you can then break
290:40 - down your billing charges by label and
290:43 - you can also use labels based on teams
290:46 - cost centers components and even
290:48 - environments
290:50 - so for example if i wanted to label my
290:52 - virtual machines by environment i can
290:56 - simply use environment as the key and as
290:58 - the value i can use anything from
291:01 - development
291:03 - to qa
291:04 - to testing
291:06 - to production and i could simply add
291:08 - this label and add all the different
291:10 - environments and later i'd be able to
291:13 - query based on these specific labels now
291:16 - a good rule of thumb is to label all of
291:19 - your resources so that this way you're
291:21 - able to find them a lot easier and
291:23 - you're able to query them a lot easier
291:26 - so moving forward with any of your
291:28 - resources that you are creating be sure
291:31 - to add some labels to give you maximum
291:33 - flexibility so i'm going to discard
291:35 - these changes and we're going to move on
291:38 - to settings and we touched on settings
291:40 - in an earlier lesson with regards to
291:43 - projects and so here i could change the
291:45 - project name it'll give me the project
291:47 - id the project number and i'm able to
291:50 - migrate or shut down the project now
291:52 - when it comes to access transparency
291:55 - this provides you with logs that capture
291:58 - the actions that google personnel take
292:01 - when they're accessing your content for
292:03 - troubleshooting so they're like cloud
292:05 - audit logs but for google support now in
292:08 - order to enable access transparency for
292:11 - your google cloud organization
292:14 - your google cloud account must have a
292:16 - premium support plan or a minimum level
292:20 - of a 400 a month support plan and
292:23 - because i don't have this i wouldn't be
292:25 - able to enable access transparency now
292:28 - although access transparency is not on
292:30 - the exam
292:31 - this is a great feature to know about in
292:34 - case you are working in any bigger
292:36 - environments that have these support
292:38 - plans and compliance is of the utmost
292:42 - importance now moving into privacy and
292:44 - security this is where google supplies
292:47 - all of their clients of google cloud the
292:50 - compliance that they need in order to
292:52 - meet regulations across the world and
292:55 - across various industries such as health
292:58 - care and education and because google
293:01 - has a broad base in europe google
293:03 - provides capabilities and contractual
293:06 - commitments created to meet data
293:09 - protection recommendations which is why
293:11 - you can see here eu model contract
293:14 - clauses
293:15 - and eu representative contacts as well
293:18 - under transparency and control i'm able
293:21 - to disable the usage data that google
293:23 - collects in order to provide better data
293:26 - insights and recommendations and this is
293:29 - done at the project level and as well i
293:31 - have the option of going over to my
293:33 - billing account and i could select a
293:36 - different billing account that's linked
293:37 - to some other projects that you can get
293:40 - recommendations on and so continuing
293:42 - forward identity aware proxy is
293:45 - something that i will be covering in a
293:46 - later lesson and so i won't be getting
293:49 - into any detail about that right now and
293:51 - so what i really wanted to dig into is
293:53 - roles now this may look familiar as i
293:56 - touched on this very briefly in a
293:59 - previous lesson and here's where i can
294:01 - create roles
294:03 - i can create some custom roles from
294:05 - different selections and here i have
294:07 - access to all the permissions
294:09 - and if i wanted to i can filter down
294:12 - from the different types the names the
294:15 - permissions even the status so let's say
294:17 - i was looking for a specific permission
294:20 - and i'm looking all the permissions for
294:22 - projects this could help me find exactly
294:24 - what it is that i'm looking for and
294:26 - these filters allow me to get really
294:28 - granular so i can find the exact
294:31 - permission and so you can get really
294:33 - granular with regards to your
294:35 - permissions and create roles that are
294:37 - custom to your environment now moving on
294:40 - to audit logs here i can enable the auto
294:43 - logs without having to use a specific
294:45 - policy by simply clicking on default
294:48 - autoconfig and here i can turn on and
294:51 - off
294:52 - all the selected logging
294:54 - as well as add any exempted users now i
294:57 - don't recommend that you turn these on
295:00 - as audit logging can create an extremely
295:02 - large amount of data and can quickly
295:05 - blow through all of your 300 credit so
295:08 - i'm going to keep that off move back to
295:10 - the main screen of the audit logs and as
295:13 - well here i'm able to get really
295:15 - granular about what i want to log now
295:18 - quickly touching on audit logs in the
295:20 - command line i wanted to quickly open up
295:23 - cloud shell and show you an example of
295:26 - how i can edit the policy in order to
295:28 - enable audit logging just going to make
295:31 - this a little bit bigger
295:32 - and i'm going to paste in my command
295:35 - gcloud projects get dash iam
295:39 - dash policy
295:40 - with the project id which is project
295:42 - tony
295:44 - 286016 and i'm gonna just hit enter
295:47 - and as you can see here this is my
295:49 - current policy and as well as expected
295:52 - audit logs are not enabled due to the
295:55 - fact that the audit config field is not
295:58 - present so in order for me to enable the
296:00 - audit config logs i'm going to have to
296:03 - edit the policy and so the easiest way
296:05 - for me to do that is for me to run the
296:07 - same command and output it to a file
296:10 - where i can edit it and i'm going to
296:12 - call this
296:14 - new dash policy dot yaml and so now that
296:17 - my policy has been outputted to this
296:19 - file i'm going to now go into the editor
296:22 - and as you can see my new policy.yaml is
296:25 - right here and so for me to enable the
296:27 - autoconfig logs i'm going to simply
296:30 - append it to the file and then i'm going
296:32 - to go over here to the top menu
296:35 - and click on file and save and so now
296:38 - for me to apply this new policy i'm
296:40 - going to go back over to the terminal
296:42 - and now i'm going to paste in the
296:44 - command
296:45 - gcloud projects set dash
296:48 - iam-policy with the project id and the
296:51 - file name new dash policy dot yaml and
296:54 - i'm just going to hit enter
296:57 - and as you can see the audit log configs
297:00 - have been enabled for all services and
297:02 - because this may take some time to
297:05 - reflect in the console it will not show
297:07 - up right away but either way audit logs
297:10 - usually take up a lot of data and i
297:13 - don't want to blow through my 300 credit
297:16 - and so i'm going to disable them now the
297:18 - easiest way for me to do this is to
297:21 - output this policy to another file edit
297:24 - it and set it again and so i'm going to
297:26 - go ahead and do that i'm going to first
297:28 - clear the screen and then i'm going to
297:29 - paste in my command while outputting it
297:32 - to a new file called updated dash policy
297:35 - dot yaml and i'm gonna hit enter and now
297:38 - i'm gonna go into the editor so i can
297:40 - edit the file now the one thing i wanted
297:43 - to point out is that i could have
297:45 - overwritten
297:46 - the file new dash policy but if you look
297:49 - here in the updated policy the e-tag
297:53 - is different than the e-tag in the old
297:55 - policy and so this allowed me to
297:57 - highlight e-tags when it comes to
298:00 - editing and creating new policies and so
298:03 - when editing policies make sure that the
298:06 - etag is correct otherwise you will
298:08 - receive an error and not be able to set
298:11 - the new policy so going back to the
298:13 - updated policy file i'm going to take
298:16 - out the audit log configs and i'm going
298:18 - to leave the auto configs field there
298:20 - and i'm going to go to the menu click on
298:22 - file and then save now i'm going to go
298:25 - back to the terminal and i'm going to
298:27 - paste in the new command and this will
298:29 - update my policy and as you can see the
298:31 - audit config logs have been disabled and
298:34 - the policy has been updated now this is
298:36 - the same process that you can use when
298:39 - you want to update any parts of the
298:41 - policy when it comes to your members or
298:44 - roles and even adding any conditions so
298:48 - now moving on to the last item on the
298:50 - menu
298:51 - is groups
298:53 - and as you can see here because i do not
298:55 - have an organization i'm not able to
298:58 - view any groups and so if i did have an
299:00 - organization i could manage my groups
299:03 - right here in this page now moving back
299:06 - over to iam
299:07 - i wanted to dig into policies in a
299:10 - little bit of further detail now what we
299:12 - see here are the permissions and roles
299:15 - that have been granted to selected
299:17 - members in this specific project which
299:20 - is project tony now remember an im
299:23 - policy is a total collection of members
299:26 - that have roles granted to them in
299:29 - what's known as a binding and then the
299:31 - binding is applied to that layer and all
299:33 - other layers underneath it and since i'm
299:36 - at the project layer this policy is
299:38 - inherited by all the resources
299:41 - underneath it and so just to verify
299:43 - through the command line i'm going to
299:45 - open up cloud shell
299:50 - and i'm going to paste in the command
299:52 - gcloud projects get dash
299:55 - iam-policy with my project id
299:59 - and i'm going to hit enter
300:00 - and as you can see here the policy is a
300:03 - reflection of exactly what you see here
300:05 - in the console so as you can see here
300:08 - here's the service agent which you will
300:10 - find here and the other two service
300:12 - accounts which you will find above
300:14 - as well as tony bowtie ace gmail.com and
300:18 - all the other roles that accompany those
300:21 - members so as i mentioned earlier i've
300:23 - gone ahead and created a new user and so
300:26 - for those who are following along you
300:28 - can go ahead and feel free to create a
300:31 - new gmail user now going ahead with this
300:34 - demonstration the user i created is
300:37 - named laura delightful now tony needed
300:40 - an extra hand and decided to bring her
300:42 - onto the team from another department
300:44 - now unfortunately in order for laura to
300:47 - help tony on the project she needs
300:49 - access to this project and as you can
300:52 - see she doesn't have any access and so
300:55 - we're going to go ahead and change that
300:57 - and give her access to this project so
300:59 - i'm going to go back over to my open tab
301:02 - for tony bowtie ace and we're gonna go
301:05 - ahead and give laura permissions and so
301:07 - i'm gonna go ahead and click on this add
301:09 - button at the top of the page and the
301:11 - prompt will ask me to add a new member
301:14 - so i'm gonna add laura in here now
301:16 - and here she is and i'm going to select
301:18 - the role
301:20 - as project viewer i'm not going to add
301:22 - any conditions and i'm simply going to
301:25 - click on save and the policy has been
301:27 - updated and as you can see here laura
301:30 - has been granted the role of project
301:32 - viewer so i'm going to move over to the
301:34 - other open tab where laura's console is
301:36 - open and i'm going to simply do a
301:38 - refresh
301:41 - and now laura has access to view all the
301:45 - resources within project tony now laura
301:48 - is able to view everything in the
301:50 - project but laura isn't actually able to
301:52 - do anything and so in order for laura to
301:55 - get things done a big part of her job is
301:58 - going to be creating files with new
302:01 - ideas for the fall winter line of bow
302:04 - ties in 2021 and so because laura holds
302:08 - the project viewer role she is able to
302:10 - see everything in cloud storage but she
302:13 - is unable to create buckets
302:16 - to upload edit or delete any files or
302:20 - even folders and as you can see here
302:22 - there is a folder marked bowtie inc
302:25 - fallwinter 2021 ideas but laura cannot
302:29 - create any new buckets because she
302:31 - doesn't have the required permissions as
302:33 - well drilling down into this bucket
302:36 - laura is unable to create any folders as
302:39 - explained earlier
302:41 - and the same stands for uploading any
302:43 - files and so i'm going to cancel out of
302:45 - this and so in order to give laura the
302:47 - proper permissions for her to do her job
302:51 - we're going to give laura the storage
302:52 - admin role and so moving back over to
302:56 - the open console for tony bowtie i'm
302:58 - going to give laura access by using the
303:01 - command line so i'm going to go up to
303:03 - the top right and open up cloud shell
303:06 - and so the command i need to run
303:08 - to give laura the role of storage admin
303:11 - would be the following gcloud projects
303:14 - add dash iam dash policy dash binding
303:18 - with the project id dash dash member
303:21 - user followed by colon and then the user
303:24 - name which is laura delightful gmail.com
303:27 - dash dash role and the role which is
303:30 - storage admin and i'm going to go ahead
303:32 - and hit enter
303:34 - and as you can see it has been executed
303:36 - successfully so if i do a refresh of the
303:38 - web page here i'm going to be able to
303:40 - see the changes reflected in the console
303:44 - and after a refresh you can see here
303:46 - storage admin has been added to the role
303:49 - for laura delightful gmail.com and so if
303:52 - i go over to the open tab where laura
303:54 - has her console open i can simply do a
303:57 - refresh
303:59 - and if i go back to the home page for
304:02 - cloud storage you can see here that
304:04 - laura now has the permissions to create
304:07 - a bucket laura also now has permissions
304:10 - to create new folders create edit and
304:14 - delete new files on top of being able to
304:17 - create new storage buckets and so that
304:20 - about wraps up this demonstration on
304:22 - getting hands-on with iam
304:25 - in both the console and the command line
304:28 - and i also hope that this demo has given
304:30 - you a bit more confidence
304:32 - on working in the shell running the
304:34 - commands needed in order to create new
304:37 - bindings along with editing existing
304:40 - policies
304:41 - and this will get you comfortable for
304:43 - when you need to assign roles to new and
304:46 - existing users that are added to your
304:49 - gcp environment and so you can now mark
304:52 - this lesson as complete
304:54 - and let's move on to the next one
305:00 - welcome back in this lesson i'm going to
305:03 - take a deep dive into service accounts
305:06 - now service accounts play a powerful
305:08 - part in google cloud and can allow a
305:11 - different approach for application
305:14 - interaction with the resources in google
305:16 - cloud now service accounts being both an
305:20 - identity and a resource can cause some
305:22 - confusion for some and so i really
305:25 - wanted to spend some time breaking it
305:27 - down for better understanding and so i'm
305:30 - first going to start off by explaining
305:32 - what exactly is a service account and so
305:35 - a service account is a special kind of
305:37 - account that is used by an application
305:41 - or a virtual machine instance and not a
305:44 - person
305:45 - an application uses the service account
305:48 - to authenticate between the application
305:51 - and gcp services so that the users
305:54 - aren't directly involved in short it is
305:56 - a special type of google account
305:59 - intended to represent a non-human user
306:03 - that needs to authenticate and be
306:05 - authorized to access data in google apis
306:09 - this way the service account is the
306:11 - identity of the service and the service
306:14 - accounts permissions control which
306:16 - resources the service can access and as
306:19 - a note a service account is identified
306:22 - by its email address
306:24 - which is unique to the account now the
306:27 - different service account types
306:29 - come in three different flavors
306:32 - user managed
306:33 - default and google managed service
306:35 - accounts when it comes to the user
306:37 - managed service accounts
306:39 - these are service accounts that you
306:41 - create
306:42 - you're responsible for managing and
306:44 - securing these accounts
306:46 - and by default you can create up to 100
306:49 - user managed service accounts in a
306:52 - project or you can also request a quota
306:55 - increase in case you need more now when
306:58 - you create a user managed service
307:00 - account in your project it is you that
307:02 - chooses a name for the service account
307:04 - this name appears in the email address
307:07 - that identifies the service account
307:09 - which uses the following format seen
307:12 - here the service account name
307:14 - at the project id dot
307:17 - iam.gserviceaccount.com
307:20 - now moving on to the default service
307:22 - accounts
307:23 - when you use some google cloud services
307:26 - they create user managed service
307:28 - accounts that enable the service to
307:31 - deploy jobs that access other google
307:34 - cloud resources these accounts are known
307:37 - as default service accounts so when it
307:39 - comes to production workloads google
307:42 - strongly recommends that you create your
307:45 - own user managed service accounts and
307:48 - grant the appropriate roles to each
307:50 - service account when a default service
307:52 - account is created it is automatically
307:55 - granted the editor role on your project
307:58 - now following the principle of lease
308:00 - privilege google strongly recommends
308:04 - that you disable the automatic role
308:06 - grant
308:07 - by adding a constraint to your
308:09 - organization policy
308:11 - or by revoking the editor role manually
308:14 - the default service account will be
308:16 - assigned an email address
308:18 - following the format you see here
308:21 - project id
308:22 - at appspot.gserviceaccount.com
308:25 - for any service accounts created by app
308:28 - engine and project number dash compute
308:32 - at developer.gserviceaccount.com
308:35 - for compute engine and so lastly when it
308:38 - comes to google managed service accounts
308:41 - these are created and managed by google
308:44 - and they are used by google services the
308:47 - display name of most google managed
308:49 - service accounts
308:50 - ends with a
308:52 - gserviceaccount.com address now some of
308:55 - these service accounts are visible but
308:57 - others are hidden so for example
308:59 - google api service agent is a service
309:03 - account named with an email address that
309:05 - uses the following format
309:07 - project number at cloudservices.gerisa
309:13 - and this runs internal google processes
309:16 - on your behalf and this is just one
309:18 - example of the many google managed
309:20 - services that run in your environment
309:23 - and just as a warning it is not
309:25 - recommended to change or revoke the
309:28 - roles that are granted to the google api
309:31 - service agent or to any other google
309:33 - managed service accounts for that matter
309:35 - if you change or revoke these roles
309:38 - some google cloud services will no
309:41 - longer work now when it comes to
309:43 - authentication for service accounts they
309:45 - authenticate using service account keys
309:48 - so each service account is associated
309:51 - with two sets of public and private rsa
309:54 - key pairs that are used to authenticate
309:57 - to google they are the google manage
309:59 - keys and the user manage keys with the
310:02 - google manage keys google stores both
310:05 - the public and private portion of the
310:08 - key
310:08 - rotates them regularly and the private
310:11 - key is always held in escrow and is
310:14 - never directly accessible iam provides
310:17 - apis to use these keys to sign on behalf
310:21 - of the service account now when using
310:24 - user managed key pairs this implies that
310:27 - you own both the public and private
310:29 - portions of a key pair you can create
310:32 - one or more user managed key pairs also
310:35 - known as external keys that can be used
310:38 - from outside of google cloud google only
310:41 - stores the public portion of a user
310:43 - managed key
310:44 - so you are responsible for the security
310:48 - of the private key as well as the key
310:51 - rotation private keys cannot be
310:53 - retrieved by google so if you're using a
310:56 - user manage key
310:58 - please be aware that if you lose your
311:00 - key your service account will
311:02 - effectively stop working google
311:04 - recommends storing these keys in cloud
311:07 - kms for better security and better
311:10 - management user managed keys are
311:12 - extremely powerful credentials and they
311:15 - can represent a security risk if they
311:18 - are not managed correctly and as you can
311:20 - see here a user managed key has many
311:23 - different areas that need to be
311:25 - addressed when it comes to key
311:27 - management now when it comes to service
311:29 - account permissions
311:31 - in addition to being an identity a
311:33 - service account is a resource which has
311:36 - im policies attached to it and these
311:39 - policies determine who can use the
311:42 - service account so for instance
311:44 - lark can have the editor role on a
311:46 - service account and laura can have a
311:49 - viewer role on a service account so this
311:52 - is just like granting roles for any
311:54 - other google cloud resource just as a
311:57 - note
311:58 - the default compute engine and app
312:00 - engine service accounts are granted
312:02 - editor roles on the project when they
312:05 - are created so that the code executing
312:08 - in your app or vm instance has the
312:11 - necessary permissions now you can grant
312:14 - the service account user role at both
312:17 - the project level for all service
312:19 - accounts in the project or at the
312:21 - service account level now granting the
312:24 - service account user role to a user for
312:27 - a project
312:28 - gives the user access to all service
312:31 - accounts in the project
312:33 - including service accounts that may be
312:35 - created in the future granting the
312:37 - service account user role to a user for
312:41 - a specific
312:42 - service account gives a user access to
312:45 - only that service account so please be
312:48 - aware when granting the service account
312:50 - user role to any member now users who
312:54 - are granted the service account user
312:55 - role on a service account can use it to
312:59 - indirectly access
313:01 - all the resources to which the service
313:03 - account has access
313:05 - when this happens the user impersonates
313:08 - the service account to perform any tasks
313:12 - using its granted roles and permissions
313:15 - and is known as service account
313:17 - impersonation now when it comes to
313:19 - service account permissions there is
313:22 - also another method use called access
313:24 - scopes service account scopes are the
313:28 - legacy method of specifying permissions
313:31 - for your instance
313:33 - and they are used in substitution of iam
313:36 - roles these are used specifically for
313:39 - default or automatically created service
313:42 - accounts based on enabled apis now
313:45 - before the existence of iam roles access
313:49 - scopes were the only way for granting
313:52 - permissions to service accounts and
313:54 - although they are not the primary way of
313:56 - granting permissions now
313:58 - you must still set service account
314:00 - scopes when configuring an instance to
314:04 - run as a service account however when
314:07 - you are using a custom service account
314:09 - you will not be using scopes rather you
314:12 - will be using iam roles
314:15 - so when you are using a default service
314:17 - account for your compute instance it
314:20 - will default to using scopes instead of
314:23 - iam roles and so i wanted to quickly
314:26 - touch on how service accounts are used
314:29 - now one way of using a service account
314:31 - is to attach this service account to a
314:34 - resource
314:35 - so if you want to start a long-running
314:37 - job that authenticates as a service
314:40 - account you need to attach a service
314:42 - account to the resource that will run
314:45 - the job and this will bind the service
314:48 - account to the resource now the other
314:50 - way of using a service account is
314:52 - directly impersonating a service account
314:55 - which i had explained a little bit
314:57 - earlier so once granted they require
314:59 - permissions a user or a service can
315:02 - directly impersonate the identity of a
315:06 - service account in a few common
315:08 - scenarios you can impersonate the
315:10 - service account without requiring the
315:12 - use of a downloaded external service
315:15 - account key as well a user may get
315:18 - artifacts signed by the google managed
315:20 - private key of the service account
315:23 - without ever actually retrieving a
315:26 - credential for the service account and
315:28 - this is an advanced use case and is only
315:31 - supported for programmatic access now
315:34 - although i'm going to be covering best
315:36 - practices
315:37 - at the end of this section i wanted to
315:40 - go over some best practices for service
315:42 - accounts specifically so you should
315:44 - always look at auditing the service
315:46 - accounts
315:47 - and their keys
315:49 - using either the service account dot
315:52 - keys dot list method
315:54 - or the logs viewer page in the console
315:57 - now if your service accounts don't need
315:59 - external keys
316:01 - you should definitely delete them you
316:03 - should always grant the service account
316:05 - only the minimum set of permissions
316:08 - required to achieve the goal
316:10 - service accounts should also be created
316:13 - for each specific service with only the
316:16 - permissions required for that service
316:19 - and finally when it comes to
316:21 - implementing key rotation you should
316:23 - take advantage of the iam service
316:25 - account api to get the job done and so
316:28 - that's all i have for this lesson on
316:30 - service accounts
316:32 - so you can now mark this lesson as
316:34 - complete
316:35 - and please join me in the next one where
316:37 - we go hands-on in the console
316:40 - [Music]
316:44 - welcome back
316:45 - so in this demonstration i'm going to
316:47 - take a hands-on tour diving through
316:49 - various aspects of working with both
316:52 - default and custom-made service accounts
316:55 - we're going to start off fresh
316:56 - observing a new service account being
316:59 - automatically created along with viewing
317:02 - scopes observing how to edit them and
317:05 - creating custom service accounts that
317:07 - get a little bit more granular with the
317:09 - permissions assigned so with that being
317:12 - said let's dive in so as you can see
317:14 - here from the top right hand corner that
317:17 - i am logged in under tony bowtie ace
317:20 - gmail.com and looking over here from the
317:22 - top drop down menu you can see that i am
317:25 - in the project of cat bow ties fall 2021
317:29 - and this is a brand new project that i
317:31 - had created specifically for this demo
317:34 - and so i currently have no resources
317:37 - created along with no apis enabled so
317:40 - now i want to navigate over to iam so
317:43 - i'm going to go up to the left hand
317:44 - corner to the navigation menu
317:46 - and i'm going to go to i am an admin
317:49 - and over to iam and as expected i have
317:52 - no members here other than myself tony
317:55 - bowtie ace gmail.com with no other
317:58 - members and if i go over here to the
318:00 - left hand menu under service accounts
318:03 - you can see that i have no service
318:05 - accounts created so now in order to
318:07 - demonstrate a default service account
318:10 - i'm going to go over to the navigation
318:12 - menu and go into compute engine and as
318:15 - you can see the compute engine api is
318:18 - starting up and so this may take a
318:20 - couple minutes to get ready okay and the
318:22 - compute engine api has been enabled so
318:25 - now if i go back over to iam to take a
318:29 - look at my service accounts as expected
318:32 - i have my compute engine default service
318:34 - account now again i did not create this
318:37 - manually this service account was
318:40 - automatically created when i had enabled
318:42 - the compute engine api along with the
318:45 - api's service agent and the compute
318:48 - engine service agent and the same would
318:50 - happen to other various apis that are
318:53 - enabled as well and so now that i have
318:56 - my default service account i want to go
318:58 - back over to compute engine
319:00 - and i'm going to go ahead and create a
319:03 - vm instance so i'm going to just click
319:05 - on create
319:07 - i'm going to keep everything as the
319:08 - default except i'm going to change the
319:11 - machine type from an e2 medium to an e2
319:14 - micro and so now i'm going to scroll
319:16 - down to where it says identity and api
319:20 - access
319:21 - now here under service account you can
319:23 - see that the compute engine default
319:26 - service account has been highlighted and
319:28 - this is because i don't have any other
319:30 - service accounts that i am able to
319:33 - select from now when a default service
319:35 - account is the only service account you
319:37 - have access to
319:39 - access scopes are the only permissions
319:41 - that will be available for you to select
319:44 - from now remember access scopes are the
319:46 - legacy method of specifying permissions
319:49 - in google cloud now under access scopes
319:52 - i can select from the allow default
319:54 - access
319:55 - allow full access to all cloud apis
319:59 - and set access for each api and so i
320:02 - want to click on set access for each api
320:04 - for just a second and so as you can see
320:07 - here i have access to set permissions
320:10 - for each api the difference being is
320:13 - that i only have access to primitive
320:16 - roles and so now that i'm looking to
320:18 - grant access to my service account i'm
320:21 - going to grant access to cloud storage
320:24 - on a read-only capacity
320:26 - and so now that i have granted
320:27 - permissions for my service account i'm
320:30 - going to now create my instance by
320:32 - simply clicking on the create button
320:34 - and so now that my instance is created
320:37 - i want to head over to cloud storage to
320:40 - see exactly what my service account will
320:42 - have access to so i'm going to go over
320:44 - to my navigation menu and scroll down
320:47 - and click on storage and as you can see
320:49 - here i have created a bucket in advance
320:52 - called bow tie ink fall winter 2012
320:55 - designs and this is due to bow tie ink
320:58 - bringing back some old designs from 2012
321:01 - and making them relevant for today and
321:04 - within that bucket there are a few files
321:07 - of different design ideas that were best
321:09 - sellers back in 2012
321:11 - that tony bowtie wanted to re-release
321:15 - for the fall winter 2012 collection and
321:18 - so with the new granted access to my
321:20 - default service account i should have
321:22 - access to view these files so in order
321:25 - to test this i'm going to go back over
321:28 - to the navigation menu and go back to
321:30 - compute engine and i'm going to ssh into
321:33 - my instance
321:36 - and so now that i've sshed into my
321:38 - virtual machine
321:40 - i wanted to first check to see who is it
321:43 - that's running the commands is it my
321:45 - user account or is it my service account
321:47 - and so i'll be able to do this very
321:49 - easily by checking the configuration and
321:52 - i can do this by running the command
321:54 - gcloud config list and as you can see my
321:57 - current configuration is showing that my
322:00 - service account is the member that is
322:02 - being used to run this command in the
322:04 - project of cat bow ties fall 2021 now if
322:08 - i wanted to run any commands using my
322:11 - tony bowtie ace
322:13 - gmail.com user account i can simply run
322:16 - the command gcloud auth login and it
322:19 - will bring me through the login process
322:21 - that we've seen earlier on in the course
322:24 - for my tony bowtie ace
322:26 - gmail.com account but now since i'm
322:29 - running all my commands using my service
322:31 - account from this compute engine
322:33 - instance i'm using the permissions
322:36 - granted to that service account that we
322:38 - saw earlier and so since i set the
322:41 - storage scope for the service account to
322:43 - read only we should be able to see the
322:46 - cloud storage bucket and all the files
322:48 - within it by simply running the gsutil
322:51 - command so to list the contents of the
322:53 - bucket i'm going to type in the command
322:56 - gsutil ls for list and the name of the
322:59 - bucket and the syntax for that would be
323:01 - gs
323:02 - colon forward slash forward slash
323:05 - followed by the name of the bucket which
323:07 - would be bowtie inc
323:09 - fw2012
323:11 - designs
323:13 - and as you can see we're able to view
323:15 - all the files that are in the bucket and
323:18 - so it is working as expected and so now
323:20 - because i've only granted viewing
323:22 - permissions for this service account i
323:25 - cannot create any files due to the lack
323:28 - of permissions so for instance if i was
323:30 - to create a file
323:32 - using the command touch
323:35 - file one i have now created that file
323:37 - here on the instance so now i want to
323:40 - copy this file to my bucket and so i'm
323:43 - going to run the gsutil command
323:46 - cp for copy
323:48 - file 1 which is the name of my file and
323:50 - gs
323:52 - colon forward slash forward slash along
323:54 - with the name of the bucket which is bow
323:57 - tie inc fw
323:59 - 2012
324:00 - designs and as expected i am getting an
324:03 - access denied exception with a prompt
324:06 - telling me that i have insufficient
324:08 - permissions and so now that i've shown
324:10 - you how to create a default service
324:13 - account and give it permissions using
324:15 - access scopes let's now create a custom
324:18 - service account and assign it proper
324:20 - permissions
324:21 - to not only read files from cloud
324:23 - storage but be able to write files to
324:26 - cloud storage as well so i'm going to
324:28 - now close down this tab
324:31 - and i'm going to go back over to the
324:32 - navigation menu
324:34 - and go back to iam where we can go in
324:36 - and create our new service account
324:39 - under service accounts
324:41 - and so as you can see here this is the
324:43 - default service account and since we
324:45 - want to create a custom one i'm going to
324:47 - go ahead and go up to the top here and
324:49 - click on the button that says create
324:51 - service account
324:53 - and so now i'm prompted to enter some
324:55 - information with regards to details of
324:58 - this service account including the
324:59 - service account name the account id
325:02 - along with a description and so i'm
325:04 - going to call this service account sa
325:07 - hyphen bowtie hyphen demo and as you can
325:10 - see it automatically propagated the
325:13 - service account id and i'm going to give
325:15 - this service account a description
325:18 - storage read write access
325:20 - and i'm going to click on the button
325:21 - create
325:23 - and so now i've been prompted to grant
325:25 - permissions to the service account and i
325:27 - can do that by simply clicking on the
325:29 - drop down and selecting a roll but i'm
325:32 - looking to get a little bit more
325:33 - granular and so i'm going to simply type
325:36 - in storage and as you can see i'm coming
325:38 - up with some more granular roles as
325:40 - opposed to the primitive roles that i
325:43 - only had access to prior to the search
325:45 - so i'm going to click on storage object
325:47 - viewer for read access to cloud storage
325:50 - i'm not going to add any conditions and
325:52 - i'm going to add another role and this
325:54 - time
325:55 - i'm going to add storage object creator
325:58 - and so those are all the permissions i
326:00 - need for read write access to cloud
326:02 - storage and so now i can simply click on
326:05 - continue and so now i'm being prompted
326:08 - to add another user to act as a service
326:10 - account and this is what we discussed in
326:12 - the last lesson about service accounts
326:15 - being both a member and a resource now
326:18 - notice that i have an option for both
326:20 - the service account users role and the
326:22 - service account admins role now as
326:25 - discussed earlier the service account
326:27 - and men's role has the ability to grant
326:29 - other users the role of service account
326:32 - user and so because we don't want to do
326:35 - that i'm going to leave both of these
326:36 - fields blank
326:38 - and simply click on done now i know in
326:40 - the last lesson i talked about creating
326:43 - custom keys for authentication
326:46 - in case you're hosting your code on
326:48 - premise or on another cloud and so if i
326:51 - wanted to do that i can simply go to the
326:53 - actions menu and click on create key and
326:56 - it'll give me the option on creating a
326:59 - private key either using json or p12
327:02 - format and because i'm not creating any
327:04 - keys i'm going to simply click on cancel
327:07 - and so in order for me to apply this
327:09 - service account
327:10 - to our vm instance i'm going to now go
327:13 - back over to the navigation menu and go
327:16 - back into compute engine and so now in
327:18 - order for me to change this service
327:20 - account that's currently assigned to
327:23 - this instance i'm going to go ahead and
327:25 - check off this instance and click on
327:27 - stop
327:28 - now please note that in order to change
327:31 - service accounts on any instance you
327:34 - must stop it first before you can edit
327:36 - the service account and so now that the
327:38 - instance has stopped i'm going to drill
327:40 - down into this instance one
327:43 - and i'm going to click on edit
327:45 - now i'm going to scroll down to the
327:47 - bottom
327:48 - and at the bottom you will find the
327:50 - service account field and clicking on
327:52 - the drop down i'll find my custom
327:54 - service account as a bow tie demo so i
327:57 - want to select this and simply click on
327:59 - save and so now that i've selected my
328:02 - new service account to be used in this
328:04 - vm instance i can now start up the
328:07 - instance again to test out the
328:09 - permissions that were granted
328:11 - and so just as a quick note here i
328:13 - wanted to bring your attention to the
328:15 - external ip whenever stopping and
328:18 - starting an instance with an ephemeral
328:20 - ip in other words it is not assigned a
328:23 - static ip your vm instance will receive
328:26 - a new ip address and i'll be getting
328:28 - into this in a lot deeper detail in the
328:31 - compute engine section of the course and
328:34 - so now i'm going to ssh into this
328:36 - instance
328:37 - now i'm going to run the same gsutil
328:40 - command that i did previously to list
328:42 - all the files in the bucket so i'm going
328:45 - to run the command gsutil ls for list
328:48 - and gs
328:50 - colon forward slash forward slash bow
328:52 - tie inc fw 2012 designs
328:57 - and as you can see i'm able to read all
329:00 - the files in the bucket now the
329:02 - difference in the permissions granted
329:04 - for the service account is that i'm able
329:06 - to write files to cloud storage and so
329:10 - in order to test that i'm going to use
329:12 - the touch command again and i'm going to
329:14 - name the file file2 and so now i'm going
329:17 - to copy this file to the cloud storage
329:20 - bucket by using the command gsutil cp
329:25 - file2 and the bucket name
329:27 - gs colon forward slash forward slash bow
329:30 - tie inc fw 2012 designs and as expected
329:35 - the file copied over successfully as we
329:37 - do have permissions to write to cloud
329:40 - storage and so before i end this
329:42 - demonstration i wanted to quickly go
329:45 - over exactly how to create service
329:48 - accounts
329:49 - using the command line and so i'm going
329:50 - to close down this tab and i'm going to
329:53 - head up to the top right hand corner and
329:55 - activate my cloud shell i'm going to
329:58 - make this window a little bit bigger and
330:00 - so now in order to view the service
330:02 - accounts i currently have
330:04 - i'm going to run the command
330:06 - gcloud
330:07 - iam
330:08 - service dash accounts
330:12 - list
330:15 - and so as expected the compute engine
330:17 - default service account along with the
330:20 - custom service account that i created
330:22 - earlier called sa bowtie demo is now
330:25 - displaying and in order to just verify
330:28 - that i'm going to go over to iam
330:30 - under service accounts and as you can
330:33 - see it is reflecting exactly the same in
330:35 - the console so now in order for me to
330:37 - create a new service account using the
330:40 - command line i'm going to run the
330:42 - command
330:43 - gcloud iam service accounts create
330:47 - and the name of the service account
330:49 - which i'm going to call sa-tony bowtie
330:52 - along with the display name as essay
330:55 - tony bowtie as well and i'm going to hit
330:57 - enter
330:59 - and my service account has been created
331:01 - so now if i run the command gcloud i am
331:04 - service accounts list
331:06 - i should see my new service account and
331:08 - as well if i did a refresh here on the
331:11 - console i can see that it is reflecting
331:14 - the same so now that we've created our
331:17 - new service account we need to assign
331:19 - some permissions to it in order for us
331:22 - to be able to use it and so if i go over
331:24 - here to iam in the console i can see
331:27 - here that my service account has not
331:29 - been assigned any permissions and so in
331:31 - order to do that i am going to simply
331:34 - run the command
331:35 - gcloud projects
331:37 - add dash iam-policy-binding
331:40 - so we're adding a policy binding and
331:42 - then the name of the project catbow ties
331:45 - fall 2021 we need to add the member
331:48 - which is the new service account email
331:50 - address along with the role of storage
331:53 - object viewer i'm going to hit enter
331:56 - and as you can see
331:57 - my member sa tony bowtie has been
332:00 - assigned the storage object viewer role
332:03 - and so if i wanted to grant some other
332:05 - roles to the service account i can do
332:07 - that as well and so if i did a refresh
332:09 - here i can see that the console reflects
332:12 - exactly the same and so in order for me
332:15 - to use this account in my instance i'm
332:17 - going to first have to stop my instance
332:20 - attach my service account and then start
332:22 - up my instance again so i'm going to go
332:24 - over to my cloud shell i'm just going to
332:26 - clear the screen and i'm going to paste
332:28 - in the command gcloud compute instances
332:31 - stop the name of the instance along with
332:34 - the zone and now that the instance has
332:36 - stopped i can now add my surface account
332:39 - to the instance and so i'm going to use
332:42 - the command gcloud compute instances
332:45 - set service account instance 1
332:48 - along with the zone and the service
332:50 - account email address i'm going to go
332:52 - ahead and hit enter
332:55 - and it has now been successfully added
332:57 - and so now that that's done i can now
332:59 - start up the instance by using the
333:01 - command gcloud compute instances start
333:04 - along with the instance name and the
333:06 - zone and so now if i go over to my
333:09 - navigation menu and go over to compute
333:11 - engine and drill down on the instance if
333:14 - i scroll down to the bottom
333:16 - i'll be able to see that my new service
333:19 - account has been added
333:20 - and so this is a great demonstration for
333:23 - when you want to add different service
333:25 - accounts for your different applications
333:28 - on different instances or even on
333:30 - different resources and so that's pretty
333:32 - much all i wanted to cover in this
333:34 - demonstration so you can now mark this
333:36 - lesson as complete and let's move on to
333:39 - the next one
333:44 - welcome back
333:45 - in this lesson i'm going to dive into
333:47 - cloud identity google's identity as a
333:50 - service offering for google cloud that
333:53 - maximizes end user efficiency protect
333:56 - company data
333:58 - and so much more
334:00 - now cloud identity as i said before is
334:02 - an identity as a service solution that
334:05 - centrally manages users and groups this
334:08 - would be the sole system for
334:10 - authentication and that provides a
334:12 - single sign-on experience for all
334:15 - employees of an organization to be used
334:19 - for all your internal and external
334:21 - applications cloud identity also gives
334:24 - you more control over the accounts that
334:27 - are used in your organization for
334:29 - example if developers in your
334:31 - organization use personal accounts such
334:34 - as gmail accounts those accounts are
334:36 - outside of your control
334:38 - so when you adopt cloud identity you can
334:41 - manage access and compliance across all
334:44 - the users in your domain now when you
334:47 - adopt cloud identity you create a cloud
334:50 - identity account for each of your users
334:53 - and groups you can then use iam to
334:55 - manage access to google cloud resources
334:58 - for each cloud identity account and you
335:01 - can also configure cloud identity to
335:04 - federate identities between google and
335:07 - other identity providers such as active
335:10 - directory and azure active directory and
335:13 - i'll be getting more into that a little
335:15 - bit later
335:17 - so now when it comes to cloud identity
335:19 - it gives you so much more than just user
335:23 - and group management it provides a slew
335:25 - of features such as device management
335:29 - security
335:31 - single sign-on
335:33 - reporting
335:34 - and directory management
335:36 - and i will be diving deeper into each
335:39 - one of these features of cloud identity
335:41 - now starting with device management
335:44 - this lets people in any organization
335:47 - access their work accounts from mobile
335:49 - devices while keeping the organization's
335:52 - data more secure in today's world
335:55 - employees want to access business
335:57 - applications from wherever they are
336:00 - whether at home at work
336:03 - or even traveling
336:04 - and many even want to use their own
336:06 - devices which is also known as bring
336:09 - your own device or byod for short using
336:13 - mobile device management there are
336:15 - several ways that you can provide the
336:18 - business applications employees need
336:21 - on their personal devices while
336:23 - implementing policies that keep the
336:25 - corporate data safe you can create a
336:27 - white list of approved applications
336:30 - where users can access corporate data
336:32 - securely through those applications
336:35 - you can enforce work profiles on android
336:37 - devices and requiring managed
336:40 - applications on ios devices policies can
336:44 - also be pushed out on these devices to
336:46 - protect corporate data and identities as
336:50 - well as keeping inventory of devices
336:53 - with corporate data present then when
336:55 - these devices are either no longer being
336:57 - used for corporate use or stolen the
337:00 - device can then be wiped of all its
337:03 - corporate data device management also
337:06 - gives organizations the power to enforce
337:09 - passcodes
337:10 - as well as auditing now moving into the
337:13 - security component of cloud identity
337:15 - this is where two-step verification
337:18 - steps in now as explained earlier
337:20 - two-step verification or to sv
337:24 - is a security feature that requires
337:26 - users to verify their identity through
337:29 - something they know such as a password
337:32 - plus something they have such as a
337:34 - physical key or access code and this can
337:36 - be anything from security keys to google
337:39 - prompt
337:40 - the authenticator app and backup codes
337:44 - so cloud identity helps by applying
337:47 - security best practices along with being
337:50 - able to deploy
337:52 - two-step verification for the whole
337:54 - company along with enforcement controls
337:57 - and can also manage passwords to make
338:00 - sure they are meeting the enforced
338:03 - password requirements automatically so
338:06 - single sign-on is where users can access
338:10 - many applications
338:11 - without having to enter their username
338:14 - and password for each application single
338:17 - sign-on also known as sso can provide a
338:21 - single point of authentication through
338:23 - an identity provider also known as idp
338:27 - for short you can set up sso using
338:30 - google as an identity provider to access
338:33 - a slew of third-party applications
338:36 - as well as any on-premise or custom
338:39 - in-house applications you can also
338:42 - access a centralized dashboard for
338:44 - conveniently accessing your applications
338:47 - so now when lisa logs in with her
338:50 - employee credentials she will then have
338:52 - access to many cloud applications that
338:56 - bowtie inc it department has approved
338:59 - through a catalog of sso applications
339:02 - and this will increase both security and
339:05 - productivity for lisa and bowtie inc as
339:09 - lisa won't have to enter
339:11 - a separate username and password for
339:13 - separate applications now getting into
339:16 - reporting this covers audit logs for
339:19 - logins groups devices and even tokens
339:24 - you're even able to export these logs to
339:27 - bigquery for analysis
339:29 - and then you can create reports from
339:31 - these logs that cover security
339:34 - applications and activity
339:36 - now moving on to the last component of
339:38 - cloud identity is directory management
339:42 - and this provides profile information
339:45 - for users in your organization
339:47 - email
339:48 - and group addresses and shared external
339:51 - contacts in the directory using google
339:54 - cloud directory sync or gcds you can
339:58 - synchronize the data in your google
340:00 - account with your microsoft active
340:02 - directory or ldap server gcds doesn't
340:06 - migrate any content such as your email
340:09 - your calendar events or your files to
340:12 - your google account gcds is used to
340:15 - synchronize all your users groups and
340:18 - shared contacts to match the information
340:21 - in your ldap server which could be your
340:23 - active directory server or your azure
340:26 - active directory domain now getting
340:28 - deeper into google cloud directory sync
340:31 - i'd like to touch on active directory
340:34 - for just a minute now active directory
340:36 - is a very common directory service
340:39 - developed by microsoft and is a
340:42 - cornerstone in most big corporate
340:44 - on-premises environments it
340:46 - authenticates and authorizes all users
340:49 - and computers in a windows domain type
340:52 - network signing and enforcing security
340:56 - policies for all computers and
340:58 - installing or updating software as
341:01 - necessary now as you can see here in the
341:03 - diagram the active directory forest
341:06 - contains the active directory domain a
341:09 - bowtieinc.co
341:11 - and the active directory federation
341:13 - services of bowtieinc.co where the
341:16 - active directory forest is the
341:18 - hierarchical structure for active
341:21 - directory the active directory domain is
341:24 - responsible for storing information
341:26 - about members of the domain including
341:29 - devices and users and it verifies their
341:32 - credentials and defines their access
341:35 - rights active directory federation
341:37 - services or adfs
341:40 - is a single sign-on service where
341:42 - federation is the means of linking a
341:45 - person's electronic identity and
341:47 - attributes stored across multiple
341:50 - distinct identity management systems so
341:54 - you can think of it as a subset of sso
341:57 - as it relates only to authentication
342:00 - technologies used for federated identity
342:03 - include some common terms that you may
342:05 - hear me or others in the industry use
342:08 - from time to time such as saml which
342:10 - stands for security assertion markup
342:13 - language oauth open id and even security
342:17 - tokens such as simple web tokens json
342:20 - web tokens and saml assertions and so
342:23 - when you have identities already in your
342:25 - on-premises environment that live in
342:28 - active directory you need a way to tie
342:31 - these identities to the cloud and so
342:34 - here's where you would use google cloud
342:36 - directory sync to automatically
342:39 - provision users and groups from active
342:42 - directory to cloud identity or g suite
342:45 - google cloud directory sync is a free
342:48 - google provided tool that implements the
342:51 - synchronization process and can be run
342:54 - on google cloud or in your on-premises
342:57 - environment synchronization is one way
343:01 - so that active directory remains the
343:03 - source of truth cloud identity or g
343:06 - suite uses active directory federation
343:09 - services or adfs for single sign-on any
343:14 - existing corporate applications and
343:16 - other sas services can continue to use
343:20 - your adfs as an identity provider now i
343:23 - know this may be a review for some who
343:26 - are advanced in this topic but for those
343:28 - who aren't this is a very important
343:31 - topic to know as google cloud directory
343:34 - sync is a big part of cloud identity and
343:38 - is a common way that is used in many
343:40 - corporate environments to sync active
343:43 - directory or any other ldap server to
343:46 - google cloud especially when you want to
343:49 - keep your active directory as the single
343:51 - source of truth and so that's pretty
343:53 - much all i wanted to cover
343:55 - when it comes to cloud identity and
343:58 - google cloud directory sync so you can
344:00 - now mark this lesson as complete and
344:02 - let's move on to the next one
344:04 - [Music]
344:08 - welcome back now i wanted to close out
344:11 - this section by briefly going over the
344:13 - best practices to follow when working
344:16 - with identity and access management so
344:19 - the phrase that was discussed in the
344:21 - beginning of this lesson that will
344:23 - continuously come up in the exam is the
344:26 - principle of least privilege and again
344:29 - this is where you would apply only the
344:31 - minimal access level required for what
344:34 - is needed to be done and this can be
344:36 - done using predefined roles which is a
344:39 - more granular level role than using
344:41 - primitive roles which are very wide
344:44 - scoped roles that are applied to the
344:46 - whole project roles should also be
344:48 - granted at the smallest scope necessary
344:52 - so for instance when assigning somebody
344:54 - the permissions needed
344:56 - for managing pre-existing compute
344:58 - instances assigning a compute instance
345:01 - admin role might be sufficient for what
345:04 - they need to do as opposed to assigning
345:06 - them the compute instance role that has
345:09 - full control of all compute engine
345:12 - instance resources now when it comes to
345:15 - child resources they cannot restrict
345:18 - access granted on its parent
345:20 - so always remember to check the policy
345:22 - granted on every resource and make sure
345:25 - you understand the hierarchical
345:28 - inheritance you also want to make sure
345:31 - that you restrict access to members
345:33 - abilities to create and manage service
345:36 - accounts
345:37 - as users who are granted the service
345:39 - account actor role for a service account
345:43 - can access all the resources for which
345:46 - the service account has access and
345:48 - granting someone with the owner role
345:50 - should be used with caution as they will
345:53 - have access to modify almost all
345:56 - resources
345:57 - project-wide including iam policies and
346:01 - billing granting an editor role might be
346:04 - more sufficient for the needs of most
346:07 - when using primitive roles
346:09 - now when dealing with resource hierarchy
346:12 - to make it easy on how to structure your
346:14 - environment
346:16 - you should look at mirroring your google
346:18 - cloud resource hierarchy structure to
346:20 - your organizational structure in other
346:23 - words the google cloud resource
346:25 - hierarchy should reflect how your
346:27 - company is organized you should also use
346:29 - projects to group resources that share
346:32 - the same trust boundary as well as
346:34 - setting policies at the organization
346:37 - level and at the project level rather
346:40 - than at the resource level now going
346:42 - back to what we discussed earlier about
346:44 - the principle of least privilege
346:46 - you should use this guideline to grant
346:49 - iam roles that is
346:51 - only give the least amount of access
346:53 - necessary to your resources and when
346:56 - granting roles across multiple projects
346:59 - it is recommended to grant them at the
347:02 - folder level instead of at the project
347:04 - level
347:05 - now diving back into service accounts a
347:08 - separate trust boundary should always be
347:11 - applied for any given application in
347:14 - other words create a new service account
347:17 - when multiple components are involved in
347:19 - your application you also want to make
347:22 - sure that you don't delete any service
347:24 - accounts that are in use by running
347:27 - instances as your application is likely
347:30 - to fail
347:31 - so you will want to schedule this during
347:33 - plan down time to avoid any outages now
347:37 - earlier on in this section we discussed
347:39 - service account keys and how they
347:42 - interact with google cloud and that is
347:44 - the main authentication mechanism used
347:47 - for keys so you want to make sure that
347:49 - any user managed keys are rotated
347:52 - periodically to avoid being compromised
347:56 - you can rotate a key by creating a new
347:58 - key switching applications to use the
348:01 - new key and then deleting the old key
348:03 - but be sure to create the new key first
348:06 - before deleting the old one as this will
348:08 - result in parts or even your entire
348:11 - application failing and also when
348:13 - working with service account keys it's
348:15 - always good practice to name your
348:18 - service keys and this will reflect your
348:20 - use for those keys and permissions for
348:22 - those keys so you know what they are
348:24 - used for when you're looking at them now
348:26 - when you are giving access to service
348:28 - accounts you want to make sure that only
348:31 - those who truly need access are the ones
348:34 - that have it
348:35 - others in your environment should be
348:37 - restricted to avoid any misuse now when
348:40 - it comes to keeping your service account
348:42 - keys safe i can't stress this enough you
348:45 - never want to check in these keys source
348:48 - code or leave them in your downloads
348:50 - directory as this is a prime way of not
348:53 - only getting your keys compromised but
348:56 - compromising your entire environment to
348:59 - be accessed publicly
349:01 - now we touched a bit on auditing but we
349:04 - haven't really gone into it in detail
349:07 - and we'll be going into it later on in
349:09 - the course but touching on best
349:11 - practices
349:12 - you want to be sure to check your cloud
349:14 - audit logs regularly and audit all i am
349:17 - policy changes whenever you edit any iam
349:21 - policies a log is generated that records
349:24 - that change and so you always want to
349:26 - periodically check these logs to make
349:29 - sure that there are no changes that are
349:32 - out of your security scope you also want
349:34 - to check to see
349:36 - who has editing permissions on these iam
349:39 - policies and make sure that those who
349:41 - hold them have the rights to do so point
349:44 - being is that you want to restrict who
349:47 - has the ability to edit policies and
349:50 - once these audit logs have been
349:52 - generated you want to export them to
349:54 - cloud storage so that you're able to
349:56 - store them for long term retention as
349:59 - these logs are typically held for weeks
350:02 - and not years getting back to service
350:04 - account keys
350:06 - service account key access should be
350:08 - periodically audited for viewing of any
350:11 - misuse or unauthorized access and lastly
350:16 - audit logs should also be restricted to
350:19 - only those who need access and others
350:22 - should have no permissions to view them
350:24 - and this can be done by adding a role to
350:27 - be able to view these logs now when
350:30 - touching on policy management you want
350:32 - to grant access to all projects in your
350:36 - organization by using an organization
350:39 - level policy you also want to grant
350:41 - roles to a google group instead of
350:44 - individual users as it is easier to add
350:47 - or remove members from a google group
350:50 - instead of updating an im policy and
350:53 - finally when you need to grant multiple
350:56 - roles to a task you should create a
350:59 - google group as it is a lot easier to
351:01 - grant the roles to that group and then
351:04 - add the users to that group as opposed
351:07 - to adding roles to each individual user
351:10 - and so that's all i wanted to cover on
351:12 - this short yet very important lesson on
351:15 - best practices when it comes to iam now
351:18 - i know this is not the most exciting
351:20 - topic but will become extremely
351:22 - necessary when you are dealing with
351:24 - managing users groups and policies in
351:27 - environments that require you to use iam
351:31 - securely and so please keep this in mind
351:34 - whenever you are working in any
351:35 - environment as it will help you grant
351:38 - the proper permissions when it comes to
351:40 - these different topics so now i highly
351:43 - recommend that you take a break grab a
351:45 - tea or coffee before moving on into the
351:48 - next section and so for now you can mark
351:50 - this lesson as complete and whenever
351:52 - you're ready please join me in the next
351:55 - section
351:55 - [Music]
351:59 - welcome back
352:01 - now i wanted to make this as easy as
352:03 - possible for those students who do not
352:05 - have a background in networking or any
352:08 - networking knowledge in general which is
352:11 - why i wanted to add this quick
352:13 - networking refresher to kick off the
352:15 - networking section of this course so
352:18 - with that being said let's dive in so
352:20 - before the internet computers were
352:22 - standalone and didn't have the
352:24 - capabilities to send emails
352:27 - transfer files or share any information
352:31 - fast forward some time
352:33 - people started to connect their
352:34 - computers together to share and be able
352:38 - to do the things that modern networks
352:40 - can do today part of being in this
352:42 - network is being able to identify each
352:45 - computer to know where to send and
352:47 - receive files
352:49 - this problem was solved by using an
352:51 - address to identify each computer on the
352:54 - network like humans use a street address
352:57 - to identify where they live
352:59 - so that mail and packages can be
353:01 - delivered to them an ip address is used
353:04 - to identify a computer or device on any
353:08 - network so communication between
353:10 - machines was done by the use of an ip
353:13 - address
353:14 - a numerical label assigned to each
353:17 - device connected to a computer network
353:20 - that uses the internet protocol for
353:22 - communication also known as ip for short
353:26 - so for this system to work a
353:28 - communication system was put in place
353:31 - that defined how the network would
353:33 - function
353:34 - this system was put together as a
353:37 - consistent model of protocol layers
353:40 - defining interoperability
353:42 - between network devices and software in
353:45 - layers
353:46 - to standardize how different protocols
353:49 - would communicate in this stack this
353:51 - stack is referred to as the open systems
353:55 - interconnection model or you may hear
353:58 - many refer to it as the seven layer osi
354:01 - model now this is not a deep dive
354:03 - networking course but i did feel the
354:06 - need to cover that which is necessary
354:09 - for the understanding of the elements
354:11 - taught in this course
354:13 - for those wanting to learn more about
354:14 - the osi model and the layers within it
354:17 - please check out the links that i have
354:19 - included in the lesson text below
354:22 - so for this lesson and the next i will
354:24 - be covering the specific layers with its
354:27 - protocols that are highlighted here and
354:30 - will help you understand the networking
354:33 - concepts in this course with a bit
354:35 - better clarity
354:37 - so i'll be covering a layer 3 being the
354:39 - network layer layer 4 being the
354:42 - transport layer and layer 7 being the
354:45 - application layer so first up i will be
354:48 - covering layer 3 which is the networking
354:50 - layer along with the internet protocol
354:54 - now there are two versions of the
354:56 - internet protocol and are managed
354:58 - globally by the regional internet
355:00 - registries also known as the rir the
355:04 - first one which is ipv4
355:06 - is the original version of the internet
355:08 - protocol that first came on the scene in
355:11 - 1981 the second version is ipv6 which is
355:15 - a newer version designed in 2017 to deal
355:19 - with the problem of ipv4 address
355:22 - exhaustion meaning that the amount of
355:24 - usable ips were slowly being used up
355:28 - and i will be covering both versions of
355:30 - the internet protocol in a little bit of
355:32 - depth so let's first dive into ipv
355:36 - version 4. so ipv4 can be read in a
355:39 - human readable notation represented in
355:43 - dotted decimal notation consisting of
355:46 - four numbers
355:47 - each ranging from 0 to 255 separated by
355:52 - dots each part between the dots
355:55 - represents a group of 8 bits also known
355:58 - as an octet a valid range for an ip
356:01 - address starts from 0.0.0.0
356:06 - and ends in 255.255.255.255.
356:11 - and this would give you a total number
356:13 - of over 4.2 billion ip addresses now
356:18 - this range was viewed as extremely large
356:20 - back then until the number of ip
356:23 - addresses available were quickly
356:26 - dwindling due to the many ipconnected
356:28 - devices that we have today
356:31 - and this is when a new addressing
356:33 - architecture was introduced called
356:35 - classful addressing where the address
356:38 - was split into smaller ranges and this
356:41 - was originally assigned to you when you
356:43 - needed an ip address by one of the
356:46 - registries noted before so for any given
356:48 - ip address they're typically made of two
356:52 - separate components the first part of
356:54 - the address is used to identify the
356:56 - network that the address is a part of
356:59 - the part that comes afterwards is used
357:01 - to specify a specific host within that
357:04 - network now the first part was assigned
357:06 - to you and your business by the
357:08 - registries
357:10 - and the second part was for you to do it
357:12 - as you'd like and so these ip addresses
357:15 - were assigned from the smaller ranges
357:17 - explained earlier called classes the
357:20 - first range of classes is class a
357:23 - and it started at 0.0.0.0
357:27 - and ended at 127.255
357:32 - and this would give a total number of
357:35 - over 2.1 billion addresses with 128
357:40 - different networks class a ip addresses
357:43 - can support over 16 million hosts per
357:45 - network and those who were assigned
357:47 - addresses in this class
357:49 - had a fixed value of the first octet the
357:52 - second third and fourth octet was free
357:55 - for the business to assign as they
357:57 - choose class a ip addresses were to be
358:00 - used by huge networks like those
358:03 - deployed by internet service providers
358:05 - and so when ips started to dwindle many
358:08 - companies return these class a network
358:10 - blocks back to the registries to assist
358:14 - with extending addressing capacity and
358:16 - so the next range is class b and this is
358:19 - half the size of the class a network the
358:22 - class b network range started at one at
358:24 - 128.0.0.0
358:28 - and end it at 191.255.255.255
358:34 - and carries a total number of over 1
358:37 - billion ip addresses with over 16 000
358:41 - networks the fixed value in this class
358:44 - is of the first and second octet the
358:46 - third and fourth octet can be done with
358:49 - as you like ip addresses in this class
358:52 - were to be used for medium and large
358:54 - size networks in enterprises and
358:57 - organizations the next range is class c
359:00 - and this is half the size of the class b
359:03 - network the class c network range starts
359:06 - at 192
359:10 - and ends at 223.255.255.255
359:16 - and carries a total of over half a
359:18 - billion addresses with over two million
359:21 - networks and can support up to
359:24 - 256 hosts the fixed value of this class
359:28 - is the first second and third octet and
359:32 - the fourth can be done with as you like
359:34 - ip addresses in this class
359:36 - were the most common class and were to
359:39 - be used in small business and home
359:41 - networks now there's a couple more
359:43 - classes that were not commonly used
359:46 - called class d and class e
359:49 - and this is beyond the scope of this
359:51 - course so we won't be discussing this
359:53 - and so this was the way that was used to
359:55 - assign public ip addresses to devices on
359:59 - the internet and allowed communication
360:01 - between devices now the problem with
360:04 - classful addressing was that with
360:06 - businesses that needed larger address
360:08 - blocks than a class c network provided
360:12 - they received a class b block which in
360:14 - most cases was much larger than required
360:17 - and the same thing happened with
360:19 - requiring more ips than class b
360:22 - and getting a class a network block this
360:25 - problem introduced a lot of wasted ips
360:28 - as there was no real middle ground and
360:30 - so this was a way to address any
360:33 - publicly routable ips now there were
360:36 - certain ranges that were allocated for
360:38 - private use and were designed to be used
360:41 - in private networks whether on-premises
360:44 - or in cloud and again they are not
360:47 - designed for public use and also didn't
360:50 - have the need to communicate over the
360:52 - public internet and so these private ip
360:55 - address spaces were standardized using
360:58 - the rfc standard 1918
361:01 - and again these ip addresses are
361:03 - designed for private use and can be used
361:06 - anywhere you like as long as they are
361:08 - still kept private chances are a network
361:11 - that you've come across whether it be a
361:13 - cloud provider your home network or
361:16 - public wi-fi will use one of these
361:18 - classes to define their network and
361:21 - these are split into three ranges
361:24 - first one being single class a with
361:26 - 10.0.0
361:29 - ending in 10.255.255.255.
361:33 - the class b range
361:35 - ranging from 172.16.0.0
361:39 - to
361:40 - 172.31 dot
361:44 - and lastly class c which was ranging
361:46 - from 192.168.0.0
361:50 - to 192.168.255.255.
361:55 - now for those networks that use these
361:57 - private ips over the public internet the
362:00 - process they would use is a process
362:02 - called network address translation
362:05 - or nat for short
362:07 - and i will be covering this in a
362:08 - different lesson later on in the section
362:11 - this method of classful addressing has
362:13 - been replaced with something a bit more
362:16 - efficient where network blocks can be
362:18 - defined more granularly and was done due
362:21 - to the internet running out of ipv4
362:24 - addresses as we needed to allocate these
362:27 - ips more efficiently now this method is
362:30 - called classless inter domain routing or
362:33 - cider for short now with cider based
362:36 - networks you aren't limited to only
362:38 - these three classes of networks class a
362:41 - b and c have been removed for something
362:44 - more efficient which will allow you to
362:47 - create networks in any one of those
362:49 - ranges cider ranges are represented by
362:53 - its starting ip address called a network
362:56 - address
362:57 - followed by what is called a prefix
363:00 - which is a slash and then a number this
363:03 - slash number represents the size of the
363:05 - network the bigger the number the
363:07 - smaller the network and the smaller the
363:10 - number the bigger the network given the
363:12 - example here
363:16 - 192.168.0.0 is the network address
363:18 - and the prefix is a slash 16. now at
363:22 - this high level it is not necessary to
363:24 - understand the math behind this but i
363:26 - will include a link in the lesson text
363:29 - for those of you who are interested in
363:31 - learning more about it all you need to
363:33 - keep in mind is as i said before
363:36 - the bigger the prefix number the smaller
363:38 - the network and the smaller the prefix
363:41 - number the bigger the network so just as
363:43 - an example the size of this slash 16
363:46 - network is represented here by this
363:49 - circle its ip range is 192.168.0.0
363:54 - ending in 192.168.255.255.
363:59 - and once you understand the math you
364:01 - will be able to tell that a slash 16
364:04 - range means that the network is the
364:07 - fixed value in the first and second
364:09 - octet the hosts on the network or the
364:12 - range are the values of anything in the
364:15 - third or fourth octets so this network
364:18 - in total will provide us with 65
364:22 - 536
364:23 - ip addresses now let's say you decided
364:26 - to create a large network such as this
364:28 - and you wanted to allocate part of it to
364:31 - another part of your business you can
364:33 - simply do so by splitting it in two
364:36 - and be left with two slash 17 networks
364:40 - so instead of one slash 16 network you
364:43 - will now have 2 17 networks and each
364:47 - network will be assigned 32
364:50 - 768 ip addresses so just to break it
364:54 - down the previous network which was
364:56 - 192.16
364:59 - forward slash 16 with the first two
365:02 - octets being the network which is
365:04 - 192.168
365:06 - it leaves the third and fourth octet to
365:09 - distribute as you like and these third
365:11 - and fourth octets
365:13 - are what you're having to create these
365:15 - two networks so looking at the blue half
365:18 - the address range will start at 0.0 and
365:21 - will end at 127.255.
365:24 - the green half will start halfway
365:26 - through the slash 16 network which will
365:29 - be
365:30 - 128.0 and end at 255.255.
365:34 - so now what if i was looking to break
365:36 - this network down even further and break
365:39 - it into four networks well using cider
365:42 - ranges this makes things fairly easy as
365:45 - i can have it again and as shown here i
365:48 - would split the two slash 17 networks to
365:52 - create four slash 18 networks so if i
365:55 - took the blue half circle and split it
365:57 - into two and then splitting the green
366:00 - half circle into
366:01 - this would leave me with four slash 18
366:04 - networks as seen here the blue quarter
366:07 - would start from 192.168.0.0
366:11 - ending with the last two octets of
366:14 - 63.255
366:16 - and the red quarter which starts from
366:18 - where the blue left off
366:20 - starting at the last two octets of 64.0
366:24 - and ending in 127.255.
366:28 - the green quarter again starting off
366:29 - with the previously defined 128.0
366:32 - network which is where the red quarter
366:34 - left off and ending with the last two
366:37 - octets being
366:39 - 191.255 and lastly the yellow quarter
366:42 - starting off from where the green
366:44 - quarter left off at 192.0 with the last
366:48 - two octets ending with
366:50 - 255.255 and so this would leave us with
366:53 - four smaller slash 18 networks broken
366:56 - down from the previous two
366:59 - 17 networks with each of these networks
367:02 - consisting of 16
367:05 - 384 ip addresses and we can continue
367:08 - this process continuously having
367:10 - networks and breaking them down into
367:12 - smaller networks this process of
367:14 - dividing each network into two smaller
367:17 - networks is known as subnetting and each
367:20 - time you subnet a network and create two
367:22 - smaller networks the number in the
367:24 - prefix will increase and so i know this
367:27 - is already a lot to take in so this
367:30 - would be a perfect time for you to grab
367:32 - a coffee or a tea and i will be ending
367:34 - part one here and part two will be
367:37 - continuing immediately after part one so
367:40 - you can now mark this lesson as complete
367:43 - and i'll see you in the next one for
367:44 - part two
367:46 - [Music]
367:50 - welcome back and in this lesson i'm
367:52 - going to be covering the second part of
367:54 - the networking refresher now part two of
367:57 - this lesson is starting immediately from
368:00 - the end of part one so with that being
368:02 - said let's dive in
368:04 - now i know this network refresher has
368:06 - been filled with a ton of numbers
368:09 - with an underlying current of math but i
368:12 - wanted you to focus on the why so that
368:15 - things will make sense later i wanted to
368:18 - introduce the hard stuff first so that
368:20 - over the length of this course you will
368:23 - be able to digest this information
368:26 - and understand where this fits into
368:29 - when discussing the different network
368:31 - parts of google cloud this will also
368:34 - help you immensely in the real world as
368:37 - well as the exam when configuring
368:40 - networks and knowing how to do the job
368:42 - of an engineer
368:44 - so getting right into it i wanted to
368:46 - just do a quick review on classless
368:48 - inter-domain routing or cider so as
368:51 - discussed in the first refresher an ipv4
368:54 - address is referenced in dotted decimal
368:57 - notation alongside the slash 16 is the
369:00 - prefix and defines how large the network
369:04 - is and so before i move on i wanted to
369:07 - give you some references that i found
369:09 - helpful in order to determine the size
369:11 - of a network and so here i have
369:13 - referenced three of the most common
369:16 - prefixes
369:17 - that i continuously run into that i
369:20 - think would be an extremely helpful
369:22 - reference for you so if you look at the
369:24 - first i p address
369:26 - 192.168.0.0
369:29 - with slash 8 as the prefix slash 8 would
369:32 - fall under a class a network 192 being
369:36 - the first octet
369:37 - as well as being the network part of the
369:39 - address would be fixed and so the host
369:42 - part of it would be anything after that
369:45 - so the address could be 192 dot anything
369:48 - and this cider range would give you over
369:50 - 16 million ip addresses the second most
369:53 - common network that i see is a slash 16
369:56 - network and this would make this ip fall
369:59 - under a class b network making the first
370:02 - two octets fixed and being the network
370:05 - part meaning that anything after 192.168
370:09 - would be the host part meaning that the
370:11 - address could be
370:14 - 192.168.anything and this would give you
370:18 - 65536 ip addresses and so for the third
370:22 - ip address which is probably the most
370:25 - common one that i see
370:26 - is a slash 24 network which falls under
370:30 - a class c network meaning that the first
370:33 - three octets are fixed and the fourth
370:35 - octet could be anything from zero to two
370:38 - five five and this would give you
370:40 - 256 ip addresses and another common one
370:44 - which is the smallest that you will see
370:46 - is a slash 32 prefix and this is one
370:50 - that i use constantly for white listing
370:53 - my ip address and because a slash 32 is
370:57 - one ip address this is a good one to
370:59 - know when you are configuring vpn for
371:02 - yourself or you're whitelisting your ip
371:05 - address from home or work and for the
371:08 - last reference as well as being the
371:11 - biggest network is the ip address of
371:14 - 0.0.0.1
371:16 - forward slash 0 which covers all ip
371:20 - addresses and you will see this commonly
371:22 - used for the internet gateway in any
371:25 - cloud environment and so these are some
371:27 - common prefixes that come up very
371:29 - frequently and so i hope this reference
371:32 - will help you now moving back to the osi
371:35 - model i've covered ipv4 in the network
371:38 - layer and so now it's time to discuss
371:41 - ipv6 now as i noted earlier
371:44 - ipv4 notation is called dotted decimal
371:48 - and each number between the dots is an
371:51 - octet with a value of 0 to 255. now
371:55 - underneath it all each octet is made up
371:58 - of an 8-bit value and having four
372:01 - numbers in an ip address that would make
372:04 - it a 32-bit value ipv6 is a much longer
372:09 - value and is represented in hexadecimal
372:12 - and each grouping is two octets which is
372:16 - 16 bits and is often referred to as a
372:19 - hextet now as these addresses are very
372:22 - long as you can see you're able to
372:24 - abbreviate them by removing redundant
372:27 - zeros so this example shown here is the
372:30 - same address as the one above it so if
372:33 - there is a sequence of zeros you can
372:35 - simply replace them with one zero so in
372:38 - this address each grouping of four zeros
372:42 - can be represented by one zero and if
372:44 - you have multiple groups of zeros in one
372:47 - address
372:48 - you can remove them all and replace them
372:50 - with double colons so each of these ipv6
372:54 - addresses that you see here are exactly
372:57 - the same now each ipv6 address is 128
373:02 - bits long and is represented in a
373:05 - similar way to
373:06 - ipv4 starting with the network address
373:10 - and ending with the prefix each hextet
373:13 - is 16 bits and the prefix number is the
373:16 - number of bits that represent the
373:18 - network with this example
373:20 - slash 64 refers to the network address
373:24 - underlined in green which is 2001
373:27 - colon de3 each hextet is 16 bits and the
373:32 - prefix is 64. so that's four groups of
373:36 - 16 and so this is how we know which part
373:39 - is the network part of the address and
373:42 - which is the host part of the address
373:44 - again notice the double colon here and
373:47 - as i explained previously any unneeded
373:50 - zeros can be replaced by a double colon
373:53 - and so this address would represent a
373:56 - slew of zeros and so adding in all the
373:58 - zeros the ipv6 starting network address
374:02 - would look like this now because the
374:05 - network address starts at 2001 colon de3
374:09 - with another two hextets of zeros as the
374:12 - network address that was determined by
374:14 - the slash 64 prefix which is four
374:17 - hextets it means a network finishes at
374:21 - that network address followed by all fs
374:24 - and so that's the process of how we can
374:27 - determine the start and end of every
374:30 - ipv6 network now as i've shown you
374:32 - before with all ipv4 addresses they are
374:36 - represented with a 0.0.0.0.0
374:42 - and because ipv6 addresses are
374:45 - represented by the same network address
374:47 - and prefix we can represent ipv6
374:50 - addresses as double colon slash zero and
374:55 - you will see this frequently when using
374:57 - ipv6 and so i know this is really
375:00 - complicated but i just wanted to give
375:02 - you the exposure of ipv6 i don't expect
375:06 - you to understand this right away
375:09 - in the end it should become a lot
375:11 - clearer as we go through the course and
375:13 - i promise you it will become a lot
375:16 - easier i had a hard time myself trying
375:18 - to understand this network concept
375:21 - but after a few days i was able to
375:23 - digest it and as i went back and did
375:26 - some practice it started to make a lot
375:29 - more sense to me and so i know as we
375:32 - move along with the course
375:33 - that it will start making sense to you
375:36 - as well so now that we've discussed
375:38 - layer 3 in the osi model i wanted to get
375:41 - into layer 4 which is the transport
375:44 - layer with ip packets discussing tcp and
375:48 - udp and so in its simplest form a packet
375:52 - is the basic unit of information in
375:54 - network transmission so most networks
375:57 - use tcpip as the network protocol or set
376:01 - of rules for communication between
376:04 - devices and the rules of tcpip require
376:08 - information to be split into packets
376:11 - that contain a segment of data to be
376:13 - transferred along with the protocol and
376:16 - its port number the originating address
376:18 - and the address of where the data is to
376:21 - be sent now udp is another protocol that
376:24 - is sent with ip and is used in specific
376:27 - applications but mostly in this course i
376:30 - will be referring to tcpip and so as you
376:33 - can see in this diagram of the ip packet
376:36 - this is a basic datagram of what a
376:39 - packet would look like again with this
376:41 - source and destination ip address
376:44 - the protocol port number and the data
376:47 - itself now this is mainly just to give
376:49 - you a high level understanding of tcpip
376:53 - and udpip and is not a deep dive into
376:56 - networking now moving on to layer 7 of
376:59 - the osi model
377:01 - this layer is used by networked
377:03 - applications or applications that use
377:06 - the internet and so there are many
377:08 - protocols that fall under this layer now
377:12 - these applications do not reside in this
377:14 - layer but use the protocols in this
377:16 - layer to function so the application
377:19 - layer provides services for networked
377:22 - applications with the help of protocols
377:25 - to perform user activities and you will
377:28 - see many of these protocols being
377:30 - addressed as we go through this course
377:33 - through resources in google cloud like
377:36 - http or https for load balancing dns
377:41 - that uses udp on port 53 and ssh on port
377:45 - 22 for logging into hosts and so these
377:49 - are just a few of the many scenarios
377:52 - where layer 7 and the protocols that
377:54 - reside in that layer come up in this
377:56 - course and we will be diving into many
377:59 - more in the lessons to come and so that
378:02 - about wraps up this networking refresher
378:04 - lesson
378:05 - and don't worry like i said before i'm
378:08 - not expecting you to pick things up in
378:10 - this first go
378:11 - things will start to make more sense as
378:14 - we go through the course and we start
378:16 - putting these networking concepts into
378:19 - practice also feel free to go back and
378:22 - review the last couple of lessons again
378:24 - if things didn't make sense to you the
378:26 - first time or if you come across some
378:29 - networking challenges in future lessons
378:32 - and so that's everything i wanted to
378:33 - cover so you can now mark this lesson as
378:36 - complete and let's move on to the next
378:38 - one
378:39 - [Music]
378:42 - welcome back in this lesson we will be
378:45 - discussing the core networking service
378:48 - of gcp virtual private cloud or vpc for
378:52 - short it is the service that allows you
378:55 - to create networks inside google cloud
378:58 - with both private and public
379:00 - connectivity options both for in-cloud
379:03 - deployments and on-premise hybrid cloud
379:06 - deployments this is a service that you
379:09 - must know well as there are many
379:11 - questions that come up on the exam with
379:14 - regards to vpcs so with that being said
379:17 - let's dive in now vpcs are what manages
379:21 - the networking functionality for your
379:23 - google cloud resources this is a
379:26 - software defined network and is not
379:29 - confined to the physical limitations of
379:32 - networking in a data center this has
379:34 - been abstracted for you vpc networks
379:38 - including their associated routes and
379:40 - firewall rules are global resources they
379:43 - are not associated with any particular
379:46 - region or zone they are global resources
379:49 - and span all available regions across
379:53 - the globe as explained earlier vpcs are
379:56 - also encapsulated within projects
379:59 - projects are the logical container where
380:02 - your vpcs live
380:04 - now these vpcs do not have ip ranges but
380:08 - are simply a construct of all of the
380:11 - individual ip addresses and services
380:14 - within that network the ip addresses and
380:17 - ranges are defined within the
380:19 - subnetworks that i will be diving into a
380:22 - bit later as well
380:24 - traffic to and from instances can be
380:27 - controlled with network firewall rules
380:30 - rules are implemented on the vms
380:33 - themselves so traffic can be controlled
380:36 - and logged as it leaves or arrives at a
380:38 - vm
380:40 - now resources within a vpc network
380:43 - can communicate with one another by
380:45 - using internal or private ipv4 addresses
380:50 - and these are subject to applicable
380:52 - network firewall rules these resources
380:55 - must be in the same vpc for
380:58 - communication
380:59 - otherwise they must traverse the public
381:02 - internet with an assigned public ip or
381:05 - use a vpc peering connection or
381:08 - establish a vpn connection another
381:11 - important thing to note is that vpc
381:13 - networks only support ipv4 unicast
381:17 - traffic they do not support ipv6 traffic
381:21 - within the network vms in the vpc
381:23 - network can only send to ipv4
381:26 - destinations
381:28 - and only receive traffic from ipv4
381:31 - sources however it is possible to create
381:34 - an ipv6 address for a global load
381:38 - balancer now unless you choose to
381:40 - disable it each new project starts with
381:43 - a default network in a vpc the default
381:47 - network is an auto mode vpc network with
381:50 - predefined subnets a subnet is allocated
381:53 - for each region with non-overlapping
381:56 - cider blocks also each default network
382:00 - has a default firewall rule
382:02 - these rules are configured to allow
382:05 - ingress traffic
382:06 - for icmp
382:08 - rdp and ssh traffic from anywhere
382:12 - as well as ingress traffic from within
382:15 - the default network for all protocols
382:18 - and ports and so there are two different
382:20 - types of vpc networks
382:23 - auto mode or custom mode an auto mode
382:26 - network also has one subnet per region
382:29 - the default network is actually an auto
382:32 - mode network as explained earlier now
382:35 - these automatically created subnets use
382:38 - a set of predefined ip ranges with a
382:41 - slash 20 cider block that can be
382:43 - expanded to a slash 16 cider block all
382:46 - of these subnets
382:48 - fit within the default
382:51 - 10.128.0.0 ford slash 9 cider block and
382:55 - as new gcp regions become available
382:58 - new subnets in those regions are
383:01 - automatically added to auto mode
383:03 - networks using an ip range on that block
383:07 - now a custom owned network does not
383:09 - automatically create subnets
383:11 - this type of network provides you with
383:13 - complete control over its subnets and ip
383:16 - ranges as well as another note an auto
383:19 - mode network can be converted to a
383:22 - custom mode network to gain more control
383:25 - but please be aware this conversion is
383:27 - one way meaning that custom networks
383:30 - cannot be changed to auto mode networks
383:32 - so when deciding on the different types
383:34 - of networks you want to use
383:36 - make sure that you review all of your
383:38 - considerations now custom mode vpc
383:41 - networks are more flexible and better
383:44 - suited to production and google
383:46 - recommends that you use custom mode vpc
383:49 - networks in production so here is an
383:52 - example of a project that contains three
383:55 - networks all of these networks span
383:58 - multiple regions across the globe as you
384:00 - can see here on the right hand side and
384:02 - each network contains separate vms and
384:06 - so this diagram is to demonstrate that
384:08 - vms that are in the same network or vpc
384:12 - can communicate privately even when
384:14 - placed in separate regions because vms
384:17 - in network a are in the same network
384:20 - they can communicate over internal ip
384:22 - addresses even though they're in
384:24 - different regions essentially your vms
384:27 - can communicate even if they exist in
384:30 - different locations across the globe as
384:32 - long as they are within the same network
384:35 - the vms in network b and network c
384:38 - are not in the same network therefore by
384:41 - default these vms must communicate over
384:44 - external ips even though they're in the
384:47 - same region as no internal ip
384:49 - communication is allowed between
384:52 - networks unless you set up vpc network
384:55 - peering or use a vpn connection now i
384:58 - wanted to bring back the focus to the
385:01 - default vpc for just a minute unless you
385:04 - create an organizational policy that
385:06 - prohibits it new projects will always
385:08 - start with a default network that has
385:11 - one subnet in each region and again this
385:13 - is an auto mode vpc network in this
385:16 - particular example i am showing a
385:18 - default vpc with seven of its default
385:21 - regions displayed along with their ip
385:24 - ranges and again i want to stress that
385:27 - vpc networks along with their associated
385:30 - routes and firewall rules are global
385:33 - resources they are not associated with
385:36 - any particular region or zone so the
385:39 - subnets within them are regional and so
385:41 - when an auto mode vpc network is created
385:45 - one subnet from each region is
385:47 - automatically created within it these
385:50 - automatically created subnets use a set
385:52 - of predefined ip ranges that fit within
385:56 - the cider block that you see here
385:58 - of 10.128.0.049
386:03 - and as new google cloud regions become
386:05 - available
386:06 - new subnets in those regions are
386:09 - automatically added to auto mode vpc
386:11 - networks
386:12 - by using an ip range from that block in
386:15 - addition to the automatically created
386:17 - subnets you can add more subnets
386:20 - manually to auto mode vpc networks in
386:23 - regions that you choose by using ip
386:26 - ranges outside of 10.128.0.049
386:32 - now if you're using a default vbc or
386:35 - have already created an auto mode vpc
386:38 - you can switch the vpc network from auto
386:41 - mode to custom mode and this is a
386:43 - one-way conversion only as custom mode
386:45 - vpc networks cannot be changed to auto
386:48 - mode vpc networks now bringing this
386:51 - theory into practice with regards to the
386:54 - default vpc
386:55 - i wanted to take the time to do a short
386:58 - demo so whenever you're ready join me in
387:01 - the console
387:02 - and so here we are back in the console
387:04 - and if i go here in the top right hand
387:06 - corner i am logged in as tony bowties at
387:10 - gmail.com and in the top drop down
387:12 - project menu i'm logged in under project
387:16 - tony and because this demo is geared
387:19 - around the default vpc i want to
387:21 - navigate to vpc networks so i'm going to
387:24 - go over here to the top left hand corner
387:26 - to the navigation menu
387:28 - and i'm going to click on it and scroll
387:30 - down
387:31 - to vpc network under networking
387:35 - and so as you can see here in the left
387:37 - hand menu there are a bunch of different
387:39 - options that i can choose from but i
387:41 - won't be touching on any of these topics
387:44 - as i have other lessons that will deep
387:47 - dive into those topics so in this demo
387:50 - i'd like to strictly touch on the
387:52 - default vpc and as you can see in
387:55 - project tony it has created a default
387:58 - vpc for me with a one subnet in every
388:01 - region having its own ip address range
388:04 - and so just as a reminder whenever you
388:07 - create a new project a default vpc will
388:11 - be automatically created for you and
388:13 - when these subnets were created each of
388:16 - them have a route out to the public
388:18 - internet and so the internet gateway is
388:21 - listed here its corresponding firewall
388:23 - rules along with global dynamic routing
388:26 - and flow logs are turned off and again i
388:29 - will be getting deeper into routing and
388:31 - flow logs in later lessons in the
388:34 - section now earlier i had pointed out
388:36 - that an auto mode vpc can be converted
388:39 - to a custom vpc and it's as simple as
388:42 - clicking this button but we don't want
388:44 - to do that just yet and what i'd like to
388:46 - do is drill down into the default vbc
388:49 - and show you all the different options
388:51 - as you can see here the dns api has not
388:54 - been enabled and so for most of you a
388:57 - good idea would be to enable it and so
388:59 - i'm going to go ahead and do that now as
389:01 - well you can see here that i can make
389:03 - adjustments to each of the different
389:05 - subnets or i can change the
389:07 - configuration of the vpc itself so if i
389:10 - click on this edit button here at the
389:12 - top i'm able to change the subnet
389:15 - creation mode along with the dynamic
389:17 - routing mode which i will get into in a
389:19 - later lesson and the same thing with the
389:21 - dns server policy and so to make this
389:24 - demo a little bit more exciting i want
389:26 - to show you the process on how to expand
389:29 - a subnet so i'm going to go into us
389:31 - central one i'm going to drill down here
389:34 - and here's all the configuration
389:36 - settings for the default subnet in the
389:39 - us central one region and so for me to
389:41 - edit this subnet i can simply click on
389:44 - the edit button up here at the top and
389:46 - so right below the ip address range i am
389:49 - prompted with a note saying that the ip
389:52 - ranges must be unique and
389:54 - non-overlapping as we stated before and
389:57 - this is a very important point to know
389:59 - when you're architecting any vpcs or its
390:02 - corresponding sub networks and so i'm
390:05 - going to go ahead and change the subnet
390:07 - from a cider range of 20
390:09 - and i'm going to change it to 16. i'm
390:11 - not going to add any secondary ip ranges
390:14 - i'm going to leave private google access
390:16 - off and so i'm going to leave everything
390:18 - else as is
390:20 - and simply click on save and so once
390:22 - this has completed i'll be able to see
390:25 - that my subnet range will go from a
390:27 - slash 20 to a slash 16. and so here you
390:31 - can see the ip address range has now
390:34 - changed to a slash 16. if i go back to
390:37 - the main page of the vpc network i can
390:39 - see that the ip address range is
390:42 - different from all the other ones now
390:44 - you're probably asking why can't i just
390:46 - change the ip address range on all the
390:49 - subnets at once and so even though i'd
390:51 - love to do that
390:52 - unfortunately google does not give you
390:55 - the option each subnet must be
390:57 - configured one by one to change the ipa
391:00 - address range now i wanted to quickly
391:02 - jump into the default firewall rules and
391:05 - as discussed earlier the rules for
391:08 - incoming ssh
391:09 - rdp
391:11 - and icmp have been pre-populated
391:14 - along with a default rule that allows
391:17 - incoming connections for all protocols
391:20 - and ports
391:21 - among instances within the same network
391:24 - so when it comes to routes with regards
391:26 - to the vpc network the only one i really
391:28 - wanted to touch on is the default route
391:31 - to the internet and so without this
391:33 - route any of the subnets in this vpc
391:36 - wouldn't have access to route traffic to
391:39 - the internet and so when the default vpc
391:41 - is created the default internet gateway
391:44 - is also created and so now going back to
391:46 - the main page of the vpc network i
391:49 - wanted to go through the process
391:51 - of making the ip address range bigger
391:53 - but doing it through the command line
391:55 - and so i'm going to go up to the right
391:57 - hand corner and open up cloud shell i'm
391:59 - going to make this a little bit bigger
392:01 - and so for this demo i'm going to
392:03 - increase the address range for the
392:05 - subnet in us west one from a slash 20 to
392:09 - a slash 16 and so i'm going to paste in
392:11 - the command
392:12 - which is gcloud compute networks
392:15 - subnets expand
392:17 - ip dash range and then the name of the
392:20 - network which is default as well as the
392:23 - region and i'm going to do uswest1 along
392:26 - with the prefix length which is going to
392:28 - be 16. so i'm going to hit enter
392:31 - i've been prompted to make sure that
392:33 - this is what i want to do and so yes i
392:35 - do want to continue so i'm going to type
392:37 - in y for yes and hit enter and so within
392:40 - a few seconds i should get some
392:41 - confirmation and as expected my subnet
392:44 - has been updated and so because i like
392:47 - to verify everything i'm going to now
392:49 - clear the screen and i'm going to paste
392:51 - in the command gcloud compute networks
392:55 - subnets describe and then the subnet
392:57 - name which is default along with the
392:59 - region which would be uswest1 i'm going
393:02 - to click on enter and as you can see
393:04 - here the ipsider range is consistent
393:07 - with what we have changed and if i do a
393:10 - quick refresh on the browser
393:12 - i'll be able to see that the console has
393:15 - reflected the same thing and as expected
393:18 - the ip address range here for us west
393:20 - one in the console reflects that which
393:24 - we see here in cloud shell and so now to
393:26 - end this demo i wanted to quickly show
393:29 - you how i can delete the default vpc and
393:32 - recreate it so all i need to do is to
393:35 - drill into the settings
393:37 - and then click on delete vpc network
393:39 - right here at the top i'm going to get a
393:41 - prompt to ask me if i'm sure and i'm
393:43 - going to simply click on delete now just
393:46 - as a note if you have any resources that
393:49 - are in any vpc networks you will not be
393:52 - able to delete the vpc you would have to
393:54 - delete the resources first and then
393:57 - delete the vpc afterwards okay and it
394:00 - has been successfully deleted and as you
394:02 - can see there are no local vpc networks
394:05 - in this current project and so i want to
394:07 - go ahead and recreate the default vpc so
394:11 - i'm going to simply click on create vpc
394:13 - network and so here i'm prompted to
394:16 - enter in a bunch of information
394:18 - for creating this new vpc network and so
394:21 - keeping with the spirit of default vpcs
394:24 - i'm going to name this vpc default
394:28 - i'm going to put default in the
394:30 - description and under subnet creation
394:32 - mode i'm going to click on automatic and
394:35 - as you can see a prompt came up telling
394:37 - me these ip address ranges will be
394:40 - assigned to each region in your vpc
394:43 - network and i'm able to review the ip
394:45 - address ranges for each region and as
394:48 - stated before the ip address ranges for
394:51 - each region will always be the same
394:54 - every time i create
394:56 - this default vpc or create a vpc in the
395:00 - automatic subnet creation mode
395:02 - now as a note here under firewall rules
395:06 - if i don't select these firewall rules
395:09 - none will actually be created so if
395:11 - you're creating a new default vpc be
395:14 - sure to check these off and so i'm going
395:16 - to leave everything else as is and i'm
395:18 - going to simply go to the bottom and
395:20 - click on the create button and within
395:22 - about a minute i should have the new
395:24 - default vpc created okay and we are back
395:28 - in business the default vpc has been
395:31 - recreated with all of these subnets in
395:34 - its corresponding regions all the ip
395:36 - address ranges the firewall rules
395:39 - everything that we saw earlier in the
395:41 - default vpc and so that's pretty much
395:44 - all i wanted to cover in this demo on
395:46 - the default vpc network along with the
395:49 - lesson on vpcs so you can now mark this
395:52 - lesson as complete and let's move on to
395:54 - the next
395:58 - one welcome back and in this lesson i'm
396:02 - going to be discussing vpc network
396:05 - subnets now the terms subnet and sub
396:08 - network are synonymous and are used
396:10 - interchangeably in google cloud as
396:12 - you'll hear me using either one in this
396:14 - lesson yet i am referring to the same
396:16 - thing now when you create a resource in
396:19 - google cloud you choose a network and a
396:22 - subnet and so because a subnet is needed
396:25 - before creating resources some good
396:27 - knowledge behind it is necessary for
396:30 - both building and google cloud as well
396:33 - as in the exam so in this lesson i'll be
396:36 - covering subnets at a deeper level with
396:39 - all of its features and functionality so
396:41 - with that being said let's dive in
396:45 - now each vpc network consists of one or
396:48 - more useful ip range partitions
396:51 - called subnets also known in google
396:54 - cloud as sub networks each subnet is
396:58 - associated with the region and vpc
397:00 - networks do not have any ip address
397:03 - ranges associated with them ip ranges
397:07 - are defined for the subnets a network
397:10 - must have at least one subnet before you
397:12 - can use it and as mentioned earlier when
397:15 - you create a project it will create a
397:17 - default vpc network with subnets in each
397:21 - region automatically auto mode will run
397:24 - under this same functionality now custom
397:27 - vpc networks on the other hand start
397:30 - with no subnets giving you full control
397:32 - over subnet creation and you can create
397:35 - more than one subnet per region you
397:37 - cannot change the name or region of a
397:39 - subnet after you've created it you would
397:42 - have to delete the subnet and replace it
397:45 - as long as no resources are using it
397:48 - primary and secondary ranges for subnets
397:51 - cannot overlap with any allocated range
397:54 - any primary or secondary range of
397:57 - another subnet in the same network
397:59 - or any ip ranges of subnets in peered
398:03 - networks in other words they must be a
398:05 - unique valid cider block now when it
398:08 - comes to ip addresses of a subnet google
398:11 - cloud vpc has an amazing feature that
398:15 - lets you increase the ip space of any
398:18 - subnets without any workload shutdown or
398:21 - downtime as demonstrated earlier in the
398:24 - previous lesson and this gives you the
398:26 - flexibility and growth options to meet
398:29 - your needs but unfortunately there are
398:32 - some caveats the new subnet must not
398:35 - overlap with other subnets in the same
398:38 - vpc network in any region also the new
398:42 - subnets must stay inside the rfc 1918
398:46 - address space the new network range must
398:49 - be larger than the original which means
398:52 - the prefix length must be smaller in
398:54 - number and once a subnet has been
398:56 - expanded you cannot undo an expansion
399:00 - now auto mode network starts with a
399:02 - slash 20 range that can be expanded to a
399:06 - 16 ip range but not larger you can also
399:11 - convert the auto mode network to a
399:13 - custom mode network to increase the ip
399:16 - range even further and again this is a
399:19 - one-way conversion
399:20 - custom mode vpc networks cannot be
399:23 - changed to auto mode vpc networks
399:26 - now in any network that is created in
399:29 - google cloud
399:30 - there will always be some ip addresses
399:33 - that you will not be able to use and
399:35 - these are reserved for google and so
399:38 - every subnet has four reserved ip
399:40 - addresses in its primary ip range and
399:44 - just as a note there are no reserved ip
399:47 - addresses in the secondary ip ranges and
399:50 - these reserved ips can be looked at as
399:53 - the first two and the last two ip
399:55 - addresses in the cider range now the
399:58 - first address in the primary ip range
400:01 - for the subnet is reserved for the
400:03 - network the second address in the
400:05 - primary ip range for the subnet is
400:08 - reserved for the default gateway and
400:10 - allows you access to the internet the
400:13 - second to last address in the primary ip
400:16 - range for the subnet is reserved for
400:18 - google cloud for potential future use
400:21 - and the last address and the ip range
400:23 - for the subnet is for broadcast
400:26 - and so that about covers this short yet
400:29 - important lesson on vpc network subnets
400:32 - these features and functionalities of
400:35 - subnets that have been presented to you
400:37 - will help you make better design
400:40 - decisions that will give you a bit more
400:42 - knowledge and flexibility when it comes
400:45 - to assigning ipspace within your vpc
400:48 - networks and so that's all i have to
400:50 - cover for this lesson so you can now
400:52 - mark this lesson as complete and let's
400:55 - move on to the next one
400:57 - [Music]
401:01 - welcome back and in this lesson i'm
401:03 - going to be going through
401:05 - routing and private google access
401:08 - now although routing doesn't really show
401:10 - up in the exam i wanted to give you an
401:12 - inside look on how traffic is routed so
401:16 - when you're building in google cloud
401:18 - you'll know exactly what you will need
401:20 - to do if you need to edit these routes
401:22 - in any way or if you need to build new
401:25 - ones to satisfy your particular need now
401:28 - private google access does pop its head
401:30 - in the exam but only at a high level but
401:33 - i wanted to get just a bit deeper with
401:36 - the service and get into the data flow
401:38 - of when the service is enabled so with
401:41 - that being said let's dive in now google
401:44 - cloud routes define the paths that
401:47 - network traffic takes from a vm instance
401:50 - to other destinations these destinations
401:53 - can be inside your google cloud vpc
401:56 - network for example in another vm or
401:59 - outside it in a vpc network a route
402:01 - consists of a single destination
402:04 - and a single next hop when an instance
402:07 - in a vpc network sends a packet google
402:09 - cloud delivers the packet to the route's
402:12 - next hop if the packet's destination
402:15 - address is within the route's
402:17 - destination range
402:18 - and so all these routes are stored in
402:21 - the routing table for the vpc now for
402:24 - those of you who are not familiar with a
402:26 - routing table in computer networking a
402:28 - routing table is a data table stored in
402:31 - a router or a network host that lists
402:34 - the routes to particular network
402:37 - destinations and so in this case the vpc
402:40 - is responsible for storing the routing
402:43 - table as well each vm instance has a
402:46 - controller that is kept informed of all
402:49 - applicable routes
402:51 - from the network's routing table each
402:53 - packet leaving a vm
402:55 - is delivered to the appropriate next hop
402:58 - of an applicable route based on a
403:00 - routing order now i wanted to take a
403:02 - couple minutes to go through the
403:04 - different routing types that are
403:06 - available on google cloud now in google
403:09 - cloud there are two types of routing
403:12 - there is the system generated which
403:14 - offers the default and subnet route and
403:17 - then there are the custom routes which
403:20 - support static routes and dynamic routes
403:23 - and so i first wanted to cover system
403:26 - generated routes in a little bit of
403:28 - depth and so every new network whether
403:31 - it be an automatic vpc or a custom vpc
403:35 - has two types of system generated routes
403:38 - a default route which you can remove or
403:40 - replace and one subnet route for each of
403:42 - its subnets now when you create a vpc
403:45 - network google cloud creates a system
403:48 - generated default route and this route
403:51 - serves two purposes it defines the path
403:54 - out of the vpc network including the
403:57 - path to the internet in addition to
403:59 - having this route instances must meet
404:02 - additional requirements if they need
404:04 - internet access the default route also
404:07 - provides a standard path for private
404:10 - google access and if you want to
404:12 - completely isolate your network from the
404:14 - internet or if you need to replace the
404:17 - default route with the custom route you
404:19 - can delete the default route now if you
404:22 - remove the default route and do not
404:23 - replace it packets destined to ip ranges
404:27 - that are not covered by other routes are
404:29 - dropped lastly the system generated
404:32 - default route has a priority of 1000
404:36 - because its destination is the broadest
404:39 - possible which covers all ip addresses
404:42 - in the
404:45 - 0.0.0.0.0 range google cloud only uses
404:48 - it if a route with a more specific
404:50 - destination does not apply to a packet
404:53 - and i'll be getting into priorities in
404:56 - just a little bit and so now that we've
404:58 - covered the default route i wanted to
405:00 - get into the subnet route now subnet
405:02 - routes are system generated routes that
405:05 - define paths to each subnet in the vpc
405:08 - network each subnet has at least one
405:11 - subnet route whose destination matches
405:14 - the primary ip range of the subnet if
405:18 - the subnet has secondary ip ranges
405:20 - google cloud creates a subnet route with
405:23 - a corresponding destination for each
405:25 - secondary range no other route can have
405:28 - a destination that matches
405:30 - or is more specific than the destination
405:33 - of a subnet route you can create a
405:36 - custom route that has a broader
405:38 - destination range that contains the
405:40 - subnet route's destination range now
405:43 - when a subnet is created a corresponding
405:46 - subnet route for the subnet's primary
405:48 - and secondary ip range is also created
405:52 - auto mode vpc networks create a subnet
405:55 - route for the primary ip ranges of each
405:58 - of their automatically created subnets
406:01 - you can delete these subnets but only if
406:03 - you convert the auto mode vpc network to
406:06 - custom mode and you cannot delete a
406:08 - subnet route unless you modify or delete
406:12 - the subnet so when you delete a subnet
406:14 - all subnet routes for both primary and
406:17 - secondary ranges are deleted
406:19 - automatically you cannot delete the
406:21 - subnet route for the subnet's primary
406:24 - range in any other way and just as a
406:26 - note when networks are connected by
406:29 - using vpc network peering which i will
406:31 - get into a little bit later some subnet
406:34 - routes from one network are imported
406:37 - into the other network and vice versa
406:39 - and cannot be removed unless you break
406:42 - the peering relationship and so when you
406:44 - break the peering relationship all
406:46 - imported subnet routes from the other
406:49 - network are automatically removed so now
406:52 - that we've covered the system generated
406:54 - routes i wanted to get into custom
406:56 - routes now custom routes are either
406:59 - static routes that you can create
407:01 - manually or dynamic routes maintained
407:04 - automatically by one or more of your
407:07 - cloud routers and these are created on
407:10 - top of the already created system
407:12 - generated routes destinations for custom
407:15 - routes cannot match or be specific than
407:18 - any subnet route in the network now
407:21 - static routes can use any of the static
407:23 - route next hops and these can be created
407:27 - manually if you use the google cloud
407:29 - console to create a cloud vpn tunnel
407:31 - that uses policy-based routing or one
407:34 - that is a route based vpn static routes
407:37 - for the remote traffic selectors are
407:40 - created for you and so just to give you
407:42 - a little bit more clarity and a little
407:44 - bit of context i've included a
407:46 - screenshot here for all the different
407:48 - routes that are available for the next
407:51 - hop we have the default internet gateway
407:54 - to define a path to external ip
407:56 - addresses specify an instance and this
407:59 - is where traffic is directed to the
408:01 - primary internal ip address of the vm's
408:04 - network interface in the vpc network
408:07 - where you define the route specify ip
408:09 - address is where you provide an internal
408:12 - ip address assigned to a google cloud vm
408:16 - as a next hop for cloud vpn tunnels that
408:19 - use policy based routing and route-based
408:22 - vpns you can direct traffic to the vpn
408:25 - tunnel by creating routes whose next
408:28 - hops refer to the tunnel by its name and
408:31 - region and just as a note google cloud
408:34 - ignores routes whose next hops are cloud
408:37 - vpn tunnels that are down and lastly for
408:40 - internal tcp and udp low balancing you
408:44 - can use a load balancer's ip address as
408:47 - a next hop that distributes traffic
408:50 - among healthy back-end instances custom
408:52 - static routes that use this next hop
408:55 - cannot be scoped to specific instances
408:58 - by network tags and so when creating
409:01 - static routes you will always be asked
409:04 - for different parameters that are needed
409:07 - in order to create this route and so
409:09 - here i've taken a screenshot from the
409:11 - console to give you a bit more context
409:14 - with regards to the information that's
409:16 - needed so first up is the name and
409:18 - description
409:20 - so these fields identify the route a
409:22 - name is required but a description is
409:25 - optional and every route in your project
409:28 - must have a unique name next up is the
409:31 - network and each route must be
409:33 - associated with exactly one vpc network
409:36 - in this case it happens to be the
409:38 - default network but if you have other
409:40 - networks available you're able to click
409:42 - on the drop down arrow and choose a
409:45 - different network the destination range
409:47 - is a single ipv4 cider block that
409:50 - contains the ip addresses of systems
409:53 - that receive incoming packets and the ip
409:56 - range must be entered as a valid ipv4
410:00 - cider block as shown in the example
410:02 - below the field now if multiple routes
410:05 - have identical destinations priority is
410:09 - used to determine which route should be
410:11 - used so a lower number would indicate a
410:14 - higher priority for example a route with
410:17 - a priority value of 100 has a higher
410:21 - priority than one with a priority value
410:23 - of 200 so the highest route priority
410:27 - means the smallest possible non-negative
410:29 - number as well another great example is
410:32 - if you look back on your default routes
410:35 - all your subnet routes are of a priority
410:37 - of zero and the default internet gateway
410:40 - is of a priority of 1000 and therefore
410:44 - the subnet routes will take priority
410:46 - over the default internet gateway and
410:48 - this is due to the smaller number so
410:50 - remember a good rule of thumb is that
410:53 - the lower the number the higher the
410:55 - priority the higher the number the lower
410:58 - the priority now to get a little bit
411:00 - more granular you can specify a list of
411:03 - network tags so that the route only
411:05 - applies to instances that have at least
411:08 - one of the listed tags and if you don't
411:11 - specify any tags then google cloud
411:14 - applies the route to all instances in
411:17 - the network and finally next hop which
411:20 - was shown previously this is dedicated
411:22 - to static routes that have next hops
411:25 - that point to the options shown earlier
411:28 - so now that i've covered static routes
411:30 - in a bit of detail i want to get into
411:32 - dynamic routes now dynamic routes are
411:35 - managed by one or more cloud routers and
411:38 - this allows you to dynamically exchange
411:40 - routes between a vpc network and an
411:43 - on-premises network with dynamic routes
411:46 - their destinations always represent ip
411:49 - ranges outside of your vpc network
411:52 - and their next hops are always bgp peer
411:56 - addresses a cloud router can manage
411:59 - dynamic routes for cloud vpn tunnels
412:02 - that use dynamic routing as well as
412:04 - cloud interconnect and don't worry i'll
412:07 - be getting into cloud routers in a bit
412:09 - of detail in a later lesson now i wanted
412:12 - to take a minute to go through routing
412:14 - order and the routing order deals with
412:17 - priorities that i touched on a little
412:19 - bit earlier now subnet routes are always
412:22 - considered first because google cloud
412:24 - requires that subnet routes have the
412:27 - most specific destinations matching the
412:30 - ip address ranges of their respective
412:32 - subnets if no applicable destination is
412:36 - found google cloud drops the packet and
412:39 - replies with a network unreachable error
412:42 - system generated routes apply to all
412:45 - instances in the vpc network the scope
412:48 - of instances to which subnet routes
412:50 - apply cannot be altered although you can
412:53 - replace the default route and so just as
412:56 - a note custom static routes apply to all
412:59 - instances or specific instances so if
413:02 - the route doesn't have a network tag the
413:04 - route applies to all instances in the
413:07 - network now vpc networks have special
413:10 - routes that are used for certain
413:12 - services and these are referred to as
413:14 - special return paths in google cloud
413:17 - these routes are defined outside of your
413:19 - vpc network in google's production
413:22 - network they don't appear in your vpc
413:24 - network's routing table you cannot
413:26 - remove them or override them or if you
413:29 - delete or replace a default route in
413:31 - your vpc network although you can
413:33 - control traffic to and from these
413:36 - services by using firewall rules and the
413:39 - services that are covered are load
413:41 - balancers internet aware proxy or iap as
413:45 - well as cloud dns and so before i end
413:48 - this lesson i wanted to touch on private
413:51 - google access now vm instances that only
413:54 - have internal ip addresses can use
413:58 - private google access and this allows
414:00 - them to reach the external ip addresses
414:03 - of google's apis and services the source
414:07 - ip address of the packet can be the
414:09 - primary internal ip address of the
414:12 - network interface or an address in an
414:15 - alias ip range that is assigned to the
414:18 - interface if you disable private google
414:20 - access the vm instances can no longer
414:24 - reach google apis and services and will
414:27 - only be able to send traffic within the
414:29 - vpc network private google access has no
414:33 - effect on instances that have
414:36 - external ip addresses and can still
414:39 - access the internet they don't need any
414:41 - special configuration to send requests
414:44 - to the external ip addresses of google
414:47 - apis and services you enable private
414:50 - google access on a subnet by subnet
414:52 - basis and it's a setting for subnets in
414:55 - a vpc network and i will be showing you
414:57 - this in an upcoming demo where we'll be
415:00 - building our own custom vpc network now
415:03 - even though the next hop for the
415:04 - required routes is called the default
415:07 - internet gateway
415:08 - and the ip addresses for google apis and
415:11 - services are external requests to google
415:14 - apis and services from vms that only
415:17 - hold internal ip addresses in subnet 1
415:20 - where private google access is enabled
415:23 - are not sent through the public internet
415:25 - those requests stay within google's
415:28 - network as well vms that only have
415:31 - internal ip addresses do not meet the
415:34 - internet access requirements
415:36 - for access to other external ip
415:39 - addresses
415:40 - beyond those for google apis and
415:42 - services now touching on this diagram
415:45 - here
415:46 - firewall rules in the vpc network have
415:48 - been configured to allow internet access
415:52 - vm1 can access google apis and services
415:56 - including cloud storage because its
415:58 - network interface is located in subnet 1
416:01 - which has private google access enabled
416:04 - and because this instance only has an
416:06 - internal ip address
416:08 - private google access applies to this
416:10 - instance now with vm2 it can also access
416:14 - google apis and services including cloud
416:16 - storage because it has an external ip
416:19 - address private google access has no
416:22 - effect on this instance as it has an
416:25 - external ip address and private google
416:27 - access has not been enabled on that
416:30 - subnet and because both of these
416:32 - instances are in the same network they
416:34 - are still able to communicate with each
416:36 - other over an internal subnet route and
416:39 - so this is just one way where private
416:41 - google access can be applied there are
416:43 - some other options for private access as
416:46 - well you can use private google access
416:48 - to connect to google apis and services
416:51 - from your on-premises network through a
416:54 - cloud vpn tunnel or cloud interconnect
416:57 - without having any external ip addresses
417:00 - you also have the option of using
417:02 - private google access through a vpc
417:05 - network peering connection which is
417:07 - known as private services access and
417:10 - finally the last option available for
417:12 - private google access is connecting
417:14 - directly from serverless google services
417:17 - through an internal vpc connection now i
417:20 - know this has been a lot of theory to
417:22 - take in but i promise it'll become a lot
417:25 - easier and concepts will become less
417:27 - complicated when we start putting this
417:30 - into practice coming up soon in the demo
417:33 - of building our own custom vpc and so
417:36 - that's pretty much all i wanted to cover
417:38 - when it comes to routing and private
417:40 - google access so you can now mark this
417:42 - lesson as complete and let's move on to
417:45 - the next one
417:46 - [Music]
417:50 - welcome back and in this lesson i'm
417:52 - going to be discussing ip addressing now
417:55 - in the network refresher lesson i went
417:57 - into a bit of depth on how i p addresses
418:00 - are broken down and used for
418:02 - communication in computer networks in
418:05 - this lesson i'll be getting into the
418:07 - available types of ip addressing in
418:10 - google cloud and how they are used in
418:13 - each different scenario please note for
418:15 - the exam a high level overview will be
418:18 - needed to know when it comes to ip
418:21 - addressing but the details behind it
418:23 - will give you a better understanding on
418:25 - when to use each type of ip address so
418:28 - with that being said let's dive in
418:31 - now ip addressing in google cloud holds
418:34 - quite a few categories
418:36 - and really start by determining whether
418:39 - you are planning for communication
418:41 - internally within your vpc or for
418:44 - external use to communicate with the
418:46 - outside world through the internet once
418:49 - you determine the type of communication
418:52 - that you're looking to apply between
418:54 - resources some more decisions need to be
418:57 - made with regards to the other options
418:59 - and i will be going through these
419:01 - options in just a sec now in order to
419:03 - make these options a little bit more
419:05 - digestible i wanted to start off with
419:07 - the options available for internal ip
419:10 - addresses
419:11 - now internal ip addresses are not
419:14 - publicly advertised they are used only
419:17 - within a network now every vpc network
419:20 - or on-premises network has at least one
419:23 - internal ip address range resources with
419:26 - internal ip addresses communicate with
419:29 - other resources as if they're all on the
419:32 - same private network now every vm
419:35 - instance can have one primary internal
419:38 - ip address that is unique to the vpc
419:41 - network and you can assign a specific
419:43 - internal ip address when you create a vm
419:47 - instance or you can reserve a static
419:50 - internal ip address for your project and
419:53 - assign that address to your resources if
419:56 - you don't specify an address one will be
419:58 - automatically assigned to the vm in
420:01 - either case the address must belong to
420:04 - the ip range of the subnet and so if
420:06 - your network is an auto mode vpc network
420:10 - the address comes from the region subnet
420:12 - if your network is a custom mode vpc
420:15 - network you must specify which subnet
420:17 - the ip address comes from now all
420:20 - subnets have a primary sider range which
420:23 - is the range of internal ip addresses
420:26 - that define the subnet each vm instance
420:29 - gets its primary internal ip address
420:32 - from this range you can also allocate
420:35 - alias ip ranges from that primary range
420:39 - or you can add a secondary range to the
420:41 - subnet and allocate alias ip ranges from
420:45 - the secondary range use of alias ip
420:48 - ranges does not require secondary subnet
420:51 - ranges these secondary subnet ranges
420:54 - merely provide an organizational tool
420:57 - now when using ip aliasing you can
421:00 - configure multiple internal ip addresses
421:03 - representing containers or applications
421:06 - hosted in a vm without having to define
421:09 - a separate network interface and you can
421:11 - assign vm alias ip ranges from either
421:14 - the subnet's primary or secondary ranges
421:17 - when alias ip ranges are configured
421:20 - google cloud automatically installs vpc
421:23 - network routes for primary and alias ip
421:27 - ranges for the subnet of your primary
421:30 - network interface your container
421:32 - orchestrator or gke does not need to
421:35 - specify vpc network connectivity for
421:38 - these routes and this simplifies routing
421:41 - traffic and managing your containers now
421:44 - when choosing either an auto mode vpc or
421:47 - a custom vpc you will have the option to
421:50 - choose either an ephemeral ip or a
421:53 - static ip now an ephemeral ip address is
421:56 - an ip address that doesn't persist
421:59 - beyond the life of the resource for
422:01 - example when you create an instance or
422:03 - forwarding rule without specifying an ip
422:06 - address google cloud will automatically
422:08 - assign the resource an ephemeral ip
422:11 - address and this ephemeral ip address is
422:14 - released when you delete the resource
422:17 - when the ip address is released it is
422:20 - free to eventually be assigned to
422:22 - another resource so is never a great
422:24 - option if you depend on this ip to
422:26 - remain the same this ephemeral ip
422:29 - address can be automatically assigned
422:32 - and will be assigned from the selected
422:34 - region subnet as well if you have
422:37 - ephemeral ip addresses that are
422:39 - currently in use
422:41 - you can promote these addresses to
422:43 - static internal ip addresses so that
422:46 - they remain with your project until you
422:48 - actively remove them and just as a note
422:51 - before you reserve an existing ip
422:53 - address you will need the value of the
422:56 - ip address that you want to promote now
422:58 - reserving a static ip address
423:01 - assigns the address to your project
423:03 - until you explicitly release it this is
423:06 - useful if you are dependent on a
423:08 - specific ip address for a specific
423:11 - service and need to prevent another
423:14 - resource from being able to use the same
423:16 - address static addresses are also useful
423:19 - if you need to move an ip address from
423:22 - one google cloud resource to another and
423:25 - you also have the same options when
423:27 - creating an internal load balancer as
423:30 - you do with vm instances and so now that
423:33 - we've covered all the options for
423:35 - internal ip addresses i would like to
423:38 - move on to cover all the available
423:40 - options for external ip addresses now
423:43 - you can assign an external ip address to
423:46 - an instance or a forwarding rule if you
423:49 - need to communicate with the internet
423:51 - with resources in another network or
423:54 - need to communicate with a public google
423:56 - cloud service sources from outside a
423:59 - google cloud vpc network can address a
424:02 - specific resource by the external ip
424:05 - address as long as firewall rules enable
424:08 - the connection and only resources with
424:11 - an external ip address can send and
424:14 - receive traffic directly to and from
424:16 - outside the network and like internal ip
424:19 - addresses external ip addresses have the
424:22 - option of choosing from an ephemeral or
424:25 - static ip address now an ephemeral
424:28 - external ip address is an ip address
424:32 - that doesn't persist beyond the life of
424:34 - the resource and so follows the same
424:36 - rules as ephemeral internal ip addresses
424:40 - so when you create an instance or
424:42 - forwarding rule without specifying an ip
424:44 - address the resource is automatically
424:47 - assigned an ephemeral external ip
424:49 - address and this is something that you
424:51 - will see quite often ephemeral external
424:54 - ip addresses are released from a
424:56 - resource if you delete the resource for
424:59 - vm instances the ephemeral external ip
425:03 - address is also released if you stop the
425:05 - instance so after you restart the
425:08 - instance it is assigned a new ephemeral
425:10 - external ip address and if you have an
425:13 - existing vm that doesn't have an
425:15 - external ip address you can assign one
425:18 - to it forwarding rules always have an ip
425:20 - address whether external or internal so
425:23 - you don't need to assign an ip address
425:26 - to a forwarding rule after it is created
425:29 - and if your instance has an ephemeral
425:31 - external ip address and you want to
425:33 - permanently assign the ip to your
425:36 - project like ephemeral internal ip
425:38 - addresses you have the option to promote
425:41 - the ip address from ephemeral to static
425:44 - and in this case promoting an ephemeral
425:47 - external ip address to a static external
425:50 - ip address now when assigning a static
425:53 - ip address these are assigned to a
425:56 - project long term until they are
425:58 - explicitly released from that assignment
426:01 - and remain attached to a resource until
426:04 - they are explicitly detached for vm
426:07 - instances static external ip addresses
426:10 - remain attached to stopped instances
426:13 - until they are removed and this is
426:15 - useful if you are dependent on a
426:17 - specific ip address for a specific
426:20 - service like a web server or a global
426:23 - load balancer that needs access to the
426:25 - internet static external ip addresses
426:28 - can be either a regional or global
426:31 - resource
426:32 - in a regional static ip address allows
426:35 - resources of that region
426:37 - or resources of zones within that region
426:40 - to use the ip address and just as a note
426:43 - you can use your own publicly routable
426:46 - ip address prefixes as google cloud
426:49 - external ip addresses and advertise them
426:52 - on the internet the only caveat is that
426:55 - you must own and bring at the minimum a
426:58 - 24 cider block and so now that we've
427:01 - discussed internal and external ip
427:04 - addressing options i wanted to move into
427:06 - internal ip address reservations now
427:09 - static internal ips provide the ability
427:12 - to reserve internal ip addresses
427:15 - from the ip range configured in the
427:18 - subnet then assign those reserved
427:20 - internal addresses to resources as
427:23 - needed reserving an internal ip address
427:26 - takes that address out of the dynamic
427:28 - allocation pool and prevents it from
427:30 - being used for automatic allocations
427:33 - with the ability to reserve static
427:35 - internal ip addresses you can always use
427:39 - the same ip address for the same
427:41 - resource even if you have to delete and
427:44 - recreate the resource so when it comes
427:47 - to internal ip address reservation you
427:50 - can either reserve a static internal ip
427:53 - address before creating the associated
427:55 - resource or you can create the resource
427:58 - with an ephemeral internal ip address
428:01 - and then promote that ephemeral ip
428:04 - address to a static internal ip address
428:07 - and so just to give you a bit more
428:09 - context i have a diagram here to run you
428:11 - through it so in the first example you
428:14 - would create a subnet from your vpc
428:16 - network you would then reserve an
428:18 - internal ip address from that subnet's
428:22 - primary ip range and in this diagram is
428:25 - marked as 10.12.4.3
428:28 - and will be held as reserved for later
428:31 - use with a resource and then when you
428:33 - decide to create a vm instance or an
428:36 - internal load balancer you can use the
428:39 - reserved ip address that was created in
428:42 - the previous step that i p address then
428:45 - becomes marked as reserved and in use
428:48 - now touching on the second example you
428:50 - would first create a subnet from your
428:52 - vpc network
428:54 - you would then create a vm instance or
428:57 - an internal load balancer with either an
429:00 - automatically allocated ephemeral ip
429:02 - address or a specific ip address that
429:05 - you've chosen from within that specific
429:08 - subnet and so once the ephemeral ip
429:10 - address is in use you can then promote
429:13 - the ephemeral ip address to a static
429:16 - internal ip address and would then
429:18 - become reserved and in use now when it
429:21 - comes to the external ip address
429:24 - reservation
429:25 - you are able to obtain a static external
429:28 - ip address by using one of the following
429:30 - two options you can either reserve a new
429:33 - static external ip address and then
429:35 - assign the address to a new vm instance
429:38 - or you can promote an existing ephemeral
429:41 - external ip address to become a static
429:44 - external ip address now in the case of
429:46 - external ip addresses you can reserve
429:49 - two different types
429:51 - a regional ip address which can be used
429:54 - by vm instances with one or more network
429:57 - interfaces or by network load balancers
430:01 - these ip addresses can be created either
430:03 - in the console or through the command
430:06 - line with the limitation that you will
430:08 - only be allowed to create ipv4 ip
430:11 - addresses the other type is a global ip
430:15 - address which can be used for global
430:17 - load balancers and can be created either
430:20 - in the console or through the command
430:22 - line as shown here the limitation here
430:26 - is that you must choose the premium
430:28 - network service tier in order to create
430:30 - a global ip address
430:32 - and after reserving the address you can
430:35 - finally assign it to an instance during
430:37 - instance creation or to an existing
430:40 - instance and so as you can see there is
430:43 - a lot to take in when it comes to
430:45 - understanding ip addressing and i hope
430:48 - this lesson has given you some better
430:50 - insight as to which type of ips should
430:52 - be used in a specific scenario now don't
430:55 - worry the options may seem overwhelming
430:58 - but once you start working with ip
431:00 - addresses more often the options will
431:03 - become so much clearer on what to use
431:06 - and when and as i said in the beginning
431:08 - only high level concepts are needed to
431:11 - know for the exam but knowing the
431:13 - options will allow you to make better
431:15 - decisions
431:16 - in your daily role as a cloud engineer
431:19 - and so that's pretty much all i wanted
431:21 - to cover when it comes to ip addressing
431:24 - in google cloud and so now that we've
431:26 - covered the theory behind ip addressing
431:29 - in google cloud i wanted to bring this
431:31 - into the console for a demo where we
431:33 - will get hands-on with creating both
431:36 - internal and external static ip
431:39 - addresses so as i explained before there
431:41 - was a lot to take in with this lesson so
431:44 - now would be a perfect opportunity to
431:46 - get up and have a stretch grab yourself
431:49 - a tea or a coffee and whenever you're
431:51 - ready join me back in the console so you
431:54 - can now mark this lesson as complete and
431:56 - i'll see you in the next
431:58 - [Music]
432:02 - welcome back in this demonstration i'm
432:05 - going to be going over how to create and
432:07 - apply both internal and external static
432:11 - ip addresses i'm going to show how to
432:14 - create them in both the console and the
432:16 - command line as well as how to promote
432:19 - ip addresses from ephemeral ips to
432:22 - static ips and once we're done creating
432:25 - all the ip addresses i'm going to show
432:28 - you the steps on how to delete them now
432:30 - there's a lot to get done here so let's
432:33 - dive in now for this demonstration i'm
432:35 - going to be using a project that has the
432:38 - default vpc created and so in my case i
432:41 - will be using project bowtieinc dev and
432:45 - so before you start make sure that your
432:47 - default vpc is created in the project
432:50 - that you had selected so in order to do
432:53 - that i'm going to head over to the
432:54 - navigation menu i'm going to scroll down
432:57 - to vpc network and we're going to see
432:59 - here that the default vpc has been
433:02 - created and so i can go ahead and start
433:05 - the demonstration and so the first thing
433:07 - i wanted to demonstrate is how to create
433:10 - a static internal ip address and so in
433:13 - order for me to demonstrate this i'm
433:15 - going to be using a vm instance and so
433:17 - i'm going to head over to the navigation
433:19 - menu again and i'm going to scroll down
433:21 - to compute engine
433:23 - and so here i'm going to create my new
433:25 - instance by simply clicking on create
433:28 - instance and so under name i'm going to
433:30 - keep it as instance 1. under region you
433:33 - want to select us east one and i'm going
433:36 - to keep the zone as the default selected
433:38 - under machine type i'm going to select
433:40 - the drop down and select e2 micro and
433:43 - i'm going to leave everything else as
433:45 - the default i'm going to scroll down
433:47 - here to management security disks
433:50 - networking and soul tenancy and i'm
433:52 - going to select the networking tab from
433:54 - there
433:55 - and so under here i'm going to select
433:57 - under network interfaces the default
434:00 - network interface and here is where i
434:03 - can create my static internal ip and so
434:06 - clicking on the drop down under primary
434:08 - internal ip you will see ephemeral
434:11 - automatic ephemeral custom and reserve
434:14 - static internal ip address and so you're
434:17 - going to select reserve static internal
434:19 - ip address and you'll get a pop-up
434:22 - prompting you with some fields to fill
434:24 - out to reserve a static internal ip
434:27 - address and so under name i'm going to
434:29 - call this
434:30 - static dash internal and for the
434:33 - purposes of this demo i'm going to leave
434:35 - the subnet and the static ip address as
434:38 - the currently selected if i wanted to
434:40 - select a specific ip address i can click
434:43 - on this drop down and select let me
434:46 - choose and this will give me the option
434:48 - to enter in a custom ip address with the
434:52 - subnet range that is selected for this
434:54 - specific sub network and so because i'm
434:57 - not going to do that i'm going to select
434:59 - assign automatically i'm going to leave
435:02 - the purpose as non-shared and i'm going
435:04 - to simply click on reserve and this is
435:06 - going to reserve this specific ip
435:09 - address and now as you can see here i
435:11 - have the primary internal ip marked as
435:14 - static internal and so this is going to
435:17 - be my first static internal ip address
435:20 - and so once you've done these steps you
435:22 - can simply click on done and you can
435:24 - head on down to the bottom and simply
435:27 - click on create to create the instance
435:30 - and when the instance finishes creating
435:32 - you will see the internal static ip
435:34 - address and as you can see here your
435:37 - static internal ip address has been
435:40 - assigned to the default network
435:41 - interface on instance 1. and so in order
435:45 - for me to view this static internal ip
435:47 - address in the console i can view this
435:50 - in vpc networks and drill down into the
435:54 - specific vpc and find it under static
435:57 - internal ip addresses but i wanted to
436:00 - show you how to view it by querying it
436:03 - through the command line and so in order
436:05 - to do this i'm going to simply go up to
436:07 - the menu bar on the right hand side and
436:09 - open up cloud shell and once cloud shell
436:12 - has come up you're going to simply paste
436:14 - in the command
436:16 - gcloud compute addresses list and this
436:19 - will give me a list of the internal ip
436:21 - addresses that are available and so now
436:24 - i'm going to be prompted to authorize
436:26 - this api call using my credentials and i
436:29 - definitely do so i'm going to click on
436:31 - authorize and as expected the static
436:34 - internal ip address that we created
436:37 - earlier has shown up it's marked as
436:39 - internal in the region of us east one in
436:43 - the default subnet and the status is in
436:45 - use and so as we discussed in the last
436:48 - lesson static ip addresses persist even
436:52 - after the resource has been deleted and
436:55 - so to demonstrate this i'm going to now
436:57 - delete the instance i'm going to simply
436:59 - check off the instance and go up to the
437:01 - top and click on delete you're going to
437:04 - be prompted to make sure if you want to
437:05 - delete this yes i do so i'm going to
437:07 - click on delete and so now that the
437:10 - instance has been deleted i'm going to
437:12 - query the ip addresses again by using
437:15 - the same command gcloud compute
437:17 - addresses list i'm going to hit enter
437:20 - and as you can see here the ip address
437:23 - static dash internal still persists but
437:26 - the status is now marked as reserved and
437:29 - so if i wanted to use this ip address
437:31 - for another instance i can do so by
437:34 - simply clicking on create instance up
437:36 - here at the top menu and then i can
437:38 - select static dash internal as my ip
437:42 - address so i'm going to quickly close
437:43 - down cloud shell
437:45 - and i'm going to leave the name as
437:46 - instance one the region
437:49 - can select us east one and we're going
437:51 - to keep the zone as the default selected
437:54 - under machine type you're going to
437:55 - select the e2 micro machine type going
437:58 - to scroll down to management security
438:00 - disks networking into soul tenancy and
438:03 - i'm going to select the networking tab
438:05 - from under here
438:06 - and under network interfaces i'm going
438:09 - to select the default network interface
438:11 - and under primary internal ip
438:14 - if i click on the drop down i have the
438:16 - option of selecting the static dash
438:19 - internal static ip address and so i
438:22 - wanted to move on to demonstrate how to
438:24 - promote an internal ephemeral ip address
438:28 - to an internal static ip address and so
438:31 - in order to do this i'm going to select
438:33 - on ephemeral automatic and i'm going to
438:36 - scroll down and click on done and i'm
438:38 - going to go ahead and create the
438:40 - instance and once the instance is ready
438:42 - i'll be able to go in and edit the
438:44 - network interface and so the instance is
438:46 - up and ready and so i'm going to drill
438:49 - down into the instance
438:50 - and i'm going to go up to the top and
438:52 - click on edit i'm going to scroll down
438:54 - to network interfaces and i'm going to
438:56 - edit the default network interface so
438:59 - i'm going to scroll down a little bit
439:00 - more and here under internal iptype i'm
439:03 - going to click on the drop down and i'm
439:05 - going to select static
439:08 - and so here you are taking the current
439:10 - ip address which is 10.142.0.4
439:14 - and promoting it to a static internal ip
439:17 - address and so you're going to be
439:18 - prompted with a pop-up confirming the
439:21 - reservation for that static internal ip
439:24 - address and so notice that i don't have
439:26 - any other options and so all i'm going
439:28 - to do is type in a name and i'm going to
439:31 - call this promoted
439:33 - static and i'm going to click on reserve
439:36 - and this will promote the internal ip
439:39 - address from an ephemeral ip address to
439:42 - a static ip address and so now i'm just
439:45 - going to click on done
439:47 - and i'm going to scroll down and click
439:48 - on save and so now because i want to
439:51 - verify the ip address i'm going to go
439:53 - ahead and open up the cloud shell again
439:56 - and i'm going to use the same command
439:58 - that i used earlier which is gcloud
440:01 - compute addresses list and i'm going to
440:03 - hit enter
440:05 - as expected the promoted static ip
440:08 - address is showing as an internal ip
440:11 - address in the region of us east 1 in
440:14 - the default subnet and its status is in
440:17 - use and so just as a recap we've created
440:21 - a static internal ip address for the
440:24 - first instance and for the second
440:26 - instance we promoted an ephemeral
440:28 - internal ip address into a static
440:31 - internal ip address and we were able to
440:34 - verify this through cloud shell using
440:37 - the gcloud compute addresses list
440:39 - command and so this is the end of part
440:42 - one of this demo it was getting a bit
440:44 - long so i decided to break it up and
440:46 - this would be a great opportunity for
440:49 - you to get up and have a stretch get
440:51 - yourself a coffee or tea and whenever
440:53 - you're ready join me in part two where
440:56 - we will be starting immediately from the
440:58 - end of part one so you can now mark this
441:01 - as complete and i'll see you in the next
441:03 - one
441:03 - [Music]
441:07 - welcome back this is part two of the
441:10 - creating internal and external ip
441:13 - addresses demo and we will be starting
441:16 - immediately from the end of part one so
441:19 - with that being said let's dive in and
441:21 - so now that we've gone through how to
441:23 - both create static ip addresses and
441:27 - promote ephemeral ip addresses to static
441:30 - ip addresses for internal ips
441:33 - i want to go ahead and go through the
441:35 - same with external ips and so i'm going
441:38 - to first start off by deleting this
441:40 - instance i'm going to go ahead and click
441:41 - on delete
441:43 - and so instead of doing it through the
441:45 - compute engine interface i want to go
441:48 - into the external ip address interface
441:51 - which can be found in the vpc network
441:53 - menu
441:54 - so i'm going to go ahead up to the left
441:56 - hand corner click on the navigation menu
441:59 - and i'm going to scroll down to vpc
442:01 - network
442:02 - and from the menu here on the left hand
442:04 - side you can simply click on external ip
442:07 - addresses
442:08 - and here you will see the console where
442:10 - you can create a static external ip
442:13 - address and so to start the process you
442:15 - can simply click on reserve static
442:17 - address and so here you'll be prompted
442:20 - with a bunch of fields to fill out to
442:22 - create this new external static ip
442:25 - address and so for the name of this
442:26 - static ip address you can simply call
442:29 - this external dash static i'm going to
442:32 - use the same in the description now here
442:35 - under network service tier i can choose
442:37 - from either the premium or the standard
442:40 - and as you can see i'm currently using
442:42 - the premium network service tier and if
442:45 - i hover over the question mark over here
442:47 - it tells me a little bit more about this
442:49 - network service tier and as you can see
442:52 - the premium tier allows me higher
442:54 - performance as well as lower latency
442:57 - routing but this premium routing comes
442:59 - at a cost
443:01 - whereas the standard network service
443:02 - tier offers a lower performance compared
443:06 - to the premium network service tier and
443:08 - is a little bit more cost effective but
443:11 - still delivering performance that's
443:13 - comparable with other cloud providers
443:16 - and so i'm just going to leave it as the
443:18 - default selected and as we discussed in
443:20 - the previous lesson ipv6 external static
443:24 - ip addresses can only be used for global
443:28 - load balancers and so since we're only
443:30 - using it for an instance an ipv4 address
443:34 - will suffice and so just as a note for
443:36 - network service tier if i click on
443:39 - standard ipv6 is grayed out as well as
443:43 - the global selection and this is because
443:45 - in order to use global load balancing
443:48 - you need to be using the premium network
443:50 - service tier so whenever you're creating
443:53 - a global load balancer please keep this
443:55 - in mind as your cost may increase so i'm
443:58 - going to switch this back to premium and
444:00 - so under type i'm going to keep it as
444:02 - regional and under region i'm going to
444:04 - select the same region that my instance
444:06 - is going to be in which is us east 1 and
444:10 - because i haven't created the instance
444:12 - yet there is nothing to attach it to and
444:14 - so i'm going to click on the drop down
444:16 - and click on none and so just as another
444:18 - note i wanted to quickly highlight this
444:21 - caution point that the static ip
444:23 - addresses
444:24 - not attached to an instance or low
444:27 - balancer are still billed at an hourly
444:30 - rate so if you're not using any static
444:33 - ip addresses
444:35 - please remember to delete them otherwise
444:37 - you will be charged and so everything
444:40 - looks good here to create my external
444:42 - static ip address so i'm going to simply
444:45 - click on reserve
444:46 - and this will create my external static
444:49 - ip address
444:50 - and put the status of it as reserved so
444:53 - as you can see here the external static
444:55 - ip address has been created and you will
444:58 - find all of your external static ip
445:01 - addresses that you create in future
445:04 - right here in this menu and you will
445:06 - still be able to query all these
445:08 - external ip addresses from the command
445:11 - line and so now in order to assign this
445:14 - ip address to a network interface i'm
445:17 - going to go back over to the navigation
445:19 - menu and scroll down to compute engine
445:22 - and create a new instance so you can go
445:24 - ahead and click on create instance i'm
445:26 - going to go ahead and keep the name of
445:28 - this instance as instance one and in the
445:30 - region i'm going to select us east one
445:33 - i'm going to keep the zone as the
445:35 - selected default and under machine type
445:37 - i'm going to select the e2 micro machine
445:40 - type i'm going to scroll down to
445:42 - management security disks networking and
445:44 - soul tenancy and i'm going to select the
445:47 - networking tab and here under network
445:49 - interfaces i'm going to select the
445:51 - default network interface i'm going to
445:53 - scroll down a little bit here and under
445:55 - external ip
445:57 - ephemeral has been selected but if i
445:59 - click on the drop down i will have the
446:01 - option to select the ip that we had just
446:04 - created which is the external dash
446:07 - static ip and so i'm going to select
446:10 - that i'm going to click on done and you
446:12 - can go down and click on create and so
446:14 - now when the instance is created i will
446:17 - see the external ip address
446:19 - of external static as the assigned
446:23 - external ip and as expected here it is
446:26 - and because i always like to verify my
446:28 - work i'm going to go ahead and open up
446:31 - the cloud shell and verify it through
446:33 - the command line
446:34 - and so now i'm going to query all my
446:36 - available static ip addresses using the
446:39 - command gcloud compute addresses list
446:42 - i'm going to hit enter
446:44 - and as you can see here the external
446:47 - static ip address of 34.75.76
446:53 - in the us east one region is now in use
446:56 - and this is because it is assigned to
446:58 - the network interface on instance one
447:01 - and so before we go ahead and complete
447:03 - this demo there's one more step that i
447:06 - wanted to go through and this is to
447:08 - promote an ephemeral external ip address
447:11 - to a static external ip address and so
447:14 - i'm going to go up here to the top menu
447:16 - and create a new instance i'm going to
447:18 - leave the name here as instance two
447:20 - under the region i'm going to select us
447:23 - east one i'm going to keep the zone as
447:25 - the selected default under machine type
447:27 - i'm going to select the e2 micro machine
447:30 - type i'm going to leave everything else
447:31 - as the default and i'm going to scroll
447:33 - down to management security disks
447:36 - networking and soul tenancy and select
447:38 - the networking tab and i'm going to
447:40 - verify that i'm going to be using an
447:42 - ephemeral external ip upon the creation
447:45 - of this instance if i scroll down here a
447:48 - little bit i can see that an external
447:50 - ephemeral ip address will be used upon
447:53 - creation and this will be the ip address
447:56 - that i will be promoting to a static ip
447:58 - through the command line so i'm going to
448:00 - go ahead and scroll down click on done
448:03 - and then i'm going to scroll down and
448:04 - click on create and once this instance
448:06 - is created then i can go ahead and
448:09 - promote the ephemeral external ip
448:11 - address okay and the instance has been
448:13 - created along with its external
448:16 - ephemeral ip address and so now i can go
448:19 - ahead and promote this ephemeral ip
448:21 - address so in order for me to do this
448:24 - i'm going to move back to my cloud shell
448:26 - and i'm going to quickly clear my screen
448:28 - and i'm going to use the command gcloud
448:31 - compute addresses create and then the
448:33 - name that we want to use for this static
448:36 - external ip address so i'm going to call
448:38 - this promoted external i'm going to use
448:41 - the flag dash dash addresses and so here
448:44 - i will need the external ip address that
448:48 - i am promoting
448:49 - which is going to be 104.196.219.42
448:55 - and so i'm going to copy this to my
448:57 - clipboard and i'm going to paste it here
448:59 - in the command line and now i'm going to
449:01 - add the region flag
449:03 - along with the region of us east one and
449:06 - i'm going to go ahead and hit enter
449:08 - and success my ephemeral external ip
449:12 - address has been promoted to a static
449:14 - external ip address and of course to
449:17 - verify it i'm going to simply type in
449:20 - the gcloud compute addresses list
449:22 - command i'm going to hit enter and as
449:25 - expected here it is the promoted
449:28 - external ip of 104.196.219.42
449:34 - marked as external in the u.s east one
449:37 - region and the status is marked as in
449:40 - use and so i wanted to take a moment to
449:43 - congratulate you on making it through
449:45 - this demonstration of creating internal
449:48 - and external ip addresses as well as
449:52 - promoting them so just as a recap you've
449:54 - created a static internal ip address in
449:58 - conjunction with creating a new instance
450:01 - and assigning it to that instance you
450:03 - then created another instance and used
450:06 - an ephemeral ip and then promoted it to
450:09 - a static internal ip address
450:12 - you then created an external static ip
450:14 - address using the console and assigned
450:17 - it to a brand new instance you then
450:20 - created another instance using an
450:22 - external ephemeral ip address and
450:25 - promoted it to a static external ip
450:28 - address and you did this all using both
450:31 - the console and the command line so i
450:33 - wanted to congratulate you on a great
450:36 - job now before we end this demonstration
450:39 - i wanted to go through the steps of
450:41 - cleaning up any leftover resources so
450:44 - the first thing you want to do is delete
450:46 - these instances so you can select them
450:48 - all and go up to the top and click on
450:50 - delete it's going to ask you if you want
450:52 - to delete the two instances yes we do
450:54 - click on delete and this will delete
450:56 - your instances and free up the external
450:59 - ip addresses so that you're able to
451:01 - delete them and so now that the
451:03 - instances have been deleted i'm going to
451:05 - go over to the vpc network menu and i'm
451:09 - going to head on over to the external ip
451:12 - address console
451:13 - and here i'm able to delete the external
451:16 - ip addresses and so i'm going to select
451:18 - all of them and i'm going to go up to
451:20 - the top menu and click on release static
451:23 - address and you should get a prompt
451:25 - asking you if you want to delete both
451:26 - these addresses the answer is yes click
451:29 - on delete and within a few seconds these
451:31 - external ip addresses should be deleted
451:34 - and so now all that's left to delete are
451:37 - the two static internal ip addresses
451:40 - and as i said before because there is no
451:42 - console to be able to view any of these
451:45 - static internal ip addresses i have to
451:48 - do it through the command line so i'm
451:50 - going to go back to my cloud shell i'm
451:52 - going to clear the screen and i'm going
451:54 - to list the ip addresses currently in my
451:56 - network and so here they are promoted
451:59 - static and static internal and so the
452:02 - command to delete any static ip
452:04 - addresses is as follows gcloud compute
452:07 - addresses delete the name of the ip
452:10 - address that i want to delete which is
452:12 - promoted static and then i will need the
452:14 - region flag and it'll be the region of
452:17 - us east one and i'm going to go ahead
452:19 - and hit enter it's going to prompt me if
452:21 - i want to continue with this and i'm
452:22 - going to type y for yes hit enter
452:26 - and success it has been deleted and so
452:29 - just a double check i'm going to do a
452:30 - quick verification
452:32 - and yes it has been deleted and so all
452:34 - that's left to delete is the static
452:37 - internal ip address and so i'm going to
452:39 - paste in the command
452:41 - gcloud compute addresses delete the name
452:44 - of the ip address that i want to delete
452:46 - which is static dash internal along with
452:48 - the region flag of us east one i'm going
452:51 - to go ahead and hit enter
452:53 - y for yes to continue
452:56 - and success and one last verification to
452:59 - make sure that it's all cleared up and
453:01 - as you can see i have no more static i p
453:03 - addresses and so this concludes this
453:06 - demonstration on creating assigning and
453:09 - deleting both static internal and static
453:13 - external ip addresses and so again i
453:16 - wanted to congratulate you on a great
453:18 - job and so that's pretty much all i
453:20 - wanted to cover
453:22 - in this demo on creating internal and
453:25 - external static ip addresses so you can
453:28 - now mark this as complete and i'll see
453:30 - you in the next one
453:31 - [Music]
453:35 - welcome back
453:37 - in this lesson i will be diving into
453:39 - some network security by introducing vpc
453:43 - firewall rules a service used to filter
453:46 - incoming and outgoing network traffic
453:49 - based on a set of user-defined rules
453:52 - a concept that you should be fairly
453:54 - familiar with for the exam
453:57 - and comes up extremely often when
454:00 - working as an engineer in google cloud
454:03 - it is definitely an essential security
454:05 - layer that prevents unwanted access to
454:09 - your cloud infrastructure
454:11 - now vpc firewall rules apply to a given
454:15 - project and network
454:17 - and if you'd like you can also apply
454:19 - firewall rules across an organization
454:22 - but i will be sticking to strictly vpc
454:25 - firewall rules in this lesson now vpc
454:28 - firewall rules let you allow or deny
454:31 - connections
454:32 - to or from your vm instances
454:35 - based on a configuration that you
454:37 - specify
454:39 - and these rules apply to either incoming
454:42 - connections or outgoing connections
454:44 - but never both at the same time enabled
454:48 - vpc firewall rules are always enforced
454:51 - regardless of their configuration and
454:54 - operating system even if they have not
454:57 - started up now every vpc network
455:00 - functions as a distributed firewall
455:03 - when firewall rules are defined at the
455:06 - network level connections are allowed or
455:09 - denied on a per instance basis
455:12 - so you can think of the vpc firewall
455:14 - rules as existing not only between your
455:17 - instances and other networks
455:20 - but also between individual instances
455:23 - within the same network now when you
455:26 - create a vpc firewall rule you specify a
455:29 - vpc network and a set of components that
455:33 - define what the rule does
455:35 - the components enable you to target
455:38 - certain types of traffic
455:40 - based on the traffic's protocol ports
455:43 - sources and destinations when you create
455:46 - or modify a firewall rule you can
455:49 - specify the instances
455:51 - to which it is intended to apply
455:54 - by using the target component of the
455:56 - rule now in addition to firewall rules
455:59 - that you create google cloud has other
456:02 - rules that can affect incoming or
456:05 - outgoing connections so for instance
456:07 - google cloud doesn't allow certain ip
456:10 - protocols
456:11 - such as egress traffic on tcp port 25
456:16 - within a vpc network and protocols other
456:19 - than tcp udp
456:22 - icmp and gre
456:24 - to external ip addresses of google cloud
456:27 - resources are blocked google cloud
456:30 - always allows communication between a vm
456:34 - instance and its corresponding metadata
456:36 - server at 169.254
456:42 - and this server is essential to the
456:45 - operation of the instance
456:47 - so the instance can access it regardless
456:50 - of any firewall rules that you configure
456:53 - the metadata server provides some basic
456:56 - services to the instance like dhcp dns
457:00 - resolution instance metadata and network
457:03 - time protocol or ntp now just as a note
457:07 - every network has two implied firewall
457:10 - rules that permit outgoing connections
457:14 - and block incoming connections firewall
457:17 - rules that you create can override these
457:20 - implied rules now the first implied rule
457:23 - is the allow egress rule and this is an
457:26 - egress rule whose action is allow and
457:30 - the destination is all ips and the
457:33 - priority is the lowest possible and lets
457:36 - any instance send traffic to any
457:38 - destination except for traffic blocked
457:41 - by google cloud the second implied
457:44 - firewall rule is the deny ingress rule
457:47 - and this is an ingress rule whose action
457:50 - is deny and the source is all ips and
457:53 - the priority is the lowest possible and
457:56 - protects all instances by blocking
457:59 - incoming connections to them now i know
458:02 - we touched on this earlier on in a
458:04 - previous lesson but i felt the need to
458:07 - bring it up as these are pre-populated
458:10 - rules and the rules that i'm referring
458:12 - to are with regards to the default vpc
458:15 - network and as explained earlier these
458:17 - rules can be deleted or modified as
458:20 - necessary the rules as you can see here
458:22 - in the table allow ingress connections
458:25 - from any source to any instance on the
458:28 - network when it comes to icmp rdp on
458:32 - port 3389 for windows remote desktop
458:35 - protocol and for ssh on port 22. and as
458:39 - well the last rule allows ingress
458:42 - connections for all protocols and ports
458:45 - among instances in the network and it
458:48 - permits incoming connections to vm
458:51 - instances from others in the same
458:53 - network and all of these have a rule
458:56 - priority of six five five four which is
459:00 - the second to lowest priority so
459:02 - breaking down firewall rules there are a
459:05 - few characteristics that google put in
459:08 - place that help define these rules and
459:11 - the characteristics are as follows each
459:13 - firewall rule applies to incoming or
459:17 - outgoing connections
459:19 - and not both firewall rules only support
459:23 - ipv4 connections so when specifying a
459:26 - source for an ingress rule or a
459:28 - destination for an egress rule by
459:31 - address
459:32 - you can only use an ipv4 address or ipv4
459:37 - block insider notation as well each
459:40 - firewall rules action is either allow or
459:43 - deny you cannot have both at the same
459:46 - time and the rule applies to connections
459:49 - as long as it is enforced so for example
459:52 - you can disable a rule for
459:54 - troubleshooting purposes and then enable
459:56 - it back again now when you create a
459:59 - firewall rule you must select a vpc
460:02 - network while the rule is enforced at
460:04 - the instance level its configuration is
460:07 - associated with a vpc network this means
460:11 - you cannot share firewall rules among
460:13 - vpc networks including
460:16 - networks connected by vpc network
460:19 - peering or by using cloud vpn tunnels
460:23 - another major thing to note about
460:25 - firewall rules is that they are stateful
460:28 - and so that means when a connection is
460:30 - allowed through the firewall in either
460:32 - direction return traffic matching this
460:35 - connection is also allowed you cannot
460:38 - configure a firewall rule to deny
460:40 - associated response traffic return
460:43 - traffic must match the five tuple of the
460:46 - accepted request traffic but with the
460:49 - source and destination addresses and
460:52 - ports reversed so just as a note for
460:54 - those who may be wondering what a five
460:56 - tuple is i was referring to the set of
460:59 - five different values that comprise a
461:02 - tcpip connection and this would be
461:05 - source ip destination ip source port
461:08 - destination port and protocol google
461:11 - cloud associates incoming packets with
461:14 - corresponding outbound packets by using
461:17 - a connection tracking table google cloud
461:20 - implements connection tracking
461:22 - regardless of whether the protocol
461:24 - supports connections if a connection is
461:27 - allowed between a source and a target or
461:29 - between a target and a destination all
461:32 - response traffic is allowed as long as
461:35 - the firewalls connections tracking state
461:38 - is active and as well as a note a
461:41 - firewall rules tracking state is
461:43 - considered active if at least one packet
461:47 - is sent every 10 minutes now along with
461:50 - the multiple characteristics that make
461:52 - up a firewall rule there are also
461:54 - firewall rule components that go along
461:57 - with it here i have a screenshot from
461:59 - the console with the configuration
462:02 - components of a firewall rule and i
462:04 - wanted to take a moment to highlight
462:07 - these components for better clarity so
462:09 - now the first component is the network
462:12 - and this is the vpc network that you
462:14 - want the firewall rule to apply to the
462:17 - next one is priority which we discussed
462:20 - earlier and this is the numerical
462:22 - priority which determines whether the
462:24 - rule is applied as only the highest
462:27 - priority rule whose other components
462:30 - match traffic is applied and remember
462:33 - the lower the number the higher the
462:35 - priority the higher the number the lower
462:38 - the priority now the next component is
462:41 - the direction of traffic and these are
462:43 - the ingress rules that apply to incoming
462:46 - connections from specified sources to
462:49 - google cloud targets and this is where
462:52 - ingress rules apply to incoming
462:54 - connections from specified sources to
462:57 - google cloud targets
462:59 - and egress rules apply to connections
463:02 - going to specify destinations from
463:04 - targets and the next one up is action on
463:08 - match and this component either allows
463:11 - or denies which determines whether the
463:13 - rule permits or blocks the connection
463:16 - now a target is what defines which
463:19 - instances to which the rule applies and
463:22 - you can specify a target by using one of
463:24 - the following three options the first
463:27 - option
463:28 - are all instances in the network and
463:30 - this is the firewall rule that does
463:33 - exactly what it says it applies to all
463:36 - the instances in the network the second
463:38 - option is instances by target tags and
463:42 - this is where the firewall rule applies
463:44 - only to instances with a matching
463:47 - network tag and so i know i haven't
463:49 - explained it earlier but a network tag
463:51 - is simply a character string added to a
463:54 - tags field in a resource so let's say i
463:58 - had a bunch of instances that were
464:00 - considered development i can simply
464:02 - throw a network tag on them using a
464:04 - network tag of dev and apply the
464:07 - necessary firewall rule for all the
464:10 - development servers holding the network
464:12 - tag dev and so the third option is
464:16 - instances by target service accounts
464:18 - this is where the firewall rule applies
464:21 - only to instances that use a specific
464:24 - service account and so the next
464:26 - component is the source filter and this
464:29 - is a source for ingress rules or a
464:31 - destination for egress rules the source
464:34 - parameter is only applicable to ingress
464:38 - rules and it must be one of the
464:39 - following three selections source ip
464:42 - ranges and this is where you specify
464:44 - ranges of ip addresses as sources for
464:47 - packets either inside or outside of
464:50 - google cloud the second one is source
464:53 - tags and this is where the source
464:55 - instances are identified by a matching
464:58 - network tag and source service accounts
465:01 - where source instances
465:03 - are identified by the service accounts
465:05 - they use you can also use service
465:07 - accounts to create firewall rules that
465:10 - are a bit more granular and so one of
465:13 - the last components of the firewall rule
465:15 - is the protocols and ports you can
465:18 - specify a protocol or a combination of
465:21 - protocols and their ports if you omit
465:24 - both protocols and ports the firewall
465:27 - rule is applicable for all traffic on
465:30 - any protocol and any port and so when it
465:33 - comes to enforcement status of the
465:36 - firewall rule there is a drop down right
465:39 - underneath all the components where you
465:41 - can enable or disable the enforcement
465:44 - and as i said before this is a great way
465:47 - to enable or disable a firewall rule
465:50 - without having to delete it and is great
465:53 - for troubleshooting or to grant
465:55 - temporary access to any instances and
465:58 - unless you specify otherwise
466:00 - all firewall rules are enabled when they
466:03 - are created but you can also choose to
466:06 - create a rule in a disabled state and so
466:09 - this covers the vpc firewall rules in
466:12 - all its entirety and i will be showing
466:14 - you how to implement vpc firewall rules
466:18 - along with building a custom vpc
466:21 - custom routes
466:23 - and even private google access
466:25 - all together in a demo following this
466:28 - lesson to give you some hands-on skills
466:31 - of putting it all into practice and so
466:34 - that's pretty much all i wanted to cover
466:36 - when it comes to vpc firewall rules so
466:39 - you can now mark this lesson as complete
466:41 - and let's move on to the next one where
466:43 - we dive in and build our custom vpc so
466:47 - now is a perfect time to grab a coffee
466:49 - or tea and whenever you're ready join me
466:52 - in the console
466:57 - welcome back
466:58 - in this demonstration i want to take all
467:01 - the concepts that we've learned so far
467:03 - in this networking section and put it
467:06 - all into practice this diagram shown
467:09 - here is the architecture of exactly what
467:12 - we will be building in this demo we're
467:15 - going to start by creating a custom vpc
467:17 - and then we're going to create two
467:19 - subnets
467:20 - one public and one private
467:23 - in two separate regions we're then going
467:26 - to create a cloud storage bucket with
467:28 - some objects in it and then we will
467:31 - create some instances to demonstrate
467:33 - access to cloud storage as well as
467:36 - communication between instances and
467:38 - finally we're going to create some
467:40 - firewall rules for routing traffic to
467:43 - all the right places we're also going to
467:45 - implement private google access
467:48 - and demonstrate accessibility to the
467:51 - files in cloud storage from the private
467:54 - instance without an external ip so this
467:57 - may be a little bit out of your comfort
467:59 - zone for some
468:01 - but don't worry i'll be with you every
468:03 - step of the way and other than creating
468:05 - the instances all the steps here have
468:08 - been covered in previous lessons now
468:10 - there's a lot to get done here so
468:12 - whenever you're ready join me in the
468:14 - console and so here we are back in the
468:16 - console and as you can see up here in
468:18 - the right hand corner i am logged in as
468:21 - tony bowtie ace gmail.com and currently
468:24 - i am logged in under project tony and so
468:27 - in order to start off on a clean slate
468:29 - i'm going to create a new project and so
468:32 - i'm going to simply click on the project
468:34 - menu drop-down and click on new project
468:37 - i'm going to call this project
468:39 - bowtie inc and i don't have any
468:41 - organizations so i'm going to simply
468:43 - click on create and as well for those of
468:46 - you doing this lesson i would also
468:48 - recommend for you to create a brand new
468:51 - project so that you can start off anew
468:53 - again i'm going to go over to the
468:55 - project drop down and i'm going to
468:57 - select bow tie ink as the project and
468:59 - now that i have a fresh new project i
469:01 - can now create my vpc network so i'm
469:04 - going to go over to the left hand corner
469:06 - to the navigation menu and i'm going to
469:08 - scroll down to vpc network and so
469:11 - because vpc networks are tied in with
469:14 - the compute engine api
469:16 - we need to enable it before we can
469:18 - create any vpc networks so you can go
469:21 - ahead and enable this api so once this
469:24 - api has finished and is enabled we'll be
469:28 - able to create our vpc network
469:31 - ok and the api has been enabled and as
469:34 - you can see the default vpc network has
469:37 - been created with a subnet in every
469:40 - region along with its corresponding ip
469:43 - address ranges and so for this demo
469:45 - we're going to create a brand new vpc
469:47 - network along with some custom subnets
469:50 - and so in order to do that i'm going to
469:52 - go up here to the top and i'm going to
469:54 - click on create vpc network and so here
469:57 - i'm prompted with some fields to fill
469:59 - out
470:00 - so under name i'm going to think of a
470:02 - creative name that i can call my vpc
470:04 - network so i'm going to simply call it
470:06 - custom under description i'm going to
470:08 - call this custom
470:10 - vpc network and i'm going to move down
470:13 - here to subnets and because i'm creating
470:15 - custom subnets i'm going to keep it
470:17 - under custom under subnet creation mode
470:20 - and so i'm going to need a public subnet
470:23 - and a private subnet and you'll be able
470:25 - to get the values from the text file in
470:28 - the github repository within the sub
470:31 - networks folder under networking
470:33 - services and so i'm going to create my
470:35 - public subnet first and i'm going to
470:37 - simply call the public subnet public for
470:40 - region i'm going to use us east one and
470:44 - the ip address range will be 10.0.0.0
470:48 - forward slash 24 and i'm going to leave
470:51 - private google access off and i'm going
470:53 - to simply click on done and now i can
470:56 - create the private subnet so underneath
470:58 - the public subnet you'll see add subnet
471:01 - you can simply click on that and the
471:03 - name of the new subnet will be as you
471:05 - guessed it private under region i'm
471:08 - going to use us east 4 and for the ip
471:11 - address range
471:13 - be sure to use 10.0.5.0.24
471:18 - and we're going to leave private google
471:19 - access off for now and we'll be turning
471:22 - that on a little bit later in the demo
471:24 - and so you can now click on done and
471:26 - before we click on create we want to
471:28 - enable the dns api and clicking on
471:31 - enable will bring you to the dns api
471:34 - home page and you can click on enable to
471:37 - enable the api okay so now that we have
471:40 - our network configured along with our
471:43 - public and private subnets as well as
471:46 - dns being enabled we can now simply
471:48 - click on create but before i do that i
471:51 - wanted to give you some insight with
471:53 - regards to the command line so as i've
471:55 - shared before everything that can be
471:57 - done in the console can be done through
471:59 - the command line and so if ever you
472:02 - wanted to do that or you wanted to get
472:04 - to know the command line a little bit
472:06 - better
472:07 - after filling out all the fields with
472:09 - regards to creating resources in the
472:11 - console you will be given the option of
472:14 - a command line link that you can simply
472:16 - click on and here you will be given all
472:18 - the commands to create all the same
472:21 - resources with all the same preferences
472:24 - through the command line and i will be
472:26 - providing these commands in the lesson
472:28 - text so that you can familiarize
472:30 - yourself with the commands to use in
472:33 - order to build any networks using the
472:35 - command line but this is a great
472:37 - reference for you to use at any time and
472:40 - so i'm going to click on close
472:42 - and now i'm going to click on create
472:44 - and within a minute or two the custom
472:46 - vpc network will be created and ready to
472:49 - use okay and the custom vpc network has
472:52 - been created along with its public and
472:55 - private subnet and so just to get a
472:57 - little bit more insight with this custom
472:59 - vpc network i'm going to drill down into
473:02 - it
473:03 - and as you can see here the subnets are
473:05 - respectively labeled private and public
473:08 - along with its region ip address range
473:11 - the gateway and private google access
473:14 - the routes as you can see here are the
473:16 - system generated routes that i had
473:18 - discussed in an earlier lesson it has
473:21 - both the subnet routes to its respective
473:23 - ip range along with the default route
473:26 - with a path to the internet as well as a
473:29 - path for private google access now we
473:32 - don't have any firewall rules here yet
473:34 - but we'll be adding those in just a few
473:36 - minutes and so now that you've created
473:38 - the vpc network with its respective
473:41 - subnets we're going to head on over to
473:43 - cloud storage and create a bucket along
473:46 - with uploading the necessary files so
473:48 - i'm going to go again over to the
473:50 - navigation menu
473:51 - and i'm going to scroll down to storage
473:54 - and so as expected there are no buckets
473:57 - present here in cloud storage and so
473:59 - we're just going to go ahead and create
474:01 - our first bucket by going up here to the
474:04 - top menu and clicking on create bucket
474:07 - and so here i've been prompted to name
474:09 - my bucket and for those of you who are
474:12 - here for the first time when it comes to
474:14 - naming a storage bucket the name needs
474:17 - to be globally unique and this means
474:19 - that the name has to be unique across
474:22 - all of the google cloud platform now
474:25 - don't worry i'm going to get into
474:26 - further detail with this in the cloud
474:29 - storage lesson with all of these
474:31 - specific details when it comes to names
474:34 - storage classes and permissions and so
474:37 - in the meantime you can come up with a
474:39 - name for your bucket something that
474:41 - resonates with you and so for me i'm
474:43 - going to name my bucket bowtie inc dash
474:46 - file dash access and so now i'm going to
474:48 - simply click continue and so just as a
474:50 - note for those who are unable to
474:52 - continue through it is because the name
474:55 - for your bucket is not globally unique
474:57 - so do try to find one that is now when
475:00 - it comes to location type i'm just going
475:02 - to click on region and you can keep the
475:04 - default location as used one and i'm
475:07 - going to leave all the other options as
475:09 - default and i'm going to go down to the
475:11 - bottom and click create and so for those
475:14 - of you who have created your bucket you
475:16 - can now upload the files and those files
475:19 - can be found in the github repository
475:22 - in the cloud storage bucket folder under
475:24 - networking services and so now i'm going
475:27 - to click on upload files
475:29 - and under the networking services
475:31 - section under cloud storage bucket you
475:34 - will find these three jpeg files and you
475:37 - can simply select them and click on open
475:40 - and so they are now uploaded into the
475:43 - bucket and so now i'm ready to move on
475:45 - to the next step so you should now have
475:48 - created the vpc network with a private
475:51 - and public subnet along with creating
475:53 - your own bucket in cloud storage and
475:56 - have uploaded the three jpeg files so
475:59 - now that this is done we can now create
476:01 - the instances
476:02 - that will have access to these files and
476:05 - so again i will go over to the
476:07 - navigation menu in the top left hand
476:09 - corner and scroll down to compute engine
476:12 - and here i will click on create
476:14 - and so again i will be prompted with
476:16 - some fields to fill out and so for this
476:19 - instance i'm going to first create the
476:21 - public instance again i'm going to get
476:24 - really creative and call this
476:26 - public dash instance under labels i'm
476:30 - going to add a label
476:32 - under key
476:33 - i'm going to type environment and under
476:36 - value i'm going to type in public i'm
476:38 - going to go down to the bottom and click
476:40 - on save and under region i'm going to
476:42 - select us east1 and you can leave the
476:45 - zone as us east 1b moving down under
476:48 - machine type i'm going to select the e2
476:51 - micro as the machine type just because
476:53 - i'm being cost conscious and i want to
476:55 - keep the cost down and so i'm going to
476:57 - scroll down to identity and api access
477:00 - and under service account you should
477:02 - have the compute engine default service
477:04 - account already pre-selected now under
477:07 - access scopes i want to be able to have
477:09 - the proper permissions to be able to
477:12 - read and write to cloud storage along
477:15 - with read and write access to compute
477:17 - engine and so you can click on set
477:19 - access for each api and you can scroll
477:22 - down to compute engine
477:24 - click on the drop down menu and select
477:26 - read write and this will give the public
477:29 - instance the specific access that it
477:31 - needs to ssh into the private instance
477:34 - and so now i'm going to set the access
477:36 - for cloud storage so i'm going to scroll
477:38 - down to storage i'm going to click on
477:40 - the drop down menu and select read write
477:43 - and this will give the instance read
477:45 - write access to cloud storage scrolling
477:47 - down a little bit further i'm going to
477:49 - go to management security disks
477:52 - networking and sold tenancy and i'm
477:54 - going to click on that
477:56 - scroll up here just a little bit
477:58 - and you can click on the networking tab
478:00 - which will prompt you for a bunch of
478:02 - options that you can configure for the
478:05 - networking of the instance so under
478:07 - network tags i want to type in public
478:10 - and you can click enter you can then
478:13 - scroll down to where it says network
478:15 - interfaces and click on the current
478:17 - interface which is the default and here
478:19 - it'll open up all your options and so
478:22 - under network you want to click on the
478:24 - drop down and set it from default to
478:26 - custom the public subnet will
478:28 - automatically be propagated so you can
478:30 - leave it as is and you also want to make
478:33 - sure that your primary internal ip
478:35 - as well as your external ip are set to
478:38 - ephemeral and you can leave all the
478:40 - other options as default and simply
478:42 - click on done and again before clicking
478:44 - on create you can click on the command
478:46 - line link and it will show you all the
478:48 - commands needed in order to create this
478:50 - instance through the command line so i'm
478:53 - going to go ahead and close this
478:55 - and so i'm going to leave all the other
478:56 - options as default and i'm going to
478:58 - click on create and so now that my
479:00 - public instance is being created i'm
479:03 - going to go ahead and create my private
479:04 - instance
479:05 - using the same steps that i did for the
479:08 - last instance so i'm going to go ahead
479:10 - and click on create instance here at the
479:12 - top and so the first thing i'm going to
479:14 - be prompted for is the name of the
479:16 - instance
479:17 - and so i'm going to call this instance
479:19 - private dash instance and here i'm going
479:22 - to add a label the key being environment
479:25 - and the value being private i'm going to
479:27 - go down here to the bottom and click on
479:29 - save
479:30 - and under region i'm going to select us
479:33 - east 4
479:34 - and you can keep the zone as the default
479:36 - selected under machine type we're going
479:39 - to select the e2 micro and again
479:41 - scrolling down to the identity and api
479:44 - access under the access scopes for the
479:46 - default service account i'm going to
479:48 - click on the set access for each api and
479:51 - i'm going to scroll down to storage i'm
479:53 - going to click on the drop down menu and
479:55 - i'm going to select access for read
479:58 - write and for the last step i'm going to
480:00 - go into the networking tab under
480:02 - management security disks networking and
480:05 - soul tenancy and under network tags i'm
480:07 - going to give this instance a network
480:09 - tag of private and under network
480:12 - interfaces we want to edit this and
480:14 - change it from default over to the
480:16 - custom network and as expected it
480:19 - selected the private subnet by default
480:22 - and because this is going to be a
480:24 - private instance we are not going to
480:26 - give this an external ip so i'm going to
480:29 - click on the drop down and select none
480:31 - and with all the other options set as
480:33 - default i'm going to simply click on
480:36 - create
480:37 - and this will create my private instance
480:40 - along with having my public instance so
480:43 - just as a recap we've created a new
480:45 - custom vpc network
480:47 - along with a private and public subnet
480:50 - we've created a storage bucket and added
480:53 - some files in it to be accessed and
480:56 - we've created a private and public
480:58 - instance and assigning the service
481:01 - account on the public instance read
481:03 - write access to both compute engine and
481:06 - cloud storage along with a public ip
481:09 - address and assigning the service
481:11 - account on the private instance read
481:13 - write access only for cloud storage and
481:16 - no public ip and so this is the end of
481:19 - part one of this demo
481:21 - and this would be a great opportunity
481:24 - for you to get up and have a stretch
481:26 - get yourself a coffee or tea
481:28 - and whenever you're ready you can join
481:30 - me in part two
481:32 - where we will be starting immediately
481:34 - from the end of part one so you can go
481:36 - ahead and complete this video and i will
481:39 - see you in part two
481:40 - [Music]
481:44 - welcome back
481:46 - this is part two of the custom vpc demo
481:49 - and we will be starting exactly where we
481:51 - left off from part one so with that
481:53 - being said let's dive in and so now the
481:56 - last thing that needs to be done is to
481:58 - simply create some firewall rules and so
482:01 - with these firewall rules this will give
482:04 - me ssh access into the public instance
482:07 - as well as allowing private
482:09 - communication from the public instance
482:12 - to the private instance as well as
482:15 - giving ssh access
482:17 - from the public instance to the private
482:19 - instance and this will allow us to
482:21 - access the files in the bucket from the
482:24 - private instance and so in order to
482:26 - create these firewall rules i need to go
482:29 - back to my vpc network so i'm going to
482:31 - go up to the left hand corner again to
482:33 - the navigation menu
482:35 - and scroll down to vpc network over here
482:38 - on the left hand menu you'll see
482:40 - firewall i'm going to click on that
482:43 - and here you will see all the default
482:45 - firewall rules for the default network
482:48 - so for us to create some new ones for
482:50 - the custom vpc i'm going to go up here
482:52 - to the top and click on create firewall
482:55 - and so the first rule i want to create
482:58 - is for my public instance and i want to
483:00 - give it public access as well as ssh
483:03 - access and so i'm going to name this
483:05 - accordingly as public dash access i'm
483:08 - going to give this the same description
483:10 - always a good idea to turn on logs but
483:13 - for this demonstration i'm going to keep
483:15 - them off under network i'm going to
483:17 - select the custom network i'm going to
483:19 - keep the priority at 1000 the direction
483:22 - of traffic will be ingress and the
483:25 - action on match will be allow and so
483:27 - here is where the target tags come into
483:29 - play when it comes to giving access to
483:32 - the network so targets we're going to
483:34 - keep it as specified target tags
483:36 - and under target tags you can simply
483:39 - type in public
483:40 - under source filter you can keep it
483:42 - under ip ranges
483:44 - and the source ip range will be 0.0.0.0
483:49 - forward slash 0. and we're not going to
483:51 - add a second source filter here so
483:54 - moving down to protocols and ports under
483:56 - tcp i'm going to click that off and add
483:59 - in port 22. and because i want to be
484:01 - able to ping the instance i'm going to
484:04 - have to add another protocol which is
484:06 - icmp
484:08 - and again as explained earlier the
484:10 - disable rule link will bring up the
484:12 - enforcement and as you can see it is
484:15 - enabled but if you wanted to create any
484:17 - firewall rules in future and have them
484:20 - disabled you can do that right here but
484:22 - we're gonna keep this enabled and we're
484:24 - gonna simply click on create and this
484:27 - will create the public firewall rule for
484:29 - our public instance in our custom vpc
484:32 - network and so we're going to now go
484:34 - ahead and create the private firewall
484:37 - rule and so i'm going to name this
484:39 - private dash access
484:41 - respectively i'm going to put the
484:43 - description as the same under network
484:46 - i'm going to select our custom network
484:48 - keep the priority at 1000 direction of
484:51 - traffic should be at ingress and the
484:53 - action on match should be allow for
484:55 - target tags you can type in private and
484:58 - then hit enter and because i want to be
485:00 - able to reach the private instance from
485:02 - the public instance the source ip range
485:05 - will be
485:06 - 10.0.0.1
485:08 - forward slash 24. we're not going to add
485:11 - a second source filter and under
485:13 - protocols and ports we're going to
485:15 - simply add tcp port 22
485:19 - and again i want to add icmp
485:22 - so that i'm able to ping the instance
485:24 - and i'm going to click on create
485:28 - and so we now have our two firewall
485:30 - rules private access and public access
485:34 - and if i go over to the custom vpc
485:36 - network and i drill into it
485:39 - i'll be able to see these selective
485:41 - firewall rules under the respective
485:43 - firewall rules tab and so now that we've
485:46 - created our vpc network along with the
485:49 - public and private subnet we've created
485:52 - the cloud storage bucket with the files
485:54 - that we need to access the instances
485:56 - that will access those files along with
485:59 - the firewall rules that will allow the
486:02 - proper communication we can now go ahead
486:04 - to test everything that we built and
486:07 - make sure that everything is working as
486:09 - expected so let's kick things off by
486:12 - first logging into the public instance
486:14 - so you can head on over to the
486:15 - navigation menu and scroll down to
486:18 - compute engine
486:20 - and you can ssh into the public instance
486:24 - by clicking on ssh under connect
486:26 - and this should open up a new tab or a
486:29 - new window logging you in with your
486:31 - currently authenticated credentials okay
486:34 - and we are logged into our instance and
486:37 - i'm going to zoom in for better viewing
486:39 - and so just to make sure that everything
486:41 - is working as expected we know that our
486:43 - firewall rule is correct because we are
486:46 - able to ssh into the instance and now i
486:49 - want to see if i have access to my files
486:51 - in the bucket and so in order to do that
486:54 - i'm going to run the gsutil command ls
486:57 - for list and then gs colon forward slash
487:00 - forward slash along with my bucket name
487:02 - which is bow tie inc
487:06 - hyphen file iphone access and i'm going
487:09 - to hit enter and as you can see i have
487:11 - access to all the files in the bucket
487:13 - and the last thing i wanted to check is
487:15 - if i can ping the private instance so
487:18 - i'm going to first clear my screen and
487:20 - i'm going to head on over back to the
487:21 - console i'm going to copy the ip address
487:23 - of the private instance to my clipboard
487:26 - and then i'm going to head back on over
487:28 - to my terminal and i'm going to type in
487:30 - ping i'm going to paste the ip address
487:32 - and success
487:34 - i am able to successfully ping the
487:36 - private instance
487:38 - from the public instance
487:40 - using the icmp protocol and you can hit
487:42 - control c to stop the ping so now that i
487:45 - know that my public instance has the
487:47 - proper permissions to reach cloud
487:49 - storage
487:50 - as well as being able to ping my private
487:53 - instance i want to be able to check if i
487:55 - can ssh
487:57 - into the private instance from my public
487:59 - instance and so i'm going to first clear
488:01 - my screen and next i'm going to paste in
488:04 - this command in order for me to ssh into
488:07 - the private instance g cloud compute ssh
488:11 - dash dash project and my project name
488:14 - which is bow tie inc dash dash zone and
488:17 - the zone that my instance is in which is
488:20 - us east 4c along with the name of the
488:23 - instance which is private dash instance
488:26 - and along with the flag dash dash
488:28 - internal dash ip stating that i am using
488:32 - the internal ip in order to ssh into the
488:35 - instance and i'm going to hit enter and
488:37 - so now i've been prompted for a
488:39 - passphrase in order to secure my rsa key
488:43 - pair as one is being generated to log
488:45 - into the private instance now it's
488:48 - always good practice when it comes to
488:50 - security to secure your key pair with a
488:53 - passphrase but for this demo i'm just
488:56 - going to leave it blank
488:58 - and so i'm just going to hit enter
489:01 - i'm going to hit enter again
489:02 - now i don't want to get too deep into it
489:05 - but i did want to give you some context
489:07 - on what's happening here so when you log
489:09 - into an instance on google cloud with os
489:13 - login disabled google manages the
489:15 - authorized keys file
489:17 - for new user accounts based on ssh keys
489:21 - in metadata and so the keys that are
489:23 - being generated that are being used for
489:25 - the first time are currently being
489:28 - stored within the instance metadata so
489:30 - now that i'm logged into my private
489:32 - instance i'm going to quickly clear my
489:34 - screen and just as a note you'll be able
489:37 - to know whether or not you're logged
489:38 - into your private instance by looking
489:41 - here at your prompt and so now i want to
489:43 - make sure that i can ping my public
489:45 - instance so i'm going to quickly type
489:47 - the ping command i'm going to head on
489:49 - over to the console i'm going to grab
489:51 - the ip address of the public instance
489:53 - i'm going to go back to my terminal and
489:55 - paste it in and as expected i'm able to
489:58 - ping my public instance from my private
490:00 - instance i'm just going to go ahead and
490:02 - hit control c to stop and i'm going to
490:04 - clear the screen so now we'd like to
490:06 - verify whether or not we have access to
490:10 - the files in the cloud storage bucket
490:12 - that we created earlier
490:14 - and so now i'm going to use the same
490:15 - command
490:16 - that i used in the public instance to
490:19 - list all the files in the cloud storage
490:21 - bucket so i'm going to use the gsutil
490:23 - command ls for list along with gs colon
490:27 - forward slash forward slash and the
490:30 - bucket name which is bow tie ink hyphen
490:32 - file if an access and i'm going to hit
490:35 - enter
490:37 - and as you can see here i'm not getting
490:39 - a response and the command is hanging
490:42 - and this is due to the fact that
490:44 - external access is needed in order to
490:47 - reach cloud storage and this instance
490:49 - only has an internal or private ip so
490:53 - accessing the files in the cloud storage
490:55 - bucket is not possible now in order to
490:58 - access cloud storage and the set of
491:00 - external ip addresses used by google
491:03 - apis and services we can do this by
491:06 - enabling private google access on the
491:08 - subnet used by the vms network interface
491:12 - and so we're going to go ahead and do
491:13 - that right now so i'm going to hit
491:15 - control c to stop and i'm going to go
491:17 - back into the console i'm going to go to
491:19 - the navigation menu and i'm going to
491:21 - scroll down to vpc network
491:24 - and then i'm going to drill down into
491:25 - the private subnet and i'm going to edit
491:28 - it under private google access i'm going
491:30 - to turn it on and i'm going to go down
491:32 - to the bottom and click on save and by
491:34 - giving this subnet private google access
491:37 - i will allow the private instance and
491:39 - any instances with private ip addresses
491:42 - to access any public apis
491:45 - such as cloud storage so now when i go
491:48 - back to my instance i'm going to clear
491:50 - the screen here and i'm going to run the
491:52 - gsutil command again
491:56 - and success
491:57 - we are now able to access cloud storage
492:00 - due to enabling private google access on
492:03 - the respective private subnet
492:05 - so i first wanted to congratulate you on
492:08 - making it to the end of this demo
492:10 - and hope that this demo has been
492:12 - extremely useful as this is a real life
492:15 - scenario that can come up and so just as
492:17 - a recap you've created a custom network
492:20 - with two custom subnets you've created a
492:22 - cloud storage bucket and uploaded some
492:25 - files to it you've created a public
492:27 - instance and a private instance and then
492:30 - created some firewall rules to route the
492:32 - traffic you then tested it all by using
492:35 - the command line for communication you
492:38 - also enable private google access for
492:40 - the instance with only the internal ip
492:43 - to access google's public apis so that
492:47 - it can access cloud storage and so again
492:50 - fantastic job on your part as this was a
492:52 - pretty complex demo
492:54 - and you can expect things like what
492:56 - you've experienced in this demo to pop
492:58 - up in your role of being a cloud
493:00 - engineer at any time so before you go be
493:04 - sure to delete all the resources you've
493:06 - created
493:07 - and again congrats on the great job so
493:10 - you can now mark this as complete and
493:12 - i'll see you in the next one
493:18 - welcome back in this lesson i will be
493:20 - going over vpc network peering and how
493:23 - you can privately communicate across
493:26 - vpcs in the same or different
493:29 - organization vpc network peering and vpc
493:32 - peering are used interchangeably in this
493:35 - lesson as they are used to communicate
493:37 - the same thing now for instances in one
493:40 - vpc to communicate with an instance in
493:43 - another vpc they would route traffic via
493:46 - the public internet however to
493:48 - communicate privately between two vpcs
493:52 - google cloud offers a service called vpc
493:55 - peering and i will be going through the
493:57 - theory and concepts of vpc peering
494:00 - throughout this lesson so with that
494:02 - being said let's dive in
494:05 - now vpc peering enables you to peer vpc
494:08 - networks
494:09 - so that workloads in different vpc
494:11 - networks can communicate in a private
494:14 - space that follows the rfc
494:17 - 1918 standard thus allowing private
494:20 - connectivity across two vpc networks
494:24 - traffic stays within google's network
494:26 - and never traverses the public internet
494:29 - vpc peering gives you the flexibility of
494:32 - peering networks that are of the same or
494:35 - different projects along with being able
494:38 - to peer with other networks in different
494:40 - organizations vpc peering also gives you
494:44 - several advantages over using external
494:47 - ip addresses or vpns to connect the
494:50 - first one is reducing network latency as
494:54 - all peering traffic stays within
494:56 - google's high-speed network vpc peering
494:59 - also offers greater network security as
495:02 - you don't need to have services exposed
495:04 - to the public internet and deal with
495:06 - greater risks of having your traffic
495:09 - getting compromised or if you're trying
495:11 - to achieve compliance standards for your
495:13 - organization vpc peering will allow you
495:16 - to achieve the standards that you need
495:19 - and finally vpc network peering reduces
495:22 - network costs as you save on egress
495:24 - costs for traffic leaving gcp so in a
495:27 - regular network google charges you for
495:30 - traffic communicating using public ips
495:33 - even if the traffic is within the same
495:35 - zone now you can bypass this and save
495:38 - money by using internal ips to
495:41 - communicate and keeping the traffic
495:43 - within the gcp network
495:46 - now there are certain properties or
495:47 - characteristics that peered vpcs follow
495:51 - and i wanted to point these out for
495:53 - better understanding first off peer vpc
495:56 - networks remain administratively
495:58 - separate so what exactly does this mean
496:01 - well it means that routes firewalls vpns
496:05 - and other traffic management tools are
496:07 - administered and applied separately in
496:10 - each of the vpc networks so this applies
496:13 - to each vpc independently which also
496:17 - means that each side of a peering
496:19 - association
496:20 - is set up independently as well so when
496:23 - you connect one vpc to the other you
496:26 - have to go into each vpc that you are
496:28 - connecting to both initiate and
496:30 - establish the connection peering becomes
496:33 - active only when the configuration from
496:35 - both sides match this also means that
496:38 - each vpc can delete the peering
496:41 - association at any given time now during
496:44 - vpc peering the vpc peers always
496:47 - exchange all subnet routes you also have
496:50 - the option of exchanging custom routes
496:53 - subnet and static routes are global and
496:55 - dynamic routes can be regional or global
496:58 - a given vpc network can peer with
497:01 - multiple vpc networks but there is a
497:04 - limit that you can reach in which you
497:06 - would have to reach out to google and
497:08 - ask the limit to be increased now when
497:10 - peering with vpc networks there are
497:12 - certain restrictions in place that you
497:15 - should be aware of first off a subnet
497:18 - cider range in one peered vpc network
497:21 - cannot overlap with a static route in
497:23 - another peered network this rule covers
497:26 - both subnet routes and static routes so
497:30 - when a vpc subnet is created or a subnet
497:33 - ip range is expanded google cloud
497:36 - performs a check to make sure that the
497:38 - new subnet range does not overlap with
497:41 - ip ranges of subnets in the same vpc
497:44 - network or in directly peered vpc
497:47 - networks if it does the creation or
497:50 - expansion will fail google cloud also
497:53 - ensures that no overlapping subnet ip
497:56 - ranges
497:57 - are allowed across vpc networks that
498:00 - have appeared network in common and
498:02 - again if it does the creation or
498:05 - expansion will fail now speaking of
498:07 - routing when you create a new subnet in
498:10 - appeared vpc network
498:12 - vpc network peering doesn't provide
498:15 - granular route controls to filter out
498:18 - which subnet cider ranges are reachable
498:20 - across pure networks these are handled
498:23 - by firewall rules so to allow ingress
498:26 - traffic from vm instances in a peer
498:29 - network you must create ingress allow
498:31 - firewall rules by default ingress
498:34 - traffic to vms is blocked by the implied
498:38 - deny ingress rule another key point to
498:40 - note is that transitive peering is not
498:43 - supported and only directly peered
498:45 - networks can communicate so they have to
498:48 - be peered directly in this diagram
498:51 - network a is peered with network b and
498:54 - network b is peered with network c and
498:57 - so if one instance is trying to
498:58 - communicate from network a to network c
499:02 - this cannot be done unless network a is
499:05 - directly peered with network c an
499:07 - extremely important point to note for
499:10 - vpc peering another thing to note is
499:12 - that you cannot use a tag or service
499:15 - account from one peered network in the
499:18 - other peered network they must each have
499:20 - their own as again they are each
499:22 - independently operated as stated earlier
499:25 - and so the last thing that i wanted to
499:27 - cover is that internal dns is not
499:30 - accessible for compute engine in peered
499:33 - networks as they must use an ip to
499:36 - communicate and so that about covers
499:38 - this short yet important lesson on the
499:41 - theory and concepts of vpc peering and
499:44 - so now that we've covered all the theory
499:46 - i'm going to be taking these concepts
499:48 - into a demo where we will be pairing two
499:50 - networks together and verifying the
499:53 - communication between them and so you
499:55 - can now mark this lesson as complete and
499:57 - whenever you're ready join me in the
499:59 - console
500:04 - welcome back in this hands-on
500:06 - demonstration
500:07 - we're going to go through the steps to
500:09 - create a peering connection from two
500:11 - vpcs
500:12 - in two separate projects as shown here
500:15 - in the diagram and then to verify that
500:17 - the connection works we're going to
500:19 - create two instances one in each network
500:23 - and ping one instance from the other
500:25 - instance this demo is very similar to
500:28 - the custom vpc demo that you had done
500:31 - earlier but we are adding in another
500:34 - layer of complexity by adding in vpc
500:37 - network peering and so there's quite a
500:39 - bit to do here so let's go ahead and
500:41 - just dive in
500:42 - okay so here we are back in the console
500:44 - as you can see up in the top right hand
500:46 - corner i am logged in as tony bowties
500:49 - gmail.com and for this specific demo i
500:53 - will be using two projects both project
500:56 - tony and project bowtie inc and if you
500:59 - currently do not have two projects you
501:02 - can go ahead and create yourself a new
501:04 - project or the two projects if you have
501:06 - none and so i'm going to continue here
501:09 - with project tony and the first thing i
501:11 - want to do is create the two networks in
501:13 - the two separate projects so i'm going
501:16 - to go up to the navigation menu in the
501:17 - top left hand corner and i'm going to
501:19 - scroll down to vpc network
501:22 - here i'm going to create my first vpc
501:24 - network and i'm going to name this
501:27 - bowtie ink
501:29 - dash a i'm going to give it the same
501:31 - description
501:32 - and then under subnets i'm going to
501:33 - leave the subnet creation mode under
501:35 - custom under the subnet name you can
501:38 - call this subnet dash a
501:40 - i'm going to use the us east one region
501:43 - and for the ip address range i'm going
501:46 - to use 10.0 that's 0.0 forward slash 20.
501:50 - and i'm going to leave all the other
501:51 - options as default and i'm going to go
501:53 - down to the bottom and click on create
501:56 - now as this network is being created i'm
501:58 - going to go over to the project bowtie
502:00 - inc and i'm going to create the vpc
502:03 - network there so under name i'm going to
502:05 - call this bowtie inc
502:07 - b and under description i'm going to use
502:10 - the same under subnets i'm going to keep
502:12 - subnet creation mode as custom and under
502:15 - new subnet i'm going to call this subnet
502:18 - subnet b the region will be used 4
502:22 - and the ip address range will be
502:24 - 10.4.0.0
502:27 - forward slash 20. you can leave all the
502:29 - other options as default and scroll down
502:31 - to the bottom and click on create as
502:34 - this network is being created i'm going
502:36 - to go back to project tony and i'm going
502:38 - to create the firewall rule for bow tie
502:41 - ink dash a
502:44 - in this firewall rule as explained in
502:46 - the last lesson we'll allow
502:48 - communication from one instance to the
502:51 - other and so i'm going to click on
502:52 - create firewall
502:54 - and under name i'm going to call this
502:56 - project
502:57 - tony dash a under description i'm going
503:00 - to use the same
503:02 - under the network i'm going to choose
503:04 - the source network which will be bowtie
503:06 - inc dash a priority i'm going to keep at
503:09 - 1000 direction of traffic should be
503:12 - ingress and action on match should be
503:14 - allow under targets i'm going to select
503:17 - all instances in the network and under
503:20 - source filter i'm going to keep ip
503:22 - ranges selected and the source ip range
503:25 - specifically for this demo is going to
503:27 - be 0.0.0.0
503:30 - forward slash 0. and again this is
503:32 - specifically used for this demo and
503:34 - should never be used in a
503:36 - production-like environment in
503:38 - production you should only use the
503:40 - source ip ranges that you are
503:42 - communicating with and under protocols
503:44 - and ports because i need to log into the
503:46 - instance to be able to ping the other
503:49 - instance i'm going to have to open up
503:51 - tcp on port 22. under other protocols
503:55 - you can add icmp and this will allow the
503:58 - ping command to be used i'm going to
504:00 - leave all the other options as default
504:02 - and i'm going to click on create and now
504:04 - that this firewall rule has been created
504:07 - i need to go back over to project bowtie
504:09 - inc and create the firewall rule there
504:11 - as well
504:13 - i'm going to call this firewall rule
504:15 - bowtie inc dash b i'm going to give it
504:18 - the same description under network i'm
504:20 - going to select bow tie ink dash b i'm
504:23 - going to keep the priority as 1000 and
504:25 - the direction of traffic should be
504:27 - ingress as well the action on match
504:29 - should be allow scrolling down under
504:32 - targets i'm going to select all
504:34 - instances in the network and again under
504:37 - source filter i'm going to keep ip
504:39 - ranges selected and under source ip
504:41 - ranges i'm going to enter in
504:44 - 0.0.0.0 forward slash 0. and under
504:47 - protocols and ports i'm going to select
504:49 - tcp with port 22 as well under other
504:53 - protocols i'm going to type in icmp i'm
504:56 - going to leave everything else as
504:57 - default and i'm going to click on create
505:00 - now once you've created both networks
505:02 - and have created both firewall rules you
505:05 - can now start creating the instances so
505:07 - because i'm already in project bowtie
505:09 - inc i'm going to go to the left-hand
505:11 - navigation menu and i'm going to scroll
505:13 - down to compute engine and create my
505:16 - instance so i'm just going to click on
505:18 - create
505:19 - and to keep with the naming convention
505:21 - i'm going to call this instance instance
505:23 - b i'm not going to add any labels for
505:25 - now under region i'm going to choose us
505:28 - east 4 and you can leave the zone as the
505:31 - default selection and under machine type
505:34 - i'm going to select e2 micro and i'm
505:36 - going to scroll down to the bottom and
505:38 - i'm going to click on management
505:40 - security disks networking and sold
505:42 - tenancy so that i'm able to go into the
505:44 - networking tab to change the network on
505:47 - the default network interface so i'm
505:49 - going to click on the default network
505:51 - interface and under network i'm going to
505:53 - select bowtie inc b and the subnet has
505:56 - already been selected for me and then
505:58 - i'm going to scroll down click on done
506:00 - and i'm going to leave all the other
506:02 - options as default and click on create
506:05 - and so as this is creating i'm going to
506:07 - go over to project tony
506:10 - and i'm going to create my instance
506:12 - there
506:14 - and i'm going to name this instance
506:16 - instance a under region i am going to
506:18 - select us east1 you can leave the zone
506:21 - as the default selected under machine
506:23 - type i'm going to select e2 micro and
506:26 - scrolling down here to the bottom i'm
506:28 - going to go into the networking tab
506:30 - under management security disks
506:33 - networking and soul
506:35 - and here i'm going to edit the network
506:37 - interface and change it from the default
506:40 - network to bow tie ink dash a and as you
506:42 - can see the subnet has been
506:44 - automatically selected for me
506:46 - so now i can just simply click on done
506:49 - i'm going to leave all the other options
506:51 - as default and i'm going to click on
506:53 - create
506:54 - so just as a recap we've created two
506:57 - separate networks in two separate
506:59 - projects along with its corresponding
507:02 - subnets and the firewall rules along
507:05 - with creating an instance in each
507:07 - network and so now that we have both
507:10 - environments set up it's now time to
507:12 - create the vbc peering connection and so
507:15 - because i'm in project tony i'm going to
507:17 - start off with this project and i'm
507:19 - going to go up to the navigation menu
507:21 - and scroll down to vpc network and under
507:24 - vpc network on the left hand menu you're
507:27 - going to click on vpc network peering
507:29 - and through the interface shown here
507:31 - we'll be able to create our vpc network
507:34 - peering so now you're going to click on
507:36 - create connection and i'm prompted with
507:38 - some information that i will need and
507:40 - because we are connecting to another vpc
507:43 - in another project you're going to need
507:46 - the project id as well as the name of
507:48 - the vpc network you want to peer with
507:50 - and just as explained in the earlier
507:52 - lesson the subnet ip ranges in both
507:55 - networks cannot overlap so please make
507:58 - sure that if you are using ip ranges
508:01 - outside of the ones that are given for
508:03 - this demonstration the ip ranges that
508:05 - you are using do not overlap so once you
508:08 - have that information you can then click
508:11 - continue
508:12 - and so here you will be prompted with
508:14 - some fields to fill out with the
508:16 - information that you were asked to
508:17 - collect in the previous screen and so
508:20 - since we have that information already
508:22 - we can go ahead and start filling in the
508:23 - fields so i'm going to call this peering
508:25 - connection peering
508:27 - a b and under vpc network i'm going to
508:30 - select bow tie ink dash a under peered
508:33 - vpc network we're going to select the
508:35 - other project which should be bowtie inc
508:38 - and the vpc network name will be bow tie
508:41 - inc dash b and i'm going to leave all
508:44 - the other options as default and so
508:46 - under vpc network name you will see
508:49 - exchange custom routes and here i can
508:52 - select to import and export custom
508:55 - routes that i have previously created so
508:57 - any special routes that i have created
509:00 - before the actual peering connection i
509:02 - can bring them over to the other network
509:05 - to satisfy my requirements and so i'm
509:07 - not going to do that right now i'm going
509:09 - to close this up and i'm going to simply
509:11 - click on create and so this is finished
509:13 - creating and is marked as inactive and
509:16 - this is because the corresponding
509:18 - peering connection in project bowtie has
509:21 - yet to be configured the status will
509:23 - change to a green check mark in both
509:25 - networks and marked as active once they
509:28 - are connected if this status remains as
509:30 - inactive then you should recheck your
509:33 - configuration and edit it accordingly so
509:36 - now i'm going to head on over to project
509:38 - bowtie inc and i'm going to create the
509:40 - corresponding peering connection i'm
509:42 - going to click on create connection once
509:44 - you have your project id and the vpc
509:46 - network you can click on continue and
509:49 - for the name of this peering connection
509:51 - i'm going to call this peering dash ba
509:54 - respectively under vpc network i'm going
509:57 - to select bowtie inc b and under peered
510:00 - vpc network i'm going to select in
510:02 - another project here you want to type in
510:04 - your project id for me i'm going to
510:07 - paste in my project tony project id and
510:10 - under vpc network name i'm going to type
510:12 - in bowtie inc
510:14 - a and i'm going to leave all the other
510:16 - options as default and i'm going to
510:18 - click on create and so now that we've
510:20 - established connections on each of the
510:22 - peering connections in each vpc if the
510:25 - information that we've entered is
510:27 - correct then we should receive a green
510:30 - check mark stating that the peering
510:32 - connection is connected and success here
510:35 - we have status as active and if i head
510:38 - on over to project tony i should have
510:41 - the same green check mark under status
510:43 - for the peering connection and as
510:45 - expected the status has a green check
510:48 - mark and is marked as active so now in
510:50 - order to do the pairing connectivity
510:52 - test i'm going to need to grab the
510:54 - internal ip of the instance in the other
510:58 - network that resides in project bowtie
511:01 - and so because it doesn't matter which
511:02 - instance i log into as both of them have
511:06 - ssh and ping access i'm going to simply
511:09 - go over to the navigation menu i'm going
511:11 - to head on over to compute engine and
511:13 - i'm going to record the internal ip of
511:16 - instance a and now i'm going to head
511:18 - over to project bowtie and log into
511:21 - instance b and ping instance a and so in
511:24 - order to ssh into this instance i'm
511:26 - going to click on the ssh button under
511:28 - connect and it should open a new browser
511:31 - tab for me logging me into the instance
511:34 - okay i'm logged in here and i'm going to
511:36 - zoom in for better viewing and so now
511:39 - i'm going to run a ping command against
511:42 - instance a using the internal ip that i
511:45 - had copied earlier and i'm going to hit
511:47 - enter and as you can see ping is working
511:51 - and so now we can confirm that the vpc
511:54 - peering connection is established and
511:56 - the two instances in the different vpc
511:59 - networks are communicating over their
512:02 - private ips and you can go ahead and hit
512:04 - control c to stop the ping and so just
512:07 - as a recap you've created two separate
512:09 - vpc networks with their own separate
512:12 - subnets in two separate projects you've
512:15 - created the necessary firewall rules in
512:17 - each of these networks along with
512:19 - creating instances in each of those
512:22 - networks you then established a vpc
512:24 - peering connection establishing the
512:26 - configuration in each vpc you then did a
512:30 - connectivity test by logging into one of
512:33 - the instances and pinging the other
512:35 - instance and so i hope this helps cement
512:37 - the theory of vpc peering that you
512:40 - learned in the previous lesson and has
512:42 - given you some context when it comes to
512:45 - configuring each end of the peering
512:47 - connection so i wanted to take a moment
512:49 - to congratulate you on completing this
512:51 - demo and so all that's left now is to
512:54 - clean up all the resources that we
512:56 - created throughout this demo and you can
512:58 - start by selecting the instances and
513:00 - deleting them in each network as well as
513:03 - the firewall rules and the networks
513:06 - themselves i'm going to go over to
513:07 - project tony and i'm going to do the
513:09 - same thing there and so you can do
513:11 - exactly what you did with the last
513:12 - instance here you can select it click on
513:15 - delete and delete the instance and so
513:17 - next we're going to delete the peering
513:19 - connection so we're going to go up to
513:21 - the navigation menu we're going to
513:23 - scroll down to vpc network and on the
513:25 - left hand menu we're going to scroll
513:26 - down to vpc network peering and so we're
513:29 - going to select appearing connection
513:31 - we're going to go to the top and click
513:32 - on delete and then delete the peering
513:34 - connection
513:35 - and so now we're going to delete the
513:37 - firewall rule so we're going to go up to
513:39 - firewall
513:42 - we're going to select the firewall rule
513:44 - at the top we're going to click delete
513:45 - and then delete the firewall rule and
513:47 - last but not least we want to delete the
513:49 - vpc network that we created so we're
513:52 - going to go up to vpc networks we're
513:54 - going to drill down into the custom vpc
513:56 - up at the top we're going to click on
513:57 - delete vpc network and then we're going
514:00 - to click on delete and so now that we've
514:01 - deleted all the resources in project
514:03 - tony we're going to go back over to our
514:06 - second project project bowtie and do the
514:08 - same thing and so we're first going to
514:10 - start off with the vpc peering
514:12 - connection so we're going to go over to
514:13 - vpc network peering we're going to
514:15 - select the appearing connection we're
514:17 - gonna click on delete at the top and
514:19 - delete the peering connection next we're
514:21 - gonna go into firewall we're gonna
514:22 - select the firewall rule go up to the
514:24 - top and click on delete and then delete
514:26 - the firewall rule and finally we're
514:28 - gonna go over to vpc networks we're
514:31 - going to drill down into the custom
514:32 - network we're going to click on delete
514:34 - vpc network at the top and delete the
514:36 - vpc network
514:39 - and so now that you've successfully
514:41 - deleted all your resources you can now
514:43 - mark this lesson as complete and i'll
514:45 - see you in the next one and congrats
514:47 - again on the great job of completing
514:49 - this demo
514:51 - [Music]
514:54 - welcome back and in this lesson i'm
514:57 - going to be discussing the concepts and
514:59 - terminology of shared vpcs i'm also
515:02 - going to go into some detailed use cases
515:05 - and how shared vpcs would be used in
515:08 - different scenarios so with that being
515:10 - said let's dive in
515:13 - now when a vpc is created it is usually
515:16 - tied to a specific project now what
515:18 - happens when you want to share resources
515:21 - across different projects but still have
515:24 - separate billing and access within the
515:26 - projects themselves
515:28 - this is where shared vpcs come into play
515:31 - shared vpcs allow an organization to
515:34 - connect resources from multiple projects
515:37 - to a common vpc network so that way they
515:40 - can communicate with each other securely
515:43 - and efficiently using internal ips from
515:46 - that network when you use shared vpcs
515:49 - you designate a project as a host
515:52 - project and attach one or more other
515:55 - service projects to it the vpc networks
515:58 - in the host project are considered the
516:00 - shared vpc networks so just as a
516:03 - reminder a project that participates in
516:05 - a shared vpc is either a host project or
516:09 - a service project a host project can
516:12 - contain one or more shared vpc networks
516:15 - a service project is any project that
516:18 - has been attached to a host project by a
516:21 - shared vpc admin this attachment allows
516:24 - it to participate in the shared vpc and
516:27 - just as a note a project cannot be both
516:30 - a host and a service project
516:32 - simultaneously it has to be one or the
516:35 - other and you can create and use
516:37 - multiple host projects however each
516:40 - service project can only be attached to
516:43 - a single host project it is also a
516:46 - common practice to have multiple service
516:49 - projects administered by different
516:51 - departments or teams in the organization
516:54 - and so just for clarity for those who
516:56 - are wondering a project that does not
516:58 - participate in a shared vpc
517:01 - is called a standalone project
517:03 - and this is to emphasize that it is
517:05 - neither a host project or a service
517:08 - project now when it comes to
517:10 - administering these shared vpcs we
517:13 - should be adhering to the principle of
517:15 - least privilege and only assigning the
517:18 - necessary access needed to specific
517:21 - users so here i broken down the roles
517:24 - that are needed to enable and administer
517:27 - the shared vpcs a shared vpc admin has
517:31 - the permissions to enable host projects
517:34 - attach service projects to host projects
517:37 - and delegate access to some or all of
517:40 - the subnets in shared vpc networks to
517:43 - service project admins when it comes to
517:46 - a service project admin this is a shared
517:48 - vpc admin for a given host project and
517:52 - is typically its project owner as well
517:54 - although when defining each service
517:56 - project admin a shared vpc admin can
517:59 - grant permission to use the whole host
518:02 - project or just some subnets and so when
518:05 - it comes to service project admins there
518:08 - are two separate levels of permissions
518:10 - that can be applied the first is project
518:12 - level permissions and this is a service
518:15 - project admin that can be defined to
518:18 - have permission to use all subnets in
518:20 - the host project when it comes to subnet
518:22 - level permissions a service project
518:25 - admin can be granted a more restrictive
518:27 - set of permissions to use only some
518:30 - subnets now i wanted to move into some
518:33 - use cases which will give you a bit more
518:35 - context on how shared vpcs are used in
518:38 - specific environments illustrated here
518:42 - is a simple shared vpc scenario here a
518:45 - host project has been created and
518:47 - attached to service projects to it the
518:50 - service project admin in service project
518:53 - a
518:54 - can be configured to access all or some
518:56 - of the subnets in the shared vpc network
519:00 - service project admin with at least
519:02 - subnet level permissions to the 10.0.2.0
519:07 - 24 subnet has created vm1 in a zone
519:11 - located in the us west one region this
519:14 - instance receives its internal ip
519:17 - address
519:19 - 10.0.2.15 from the 10.0.2.0
519:24 - 24 cider block now service project
519:26 - admins in service project b can be
519:29 - configured to access all or some of the
519:31 - subnets in the shared vpc network a
519:34 - service project admin with at least
519:36 - subnet level permissions to the
519:40 - 10.10.4.0 forward slash 24 subnet has
519:43 - created vm2 in a zone located in the us
519:47 - central 1 region this instance receives
519:50 - its internal ip address
519:52 - 10.10.4.1
519:54 - from the
519:56 - 10.10.4.0 forward slash 24 cider block
519:59 - and of course the standalone project
520:01 - does not participate in the shared vpc
520:04 - at all as it is neither a host nor a
520:06 - service project and the last thing to
520:08 - note
520:09 - instances in service projects attached
520:12 - to a host project using the same shared
520:15 - vpc network
520:17 - can communicate with one another using
520:20 - either ephemeral or reserve static
520:23 - internal ip addresses and i will be
520:25 - covering both ephemeral and static ip
520:29 - addresses in a later section under
520:31 - compute engine external ip addresses
520:34 - defined in the host project are only
520:36 - usable by resources in that project they
520:39 - are not available for use in service
520:42 - projects moving on to the next use case
520:45 - is a multiple hosts project for this use
520:48 - case an organization is using two
520:50 - separate host projects development and
520:53 - production
520:54 - and each host project has two service
520:56 - projects attached to them both host
520:59 - projects have one shared vpc network
521:02 - with subnets configured to use the same
521:04 - cider ranges both the testing and
521:07 - production networks have been purposely
521:09 - configured in the same way so this way
521:12 - when you work with resources tied to a
521:14 - subnet range it will automatically
521:17 - translate over from one environment to
521:19 - the other moving on to the next use case
521:22 - is the hybrid environment now in this
521:24 - use case the organization has a single
521:27 - host project with a single shared vpc
521:30 - network the shared vpc network is
521:33 - connected via cloud vpn to an
521:36 - on-premises network some services and
521:38 - applications are hosted in gcp while
521:42 - others are kept on premises and this way
521:45 - separate teams can manage each of their
521:47 - own service projects
521:49 - and each project has no permissions to
521:51 - the other service projects as well each
521:54 - service project can also be billed
521:56 - separately subnet level or project level
521:59 - permissions have been granted to the
522:01 - necessary service project admins
522:04 - so they can create instances that use
522:06 - the shared vpc network and again
522:09 - instances in these service projects can
522:11 - be configured to communicate with
522:14 - internal services
522:16 - such as database or directory servers
522:18 - located on premises and finally the last
522:21 - use case is a two-tier web service here
522:25 - an organization has a web service that
522:27 - is separated into two tiers and
522:30 - different teams manage each tier the
522:32 - tier one service project represents the
522:35 - externally facing component behind an
522:38 - http or https load balancer the tier 2
522:42 - service project represents an internal
522:45 - service upon which tier 1 relies on and
522:48 - it is balanced using an internal tcp or
522:52 - udp load balancer the shared vpc allows
522:55 - mapping of each tier of the web service
522:58 - to different projects so that they can
523:01 - be managed by different teams while
523:03 - sharing a common vpc network to host
523:06 - resources that are needed for both tiers
523:09 - now we cover quite a bit in this lesson
523:11 - when it comes to all the concepts of
523:13 - shared vpcs we covered both host and
523:16 - service projects and the roles that they
523:18 - play and their limitations we also went
523:21 - over the different roles that are needed
523:23 - to administrate these shared vpcs and we
523:26 - went over different use cases on how to
523:28 - use shared vpcs for different scenarios
523:32 - and so that about covers everything i
523:33 - wanted to discuss in this lesson
523:36 - so you can now mark this lesson as
523:37 - complete and let's move on to the next
523:40 - one
523:40 - [Music]
523:44 - welcome back and in this lesson i'm
523:47 - going to be discussing vpc flow logs
523:50 - flow logs is an essential tool for
523:52 - monitoring and analyzing traffic
523:55 - coming in and going out of vpcs from vm
523:59 - instances flow logs are essential to
524:01 - know for the exam as you should know the
524:04 - capabilities and use cases and so with
524:06 - that being said let's dive in
524:09 - so vpc flow logs records a sample of
524:12 - network flows
524:14 - sent from and received by vm instances
524:18 - including instances used as google
524:21 - kubernetes engine nodes these logs can
524:24 - be used for network monitoring forensics
524:27 - real-time security analysis and expense
524:30 - optimization when you enable vpc flow
524:33 - logs you enable for all vms in a subnet
524:37 - so basically you would be enabling vpc
524:40 - flow logs on a subnet by subnet basis
524:43 - flow logs are aggregated by connection
524:46 - from compute engine vms and exported in
524:49 - real time these logs can be exported to
524:53 - cloud logging previously known as
524:55 - stackdriver for 30 days if logs need to
524:58 - be stored for longer than 30 days they
525:01 - can be exported to a cloud storage
525:03 - bucket for longer term storage and then
525:07 - read and queried by cloud logging google
525:10 - cloud samples packets that leave and
525:13 - enter a vm to generate flow logs now not
525:17 - every packet is captured into its own
525:19 - log record about one out of every 10
525:22 - packets is captured but this sampling
525:25 - rate might be lower depending on the
525:27 - vm's load and just as a note you cannot
525:30 - adjust this rate this rate is locked by
525:33 - google cloud and cannot be changed in
525:35 - any way and because vpc flow logs do not
525:39 - capture every packet it compensates for
525:42 - missed packets by interpolating from the
525:45 - captured packets now there are many
525:47 - different use cases for vpc flow logs
525:50 - and i wanted to take a quick minute to
525:52 - go over them the first one i wanted to
525:54 - mention is network monitoring vpc flow
525:57 - logs provide you with real-time
525:59 - visibility into network throughput and
526:02 - performance so you can monitor the vpc
526:04 - network perform network diagnostics
526:08 - understand traffic changes and help
526:10 - forecast capacity
526:12 - for capacity planning you can also
526:14 - analyze network usage with vpc flow logs
526:18 - and you can analyze the network flows
526:20 - for traffic between regions and zones
526:23 - traffic to specific countries on the
526:25 - internet
526:26 - and based on the analysis you can
526:28 - optimize your network traffic expenses
526:31 - now a great use case for vpc flow logs
526:34 - is network forensics
526:36 - so for example if an incident occurs you
526:39 - can examine which ips talked with whom
526:43 - and when and you can also look at any
526:45 - compromised ips by analyzing all the
526:49 - incoming and outgoing network flows and
526:52 - lastly
526:53 - vpc flow logs can be used for real-time
526:56 - security analysis
526:57 - you can leverage the real-time streaming
527:00 - apis using pub sub and integrate them
527:03 - with a sim or security information in
527:06 - event management system like splunk
527:09 - rapid7 or logarithm and this is a very
527:12 - common way to add an extra layer of
527:15 - security to your currently existing
527:18 - environment as well as a great way to
527:20 - meet any compliance standards that are
527:23 - needed for your organization now vpc
527:26 - flow logs are recorded in a specific
527:29 - format log records contain base fields
527:32 - which are the core fields of every log
527:34 - record and meta data fields that add
527:38 - additional information metadata fields
527:41 - may be omitted to save storage costs but
527:44 - base fields are always included and
527:46 - cannot be omitted some log fields are in
527:50 - a multi-field format with more than one
527:52 - piece of data in a given field
527:55 - for example
527:57 - the connection field that you see from
527:59 - the base is of the ip details format
528:03 - which contains the source and
528:05 - destination ip address and port
528:08 - plus the protocol in a single field
528:11 - flows that have an endpoint in a gke
528:14 - cluster can be annotated with gke
528:17 - annotations which can include details of
528:20 - the cluster
528:21 - pod and service of the endpoint gke
528:25 - annotations are only available with a
528:28 - custom configuration of metadata fields
528:31 - now when you enable vpc flow logs you
528:34 - can set a filter based on both base and
528:37 - metadata fields that only preserves logs
528:40 - that match the filter all other logs are
528:43 - discarded before being written to
528:46 - logging which saves you money and
528:48 - reduces the time needed to find the
528:50 - information you're looking for shown
528:53 - here is a sample from the console in
528:55 - both the classic logs viewer as well as
528:58 - the logs viewer in preview and so in the
529:00 - classic logs viewer you can simply
529:03 - select the sub network from the first
529:05 - pull down menu and from the second pull
529:07 - down menu you can select the
529:09 - compute.googleapis.com
529:12 - forward slash vpc underscore flows and
529:15 - this will give you the information that
529:17 - you need to pull up all your vpc flow
529:20 - logs in the logs viewer preview it is
529:23 - done in a similar way but the query is
529:25 - shown here in the query builder and can
529:28 - be adjusted accordingly pulling up any
529:30 - vpc flow logs must be done within the
529:33 - console when viewing them in google
529:35 - cloud and so the last thing i wanted to
529:38 - show you before ending this lesson is a
529:41 - sample of the log itself the log shown
529:43 - here is a sample of what a vpc flow log
529:47 - looks like
529:48 - and as you can see here beside each
529:50 - field you will see a small arrow
529:53 - clicking on these arrows will expand the
529:56 - field and reveal many of the subfields
529:59 - that you saw on the last slide and will
530:01 - give you the necessary information you
530:04 - need to analyze your vpc flow logs in
530:07 - this example of the connection field it
530:10 - shows the five tuple that describes this
530:12 - connection which you can clearly see up
530:15 - here at the top and if i were to go
530:18 - further down and expand more of these
530:20 - fields i would find more information
530:23 - that could help me better analyze more
530:26 - logging info for my given problem that i
530:29 - am trying to solve now i didn't want to
530:31 - go too deep into logging as i will be
530:34 - diving into a complete section on its
530:37 - own in a later section of the course but
530:40 - i did want you to get a feel for what
530:42 - type of data vpc flow logs can give you
530:45 - and how it can help you in your specific
530:48 - use case as well as on the exam and so
530:51 - that's pretty much all i wanted to cover
530:53 - with regards to vpc flow logs so you can
530:56 - now mark this lesson as complete and
530:58 - let's move on to the next one
531:00 - [Music]
531:04 - welcome back in this lesson i'm going to
531:07 - cover a high-level overview of a basic
531:09 - foundational service that supports the
531:12 - backbone of the internet as we know it
531:14 - today this foundation is called dns or
531:18 - the domain name system dns is used
531:21 - widely in google cloud from mostly an
531:24 - infrastructure perspective and is used
531:26 - in pretty much any other cloud
531:28 - environment or computer network on the
531:30 - planet now there is quite a bit to cover
531:32 - in this lesson with regards to dns so
531:35 - with that being said let's dive in
531:37 - now dns or domain name system is a
531:41 - global decentralized distributed
531:43 - database that lets you store ip
531:46 - addresses and other data
531:48 - and look them up by name this system
531:51 - uses human readable names like
531:54 - google.com and translates it into a
531:57 - language that computers understand which
531:59 - are numeric ip addresses for example
532:03 - humans access information online through
532:06 - a domain name like google.com computers
532:09 - use ip addresses to access information
532:12 - online like 172.217.
532:18 - now whether you type google.com or the
532:21 - ip address into a web browser both will
532:24 - connect to google.com dns translates the
532:27 - domain name to an ip address so that the
532:30 - web browser knows where to connect to
532:33 - and we know what to enter into the web
532:36 - browser through dns you can connect a
532:39 - domain name to web hosting
532:42 - mail
532:42 - and other services
532:44 - now getting a bit deeper into it as ip
532:47 - addresses are at the core of
532:49 - communicating between devices on the
532:51 - internet they are hard to memorize and
532:54 - can change often even for the same
532:57 - service to get around these problems we
533:00 - gave names to ip addresses for example
533:03 - when it comes to our computer
533:05 - communicating with
533:08 - www.google.com it will use the dns
533:11 - system to do this now in the dns
533:13 - database contains the information needed
533:16 - to convert the www.google.com
533:20 - domain name to the ip address and this
533:23 - piece of information is stored in a
533:25 - logical container called a zone the way
533:28 - that the zone is stored is through
533:30 - what's commonly known as a zone file now
533:33 - within this zone file is a dns record
533:37 - which links the name
533:38 - www and the ip address that your laptop
533:42 - needs to communicate
533:44 - with the specific website and this zone
533:46 - file is hosted by what's known as a name
533:49 - server or ns server for short and i will
533:53 - be going into further detail on this in
533:55 - just a minute so in short if you can
533:58 - query the zone for the record
534:00 - www.google.com
534:03 - then your computer can communicate with
534:05 - the web server and dns is what makes it
534:08 - all happen
534:09 - now i wanted to go into a bit of history
534:12 - of how dns came about so in early
534:15 - computer networks a simple text file
534:18 - called a host file was created that
534:21 - mapped hostnames to ip addresses and
534:24 - this enabled people to refer to other
534:26 - computers by the name and their
534:29 - computers translated that name to an ip
534:31 - address when it needed to communicate
534:34 - with it the problem is as network sizes
534:36 - increased the host file approach became
534:40 - impractical due to the fact that it
534:42 - needed to be stored on each computer as
534:45 - each computer would have to resolve the
534:47 - same host names as well updates were
534:49 - difficult to manage as all of the
534:51 - computers would need to be given an
534:54 - updated file all in all this system was
534:57 - not scalable
534:59 - now to overcome these and other
535:01 - limitations the dns system was developed
535:04 - and the dns system essentially provided
535:07 - for a way to organize the names using a
535:10 - domain name structure it also provided a
535:13 - dynamic system for protocols services
535:17 - and methods
535:18 - for storing updating and retrieving ip
535:21 - addresses for host computers
535:24 - now that i've covered what dns is
535:26 - and why we use it i wanted to dive into
535:29 - the structure of the dns system now the
535:32 - structure all begins with a dot the root
535:35 - if you will and this can be found after
535:38 - every domain name that you type into
535:40 - your browser you will almost never see
535:42 - it and this is because your browser will
535:45 - automatically put it in without your
535:47 - knowing you can try it with any domain
535:49 - in any browser and you will almost
535:52 - always come up with the same result this
535:54 - dot is put in for you and will provide
535:58 - the route for you and this is where we
536:00 - start to break down the dns system
536:03 - now the domain name space consists of a
536:06 - hierarchical data structure like the one
536:09 - you have on your computer each node has
536:12 - a label and zero or more resource
536:14 - records which hold information
536:17 - associated with the domain name the
536:19 - domain name itself consists of the label
536:22 - concatenated with the name of its parent
536:25 - node on the right separated by a dot so
536:28 - when it comes to dns the domain name is
536:31 - always assembled from right to left this
536:34 - hierarchy or tree is subdivided into
536:37 - zones beginning at the root zone a dns
536:40 - zone may consist of only one domain or
536:43 - may consist of many domains and sub
536:46 - domains depending on the administrative
536:49 - choices of the zone manager now getting
536:51 - right into it the root server is the
536:54 - first step in translating human readable
536:57 - hostnames into ip addresses the root
537:00 - domain is comprised of 13 dns systems
537:04 - dispersed around the world known
537:07 - collectively as the dns root servers
537:10 - they are indicated by the letters a
537:12 - through m
537:14 - operated by 12 organizations such as
537:17 - verisign cogent and nasa while there are
537:21 - 13 ip addresses that represent these
537:23 - systems there are actually more than 13
537:26 - servers some of the ip addresses are
537:29 - actually a cluster of dns servers and so
537:32 - each of these dns servers also consists
537:35 - of the root zone file which contains the
537:38 - address of the authoritative name server
537:41 - for each top level domain and because
537:44 - this is such a big undertaking to keep
537:46 - updated iana or the internet assigned
537:49 - numbers authority was appointed as the
537:52 - authority that manages and administrates
537:54 - this file and i will include a link in
537:57 - the lesson text for those of you who are
537:59 - looking to dive deeper into the contents
538:02 - of this root zone file as well as
538:04 - getting to know a little bit more about
538:07 - the iana organization now while the dns
538:10 - root servers establish the hierarchy
538:12 - most of the name resolution process is
538:15 - delegated to other dns servers so just
538:18 - below the dns route in the hierarchy are
538:21 - the top level domain servers
538:23 - also known as tld for short the top
538:26 - level domain takes the tld provided in
538:29 - the user's query for example www.google
538:34 - and provides details for the dot-com tld
538:38 - name server the companies that
538:40 - administer these domains are named
538:42 - registries and they operate the
538:44 - authoritative name servers for these top
538:48 - level domains for example verisign is
538:51 - the registry for the dot com top level
538:53 - domain over a hundred million domains
538:56 - have been registered in the dot com top
538:58 - level domain and these top level dns
539:01 - servers
539:02 - handle top level domains such as com dot
539:06 - org dot net and dot io and this can also
539:09 - be referred to as the gtld which is the
539:13 - general top level domains and the cctld
539:17 - which is the country code top level
539:18 - domain like dot ca for canada dot uk for
539:22 - the united kingdom and dot it for italy
539:25 - the top level dns servers delegate to
539:28 - thousands of second level dns servers
539:31 - now second level domain names are sold
539:34 - to companies and other organizations and
539:37 - over 900 accredited registrars register
539:40 - and manage the second level domains in
539:43 - the dot com domain for end users the
539:46 - second level of this structure is
539:49 - comprised of millions of domain names
539:52 - second level dns servers can further
539:54 - delegate the zone but most commonly
539:57 - store the individual host records for a
540:00 - domain name this is the server at the
540:02 - bottom of the dns lookup chain where you
540:05 - would typically find resource records
540:07 - and it is these resource records that
540:10 - maps services and host names to ip
540:13 - addresses and will respond with the
540:15 - queried resource record ultimately
540:17 - allowing the web browser making the
540:20 - request to reach the ip address needed
540:23 - to access a website or other web
540:25 - resources now there is one more concept
540:28 - that i wanted to cover
540:30 - before we move on and this is the sub
540:32 - domain now some of you have noticed and
540:34 - wondered where does the sub domain come
540:37 - into play with regards to the dns
540:39 - structure well this is a resource record
540:42 - that falls under the second level domain
540:44 - and in dns hierarchy a sub domain is a
540:47 - domain that is a part of another main
540:50 - domain but i wanted to put it in here
540:52 - just to give you an understanding of
540:54 - where subdomains would fall so now that
540:57 - we understand how dns is structured i
541:00 - wanted to go through the breakdown of
541:02 - the data flow of dns to give you some
541:04 - better contacts now there are eight
541:07 - steps in a dns lookup first we start off
541:10 - with the dns client which is shown here
541:13 - as tony bowtie's laptop and this is a
541:16 - client device which could also be a
541:18 - phone or a tablet and is configured with
541:21 - software to send name resolution queries
541:24 - to a dns server so when a client needs
541:27 - to resolve a remote host name into its
541:30 - ip address in most cases it sends a
541:33 - request to the dns recursive resolver
541:36 - which returns the ip address of the
541:39 - remote host to the client a recursive
541:42 - resolver
541:43 - is a dns server that is configured to
541:46 - query other dns servers until it finds
541:49 - the answer to the question it will
541:51 - either return the answer or an error
541:53 - message to the client if it cannot
541:55 - answer the query and the query will
541:58 - eventually be passed off to the dns
542:00 - client the recursive resolver in essence
542:03 - acts as the middle man between a client
542:06 - and a dns name server which is usually
542:09 - the internet service provider a service
542:12 - carrier or a corporate network now to
542:14 - make sure that a resolver is able to
542:16 - properly run dns a root hints file is
542:20 - supplied with almost every operating
542:22 - system and this file holds the ip
542:25 - addresses for the root name servers this
542:28 - also includes the dns resolver but in
542:31 - case it is unable to answer the query
542:34 - the client will be able to still make
542:36 - the query to the dns name servers now
542:39 - after receiving a dns query from a
542:41 - client
542:42 - this recursive resolver will either
542:44 - respond with cache data or send a
542:47 - request to a root name server and in
542:50 - this case the resolver queries a dns
542:53 - root name server the root server then
542:55 - responds to the resolver with the
542:57 - address of a top level domain or tld dns
543:02 - server such as com or dot net which
543:05 - stores the information for its domains
543:08 - now when searching for google.com the
543:10 - request is pointed towards the dot-com
543:13 - tld so naturally the resolver then makes
543:16 - a request to the com tld then the tld
543:20 - name server then responds with the ip
543:23 - address of the domain's name server
543:26 - google.com and lastly the resolver then
543:29 - sends a query to the domain's name
543:31 - server the ip address for google.com is
543:35 - then returned to the resolver from the
543:37 - name server this ip address is cache for
543:40 - a period of time determined by the
543:43 - google.com name server and this process
543:46 - is so that a future request for this
543:48 - hostname could be resolved from its
543:51 - cache rather than performing the entire
543:54 - process from beginning to end and so for
543:56 - those of you who are unaware cache is a
543:59 - component that stores data so that
544:02 - future requests for that data can be
544:04 - served faster the purpose of this
544:06 - caching is to temporarily store data in
544:10 - a location that results in improvements
544:13 - in performance and reliability for data
544:16 - requests dns caching involves storing
544:19 - the data closer to the requesting client
544:22 - so that the dns query can be resolved
544:25 - earlier and additional queries further
544:28 - down the dns lookup chain can be avoided
544:31 - and thus improving load times dns data
544:34 - can be cached in a variety of locations
544:37 - down the chain each of which will store
544:39 - dns records for a set amount of time
544:42 - determined by a time to live also known
544:45 - as ttl for short and this value is the
544:48 - time to live for that domain record a
544:51 - high ttl for a domain record means that
544:54 - local dns resolvers will cache responses
544:58 - for longer and give quicker responses
545:01 - however making changes to dns records
545:04 - can take longer due to the need to wait
545:07 - for all cash records to expire
545:10 - alternatively domain records with low
545:13 - ttls can change much more quickly but
545:16 - dns resolvers will need to refresh their
545:19 - records more often and so in this final
545:21 - step the dns resolver then responds to
545:24 - the web browser with the ip address of
545:27 - the domain requested initially and once
545:30 - these eight steps of the dns lookup have
545:32 - returned the ip address for
545:35 - www.google.com
545:37 - the browser is able to make the request
545:40 - for the webpage and so the browser will
545:42 - reach out to the ip address of the
545:44 - server and request the web page which
545:47 - will be loaded up in the browser now i
545:49 - know this probably has been a review for
545:52 - those who are a bit more advanced when
545:54 - it comes to understanding dns but for
545:57 - others who are fairly new to the
545:58 - underpinnings of dns i hope this has
546:01 - given you a basic understanding of what
546:04 - it is why we use it and how it works
546:07 - moving forward in the course i will be
546:10 - discussing dns with regards to different
546:12 - services and the needed resource records
546:15 - within zones that are used by these
546:18 - given services and so that's pretty much
546:20 - all i wanted to cover when it comes to
546:23 - the fundamentals of dns so you can now
546:25 - mark this lesson as complete and let's
546:28 - move on to the next one
546:29 - [Music]
546:33 - welcome back in this lesson i'm going to
546:36 - be diving into dns record types now dns
546:40 - resource records are the basic
546:41 - information elements of the domain name
546:44 - system they are entries in the dns
546:46 - database which provide information about
546:49 - hosts these records are physically
546:52 - stored in the zone files on the dns
546:55 - server this lesson will go through some
546:57 - of the most commonly used dns records
547:00 - that we will be coming across throughout
547:02 - this course so with that being said
547:05 - let's dive in now the first record that
547:07 - i wanted to touch on are the name server
547:10 - records also known as ns records for
547:13 - short this record identifies which dns
547:16 - server contains the current records for
547:19 - a domain these servers are usually found
547:22 - at a registrar internet service provider
547:25 - or hosting company ns records are
547:28 - created to identify the name server used
547:31 - for each domain name within a given zone
547:34 - in this example we have the dot co zone
547:37 - that will have multiple name server
547:40 - records for
547:41 - bowtieinc.co now these name server
547:44 - records are how the dot co delegation
547:47 - happens for bowtieinc.co and they point
547:50 - at servers that host the
547:53 - inc.co zone that is managed by bowtie
547:56 - inc and the flow shown here of the query
547:59 - starts from the root zone going to the
548:01 - dot co zone where the record lies for
548:04 - the name servers for bowtieinc.com
548:08 - and flows down to the bowtieinc.cozone
548:11 - that contain all the necessary records
548:13 - for bowtieinc.co
548:15 - the next record that i wanted to touch
548:17 - on are the a and aaa records and this is
548:21 - short for address records for ipv4 and
548:25 - ipv6 ip addresses respectively and this
548:29 - record points a domain name to an ip
548:31 - address for example when you type wwe
548:37 - in a web browser the dns system will
548:40 - translate that domain name
548:42 - to the ip address of 52.54.92.195
548:49 - using the a record information stored in
548:52 - the bowtieinc.co
548:53 - dns zone file the a record links a
548:57 - website's domain name to an ipv4 address
549:00 - that points to the server where the
549:02 - website's files live now when it comes
549:05 - to an aaa record this links a website's
549:09 - domain to an ipv6 address that points to
549:13 - the same server where the website's
549:15 - files live a records are the simplest
549:18 - type of dns records and one of the
549:20 - primary records used in dns servers you
549:24 - can do a lot with a records including
549:26 - using multiple a records for the same
549:29 - domain in order to provide redundancy
549:32 - the same can be said
549:34 - for aaa records additionally multiple
549:37 - domains could point to the same address
549:40 - in which case each would have its own a
549:43 - or aaa record pointing to that same ip
549:47 - address
549:48 - moving on to cname records a c name
549:51 - record short for canonical name record
549:54 - is a type of resource record that maps
549:57 - one domain name to another this can be
549:59 - really convenient when running multiple
550:02 - services like an ftp server and an
550:05 - e-commerce server each running on
550:07 - different ports from a single ip address
550:10 - you can for example
550:12 - point ftp ftp.bowtieinc.co
550:15 - and shop.bowtieinc.co
550:18 - to the dns entry for bowtieinc.co
550:22 - which in turn has an a record which
550:25 - points to the ip address so if the ip
550:28 - address ever changes
550:29 - you only have to change the record in
550:32 - one place in the dns a record for bow
550:36 - tie inc dot co and just as a note cname
550:39 - records must always point to another
550:41 - domain name and never directly to an ip
550:44 - address next up are txt records a text
550:48 - record or txt for short is a type of
550:52 - resource record that provides text
550:54 - information to sources outside your
550:57 - domain that can be used for a number of
551:00 - arbitrary purposes the records value can
551:03 - be either human or machine readable text
551:06 - in many cases text records are used to
551:09 - verify domain ownership or even to
551:12 - provide human readable information about
551:15 - a server a network or a data center it
551:18 - is also often used in a more structured
551:21 - fashion to record small amounts of
551:24 - machine readable data into the dns
551:27 - system a domain may have multiple tax
551:29 - records associated with it
551:32 - provided the dns server implementation
551:34 - supports this each record can in turn
551:37 - have one or more character strings in
551:40 - this example
551:41 - google wants to verify the bowtieinc.co
551:44 - domain so that g suite can be set up and
551:48 - needs verification through the domain to
551:51 - google through creating a text record
551:53 - and adding it to the zone google will
551:56 - then supply a text verification record
551:58 - to add to the domain host's dns records
552:02 - and start to scan for the text record to
552:05 - verify the domain
552:07 - the supplied text record is then added
552:10 - by the domain administrator and behind
552:13 - the scenes google is doing a
552:15 - verification check at timed intervals
552:18 - when google finally sees the record
552:20 - exists the domain ownership is confirmed
552:23 - and g suite can be enabled for the
552:25 - domain and this is a typical example of
552:28 - how tax records are used now moving on
552:31 - to mx records a dns
552:34 - mx record also known as the mail
552:37 - exchange record is the resource record
552:40 - that directs email to a mail server the
552:43 - mx record indicates how email messages
552:46 - should be routed and to which server
552:49 - mail should go to like cname records an
552:52 - mx record must always point to another
552:55 - domain now mx records consist of two
552:58 - parts the priority
553:00 - and the domain name the priority are the
553:03 - numbers before the domains for these mx
553:06 - records and indicate the preference of
553:09 - the order in which the mail server
553:11 - should be used the lower the preference
553:13 - number the higher the priority so in
553:16 - this example
553:17 - laura is emailing tony bowtie at tony at
553:21 - bowtieinc.co
553:23 - the mx records are part of this process
553:26 - as dns needs to know where to send the
553:29 - mail to and we'll look at the domain
553:31 - attached to the email address which is
553:34 - bowtieinc.co so the dns client will run
553:37 - a regular dns query by first going to
553:40 - the root then to the
553:42 - cotld and finally to bowtieinc.co
553:46 - it will then receive the mx record which
553:49 - in this example is two of them the first
553:52 - one being mail representing
553:54 - mail.bowtieinc.co
553:56 - and then the second one is a different
553:58 - mail server outside the current domain
554:01 - and in this case is a google mail server
554:04 - of aspmx.l.google.com
554:08 - and this is a fully qualified domain
554:10 - name as the dot on the right of this
554:12 - record suggests so here the server will
554:15 - always try mail.bowtieinc.co
554:18 - first because 5 is lower than 10. and
554:22 - this will give mail.bowtieinc.co
554:25 - the higher priority in the result of a
554:28 - message send failure the server will
554:30 - default to aspmx.l.google.com
554:35 - if both values are the same then it
554:37 - would be low balanced across both
554:39 - servers whichever is used the server
554:42 - gets the result of the query back and it
554:45 - uses this to connect to the mail server
554:47 - for bowtieinc.co via the smtp protocol
554:52 - and it uses this protocol to deliver all
554:55 - email and this is how mx records are
554:58 - used for email the next record i wanted
555:00 - to cover are the pointer records
555:03 - also known as ptr records for short and
555:06 - this provides the domain name associated
555:09 - with an ip address so a dns pointer
555:11 - record is exactly the opposite of the a
555:14 - record which provides the ip address
555:17 - associated with the domain name dns
555:20 - pointer records are used in reverse dns
555:23 - lookups as we discussed earlier when a
555:26 - user attempts to reach a domain name in
555:28 - their browser a dns lookup occurs
555:31 - matching the domain name to the ip
555:33 - address a reverse dns lookup is the
555:36 - opposite of this process and it is a
555:38 - query that starts with the ip address
555:41 - and looks up the domain name while dnsa
555:44 - records are stored under the given
555:46 - domain name dns pointer records are
555:49 - stored under the ip address reverse and
555:52 - ending in dot i n
555:55 - a d d r dot arpa so in this example the
555:59 - pointer record for the iap address
556:01 - 52.54.90
556:05 - would be stored under 195.92.54.52
556:12 - dot in
556:13 - addr dot arpa ipv6 addresses are
556:17 - constructed differently from ipv4
556:19 - addresses and ipv6 pointer records exist
556:23 - in a different namespace
556:25 - within.arpa ipv6 pointer records are
556:28 - stored under the ipv6 address reversed
556:32 - and converted into 4-bit sections as
556:35 - opposed to 8-bit sections as in ipv4 and
556:39 - as well the domain.ip6.arpa
556:43 - is added at the end pointer records are
556:46 - used most commonly in reverse dns
556:48 - lookups for anti-spam troubleshooting
556:51 - email delivery issues and logging and so
556:54 - the last record that i wanted to cover
556:56 - are the soa records also known as the
556:59 - start of authority records and this
557:02 - resource record is created for you when
557:04 - you create your managed zone and
557:07 - specifies the authoritative information
557:10 - including global parameters about a dns
557:13 - zone the soa record stores important
557:16 - information about a domain or zone such
557:19 - as the email address of the
557:20 - administrator when the domain was last
557:23 - updated and how long the server should
557:25 - wait between refreshes every dns zone
557:28 - registered must have an soa record as
557:31 - per the rfc 1035 and there is exactly
557:35 - one soa record per zone the soa record
557:39 - contains the core information about your
557:42 - zone so it is not possible for your zone
557:45 - to work without that information and i
557:48 - will include a link in the lesson text
557:50 - for those who are interested in diving
557:52 - deeper and understanding all the
557:54 - information that is covered under these
557:57 - soa records a properly optimized and
558:00 - updated soa record can reduce bandwidth
558:03 - between name servers increase the speed
558:06 - of website access and ensure the site is
558:08 - alive even when the primary dns server
558:12 - is down and so that about covers
558:14 - everything that i wanted to discuss when
558:16 - it comes to resource records within dns
558:20 - so you can now mark this lesson as
558:21 - complete
558:22 - and let's move on to the next one
558:24 - [Music]
558:28 - welcome back
558:29 - in this lesson i'm going to be covering
558:32 - network address translation also known
558:35 - as nat for short this is a common
558:38 - process
558:39 - used in home business and any cloud
558:42 - networks that you will encounter knowing
558:45 - and understanding that will help you
558:47 - achieve why you would use it and what
558:50 - makes it such a necessary process
558:53 - now there's quite a bit to cover here so
558:55 - with that being said let's dive in
558:58 - now at a high level nat is a way to map
559:01 - multiple local private ip addresses to a
559:05 - public ip address before transferring
559:08 - the information this is done by altering
559:11 - the network address data in the ip
559:14 - header of the data packet while
559:17 - traveling through a network towards the
559:19 - destination
559:20 - as packets pass through a nat device
559:23 - either the source or destination ip
559:25 - address is changed
559:27 - then packets returning in the other
559:30 - direction are translated back to the
559:33 - original addresses
559:34 - and this is a process that is typically
559:37 - used in most home routers that are
559:40 - provided by your internet service
559:42 - provider now originally nat was designed
559:45 - to deal with the scarcity of free ipv4
559:49 - addresses increasing the number of
559:51 - computers that can operate off a single
559:54 - publicly routable ip address and so
559:57 - because devices in the private ip space
560:01 - such as 192.168.0.0
560:05 - cannot traverse the public internet
560:08 - that is needed for those devices to
560:11 - communicate with the public internet now
560:14 - ipv6 was designed to overcome the ipv4
560:18 - shortage and has tons of available
560:21 - addresses and therefore there is no real
560:23 - need for nat when it comes to ipv6 now
560:27 - nat has an additional benefit of adding
560:30 - a layer of security and privacy by
560:33 - hiding the ip address of your devices
560:36 - from the outside world and only allowing
560:39 - packets to be sent and received from the
560:42 - originating private device and so this
560:45 - is a high level of what nat is now there
560:48 - are multiple types of not that i will be
560:50 - covering
560:51 - which at a high level do the same thing
560:54 - which is translate private i p addresses
560:57 - to public ip addresses yet different
561:00 - types of nat handles the process
561:02 - differently so first we have static nat
561:05 - which maps a single private ip address
561:09 - to a public ip address
561:11 - so a one-to-one mapping that gives the
561:13 - device with the private ip address
561:16 - access to the public internet in both
561:19 - directions
561:20 - this is commonly used where one specific
561:23 - device with a private address needs
561:26 - access to the public internet the next
561:29 - type of nat is dynamic nan and this is
561:31 - similar to static nat but doesn't hold
561:34 - the same static allocation a private ip
561:37 - address space
561:39 - is mapped to a pool of public ip
561:41 - addresses and are allocated randomly as
561:45 - needed when the ip address is no longer
561:47 - needed the ip address is returned back
561:50 - to the pool ready to be used by another
561:53 - device
561:54 - this method is commonly used where
561:57 - multiple internal hosts with private i p
562:00 - addresses
562:02 - are sharing an equal or fewer amount of
562:04 - public i p addresses
562:06 - and is designed to be an efficient use
562:09 - of public ips and finally there is port
562:12 - address translation or pat
562:15 - where multiple private ip addresses are
562:18 - translated using a single public ip
562:21 - address and a specific port
562:24 - and this is probably what your home
562:25 - router is using and will cover all the
562:28 - devices you use in your home network
562:31 - this method uses ports to help
562:34 - distinguish individual devices
562:37 - and is also the method that is used for
562:39 - cloudnat in google cloud which i will be
562:43 - covering in a later lesson and so i
562:45 - wanted to get into a bit more detail on
562:48 - how these methods work
562:50 - starting with static not
562:53 - now to set the stage for static not i'm
562:55 - going to start off with a private
562:57 - network here on the left
562:59 - and the public ip space here on the
563:02 - right
563:02 - and the router or not device in the
563:05 - middle in this example there is a server
563:08 - on the left that needs access to
563:11 - external services and for this example
563:14 - the external service we are using is the
563:17 - bowtress service an image sharing site
563:20 - for all sorts of awesome bow ties
563:23 - so the server on the left is private
563:26 - with a private ip address of 192.168.0.5
563:32 - and this means it has an address in the
563:35 - ip version 4 private address space
563:39 - meaning that it cannot route packets
563:41 - over the public internet because it only
563:44 - has a private ip
563:46 - the beautress service on the other hand
563:48 - has a public ip address which is
563:51 - 54.5.4.9
563:54 - so the issue we run into is that the
563:57 - private address can't be routed over the
564:00 - public internet because it's private and
564:02 - the public address of the beau trust
564:04 - service
564:05 - can't directly communicate with any
564:08 - private address because public and
564:10 - private addresses can communicate over
564:13 - the public internet what we need is to
564:16 - translate the private address that the
564:18 - server on the left has
564:20 - to a public ip that can communicate with
564:23 - the service on the right and vice versa
564:26 - now then that device will map the
564:28 - private ip to public ip
564:31 - using and maintaining a nat table and in
564:34 - this case of static nat the nat device
564:37 - will have a one-to-one mapping of the
564:40 - private ip address to a public ip
564:43 - address and can be allocated to the
564:46 - device specified which in this case is
564:48 - the server marked as 192.168.0.15
564:54 - and so in order for the server on the
564:55 - left
564:56 - to communicate with the beautress
564:58 - service the server will generate a
565:00 - packet as normal with the source ip of
565:03 - the packet being the server's private ip
565:06 - address and the destination ip of the
565:09 - packet being the ip of the bowtrust
565:12 - service now the router in the middle is
565:14 - the default gateway for any destination
565:17 - so any ip packets which are destined for
565:20 - anything but the local network are sent
565:23 - to the router so as you can see here
565:25 - with the entry in the table it will
565:27 - contain the private i p address of
565:30 - 192.168.0.15
565:34 - and mapped to the public address which
565:37 - in this case is 73.6.2.33
565:42 - and these are statically mapped to one
565:44 - another and so as the packet passes
565:47 - through the nat device the source
565:49 - address of the packet is translated
565:52 - from the private address to the mapped
565:55 - public address and this results in a new
565:58 - packet so this new packet still has
566:01 - beautrest as the destination
566:03 - but now it has a valid public ip address
566:07 - as the source
566:08 - and so this is the translation that
566:10 - happens through nat now this process
566:13 - works in a similar way in the other
566:15 - direction
566:16 - so when the beautress service receives
566:18 - the packet it sees the source as this
566:22 - public ip
566:23 - so when it responds with data its packet
566:26 - has its ip address as the source
566:29 - and the previous server's public ip
566:32 - address as the destination
566:34 - so it sends this packet back to this
566:37 - public ip so when the packet arrives at
566:40 - the nat device the table is checked
566:43 - it recognizes then that the ip is for
566:46 - the server and so this time for incoming
566:50 - traffic
566:50 - the destination ip address is updated to
566:54 - the corresponding private ip address and
566:58 - then the packet is forwarded through to
567:00 - the private server and this is how
567:03 - static nat works the source i p address
567:06 - is translated from the mapped private ip
567:09 - to public ip
567:11 - and for incoming traffic the destination
567:14 - i p address is translated from the
567:17 - allocated public ip to the corresponding
567:20 - private ip all without having to
567:22 - configure a public ip
567:25 - on any private device
567:27 - as they always hold their private ip
567:30 - addresses
567:31 - now i wanted to supply an analogy for
567:34 - nat and so a very common analogy that is
567:37 - used is that of a phone service so in
567:40 - this example
567:41 - laura is the new manager of bow tie inc
567:44 - new location in montreal and has put in
567:48 - a new public phone number of
567:49 - 514-555-8437
567:53 - although as you can see here laura also
567:56 - has a private extension
567:58 - of one three three seven now if george
568:01 - called laura at that public phone number
568:03 - he would reach laura without ever
568:06 - knowing her private extension so the
568:08 - private extension acts as that private
568:11 - ip address
568:12 - and the public phone number would act as
568:14 - the public ip address and this would be
568:17 - the telephone analogy for static nat and
568:20 - so this is the end of part one of this
568:23 - lesson it was getting a bit long so i
568:25 - decided to break it up this would be a
568:28 - great opportunity for you to get up and
568:30 - have a stretch get yourself a coffee or
568:32 - a tea and whenever you're ready you can
568:35 - join me in part two where we will be
568:37 - starting immediately from the end of
568:40 - part one so you can go ahead and
568:42 - complete this video and i will see you
568:44 - in part two
568:45 - [Music]
568:50 - welcome back this is part two of the
568:52 - network address translation lesson and
568:55 - we will be starting exactly where we
568:57 - left off from part 1.
569:00 - so with that being said let's dive in
569:03 - now moving on to dynamic nat
569:05 - this method is similar to static nat
569:08 - except that devices are not allocated a
569:12 - permanent public ip
569:14 - a public ip address is allocated from a
569:17 - pool of ip addresses
569:20 - as they are needed and the mapping of
569:22 - public to private is allocation base in
569:25 - this example there are two devices on
569:28 - the left and according to the nat table
569:31 - there are two public ip addresses
569:33 - available for use
569:35 - 73.6.2.33
569:38 - and 73.6.2.34
569:42 - so when the laptop on the left is
569:44 - looking to access the beautress service
569:46 - it will generate a packet where the
569:49 - source ip
569:50 - is the private address of 192.168.0.13
569:56 - and the destination ip is 54.5.4.9
570:02 - so it sends this packet and again the
570:04 - router in the middle is the default
570:06 - gateway for anything that isn't local as
570:10 - the packet passes through the router or
570:13 - the nat device
570:14 - it checks if the private ip has a
570:17 - current allocation of public addressing
570:20 - from the pool and if it doesn't and one
570:23 - is available it allocates one
570:25 - dynamically and in this case
570:27 - 73.6.2.34
570:30 - is allocated
570:32 - so the packet's source i p address
570:35 - is translated to this address
570:37 - and the packets are sent to the
570:39 - beautress service and so this process is
570:42 - the same as static not thus far
570:45 - but because dynamic nat allocates these
570:48 - ip addresses dynamically multiple
570:50 - private devices can share a single
570:53 - public ip
570:55 - as long as the devices are not using the
570:58 - same public ip at the same time and so
571:01 - once the device is finished
571:03 - communication the ip is returned back to
571:06 - the pool and is ready for use by another
571:09 - device now just as a note if there's no
571:12 - public ip addresses available
571:15 - the router rejects any new connections
571:18 - until you clear the nat mappings
571:21 - but if you have as many public ip
571:23 - addresses as hosts in your network
571:26 - you won't encounter this problem and so
571:29 - in this case since the lower server is
571:31 - looking to access the fashion tube
571:33 - service
571:34 - there is an available public ip address
571:37 - in the pool
571:38 - of 73.6.2.33
571:42 - thus giving it access to the public
571:45 - internet and access to fashion tube so
571:48 - in summary the nat device maps a private
571:51 - ip with the public ip in a nat table and
571:55 - public ips are allocated randomly and
571:58 - dynamically from a pool now this type of
572:02 - knot is used where multiple internal
572:04 - hosts with private ip addresses
572:07 - are sharing an equal or fewer amount of
572:10 - public ip addresses when all of those
572:13 - private devices at some time will need
572:16 - public access
572:18 - now an example of dynamic nat using the
572:21 - telephone analogy
572:22 - would be if laura and two other bow tie
572:25 - inc employees
572:27 - lisa and jane
572:28 - had private phone numbers
572:30 - and this would represent your private
572:33 - ips
572:34 - in this example bowtie inc has three
572:36 - public phone numbers
572:38 - now when any employee makes an outbound
572:41 - call they are routed to whichever public
572:44 - line is open at the time so the caller
572:47 - id on the receiver's end
572:49 - would show any one of the three public
572:52 - phone numbers depending on which one was
572:55 - given to the caller and this would
572:58 - represent the public ips in the public
573:01 - ip pool
573:03 - now the last type of nat which i wanted
573:05 - to talk about is the one which you're
573:07 - probably most familiar with and this is
573:10 - port address translation which is also
573:13 - known as not overload and this is the
573:16 - type of not you likely use on your home
573:19 - network port address translation is what
573:22 - allows a large number of private devices
573:25 - to share one public ip address
573:28 - giving it a many to one mapping
573:31 - architecture now in this example we'll
573:33 - be using three private devices on the
573:36 - left
573:37 - all wanting to access fashiontube on the
573:39 - right
573:40 - a popular video sharing website of the
573:43 - latest men's fashions
573:45 - shared by millions across the globe
573:48 - and this site has a public ip of
573:51 - 62.88.44.88
573:55 - and accessed using tcp port 443 now the
573:59 - way that port address translation or pat
574:02 - works
574:02 - is to use both the ip addresses and
574:05 - ports
574:06 - to allow for multiple devices to share
574:09 - the same public ip every tcp connection
574:13 - in addition to a source and destination
574:16 - ip address
574:17 - has a source and destination port the
574:20 - source port is randomly assigned by the
574:23 - client so as long as the source port is
574:26 - always unique then many private clients
574:29 - can use the same public ip address and
574:32 - all this information is recorded in the
574:35 - nat table on the nat device
574:38 - in this example let's assume that the
574:40 - public ip address of this nat device is
574:43 - 73.6.2.33
574:47 - so when the laptop in the top left
574:49 - generates a packet and the packet is
574:52 - going to fashion tube its destination ip
574:55 - address is 62.80
575:00 - and its destination port is 443. now the
575:04 - source ip of this packet is the laptop's
575:07 - private ip address of 192.168.6
575:13 - and the source port is 35535
575:17 - which is a randomly assigned ephemeral
575:20 - port so the packet is routed through the
575:22 - nat device
575:23 - and in transit the nat device records
575:26 - the source ip and the original source
575:29 - private port
575:31 - and it allocates a new public ip address
575:34 - and a new public source port which in
575:36 - this case is 8844
575:39 - it records this information inside the
575:42 - not table as shown here and it adjusts
575:44 - the pocket so that its source ip address
575:47 - is the public ip address that the nat
575:50 - device is using and the source port is
575:53 - this newly allocated source port and
575:56 - this newly adjusted packet is forwarded
575:58 - on to fashiontube now the process is
576:01 - very similar with the return traffic
576:04 - where the packet will verify the
576:06 - recorded ips and ports
576:08 - in the nat table before forwarding the
576:11 - packet back to the originating source
576:14 - now if the middle laptop with the ip of
576:16 - 192.168.0.14
576:20 - did the same thing then the same process
576:23 - would be followed all of this
576:25 - information would be recorded in the nat
576:28 - table a new public source port would be
576:30 - allocated and would translate the packet
576:33 - adjusting the packet's source ip address
576:36 - and source port as well the same process
576:39 - would happen for the laptop on the
576:41 - bottom generating a packet with the
576:44 - source and destination ip with the
576:46 - addition of the source and destination
576:49 - ports and when routed through the nat
576:51 - device goes through its translation
576:54 - recording the information in the nat
576:56 - table and reaching its destination again
576:59 - return traffic will be verified by the
577:02 - recorded ips and ports in the nat table
577:06 - before forwarding the packet back to its
577:09 - originating source and so just as a
577:11 - summary when it comes to port address
577:14 - translation the nat device records the
577:16 - source ip and source port in a nat table
577:20 - the source ip is then replaced with a
577:23 - public ip and public source port
577:26 - and are allocated from a pool that
577:28 - allows overloading and this is a
577:31 - many-to-one architecture
577:33 - and so for the telephone analogy for pat
577:36 - let's use a phone operator example so in
577:39 - this instance george is trying to call
577:41 - laura now george only knows lark laura's
577:45 - executive admin and only has lark's
577:48 - phone number george does not have
577:50 - laura's private line lark's public phone
577:52 - number is the equivalent to having a
577:55 - public ip address george calls lark who
577:58 - then connects george to laura the caveat
578:01 - here is that lark never gives out
578:03 - laura's phone number in fact laura
578:06 - doesn't have a public phone number and
578:08 - can only be called by lark and here's
578:11 - where nat can add an extra layer of
578:14 - security by only allowing needed ports
578:17 - to be accessed without allowing anyone
578:20 - to connect to any port now i hope this
578:23 - has helped you understand the process of
578:26 - network address translation
578:28 - how the translation happens
578:30 - and the process of using a nat table to
578:33 - achieve packet translation
578:35 - along with its destination this is so
578:38 - common in most environments that you
578:41 - will encounter and it's very important
578:44 - to fully understand the different types
578:46 - of not
578:47 - and how it can be used in these types of
578:50 - environments and so that's pretty much
578:52 - all i wanted to cover
578:54 - on this lesson of network address
578:56 - translation so you can now mark this
578:59 - lesson as complete
579:00 - and let's move on to the next one
579:02 - [Music]
579:06 - welcome back so now that we've covered
579:08 - the fundamentals of dns along with the
579:11 - different record types i wanted to focus
579:14 - in on google cloud's dns service called
579:17 - cloud dns now cloud dns is a fully
579:20 - managed service that manages dns servers
579:24 - for your specific zones and since cloud
579:26 - dns shows up on the exam only on a high
579:29 - level i will be giving an overview of
579:32 - what this service can do so with that
579:34 - being said let's dive in now cloud dns
579:38 - acts as an authoritative dns server for
579:41 - public zones that are visible to the
579:44 - internet or for private zones that are
579:46 - visible only within your network and is
579:49 - commonly referred to as google's dns as
579:52 - a service cloud dns has servers that
579:55 - span the globe making it a globally
579:58 - resilient service now while it is a
580:00 - global service there is no way to select
580:03 - specific regions to deploy your zones
580:07 - and dns server policies you simply add
580:09 - your zones records and policies and it
580:12 - is distributed amongst google's dns
580:15 - servers across the globe cloud dns is
580:18 - also one of the few google cloud
580:20 - services that offers 100
580:23 - availability along with low latency
580:26 - access
580:27 - by leveraging google's massive global
580:30 - network backbone now in order to use
580:32 - cloud dns with a specific publicly
580:35 - available domain a domain name must be
580:38 - purchased through a domain name
580:40 - registrar and you can register a domain
580:42 - name through google domains or another
580:45 - domain registrar of your choice cloud
580:48 - dns does not provide this service and
580:50 - just as a note that to create private
580:53 - zones the purchasing of a domain name is
580:56 - not necessary now as stated earlier
580:58 - cloud dns offers the flexibility of
581:01 - hosting both public zones and privately
581:04 - managed dns zones now public zones are
581:08 - zones that are visible to the public
581:10 - internet and so when cloud dns is
581:12 - managing your public domain it has
581:15 - public authoritative name servers that
581:18 - respond to public zone dns queries for
581:21 - your specific domain now when it comes
581:23 - to private zones these enable you to
581:26 - manage custom domain names
581:28 - for your google cloud resources without
581:31 - exposing any dns data to the public
581:34 - internet a private zone can only be
581:36 - queried by resources in the same project
581:39 - where it is defined and as we discussed
581:42 - earlier a zone is a container of dns
581:45 - records that are queried by dns so from
581:48 - a private zone perspective these can
581:50 - only be queried by one or more vpc
581:53 - networks that you authorize to do so and
581:56 - just as a note the vpc networks that you
581:58 - authorize must be located in the same
582:01 - project as the private zone to query
582:04 - records hosted in manage private zones
582:06 - in other projects the use of dns peering
582:09 - is needed now i don't want to get too
582:11 - deep into dns peering
582:14 - but just know that vpc network peering
582:16 - is not required for the cloud dns
582:19 - peering zone to operate peering zones do
582:22 - not depend on vpc network peering now
582:26 - each managed zone that you create is
582:28 - associated with a google cloud project
582:31 - and once this zone is created it is
582:34 - hosted by google's managed name servers
582:36 - now these zones are always hosted on
582:39 - google's manage name servers within
582:42 - google cloud so you would create records
582:44 - and record sets and these servers would
582:46 - then become allocated to that specific
582:49 - zone hosting your records and record
582:52 - sets and just as a quick reminder a
582:54 - record set is the collection of dns
582:57 - records in a zone that have the same
582:59 - name and are of the same type most
583:02 - records contain a single record but it's
583:04 - not uncommon to see record sets a great
583:07 - example of this are a records or ns
583:10 - records which we discussed earlier and
583:12 - these records can usually be found in
583:15 - pairs
583:16 - and so now to give you a practical
583:18 - example of cloud dns i wanted to bring
583:20 - the theory into practice through a short
583:23 - demo where i'll be creating a managed
583:26 - private zone so whenever you're ready
583:28 - join me in the console and so here we
583:30 - are back in the console and i'm logged
583:32 - in as tonybowties gmail.com and i'm
583:36 - currently in project bowtie inc so now
583:38 - to get to cloud dns i'm going to go over
583:40 - to the navigation menu i'm going to
583:42 - scroll down to network services and go
583:46 - over to cloud dns
583:49 - and because i currently don't have any
583:50 - zones
583:52 - i'm prompted with only one option which
583:54 - is to create a zone and so i'm going to
583:56 - go ahead and create a zone and so here
583:58 - i've been prompted with a bunch of
584:00 - different options in order to create my
584:02 - dns zone and so the first option that i
584:05 - have is zone type and because i'm
584:08 - creating a private zone i'm going to
584:10 - simply click on private and i need to
584:12 - provide a zone name which i'm going to
584:14 - call tony bowtie next i'm going to have
584:17 - to provide a dns name which i will call
584:20 - tony bowtie dot private and under the
584:23 - description i'm just going to type in
584:25 - private zone for tony bowtie and so the
584:29 - next field i've been given is the
584:31 - options field where it is currently
584:33 - marked as default private and so if i go
584:36 - over here to the right hand side and
584:38 - open up the drop down menu i'm given the
584:40 - options to forward queries to another
584:42 - server dns peering manage reverse lookup
584:46 - zones and use a service directory
584:48 - namespace and so depending on your type
584:51 - of scenario one of these five options in
584:53 - most cases will suffice so i'm going to
584:56 - keep it under default private and under
584:58 - networks it says your private zone will
585:00 - be visible to the selected networks
585:02 - and so i'm going to click on the drop
585:04 - down and i'm giving only the option of
585:06 - the default network because it's the
585:08 - only network that i have and so i'm
585:10 - going to select it
585:12 - and i'm going to click on the white
585:13 - space and if i feel so inclined i can
585:16 - simply click on the shortcut for the
585:18 - command line and here i'm given this
585:20 - specific commands if i was to use the
585:22 - command line in order to create this dns
585:25 - zone so i'm going to click on close here
585:27 - and i'm going to click on create and as
585:29 - you can see here my zone has been
585:32 - created along with a couple of dns
585:34 - records the first one being my name
585:36 - server records as well as my start of
585:39 - authority records and so as a note to
585:41 - know for the exam when creating a zone
585:44 - these two records will always be created
585:47 - both the soa record and the ns record
585:50 - and moving on to some other options here
585:53 - i can add another record set if i choose
585:55 - to again the dns name the record type
585:58 - which i have a whole slew of record
586:01 - types to choose from it's ttl and the ip
586:04 - address but i'm not going to add any
586:06 - records so i'm just going to cancel and
586:08 - by clicking in use by i can view which
586:11 - vpc network is using this zone and as
586:13 - expected the default network shows up
586:16 - and i also have the choice of adding
586:17 - another network but since i don't have
586:19 - any other networks i can't add anything
586:22 - so i'm going to simply cancel i also
586:23 - have the option of removing any networks
586:26 - so if i click on this i can remove the
586:28 - network or i can also remove the network
586:30 - by clicking on the hamburger menu and so
586:32 - as you can see i have a slew of options
586:35 - to choose from when creating zones and
586:38 - record sets and so that about covers
586:40 - everything that i wanted to show you
586:42 - here in cloud dns
586:44 - but before i go i'm going to go ahead
586:45 - and clean up and i'm just going to click
586:47 - on the garbage can here on the right
586:49 - hand side of the zone and i'm going to
586:51 - be prompted if i want to delete the zone
586:53 - yes i do so i'm going to click on delete
586:56 - and so that pretty much covers
586:57 - everything that i wanted to show you
586:59 - with regards to cloud dns so you can now
587:01 - mark this lesson as complete and let's
587:04 - move on to the next one
587:09 - welcome back now before we step into the
587:12 - compute engine section of the course
587:14 - i wanted to cover a basic foundation of
587:17 - what makes these vms possible
587:20 - and this is where a basic understanding
587:22 - of virtualization comes into play now
587:25 - this is merely an introductory lesson to
587:28 - virtualization and i won't be getting
587:30 - too deep into the underpinnings it
587:33 - serves as just a basic foundation as to
587:36 - how compute engine gets its features
587:38 - under the hood and how they are possible
587:41 - through the use of virtualization for
587:43 - more in-depth understanding on
587:45 - virtualization i will be including some
587:47 - links in the lesson text for those who
587:50 - are looking to learn more but for now
587:53 - this will provide just enough theory to
587:55 - help you understand how compute engine
587:58 - works
587:59 - so with that being said let's dive in so
588:02 - what exactly is virtualization well
588:05 - virtualization is the process of running
588:07 - multiple operating systems on a server
588:11 - simultaneously now before virtualization
588:14 - became popular a standard model was used
588:17 - where an operating system would be
588:19 - installed on a server
588:21 - so the server would consist of typical
588:24 - hardware like cpu
588:26 - memory network cards and other devices
588:29 - such as video cards usb devices and
588:33 - storage and then the operating system
588:35 - would run on top of the hardware now
588:37 - there is a middle layer of the operating
588:39 - system a supervisor if you will that is
588:42 - responsible for interacting with
588:45 - underlying hardware and this is known as
588:48 - the kernel the kernel manages the
588:50 - distribution of the hardware resources
588:52 - of the computer efficiently and fairly
588:55 - among all the various processes running
588:58 - on the computer now the kernel operates
589:01 - under what is called kernel mode or
589:04 - privilege mode as it runs privileged
589:06 - instructions that interacts with the
589:09 - hardware directly now the operating
589:11 - system allows other software to run on
589:14 - top of it like an application
589:16 - but cannot interact directly with the
589:19 - hardware it must interact with the
589:21 - operating system in user mode or
589:24 - non-privileged mode so when lark decides
589:27 - to do something on an application that
589:29 - needs to use the system hardware that
589:32 - application needs to go through the
589:34 - operating system it needs to make what's
589:37 - known as a system call and this is the
589:39 - model of running one operating system on
589:43 - a single server now when passed servers
589:46 - would traditionally run one application
589:48 - on one server with one operating system
589:52 - in the old system the number of servers
589:54 - would continue to mount
589:56 - since every new application required its
589:58 - own server and its own operating system
590:01 - as a result expensive hardware resources
590:04 - were purchased but not used and each
590:07 - server would use approximately under 20
590:11 - of its resources on average server
590:14 - resources were then known as
590:17 - underutilized now there came a time when
590:19 - multiple operating systems were
590:21 - installed on one computer
590:23 - isolated from each other with each
590:26 - operating system running their own
590:28 - applications this was a perfect model to
590:31 - consolidate hardware and keep
590:33 - utilization high but there is a major
590:36 - issue that arose each cpu at this given
590:39 - moment in time could only have one thing
590:42 - running as privileged so having multiple
590:44 - operating systems running on their own
590:47 - in an unmodified state
590:49 - and expecting to be running on their own
590:51 - in a privileged state running privileged
590:54 - instructions
590:55 - was causing instability in systems
590:58 - causing not just application crashes but
591:02 - system crashes now a hypervisor is what
591:06 - solved this problem it is a small
591:08 - software layer that enables multiple
591:11 - operating systems to run alongside each
591:14 - other
591:15 - sharing the same physical computing
591:17 - resources these operating systems come
591:20 - as virtual machines or vms and these are
591:24 - files that mimic an entire computing
591:27 - hardware environment in software the
591:29 - hypervisor also known as a virtual
591:32 - machine monitor or vmm
591:35 - manages these vms as they run alongside
591:38 - each other it separates virtual machines
591:41 - from each other logically assigning each
591:44 - its own slice of the underlying
591:46 - computing cpu memory and other devices
591:50 - like graphics network and storage this
591:53 - prevents the vms from interfering with
591:56 - each other so if for example one
591:59 - operating system suffers a crash or a
592:01 - security compromise the others will
592:04 - survive and continue running now the
592:06 - hypervisor was never as efficient as how
592:09 - you see it here it went through some
592:11 - major iterations that gave its structure
592:14 - as we know it today initially
592:17 - virtualization had to be done in
592:18 - software or what we now refer to as the
592:21 - host machine
592:23 - and the operating system with its
592:24 - applications put in logical containers
592:27 - known as virtual machines or guests the
592:30 - operating system would be installed on
592:32 - the host which included additional
592:35 - capabilities called a hypervisor and
592:38 - allowed it to make the necessary
592:40 - privileged calls to the hardware
592:43 - having full access to the host the
592:45 - hypervisor exposed the interface of the
592:48 - hardware device that is available on the
592:51 - host
592:52 - and allowed it to be mapped to the
592:54 - virtual machine and emulated the
592:56 - behavior of this device and this allowed
592:59 - the virtual machine using the operating
593:01 - system drivers that were designed to
593:04 - interact with the emulated device
593:07 - without installing any special drivers
593:09 - or tools
593:11 - as well as keeping the operating system
593:13 - unmodified the problem here is that it
593:16 - was all emulated and so every time the
593:19 - virtual machines made calls back to the
593:22 - host each instruction needed to be
593:25 - translated by the hypervisor
593:27 - using what's called a binary translation
593:30 - now without this translation the
593:32 - emulation wouldn't work and would cause
593:35 - system crashes bringing down all virtual
593:38 - machines in the process now the problem
593:41 - with this process is that it made the
593:44 - system painfully slow and it was this
593:46 - performance penalty that caused this
593:49 - process to not be so widely adopted but
593:52 - then another type of virtualization came
593:55 - on the scene called para virtualization
593:59 - now in this model a modified guest
594:01 - operating system is able to speak
594:04 - directly to the hypervisor and this
594:06 - involves having the operating system
594:08 - kernel to be modified and recompiled
594:12 - before installation onto the virtual
594:15 - machine this would allow the operating
594:17 - system to talk directly with the
594:20 - hypervisor without any performance hits
594:23 - as there is no translation going on like
594:26 - an emulation para virtualization
594:29 - replaces instructions that cannot be
594:31 - virtualized with hyper calls that
594:34 - communicate directly with the hypervisor
594:36 - so a hypercall is based on the same
594:39 - concept as a system call privileged
594:41 - instructions
594:42 - that accept instead of calling the
594:44 - kernel directly it calls the hypervisor
594:47 - and due to the modification in this
594:49 - guest operating system performance is
594:52 - enhanced as the modified guest operating
594:55 - system communicates directly with the
594:58 - hypervisor and emulation overhead is
595:01 - removed the guest operating system
595:03 - becomes almost virtualization aware yet
595:07 - there is still a process whereby
595:09 - software was used to speak to the
595:11 - hardware the virtual machines could
595:14 - still not access the hardware directly
595:17 - although
595:18 - things changed in the world of
595:19 - virtualization when the physical
595:21 - hardware on the host became
595:23 - virtualization aware and this is where
595:26 - hardware assisted virtualization came
595:28 - into play now hardware assisted
595:31 - virtualization is an approach that
595:34 - enables efficient
595:36 - full virtualization using help from
595:38 - hardware capabilities
595:40 - on the host cpu using this model the
595:44 - operating system has direct access to
595:46 - resources without any hypervisor
595:49 - emulation or operating system
595:51 - modification the hardware itself becomes
595:54 - virtualization aware the cpu contains
595:58 - specific instructions and capabilities
596:01 - so that the hypervisor can directly
596:03 - control and configure this support it
596:06 - also provides improved performance
596:09 - because the privileged instructions from
596:12 - the virtual machines are now trapped and
596:15 - emulated in the hardware directly this
596:17 - means that the operating system kernels
596:20 - no longer need to be modified and
596:22 - recompiled like in para virtualization
596:25 - and can run as is at the same time the
596:29 - hypervisor also does not need to be
596:31 - involved in the extremely slow process
596:35 - of binary translation now there is one
596:37 - more iteration that i wanted to discuss
596:40 - when it comes to virtualization and that
596:42 - is kernel level virtualization
596:45 - now instead of using a hypervisor
596:48 - kernel level virtualization runs a
596:51 - separate version of the linux kernel and
596:54 - sees the associated virtual machine as a
596:56 - user space process on the physical host
597:00 - this makes it easy to run multiple
597:02 - virtual machines on a single host a
597:05 - device driver is used for communication
597:08 - between the main linux kernel and the
597:11 - virtual machine every vm is implemented
597:14 - as a regular linux process
597:17 - scheduled by the standard linux
597:19 - scheduler
597:20 - with dedicated virtual hardware like a
597:23 - network card
597:24 - graphics adapter
597:25 - cpu memory and disk hardware support by
597:29 - the cpu is required for virtualization a
597:33 - slightly modified emulation process is
597:36 - used as the display and execution
597:38 - containers for the virtual machines in
597:41 - many ways kernel level virtualization is
597:44 - a specialized form of server
597:47 - virtualization and this is the type of
597:49 - virtualization platform that is used in
597:52 - all of google cloud now with this type
597:54 - of virtualization because of the kernel
597:57 - acting as the hypervisor it enables a
598:00 - specific feature called nested
598:02 - virtualization now with nested
598:05 - virtualization it is made possible to
598:08 - install a hypervisor on top of the
598:10 - already running virtual machine
598:13 - and so this is what google cloud has
598:15 - done now you're probably wondering after
598:17 - going through all the complexities
598:19 - involved with previous virtualization
598:22 - models
598:23 - what makes this scenario worthwhile well
598:26 - using nested virtualization it makes it
598:29 - easier for users to move their
598:31 - on-premises
598:33 - virtualized workloads to the cloud
598:36 - without having to import and convert vm
598:39 - images so in essence
598:41 - it eases the use when migrating to cloud
598:45 - a great use case for many but wouldn't
598:47 - be possible on google cloud without the
598:50 - benefit of running kernel level
598:52 - virtualization now this is an advanced
598:54 - concept that does not show up on the
598:56 - exam but i wanted you to understand
598:59 - virtualization at a high level
599:01 - so that you can understand nested
599:03 - virtualization within google cloud
599:06 - as it is a part of the feature set of
599:08 - compute engine and so that's pretty much
599:10 - all i wanted to cover when it comes to
599:13 - virtualization
599:14 - so you can now mark this lesson as
599:16 - complete and let's move on to the next
599:18 - one
599:18 - [Music]
599:22 - welcome back now earlier on in the
599:25 - course i discussed compute engine at a
599:27 - high level to understand what it is and
599:31 - what it does the goal for this section
599:33 - is to dive deeper into compute engine as
599:36 - it comes up heavily on the exam and so i
599:39 - want to make sure i expose all the
599:41 - nuances
599:42 - as well it is the go-to service offering
599:45 - from google cloud when looking to solve
599:48 - any general computing needs with this
599:50 - lesson specifically i will be going into
599:53 - what makes up an instance and the
599:55 - different options that are available
599:57 - when creating the instance so with that
599:59 - being said let's dive in
600:02 - now compute engine lets you create and
600:05 - run virtual machines known as instances
600:09 - and host them on google's infrastructure
600:11 - compute engine is google's
600:13 - infrastructure as a service virtual
600:16 - machine offering so it being an is
600:19 - service google takes care of the
600:21 - virtualization platform the physical
600:23 - servers the network and storage along
600:26 - with managing the data center and these
600:28 - instances are available in different
600:31 - sizes depending on how much cpu and
600:34 - memory you might need as well compute
600:36 - engine offers different family types for
600:39 - the type of workload you need it for
600:41 - each instance is charged by the second
600:44 - after the first minute as this is a
600:47 - consumption based model and as well
600:49 - these instances are launched in a vpc
600:52 - network in a specific zone and these
600:55 - instances will actually sit on hosts in
600:58 - these zones and you will be given the
601:00 - option of using a multi-tenant host
601:03 - where the server that is hosting your
601:05 - machine is shared with others
601:07 - but please note that each instance is
601:10 - completely isolated from the other so no
601:13 - one can see each other's instances
601:15 - now you're also given the option of
601:18 - running your instance on a sole tenant
601:20 - node whereby your instance is on its own
601:23 - dedicated hosts that is reserved just
601:26 - for you and you alone you don't share it
601:28 - with anyone else and this is strictly
601:31 - for you only now although this option
601:33 - may sound really great it does come at a
601:36 - steep cost
601:37 - so only if your use case requires you to
601:39 - use a sole tenant node for security or
601:43 - compliance purposes i recommend that you
601:45 - stick with a multi-tenant host when
601:48 - launching your instances and this is
601:50 - usually the most common selection for
601:52 - most
601:53 - now compute engine instances can be
601:55 - configured in many different ways and
601:57 - allow you the flexibility to fulfill the
602:00 - requests for your specific scenario and
602:02 - as you can see here there are four
602:05 - different base options when it comes to
602:07 - configuration of the instance that you
602:09 - are preparing to launch and so i wanted
602:12 - to take time to go through them in just
602:14 - a bit of detail for context starting
602:16 - first with the machine type which covers
602:20 - vcpu and memory now there are many
602:23 - different predefined machine types that
602:25 - i will be covering in great depth in a
602:28 - different lesson but for now just know
602:30 - that they are available in different
602:32 - families depending on your needs and can
602:35 - be chosen from the general
602:37 - compute optimize and memory optimize
602:40 - machine types they are available in
602:43 - intel or amd flavors and if the
602:45 - pre-defined options doesn't fit your
602:48 - need you have the option of creating a
602:50 - custom machine that will suit your
602:52 - specific workload now when creating a vm
602:55 - instance on compute engine each virtual
602:58 - cpu or vcpu is implemented as a single
603:02 - hardware hyper thread on one of the
603:05 - available cpu processors that live on
603:08 - the host now when choosing the amount of
603:10 - vcpus on an instance
603:12 - you must take into consideration the
603:15 - desired network throughput as the amount
603:18 - of vcpus will determine this throughput
603:21 - as the bandwidth is determined per vm
603:24 - instance not per network interface or
603:27 - per ip address and so the network
603:30 - throughput is determined by calculating
603:33 - 2 gigabits per second for every vcpu on
603:36 - your instance so if you're looking for
603:39 - greater network throughput then you may
603:41 - want to select an instance with more
603:44 - vcpus and so once you've determined a
603:46 - machine type for your compute engine
603:48 - instance you will need to provide it an
603:51 - image with an operating system to boot
603:53 - up with now when creating your vm
603:55 - instances you must use an operating
603:58 - system image to create boot disks for
604:01 - your instances now compute engine offers
604:04 - many pre-configured public images that
604:07 - have compatible linux or windows
604:10 - operating systems and these operating
604:12 - system images can be used to create and
604:15 - start instances compute engine uses your
604:18 - selected image to create a persistent
604:21 - boot disk for each instance by default
604:24 - the boot disk for your instance is the
604:27 - same size as the image that you selected
604:30 - and you can use most public images at no
604:33 - additional cost but please be aware that
604:36 - there are some premium images that do
604:38 - add additional cost to your instances
604:41 - now moving on to custom images this is a
604:44 - boot disk image that you own and control
604:47 - access to a private image if you will
604:50 - custom images are available only to your
604:53 - cloud project unless you specifically
604:56 - decide to share them with another
604:58 - project or another organization you can
605:01 - create a custom image from boot disks or
605:05 - other images then use the custom image
605:08 - to create an instance custom images that
605:10 - you import to compute engine add no cost
605:14 - to your instances but do incur an image
605:17 - storage charge
605:18 - while you keep your custom image in your
605:20 - project now the third option that you
605:22 - have is by using a marketplace image now
605:26 - google cloud marketplace lets you
605:28 - quickly deploy
605:29 - functional software packages that run on
605:32 - google cloud you can start up a software
605:35 - package without having to manually
605:37 - configure the software the vm instances
605:41 - the storage or even the network settings
605:43 - this is a all-in-one instance template
605:46 - that includes the operating system and
605:49 - the software pre-configured and you can
605:51 - deploy a software package whenever you
605:53 - like and is by far the easiest way to
605:56 - launch a software package and i will be
605:59 - giving you a run through on these
606:00 - marketplace images in a later demo now
606:04 - once you've decided on your machine type
606:06 - as well as the type of image that you
606:08 - wanted to use moving into the type of
606:11 - storage that you want would be your next
606:13 - step now when configuring a new instance
606:15 - you will need to create a new boot disk
606:18 - for it and this is where performance
606:20 - versus cost comes into play as you have
606:23 - the option to pay less and have a slower
606:26 - disk speed or lower iops or you can
606:29 - choose to have fast disk speed with
606:32 - higher iops but pay a higher cost and so
606:35 - the slowest and most inexpensive of
606:38 - these options is the standard persistent
606:40 - disk which are backed by standard hard
606:43 - disk drives the balance persistent disks
606:46 - are backed by solid state drives and are
606:49 - faster and can provide higher iops than
606:52 - the standard option and lastly ssd is
606:56 - the fastest option which also brings
606:58 - with it the highest iops available for
607:01 - persistent disks now outside of these
607:04 - three options for persistent disks you
607:06 - also have the option of choosing a local
607:09 - ssd and these are solid state drives
607:12 - that are physically attached to the
607:14 - server that hosts your vm instances and
607:17 - this is why they have the highest
607:19 - throughput and lowest latency than any
607:22 - of the available persistent disks just
607:25 - as a note the data that you store on a
607:27 - local ssd persists only until the
607:31 - instance is stopped or deleted which is
607:34 - why local ssds are suited only for
607:36 - temporary storage such as caches or swap
607:40 - disk and so lastly moving into
607:43 - networking
607:44 - each network interface of a compute
607:46 - engine instance is associated with a
607:49 - subnet of a unique vpc network as you've
607:53 - seen in the last section you can do this
607:55 - with an auto a default or a custom
607:58 - network each network is available in
608:01 - many different regions and zones within
608:04 - that region we've also experienced
608:06 - routing traffic for our instance both in
608:10 - and out of the vpc network
608:12 - by use of firewall rules targeting ip
608:15 - ranges
608:16 - specific network tags or by instances
608:19 - within the network now load balancers
608:22 - are responsible for helping distribute
608:25 - user traffic
608:26 - across multiple instances either within
608:29 - the network or externally using a
608:32 - regional or global load balancer
608:34 - and i will be getting into low balancing
608:37 - in another section of the course but i
608:39 - wanted to stress that load balancers are
608:42 - part of instance networking that help
608:45 - route and manage traffic coming in and
608:48 - going out of the network
608:50 - and so this is a high level overview of
608:52 - the different configuration types that
608:55 - go into putting together an instance and
608:57 - i will be diving deeper into each
609:00 - in this section as well i will be
609:02 - putting a hands-on approach to this by
609:05 - creating an instance in the next lesson
609:07 - and focusing on the different available
609:10 - features that you can use for your
609:12 - specific use case and so this is all i
609:15 - wanted to cover for this lesson so you
609:17 - can now mark this lesson as complete and
609:20 - let's move on to the next one
609:21 - [Music]
609:25 - welcome back now i know in previous
609:28 - demonstrations we've built quite a few
609:30 - compute engine instances and have
609:33 - configured them accordingly in this
609:34 - demonstration we're going to go through
609:37 - a build of another instance but i wanted
609:39 - to dig deeper into the specific
609:42 - configurations that are available for
609:44 - compute engine so with that being said
609:47 - let's dive in and so i am now logged in
609:50 - under tony bowties gmail.com as well i
609:54 - am logged in under the bowtie inc
609:56 - project so in order to kick off this
609:58 - demo i'm going to head on over to the
610:00 - compute engine console so i'm going to
610:02 - go over to the navigation menu and i'm
610:04 - going to scroll down to compute engine
610:07 - and so here i'm prompted
610:09 - to either create or import a vm instance
610:12 - as well as taking the quick start and so
610:14 - i'm not going to import or take the
610:16 - quick start so i'm going to simply click
610:18 - on create
610:20 - and so i want to take a moment here to
610:22 - focus on the left hand menu where there
610:24 - are a bunch of different options to
610:26 - create any given instance so the first
610:29 - and default option allows me to create
610:32 - the instance from scratch choosing the
610:34 - new vm instance from template option
610:37 - allows me to create a new instance from
610:40 - an instance template and because i don't
610:42 - have any instance templates i am
610:44 - prompted here with the option to create
610:46 - one and so for those of you who are
610:48 - unfamiliar with instance templates
610:51 - templates are used in managed instance
610:53 - groups and define instance properties
610:56 - for when instances are launched within
610:58 - that managed instance group but don't
611:00 - worry i will be covering instance groups
611:03 - and instant templates in a later lesson
611:06 - the next option that's available is new
611:08 - vm instance from machine image and an
611:11 - image is a clone or a copy of an
611:13 - instance and again i will be covering
611:16 - this in a separate lesson and going
611:18 - through all the details of machine
611:20 - images but if i did have any machine
611:23 - images i would be able to create my
611:25 - instance from here but since i do not i
611:28 - am prompted with the option to create a
611:30 - new machine image now the last option
611:32 - that i wanted to show you is the
611:34 - marketplace
611:36 - and so the marketplace has existing
611:38 - machine images that are all
611:40 - pre-configured with its proper operating
611:43 - system as well as the software to
611:45 - accompany it so for instance if i'm
611:48 - looking to create a vm with a wordpress
611:50 - installation on it i can simply go up to
611:53 - the top to the search bar type in
611:55 - wordpress and i will be presented with
611:57 - many different options and i'm just
611:59 - going to choose the one here at the top
612:01 - and i am presented with 49 results of
612:05 - virtual machines with different types of
612:08 - wordpress installations on them and
612:10 - these are all different instances that
612:12 - have been configured specifically for
612:14 - wordpress by different companies like
612:17 - lightspeed analog innovation and
612:20 - cognosis inc and so for this
612:22 - demonstration i'm going to choose
612:24 - wordpress on centos 7
612:28 - and here i'm giving an overview about
612:30 - the software itself i'm also given
612:32 - information about the company that
612:34 - configured this as well at the top i'm
612:36 - given a monthly estimated cost for this
612:39 - specific instance and if i scroll down
612:41 - the page i can get a little bit more
612:44 - information with regards to this image
612:46 - and as shown here on the right i can see
612:48 - my pricing the usage fee will cost me
612:51 - 109 a month along with the vm instance
612:54 - type that the software is configured for
612:57 - the amount of disk space and the
612:59 - sustained use discount i've also been
613:01 - given some links here for tutorials and
613:03 - documentation
613:05 - and i've also been given instructions
613:07 - for maintenance and support i've been
613:09 - given both an email and a link to live
613:12 - support and of course at the bottom we
613:14 - have the terms of service and this is a
613:16 - typical software package amongst many
613:19 - others that's available in the google
613:21 - cloud marketplace now i can go ahead and
613:23 - launch this if i choose but i'm going to
613:25 - choose not to launch this and i'm going
613:27 - to back out and so just to give you some
613:29 - context with regards to enterprise
613:31 - software
613:32 - software packages like f5 and jenkins
613:36 - are also available in the google cloud
613:38 - marketplace and again when i click on
613:40 - the first option it'll give me a bunch
613:42 - of available options on jenkins and its
613:45 - availability from different companies on
613:48 - different platforms now just as a note
613:51 - to update your existing deployment of a
613:54 - software package
613:55 - you have to redeploy the software
613:58 - package from marketplace in order to
614:01 - update it but other than that caveat the
614:04 - easiest way to deploy a software package
614:07 - is definitely through the marketplace
614:09 - and so now that we've gone through all
614:11 - the different options on how to create
614:14 - an instance i'm gonna go back and select
614:16 - new vm instance so i can create a new vm
614:20 - from scratch and so i am prompted here
614:22 - at the top with a note telling me that
614:25 - there was a draft that was saved from
614:27 - when i started to create in my new
614:29 - instance but i navigated away from it
614:32 - and i have the option to restore the
614:34 - configuration i was working on and so
614:37 - just know that when you are in the midst
614:39 - of creating an instance
614:40 - google cloud will automatically save a
614:43 - draft of your build so that you are able
614:46 - to continue working on it later now i
614:48 - don't really need this draft but i will
614:51 - just hit restore
614:53 - and for the name i'm going to keep it as
614:55 - instance 1 and for the sake of this demo
614:57 - i'm going to add a label
614:59 - the key is going to be environment and
615:01 - the value will be testing i'm going to
615:03 - go down to the bottom click save now
615:05 - when it comes to the geographic location
615:08 - of the instance using regions i can
615:10 - simply click on the drop down and i will
615:13 - have access to deploy this instance in
615:16 - any currently available region as
615:19 - regions are added they will be added
615:21 - here as well and so i'm going to keep it
615:23 - as us east one
615:26 - and under zone i have the availability
615:29 - of putting it in any zone within that
615:31 - region and so i'm going to keep it as us
615:33 - east 1b and just as another note once
615:36 - you've deployed the instance in a
615:38 - specific region you will not be able to
615:41 - move that instance to a different region
615:43 - you will have to recreate it using a
615:45 - snapshot in another region and i will be
615:47 - going over this in a later lesson now
615:50 - scrolling down to machine configuration
615:52 - there are three different types of
615:54 - families that you can choose from when
615:56 - it comes to machine types the general
615:58 - purpose the compute optimized and the
616:01 - memory optimized the general purpose
616:03 - machine family has a great available
616:06 - selection of different series types that
616:08 - you can choose from and is usually the
616:10 - go to machine family if you're unsure
616:13 - about which machine type to select so
616:16 - for this demo i'm going to keep my
616:17 - selection for series type as e2 and
616:20 - under machine type i'm given a very
616:23 - large selection of different sizes when
616:25 - it comes to vcpu and memory and so i can
616:29 - select from a shared core a standard
616:31 - type a high memory type or a high cpu
616:35 - type and i will be going over this in
616:38 - greater detail in another lesson on
616:40 - machine types now in case the predefined
616:43 - machine types do not fit my needs
616:46 - or the scope for the amount of vcpus and
616:49 - memory that i need
616:50 - fall in between those predefined machine
616:53 - types i can simply select the custom
616:56 - option and this will bring up a set of
616:58 - sliders
616:59 - where i am able to select both the
617:01 - amount of vcpus and amount of memory
617:04 - that i need for the instance that i am
617:07 - creating now as i change the course
617:10 - slider to either more vcpus or less my
617:14 - core to memory ratio for this series
617:16 - will stay the same and therefore my
617:18 - memory will be adjusted automatically i
617:21 - also have the option to change the
617:23 - memory as i see fit to either add more
617:26 - memory or to remove it and so this is
617:29 - great for when you're in between sizes
617:32 - and you're looking for something
617:33 - specific that fits your workload and so
617:36 - i'm going to change back the machine
617:37 - type to an e2 micro
617:40 - and as you can see in the top right
617:43 - i will find a monthly estimate of how
617:46 - much the instance will cost me
617:48 - and i can click on this drop down and it
617:51 - will give me a breakdown of the cost for
617:54 - vcpu in memory the cost for my disks as
617:58 - well as my sustained use discount and if
618:01 - i had any other resources that i was
618:03 - consuming like a static ip or an extra
618:06 - attached disk those costs would show up
618:09 - here as well and so if i went to a
618:11 - compute optimized you can see how the
618:14 - price has changed but i'm given the
618:16 - breakdown so that i know exactly what
618:18 - i'm paying for so i'm going to switch it
618:20 - back to general purpose
618:23 - and i wanted to point out here the cpu
618:26 - platform and gpu as you can add gpus to
618:29 - your specific machine configuration and
618:32 - so just as another note
618:34 - gpus can only be added to an n1 machine
618:38 - type as any other type will show the gpu
618:41 - selection as grayed out and so here i
618:44 - can add the gpu type as well as adding
618:47 - the number of gpus that i need but for
618:49 - the sake of this demonstration i'm not
618:51 - going to add any gpus
618:53 - and i'm going to select the e2 series
618:56 - and change it back to e2 micro scrolling
618:59 - down a little bit here
619:01 - when it comes to cpu platform depending
619:04 - on the machine type you can choose
619:06 - between intel or amd if you are looking
619:09 - for a specific cpu but just know that
619:12 - your configuration is permanent now
619:14 - moving down a little bit more you will
619:17 - see here display device now display
619:19 - device is a feature on compute engine
619:22 - that allows you to add a virtual display
619:25 - to a vm for system management tools
619:28 - remote desktop software and any
619:31 - application that requires you to connect
619:34 - to a display device on a remote server
619:37 - this is an especially great feature to
619:39 - have for when your server is stuck at
619:41 - boot patching or hardware failure and
619:44 - you can't log in and the drivers are
619:46 - already included for both windows and
619:49 - linux vms this feature works with the
619:52 - default vga driver right out of the box
619:55 - and so i'm going to keep this checked
619:57 - off as i don't need it and i'm going to
619:59 - move down to confidential vm service now
620:02 - confidential computing is a security
620:04 - feature to encrypt sensitive code and
620:07 - data that's in memory so even when it's
620:11 - being processed it is still encrypted
620:14 - and is a great use case when you're
620:16 - dealing with very sensitive information
620:18 - that requires strict requirements now
620:21 - compute engine also gives you the option
620:24 - of deploying containers on it and this
620:26 - is a great way to test your containers
620:29 - instead of deploying a whole kubernetes
620:31 - cluster and may even suffice for
620:34 - specific use cases but just note that
620:36 - you can only deploy one container per vm
620:40 - instance and so now that we've covered
620:42 - most of the general configuration
620:44 - options for compute engine i wanted to
620:46 - take a minute to dive into the options
620:48 - that are available for boot disk so i'm
620:51 - going to go ahead and click on change
620:53 - and here i have the option of choosing
620:55 - from a bunch of different public images
620:58 - with different operating systems that i
621:00 - can use for my boot disk so if i wanted
621:03 - to load up ubuntu i can simply select
621:05 - ubuntu and i can choose from each
621:08 - different version that's available
621:10 - as well i'm shown here the boot disk
621:13 - type which is currently selected as the
621:15 - standard persistent disk but i also have
621:18 - the option of selecting either a
621:21 - balanced persistent disk or ssd
621:23 - persistent disk and i'm going to keep it
621:25 - as standard persistent disk and if i
621:28 - wanted to i can increase the boot disk
621:30 - size so if i wanted 100 gigs i can
621:32 - simply add it and if i select it and i
621:35 - go back up to the top right hand corner
621:37 - i can see that my price for the instance
621:40 - has changed now i'm not charged for the
621:42 - operating system due to it being an open
621:45 - source image but i am charged more for
621:48 - the standard persistent disk because i'm
621:50 - no longer using 10 gigs but i'm using
621:52 - 100 gigabytes
621:54 - now let's say i wanted to go back and i
621:57 - wanted to change this image to a windows
622:00 - image i'm going to go down here to
622:01 - windows server and i want to select
622:03 - windows server 2016 i'm going to load up
622:06 - the data center version and i'm going to
622:08 - keep the standard persistent disk along
622:10 - with 100 gigabytes i'm going to select
622:13 - it if i scroll back up i can see that
622:15 - i'm charged a licensing fee for windows
622:18 - server and these images with these
622:20 - licensing fees are known as premium
622:23 - images so please make sure that you are
622:25 - aware of these licensing fees when
622:28 - launching your instances and because i
622:29 - want to save on money just for now i'm
622:32 - going to scroll back down to my boot
622:33 - disk and change it back to ubuntu
622:37 - and i'm going to change the size back
622:39 - down to 10 gigabytes as well before you
622:42 - move on i wanted to touch on custom
622:44 - images and so if i did have any custom
622:47 - images i could see them here and i would
622:49 - be able to create instances from my
622:52 - custom images using this method i also
622:55 - have the option of creating an instance
622:57 - from a snapshot and because i don't have
622:59 - any nothing shows up and lastly i have
623:02 - the option of using existing disks so
623:05 - let's say for instance i had a vm
623:07 - instance and i had deleted it but i
623:10 - decided to keep the attached boot disk
623:12 - it would show up as unattached and i am
623:15 - able to attach that to a new instance
623:18 - and so now that i've shown you all the
623:20 - available options when it comes to boot
623:22 - disk i'm going to go ahead and select
623:24 - the ubuntu operating system and move on
623:27 - to the next option here we have identity
623:29 - and api access which we've gone through
623:32 - in great depth in a previous demo as
623:34 - well i'm given an option to create a
623:37 - firewall rule automatically for http and
623:41 - https traffic and as for networking as
623:44 - we covered it in great depth in the last
623:46 - section
623:47 - i will skip that part of the
623:49 - configuration and simply launch it in
623:51 - the default vpc and so just as a quick
623:54 - note i wanted to remind you that down at
623:57 - the bottom of the page you can find the
623:59 - command line shortcut and when you click
624:01 - on it it will give you the gcloud
624:03 - command to run that you can use in order
624:06 - to create your instance and so i want to
624:08 - deploy this as is so i'm going to click
624:10 - here on close and i'm going to click on
624:13 - create
624:14 - and so i'm just going to give it a
624:16 - minute now so the instance can be
624:17 - created and it took a few seconds but
624:20 - the instance is created and this is
624:22 - regarded as the inventory page to view
624:25 - your instance inventory and to look up
624:27 - any correlating information on any of
624:30 - your instances and so this probably
624:32 - looks familiar to you from the previous
624:35 - instances that you've launched so here
624:37 - we have the name of the instance the
624:39 - zone
624:40 - the internal ip along with the external
624:43 - ip and a selection to connect to the
624:46 - instance as well i'm also given the
624:48 - option to connect to this instance in
624:50 - different ways you also have the option
624:53 - of adding more column information to
624:55 - your inventory dashboard with regards to
624:58 - your instance
625:00 - and you can do this by simply clicking
625:01 - on the columns button right here above
625:04 - the list of instances and you can select
625:06 - from creation time
625:08 - machine type preserve state and even the
625:12 - network and this may bring you more
625:14 - insight on the information available for
625:16 - that instance or even grouping of
625:19 - instances with common configurations
625:22 - this will also help you identify your
625:24 - instances visually in the console and so
625:27 - i'm just going to put the columns back
625:29 - to
625:30 - exactly what it was
625:34 - and so now i want to take a moment to
625:36 - dive right into the instance and have a
625:39 - look at the instance details so as you
625:42 - remember we selected the machine type of
625:45 - e2 micro which has two vcpus and one
625:48 - gigabyte of memory here we have the
625:50 - instance id as well scrolling down we
625:54 - have the cpu platform we have the
625:57 - display device that i was mentioning
625:59 - earlier along with the zone the labels
626:02 - the creation time as well as the network
626:05 - interface and scrolling down i can see
626:07 - here the boot disk with the ubuntu image
626:10 - as well as the name of the boot disk so
626:12 - there are quite a few configurations
626:14 - here and if i click on edit i can edit
626:17 - some of these configurations on the fly
626:20 - and with some configurations i need to
626:22 - stop the instance before editing them
626:24 - and there are some configurations like
626:26 - the network interface where i would have
626:29 - to delete the instance in order to
626:32 - recreate it so for instance if i wanted
626:34 - to change the machine type i need to
626:37 - stop the instance in order to change it
626:39 - and the same thing goes for my display
626:41 - device as well the network interface in
626:45 - order for me to change it from its
626:46 - current network or subnetwork i'm going
626:49 - to have to stop the instance in order to
626:52 - change it as well and so i hope this
626:54 - general walkthrough of configuring an
626:56 - instance has given you a sense of what
626:58 - can be configured on launch
627:01 - and allowed you to gain some insight on
627:03 - editing features of an instance after
627:06 - launch a lot of what you've seen here in
627:08 - this demo will come up in the exam and
627:11 - so i would recommend that before going
627:13 - into the exam to spend some time
627:16 - launching instances knowing exactly how
627:18 - they will behave and what can be edited
627:21 - after creation that can be done on the
627:23 - fly edits that need the instance to be
627:26 - shut down and edits that need the
627:28 - instance to be recreated and so that's
627:30 - pretty much all i wanted to cover when
627:32 - it comes to creating an instance so you
627:35 - can now mark this as complete and let's
627:37 - move on to the next one
627:43 - welcome back now in this lesson i'm
627:45 - going to be discussing compute engine
627:47 - machine types now a machine type is a
627:50 - set of virtualized hardware resources
627:53 - that's available to a vm instance
627:56 - including the system memory size
627:58 - virtual cpu count
628:00 - and persistent disks in compute engine
628:03 - machine types are grouped and curated by
628:06 - families for different workloads you
628:09 - must always choose a machine type when
628:11 - you create an instance and you can
628:13 - select from a number of pre-defined
628:16 - machine types in each machine type
628:18 - family if the pre-defined machine types
628:21 - don't meet your needs then you can
628:23 - create your own custom machine types in
628:26 - this lesson i will be going through all
628:28 - the different machine types their
628:30 - families and their use cases so with
628:33 - that being said let's dive in
628:36 - now each machine type family displayed
628:39 - here includes different machine types
628:42 - each family is curated for specific
628:45 - workload types the following primary
628:48 - machine types are offered on compute
628:50 - engine which is general purpose compute
628:53 - optimized and memory optimized and so i
628:56 - wanted to go through each one of these
628:58 - families in a little bit of detail now
629:01 - before diving right into it
629:03 - defining what type of machine type you
629:06 - are running can be overwhelming for some
629:09 - but can be broken down to be understood
629:11 - a bit better they are broken down into
629:14 - three parts and separated by hyphens the
629:17 - first part in this example shown here
629:20 - is the series so for this example the
629:23 - series is e2 and the number after the
629:26 - letter is the generation type in this
629:28 - case it would be the second generation
629:31 - now the series come in many different
629:33 - varieties and each are designed for
629:36 - specific workloads now moving on to the
629:39 - middle part of the machine type this is
629:41 - the actual type and types as well can
629:44 - come in a slew of different flavors and
629:46 - is usually coupled with a specific
629:48 - series so in this example the type here
629:51 - is standard and so moving on to the
629:53 - third part of the machine type this is
629:55 - the amount of vcp use
629:57 - in the machine type and so with vcpus
630:00 - they can be offered anywhere from one
630:03 - vcpu up to 416 vcpus and so for the
630:08 - example shown here this machine type has
630:11 - 32 vcpus and so there is one more aspect
630:15 - of a machine type
630:16 - which is the gpus
630:18 - but please note that gpus are only
630:21 - available for the n1 series and so
630:24 - combining the series the type and the
630:27 - vcpu
630:28 - you will get your machine type and so
630:31 - now that we've broken down the machine
630:32 - types in order to properly define them
630:35 - i wanted to get into the predefined
630:38 - machine type families
630:40 - specifically starting off with the
630:42 - general purpose predefined machine type
630:44 - and all the general purpose machine
630:46 - types are available in the standard type
630:50 - the high memory type and the high cpu
630:53 - type so the standard type
630:55 - is the balance of cpu and memory and
630:58 - this is the most common general purpose
631:00 - machine type general purpose also comes
631:03 - in high memory and this is a high memory
631:06 - to cpu ratio so very high memory a lower
631:10 - cpu
631:11 - and lastly we have the high cpu machine
631:14 - type and this is a high cpu to memory
631:18 - ratio so this would be the opposite of
631:20 - the high memory so very high cpu to
631:23 - lower memory so now digging into the
631:26 - general purpose machine family i wanted
631:28 - to start off with the e2 series and this
631:32 - is designed for day-to-day computing at
631:35 - a low cost so if you're looking to do
631:37 - things like web serving
631:39 - application serving
631:41 - back office applications
631:43 - small to medium databases microservices
631:46 - virtual desktops or even development
631:49 - environments the e2 series would serve
631:52 - the purpose perfectly
631:54 - now the e2 machine types are cost
631:57 - optimized machine types that offer
631:59 - sizing between 2 to 32 vcpus and half a
632:04 - gigabyte to 128 gigabytes of memory so
632:08 - small to medium workloads that don't
632:10 - require as many vcpus and applications
632:13 - that don't require local ssds or gpus
632:17 - are an ideal fit for e2 machines e2
632:21 - machine types do not offer sustained use
632:23 - discounts however they do provide
632:26 - consistently
632:28 - low on-demand and committed use pricing
632:31 - in other words they offer the lowest
632:33 - on-demand pricing across the general
632:36 - purpose machine types as well the e2
632:39 - series machines are available in both
632:43 - pre-defined and custom machine types
632:46 - moving on i wanted to touch on all the
632:48 - machine types available in the n-series
632:51 - and these are a balanced machine type
632:53 - with price and performance across a wide
632:56 - range of vm flavors and these machines
632:59 - are designed for web servers application
633:02 - servers back office applications medium
633:05 - to large databases as well as caching
633:08 - and media streaming and they are offered
633:11 - in the standard high memory and high cpu
633:15 - types
633:16 - now the n1 machine types are compute
633:19 - engines first generation general purpose
633:22 - machine types now this machine type
633:25 - offers up to 96 vcpus and 624 gigabytes
633:30 - of memory and again as i mentioned
633:32 - earlier this is the only machine type
633:35 - that offers both gpu support and tpu
633:39 - support the n1 type is available as both
633:42 - pre-defined machine types and custom
633:45 - machine types and the n1 series offers a
633:48 - larger sustained use discount than n2
633:52 - machine types speaking of which
633:54 - the n2 machine types are the second
633:57 - generation general purpose machine types
634:00 - and these offer flexible sizing between
634:03 - two 280 vcpus and half a gigabyte of
634:07 - memory to 640 gigabytes of memory and
634:11 - these machine types also offer an
634:13 - overall performance improvement over the
634:16 - n1 machine types workloads that can take
634:18 - advantage of the higher clock frequency
634:21 - of the cpu
634:23 - are a good choice for n2 machine types
634:25 - and these workloads can get higher per
634:28 - thread performance while benefiting from
634:30 - all the flexibility that a general
634:33 - purpose machine type offers and two
634:35 - machine types also offer the extended
634:39 - memory feature and this helps control
634:41 - per cpu software licensing costs now
634:44 - getting into the last n series machine
634:46 - type the n2d machine type is the largest
634:50 - general purpose machine type with up to
634:53 - 224 vcpus and
634:56 - 896 gigabytes of memory this machine
634:59 - type is available in predefined and
635:02 - custom machine types and this machine
635:04 - type as well has the extended memory
635:07 - feature which i discussed earlier that
635:09 - helps you avoid per cpu software
635:12 - licensing the n2d machine type supports
635:15 - the committed use and sustain use
635:17 - discounts now moving on from the general
635:20 - purpose machine type family i wanted to
635:23 - move into the compute optimize machine
635:25 - family now this series
635:27 - offers ultra high performance for
635:29 - compute intensive workloads such as high
635:32 - performance computing
635:34 - electronic design automation
635:36 - gaming and single threaded applications
635:39 - so anything that is designed for compute
635:42 - intensive workloads this will definitely
635:44 - be your best choice
635:46 - now compute engine optimized machine
635:48 - types are ideal for as i said earlier
635:51 - compute intensive workloads and these
635:54 - machine types offer the highest
635:56 - performance per core
635:58 - on compute engine compute optimized
636:00 - types are only available as predefined
636:03 - machine types and so they are not
636:06 - available for any custom machine types
636:09 - the c2 machine types offer a maximum of
636:12 - 60 vcpus and a maximum of 240 gigabytes
636:17 - of memory now although the c2 machine
636:19 - type works great for compute intensive
636:21 - workloads it does come with some caveats
636:25 - and so you cannot use regional
636:26 - persistent disks with compute optimized
636:29 - machine types and i will be getting into
636:31 - the details of persistent disks in a
636:34 - later lesson and they are only available
636:36 - in select zones and regions on select
636:40 - cpu platforms and so now moving into the
636:43 - last family is the memory optimize
636:45 - machine family and this is for ultra
636:48 - high memory workloads this family is
636:51 - designed for large in memory databases
636:54 - like sap hana as well as in memory
636:57 - analytics
636:58 - now the m series comes in two separate
637:01 - generations
637:03 - m1 and m2 the m1 offering a maximum of
637:07 - 160 vcpus and a maximum memory of
637:13 - 3844 gigabytes whereas the m2 offering
637:17 - again a maximum of 160 vcpus but
637:21 - offering a whopping 11
637:24 - 776 gigabytes of maximum memory and as i
637:28 - said before these machine types they're
637:30 - ideal for tasks that require intensive
637:34 - use of memory so they are suited for
637:36 - in-memory databases and in memory
637:39 - analytics data warehousing workloads
637:42 - genomics analysis and sql analysis
637:45 - services memory optimized machine types
637:47 - are only available as predefined machine
637:50 - types and the caveats here is that you
637:53 - cannot use regional persistent disks
637:55 - with memory optimized machine types as
637:58 - well they're only available in specific
638:01 - zones now i wanted to take a moment to
638:03 - go back
638:04 - to the general purpose machine type so
638:07 - that i can dig into the shared cord
638:09 - machine type and this is spread amongst
638:11 - the e2 and n1 series and these shared
638:15 - core machine types are used for
638:17 - burstable workloads are very cost
638:19 - effective as well they're great for
638:22 - non-resource intensive applications
638:25 - shared core machine types use context
638:27 - switching to share a physical core
638:30 - between vcpus for the purpose of
638:33 - multitasking different shared core
638:35 - machine types sustain different amounts
638:38 - of time on a physical core which allows
638:42 - google cloud to cut the price in general
638:44 - share core instances can be more cost
638:48 - effective for running small
638:50 - non-resource intensive applications than
638:53 - standard high memory or high cpu machine
638:57 - types now when it comes to cpu bursting
639:00 - these shared core machine types offer
639:03 - bursting capabilities that allow
639:05 - instances to use additional physical cpu
639:09 - for short periods of time bursting
639:11 - happens automatically when your instance
639:14 - requires more physical cpu than
639:16 - originally allocated during these spikes
639:20 - your instance will take advantage of
639:22 - available physical cpu in bursts and the
639:26 - e2 shared core machine type is offered
639:28 - in micro small and medium while the n1
639:32 - series is offered in the f1 micro and
639:35 - the g1 small and both of these series
639:38 - have a maximum of two vcpus with a
639:42 - maximum of four gigabytes of memory now
639:45 - i wanted to take a moment to touch on
639:47 - custom machine types and these are
639:50 - available for any general purpose
639:52 - machine and so this is customer defined
639:54 - cpu and memory designed for custom
639:57 - workloads
639:59 - now if none of the general purpose
640:01 - predefined machine types cater to your
640:04 - needs
640:05 - you can create a custom machine type
640:07 - with a specific number of vcpus and
640:10 - amount of memory that you need for your
640:12 - instance these machine types are ideal
640:15 - for workloads that are not a good fit
640:18 - for the pre-defined machine types that
640:21 - are available they're also great for
640:23 - when you need more memory or more cpu
640:26 - but the predefined machine types don't
640:28 - quite fit exactly what you need for your
640:31 - workload just as a note it costs
640:33 - slightly more to use a custom machine
640:35 - type than a pre-defined machine type and
640:38 - there are limitations in the amount of
640:40 - memory and vcpu you can select and as i
640:43 - stated earlier when creating a custom
640:45 - machine type you can choose from the e2
640:49 - n2
640:50 - and 2d and n1 machine types and so the
640:54 - last part i wanted to touch on are the
640:56 - gpus that are available and these are
640:58 - designed for the graphic intensive
641:01 - workloads and again are only available
641:03 - for the n1 machine type and gpus come in
641:07 - five different flavors from nvidia
641:10 - showing here as the tesla k80 the tesla
641:13 - p4 the tesla t4 the tesla v100 and the
641:18 - tesla p100 and so these are all the
641:21 - families and machine types that are
641:23 - available for you in google cloud and
641:25 - will allow you to be a little bit more
641:27 - flexible with the type of workload that
641:30 - you need them for and so for the exam
641:32 - you won't have to memorize each machine
641:35 - type but you will need to know an
641:37 - overview of what each machine type does
641:40 - now i know there's been a lot of theory
641:42 - presented here in this lesson but i hope
641:44 - this is giving you a better
641:45 - understanding of all the available
641:48 - pre-defined machine types in google
641:50 - cloud and so that's pretty much all i
641:52 - wanted to cover in this lesson on
641:54 - compute engine machine types so you can
641:56 - now mark this lesson as complete and
641:58 - let's move on to the next one
642:00 - [Music]
642:04 - welcome back in this lesson i'm going to
642:07 - be reviewing managing your instances now
642:10 - how you manage your instances is a big
642:12 - topic in the exam
642:14 - as well it's very useful to know for
642:17 - your work as a cloud engineer in the
642:19 - environments you are responsible for
642:22 - knowing both the features that are
642:24 - available as well as the best practices
642:26 - will allow you to make better decisions
642:29 - with regards to your instances and allow
642:32 - you to keep your environment healthy
642:34 - this lesson will dive into the many
642:37 - features that are available in order to
642:39 - better manage your instances using the
642:42 - specific features within google cloud so
642:45 - with that being said let's dive in
642:48 - now i wanted to start off this lesson
642:50 - discussing the life cycle of an instance
642:53 - within google cloud every instance has a
642:56 - predefined life cycle from its starting
642:59 - provisioning state to its deletion an
643:02 - instance can transition through many
643:05 - instant states as part of its life cycle
643:07 - when you first create an instance
643:09 - compute engine provisions resources to
643:12 - start your instance next the instance
643:15 - moves into staging where it prepares the
643:18 - first boot and then it finally boots up
643:20 - and is considered running during its
643:22 - lifetime a running instance can be
643:25 - repeatedly stopped and restarted or
643:28 - suspended and resumed so now i wanted to
643:31 - take a few minutes to go through the
643:33 - instance life cycle in a bit of detail
643:36 - starting with the provisioning state
643:38 - now this is where resources are being
643:41 - allocated for the instance the instance
643:44 - is not yet running and the instance is
643:46 - being allocated its requested amount of
643:49 - cpu and memory along with its root disk
643:52 - any additional disks that are attached
643:54 - to it and as well some additional
643:57 - feature sets that are assigned to this
643:59 - instance and when it comes to the cost
644:01 - while in the provisioning state there
644:03 - are no costs that are being incurred
644:05 - moving right along to the staging state
644:08 - after finishing the provisioning state
644:10 - the life cycle continues with the
644:12 - staging state and this is where
644:14 - resources have been acquired and the
644:16 - instance is being prepared for first
644:18 - boot both internal and external ips are
644:22 - allocated and can be either static or
644:25 - ephemeral in the system image that was
644:27 - originally chosen for this instance
644:30 - is used to boot up the instance and this
644:32 - can be either a public image or a custom
644:35 - image costs in the state are still not
644:38 - incurred as the instance is still in the
644:41 - pre-boot state
644:43 - now once the instance has left staging
644:46 - it will move on to the running state and
644:49 - this is where the instance is booting up
644:51 - or running and should allow you to log
644:54 - into the instance either using ssh or
644:57 - rdp within a short waiting period due to
645:00 - any startup scripts or any boot
645:03 - maintenance tasks for the operating
645:05 - system now during the running state you
645:08 - can reset your instance and this is
645:10 - where you would wipe the memory contents
645:13 - of the vm instance and reset the virtual
645:16 - machine to its initial state resetting
645:18 - an instance
645:20 - causes an immediate hard reset of the vm
645:23 - and therefore the vm does not do a
645:25 - graceful shutdown for the guest
645:27 - operating system however
645:29 - the vm retains all persistent disk data
645:33 - and none of the instance properties
645:35 - change the instance remains in running
645:38 - state through the reset now as well in
645:40 - the running state a repair can happen
645:43 - due to the instance encountering an
645:46 - internal error or the underlying machine
645:48 - is unavailable due to maintenance during
645:51 - this time the instance is unusable and
645:54 - if the repair is successful the instance
645:56 - returns back to the running state paying
645:59 - attention to costs
646:00 - this state is where the instance starts
646:02 - to occur them and is related to the
646:05 - resources assigned to the instance like
646:07 - the cpu and memory any static ips and
646:11 - any disks that are attached to the
646:13 - instance and i will be going into a bit
646:15 - of detail in just a bit with regards to
646:18 - this state
646:20 - and finally we end the life cycle with
646:22 - the stopping suspended and terminated
646:25 - states now when you are suspending an
646:28 - instance it is like closing the lid of
646:30 - your laptop suspending the instance will
646:33 - preserve the guest operating system
646:35 - memory and application state of the
646:37 - instance otherwise it'll be discarded
646:41 - and from this state you can choose
646:43 - either to resume or to delete it when it
646:46 - comes to stopping either a user has made
646:48 - a request to stop the instance or there
646:51 - was a failure and this is a temporary
646:54 - status and the instance will move to
646:56 - terminated touching on costs for just a
646:58 - second when suspending or stopping an
647:01 - instance you pay for resources that are
647:04 - still attached to the vm instance
647:07 - such as static ips and persistent disk
647:10 - data you do not pay the cost of a
647:12 - running vm instance ephemeral external
647:15 - ip addresses are released from the
647:17 - instance and will be assigned a new one
647:20 - when the instance is started now when it
647:22 - comes to stopping suspending or
647:24 - resetting an instance you can stop or
647:27 - suspend an instance if you no longer
647:29 - need it but want to keep the instance
647:31 - around for future use compute engine
647:34 - waits for the guest to finish shutting
647:36 - down and then transitions the instance
647:39 - to the terminated state so touching on
647:41 - the terminated state this is where a
647:44 - user either shuts down the instance or
647:47 - the instance encounters a failure you
647:49 - can choose to restart the instance or
647:51 - delete it as well as holding some reset
647:54 - options within the availability policy
647:57 - in this state you still pay for static
648:00 - ips and disks
648:01 - but like the suspending or stopping
648:03 - state you do not pay for the cpu and
648:06 - memory resources allocated to the
648:09 - instance
648:10 - and so this covers a high level overview
648:13 - of the instance lifecycle in google
648:15 - cloud and all of the states that make up
648:18 - this lifecycle now to get into some
648:20 - detail with regards to some feature sets
648:23 - for compute engine i wanted to revisit
648:25 - the states where those features apply
648:28 - now when creating your instance you have
648:30 - the option of using shielded vms for
648:34 - added security and when using them the
648:36 - instance would instantiate them as the
648:39 - instance boots and enters into the
648:41 - running state
648:43 - so what exactly is a shielded vm
648:46 - well shielded vms offer verifiable
648:49 - integrity of your compute engine vm
648:52 - instances so you can be sure that your
648:54 - instances haven't been compromised by
648:57 - boot or kernel level malware or rootkits
649:00 - and this is achieved through a four-step
649:02 - process
649:03 - which is covered by secure boot virtual
649:06 - trusted platform module also known as
649:09 - vtpm measure boot which is running on
649:12 - vtpm and integrity monitoring so i
649:15 - wanted to dig into this for just a sec
649:17 - to give you a bit more context
649:20 - now the boot process for shielded vms
649:24 - start with secure boot and this helps
649:26 - ensure that the system only runs
649:29 - authentic software by verifying the
649:32 - digital signature for all boot
649:34 - components and stopping the boot process
649:37 - if signature verification fails so
649:40 - shielded vm instances run firmware
649:42 - that's signed and verified using
649:45 - google's certificate authority and on
649:47 - each and every boot any boot component
649:50 - that isn't properly signed or isn't
649:52 - signed at all is not allowed to run and
649:55 - so the first time you boot a vm instance
649:58 - measure boot creates the integrity
650:00 - policy baseline from the first set of
650:03 - these measurements and then securely
650:05 - stores this data each time the vm
650:07 - instance boots after that these
650:09 - measurements are taken again and stored
650:11 - in secure memory until the next reboot
650:14 - having these two sets of measurements
650:16 - enables integrity monitoring which is
650:19 - the next step and allows it to determine
650:22 - if there have been changes to a vm
650:24 - instance's boot sequence and this policy
650:27 - is loaded onto a virtualized trusted
650:30 - platform module again known as the vtpm
650:33 - for short which is a specialized
650:35 - computer chip that you can use to
650:37 - protect objects like keys and
650:40 - certificates that you use to
650:43 - authenticate access to your system with
650:45 - shielded vms vtpm enables measured boot
650:49 - by performing the measurements needed to
650:52 - create a known good boot baseline and
650:55 - this is called the integrity policy
650:57 - baseline the integrity policy baseline
651:00 - is used for comparison
651:02 - with measurements from subsequent vm
651:04 - boots to determine if anything has
651:07 - changed integrity monitoring relies on
651:10 - the measurements created by measured
651:12 - boot for both the integrity policy
651:14 - baseline and the most recent boot
651:16 - sequence integrity monitoring compares
651:19 - the most recent boot measurements
651:22 - to the integrity policy baseline and
651:24 - returns a pair of pass or failed results
651:28 - depending on whether they match or not
651:30 - one for the early boot sequence and one
651:33 - for the late boot sequence and so in
651:35 - summary this is how shielded vms help
651:39 - prevent data exfiltration so touching
651:42 - now on the running state when you start
651:45 - a vm instance using google provided
651:48 - public images a guest environment is
651:51 - automatically installed on the vm
651:54 - instance a guest environment is a set of
651:56 - scripts daemons and binaries that read
652:00 - the content of the metadata server to
652:02 - make a virtual machine run properly on
652:06 - compute engine a metadata server is a
652:09 - communication channel for transferring
652:11 - information from a client to the guest
652:14 - operating system vm instances created
652:17 - using google provided public images
652:19 - include a guest environment that is
652:21 - installed by default creating vm
652:24 - instances
652:25 - using a custom image will require you to
652:28 - manually install the guest environment
652:30 - this guest environment is available for
652:32 - both linux and windows systems and each
652:36 - supported operating system that is
652:38 - available on compute engine requires
652:40 - specific guest environment packages
652:42 - either google or the owner of the
652:44 - operating system builds these packages
652:47 - now when it comes to the linux guest
652:49 - environment it is either built by google
652:52 - or the owner of the operating system
652:54 - and there are some key components that
652:56 - are applicable to all builds which can
652:59 - be found in the link that i have
653:00 - included in the lesson text the base
653:02 - components of a linux guest environment
653:05 - is a python package that contains
653:08 - scripts daemons and packages for the
653:11 - supported linux distributions when it
653:13 - comes to windows a similar approach
653:16 - applies where a package is available
653:18 - with main scripts and binaries as a part
653:21 - of this guest environment
653:23 - now touching back on the metadata server
653:26 - compute engine provides a method for
653:28 - storing and retrieving metadata in the
653:31 - form of the metadata server this service
653:35 - provides a central point to set metadata
653:38 - in the form of key value pairs which is
653:41 - then provided to virtual machines at
653:43 - runtime and you can query this metadata
653:45 - server programmatically from within the
653:48 - instance and from the compute engine api
653:51 - this is great for use with startup and
653:53 - shutdown scripts or gaining more insight
653:56 - with your instance metadata can be
653:58 - assigned to projects as well as
654:00 - instances and project metadata
654:02 - propagates to all instances within the
654:05 - project while instance metadata only
654:08 - impacts that instance and you can access
654:10 - the metadata using the following url
654:14 - with the curl command you see here on
654:16 - the screen so if you're looking for the
654:18 - metadata for a project you would use the
654:21 - first url that ends in project and for
654:24 - any instance metadata you can use the
654:26 - second url that ends in instance now
654:29 - please note that when you make a request
654:31 - to get information from the metadata
654:33 - server your request and the subsequent
654:36 - metadata response never leaves the
654:39 - physical host running the virtual
654:41 - machine instance now once the instance
654:44 - has booted and has gone through the
654:46 - startup scripts you will then have the
654:48 - ability to login to your instance using
654:51 - ssh or rdp now there are some different
654:54 - methods that you can use to connect and
654:57 - access both your linux instances and
655:00 - your windows instances that i will be
655:02 - going over
655:03 - now when it comes to linux instances
655:06 - we've already gone through accessing
655:08 - these types of instances in previous
655:11 - lessons and demos but just as a
655:13 - refresher you would typically connect to
655:16 - your vm instance via ssh access on port
655:19 - 22. please note that you will require a
655:23 - firewall rule as we have done in
655:25 - previous demos to allow this access and
655:28 - you can connect to your linux instances
655:31 - through the google cloud console or the
655:33 - cloud shell using the cloud sdk now i
655:36 - know that the use of ssh keys are the
655:39 - defacto when it comes to logging into
655:42 - linux instances now in most scenarios on
655:45 - google cloud google recommends using os
655:48 - login over using ssh keys the os login
655:52 - feature lets you use compute engine iam
655:55 - roles to manage ssh access to linux
655:58 - instances and then if you'd like you can
656:01 - add an extra layer of security by
656:03 - setting up os login with two-step
656:06 - verification and manage access at the
656:09 - organization level by setting up
656:11 - organizational policies os login
656:14 - simplifies ssh access management by
656:17 - linking your linux user account to your
656:19 - google identity administrators can
656:22 - easily manage access to instances
656:25 - at either an instance or project level
656:28 - by setting iam permissions now if you're
656:31 - running your own directory service for
656:33 - managing access or are unable to set up
656:36 - os login you can manually manage ssh
656:40 - keys and local user accounts in metadata
656:43 - by manually creating ssh keys and
656:46 - editing the public ssh key metadata now
656:49 - when it comes to windows instances you
656:52 - would typically connect to your vm
656:54 - instance via rdp access on port 3389 and
656:59 - please note that you will also require a
657:02 - firewall rule as shown here to allow
657:05 - this access you can connect to your
657:07 - windows instances through the rdp
657:09 - protocol or through a powershell
657:12 - terminal now when logging into windows
657:15 - this requires setting a windows password
657:18 - and can be done either through the
657:19 - console or the gcloud command line tool
657:22 - and then after setting your password you
657:25 - can then log in from the recommended rdp
657:28 - chrome extension or using a third-party
657:30 - rdp client and i will provide a link to
657:33 - this rdp chrome extension in the lesson
657:36 - text now once the instance has booted up
657:39 - and your instance is ready to be logged
657:41 - into you always have the option of
657:44 - modifying your instance and you can do
657:46 - it manually by either modifying it on
657:48 - the fly or you can take the necessary
657:50 - steps to edit your instance like i
657:53 - showed you in a previous lesson by
657:55 - stopping it editing it and then
657:57 - restarting it although when it comes to
658:00 - google having to do maintenance on a vm
658:02 - or you merely want to move your instance
658:05 - to a different zone in the same region
658:07 - this has all become possible without
658:10 - shutting down your instance
658:12 - using a feature called live migration
658:15 - now when it comes to live migration
658:17 - compute engine migrates your running
658:20 - instances to another host
658:22 - in the same zone instead of requiring
658:25 - your vms to be rebooted this allows
658:28 - google to perform maintenance reliably
658:31 - without interrupting any of your vms
658:34 - when a vm is scheduled to be live
658:36 - migrated google provides a notification
658:38 - to the guest that a migration is coming
658:41 - soon live migration keeps your instances
658:44 - running during compute engine hosts that
658:47 - are in need of regular infrastructure
658:49 - maintenance and upgrades replacement of
658:51 - failed hardware and system configuration
658:54 - changes when google migrates a running
658:56 - vm instance from one host to another
658:59 - it moves the complete instance state
659:02 - from the source to the destination in a
659:05 - way that is transparent to the guest os
659:08 - and anyone communicating with it google
659:11 - also gives you the option of doing live
659:13 - migration manually from one zone to
659:16 - another within the same region either
659:18 - using the console or running the command
659:21 - line you see here gcloud compute
659:23 - instances move the name of the vm with
659:26 - the zone flag and the zone that it's
659:28 - currently in and then the destination
659:30 - zone flag with the zone that you wanted
659:33 - to go to and just as a note with some
659:35 - caveats instances with gpus attached
659:38 - cannot be live migrated and you can't
659:41 - configure a preemptable instance to live
659:44 - migrate and so instance lifecycle is
659:47 - full of different options and
659:49 - understanding them can help better
659:51 - coordinate moving editing and repairing
659:54 - vm instances no matter where they may
659:57 - lie in this life cycle now i hope this
660:00 - lesson has given you the necessary
660:02 - theory that will help better use the
660:04 - discuss feature sets and giving you some
660:07 - ideas on how to better manage your
660:09 - instances now there is a lot more to
660:12 - know than what i've shown you here to
660:14 - manage your instances but topics shown
660:16 - here are what shows up in the exam as
660:19 - well are some really great starting
660:21 - points to begin managing your instances
660:24 - and so that's pretty much all i wanted
660:26 - to cover when it comes to managing
660:28 - instances so you can now mark this
660:30 - lesson as complete and join me in the
660:32 - next one where i will cement the theory
660:34 - in this lesson with the hands-on demo
660:37 - [Music]
660:41 - welcome back in this demonstration i'm
660:44 - going to be cementing some of the theory
660:46 - that we learned in the last lesson with
660:49 - regards to the different login methods
660:52 - for windows and linux instances how to
660:55 - implement these methods are extremely
660:57 - useful to know both for the exam and for
661:00 - managing multiple instances in different
661:03 - environments now there's a lot to cover
661:05 - here so with that being said let's dive
661:08 - in so as you can see i am logged in here
661:11 - under tony bowtie ace
661:13 - gmail.com as well i am in the project of
661:16 - bowtie inc and so the first thing that i
661:19 - want to do is create both a linux
661:22 - instance and a windows instance and this
661:25 - is to demonstrate the different options
661:27 - you have for logging into an instance
661:30 - and so in order for me to do that i need
661:32 - to head on over to compute engine so i'm
661:35 - going to go over to the navigation menu
661:37 - and i'm going to scroll down to compute
661:38 - engine and so just as a note before
661:41 - creating your instances please make sure
661:44 - that you have a default vpc created
661:47 - before going ahead and creating these
661:49 - instances if you've forgotten how to
661:51 - create a default vpc please go back to
661:54 - the networking services section and
661:56 - watch the vpc lesson for a refresher and
661:59 - so i'm going to go ahead and create my
662:01 - first instance and i'm going to start
662:03 - with the windows instance so i'm going
662:06 - to simply click on create
662:08 - and so for the name of this instance you
662:10 - can simply call this windows dash
662:13 - instance
662:14 - and i'm not going to add any labels and
662:17 - for the region you should select us
662:19 - east1 and you can keep the zone as the
662:22 - default for us east 1b and scrolling
662:25 - down to the machine configuration for
662:27 - the machine type i'm going to keep it as
662:30 - is as it is a windows instance and i'm
662:33 - going to need a little bit more power
662:35 - scrolling down to boot disk we need to
662:37 - change this from debian over to windows
662:40 - so i'm going to simply click on the
662:42 - change button and under operating system
662:44 - i'm going to click on the drop down and
662:46 - select windows server for the version
662:49 - i'm going to select the latest version
662:51 - of windows server which is the windows
662:53 - server 2019 data center and you can keep
662:57 - the boot disk type and the size as its
663:00 - default and simply head on down and
663:02 - click on select and we're going to leave
663:04 - everything else as the default and
663:07 - simply click on create
663:10 - and success our windows instance has
663:12 - been created and so the first thing that
663:14 - you want to do is you want to set a
663:16 - windows password for this instance and
663:19 - so i'm going to head on over to the rdp
663:21 - button and i'm going to click on the
663:23 - drop-down and here i'm going to select
663:25 - set windows password and here i'm going
663:28 - to get a pop-up to set a new windows
663:30 - password the username has been
663:32 - propagated for me as tony bowties i'm
663:35 - going to leave it as is and i'm going to
663:37 - click on set
663:40 - and i'm going to be prompted with a new
663:42 - windows password that has been set for
663:44 - me so i'm going to copy this and i'm
663:46 - going to paste it into my notepad so be
663:49 - sure to record it somewhere either write
663:51 - it down
663:52 - or copy and paste it into a text editor
663:55 - of your choice i'm going to click on
663:56 - close and so now for me to log into this
663:59 - i need to make sure of a couple things
664:02 - the first thing is i need to make sure
664:04 - that i have a firewall rule open for
664:07 - port 3389 the second is i need to make
664:11 - sure that i have an rdp client and so in
664:14 - order to satisfy my first constraint i'm
664:16 - going to head on over to the navigation
664:19 - menu and go down to vpc network
664:22 - here i'm going to select firewall and as
664:25 - expected the rdp firewall rule has been
664:29 - already created due to the fact that
664:31 - upon creation of the default vpc network
664:34 - this default firewall rule is always
664:37 - created and so now that i've gotten that
664:39 - out of the way i'm going to head back on
664:41 - over to compute engine
664:43 - and what i'm going to do is i'm going to
664:44 - record the external ip so that i'll be
664:47 - able to log into it now i'm going to be
664:50 - logging into this instance
664:52 - from both a windows client and a mac
664:55 - client so starting with windows i'm
664:57 - going to head on over to my windows
664:58 - virtual machine and because i know
665:01 - windows has a default rdp client already
665:04 - built in i'm going to simply bring it up
665:06 - by hitting the windows key and typing
665:08 - remote desktop connection
665:11 - i'm going to click on that i'm going to
665:13 - paste in the public ip for the instance
665:15 - that i just recorded and i'm going to
665:17 - click on connect you should get a pop-up
665:19 - asking for your credentials i'm going to
665:21 - type in my username as tony bowtie ace
665:25 - as well i'm going to paste in the
665:27 - password and i'm going to click on ok
665:29 - i'm prompted to accept the security
665:31 - certificate and i'm going to select yes
665:34 - and success
665:36 - i'm now connected to my windows server
665:38 - instance and it's going to run all its
665:41 - necessary startup scripts you may get a
665:43 - couple of prompts that come up
665:45 - asking you if you want to connect to
665:47 - your network
665:48 - absolutely i'm going to close down
665:50 - server manager just for now
665:53 - and another thing that i wanted to note
665:55 - is that when you create a windows
665:56 - instance there will automatically be
665:59 - provisioned a google cloud shell with
666:02 - the sdk pre-installed and so you'll be
666:05 - able to run all your regular commands
666:07 - right from this shell without having to
666:10 - install it and this is due to the guest
666:12 - environment that was automatically
666:14 - installed on the vm instance upon
666:17 - creation and this is a perfect example
666:19 - of some of the scripts that are
666:21 - installed with the guest environment i'm
666:23 - going to go ahead and close out of this
666:25 - and i'm going to go ahead and close out
666:26 - of my instance
666:29 - hit ok and so being here in windows i
666:31 - wanted to show you an alternate way of
666:34 - logging into your instance through
666:36 - powershell so for those of you who are
666:38 - quite versed in windows and use
666:41 - powershell in your day-to-day there is
666:43 - an easy way to log into your instance
666:45 - using powershell now in order for me to
666:48 - do that i need to open another firewall
666:51 - rule covering tcp port 5986 so i'm going
666:55 - to head on over back to the google cloud
666:57 - console i'm going to head over to the
666:59 - navigation menu and i'm going to scroll
667:01 - down to vpc network
667:04 - i'm going to go into firewall and i'm
667:06 - going to create a new firewall rule and
667:08 - under name i'm going to name this as
667:12 - allow
667:13 - powershell i'm going to use the same for
667:15 - the description i'm going to scroll down
667:17 - to targets and i'm going to select all
667:20 - instances in the network and under
667:22 - source ip ranges for this demonstration
667:25 - i'm going to use
667:27 - 0.0.0.0 forward slash 0. and again this
667:30 - should not be used in a production
667:32 - environment but is used merely for this
667:34 - demo i'm going to leave everything else
667:36 - as is and i'm going to go down to
667:38 - protocols and ports i'm going to click
667:40 - on tcp and i'm going to type in 5986 for
667:44 - the port and i'm going to click on
667:46 - create i'm going to give it a second
667:48 - just to create and it took a couple
667:50 - seconds but our firewall rule is now
667:53 - created and so now i'm gonna head over
667:55 - to my windows vm and i'm gonna open up a
667:58 - powershell command prompt and hit the
668:00 - windows key and type in powershell
668:03 - and so in order for me to not get
668:05 - constantly asked about my username and
668:08 - password i'm going to use a variable
668:11 - that will keep my password for me and so
668:13 - every time i connect to my windows
668:15 - instance i won't need to type it in all
668:17 - the time and so the command for that is
668:19 - dollar sign credentials equals get dash
668:22 - credential i'm going to hit enter and
668:24 - i'm going to get a prompt to type in my
668:26 - username and password so i'm going to
668:28 - simply type that in now along with my
668:30 - password and hit ok and if you don't get
668:33 - a prompt with any errors then chances
668:36 - are that you've been successful at
668:38 - entering your credentials and so now in
668:40 - order to connect to the instance you're
668:42 - going to need the public ip address
668:44 - again so i'm going to head on over back
668:46 - to the console i'm going to head on over
668:48 - to the navigation menu and back to
668:50 - compute engine here i'm going to record
668:52 - the external ip and i'm going to head on
668:54 - over back to my windows virtual machine
668:56 - and so you're going to enter this
668:58 - command which i will include in the
669:00 - lesson text and you'll also be able to
669:02 - find it in the github repository beside
669:04 - computer name you're going to put in
669:06 - your public ip address of your windows
669:09 - instance and make sure at the end you
669:11 - have your credentials variable i'm going
669:14 - to simply click enter and success i'm
669:17 - now connected to my windows instance in
669:20 - google cloud so as you can see here on
669:22 - the left
669:23 - is the public ip of my windows instance
669:26 - and so these are the various ways that
669:28 - you can connect to your windows instance
669:31 - from a windows machine and so now for me
669:33 - to connect to my windows instance on a
669:35 - mac i'm going to head on over there now
669:37 - and like i said before i need to satisfy
669:40 - the constraint of having an rdp client
669:43 - unfortunately mac does not come with an
669:45 - rdp client and so the recommended tool
669:48 - to use is the chrome extension but i
669:51 - personally like microsoft's rdp for mac
669:54 - application and so i'm going to go ahead
669:56 - and do a walkthrough of the installation
669:58 - so i'm going to start off by opening up
670:00 - safari and i'm going to paste in this
670:02 - url which i will include in the lesson
670:04 - text
670:06 - and microsoft has made available a
670:08 - microsoft remote desktop app available
670:11 - in the app store i'm going to go ahead
670:13 - and view it in the app store and i'm
670:15 - going to simply click on get and then
670:17 - install and once you've entered your
670:19 - credentials and you've downloaded and
670:21 - installed it you can simply click on
670:23 - open i'm going to click on not now and
670:26 - continue and i'm going to close all
670:28 - these other windows for better viewing
670:30 - i'm going to click on add pc i'm going
670:32 - to paste in the public ip address of my
670:34 - windows instance and under user account
670:37 - i'm going to add my user account type in
670:40 - my username paste in my password you can
670:42 - add a friendly name here i'm going to
670:44 - type in windows dash gc for google cloud
670:48 - and i'm going to click on add and then
670:50 - once you've pasted in all the
670:51 - credentials and your information you can
670:54 - then click on add and i should be able
670:56 - to connect to my windows instance by
670:58 - double clicking on this window it's
671:00 - asking me for my certificates i'm going
671:02 - to hit continue
671:04 - and success i'm connected to my windows
671:07 - instance and so this is how you would
671:09 - connect to a windows instance from a
671:11 - windows machine as well as from a mac as
671:14 - well there are a couple of other options
671:17 - that i wanted to show you over here on
671:19 - the drop down beside rdp i can download
671:22 - an rdp file which will contain the
671:25 - public ip address of the windows
671:27 - instance along with your username if i
671:29 - need to reset my password i can view the
671:32 - gcloud command to do it or i can set a
671:34 - new windows password if i forgotten my
671:37 - old one and so that's everything i had
671:39 - to show you with regards to connecting
671:41 - to a windows instance and so since this
671:44 - demo was getting kind of long i decided
671:46 - to split it up into two parts
671:48 - and so this is the end of part one of
671:50 - this demo and this would be a great
671:52 - opportunity to get up and have a stretch
671:55 - grab yourself a tea or a coffee and
671:57 - whenever you're ready you can join me in
671:59 - part two where we will be starting
672:01 - immediately from the end of part 1 so
672:04 - you can complete this video and i'll see
672:06 - you in part 2.
672:08 - [Music]
672:12 - welcome back this is part 2 of the
672:15 - connecting to your instances demo and we
672:18 - will be starting exactly where we left
672:20 - off in part one so with that being said
672:23 - let's dive in and so now that we've
672:25 - created our windows instance and went
672:27 - through all the methods of how to
672:29 - connect to it let's go ahead and create
672:31 - a linux instance i'm going to go up to
672:34 - the top menu here and click on create
672:36 - instance and i'm going to name this
672:38 - instance
672:39 - linux instance i'm not going to give it
672:41 - any labels under region i'm going to
672:44 - select the us east one region and the
672:47 - zone i'm going to leave it as its set
672:49 - default as us east 1b the machine
672:52 - configuration i'm going to leave it as
672:54 - is under boot disk i'm going to leave
672:56 - this as is with the debian distribution
672:59 - and i'm going to go ahead and click on
673:01 - create
673:03 - okay and our linux instance has been
673:06 - created and in order for me to connect
673:08 - to it
673:09 - i am going to ssh into it but first i
673:12 - need to satisfy the constraint of having
673:14 - a firewall rule with tcp port 22 open so
673:18 - i'm going to head on over to the
673:19 - navigation menu
673:21 - and i'm going to scroll down to vpc
673:23 - network i'm going to head on over to
673:25 - firewall and as expected the allow ssh
673:28 - firewall rule has been created alongside
673:32 - the default vpc network and so since
673:34 - i've satisfied that constraint i can
673:36 - head back on over to compute engine and
673:39 - so here i have a few different options
673:41 - that i can select from for logging into
673:43 - my linux instance i can open in a
673:46 - browser window if i decided i wanted to
673:48 - put it on a custom port i can use this
673:51 - option here if i provided a private ssh
673:54 - key to connect to this linux instance i
673:57 - can use this option here i have the
673:59 - option of viewing the gcloud command in
674:02 - order to connect to it
674:04 - and i've been presented with a pop-up
674:06 - with the command to use within the
674:09 - gcloud command line in order to connect
674:11 - to my instance i can run it now in cloud
674:14 - shell but i'm going to simply close it
674:16 - and so whether you are on a mac a
674:19 - windows machine or a linux machine you
674:22 - can simply click on ssh and it will open
674:25 - a new browser window connecting you to
674:27 - your instance
674:31 - now when you connect to your linux
674:33 - instance for the first time
674:35 - compute engine generates an ssh key pair
674:38 - for you this key pair by default is
674:42 - added to your project or instance
674:44 - metadata and this will give you the
674:45 - freedom of not having to worry about
674:48 - managing keys now if your account is
674:50 - configured to use os login compute
674:54 - engine stores the generated key pair
674:56 - with your user account
674:58 - now when connecting to your linux
675:00 - instance in most scenarios google
675:03 - recommends using os login this feature
675:06 - lets you use iam roles to manage ssh
675:09 - access to linux instances and this
675:12 - relieves the complexity of having to
675:14 - manage multiple key pairs and is the
675:17 - recommended way to manage many users
675:20 - across multiple instances or projects
675:23 - and so i'm going to go ahead now and
675:25 - show you how to configure os login for
675:28 - your linux instance and the way to do
675:31 - this will be very similar on all
675:33 - platforms so i'm going to go ahead and
675:35 - go back to my mac vm and i'm going to
675:38 - open up my terminal
675:39 - make this bigger for better viewing
675:42 - and i'm going to start by running the
675:43 - gcloud init command in order to make
675:46 - sure i'm using the right user and for
675:48 - the sake of this demonstration i'm going
675:51 - to re-initialize this configuration so
675:54 - i'm going to click on one hit enter
675:56 - number two for tony bowtie ace and i'm
675:59 - going to use project bow tie ink so 1
676:02 - and i'm not going to configure a default
676:05 - compute region in zone and so if i run
676:08 - the gcloud config list command i can see
676:11 - that the account that i'm using is tony
676:13 - bowties gmail.com in project bowtie inc
676:17 - and so because os login requires a key
676:20 - pair i'm going to have to generate that
676:22 - myself so i'm going to go ahead and
676:24 - clear the screen and i'm going to use
676:26 - the command ssh keygen and this is the
676:29 - command to create a public and private
676:32 - key pair i'm going to use the default
676:34 - path to save my key and i'm going to
676:36 - enter a passphrase
676:38 - i'm going to enter it again and i
676:40 - recommend that you write down your
676:41 - passphrase so that you don't forget it
676:43 - as when you lose it you will be unable
676:46 - to use your key pair and so if i change
676:48 - directory to dot ssh and do an ls for
676:52 - list i can see that i now have my public
676:55 - and private key pair the private key
676:57 - lying in id underscore rsa and the
677:00 - public key lying in id underscore
677:03 - rsa.pub and so another constraint that i
677:06 - have is i need to enable os login for my
677:09 - linux instance so i'm going to go ahead
677:12 - and go back to the console and i'm going
677:14 - to go ahead and go into my linux
677:16 - instance
677:17 - i'm going to click on edit and if you
677:19 - scroll down you will come to some fields
677:22 - marked as custom metadata and under key
677:25 - you will type in enable dash os login
677:28 - and under value you will type in all
677:30 - caps true now i wanted to take a moment
677:33 - here to discuss this feature here under
677:36 - ssh keys for block project wide ssh keys
677:41 - now project wide public ssh keys are
677:44 - meant to give users access to all of the
677:47 - linux instances in a project that allow
677:50 - project project-wide public ssh keys so
677:53 - if an instance blocks project-wide
677:56 - public ssh keys as you see here
677:59 - a user can't use their project-wide
678:01 - public ssh key to connect to the
678:04 - instance
678:05 - unless the same public ssh key is also
678:08 - added to the instance metadata this
678:11 - allows only users whose public ssh key
678:15 - is stored in instance level metadata to
678:18 - access the instance
678:20 - and so this is an important feature to
678:22 - note for the exam and so we're going to
678:24 - leave this feature checked off for now
678:27 - and then you can go to the bottom and
678:28 - click on save now if i wanted to enable
678:31 - os login for all instances in my project
678:35 - i can simply go over to the menu on the
678:37 - left and click on metadata and add the
678:40 - metadata here with the same values so
678:43 - under key i type in enable dash os login
678:47 - and under value i type in in all caps
678:49 - true but i don't want to enable it for
678:52 - all my instances
678:54 - only for that one specific instance so
678:56 - with regards to project-wide public keys
679:00 - these keys can be managed through
679:02 - metadata and should only be used as a
679:05 - last resort if you cannot use the other
679:08 - tools such as ssh from the console or os
679:12 - login these are where the keys are
679:14 - stored and so you can always find them
679:17 - here when looking for them here as you
679:19 - can see there are a couple of keys for
679:22 - tony bowtie ace that i have used for
679:24 - previous instances and so i'm going to
679:27 - go back to metadata just to make sure
679:29 - that my key value pair for os login has
679:32 - not been saved and it is not and i'm
679:34 - going to head back on over to my
679:36 - instances and so now that my constraint
679:39 - has been fulfilled where i've enabled
679:41 - the os login feature by adding the
679:43 - unnecessary metadata i'm going to head
679:46 - on over back to my mac vm
679:48 - i'm going to go ahead and clear the
679:50 - screen so now i'm going to go ahead and
679:52 - log into my instance using os login by
679:55 - using the command gcloud compute os dash
680:00 - login ssh dash keys add and then the
680:04 - flag key dash file and then the path for
680:07 - my public key which is dot ssh forward
680:11 - slash id underscore rsa.pub i'm gonna
680:15 - hit enter
680:16 - and so my key has been successfully
680:18 - stored with my user account i'm gonna go
680:21 - ahead and make this a little bigger for
680:22 - better viewing and so in order to log
680:25 - into my instance i'm going to need my
680:27 - username which is right up here under
680:30 - username i'm going to copy that and i'm
680:32 - just going to clear my screen for a
680:34 - second here for better viewing and so in
680:36 - order for me to ssh into my instance i'm
680:39 - going to type in the command ssh minus i
680:43 - i'm going to have to provide my private
680:45 - key which is in dot ssh forward slash id
680:49 - underscore rsa and then my username that
680:51 - i had recorded earlier at and then i'm
680:54 - going to need my public ip address of my
680:56 - linux instance so i'm going to head back
680:59 - over to the console for just a sec
681:01 - i'm going to copy the ip address head
681:04 - back over to my mac vm paste it in and
681:06 - hit enter it's asking if i want to
681:08 - continue yes i do
681:10 - enter the passphrase for my key
681:13 - and success i am connected and so there
681:16 - is one caveat that i wanted to show you
681:19 - with regards to permissions for os login
681:22 - so i'm going to head back over to the
681:23 - console and i'm going to go up to the
681:25 - navigation menu and head over to i am an
681:28 - admin now as you can see here tony
681:31 - bowties gmail.com has the role of owner
681:35 - and therefore i don't need any granular
681:38 - specific permissions i have the access
681:41 - to do absolutely anything now in case i
681:44 - was a different user and i didn't hold
681:46 - the role of owner i would be looking for
681:49 - specific permissions
681:51 - that would be under compute
681:54 - os login and this would give me
681:56 - permissions as a standard user now if i
681:59 - wanted super user access or root access
682:03 - i would need to be given the compute os
682:06 - admin login role and as you can see it
682:08 - would allow me administrator user
682:10 - privileges so when using os login and
682:13 - the member is not an owner one of these
682:16 - two roles are needed so i'm going to
682:18 - exit out of here i'm going to hit cancel
682:20 - and so that about covers everything that
682:22 - i wanted to show you with regards to all
682:25 - the different methods that you can use
682:28 - for connecting to vm instances for both
682:31 - windows and linux instances now i know
682:34 - this may have been a refresher for some
682:37 - but for others
682:38 - knowing all the different methods of
682:41 - connecting to instances can come in very
682:43 - useful especially when coordinating many
682:47 - instances in bigger environments i want
682:49 - to congratulate you on making it to the
682:51 - end of this demo and gaining a bit more
682:54 - knowledge on this crucial part of
682:56 - managing your instances so before you go
682:59 - be sure to delete any resources that
683:02 - you've created and again congrats on the
683:05 - great job so you can now mark this as
683:07 - complete and i'll see you in the next
683:09 - one
683:14 - welcome back in this demonstration i'll
683:16 - be discussing metadata and how it can
683:19 - pertain to a project as well as an
683:22 - instance as well i'm going to touch on
683:24 - startup and shutdown scripts and it's
683:26 - real world use cases in the last lesson
683:30 - we touched the tip of the iceberg when
683:32 - it came to metadata and wanted to go a
683:34 - bit deeper on this topic as i personally
683:37 - feel that it holds so much value
683:40 - and give you some ideas on how you can
683:42 - use it i'm also going to combine the
683:44 - metadata using variables in a startup
683:47 - script and i'm going to bring to life
683:49 - something that's dynamic in nature so
683:51 - with that being said let's dive in so i
683:54 - am currently logged in as tony at bowtie
683:57 - ace gmail.com
683:59 - under the project of bow tie inc and so
684:02 - in order to get right into the metadata
684:04 - i'm going to head on over to my
684:06 - navigation menu and go straight to
684:08 - compute engine and over here on the left
684:11 - hand menu you will see metadata and you
684:14 - can drill down into there now as i
684:16 - explained in a previous lesson
684:18 - metadata can be assigned to both
684:20 - projects and instances while instance
684:23 - metadata
684:24 - only impacts a specific instance so here
684:27 - i can add and store metadata which will
684:30 - be used on a project-wide basis as well
684:34 - as mentioned earlier metadata is stored
684:36 - in key value pairs and can be added at
684:39 - any time now this is a way to add custom
684:42 - metadata but there is a default set of
684:45 - metadata entries that every instance has
684:48 - access to and again this applies for
684:50 - both project and instance metadata so
684:53 - here i have the option of setting my
684:55 - custom metadata for the entire project
684:58 - and so i'm going to dive into where to
685:00 - store custom metadata on an instance and
685:03 - so in order for me to show you this i'm
685:05 - going to first head over to vm instances
685:08 - and create my instance and so just as a
685:11 - note before creating your instance make
685:13 - sure that you have the default vpc
685:15 - created and so because i like to double
685:18 - check things i'm going to head over to
685:19 - the navigation menu i'm going to scroll
685:21 - down to vpc network and as expected i
685:24 - have the default vpc already created and
685:28 - so this means i can go ahead and create
685:30 - my instance so i'm going to head back on
685:32 - over to compute engine
685:35 - and i'm going to create my instance and
685:37 - i'm going to name this instance
685:39 - bowtie dash web server i'm not going to
685:43 - add any labels and under the region i'm
685:45 - going to select us east one and you can
685:48 - keep the zone as the default as us east
685:51 - 1b under machine type i want to keep
685:53 - things cost effective so i'm going to
685:55 - select the e2 micro i'm going to scroll
685:58 - down and under identity and api access i
686:02 - want to set access for each api
686:04 - and scroll down to compute engine i want
686:07 - to select it and i want to select on
686:09 - read write and i'm going to leave the
686:11 - rest as is and scrolling down to the
686:14 - bottom i want to click on management
686:16 - security disks networking and sold
686:18 - tenancy
686:19 - and under here you will find the option
686:22 - to add any custom metadata and you can
686:25 - provide it right here under metadata as
686:28 - a key value pair but we're not going to
686:30 - add any metadata right now so i'm just
686:32 - going to scroll down to the bottom i'm
686:34 - going to leave everything else as is and
686:36 - simply click on create
686:38 - and it should take a few moments for my
686:40 - instance to be created okay and now that
686:42 - my instance is up i want to go ahead and
686:45 - start querying the metadata now just as
686:47 - a note metadata must be queried from the
686:50 - instance itself and can't be done from
686:53 - another instance or even from the cloud
686:55 - sdk on your computer so i'm going to go
686:57 - ahead and log into the instance using
687:00 - ssh
687:02 - okay and now that i'm logged into my
687:04 - instance i want to start querying the
687:06 - metadata now normally you would use
687:09 - tools like wget or curl to make these
687:12 - queries
687:13 - in this demo i will use curl and for
687:16 - those who don't know curl is a command
687:18 - line tool to transfer data to or from a
687:21 - server using supported protocols like
687:24 - http ftp scp and many more this tool is
687:29 - fantastic for automation since it's
687:32 - designed to work without any user
687:34 - interaction and so i'm going to paste in
687:36 - the url that i am going to use to query
687:39 - the instance metadata and this is the
687:42 - default url that you would use to query
687:45 - any metadata on any instance getting a
687:48 - little deeper into it a trailing slash
687:51 - shown here shows that the instance value
687:54 - is actually a directory and will have
687:56 - other values that append to this url
687:59 - whether they are other directories or
688:01 - just endpoint values now when you query
688:04 - for metadata you must provide the
688:06 - following header in all of your requests
688:09 - metadata dash flavor colon google and
688:13 - should be put in quotations if you don't
688:15 - provide this header the metadata server
688:18 - will deny your request so i'm going to
688:20 - go ahead and hit enter and as you can
688:22 - see i've been brought up a lot of
688:23 - different values that i can choose from
688:26 - in order to retrieve different types of
688:28 - metadata and as stated before anything
688:30 - with a trailing slash is actually a
688:33 - directory and will have other values
688:36 - underneath it so if i wanted to query
688:38 - the network interfaces
688:40 - and because it's a directory i need to
688:42 - make sure that i add the trailing slash
688:44 - at the end and as you can see here i
688:47 - have the network interface of 0 and i'm
688:49 - going to go ahead and query that
688:51 - and here i will have access to all the
688:54 - information about the network interface
688:57 - on this instance so i'm going to go
688:59 - ahead and query the network on this
689:01 - interface and as expected the default
689:04 - network is displayed i'm going to
689:06 - quickly go ahead and clear my screen and
689:08 - i'm going to go ahead and query some
689:10 - more metadata this time i'm going to do
689:12 - the name of the server and as expected
689:15 - bowtie dash web server showed up and
689:18 - because it's an endpoint i don't need
689:20 - the trailing slash at the end i'm going
689:22 - to go ahead and do one more this time
689:24 - i'm going to choose machine type
689:27 - and again as expected the e2 micro
689:30 - machine type is displayed and so just as
689:33 - a note for those who haven't noticed any
689:35 - time that you query metadata it will
689:38 - show up to the left of your command
689:40 - prompt now what i've shown you here is
689:42 - what you can do with instance metadata
689:45 - and so how about if you wanted to query
689:46 - any project metadata well instead of
689:49 - instance at the end you would use
689:51 - project with the trailing slash i'm
689:53 - going to simply click on enter and as
689:55 - you can see here project doesn't give me
689:57 - a whole lot of options but it does give
690:00 - me some important values like project id
690:03 - so i'm going to simply query that right
690:05 - now and as expected bowtie inc is
690:08 - displayed and so this is a great example
690:10 - of how to query any default metadata for
690:14 - instances and for projects now you're
690:16 - probably wondering how do i query my
690:19 - custom metadata well once custom
690:21 - metadata has been set you can then query
690:24 - it from the attributes directory in the
690:27 - attributes directory can be found in
690:30 - both the instance and project metadata
690:32 - so i'm going to go ahead and show you
690:34 - that now but first i wanted to add some
690:37 - custom metadata and this can be set in
690:39 - either the console the gcloud command
690:42 - line tool or using the api and so i'm
690:45 - going to run the command here gcloud
690:47 - compute instances add dash metadata the
690:50 - name of your instance and when you're
690:52 - adding custom metadata you would add the
690:54 - flag dash dash metadata with the key
690:57 - value pair which in this example is
691:00 - environment equals dev and then i'm also
691:03 - going to add the zone of the instance
691:05 - which is us east 1a and i'm going to hit
691:08 - enter
691:09 - and because i had a typo there i'm going
691:11 - to go ahead and try that again using us
691:14 - east 1b
691:15 - i'm going to hit on enter
691:17 - and success and so to verify that this
691:20 - command has worked
691:22 - i'm going to go ahead and query the
691:23 - instance and i'm going to go under
691:25 - attributes
691:27 - i'm going to hit on enter and as you can
691:29 - see here the environment endpoint has
691:31 - been populated so i'm going to query
691:34 - that and as expected dev is displaying
691:37 - as the environment value now if i wanted
691:39 - to double check that in the console i
691:41 - can go over to the console i can drill
691:43 - down into bowtie web server
691:46 - and if i scroll down to the bottom under
691:48 - custom metadata you can see the key
691:51 - value pair here has m as the key and dev
691:55 - being the value and so these are the
691:57 - many different ways that you can query
691:58 - metadata for any instances or projects
692:02 - now i wanted to take a quick moment to
692:04 - switch gears and talk about startup and
692:07 - shutdown scripts now compute engine lets
692:10 - you create and run your own startup and
692:13 - shutdown scripts on your vm instance and
692:16 - this allows you to perform automation
692:18 - that can perform actions when starting
692:20 - up such as installing software
692:22 - performing updates or any other tasks
692:25 - that are defined in the script and when
692:27 - shutting down you can allow instances
692:30 - time to clean up on perform tasks such
692:33 - as exporting logs to cloud storage or
692:36 - bigquery or syncing with other systems
692:39 - and so i wanted to go ahead and show you
692:41 - how this would work while combining
692:43 - metadata into the script so i'm going to
692:45 - go ahead and drill down into bow tie web
692:47 - server
692:48 - i'm going to click on edit and i'm going
692:50 - to scroll down here
692:52 - to custom metadata i'm going to click on
692:54 - add item and under key i'm going to type
692:57 - in
692:57 - startup dash script and under value i'm
693:02 - going to paste in my script i'm going to
693:04 - just enlarge this here for a second and
693:06 - i will be providing the script in the
693:08 - github repository now just to break it
693:10 - down this is a bash script i'm pulling
693:13 - in a variable called name which will
693:15 - query the instance name as well i have a
693:18 - variable called zone which will query
693:21 - the instance zone i'm going to be
693:23 - installing an apache web server and it's
693:25 - going to display on a web browser both
693:27 - the server name and the zone that it's
693:30 - in and so in order for me to see this
693:32 - web page i also need to open up some
693:34 - firewall rules and so an easy way to do
693:37 - this would be to scroll up to firewalls
693:39 - and simply click on allow http and allow
693:43 - https traffic this will tag the instance
693:46 - with some network tags as http server
693:50 - and https server and create two separate
693:53 - firewall rules that will allow traffic
693:56 - for port 80 and port 443 so i'm going to
693:59 - leave everything else as is i'm going to
694:01 - scroll down to the bottom and click on
694:03 - save okay and it took a few seconds
694:05 - there but it did finish saving i'm going
694:08 - to go ahead and go up to the top and
694:10 - click on reset and this will perform a
694:12 - hard reset on the instance and will
694:14 - allow the startup script to take effect
694:17 - so i'm going to click on reset it's
694:18 - going to ask me if i really want to do
694:20 - this and for the purposes of this
694:22 - demonstration i'm going to click on
694:24 - reset please note you should never do
694:26 - this in production as it doesn't do a
694:28 - clean shutdown on the operating system
694:31 - but as this is an instance with nothing
694:33 - on it i'm going to simply click on reset
694:36 - now i'm going to head on back to the
694:38 - main console for my vm instances and i'm
694:41 - going to record my external ip i'm going
694:44 - to open up a new browser i'm going to
694:46 - zoom in for better viewing and i'm going
694:48 - to paste in my ip address and hit enter
694:51 - and as you can see here
694:53 - i've used my startup script to display
694:56 - not only this web page but i was able to
694:58 - bring in metadata that i pulled using
695:00 - variables and was able to display it
695:03 - here in the browser and so before i end
695:05 - this demonstration i wanted to show you
695:08 - another way of using a startup script
695:10 - but being able to pull it in from cloud
695:12 - storage so i'm going to go back to the
695:14 - navigation menu and i'm going to scroll
695:17 - down to storage
695:20 - here i will create a new bucket
695:23 - and for now find a globally unique name
695:25 - to name your bucket and i'm going to
695:27 - call my bucket bowtie web server site
695:31 - and i'm going to leave the rest as its
695:32 - default and i'm going to simply click on
695:34 - create
695:35 - and if you have a globally unique name
695:37 - for your bucket you will be prompted
695:39 - with this page without any errors and
695:42 - i'm going to go ahead and upload the
695:44 - script
695:45 - and you can find this script in the
695:47 - github repository so i'm going to go
695:49 - into my repo and i'm going to look for
695:51 - bow tie start up final sh i'm going to
695:54 - open it
695:56 - and now that i have the script uploaded
695:58 - i'm going to drill into this file so i
696:00 - can get some more information that i
696:02 - need for the instance and what i need
696:05 - from here is to copy the uri so i'm
696:07 - going to copy this to my clipboard and
696:09 - i'm going to head back on over to
696:11 - compute engine i'm going to drill down
696:13 - into my instance
696:15 - i'm going to click on edit at the top
696:17 - and i'm going to scroll down to where it
696:19 - says custom metadata and here i'm going
696:22 - to remove the startup script metadata
696:25 - and i'm going to add a new item and i'm
696:27 - going to be adding startup dash script
696:30 - dash url and in the value i'm going to
696:33 - paste in the uri that i had just copied
696:36 - over and this way on startup my instance
696:39 - will use this startup script that's in
696:42 - cloud storage so i'm going to scroll
696:44 - down to the bottom click on save
696:46 - and now i'm going to click on reset i'm
696:48 - going to reset here i'm going to go back
696:50 - to the main page for my vm instances and
696:52 - i can see that my external ip hasn't
696:55 - changed so i'm going to go back to my
696:56 - open web browser and i'm going to click
696:59 - on refresh and success and as you can
697:02 - see here i've taken a whole bunch of
697:04 - different variables including the
697:06 - machine name
697:07 - the environment variable the zone as
697:10 - well as the project and i've displayed
697:12 - it here in a simple website and although
697:15 - you may not find this website
697:17 - specifically useful in your production
697:19 - environment this is just an idea to get
697:22 - creative using default and custom
697:24 - metadata along with a startup script
697:27 - i've seen in some environments where
697:29 - people have multiple web servers and
697:32 - create a web page to display all the
697:35 - specific web servers in their different
697:38 - environments along with their ips their
697:40 - data and their configurations and so
697:43 - just as a recap we've gone through the
697:45 - default and custom metadata and how to
697:48 - query it in an instance we also went
697:51 - through startup scripts and how to apply
697:53 - them both locally and using cloud
697:55 - storage and so i hope you have enjoyed
697:58 - having fun with metadata and using them
698:01 - in startup scripts such as this one i
698:03 - also hope you find some fascinating use
698:05 - cases in your current environments and
698:08 - so before you go just a quick reminder
698:11 - to delete any resources that you've
698:13 - created to not incur any added costs and
698:16 - so that's pretty much all i wanted to
698:17 - cover with this demonstration so you can
698:20 - now mark this as complete and let's move
698:22 - on to the next one
698:24 - [Music]
698:28 - welcome back and in this lesson i'm
698:30 - going to be discussing compute engine
698:32 - billing now when it comes to pricing
698:34 - with regards to compute engine i've only
698:37 - gone over the fact that instances are
698:39 - charged by the second after the first
698:41 - minute but i never got into the depths
698:44 - of billing and the various ways to save
698:46 - money when using compute engine in this
698:49 - lesson i will be unveiling how both
698:52 - costs and discounts are broken down in
698:55 - google cloud as it refers to the
698:57 - resource based billing model and the
698:59 - various savings that can be had when
699:01 - using compute engine so with that being
699:04 - said let's dive in
699:07 - now each vcpu and each gigabyte of
699:10 - memory on compute engine is built
699:12 - separately rather than as part of a
699:15 - single machine type you are still
699:17 - creating instances using pre-defined
699:20 - machine types but your bill shows them
699:22 - as individual cpus and memory used per
699:26 - hour and this is what google refers to
699:29 - as resource-based billing which i will
699:31 - get into in just a bit the billing model
699:34 - applies to all vcpus gpus and memory
699:38 - resources and are charged a minimum of
699:42 - one minute for example if you run your
699:44 - virtual machine for 30 seconds you will
699:47 - be billed for one minute of usage after
699:50 - one minute instances are charged in one
699:53 - second increments instance up time is
699:56 - another determining factor for cost and
699:59 - is measured as the number of seconds
700:01 - between when you start an instance and
700:03 - when you stop an instance in other words
700:06 - when your instance is in the terminated
700:08 - state if an instance is idle but still
700:11 - has a state of running it will be
700:13 - charged for instance uptime but again
700:16 - you will not be charged if your instance
700:18 - is in a terminated state
700:21 - now getting into reservations these are
700:24 - designed to reserve the vm instances you
700:27 - need so after you create a reservation
700:30 - the reservation ensures that those
700:33 - resources are always available for you
700:36 - to use during the creation process you
700:39 - can choose how a reservation is to be
700:42 - used for example you can choose for a
700:44 - reservation to be automatically applied
700:47 - to any new or existing instances that
700:50 - match the reservation's properties which
700:53 - is the default behavior or you can
700:55 - specify that reservation to be consumed
700:58 - by a specific instance in all cases a vm
701:02 - instance can only use a reservation if
701:05 - its properties exactly match the
701:07 - properties of the reservation after you
701:10 - create a reservation you begin paying
701:13 - for the reserved resources immediately
701:15 - and they remain available for your
701:17 - project to use indefinitely until the
701:20 - reservation is deleted reservations are
701:23 - great to ensure that your project has
701:26 - resources for future increases in demand
701:30 - including planned or unplanned spikes
701:33 - backup and disaster recovery or for a
701:36 - buffer when you're planning growth when
701:38 - you no longer need a reservation you can
701:40 - simply delete the reservation to stop
701:43 - incurring charges each reservation like
701:46 - normal vms are charged based on existing
701:49 - on-demand rates which include sustained
701:52 - use discounts and are eligible for
701:55 - committed use discounts which i will be
701:57 - getting into in just a bit now
701:59 - purchasing reservations do come with
702:02 - some caveats
702:03 - reservations apply only to compute
702:06 - engine data proc
702:08 - and google kubernetes engine as well
702:11 - reservations don't apply to shared core
702:13 - machine types preemptable vms sole
702:17 - tenant nodes
702:18 - cloud sql and data flow now as i
702:21 - explained before each vcpu and each
702:24 - gigabyte of memory on compute engine is
702:27 - built separately rather than as a part
702:30 - of a single machine type and is billed
702:33 - as individual cpus and memory used per
702:36 - hour resource-based pricing allows
702:39 - compute engine to apply sustained use
702:41 - discounts
702:42 - to all of your pre-defined machine type
702:45 - usage in a region collectively
702:48 - rather than to individual machine types
702:51 - and this way vcpu and memory usage for
702:54 - each machine type can receive any one of
702:57 - the following discounts sustained use
703:00 - discounts committed use discounts and
703:03 - preemptable vms and i'd like to take a
703:06 - moment to dive into a bit of detail on
703:09 - each of these discount types starting
703:12 - with sustained use discounts
703:15 - now sustained use discounts are
703:17 - automatic discounts for running specific
703:20 - compute engine resources a significant
703:23 - portion of the billing month for example
703:26 - when you run one of these resources for
703:29 - more than 25 percent of a month compute
703:32 - engine automatically gives you a
703:34 - discount for every incremental minute
703:36 - that you use for that instance now the
703:39 - following tables show the discounts
703:41 - applied for the specific resources
703:44 - described here now for the table on the
703:47 - left for general purpose n2 and n2d
703:51 - predefined and custom machine types and
703:54 - for compute optimized machine types you
703:57 - can receive a discount of up to 20
703:59 - percent the table on the right shows
704:02 - that for general purpose n1 predefined
704:06 - and custom machine types as well as sole
704:09 - tenant nodes and gpus you can get a
704:12 - discount of up to 30 percent sustained
704:15 - use discounts are applied automatically
704:18 - to usage within a project separately for
704:21 - each region so there is no action
704:23 - required on your part to enable these
704:26 - discounts now some notes that i wanted
704:28 - to cover here is that sustained use
704:30 - discounts automatically apply to vms
704:33 - created by both google kubernetes engine
704:36 - and compute engine as well they do not
704:39 - apply to vms created using the app
704:42 - engine flexible environment as well as
704:45 - data flow and the e-2 machine types
704:48 - sustained use discounts are applied on
704:50 - incremental use after you reach certain
704:54 - usage thresholds this means that you pay
704:57 - only for the number of minutes that you
704:59 - use an instance and compute engine
705:02 - automatically gives you the best price
705:04 - google truly believes that there's no
705:07 - reason to run an instance for longer
705:09 - than you need it
705:11 - now sustained use discounts are applied
705:14 - on incremental use after you reach
705:17 - certain usage thresholds this means that
705:20 - you pay only for the number of minutes
705:22 - that you use an instance and compute
705:24 - engine automatically gives you the best
705:27 - price now consider a scenario where you
705:30 - have two instances or sole tenant nodes
705:32 - in the same region that have different
705:35 - machine types and run at different times
705:37 - of the month
705:38 - compute engine
705:40 - breaks down the number of vcpus and
705:42 - amount of memory used across all
705:45 - instances that use predefined machine
705:48 - types and combines the resources to
705:51 - qualify for the largest sustained usage
705:54 - discounts possible now in this example
705:57 - assume you run the following two
705:59 - instances
706:00 - in the us east one region during a month
706:03 - for the first half you run an n1
706:06 - standard four instance with four vcpus
706:10 - and 15 gigabytes of memory for the
706:12 - second half of the month you run a
706:14 - larger and one standard 16 instance with
706:18 - 16 vcpus and 60 gigabytes of memory in
706:22 - this scenario compute engine reorganizes
706:25 - these machine types into individual vcpu
706:28 - and memory resources and combines their
706:31 - usage to create the following resources
706:34 - for vcpus so because four vcpus were
706:38 - being used for the whole month the
706:40 - discount here would be thirty percent
706:43 - the additional twelve vcpus were added
706:46 - on week two in the month and so for
706:48 - those 12 vcpus they would receive a 10
706:52 - discount and this is how discounts are
706:54 - applied when it comes to sustained use
706:56 - discounts now moving on to the next
706:59 - discount type is committed use discounts
707:02 - so compute engine lets you purchase
707:04 - committed use contracts in return for
707:08 - deeply discounted prices for vm usage so
707:11 - when you purchase a committed use
707:13 - contract you purchase compute resource
707:16 - which is comprised of vcpus memory
707:20 - gpus and local ssds and you purchase
707:24 - these resources at a discounted price in
707:27 - return for committing to paying for
707:29 - those resources for one year or three
707:32 - years committed use discounts are ideal
707:35 - for workloads with predictable resource
707:38 - needs so if you know exactly what you're
707:41 - going to use committed use discounts
707:43 - would be a great option for this and the
707:46 - discount is up to 57
707:49 - for most resources like machine types or
707:52 - gpus when it comes to memory optimized
707:55 - machine types the discount is up to 70
707:58 - percent now when you purchase a
708:00 - committed use contract you can purchase
708:02 - it for a single project and applies to a
708:05 - single project by default or you can
708:08 - purchase multiple contracts which you
708:10 - can share across many projects by
708:13 - enabling shared discounts once purchased
708:16 - your billed monthly for the resources
708:19 - you purchased for the duration of the
708:21 - term you selected whether you use the
708:23 - services or not if you have multiple
708:26 - projects that share the same cloud
708:27 - billing account you can enable committed
708:30 - use discount sharing so that all of your
708:33 - projects within that cloud billing
708:35 - account share all of your committed use
708:38 - discount contracts your sustained use
708:40 - discounts are also pooled at the same
708:44 - time now some caveats when it comes to
708:46 - committed use discounts shared core
708:48 - machines are excluded on this as well
708:51 - you can purchase commitments only on a
708:54 - per region basis if a reservation is
708:57 - attached to a committed use discount the
708:59 - reservation can't be deleted for the
709:02 - duration of the commitment so please be
709:05 - aware now to purchase a commitment for
709:07 - gpus or local ssds you must purchase a
709:12 - general purpose and one commitment and
709:14 - lastly after you create a commitment you
709:17 - cannot cancel it you must pay the agreed
709:20 - upon monthly amount for the duration of
709:23 - the commitment now committed use
709:25 - discount recommendations give you
709:27 - opportunities to optimize your compute
709:29 - costs by analyzing your vm spending
709:32 - trends with and without a committed use
709:35 - discount contract by comparing these
709:37 - numbers you can see how much you can
709:40 - save each month with a committed use
709:42 - contract and this can be found under the
709:44 - recommendations tab on the home page in
709:47 - the console and so i wanted to move on
709:49 - to the last discount type which are
709:52 - preemptable vms now preemptable vms are
709:55 - up to eighty percent cheaper than
709:58 - regular instances pricing is fixed and
710:01 - you never have to worry about variable
710:04 - pricing these prices can be found on the
710:06 - link to instance pricing that i have
710:08 - included in the lesson text a
710:11 - preemptable vm is an instance that you
710:13 - can create and run at a much lower price
710:17 - than normal instances however compute
710:20 - engine might stop or preempt these
710:22 - instances if it requires access to those
710:25 - resources for other tasks as preemptable
710:29 - instances our access compute engine
710:32 - capacity so their availability varies
710:34 - with usage now generally compute engine
710:37 - avoids preempting instances
710:40 - but compute engine does not use an
710:42 - instant cpu usage or other behavior to
710:46 - determine whether or not to preempt it
710:48 - now a crucial characteristic to know
710:50 - about preemptable vms is that compute
710:53 - engine always stops them after they run
710:56 - for 24 hours and this is something to be
710:58 - aware of for the exam preemptable
711:01 - instances are finite compute engine
711:03 - resources so they might not always be
711:06 - available and if you happen to
711:08 - accidentally spin up a preemptable vm
711:11 - and you want to shut it down there is no
711:13 - charge if it's running for less than 10
711:16 - minutes now another thing to note is
711:18 - that preemptable instances can't live
711:21 - migrate to a regular vm instance or be
711:24 - set to automatically restart when there
711:27 - is a maintenance event due to the
711:29 - limitations preemptable instances are
711:32 - not covered by any service level
711:34 - agreement and when it comes to the
711:36 - google cloud free tier credits for
711:38 - compute engine this does not apply to
711:40 - preemptable instances so you're probably
711:43 - asking when is a great time to use
711:46 - preemptable vms well if your apps are
711:49 - fault tolerant and can withstand
711:51 - possible instance preemptions then
711:53 - preemptable instances can reduce your
711:56 - compute engine costs significantly for
711:59 - example batch processing jobs can run on
712:02 - preemptable instances if some of those
712:04 - instances stop during processing the job
712:07 - slows down but does not completely stop
712:10 - preemptable instances create your batch
712:13 - processing tasks without placing any
712:16 - additional workload on your existing
712:18 - instances and without requiring for you
712:21 - to pay full price for additional normal
712:24 - instances and since containers are
712:26 - naturally stateless and fault tolerant
712:29 - this makes containers an amazing fit for
712:32 - preemptable vms so running preemptable
712:34 - vms for google kubernetes engine is
712:38 - another fantastic use case now it's
712:40 - really critical that you have an
712:42 - understanding for each different
712:44 - discount type and when is a good time to
712:46 - use each as you may be presented
712:49 - different cost-effective solutions in
712:50 - the exam and understanding these
712:52 - discount types will prepare you to
712:55 - answer them understanding the theory
712:57 - behind this resource-based pricing model
713:00 - all the available discount types along
713:02 - with the types of workloads that are
713:04 - good for each will guarantee that you
713:07 - will become familiar with what types of
713:09 - questions are being asked in the exam
713:12 - and will also make you a better cloud
713:14 - engineer as you will be able to spot
713:17 - where you can save money and be able to
713:19 - make the appropriate changes and so
713:22 - that's pretty much all i wanted to cover
713:24 - when it comes to compute engine billing
713:26 - and its discount types so you can now
713:28 - mark this lesson as complete and let's
713:30 - move on to the next one
713:36 - welcome back in this lesson i'm going to
713:39 - be covering the fundamentals as it
713:41 - pertains to storage these concepts are
713:44 - needed to know in order to fully
713:46 - understand the different google cloud
713:48 - storage options that i will be diving
713:50 - into later as well the exam expects that
713:54 - you know the different types of storage
713:55 - that's available for all the various
713:58 - services
713:59 - and so before i get into the different
714:01 - types of storage i wanted to cover the
714:03 - underlying theory behind it so with that
714:06 - being said let's dive in
714:09 - so i wanted to start off by going
714:10 - through the three types of storage and
714:13 - how data is presented to a user or to
714:16 - the server there is block storage file
714:19 - storage and object storage these types
714:22 - of storage tie into the available
714:24 - services that are available in google
714:27 - cloud and they offer different options
714:29 - for different types of workloads and i
714:31 - will be going over each of these in a
714:33 - bit of depth and so the first one i
714:36 - wanted to touch on is block storage
714:38 - now block storage is sometimes referred
714:41 - to as block level storage and is a
714:44 - technology that is used to store data
714:46 - files on storage systems or cloud-based
714:49 - storage environments block storage is
714:52 - the fastest available storage type and
714:55 - it is also efficient and reliable with
714:58 - block storage files are split into
715:01 - evenly sized blocks of data each with
715:03 - its own unique identifier it is
715:06 - presented to the operating system as
715:08 - structureless raw data in the form of a
715:11 - logical volume or a hard drive and the
715:14 - operating system structures it with a
715:17 - file system like ext3 or ext4 on linux
715:22 - and ntfs for windows it would then mount
715:25 - this volume or drive as the root volume
715:28 - in linux or a c or d drive in windows
715:32 - block storage is usually delivered on
715:34 - physical media in the case of google
715:36 - cloud it is delivered as either spinning
715:39 - hard drives or solid state drives so in
715:43 - google cloud you're presented with block
715:45 - storage that consists of either
715:47 - persistent disks or local ssd
715:51 - which can both be mountable and bootable
715:54 - block storage volumes can then be used
715:56 - as your boot volumes for compute
715:59 - instances in google cloud
716:01 - installed with your operating system of
716:04 - choice and structured so that your
716:06 - operating system database or application
716:10 - will then be able to consume it now
716:12 - moving on to the second type of storage
716:15 - is file storage
716:17 - now file storage is also referred to as
716:20 - file level or file based storage and is
716:23 - normally storage that is presented to
716:26 - users and applications as a traditional
716:29 - network file system in other words the
716:32 - user or application receives data
716:35 - through directory trees folders and
716:38 - files file storage also allows you to do
716:41 - the same this functions similarly to a
716:44 - local hard drive however a structure has
716:47 - already been applied and cannot be
716:50 - adjusted after the fact this type of
716:52 - structure only has the capabilities of
716:55 - being mountable but not bootable you
716:58 - cannot install an operating system on
717:00 - file storage as i said before the
717:03 - structure has already been put in place
717:05 - for you and is ready for you or your
717:08 - application to consume due to this
717:10 - structure the service that is serving
717:13 - the file system has some underlying
717:15 - software that can handle access rights
717:18 - file sharing file locking and other
717:21 - controls related to file storage in
717:24 - google cloud this service that serves
717:26 - this type of storage is known as cloud
717:29 - file store and is usually presented over
717:31 - the network to users in your vpc network
717:35 - using the nfs protocol or in this case
717:39 - nfs version 3. but i'll be diving into
717:42 - that a little bit later and the last
717:44 - storage type that i wanted to cover is
717:46 - object storage
717:48 - now object storage also referred to as
717:51 - object-based storage
717:53 - is a general term that refers to the way
717:55 - in which we organize and work with units
717:58 - of storage called objects and this is a
718:01 - storage type that is a flat collection
718:04 - of unstructured data and this type of
718:06 - storage holds no structure like the
718:09 - other two types of storage and is made
718:11 - up of three characteristics the first
718:14 - one is the data itself and this could be
718:17 - anything from movies songs and even
718:20 - photos of men in fancy bow ties the data
718:23 - could also be binary data as well the
718:26 - second characteristic is the metadata
718:29 - and this is usually related to any
718:31 - contextual information about what the
718:34 - data is or anything that is relevant to
718:37 - the data and the third characteristic is
718:40 - a globally unique identifier and this
718:43 - way it's possible to find the data
718:45 - without having to know the physical
718:47 - location of the data and this is what
718:49 - allows object storage to be infinitely
718:52 - scalable as it doesn't matter where the
718:55 - object is stored this type of storage
718:58 - can be found in google cloud and is
719:00 - known as cloud storage cloud storage is
719:03 - flat storage with a logical container
719:06 - called a bucket that you put objects
719:08 - into now although this type of storage
719:11 - is not bootable using an open source
719:13 - tool called fuse this storage type can
719:17 - be mounted in google cloud and i will be
719:19 - covering that a little bit later in the
719:22 - cloud storage lesson but in most cases
719:25 - object store is designed as the type of
719:27 - storage that is not bootable or
719:30 - mountable and because of the
719:32 - characteristics of this storage
719:34 - it allows object storage again to be
719:37 - infinitely scalable and so these are the
719:40 - three main types of storage that you
719:42 - will need to know and understand as each
719:46 - has its use cases so if you're looking
719:48 - for high performance storage you will
719:50 - always look to block storage to satisfy
719:53 - your needs if you're looking to share
719:55 - files across multiple systems or have
719:58 - multiple applications
720:00 - that need access to the same files and
720:02 - directories then file storage might be
720:05 - your best bet if you're looking to store
720:07 - terabytes of pictures for a web
720:09 - application and you don't want to worry
720:12 - about scaling object storage will allow
720:14 - you to read and write an infinite amount
720:17 - of pictures that will meet your
720:19 - requirements so now that we've covered
720:21 - these storage types let's take a few
720:24 - moments to discuss storage performance
720:27 - terms now when discussing storage
720:29 - performance there are some key terms to
720:31 - understand that when used together
720:34 - define the performance of your storage
720:37 - first there is io which stands for input
720:40 - output
720:41 - and is a single read write request and
720:44 - can be measured in block size and this
720:46 - block size can vary anywhere from one
720:49 - kilobyte to four megabytes and beyond
720:52 - depending on your workload now q depth
720:55 - when it comes to storage is the number
720:58 - of pending input output requests
721:01 - waiting to be performed on a disk io
721:03 - requests become queued when reads or
721:07 - writes are requested faster than they
721:09 - can be processed by the disk when io
721:12 - requests are queued the total amount of
721:14 - time it takes to read or write data to
721:17 - disk becomes significantly higher this
721:20 - is where performance degradation can
721:22 - occur and queue depth must be adjusted
721:25 - accordingly now the next term is a
721:27 - common touch point when it comes to
721:29 - discussing storage performance on gcp
721:32 - and on the exam which is iops and this
721:35 - is a metric that stands for input output
721:38 - operations per second this value
721:41 - indicates how many different input or
721:44 - output operations a device or group of
721:47 - devices can perform in one second more
721:50 - value in the iops signifies the
721:53 - capability of executing more operations
721:57 - per second and again this is a common
721:59 - touch point that i will be diving into a
722:01 - little bit later now next up is
722:04 - throughput and this is the speed at
722:06 - which the data is transferred in a
722:08 - second and is most commonly measured in
722:11 - megabytes per second this is going to be
722:14 - another common topic that comes up
722:16 - frequently when discussing storage on
722:18 - gcp as well
722:20 - latency is the measurement of delay
722:23 - between the time data is requested when
722:27 - the data starts being returned and is
722:29 - measured in milliseconds so the time
722:32 - each io request will take to complete
722:35 - results in being your average latency
722:37 - and the last two terms i wanted to bring
722:39 - up is sequential and random access
722:42 - sequential would be a large single file
722:46 - like a video and random access would be
722:48 - loading an application or an operating
722:51 - system so lots of little files that are
722:53 - all over the place it's obvious that
722:55 - accessing data randomly is much slower
722:59 - and less efficient than accessing it
723:01 - sequentially and this can also affect
723:03 - performance now why i bring up all these
723:06 - terms is not about calculating the
723:08 - average throughput but to give you a
723:10 - holistic view on storage performance
723:13 - as all these characteristics play a part
723:16 - in defining the performance of your
723:19 - storage
723:20 - there is not one specific characteristic
723:22 - that is responsible for disk performance
723:25 - but all have a role in achieving the
723:28 - highest performance possible
723:30 - for your selected storage now i know
723:32 - this is a lot of theory to take in but
723:34 - this will all start to make more sense
723:37 - when we dive into other parts of the
723:39 - course where we will discuss disk
723:41 - performance with all these
723:43 - characteristics as it relates to compute
723:46 - engine and other services that use
723:49 - storage it is crucial to know the
723:51 - storage types as well as the performance
723:53 - characteristics as it will bring clarity
723:56 - to questions in the exam and also give
723:59 - you a better sense on how to increase
724:02 - your storage performance in your work
724:04 - environment and so that's pretty much
724:06 - all i wanted to cover when it comes to
724:08 - storage types and storage performance as
724:11 - it pertains to storage as a whole so you
724:14 - can now mark this lesson as complete and
724:16 - let's move on to the next one
724:18 - [Music]
724:22 - welcome back and in this lesson i'm
724:24 - going to be covering persistent disks
724:27 - and local ssds i'm going to be getting
724:29 - into the detail with the most commonly
724:32 - used storage types for instances which
724:35 - are both persistent disks and local ssds
724:39 - this lesson will sift through all the
724:41 - different types of persistent disks and
724:44 - local ssds along with the performance of
724:47 - each knowing what type of disk to use
724:50 - for your instance and how to increase
724:53 - disk performance shows up on the exam
724:56 - and so i want to make sure to cover it
724:58 - in detail and leave no stone unturned so
725:01 - with that being said let's dive in now
725:04 - persistent disks and local ssds are the
725:08 - two available types of block storage
725:10 - devices
725:11 - available in google cloud and the
725:14 - determining factor of what you will use
725:16 - for your particular scenario will depend
725:19 - on your use case and the specific
725:22 - characteristics that you require from
725:24 - each storage medium now by default each
725:27 - compute engine instance has a single
725:30 - boot persistent disk that contains the
725:33 - operating system when you require
725:35 - additional storage space you can add one
725:38 - or more additional persistent disks or
725:41 - local ssds to your instance and i will
725:44 - be going through these storage options
725:47 - along with their characteristics now as
725:49 - you can see here persistent disks and
725:52 - local ssds come in a slew of different
725:55 - types as well with persistent disks they
725:58 - are available in both zonal and regional
726:01 - options so starting off with persistent
726:04 - disks you have three different types you
726:06 - can choose from as well you have the
726:09 - flexibility of choosing from two
726:11 - different geographic options when it
726:13 - comes to the redundancy of your
726:15 - persistent disks and i will be covering
726:17 - the zonal and regional options in detail
726:20 - in just a bit now persistent disks are
726:23 - durable network storage devices
726:26 - that your instances can access like
726:28 - physical disks in a computer so these
726:31 - are not physically attached disks but
726:34 - network disks that are connected over
726:36 - google's internal network persistent
726:39 - disks are independent of your instance
726:42 - and can persist after your instance has
726:44 - been terminated and this can be done by
726:46 - turning on this flag upon creation you
726:49 - can even detach your disk and move it to
726:52 - other instances when you need to scaling
726:54 - persistent disks can be done
726:56 - automatically and on the fly by using
727:00 - the disk resize feature and this gives
727:02 - you the flexibility to resize your
727:05 - current persistent disks with no
727:07 - downtime and even add additional disks
727:10 - to your instance for additional
727:12 - performance and storage persistent disks
727:15 - are also encrypted by default and google
727:18 - also gives you the option of using your
727:21 - own custom keys each persistent disk can
727:24 - be up to 64 terabytes in size and most
727:28 - instances can have up to 128 persistent
727:32 - disks and up to 257 terabytes of total
727:36 - persistent disk space attached and just
727:39 - as a note share core machine types are
727:41 - limited to 16 persistent disks and 3
727:45 - terabytes of total persistent disk space
727:48 - and so now that i've gone through the
727:49 - details of persistent disks i wanted to
727:52 - dive into the two geographic options
727:55 - that's available for persistent disks
727:58 - first starting with zonal now zonal
728:01 - persistent disks are disks that are
728:03 - available in one zone in one region
728:06 - these disks are the most commonly used
728:08 - persistent disks for general day-to-day
728:11 - usage and used for those whose workloads
728:15 - are not sensitive to specific zone
728:17 - outages they are redundant within the
728:19 - zone you've created them in but cannot
728:22 - survive an outage of that zone and may
728:24 - be subjected to data loss if that
728:27 - specific zone is affected and this is
728:29 - where snapshots should be a part of your
728:32 - high availability strategy when using
728:35 - zonal persistent disks snapshots are
728:38 - incremental and can be taken even if you
728:41 - snapshot disks that are attached to
728:43 - running instances and i'll be going into
728:45 - detail about snapshots in a later lesson
728:49 - zonal persistent disks can also be used
728:52 - with any machine type including
728:54 - pre-defined shared core and custom
728:57 - machine types now when it comes to
728:59 - regional persistent disks they have
729:02 - storage qualities that are similar to
729:04 - zonal persistent disks however regional
729:07 - persistent disks provide durable storage
729:10 - and replication of data between two
729:14 - zones in the same region if you are
729:16 - designing systems that require high
729:18 - availability on compute engine you
729:21 - should use regional persistent disks
729:24 - combined with snapshots for durability
729:27 - regional persistent disks are also
729:29 - designed to work with regional managed
729:31 - instance groups in the unlikely event of
729:34 - a zonal outage you can usually fail over
729:37 - your workload running on regional
729:39 - persistent disks to another zone by
729:42 - simply using the force attached flag
729:45 - regional persistent disks are slower
729:48 - than zonal persistent disks and should
729:50 - be taken into consideration when write
729:53 - performance is less critical than data
729:56 - redundancy across multiple zones now
729:59 - noting a couple of caveats here when it
730:01 - comes to disk limits regional persistent
730:04 - disks are similar to zonal persistent
730:07 - disks however regional standard
730:09 - persistent disks have a 200 gigabyte
730:12 - size minimum and may be a major factor
730:15 - when it comes to cost so please be aware
730:18 - as well you can't use regional
730:20 - persistent disks with memory optimized
730:23 - machine types or compute optimized
730:26 - machine types now these two geographic
730:29 - options are available for all three
730:32 - persistent disk types whose
730:34 - characteristics i will dive into now
730:37 - starting off with the standard
730:38 - persistent disk type also known in
730:41 - google cloud as pd standard now these
730:44 - persistent disks are backed by standard
730:47 - hard disk drives and these are your
730:49 - standard spinning hard disk drives and
730:52 - allows google cloud to give a cost
730:54 - effective solution for your specific
730:56 - needs standard persistent disks are
730:59 - great for large data processing
731:01 - workloads that primarily use sequential
731:04 - ios now as explained earlier sequential
731:08 - access would be accessing larger files
731:11 - and would require less work by the hard
731:13 - drive thus decreasing latency as there
731:16 - are physical moving parts in this hard
731:18 - drive this would allow the disc to do
731:21 - the least amount of work as possible and
731:24 - therefore making it the most efficient
731:26 - as possible and therefore sequential ios
731:29 - are best suited for this type of
731:31 - persistent disk and again this is the
731:34 - lowest price persistent disks out of all
731:37 - the persistent disk types now stepping
731:39 - into the performance of standard
731:41 - persistent disks for just a second
731:44 - please remember that iops and throughput
731:46 - performance depends on disk size
731:49 - instance vcpu count and i o block size
731:52 - among other factors and so this table
731:55 - here along with the subsequent tables
731:57 - you will see later are average speeds
732:00 - that google has deemed optimum for these
732:03 - specific disk types they cover the
732:05 - maximum sustained iops as well as the
732:08 - maximum sustained throughput along with
732:10 - the granular breakdown of each here you
732:13 - can see the differences between both the
732:16 - zonal and regional standard pd and as
732:19 - you can see here in the table the zonal
732:22 - standard pd and the regional standard pd
732:25 - are pretty much the same when it comes
732:27 - to most of these metrics but when you
732:29 - look closely at the read iops per
732:31 - instance this is where they differ where
732:34 - the zonal standard pd has a higher read
732:37 - iops per instance than the regional
732:40 - standard pd and this is because the
732:42 - regional standard pd is accessing two
732:45 - different disks in two separate zones
732:48 - and so the latency will be higher the
732:50 - same thing goes for right throughput per
732:52 - instance and so this would be a decision
732:55 - between high availability versus speed
732:58 - moving on to the next type of persistent
733:00 - disk is the balanced persistent disk in
733:03 - google cloud known as pd balance this
733:06 - disk type is the alternative to the ssd
733:09 - persistent disks that balance both
733:12 - performance and cost as this disk type
733:15 - has the same maximum iops as the ssd
733:18 - persistent disk type but holds a lower
733:21 - iops per gigabyte and so this disk is
733:24 - designed for general purpose use the
733:26 - price for this disk also falls in
733:29 - between the standard and the ssd
733:32 - persistent disks so this is basically
733:34 - your middle of the road disk when you're
733:36 - trying to decide between price and speed
733:40 - moving straight into performance i put
733:42 - the standard pd metric here so that you
733:45 - can see a side-by-side comparison
733:48 - between the balance pd and the standard
733:50 - pd and as you can see here when it comes
733:53 - to the metrics under the maximum
733:55 - sustained iops the balance pd is
733:58 - significantly higher than the standard
734:01 - pd in both the zonal and regional
734:04 - options as well looking at the maximum
734:06 - sustained throughput the read write
734:08 - throughput per gigabyte is a little over
734:11 - two times faster and the right
734:13 - throughput per instance is three times
734:16 - faster so quite a bit of jump from the
734:18 - standard pd to the balance pd and moving
734:21 - on to the last persistent disk type is
734:24 - the ssd persistent disk type also known
734:27 - in google cloud as a pd ssd and these
734:31 - are the fastest persistent disks that
734:33 - are available
734:34 - and are great for enterprise
734:36 - applications and high performance
734:38 - databases that demand lower latency and
734:41 - more iops so this would be great for
734:44 - transactional databases or applications
734:47 - that require demanding and near
734:50 - real-time performance the pd ssds have a
734:53 - single digit millisecond latency and
734:56 - because of this comes at a higher cost
734:58 - and therefore is the highest price
735:01 - persistent disk moving on to the
735:03 - performance of this persistent disk this
735:05 - disk type is five times faster when it
735:08 - comes to read iops per gigabyte than the
735:10 - balance pd as well as five times faster
735:14 - for the right iops per gigabyte and so
735:16 - the table here on the left shows the
735:19 - performance for the pd ssd and the table
735:22 - on the right shows the performance of
735:25 - both the standard pd and the balance pd
735:28 - and so here you can see the difference
735:30 - moving from the standard pd over to the
735:33 - ssd pd the read write throughput per
735:35 - instance stays the same from the
735:37 - standard pd all the way up to the ssd pd
735:41 - but where the ssd outperforms all the
735:43 - other ones is through the read write
735:45 - throughput per gigabyte it's one and a
735:48 - half times faster than the balance pd
735:50 - and four times faster than the standard
735:52 - pd and again you will also notice a drop
735:55 - in performance from the zonal option to
735:58 - the regional option and so this is the
736:00 - end of part one of this lesson as it
736:03 - started to get a little bit long and so
736:05 - whenever you're ready you can join me in
736:06 - part two where i will be starting
736:09 - immediately from the end of part one so
736:12 - you can complete this video and i will
736:14 - see you in the next
736:15 - [Music]
736:19 - welcome back this is part two of the
736:21 - persistent disks and local ssds lesson
736:24 - and we will be starting exactly where we
736:26 - left off in part one so with that being
736:29 - said let's dive in and so now that i've
736:32 - covered all the persistent disk types i
736:34 - wanted to move into discussing the
736:36 - characteristics of the local ssd local
736:39 - ssds are physically attached to the
736:42 - server that hosts your vm instance local
736:45 - ssds have higher throughput and lower
736:48 - latency than any of the available
736:51 - persistent disk options and again this
736:53 - is because it's physically attached and
736:56 - the data doesn't have to travel over the
736:58 - network now the crucial thing to know
737:00 - about local ssds is that the data you
737:03 - store on a local ssd persists only until
737:07 - the instance is stopped or deleted once
737:10 - the instance is stopped or deleted your
737:13 - data will be gone and there is no chance
737:15 - of getting it back now each local ssd is
737:19 - 375 gigabytes in size but you can attach
737:22 - a maximum of 24 local ssd partitions for
737:27 - a total of 9 terabytes per instance
737:30 - local ssds are designed to offer very
737:34 - high iops and very low latency and this
737:37 - is great for when you need a fast
737:39 - scratch disk or a cache and you don't
737:43 - want to use instance memory local ssds
737:46 - are also available in two flavors scuzzy
737:49 - and mvme now for those of you who are
737:52 - unaware
737:53 - scuzzy is an older protocol and made
737:56 - specifically for hard drives it also
737:58 - holds the limitation of having one queue
738:02 - for commands nvme on the other hand also
738:05 - known as non-volatile memory express is
738:09 - a newer protocol and is designed for the
738:12 - specific use of flash memory and
738:15 - designed to have up to 64 000 qs as well
738:19 - each of those queues in turn can have up
738:22 - to 64 000 commands running at the same
738:26 - time and thus making nvme infinitely
738:29 - faster now although nvme comes with
738:32 - these incredible speeds it does come at
738:35 - a cost and so when it comes to the
738:37 - caveats of local ssd although compute
738:40 - engine automatically encrypts your data
738:42 - when it's written to local ssd storage
738:45 - space you can't use customer supplied
738:47 - encryption keys with local ssds as well
738:51 - local ssds are only available for the n1
738:55 - n2 and compute optimized machine types
738:58 - now moving on to the performance of
739:01 - local ssds throughput is the same
739:03 - between scuzzy and nvme but the read
739:07 - write iops per instance is where nvme
739:10 - comes out on top and as you can see here
739:13 - the read iops per instance is a whopping
739:16 - two million four hundred thousand read
739:18 - iops per instance as well the right iops
739:21 - per instance is 1.2 million over the 800
739:25 - 000 for local ssd now before i end this
739:28 - lesson i wanted to cover a few points on
739:31 - performance scaling
739:32 - as it pertains to block storage on
739:35 - compute engine now persistent disk
739:38 - performance scales with the size of the
739:40 - disk and with the number of vcpus on
739:43 - your vm instance persistent disk
739:46 - performance scales linearly until it
739:49 - reaches either the limits of the volume
739:52 - or the limits of each compute engine
739:54 - instance whichever is lower now this may
739:57 - seem odd that the performance of your
739:59 - disk scales with cpu count but you have
740:02 - to remember persistent disks aren't
740:04 - physically attached to your vm they are
740:07 - independently located as such i o on a
740:10 - pd is a network operation and thus it
740:14 - takes cpu to do i o which means that
740:17 - smaller instances run out of cpu to
740:21 - perform disk io at higher rates so in
740:24 - order for you to get better performance
740:27 - you can increase the iops for your disk
740:29 - by resizing them to their maximum
740:32 - capacity but once that size has been
740:34 - reached you will have to increase the
740:36 - number of cpus on your instance in order
740:39 - to increase your disk performance a
740:42 - recommendation by google is that you
740:44 - have one available vcpu for every 2000
740:50 - to
740:51 - iops of expected traffic so to sum it up
740:55 - performance scales until it reaches
740:58 - either the limits of the disk or the
741:00 - limits of the vm instance to which the
741:03 - disk is attached the vm instance limits
741:06 - are determined by the machine type and
741:08 - the number of vcpus of the instance now
741:11 - if you want to get more granular with
741:13 - regards to disk performance i've
741:15 - included a few links in the lesson text
741:18 - that will give you some more insight but
741:20 - for most general purposes and for the
741:23 - exam remember that persistent disk
741:26 - performance is based on the total
741:28 - persistent disk capacity
741:30 - attached to an instance and the number
741:32 - of vcpus that the instance has and so
741:36 - that's pretty much all i wanted to cover
741:38 - when it comes to persistent disks and
741:41 - local ssds so you can now mark this
741:43 - lesson as complete and let's move on to
741:46 - the next one
741:51 - welcome back in this demo i'm going to
741:53 - be covering how to manage and interact
741:56 - with your disks on compute engine this
741:59 - demo is designed to give you both
742:00 - experience and understanding on working
742:03 - with persistent disks and how you would
742:06 - interact with them we're going to start
742:08 - the demo off by creating an instance
742:10 - we're then going to create a separate
742:12 - persistent disk and attach it to the
742:14 - instance we're going to then interact
742:16 - with the disk and then resize the disk
742:19 - while afterwards we will delete it and
742:22 - we're going to do this all by both using
742:24 - the console and the command line so with
742:27 - that being said let's dive in so here i
742:30 - am in the console i'm logged in as tony
742:32 - bowties gmail.com and i am in project
742:36 - bowtie inc and so the first thing we
742:38 - need to do to kick off this demo is to
742:41 - create an instance that we can attach
742:42 - our disk to but first i always like to
742:45 - make sure that i have a vpc to deploy my
742:48 - instance into with its corresponding
742:51 - default firewall rules so i'm going to
742:53 - head on over to the navigation menu
742:56 - and i'm going to go down to vpc network
743:00 - and as expected my default vpc has been
743:03 - created and just to make sure that i
743:05 - have all my necessary firewall rules i'm
743:08 - going to drill down into the vpc and
743:10 - head on over to firewall rules i'm going
743:12 - to click on firewall rules and the
743:15 - necessary firewall rule that i need for
743:17 - ssh is created and so i can go ahead and
743:21 - create my instance so i'm going to go
743:23 - back up to the navigation menu and i'm
743:25 - going to go over to compute engine so
743:27 - i'm going to go ahead and click on
743:28 - create and i'm going to name this
743:30 - instance bowtie dash instance and for
743:34 - the sake of this demo i'll add in a
743:36 - label here the key is going to be
743:38 - environment and the value will be
743:40 - testing i'm going to go down to the
743:42 - bottom click on save with regards to the
743:44 - region i'm going to select us east 1 and
743:47 - i'm going to keep the zone as the
743:49 - default for us east 1b and under machine
743:52 - type to keep things cost effective i'm
743:55 - going to use an e2 micro shared core
743:57 - machine and i'm going to scroll down to
744:00 - service account and under service
744:02 - account you want to select the set
744:03 - access for each api
744:05 - you want to scroll down to compute
744:07 - engine and here you want to select read
744:10 - write and this will give us the
744:11 - necessary permissions in order to
744:13 - interact with our disk that we will be
744:16 - creating later so i'm going to scroll
744:17 - down to the bottom here and i'm going to
744:19 - leave everything else set at its default
744:22 - and just before creating the instance
744:24 - please do remember you can always click
744:26 - on the command line link where you can
744:28 - get the gcloud command to create this
744:30 - instance through the command line i'm
744:33 - going to close this up and i'm going to
744:34 - simply click on create i'm just going to
744:37 - wait a few seconds here for my instance
744:39 - to come up okay and my instance is up
744:42 - and so now what we want to do is we want
744:44 - to create our new disk so i'm going to
744:46 - go over here to the left hand menu and
744:48 - i'm going to click on disks and as you
744:50 - can see here the disk for the instance
744:53 - that i had just created has 10 gigabytes
744:56 - in us east 1b and we want to leave that
744:58 - alone and we want to create our new disk
745:01 - so i'm going to go up to the top here
745:02 - and simply click on create disk
745:05 - and so for the name of the disk i'm
745:06 - going to call this disk new pd for
745:09 - persistent disk and i'm going to give it
745:11 - the same description i'm going to keep
745:13 - the type as standard persistent disk and
745:16 - for the region i want to select us east
745:19 - one i'm going to keep the zone as its
745:21 - default in us east 1b and as the disk is
745:24 - in us east 1b i'll be able to attach it
745:27 - to my instance and so just as a note
745:29 - here there is a selection where you can
745:32 - replicate this disk within the region if
745:35 - i click that off i've now changed this
745:37 - from a zonal persistent disk to a
745:39 - regional persistent disk and over here
745:42 - in zones it'll give me the option to
745:44 - select any two zones that i prefer and
745:47 - so if you're looking at creating some
745:48 - regional persistent disks these are the
745:51 - steps you would need to take in order to
745:53 - get it done in the console now in order
745:55 - to save on costs i'm going to keep this
745:57 - as a zonal persistent disk so i'm going
745:59 - to click on cancel i'm going to uncheck
746:01 - the option and make sure your region is
746:03 - still set at us east 1 and your zone is
746:06 - selected as us east 1b we're going to
746:09 - leave the snapshot schedule alone and
746:11 - i'll be diving into snapshot schedules
746:14 - in a later lesson i'm going to scroll
746:16 - down here to source type i'm going to
746:17 - keep it as blank disk and the size here
746:20 - is set at 500 gigabytes and we want to
746:23 - set it to 100 gigabytes but before we do
746:26 - that i wanted to bring your attention to
746:28 - the estimated performance here you can
746:30 - see the sustain random iops limits as
746:34 - well as the throughput limit and so
746:36 - depending on the size of the disk that
746:37 - you want to add these limits will change
746:40 - accordingly so if i change this to 100
746:43 - my sustained random iops limit on read
746:46 - went from 375 iops to 75 iops and so
746:51 - this is a great demonstration that the
746:53 - larger your disc the better your
746:55 - performance and so this is a great way
746:58 - to figure out on what your performance
747:00 - will be before you create your disk and
747:03 - i've also been prompted with a note here
747:05 - saying that because my disk is under 200
747:07 - gigabytes that i will have reduced
747:10 - performance and so for this demo that's
747:12 - okay i'm going to keep my encryption as
747:14 - the google manage key and under labels i
747:17 - will add environment as the key and
747:20 - value is testing
747:22 - and so now that i've entered all my
747:23 - options i'm going to simply click on
747:25 - create
747:27 - and i'm going to give it a few seconds
747:29 - and my new disk should be created okay
747:32 - and my new disk has been created and you
747:34 - can easily create this disk through the
747:36 - command line and i will be supplying
747:38 - that in the lesson text i merely want to
747:41 - go through the console setup so that you
747:43 - are aware of all the different options
747:45 - and so now that i've created my disk and
747:48 - i've created my instance i want to now
747:50 - log into my instance and attach this new
747:53 - disk so i'm going to go back to vm
747:55 - instances and here i want to ssh into
747:58 - the bowtie instance and i'm going to
748:00 - give it a few seconds here to connect
748:02 - and i'm going to zoom in for better
748:04 - viewing i'm going to clear my screen
748:07 - and so the first thing i want to do is i
748:08 - want to list all my block devices that
748:11 - are available to me on this instance and
748:13 - the linux command for that is ls blk
748:17 - and as you can see my boot disk has been
748:19 - mounted
748:20 - and is available to me and so now i want
748:22 - to attach the new disk that we just
748:24 - created and just as a note i could as
748:27 - easily have done this in the console but
748:29 - i wanted to give you an idea of what it
748:31 - would look like doing it from the
748:33 - command line and so i'm going to paste
748:35 - in the command to attach the disk which
748:38 - is gcloud compute instances attach dash
748:41 - disk the name of the instance which is
748:43 - bow tie dash instance along with the
748:46 - flag dash dash disk the disk name which
748:48 - is new pd and the zone of the disk using
748:51 - the zone flag with us east 1b so i'm
748:54 - going to go ahead and hit enter
748:56 - and no errors came up so i'm assuming
748:58 - that this had worked and so just to
749:00 - double check i'm gonna run the lsblk
749:03 - command again and success as you can see
749:06 - here my block device sdb has been
749:08 - attached to my instance and is available
749:11 - to me with the size of 100 gigabytes and
749:14 - so now i want to look at the state that
749:16 - this roblox device is in and so the
749:18 - command for that will be sudo
749:21 - file dash s followed by the path of the
749:23 - block device which is forward slash dev
749:26 - forward slash sdb i'm going to hit on
749:29 - enter and as you can see it is showing
749:31 - data which means that it is just a raw
749:34 - data device and so in order for me to
749:36 - interact with it i need to format the
749:38 - drive with a file system that the
749:40 - operating system will be able to
749:42 - interact with and so the command to
749:44 - format the drive would be sudo mkfs
749:48 - which is make file system i'm going to
749:50 - use ext4 as the file system minus
749:53 - capital f along with the path of the new
749:55 - disk so i'm going to hit on enter and no
749:58 - errors so i'm assuming that it was
750:00 - successful so just to verify i'm going
750:02 - to run the sudo file minus s command and
750:06 - as you can see here because the disk now
750:08 - has a file system i've been given the
750:10 - information with regards to this disk
750:12 - whereas before it was simply raw data
750:15 - and so now that we've created our disk
750:17 - and we've formatted our disk to a file
750:20 - system that the operating system is able
750:22 - to read we need to now mount the disk
750:25 - and so in order to do that we need to
750:27 - create a mount point so i'm going to
750:28 - first clear the screen and i'm going to
750:31 - run the command sudo mkdir and the new
750:34 - mount point i'm going to call it slash
750:37 - new pd i'm going to hit enter and now
750:39 - i'm going to mount the disk and the
750:41 - command for that is sudo mount the path
750:43 - for the block device which is forward
750:45 - slash dev forward slash sdb and then the
750:48 - mount point which is forward slash new
750:50 - pd i'm going to hit enter no errors so
750:53 - i'm assuming that it had worked but just
750:55 - to verify i'm going to run the command
750:58 - lsblk
751:00 - and success as you can see sdb has now
751:03 - been mounted as new pd and so now i can
751:07 - interact with this disk so the first
751:09 - thing i want to do is i want to change
751:11 - directories to this mount point i'm in
751:13 - now new pd i'm going to do an ls and so
751:16 - just as a note for those of you who are
751:18 - wondering the lost and found directory
751:21 - is found on each linux file system and
751:24 - this is designed to place orphaned or
751:26 - corrupted files or any corrupted bits of
751:29 - data from the file system to be placed
751:32 - here and so it's not something that you
751:34 - would interact with but always a good to
751:36 - know so i'm going to now create a file
751:38 - in new pd so i'm going to run the
751:40 - command sudo nano file a bow ties dot
751:43 - text so file a bow ties is the file that
751:46 - i'm going to create nano is my text
751:49 - editor and so i'm going to hit on enter
751:51 - and so in this file i'm going to type in
751:54 - bow ties are so classy
751:57 - because after all they are i'm going to
751:59 - hit ctrl o to save i'm going to hit
752:01 - enter to verify it and ctrl x to exit so
752:05 - if i do another ls i can see the file of
752:08 - bow ties has been created also by
752:10 - running the command df minus k i'll be
752:13 - able to see the file system here as well
752:15 - and so this is the end of part one of
752:17 - this demo it was getting a bit long so i
752:19 - decided to break it up this would be a
752:21 - great opportunity for you to get up have
752:24 - a stretch get yourself a coffee or tea
752:27 - and whenever you're ready you can join
752:28 - me in the next one where part two will
752:31 - be starting immediately from the end of
752:33 - part one
752:34 - [Music]
752:38 - welcome back this is part two of this
752:40 - demo and we're gonna continue
752:42 - immediately from the end of part one so
752:45 - with that being said let's dive in and
752:47 - so what i want to do now is i want to
752:50 - reboot the instance in order to
752:52 - demonstrate the mounting of this device
752:54 - and i'm going to do that by using the
752:56 - command sudo reboot it's going to
752:58 - disconnect me i'm going to click on
753:00 - close and i'm going to wait about a
753:01 - minute for it to reboot okay and it's
753:03 - been about a minute so i'm going to now
753:05 - ssh into my instance
753:09 - okay and here i am back again logged
753:11 - into my instance i'm going to quickly
753:13 - clear the screen and i'm going to run
753:15 - the lsblk command now what i wanted to
753:18 - demonstrate here is that although i
753:20 - mounted the new device it did not stay
753:23 - mounted through the reboot and this is
753:25 - because there is a configuration file in
753:27 - linux that points to which partitions
753:30 - get mounted automatically upon startup
753:32 - that i need to edit in order to make
753:34 - sure that this device is mounted every
753:36 - time the instance reboots and so in
753:38 - order to do that i need to edit a file
753:41 - called fstab and i'm going to have to
753:44 - add the unique identifier for this
753:46 - partition also known as the device sdb
753:49 - and this will mount the partition
753:50 - automatically every time there happens
753:53 - to be a reboot so in order to do that
753:55 - i'm going to run the command sudo blk id
753:59 - and the path of the block device forward
754:01 - slash dev forward slash sdb i'm going to
754:04 - hit on enter and here is the identifier
754:06 - also known as the uuid that i need to
754:09 - append to the fstab file so i'm going to
754:12 - copy the uuid
754:14 - and i'm going to use the command
754:16 - sudo nano
754:18 - etc
754:19 - fs tab and i'm going to hit on enter and
754:21 - here you will find the uuid for your
754:24 - other partitions and so you're going to
754:26 - be appending a line here right at the
754:28 - end so i'm going to move my cursor down
754:29 - here i'm going to type in uuid equals
754:33 - and then the uuid that i had copied
754:35 - earlier the amount point which is going
754:37 - to be forward slash new pd the type of
754:40 - file system which is ext4 along with
754:43 - defaults
754:45 - comma no fail i'm going to hit control o
754:47 - to save hit enter to verify and control
754:50 - x to exit and so now i'm going to mount
754:52 - this device by running the command sudo
754:55 - mount dash a and hit enter and this
754:57 - command will mount all the partitions
754:59 - that are available in the fstab file and
755:02 - so when i run a lsblk
755:05 - i can see here that my block device sdb
755:09 - is now mounted on forward slash new pd
755:12 - now i know this may be a refresher for
755:14 - some but this is a perfect demonstration
755:16 - of the tasks that need to be done when
755:19 - creating and attaching a new disk to an
755:22 - instance and is a common task for many
755:25 - working on linux instances and working
755:27 - in cloud this can definitely be scripted
755:30 - but i wanted to show you the steps that
755:32 - need to be taken in order to get a new
755:34 - disk in a usable state okay so great we
755:38 - have created a new disk we had attached
755:41 - the disk created a file system and had
755:44 - mounted the disk along with editing the
755:46 - configuration file to make sure that the
755:48 - device mounts whenever the instance
755:51 - starts up so now that we've done all
755:53 - that i wanted to demonstrate resizing
755:55 - this disk from 100 gigabytes to 150
755:59 - gigabytes and so just to show you where
756:01 - it is in the console i'm going to
756:03 - quickly go back to my console tab and so
756:05 - here i'm going to go to the left hand
756:07 - menu i'm going to click on disks i'm
756:09 - going to drill down into new pd and at
756:11 - the top i'm going to click on edit and
756:13 - so here i'm able to adjust the disk
756:15 - space size and simply click on save not
756:18 - much that i really need to do here but i
756:20 - did want to show you how to do this in
756:22 - the command line so i'm going to go back
756:24 - to the tab of my instance and i'm going
756:26 - to quickly clear the screen and i'm
756:29 - going to paste in the command gcloud
756:31 - compute disks
756:32 - resize the name of the disk which is new
756:35 - pd and the new size in gigabytes using
756:38 - the dash dash size flag 150 which is the
756:42 - new size of the disc along with the dash
756:45 - dash zone flag of us east 1b i'm going
756:48 - to hit enter it's going to ask me if i
756:50 - want to do this as this is not
756:52 - reversible and please remember when you
756:54 - resize a disk you can only make it
756:56 - bigger and never smaller so i'm going to
756:59 - hit y to continue
757:01 - and it took a few seconds there but it
757:03 - was successful so if i run a df minus k
757:06 - you can see here that i only have 100
757:09 - gigabytes available to me and this is
757:11 - because i have to extend the file system
757:14 - on the disk so i've made the disk larger
757:16 - but i haven't allocated those raw blocks
757:18 - to the file system so in order for the
757:20 - file system to see those unallocated
757:23 - blocks that's available to it i need to
757:25 - run another command so i'm going to
757:27 - quickly clear my screen again
757:29 - and i'm going to run the command sudo
757:31 - resize to fs along with the block device
757:34 - i'm going to hit enter and as you can
757:36 - see it was successful showing the old
757:38 - blocks as 13 and the new blocks as 19.
757:42 - so if i run a df minus k i can now see
757:45 - my 150 gigabytes that's available to me
757:48 - and so just to demonstrate after
757:50 - resizing the disk along with mounting
757:53 - and then remounting the disk that the
757:55 - file that i've created still exists i'm
757:57 - going to run an ls minus al but first i
758:00 - will need to
758:01 - change directories into new pd clear my
758:04 - screen and run an ls and phyla bow ties
758:07 - is still there and so this is a great
758:09 - example demonstrating how the data on
758:12 - persistent disks persist through the
758:15 - lifetime of a disk even when mounting
758:18 - unmounting rebooting and resizing and so
758:22 - as you can see we've done a lot of work
758:24 - here and so just as a recap where we've
758:26 - created a new disk we attached this disk
758:29 - to an instance we formatted the disk
758:31 - into an ext4 file system we've mounted
758:35 - this disk we've written a file to it
758:37 - added its unique identifier to the
758:39 - configuration file so that it mounts on
758:42 - startup and then we've resized the disk
758:44 - along with extending the file system on
758:46 - the disk and so this is the end of the
758:48 - demo and i wanted to congratulate you on
758:51 - making it to the end and i hope this
758:53 - demo has been extremely useful and again
758:56 - fantastic job on your part now before
758:58 - you go i wanted to quickly walk through
759:01 - the steps of deleting all the resources
759:03 - you've created and so the first thing
759:05 - that i want to do is delete the disk
759:07 - that was created for this demo and so
759:10 - before i can delete the disk i'm going
759:11 - to first detach the disk from the
759:14 - instance and the easiest way to do that
759:16 - is through the command line so i'm going
759:17 - to quickly clear my screen and so i'm
759:19 - going to show you how to detach the disk
759:22 - from the instance and so i'm going to
759:24 - paste in this command gcloud compute
759:26 - instances detach disk the instance name
759:30 - which is bow tie dash instance along
759:32 - with the disc with the flag dash dash
759:34 - disc the name of the disc which is new
759:36 - pd along with the zone i'm going to hit
759:38 - enter
759:39 - and it's been successfully detached and
759:42 - so now that it's detached i can actually
759:44 - delete the disk and so i'm going to head
759:46 - on over back to the console and i'm
759:48 - going to go ahead and delete the new pd
759:50 - disk i'm going to click on delete i'm
759:52 - going to get a prompt asking me if i'm
759:54 - sure yes i am if i go back to the main
759:57 - menu for my disks and this should just
759:59 - take a moment and once it's deleted you
760:01 - will no longer see it here and i'm going
760:03 - to go back over to vm instances and i'm
760:06 - going to delete this as well
760:10 - and so there's no need to delete your
760:11 - default vpc unless you'd like to
760:13 - recreate it again but don't worry for
760:16 - those who decide to keep it you will not
760:18 - be charged for your vpc as we will be
760:20 - using it in the next demo and so that's
760:23 - pretty much all i wanted to cover when
760:25 - it comes to managing disks with compute
760:27 - engine so you can now mark this as
760:29 - complete and let's move on to the next
760:32 - one
760:33 - [Music]
760:37 - welcome back in this lesson i'll be
760:39 - discussing persistent disk snapshots now
760:42 - snapshots are a great way to backup data
760:45 - from any running or stopped instances
760:48 - from unexpected data loss
760:50 - snapshots are also a great strategy for
760:53 - use in a backup plan for any and all
760:56 - instances no matter where they are
760:58 - located and so as cloud engineers and
761:01 - architects this is a great tool for
761:04 - achieving the greatest uptime for your
761:06 - instances so diving right into it
761:09 - snapshots as i mentioned before are a
761:12 - great way for both backing up and
761:14 - restoring the data of your persistent
761:17 - disks you can create snapshots from
761:19 - disks even while they are attached to
761:21 - running instances snapshots are global
761:24 - resources so any snapshot is accessible
761:27 - by any resource within the same project
761:30 - you can also share snapshots across
761:33 - projects as well snapshots also support
761:36 - both zonal and regional persistent disks
761:40 - snapshots are incremental and
761:42 - automatically compressed so you can
761:44 - create regular snapshots on a persistent
761:47 - disk
761:48 - faster and at a much lower cost than if
761:50 - you regularly created a full image of a
761:53 - disk now when you create a snapshot you
761:56 - have the option of choosing a storage
761:58 - location snapshots are stored in cloud
762:01 - storage
762:02 - and can be stored in either a
762:04 - multi-regional location or a regional
762:07 - cloud storage bucket a multi-regional
762:09 - storage location provides higher
762:12 - availability but will drive up costs
762:15 - please be aware that the location of a
762:17 - snapshot affects its availability and
762:20 - can incur
762:22 - networking costs when creating the
762:24 - snapshot or restoring it to a new disk
762:26 - if you do not specify storage location
762:29 - for a snapshot google cloud uses the
762:31 - default location which stores your
762:34 - snapshot in a cloud storage
762:36 - multi-regional location closest to the
762:39 - region of your source disk if you store
762:41 - your snapshot in the same region as your
762:43 - source disk there is no network charge
762:46 - when you access that snapshot from the
762:48 - same region if you access the snapshot
762:50 - from a different region you will incur a
762:53 - network cost compute engine stores
762:55 - multiple copies of each snapshot across
762:58 - multiple locations as well you cannot
763:01 - change the storage location of an
763:03 - existing snapshot once a snapshot has
763:06 - been taken it can be used to create a
763:08 - new disk in any region and zone
763:11 - regardless of the storage location of
763:14 - the snapshot now as i explained earlier
763:17 - snapshots are incremental and i wanted
763:19 - to take a moment to dive into that for
763:21 - just a minute so when creating snapshots
763:24 - the first successful snapshot of a
763:27 - persistent disk is a full snapshot that
763:30 - contains all the data on the persistent
763:32 - disk the second snapshot only contains
763:36 - any new data or modify data since the
763:39 - first snapshot data that hasn't changed
763:41 - since snapshot 1 isn't included instead
763:44 - snapshot 2 contains references to
763:47 - snapshot 1 for any unchanged data as
763:51 - shown here snapshot 3 contains any new
763:54 - or changed data since snapshot 2 but
763:58 - won't contain any unchanged data from
764:01 - snapshot 1 or 2. instead snapshot 3
764:04 - contains references to blocks in
764:07 - snapshot 1 and snapshot 2 for any
764:10 - unchanged data this repeats for all
764:13 - subsequent snapshots of the persistent
764:16 - disk snapshots are always created based
764:19 - on the last successful snapshot taken
764:22 - and so now you're probably wondering
764:24 - what happens when you decide to delete a
764:26 - snapshot are they dependent on each
764:28 - other well when you delete a snapshot
764:31 - compute engine immediately marks the
764:34 - snapshot as deleted in the system if the
764:36 - snapshot has no dependent snapshots it
764:39 - is deleted outright however if the
764:41 - snapshot does have dependent snapshots
764:44 - then there are some steps that happen
764:46 - behind the scenes so shown here in this
764:49 - diagram snapshot 2 is deleted the next
764:52 - snapshot from the full snapshot no
764:54 - longer references the snapshot for
764:57 - deletion in this example snapshot 1 then
765:00 - becomes the reference for snapshot 3 and
765:03 - any data that is required for restoring
765:06 - other snapshots is moved into the next
765:09 - snapshot increasing its size shown here
765:13 - blocks that were unique to snapshot 2
765:15 - are moved to snapshot 3 and the size of
765:18 - snapshot 3 increases any data that is
765:22 - not required for restoring other
765:24 - snapshots is deleted so in this case
765:27 - blocks that are already in snapshot 3
765:30 - are deleted from snapshot 2 and the size
765:33 - of all snapshots are lower now because
765:35 - subsequent snapshots might require
765:38 - information stored in a previous
765:40 - snapshot
765:41 - please be aware that deleting a snapshot
765:44 - does not necessarily delete all the data
765:47 - on the snapshot if you're looking to
765:49 - make sure that your data has indeed been
765:51 - deleted from your snapshots you should
765:54 - delete all snapshots if your disk has a
765:57 - snapshot schedule you must detach the
766:00 - snapshot schedule from the disk before
766:02 - you can delete the schedule
766:04 - removing the snapshot schedule from the
766:06 - disk prevents further snapshot activity
766:09 - from occurring
766:11 - now touching on the topic of scheduled
766:13 - snapshots by far the best way to backup
766:16 - your data on compute engine is to use
766:19 - scheduled snapshots this way you will
766:22 - never have to worry about manually
766:24 - creating snapshots or even worry about
766:26 - using other tools to kick off those
766:29 - snapshots you can simply use this
766:31 - built-in tool by google which is why
766:34 - snapshot schedules are considered best
766:36 - practice to backup any compute engine
766:39 - persistent disks now in order to create
766:42 - any snapshot schedules you must create
766:44 - your snapshot schedule in the same
766:47 - region where your persistent disk
766:49 - resides now there are two ways to create
766:52 - a snapshot schedule the first one is to
766:54 - create a snapshot schedule and then
766:57 - attach it to an existing persistent disk
767:00 - the other way is to create a new
767:02 - persistent disk with a snapshot schedule
767:05 - you also have the option of setting up a
767:08 - snapshot retention policy that defines
767:10 - how long you want to keep your snapshots
767:13 - some options when creating snapshot
767:15 - schedules are both retention policies
767:18 - and source disk deletion rules now if
767:20 - you choose to set up a snapshot
767:22 - retention policy you must do it as part
767:25 - of your snapshot schedule when you
767:27 - create a snapshot schedule is when you
767:29 - can also set a source disk deletion rule
767:32 - the source disk deletion rule controls
767:35 - what happens to your snapshots if the
767:37 - source disk is deleted now a few caveats
767:40 - here on the scheduled snapshots is that
767:42 - a persistent disk can only have one
767:45 - snapshot schedule attached to it at a
767:48 - time also you cannot delete a snapshot
767:51 - schedule if it is attached to a disk you
767:54 - must detach the schedule from all disks
767:57 - then delete the schedule as well after
767:59 - you create a snapshot schedule you
768:01 - cannot edit it to update a snapshot
768:04 - schedule you must delete it and create a
768:07 - new one now before i end this lesson i
768:10 - wanted to touch on managing snapshots
768:12 - for just a minute so when managing
768:14 - snapshots there's a few things to
768:16 - remember in order to use snapshots to
768:18 - manage your data efficiently you can
768:21 - snapshot your disks at most once every
768:24 - 10 minutes you are unable to snapshot
768:26 - your disks at intervals less than 10
768:29 - minutes so please keep that in mind when
768:32 - creating your schedules also you should
768:34 - create snapshots on a regular schedule
768:37 - to minimize data loss if there was an
768:39 - unexpected failure if you have existing
768:42 - snapshots of a persistent disk the
768:44 - system automatically uses them as a
768:46 - baseline for any subsequent snapshots
768:50 - that you create from that same disk
768:52 - so in order to improve performance you
768:55 - can eliminate excessive snapshots by
768:58 - creating an image and reusing it using
769:01 - this method would not only be ideal for
769:03 - storage and management of snapshots but
769:06 - also help to reduce costs and if you
769:09 - schedule regular snapshots for your
769:11 - persistent disks you can reduce the time
769:14 - that it takes to complete each snapshot
769:16 - by creating them during off-peak hours
769:19 - when possible and lastly for those of
769:21 - you who use windows for most situations
769:24 - you can use the volume shadow copy
769:27 - service to take snapshots of persistent
769:30 - disks that are attached to windows
769:32 - instances you can create vss snapshots
769:36 - without having to stop the instance or
769:38 - detach the persistent disk and so that's
769:41 - pretty much all i wanted to cover when
769:43 - it comes to the theory of persistent
769:45 - disk snapshots their schedules and how
769:48 - to manage them in the next lesson i'll
769:50 - be doing a hands-on demo
769:52 - demonstrating snapshots and putting this
769:55 - theory into practice and get a feel for
769:58 - how snapshots work and how they can be
770:00 - applied to persistent disks so you can
770:03 - now mark this lesson as complete and
770:05 - whenever you're ready join me in the
770:06 - console
770:07 - [Music]
770:11 - welcome back
770:12 - in this demonstration we're going to
770:14 - dive into snapshots and snapshot
770:17 - schedules this demo will give you the
770:19 - hands-on knowledge you need to create
770:21 - and delete snapshots along with how to
770:24 - manage snapshot schedules we're going to
770:27 - start the demo off by creating an
770:29 - instance we're going to interact with it
770:31 - and then take a snapshot of the disk
770:33 - we're going to then create another
770:35 - instance from the snapshot and then
770:37 - create some snapshot schedules for both
770:39 - of these instances by using both the
770:42 - console and the command line so there's
770:44 - a lot to do here so with that being said
770:47 - let's dive in and so i'm currently
770:49 - logged in as tony bowties gmail.com as
770:53 - well i'm in project bowtie inc so the
770:56 - first thing that we need to do to kick
770:57 - off this demo is to create an instance
771:00 - but first as always i like to make sure
771:03 - that i have a vpc to deploy my instance
771:05 - into with its corresponding default
771:08 - firewall rules and so i'm going to head
771:10 - on over to the navigation menu and
771:12 - scroll down to vpc network
771:17 - and because i didn't delete my default
771:19 - vpc from the last demo i still have it
771:21 - here i'm just going to drill down and
771:24 - make sure that i have my firewall rules
771:26 - i'm gonna go over to firewall rules and
771:28 - as expected the ssh firewall rule that i
771:31 - need has already been created and so now
771:34 - that i have everything in order i'm
771:35 - gonna go back over to the navigation
771:37 - menu and head on over to compute engine
771:40 - to create my instance now i figure for
771:43 - this demo i'd switch it up a little bit
771:45 - and create the instance by the command
771:47 - line so i'm going to head on over to
771:49 - cloud shell i'm going to open that up
771:51 - and it took a minute to provision and so
771:53 - what i'm going to do now is i'm going to
771:55 - open it up in a new tab i'm going to
771:57 - zoom in for better viewing and i'm going
771:59 - to paste in my command to create my
772:02 - instance and this gcloud command to
772:04 - create these instances will be available
772:07 - in the github repository and you will
772:09 - find all the instructions and the
772:11 - commands
772:12 - under managing snapshots in compute
772:14 - engine so i'm going to hit enter
772:17 - and you may get a prompt to authorize
772:19 - this api call and i'm going to click on
772:22 - authorize
772:23 - and success our instance has been
772:25 - created and is up and running and so now
772:28 - what i want to do is ssh into the
772:30 - instance and so i'm just going to run
772:32 - the command from here which is gcloud
772:34 - compute ssh dash dash zone the zone that
772:38 - i'm in which is used 1b and the instance
772:40 - which is bowtie dash instance i'm going
772:43 - to hit enter it's going to prompt me if
772:45 - i want to continue i'm going to say yes
772:47 - and i'm going to enter my passphrase and
772:50 - enter it again
772:51 - it's going to update my metadata and
772:53 - it's going to ask me again for my
772:55 - passphrase and i'm in so i'm going to
772:57 - just quickly clear my screen and so the
772:59 - first thing i want to do is i want to
773:01 - verify the name of my instance so i'm
773:04 - going to type in the command hostname
773:06 - and as expected bowtie dash instance
773:09 - shows up and so now i want to create a
773:11 - text file and so i'm going to run the
773:13 - command
773:14 - sudo nano file a
773:17 - text i'm going to hit enter and it's
773:19 - going to open up my nano text editor and
773:21 - you can enter a message of any kind that
773:23 - you'd like for me i'm going to enter
773:26 - more bow tie needed because you can
773:28 - never get enough bow ties i'm going to
773:30 - hit ctrl o to save press enter to verify
773:33 - the file name to write and then ctrl x
773:35 - to exit i'm going to run the command ls
773:38 - space minus al to list my files so i can
773:42 - verify that my file has been created and
773:44 - as you can see here file a bowties.txt
773:47 - has been created and so now that i've
773:49 - created my instance and i've written a
773:51 - file to disk i'm going to now head on
773:53 - over to the console and take a snapshot
773:55 - of this disk and because my session was
773:58 - transferred to another tab i can now
774:00 - close the terminal and you want to head
774:02 - over to the left-hand menu and go to
774:04 - disks and so now i want to show you two
774:07 - ways on how you can create this snapshot
774:09 - the first one is going to disks and
774:12 - choosing the disk that you want for me
774:14 - it's bowtie instance and under actions
774:17 - i'm going to click on the hamburger menu
774:19 - and here i can create snapshot and this
774:21 - will bring me straight to my snapshot
774:23 - menu but for this demo i'm going to go
774:25 - over to the left hand menu and i'm going
774:27 - to click on snapshots and here i'm going
774:30 - to click on create snapshot and so for
774:32 - the name of the snapshot i'm going to
774:34 - type in bowtie snapshot and i'm going to
774:37 - use the same for the description moving
774:39 - down on the source disk the only one
774:41 - that i can select is bow tie instance
774:43 - and that's the one that i want anyways
774:46 - so i'm going to click on that the
774:47 - location in order to cut down on costs
774:49 - we don't need multi-regional we're going
774:51 - to just select regional and if you
774:53 - select on the location i'm able to
774:56 - select any other locations like tokyo
774:59 - and i can create my snapshot in tokyo
775:01 - but i want to keep my snapshot in the
775:03 - same region so i'm going to go back and
775:05 - select us east one where it is based on
775:08 - the source disk location and i'm going
775:10 - to add a label here with the key
775:12 - environment and the value of testing i'm
775:15 - going to leave my encryption type as
775:16 - google managed and i'm going to simply
775:18 - click on create and this will create a
775:21 - snapshot of the boot disk on bow tie
775:23 - instance and that took about a minute
775:25 - there and so just as a note if you have
775:28 - any bigger discs they will take a little
775:30 - bit longer to snapshot okay and now that
775:33 - i've created my snapshot i'm going to go
775:35 - back up to vm instances and i'm going to
775:37 - create a new instance from that snapshot
775:40 - and so i'm going to name this instance
775:42 - bowtie dash instance dash 2 and i'm
775:44 - going to give this a label i'm going to
775:46 - add a label here the key of environment
775:48 - and the value of testing and hit save
775:51 - the region is going to be used 1 and you
775:54 - can leave the zone as its default as us
775:57 - east 1b and under machine type you can
775:59 - select the e2 micro and you want to go
776:02 - down to boot disk and select the change
776:04 - button and here i'm going to select
776:07 - snapshots instead of using a public
776:09 - image so i'm going to click on snapshots
776:11 - and if i select the snapshot drop down
776:13 - menu i will see here my bowtie snapshot
776:17 - so i'm going to select this i'm going to
776:18 - leave the rest as default and i'm going
776:20 - to go down to select and i'm going to
776:22 - leave everything else as its default and
776:24 - i'm going to click on create i'm going
776:26 - to just give it a minute here so bowtie
776:28 - instance 2 can be created okay and it
776:31 - took a minute there so now i'm going to
776:33 - ssh into this instance
776:35 - and i'm going to zoom in for better
776:37 - viewing and even though i know the
776:39 - instance is named bowtie.instance2
776:43 - i'm still going to run the hostname
776:44 - command and as expected the same name
776:47 - pops up but what i was really curious
776:49 - about is if i run the command ls space
776:52 - dash al i can see here my file of file
776:56 - of bowties.text
776:58 - and if i cat the file
776:59 - [Music]
777:01 - i'll be able to see the text that i
777:03 - inputted into that file and so although
777:05 - it was only one file and a text file at
777:08 - that i was able to verify that my
777:11 - snapshot had worked as there will be
777:13 - times where your snapshot can get
777:14 - corrupted and so doing some various spot
777:17 - checks on your snapshots is some good
777:19 - common practice and so now i want to
777:21 - create a snapshot schedule for both of
777:24 - these instances and so i'm going to go
777:26 - back to the console and on the left hand
777:28 - menu i'm going to head down to snapshots
777:31 - and if i go over to snapshot schedules
777:33 - you can see that i have no snapshot
777:35 - schedules so let's go ahead and create a
777:37 - new one by clicking on create snapshot
777:40 - schedule and so as mentioned in the last
777:42 - lesson we need to create this schedule
777:44 - first before we can attach it to a disk
777:47 - and so i'm going to name this snapshot
777:49 - schedule as bow tie dash disk schedule
777:53 - i'm going to use the same for the
777:54 - description the region i'm going to
777:56 - select it as us east one and i'm going
777:58 - to keep the snapshot location as
778:00 - regional under us east one you scroll
778:04 - down here and under schedule options you
778:06 - can leave the schedule frequency as
778:08 - daily and just as a note for start time
778:12 - this time is measured in utc so please
778:15 - remember this when you're creating your
778:17 - schedule in your specific time zone and
778:19 - so i'm going to put the start time as o
778:22 - 600 and this will be 1 am eastern
778:25 - standard time as backups are always best
778:28 - done when there is the least amount of
778:30 - activity and i'm going to keep the auto
778:32 - delete snapshots after 14 days i'm going
778:35 - to keep the deletion rule as keep
778:36 - snapshots as well i can enable the
778:39 - volume shadow copy service for windows
778:42 - but since we're running linux i don't
778:44 - need to enable this and since we labeled
778:46 - everything else i might as well give
778:48 - this a label i'm going to use the key as
778:50 - environment and the value of testing and
778:52 - once you've filled everything out then
778:54 - you can simply click on create and it
778:56 - took a minute there but the schedule was
778:58 - created and so now that i have my
779:00 - snapshot schedule i need to attach it to
779:03 - a disk so i'm going to head on over to
779:05 - the left hand menu and click on disks
779:07 - and here i'm going to drill down into
779:09 - bow tie instance i'm going to go up to
779:11 - the top and click on edit and under
779:13 - snapshot schedule i'm going to click on
779:15 - the drop down and here i will find bow
779:18 - tie disk schedule i'm going to select
779:20 - that i'm going to click on save and so
779:22 - now that i have my snapshot schedule
779:25 - attached to my disk for the bowtie
779:28 - instance instance i now want to create a
779:30 - snapshot schedule for my other instance
779:33 - and so instead of using the console i'm
779:35 - going to go ahead and do it through the
779:37 - command line so i'm going to go up to
779:38 - the top to my open shell and i'm going
779:41 - to quickly clear the screen and so in
779:43 - order to create my schedule i'm going to
779:45 - run this command gcloud compute resource
779:48 - policies create snapshot schedule the
779:51 - name of the snapshot schedule which is
779:53 - bow tie disk schedule 2 the region the
779:56 - maximum retention days the retention
779:59 - policy and the schedule followed by the
780:01 - storage location and like i said before
780:04 - these commands you will find in the
780:06 - github repository so i'm going to go
780:08 - ahead and hit enter
780:10 - and so i wanted to leave this error in
780:12 - here to show you that i needed the
780:14 - proper permissions in order to create
780:16 - this snapshot schedule a great reminder
780:19 - to always check if you have the right
780:21 - role for the task at hand and so i have
780:24 - two options i can either change users
780:27 - from my service account user to tony
780:29 - bowtie or i can simply head on over to
780:32 - my instance and edit the service account
780:34 - permissions and so the easiest way to do
780:36 - it would be to just switch users and so
780:39 - i'm going to go ahead and do that so i'm
780:41 - going to go ahead and run the command
780:43 - gcloud auth login and remember that this
780:46 - is something that you don't have to do i
780:48 - merely wanted to show you that you
780:50 - require the proper permissions on
780:53 - creation of specific resources okay and
780:56 - i quickly went through the
780:57 - authentication process i'm gonna just
780:59 - clear my screen and i'm going to go
781:00 - ahead and run the command again
781:03 - and as expected the snapshot schedule
781:06 - was created with no errors and so now
781:08 - that my schedule has been created i can
781:10 - now attach it to the disk so i'm going
781:12 - to run the command gcloud compute disks
781:15 - add resource policies the instance name
781:18 - which is bowtie instance 2 and the
781:20 - resource policy which is the snapshot
781:22 - schedule named as bowtie disk schedule 2
781:25 - in the zone of us east 1b i'm going to
781:28 - hit enter
781:29 - and success and so just to verify that
781:32 - the snapshot schedule has been attached
781:34 - to my disk i'm going to go back to the
781:36 - console i'm going to head back on over
781:38 - to the main page of disks i'm going to
781:41 - drill down into bow tie instance 2 and
781:43 - here it is the snapshot schedule has
781:46 - been attached and so i want to
781:47 - congratulate you on making it to the end
781:50 - of this demo and i hope this demo has
781:52 - been useful as snapshots in the role of
781:55 - an engineer is a common task that can
781:58 - save you from any data loss once set
782:01 - into place and so just as a recap you've
782:03 - created an instance you created a file
782:06 - on that instance and then you've created
782:08 - a snapshot of the disk of that instance
782:11 - and used it to create another instance
782:13 - you then verified the snapshot and then
782:15 - created a snapshot schedule for both
782:18 - boot disks of the instances using the
782:21 - console and the command line well done
782:23 - on another great job now before you go i
782:26 - wanted to take a moment to clean up any
782:28 - resources we've used so we don't
782:30 - accumulate any costs and so the first
782:33 - thing we want to do is we want to detach
782:35 - the snapshot schedules from the disks
782:38 - and so since we're in bow tie instance 2
782:40 - i'm going to go ahead and click on edit
782:42 - under snapshot schedule i'm going to
782:44 - select the no schedule hit save and i'm
782:47 - going to do the same thing with my other
782:49 - disk
782:53 - now i'm going to head back on over to
782:55 - snapshots i'm going to delete this
782:57 - snapshot and i'm going to head back on
782:59 - over to snapshot schedules i'm going to
783:02 - select all the snapshot schedules and
783:04 - i'm going to click on delete
783:07 - and now that everything's cleaned up
783:09 - with regards to snapshots and snapshot
783:11 - schedules i can now go over to vm
783:14 - instances and delete the instances
783:17 - i'm going to select them all and simply
783:19 - click on delete
783:22 - and so that's pretty much all i wanted
783:24 - to cover in this demo when it comes to
783:26 - snapshots and snapshot schedules
783:29 - so you can now mark this as complete and
783:31 - let's move on to the next one
783:37 - welcome back in this lesson we're going
783:40 - to switch gears and take an automated
783:42 - approach to deployment by diving into
783:45 - google's tool for infrastructure as code
783:47 - called deployment manager now deployment
783:50 - manager allows you to deploy
783:52 - update and tear down resources from
783:54 - within google cloud using yaml jinja and
783:59 - python code templates it allows you to
784:01 - automate the deployment of all the
784:03 - resources that are available in google
784:05 - cloud and deploy it in a fast easy and
784:09 - repeatable way for consistency and
784:12 - efficiency in this lesson we're going to
784:14 - explore the architecture of deployment
784:16 - manager and dive into all the different
784:19 - components that gives it its flexibility
784:22 - and the features that make this tool an
784:24 - easy solution for deploying complex
784:27 - environments so with that being said
784:30 - let's dive in
784:31 - now breaking down the components that i
784:33 - mentioned earlier i wanted to start off
784:36 - with the first component being the
784:38 - configuration now a configuration
784:40 - defines the structure of your deployment
784:43 - as you must specify a configuration to
784:46 - create a deployment a configuration
784:48 - describes all the resources you want for
784:51 - a single deployment and is written in
784:54 - yaml syntax that lists each of the
784:56 - resources you want to create and its
784:58 - respective resource properties a
785:00 - configuration must contain a resources
785:03 - section followed by the list of
785:05 - resources to create and so each resource
785:08 - must contain these three components the
785:12 - name the type and properties without
785:15 - these three components a deployment will
785:17 - not instantiate and so i wanted to take
785:19 - a moment to go over these three
785:21 - components in a bit of depth so the
785:24 - first component of the configuration is
785:26 - the name and the name is a user defined
785:29 - string to identify this resource and can
785:32 - be anything you choose from names like
785:35 - instance one my-vm
785:38 - bowtie dash instance and you can even go
785:41 - as far to use larks dash instance dash
785:45 - don't dash touch and the syntax can be
785:48 - found here and must not contain any
785:51 - spaces or invalid characters next
785:53 - component in a configuration is type and
785:56 - there are a couple of different types
785:58 - that you can choose from a type can
786:00 - represent a single api source known as a
786:04 - base type
786:05 - or a set of resources known as a
786:07 - composite type and either one of these
786:10 - can be used to create part of your
786:12 - deployment the type of the resource
786:14 - being deployed here in this diagram is
786:16 - shown as a base type of
786:18 - compute.v1.instance
786:21 - and there are many other api resources
786:23 - that can be used such as compute.v1.disk
786:27 - app engine dot v1 as well as
786:31 - bigquery.v2 and the syntax is shown here
786:34 - as api dot version dot resource now a
786:38 - composite type contains one or more
786:40 - templates that are pre-configured to
786:43 - work together these templates expand to
786:45 - a set of base types when deployed in a
786:48 - deployment composite types are
786:50 - essentially hosted templates that you
786:53 - can add to deployment manager the syntax
786:55 - is shown here as gcp dash types forward
786:59 - slash provider colon resource and to
787:02 - give you an example of what a composite
787:04 - type looks like
787:05 - here is shown the creation of a reserved
787:08 - ip address using the compute engine v1
787:11 - api and you could also use composite
787:14 - types with other apis in the same way
787:17 - such as gcp dash types forward slash app
787:21 - engine dash v1 colon apps or bigquery
787:25 - v2 colon data sets and for the last
787:29 - component in a configuration is
787:31 - properties and this is the parameters
787:34 - for the resource type this includes all
787:36 - the parameters you see here in this
787:38 - example including the zone
787:41 - machine type the type of disk along with
787:43 - its parameters pretty much everything
787:46 - that gives detail on the resource type
787:48 - now just as a note they must match the
787:51 - properties for this type so what do i
787:53 - mean by this so let's say you entered a
787:56 - zone but that particular zone doesn't
787:58 - exist or that compute engine machine
788:00 - type doesn't exist in that zone you will
788:02 - end up getting an error as deployment
788:05 - manager will not be able to parse this
788:07 - configuration and thus failing
788:09 - deployment so make sure when you add
788:11 - your properties that they match those of
788:14 - the resource now a configuration can
788:17 - contain templates which are essentially
788:19 - parts of the configuration file that
788:22 - have been abstracted into individual
788:24 - building blocks a template is a separate
788:27 - file that is imported and used as a type
788:30 - in a configuration and you can use as
788:32 - many templates as you want in a
788:34 - configuration and allow you to separate
788:36 - your configuration out into different
788:39 - pieces that you can use and reuse across
788:42 - different deployments templates can be
788:44 - as generalized or specific as you need
788:48 - and they also allow you to take
788:49 - advantage of features like template
788:51 - properties environment variables and
788:54 - modules to create dynamic configuration
788:57 - as shown here templates can be written
789:00 - in a couple of different ways they can
789:02 - be written in either ginger 2.1 or
789:05 - python 3. the example shown on the left
789:08 - has been written in ginger and is very
789:11 - similar to the yaml syntax so if you're
789:13 - familiar with yaml this might be better
789:16 - for you the example on the right has
789:18 - been written in python and is pretty
789:20 - amazing as you can take advantage of
789:23 - programmatically generating parts of
789:25 - your templates if you are familiar with
789:27 - python this might be a better format for
789:30 - you now one of the advantages of using
789:32 - templates is the ability to create and
789:35 - define custom template properties
789:38 - template properties are arbitrary
789:40 - variables that you define in template
789:43 - files any configuration file or template
789:46 - file that uses the template in question
789:48 - can provide a value for the template
789:51 - property without changing the template
789:53 - directly this lets you abstract the
789:56 - property so that you can change the
789:58 - property's value for each unique
790:01 - configuration without updating the
790:03 - underlying template and just as a note
790:06 - deployment manager creates predefined
790:08 - environment variables that you can use
790:11 - in your deployment in this example the
790:14 - project variable will use the project id
790:17 - for this specific project and so
790:19 - combining all these components together
790:21 - will give you a deployment and so a
790:24 - deployment is a collection of resources
790:26 - that are deployed and managed together
790:29 - using a configuration you can then
790:31 - deploy update or delete this deployment
790:34 - by merely changing some code or at the
790:37 - click of a button now when you deploy
790:40 - you provide a valid configuration in the
790:43 - request to create the deployment a
790:45 - deployment can contain a number of
790:47 - resources across a number of google
790:50 - cloud services when you create a
790:52 - deployment deployment manager creates
790:55 - all of the described resources to deploy
790:57 - a configuration it must be done through
791:00 - the command line and cannot be done
791:02 - through the console you can simply use
791:04 - the syntax shown here and a deployment
791:06 - will be instantiated from the
791:08 - configuration file that you have entered
791:11 - where bow tie deploy is the name of the
791:13 - deployment and the file after the dash
791:16 - dash config is your configuration file
791:19 - google cloud also offers pre-defined
791:22 - templates that you can use to deploy
791:25 - from the gcp marketplace and can be
791:27 - found right in the console of deployment
791:30 - manager this way all the configuration
791:33 - and template creation is handled for you
791:35 - and you just deploy the solution through
791:38 - the console now after you've created a
791:40 - deployment you can update it whenever
791:43 - you need to you can update a deployment
791:45 - by adding or removing resources from a
791:47 - deployment or updating the properties of
791:50 - existing resources in a deployment a
791:52 - single update can contain any
791:55 - combination of these changes so you can
791:57 - make changes to the properties of
791:59 - existing resources and add new resources
792:02 - in the same request you update your
792:04 - deployment by first making changes to
792:07 - your configuration file or you can
792:09 - create a configuration file with the
792:11 - changes you want you will then have the
792:13 - option to pick the policies to use for
792:15 - your updates or you can use the default
792:18 - policies and finally you then make the
792:21 - update request to deployment manager and
792:23 - so once you've launched your deployment
792:26 - each deployment has a corresponding
792:28 - manifest as the example shown here a
792:31 - manifest is a read-only property that
792:33 - describes all the resources in your
792:36 - deployment and is automatically created
792:39 - with each new deployment manifests
792:41 - cannot be modified after they have been
792:43 - created as well it's not the same as a
792:46 - configuration file but is created based
792:49 - on the configuration file and so when
792:52 - you delete a deployment all resources
792:55 - that are part of the deployment are also
792:57 - deleted if you want to delete specific
792:59 - resources from your deployment and keep
793:01 - the rest delete those resources from
793:04 - your configuration file and update the
793:06 - deployment instead
793:08 - and so as you can see here deployment
793:10 - manager gives you a slew of different
793:12 - options to deploy update or delete
793:16 - resources simultaneously in google cloud
793:20 - now like most services in gcp there are
793:23 - always some best practices to follow
793:25 - note that there are many more best
793:27 - practices to add to this and can be
793:29 - found in the documentation which i will
793:32 - be providing the link to in the lesson
793:34 - text but i did want to point out some
793:36 - important ones to remember so the first
793:38 - one i wanted to bring up is to break
793:40 - your configurations up into logical
793:43 - units so for example you should create
793:45 - separate configurations for networking
793:48 - services security services and compute
793:51 - services so this way each team will be
793:54 - able to easily take care of their own
793:56 - domain without having to sift through a
793:59 - massive template containing the code to
794:02 - the entire environment another best
794:04 - practice to follow is to use references
794:07 - and references should be used for values
794:09 - that are not defined until a resource is
794:12 - created such as resources self-link ip
794:15 - address or system generated id without
794:19 - references deployment manager creates
794:21 - all resources in parallel so there's no
794:24 - guarantee that dependent resources are
794:27 - created in the correct order using
794:29 - references would enforce the order in
794:32 - which resources are created the next one
794:35 - is to preview your deployments using the
794:37 - preview flag so you should always
794:39 - preview your deployments to assess how
794:42 - making an update will affect your
794:44 - deployment deployment manager does not
794:46 - actually deploy resources when you
794:49 - preview a configuration but runs a mock
794:52 - deployment of those resources instead
794:54 - this gives you the opportunity to see
794:57 - the changes to your deployment before
794:59 - committing to it you also want to
795:01 - consider automating the creation of
795:03 - projects as well as automating the
795:06 - creation of resources contained within
795:08 - the projects and this enables you to
795:10 - adopt an infrastructure as code approach
795:14 - for project provisioning this will allow
795:16 - you to provide a series of predefined
795:19 - project environments that can be quickly
795:21 - and easily provisioned it will also
795:23 - allow you to use version control to
795:26 - manage your base project configuration
795:28 - and it will also allow you to deploy
795:31 - reproducible and consistent project
795:33 - configurations and lastly using a
795:36 - version control system as part of the
795:39 - development process for your deployments
795:42 - is a great best practice to follow as it
795:44 - allows you to fall back to a previous
795:47 - known good configuration it provides an
795:49 - audit trail for changes as well it uses
795:53 - the configuration as part of a
795:55 - continuous deployment system now as
795:57 - you've seen here in this lesson
795:59 - deployment manager can be a powerful
796:01 - tool in your tool belt when it comes to
796:03 - implementing infrastructure as code and
796:06 - it has endless possibilities that you
796:08 - can explore on your own it can also
796:11 - provide a massive push
796:13 - towards devops practices and head down
796:16 - the path of continuous automation
796:19 - through continuous integration
796:21 - continuous delivery and continuous
796:23 - deployment and so that's pretty much all
796:25 - i wanted to cover when it comes to
796:27 - deployment manager and so whenever
796:29 - you're ready join me in the next one
796:31 - where we will go hands-on in a
796:33 - demonstration to deploy a configuration
796:36 - in deployment manager so you can now
796:38 - mark this lesson as complete and
796:39 - whenever you're ready join me in the
796:41 - console
796:42 - [Music]
796:46 - welcome back in this demonstration we're
796:48 - gonna go hands-on with deployment
796:50 - manager and deploy a small web server
796:53 - we're gonna first use the google cloud
796:55 - editor to copy in our code and we're
796:58 - gonna then do a dry run and then finally
797:00 - deploy our code we're gonna then do a
797:02 - walkthrough of deployment manager in the
797:04 - console and go through the manifest as
797:07 - well as some of the other features we're
797:09 - then going to verify all the deployed
797:11 - resources and we get to do an easy
797:13 - cleanup in the end by hitting the delete
797:16 - button and taking care of removing any
797:18 - resources that were created so there's
797:20 - quite a bit to go through here and so
797:22 - with that being said let's dive in and
797:25 - so as you can see here i am logged in as
797:27 - tonybowties gmail.com in the project of
797:31 - bowtie inc now since we're going to be
797:34 - doing most of our work in code the first
797:36 - thing that we want to do is go to the
797:38 - google cloud editor so i'm going to go
797:40 - up here to the top and open up cloud
797:42 - shell and i'm going to then click on the
797:44 - button open editor i'm going to make
797:46 - this full screen for better viewing and
797:48 - so in order to get the terminal in the
797:50 - same viewing pane as the editor i'm
797:52 - going to simply go up to the top menu
797:54 - and click on terminal and select new
797:57 - terminal now for better viewing and this
797:59 - is totally optional for you i'm going to
798:01 - change the color theme into a dark mode
798:04 - and so i'm going to go up to the menu
798:06 - click on file go down to settings and go
798:08 - over to color theme and i'm going to
798:10 - select dark visual studio and for those
798:13 - of you who are working in visual studio
798:15 - code this may look very familiar to you
798:18 - and i'm also going to increase the font
798:20 - size by again going back up to file over
798:23 - to settings and then over to open
798:25 - preferences here under workspace and
798:28 - then scroll down to terminal
798:30 - and if you scroll down to integrated
798:32 - font size i'm going to adjust the font
798:34 - size to 20 for better viewing and my
798:37 - cloud shell font size is a little bit
798:39 - easier to see and so once you've done
798:40 - that you can then close the preferences
798:42 - tab and we're now ready to create files
798:45 - in our editor okay so next up i want to
798:48 - create a folder for all my files to live
798:50 - in so i'm going to go up to the menu
798:52 - here i'm going to select on file and
798:54 - select new folder and i'm going to
798:55 - rename this folder as templates and hit
798:58 - ok and so now that we have the folder
799:01 - that all of our files are going to live
799:03 - in the next step is to open up the
799:05 - github repository in your text editor
799:08 - and have your files ready to copy over
799:11 - and so just as a note for those who are
799:13 - fluent in how to use git
799:15 - you can use this new feature in the
799:17 - cloud shell editor to clone the course
799:20 - repo without having to recreate the
799:22 - files so i'm going to go over my text
799:24 - editor and make sure that you've
799:25 - recently done a git pull we're going to
799:28 - open up the files under compute engine
799:30 - deployment manager and you'll see
799:32 - templates with a set of three files and
799:35 - i've already conveniently opened them up
799:37 - i'm going to go up to bow tie
799:38 - deploy.yaml and this is going to be the
799:41 - configuration file that i'm going to be
799:43 - copying over and once i finish copying
799:45 - all these files over i'll be going
799:47 - through this in a little bit of detail
799:50 - just so you can understand the format of
799:52 - this configuration and so i'm going to
799:54 - select all of this i'm going to copy
799:55 - this head back on over to the editor and
799:58 - here i'm going to select file new file
800:01 - so i'm going to rename this as bow tie
800:04 - dash deploy dot yaml hit okay and i'm
800:08 - going to paste in my code and so this
800:10 - configuration file is showing that i'm
800:12 - going to be importing two templates by
800:15 - the name of bowtie.webserver.jinja
800:18 - as well as
800:20 - bowtie.network.jinja so i'm going to
800:22 - have a template for my web server and a
800:24 - template for the network and under
800:26 - resources as you can see this code here
800:29 - will create my bow tie dash web server
800:31 - the type is going to be the template the
800:33 - properties will have the zone the
800:36 - machine type as well as a reference for
800:38 - the network as well underneath the
800:41 - bowtie web server is the bowtie network
800:44 - and again this is pulling from type
800:46 - bowtie.network.jinja
800:49 - so this is a another template file and
800:51 - under the properties we have the region
800:54 - of us east one and so we're going to
800:56 - copy over these two templates bowtie web
800:59 - server and bowtie network as we need
801:01 - both of these templates in order to
801:03 - complete this deployment and so i'm
801:05 - going to go ahead and do that now head
801:07 - back on over to my code editor i'm going
801:09 - to go to bowtie web server i'm going to
801:11 - copy everything here back to my editor
801:13 - and i'm going to create the new file
801:15 - called bowtie
801:17 - web server it's going to be dot jinja
801:19 - hit enter i'm going to paste the code in
801:22 - and just to do a quick run through of
801:23 - the template the instance name is going
801:25 - to be bow tie dash website the type is
801:28 - compute.v1.instance
801:30 - and as you can see here we are using a
801:32 - bunch of different properties here under
801:35 - zone we have property zone which is
801:37 - going to reference back to the yaml
801:39 - template here under zone you will see us
801:42 - east 1b and so this way if i have to
801:44 - create another web server
801:46 - i can enter whatever zone i like here in
801:48 - the configuration file and leave the bow
801:50 - tie dash web server template just the
801:53 - way it is under machine type i have
801:55 - variables set for both the zone and
801:57 - machine type under disks i'm going to
802:00 - have the device name as an environment
802:02 - variable and it's going to be a
802:04 - persistent disk and the source image is
802:07 - going to be debian9 i also put in some
802:09 - metadata here that will bring up the web
802:12 - server and lastly i have a network tag
802:14 - of http server as well as the
802:17 - configuration for the network interface
802:19 - the network referring to bowtie dash
802:22 - network and a sub network called public
802:25 - which i will be showing to you in just a
802:27 - moment and as well the access configs of
802:30 - the type one to one nat and this will
802:32 - give the instance a public ip address
802:35 - and so now that we've gone through that
802:36 - template we need to create one last
802:38 - template which is the bowtie dash
802:40 - network so i'm going to head back on
802:42 - over to my code editor and open up
802:44 - bowtie network select the code copy it
802:48 - back over to cloud editor and i'm going
802:50 - to create a new file call this bowtie
802:53 - network dot jinja hit enter paste in my
802:56 - code and to quickly walk you through
802:58 - this we're going to be creating a new
803:00 - custom network called bow tie dash
803:02 - network the type is going to be
803:04 - compute.v1.network
803:06 - as the vpc uses the compute engine api
803:10 - it's going to be a custom network so the
803:12 - value of the auto create sub networks is
803:15 - going to be false the name is going to
803:16 - be public here we have the custom
803:19 - ipcider range and you can also use this
803:22 - as a variable but for this demo i
803:24 - decided to just leave it under network i
803:26 - have a reference to the bowtie network
803:29 - the value for private google access is
803:31 - false and the region variable is
803:33 - fulfilled through the configuration file
803:36 - moving right along i have two firewall
803:38 - rules here one for ssh access and the
803:42 - other for web server access one opening
803:44 - up port 22 to the world as well as port
803:47 - 80. as well the web server access
803:50 - firewall rule has a target tag of http
803:53 - server referencing back to the network
803:55 - tag of the bowtie web server instance
803:58 - okay and so now we've finished creating
804:00 - the configuration file along with the
804:02 - templates so i'm going to head back on
804:04 - up to the menu click on file and select
804:08 - save all and since we've finished
804:10 - creating all of our files the next thing
804:12 - to do is to execute a mock deploy using
804:15 - the bowtie deploy configuration but
804:17 - first i know that we haven't used
804:19 - deployment manager before and so i need
804:22 - to go in and turn on the api and so i'm
804:25 - just going to go up here to the top to
804:27 - the search bar and i'm going to type in
804:29 - deployment and you should see deployment
804:31 - manager as the first result and bring
804:34 - this down a little bit and as expected
804:36 - the deployment manager api has not been
804:38 - enabled yet so i'm going to click on
804:40 - enable and after a few moments we should
804:42 - be good to go
804:44 - okay and as you can see here deployment
804:46 - manager is pretty empty
804:48 - as most of it is done through the
804:49 - command line but if you're looking to
804:51 - deploy a marketplace solution you can do
804:54 - that right here at the top and this will
804:56 - bring you right to the marketplace and
804:58 - will allow you to deploy from a large
805:00 - selection of pre-configured templates
805:03 - but i don't want to do that and so i'm
805:05 - just going to bring this up a little bit
805:06 - and i'm going to head on over to the
805:08 - terminal i'm going to run an ls i'm
805:10 - going to run the command ls and you
805:12 - should be able to see the templates
805:14 - folder i'm going to change my directory
805:16 - into the templates folder do another ls
805:18 - and here are all my files and so before
805:21 - we do a mock deploy of this
805:22 - configuration we want to make sure that
805:25 - we're deploying to the correct project i
805:27 - can see here that i am currently in bow
805:29 - tie inc but if you are ever unsure about
805:32 - the project that you're in you can
805:34 - always run the gcloud config list
805:35 - command in order to confirm so i'm going
805:38 - to quickly clear my screen and i'm going
805:40 - to run the command gcloud config list
805:43 - it's going to prompt me to authorize
805:45 - this api call and i'm going to authorize
805:48 - and as expected my project is set to
805:50 - deploy in project bowtie inc and so now
805:53 - that i've verified it i'm going to
805:54 - quickly clear my screen again and so i'm
805:57 - going to paste in my command gcloud
805:59 - deployment dash manager deployments
806:02 - create bowtie deploy which is the name
806:06 - of the deployment along with the
806:07 - configuration file flag dash dash config
806:11 - and then the name of the configuration
806:12 - file which is bowtie
806:15 - deploy.yaml and the preview flag as
806:17 - we're only doing a mock deploy and so if
806:20 - there are any errors i'll be able to see
806:22 - this before i actually deploy all the
806:24 - resources so i'm going to go ahead and
806:26 - hit enter and in just a minute we'll
806:29 - find out exactly what happens
806:31 - and as you can see here the mock
806:33 - deployment was a success and there are
806:35 - no errors and if i do a quick refresh up
806:38 - here in the console i'll be able to see
806:40 - my deployment which i can drill down
806:42 - into and here i will see my manifest
806:45 - file with my manifest name and i can
806:47 - view the config as well as my templates
806:50 - that it imported the layout as well as
806:53 - the expanded config so if i click on
806:55 - view of the config it'll show me here in
806:58 - the right hand panel exactly what this
807:00 - deployment has used for the config and i
807:03 - can do the same thing with my template
807:05 - files so i'm going to open up my network
807:08 - template and i can quickly go through
807:10 - that if i'd like as well i also have the
807:12 - option to download it and if i really
807:14 - want to get granular i can go over here
807:16 - to the left hand pane i can select on vm
807:19 - instance and it'll show me all the
807:21 - resource properties everything from the
807:23 - disks to the machine type to the
807:25 - metadata the network interfaces the zone
807:28 - that it's in and the network tag same
807:31 - thing if i go over here to the network
807:33 - and again because this is a custom
807:35 - network the value for the autocreate
807:37 - subnetworks is false i can check on the
807:40 - public sub network as well as the
807:42 - firewall rules and so because this is a
807:44 - preview it has not actually deployed
807:47 - anything now taking a look at compute
807:49 - engine instances in a new tab you can
807:52 - see here that i have no instances
807:53 - deployed and so the same goes for any of
807:56 - the other resources and so what we want
807:58 - to do now is we want to deploy this
808:00 - deployment and we can do that one of two
808:02 - ways we can simply click on the button
808:05 - here that says deploy or we can run the
808:07 - command in the command line and so i'm
808:09 - looking to show you how to do it in the
808:10 - command line so i'm going to move down
808:12 - to the command line i'm going to quickly
808:14 - clear my screen
808:16 - i'm going to paste in the code which is
808:18 - gcloud deployment dash manager
808:20 - deployments update bowtie deploy now
808:24 - you're probably wondering why update and
808:27 - this is because the configuration has
808:29 - been deployed even though it's a preview
808:31 - deployment manager still sees it as a
808:34 - deployment and has created what google
808:36 - cloud calls a shell and so by using
808:38 - update you can fully deploy the
808:41 - configuration using your last preview to
808:44 - perform that update and this will deploy
808:46 - your resources exactly how you see it in
808:49 - the manifest and so anytime i make an
808:51 - adjustment to either the configuration
808:54 - or the templates i can simply run the
808:56 - update command instead of doing the
808:59 - whole deployment again so i want to get
809:00 - this deployed now and so i'm going to
809:02 - hit enter
809:04 - and i'll be back in a minute once it's
809:06 - deployed all the resources and success
809:09 - my deployment is successful and as you
809:11 - can see here there are no errors and all
809:14 - the resources are in a completed state
809:16 - so i'm going to select my bow tie
809:18 - website in my manifest and i'll have
809:20 - access to the resource with a link up
809:23 - here at the top that will bring me to
809:25 - the instance as well i can ssh into the
809:28 - instance and i have all the same options
809:31 - that i have in the compute engine
809:33 - console and so in order to verify that
809:35 - all my resources have been deployed i'm
809:38 - going to go back over to the tab that i
809:40 - already have open and as you can see my
809:42 - instance has been deployed and i want to
809:44 - check to see if my network has been
809:46 - deployed so i'm going to go up to the
809:48 - navigation menu and i'm going to head on
809:50 - down to vpc network and as you can see
809:53 - here bowtie network has been deployed
809:55 - with its two corresponding firewall
809:57 - rules i'm going to drill down into
809:59 - bowtie network and check out the
810:01 - firewall rules and as you can see here
810:03 - ssh access and web server access have
810:07 - been created with its corresponding
810:09 - protocols and ports and so now that i
810:11 - know that all my resources have been
810:13 - deployed i want to head back on over to
810:15 - compute engine to see if my instance has
810:18 - been configured properly so i'm going to
810:20 - click on ssh to see if i can ssh into
810:23 - the instance and success with ssh so i
810:26 - know that this is working properly and
810:28 - so i'm going to close this tab down and
810:30 - i also want to see whether or not my web
810:32 - server has been configured properly with
810:35 - the metadata that i provided it and so i
810:37 - can directly open up the webpage by
810:40 - simply clicking on this link and success
810:43 - my you look dapper today why thank you
810:45 - tony bowtie and so as you can see the
810:48 - web server has been configured properly
810:50 - using the metadata that i provided so i
810:52 - wanted to congratulate you on making it
810:54 - to the end of this demo and hope it has
810:57 - been extremely useful and gave you an
810:59 - understanding of how infrastructure is
811:01 - code is used in google cloud using their
811:04 - native tools i hope this also triggered
811:06 - some possible use cases for you that
811:09 - will allow you to automate more
811:11 - resources and configurations in your
811:14 - environment and allow you to start
811:16 - innovating on fantastic new ways for
811:19 - cicd for those of you who are familiar
811:22 - with infrastructure as code this may
811:24 - have been a refresher but will give you
811:26 - some insight for questions on the exam
811:29 - that cover deployment manager and just
811:31 - as a quick note for those of you who are
811:33 - looking to learn more about
811:35 - infrastructure as code i have put a few
811:37 - links in the lesson text going into
811:40 - depth on deployment manager and another
811:42 - tool that google recommends called
811:44 - terraform and so now before you go we
811:47 - want to clean up all the resources that
811:49 - we've deployed to reduce any incurred
811:52 - costs and because deployment manager
811:54 - makes it easy we can do it in one simple
811:56 - step so i'm going to head back on over
811:58 - to my open tab where i have my console
812:01 - open to deployment manager and i'm going
812:03 - to head on over to the delete button and
812:05 - simply click on delete now deployment
812:08 - manager gives me the option of deleting
812:10 - all the resources it created or simply
812:13 - deleting the manifest but keeping the
812:15 - resources untouched and so you want to
812:17 - select delete bowtie deploy with all of
812:20 - its resources and simply click on delete
812:23 - all and this will initiate the teardown
812:25 - of all the resources that have been
812:27 - deployed from the bowtie deploy
812:30 - configuration and this will take a few
812:32 - minutes to tear down but if you ever
812:34 - have a larger configuration to deploy
812:37 - just as a note it may take a little bit
812:39 - longer to both deploy and to tear down
812:42 - and so just as a recap you've created a
812:45 - configuration file and two templates in
812:47 - the cloud shell editor you then deployed
812:50 - your configuration using deployment
812:52 - manager through the command line in
812:54 - cloud shell you then verified each
812:57 - individual resource that was deployed
812:59 - and verified the configuration of each
813:02 - resource congratulations again on a job
813:05 - well done and so that's pretty much all
813:07 - i wanted to cover in this demo when it
813:09 - comes to deploying resources using
813:12 - deployment manager so you can now mark
813:14 - this as complete and let's move on to
813:16 - the next one
813:17 - [Music]
813:21 - welcome back and in this lesson we're
813:24 - going to learn about google cloud load
813:26 - balancing and how it's used to
813:27 - distribute traffic within the google
813:30 - cloud platform google cloud load
813:32 - balancing is essential when using it
813:34 - with instance groups kubernetes clusters
813:38 - and is pretty much the defacto when it
813:40 - comes to balancing traffic coming in as
813:43 - well as within your gcp environment
813:46 - knowing the differences between the
813:47 - types of load balancers and which one to
813:49 - use for specific scenarios is crucial
813:53 - for the exam as you will be tested on it
813:55 - and so there's a lot to cover here so
813:57 - with that being said let's dive in now i
814:00 - wanted to start off with some basics
814:02 - with regards to what is low balancing
814:05 - and so when it comes to the low balancer
814:07 - itself a low balancer distributes user
814:10 - traffic across multiple instances of
814:13 - your application so by spreading the
814:15 - load you reduce the risk of your
814:17 - applications experiencing performance
814:20 - issues a load balancer is a single point
814:22 - of entry with either one or multiple
814:25 - back ends and within gcp these back ends
814:28 - could consist of either instance groups
814:31 - or negs and i'll be getting into any g's
814:34 - in just a little bit low balancers on
814:36 - gcp are fully distributed and software
814:40 - defined so there is no actual hardware
814:43 - load balancer involved in low balancing
814:46 - on gcp it is completely software defined
814:49 - and so there's no need to worry about
814:51 - any hardware any pre-warming time as
814:54 - this is all done through software now
814:56 - depending on which low balancer you
814:58 - choose google cloud gives you the option
815:01 - of having either a global load balancer
815:04 - or a regional load balancer the load
815:06 - balancers are meant to serve content as
815:09 - close as possible to the users so that
815:12 - they don't experience increased latency
815:14 - and gives the users a better experience
815:17 - as well as reducing latency on your
815:19 - applications when dealing with low
815:22 - balancers in between services google
815:25 - cloud also offers auto scaling with
815:27 - health checks in their load balancers to
815:30 - make sure that your traffic is always
815:32 - routed to healthy instances and by using
815:35 - auto scaling able to scale up the amount
815:37 - of instances you need in order to handle
815:40 - the load automatically now as there are
815:43 - many different low balancers to choose
815:45 - from it helps to know what specific
815:47 - aspects you're looking for and how you
815:50 - want your traffic distributed and so
815:52 - google has broken them down for us into
815:55 - these three categories the first
815:57 - category is global versus regional
816:00 - global load balancing is great for when
816:02 - your back ends are distributed across
816:05 - multiple regions
816:06 - and your users need access to the same
816:09 - applications and content
816:11 - using a single anycast ip address
816:14 - as well when you're looking for ipv6
816:16 - termination global load balancing will
816:19 - take care of that now when it comes to
816:21 - regional load balancing this is if
816:23 - you're looking at serving your back ends
816:25 - in a single region and handling only
816:28 - ipv4 traffic now once you've determined
816:31 - whether or not you need global versus
816:33 - regional low balancing the second
816:35 - category to dive into is external versus
816:39 - internal external load balancers are
816:41 - designed to distribute traffic coming
816:44 - into your network from the internet
816:46 - and internal load balancers are designed
816:49 - to distribute traffic within your
816:51 - network and finally the last category
816:54 - that will help you decide on what type
816:56 - of load balancer you need is the traffic
816:59 - type and shown here are all the traffic
817:02 - types that cover http https tcp and udp
817:07 - and so now that we've covered the
817:09 - different types of load balancing that's
817:11 - available on google cloud i wanted to
817:13 - dive into some more depth on the low
817:16 - balancers themselves here you can see
817:18 - that there are five load balancers
817:20 - available and i will be going through
817:22 - each one of these in detail now before
817:25 - diving into the low balancers themselves
817:28 - i wanted to introduce you to a concept
817:30 - using gcp
817:32 - for all load balancers called back end
817:35 - services how a low balancer knows
817:37 - exactly what to do is defined by a
817:40 - backend service and this is how cloud
817:42 - load balancing knows how to distribute
817:45 - the traffic the backend service
817:47 - configuration contains a set of values
817:50 - such as the protocol used to connect to
817:52 - back ends various distribution in
817:54 - session settings health checks and
817:57 - timeouts these settings provide fine
818:00 - grain control over how your load
818:02 - balancer behaves an external http or
818:05 - https load balancer must have at least
818:08 - one backend service and can have
818:11 - multiple backend services the back ends
818:13 - of a backend service can be either
818:16 - instance groups or network endpoint
818:18 - groups also known as negs but not a
818:21 - combination of both and so just as a
818:24 - note you'll hear me refer to negs over
818:27 - the course of this lesson and so a
818:29 - network endpoint group also known as neg
818:32 - is a configuration object that specifies
818:35 - a group of back-end endpoints or
818:38 - services and a common use case for this
818:41 - configuration is deploying services into
818:44 - containers now moving on to the values
818:46 - themselves i wanted to first start with
818:48 - health checks and google cloud uses the
818:51 - overall health state of each back end to
818:54 - determine its eligibility for receiving
818:57 - new requests or connections back ends
819:00 - that respond successfully for the
819:02 - configured number of times are
819:04 - considered healthy back-ends that fail
819:06 - to respond successfully for a separate
819:08 - number of times are considered unhealthy
819:11 - and when a back-end is considered
819:13 - unhealthy traffic will not be routed to
819:15 - it next up is session affinity and
819:18 - session affinity sends all requests from
819:21 - the same client to the same back end if
819:24 - the back end is healthy and it has
819:26 - capacity service timeout is the next
819:29 - value and this is the amount of time
819:31 - that the load balancer waits for a
819:33 - backend to return a full response to a
819:36 - request next up is traffic distribution
819:39 - and this comprises of three different
819:41 - values the first one is a balancing mode
819:44 - and this defines how the load balancer
819:46 - measures back-end readiness for the new
819:48 - requests or connections the second one
819:51 - is target capacity and this defines a
819:53 - target maximum number of connections a
819:55 - target maximum rate or target maximum
819:58 - cpu utilization and the third value for
820:01 - traffic distribution is capacity scalar
820:04 - and this adjusts overall available
820:06 - capacity without modifying the target
820:09 - capacity and the last value for back-end
820:12 - services are back-ends and a back-end is
820:15 - a group of endpoints that receive
820:17 - traffic from a google cloud load
820:19 - balancer and there are several types of
820:21 - back-ends but the one that we are
820:23 - concentrating on for this section and
820:25 - for the exam is the instance group now
820:28 - backend services are not critical to
820:30 - know for the exam but i wanted to
820:32 - introduce you to this concept to add a
820:35 - bit more context for when you are
820:37 - creating low balancers in any
820:39 - environment
820:40 - and will help you understand other
820:42 - concepts in this lesson and so this is
820:44 - the end of part one of this lesson it
820:46 - was getting a bit long so i decided to
820:48 - break it up this would be a great
820:50 - opportunity for you to get up and have a
820:53 - stretch get yourself a coffee or tea and
820:56 - whenever you're ready join me in part
820:58 - two where we will be starting
821:00 - immediately from the end of part one so
821:02 - you can now complete this video and i
821:04 - will see you in part two
821:10 - this is part two of the cloud load
821:12 - balancers lesson and we'll be starting
821:14 - exactly where we left off in part one so
821:17 - with that being said let's dive in now
821:20 - before jumping right into the first load
821:22 - balancer that i wanted to introduce
821:24 - which is http and https low balancer
821:28 - there's a couple of different concepts
821:30 - that i wanted to introduce and these are
821:32 - the methods of how an http and https
821:36 - load balancer distributes traffic using
821:38 - forwarding rules and these are cross
821:41 - region low balancing and content based
821:43 - load balancing now touching on cross
821:45 - region load balancing when you configure
821:48 - an external http or https load balancer
821:52 - in premium tier it uses a global
821:55 - external ip address and can
821:57 - intelligently route requests from users
822:00 - to the closest backend instance group or
822:03 - neg based on proximity for example if
822:07 - you set up instance groups in north
822:09 - america and europe and attach them to a
822:12 - low balancers back-end service user
822:14 - requests around the world are
822:16 - automatically sent to the vms closest to
822:19 - the users assuming that the vms pass
822:21 - health checks and have enough capacity
822:24 - if the closest vms are all unhealthy or
822:26 - if the closest instance group is at
822:28 - capacity and another instance group is
822:31 - not at capacity the load balancer
822:33 - automatically sends requests to the next
822:36 - closest region that has available
822:39 - capacity and so here in this diagram a
822:41 - user in switzerland hits the low
822:44 - balancer by going to bowtieinc.co and
822:47 - because there are vms that are able to
822:49 - serve that traffic in europe west 6
822:51 - traffic is routed to that region and so
822:54 - now getting into content based load
822:56 - balancing http and https low balancing
823:00 - supports content based load balancing
823:03 - using url maps to select a backend
823:06 - service based on the requested host name
823:09 - request path or both for example you can
823:12 - use a set of instance groups or negs to
823:16 - handle your video content and another
823:18 - set to handle static as well as another
823:21 - set to handle any images you can also
823:24 - use http or https low balancing with
823:28 - cloud storage buckets and then after you
823:30 - have your load balancer set up you can
823:32 - add cloud storage buckets to it now
823:34 - moving right along when it comes to http
823:38 - and https load balancer this is a global
823:42 - proxy based layer 7 low balancer which
823:45 - is at the application layer and so just
823:47 - as a note here with all the other low
823:50 - balancers that are available in gcp the
823:52 - http and https low balancer is the only
823:57 - layer 7 load balancer all the other low
823:59 - balancers in gcp are layer 4 and will
824:02 - work at the network layer and so this
824:04 - low balancer enables you to serve your
824:07 - applications worldwide behind a single
824:10 - external unicast ip address external
824:13 - http and https load balancing
824:17 - distributes http and https traffic to
824:21 - back ends hosted on compute engine and
824:24 - gke external http and https load
824:28 - balancing is implemented on google front
824:31 - ends or gfes as shown here in the
824:34 - diagram gfes are distributed globally
824:37 - and operate together using google's
824:39 - global network and control plane in the
824:42 - premium tier gfes offer cross-regional
824:45 - low balancing directing traffic to the
824:48 - closest healthy backend that has
824:50 - capacity and terminating http and https
824:54 - traffic as close as possible to your
824:57 - users with the standard tier the load
825:00 - balancing is handled regionally and this
825:02 - load balancer is available to be used
825:04 - both externally and internally that
825:07 - makes this load balancer global external
825:11 - and internal this load balancer also
825:13 - gives support for https and ssl which
825:17 - covers tls for encryption in transit as
825:20 - well this load balancer accepts all
825:22 - traffic whether it is ipv4 or ipv6
825:26 - traffic and just know that ipv6 traffic
825:30 - will terminate at the low balancer and
825:33 - then it will forward traffic as ipv4 so
825:36 - it doesn't really matter which type of
825:37 - traffic you're sending the load balancer
825:40 - will still send the traffic to the back
825:42 - end using ipv4 this traffic is
825:45 - distributed by location or by content as
825:48 - shown in the previous diagram forwarding
825:50 - rules are in place to distribute defined
825:53 - targets to each target pool for the
825:56 - instance groups again defined targets
825:59 - could be content based and therefore as
826:01 - shown in the previous diagram video
826:04 - content could go to one target whereas
826:06 - static content could go to another
826:08 - target url maps direct your requests
826:11 - based on rules so you can create a bunch
826:14 - of rules depending on what type of
826:15 - traffic you want to direct and put them
826:17 - in maps for requests ssl certificates
826:20 - are needed for https and these can be
826:23 - either google managed or self-managed
826:26 - and so just as a quick note here the
826:28 - ports used for http are on 80 and 8080
826:33 - as well on https the port that is used
826:37 - is port 443 now moving into the next low
826:40 - balancer is ssl proxy an ssl proxy low
826:45 - balancing is a reverse proxy load
826:47 - balancer that distributes ssl traffic
826:50 - coming from the internet to your vm
826:53 - instances when using ssl proxy load
826:56 - balancing for your ssl traffic user ssl
826:59 - connections are terminated at the low
827:02 - balancing layer and then proxied to the
827:05 - closest available backend instances by
827:08 - either using ssl or tcp with the premium
827:11 - tier ssl proxy low balancing can be
827:14 - configured as a global load balancing
827:17 - service with the standard tier the ssl
827:20 - proxy load balancer handles low
827:22 - balancing regionally this load balancer
827:25 - also distributes traffic by location
827:28 - only ssl proxy low balancing lets you
827:31 - use a single ip address for all users
827:34 - worldwide and is a layer 4 load balancer
827:37 - which works on the network layer this
827:39 - load balancer shows support for tcp with
827:42 - ssl offload and this is something
827:45 - specific to remember for the exam this
827:48 - is not like the http or https load
827:51 - balancer where we can use specific rules
827:54 - or specific configurations in order to
827:56 - direct traffic ssl proxy low balancer
827:59 - supports both ipv4 and ipv6 but again it
828:04 - does terminate at the load balancer and
828:06 - forwards the traffic to the back end as
828:09 - ipv4 traffic and forwarding rules are in
828:12 - place to distribute each defined target
828:15 - to its proper target pool and encryption
828:18 - is supported by configuring back-end
828:20 - services to accept all the traffic over
828:23 - ssl now just as a note it can also be
828:26 - used for other protocols that use ssl
828:30 - such as web sockets and imap over ssl
828:33 - and carry a number of open ports to
828:36 - support them moving on to the next load
828:38 - balancer is tcp proxy now the tcp proxy
828:42 - load balancer is a reverse proxy load
828:44 - balancer that distributes tcp traffic
828:48 - coming from the internet to your vm
828:50 - instances when using tcp proxy load
828:52 - balancing traffic coming over a tcp
828:55 - connection is terminated at the load
828:58 - balancing layer and then forwarded to
829:00 - the closest available backend using tcp
829:04 - or ssl so this is where the low balancer
829:06 - will determine which instances are at
829:08 - capacity and send them to those
829:11 - instances that are not like ssl proxy
829:14 - load balancing tcp proxy load balancing
829:17 - lets you use a single ip address for all
829:20 - users worldwide the tcp proxy load
829:23 - balancer automatically routes traffic to
829:26 - the back ends that are closest to the
829:27 - user this is a layer 4 load balancer and
829:30 - again can serve traffic both globally
829:33 - and externally tcp proxy distributes
829:36 - traffic by location only and is intended
829:39 - for specifically non-http traffic
829:42 - although you can decide if you want to
829:44 - use ssl between the proxy and your back
829:46 - end and you can do this by selecting a
829:49 - certificate on the back end again this
829:51 - type of load balancer supports ipv4 and
829:54 - ipv6 traffic and ipv6 traffic will
829:58 - terminate at the low balancer and
830:00 - forwards that traffic to the back end as
830:03 - ipv4 traffic now tcp proxy low balancing
830:07 - is intended for tcp traffic and supports
830:10 - many well-known ports such as port 25
830:14 - for simple mail transfer protocol or
830:16 - smtp next up we have the network load
830:19 - balancer now the tcp udp network load
830:22 - balancer is a regional pass-through load
830:25 - balancer a network load balancer
830:27 - distributes tcp or udp traffic among
830:31 - instances in the same region network
830:33 - load balancers are not proxies and
830:36 - therefore responses from the back end
830:38 - vms go directly to the clients
830:41 - not back through the load balancer the
830:43 - term known for this is direct server
830:46 - return as shown here in the diagram this
830:48 - is a layer 4 regional load balancer and
830:51 - an external load balancer as well that
830:54 - can serve to regional locations it
830:57 - supports either tcp or udp but not both
831:01 - although it can low balance udp tcp and
831:05 - ssl traffic on the ports that are not
831:07 - supported by the tcp proxy and ssl proxy
831:12 - ssl traffic can still be decrypted by
831:14 - your back end instead of the load
831:17 - balancer itself traffic is also
831:19 - distributed by incoming protocol data
831:22 - this being protocols scheme and scope
831:25 - there is no tls offloading or proxying
831:28 - and forwarding rules are in place to
831:30 - distribute and define targets to their
831:33 - target pools and this is for tcp and udp
831:36 - only now with other protocols they use
831:39 - target instances as opposed to instance
831:42 - groups lastly a network load balancer
831:44 - can also only support
831:47 - self-managed ssl certificates as opposed
831:50 - to the google managed certificates as
831:52 - well and so the last low balancer to
831:54 - introduce is the internal load balancer
831:57 - now an internal tcp or udp load balancer
832:01 - is a layer 4 regional load balancer that
832:04 - enables you to distribute traffic behind
832:07 - an internal load balancing ip address
832:10 - that is accessible only to your internal
832:13 - vm instances internal tcp and udp load
832:17 - balancing distributes traffic among vm
832:20 - instances in the same region this load
832:22 - balancer supports tcp or udp traffic but
832:26 - not both and as i said before this type
832:28 - of load balancer is used to balance
832:30 - traffic within gcp across instances this
832:34 - low balancer cannot be used for
832:37 - balancing internet traffic as it is
832:39 - internal only traffic is automatically
832:42 - sent to the back end as it does not
832:44 - terminate client connections and for
832:47 - forwarding rules this load balancer
832:49 - follows specific specifications where
832:52 - you need to specify at least one and up
832:54 - to five ports by number as well you must
832:58 - specify all to forward traffic to all
833:01 - ports now again like the network load
833:04 - balancer you can use either tcp or udp
833:08 - and so that's pretty much all i had to
833:09 - cover with this lesson on low balancing
833:12 - please remember that for the exam you
833:14 - will need to know the differences
833:16 - between them all
833:18 - in my experience there are a few
833:20 - questions that come up on the exam where
833:22 - you will need to know what low balancer
833:24 - to use and so a good idea might be to
833:27 - dive into the console and have a look at
833:30 - the options as well as going back
833:32 - through this lesson as a refresher to
833:35 - understand each use case this is also a
833:37 - crucial component in any environment
833:40 - that is used especially when serving
833:42 - applications to the internet for any
833:45 - three-tier web application or kubernetes
833:48 - cluster and so that pretty much sums up
833:49 - this lesson on low balancing so you can
833:52 - now mark this lesson as complete and
833:54 - let's move on to the next one
834:00 - welcome back in this lesson i will be
834:02 - going into depth on instance groups
834:05 - along with instance templates instance
834:07 - groups are a great way to set up a group
834:10 - of identical servers used in conjunction
834:12 - with instance groups instance templates
834:15 - handles the instance properties to
834:17 - deploy the instance groups into your
834:19 - environment this lesson will dive into
834:22 - the details of the features use cases
834:25 - and how instance groups and instance
834:27 - templates work together to create a
834:30 - highly scalable and performing
834:32 - environment now there's a lot to cover
834:34 - here so with that being said let's dive
834:37 - in now an instance group is a collection
834:40 - of vm instances that you can manage as a
834:42 - single entity compute engine offers two
834:45 - kinds of vm instance groups managed and
834:48 - unmanaged manage instance groups or migs
834:52 - let you operate applications on multiple
834:55 - identical vms you can make your workload
834:58 - scalable and highly available by taking
835:01 - advantage of automated mig services like
835:04 - auto scaling auto healing regional and
835:07 - zonal deployments and automatic updating
835:10 - and i'll be getting into these services
835:11 - in just a sec now when it comes to
835:14 - unmanaged instance groups they also let
835:16 - you low balance across a fleet of vms
835:19 - but this is something that you need to
835:21 - manage and i'll be going deeper into
835:23 - unmanaged instance groups a bit later
835:25 - right now i wanted to take some time to
835:28 - go through the features and use cases of
835:30 - migs in a bit more detail for some more
835:33 - context starting off with its use cases
835:36 - now migs are great for stateless serving
835:39 - workloads such as website front ends web
835:42 - servers and website applications as the
835:45 - application does not preserve its state
835:48 - and saves no data to persistent storage
835:51 - all user and session data stays with the
835:53 - client and makes scaling up and down
835:56 - quick and easy migs are also great for
835:58 - stateless batch workloads and these are
836:01 - high performance or high throughput
836:04 - compute workloads such as image
836:06 - processing from a queue and lastly you
836:09 - can build highly available stateful
836:11 - workloads using stateful managed
836:14 - instance groups or stateful migs
836:16 - stateful workloads include applications
836:19 - with stateful data or configuration such
836:22 - as databases
836:24 - legacy monolith type applications and
836:26 - long running batch computations with
836:29 - checkpointing you can improve uptime and
836:31 - resiliency of these types of
836:33 - applications with auto healing
836:35 - controlled updates and multi-zone
836:37 - deployments while preserving each
836:40 - instance's unique state including
836:42 - instance names persistent disks and
836:45 - metadata now that i've covered the type
836:47 - of workloads that are used with migs i
836:50 - wanted to dive into the features
836:52 - starting with auto healing
836:54 - now when it comes to auto healing
836:56 - managed instance groups maintain high
836:59 - availability of your applications by
837:01 - proactively keeping your instances in a
837:04 - running state a mig automatically
837:07 - recreates an instance that is not
837:09 - running and managed instance groups also
837:12 - take care of application-based auto
837:14 - healing and this improves application
837:16 - availability by relying on a health
837:18 - check that detects things like freezing
837:22 - crashing or overloading if a health
837:24 - check determines that an application has
837:26 - failed on a vm the mig auto healer
837:29 - automatically recreates that vm instance
837:32 - the health check used to monitor the
837:34 - migs are similar to the health checks
837:36 - used for low balancing with a few little
837:38 - differences low balancing health checks
837:41 - help direct traffic away from
837:43 - unresponsive instances and towards
837:46 - healthy ones these health checks cannot
837:48 - recreate instances whereas mig health
837:50 - checks proactively signal to delete and
837:53 - recreate instances that become unhealthy
837:57 - moving on to managed instance groups
837:59 - regional or multi-zone feature now you
838:01 - have the option of creating regional
838:03 - migs or zonal migs
838:05 - regional migs provide higher
838:07 - availability compared to zonal migs
838:10 - because the instances in a regional mig
838:13 - are spread across multiple zones in a
838:16 - single region google recommends regional
838:18 - migs over zonal migs as you can manage
838:21 - twice as many migs as zonal migs so you
838:24 - can manage 2 000 migs instead of 1000
838:28 - you can also spread your application
838:29 - load across multiple zones instead of a
838:32 - single zone or managing multiple zonal
838:35 - migs across different zones and this
838:37 - protects against zonal failures and
838:40 - unforeseen scenarios where an entire
838:42 - group of instances in a single zone
838:44 - malfunctions in the case of a zonal
838:47 - failure or if a group of instances in a
838:50 - zone stops responding a regional mig
838:52 - continues supporting your instances by
838:55 - continuing to serve traffic to the
838:57 - instances in the remaining zones now
839:00 - cloud low balancing can use instance
839:02 - groups to serve traffic so you can add
839:05 - instance groups to a target pool or to a
839:08 - back end an instance group is a type of
839:10 - back end and the instances in the
839:12 - instance group respond to traffic from
839:15 - the load balancer the back end service
839:18 - in turn knows which instances it can use
839:21 - and how much traffic they can handle and
839:23 - how much traffic they are currently
839:25 - handling in addition the back-end
839:27 - service monitors health checking and
839:29 - does not send new connections to
839:32 - unhealthy instances now when your
839:34 - applications require additional compute
839:36 - resources migs support auto scaling that
839:39 - dynamically add or remove instances from
839:42 - the mig in response to an increase or
839:45 - decrease in load you can turn on auto
839:48 - scaling and configure an auto scaling
839:50 - policy to specify how you want the group
839:53 - to scale not only will auto scaling
839:55 - scale up to meet the load demands but
839:58 - will also shrink and remove instances as
840:01 - the load decreases to reduce your costs
840:04 - auto scaling policies include scaling
840:07 - based on cpu utilization load balancing
840:10 - capacity and cloud monitoring metrics
840:13 - and so when it comes to auto updating
840:15 - you can easily and safely deploy new
840:18 - versions of software to instances in a
840:21 - mig the rollout of an update happens
840:24 - automatically based on your
840:25 - specifications you can also control the
840:28 - speed and scope of the deployments in
840:30 - order to minimize disruptions to your
840:32 - application you can optionally perform
840:35 - rolling updates as well as partial
840:38 - rollouts for canary testing and for
840:40 - those who don't know rolling updates
840:42 - allow updates to take place with zero
840:45 - downtime by incrementally updating
840:48 - instances with new ones as well canary
840:51 - testing is a way to reduce risk and
840:53 - validate new software by releasing
840:56 - software to a small percentage of users
840:59 - with canary testing you can deliver to
841:01 - certain groups of users at a time and
841:04 - this is also referred to as stage
841:06 - rollouts and this is a best practice in
841:09 - devops and software development now
841:11 - there are a few more things that i
841:13 - wanted to point out that relate to migs
841:15 - you can reduce the cost of your workload
841:18 - by using preemptable vm instances in
841:20 - your instance group and when they are
841:22 - deleted auto healing will bring the
841:24 - instances back when preemptable capacity
841:27 - becomes available again you can also
841:29 - deploy containers to instances in
841:32 - managed instance groups when you specify
841:34 - a container image in an instance
841:36 - template and is used to create a mig
841:38 - each vm is created with the container
841:41 - optimized os that includes docker and
841:44 - your container starts automatically on
841:47 - each vm in the group and finally when
841:49 - creating migs you must define the vpc
841:52 - network that it will reside in although
841:54 - when you don't define the network google
841:56 - cloud will attempt to use the default
841:58 - network now moving on into unmanaged
842:01 - instance groups for just a minute
842:03 - unmanaged instance groups can contain
842:05 - heterogeneous instances and these are
842:08 - instances that are of mixed sizes of cpu
842:11 - ram as well as instance types and you
842:14 - can add and remove these instances from
842:16 - the group whenever you choose there's a
842:18 - major downside to this though unmanaged
842:21 - instance groups do not offer auto
842:23 - scaling auto healing rolling update
842:26 - support multi-zone support or the use of
842:29 - instance templates and are not a good
842:31 - fit for deploying highly available and
842:34 - scalable workloads you should only use
842:36 - unmanaged instance groups if you need to
842:39 - apply load balancing to groups of these
842:42 - mixed types of instances or if you need
842:45 - to manage the instances yourself so
842:47 - unmanaged instance groups are designed
842:50 - for very special use cases where you
842:52 - will need to mix instance types in
842:55 - almost all cases you will be using
842:57 - managed instance groups as they were
842:59 - intended to capture the benefits of all
843:02 - the features they have to offer
843:04 - now in order to launch an instance group
843:06 - into any environment you will need
843:08 - another resource to do this and this is
843:11 - where instance templates come into play
843:14 - an instance template is a resource that
843:16 - you can use to create vm instances and
843:19 - managed instance groups instance
843:22 - templates define the machine type boot
843:24 - disk image or container image as well as
843:28 - labels and other instance properties you
843:30 - can then use an instance template to
843:32 - create a mig or vm instance instance
843:35 - templates are an easy way to save a vm
843:38 - instances configuration so you can use
843:41 - it later to recreate vms or groups of
843:44 - vms an instance template
843:46 - is a global resource that is not bound
843:48 - to a zone or region although you can
843:51 - restrict a template to a zone by calling
843:54 - out specific zonal resources now there
843:56 - is something to note for when you are
843:58 - ever using migs if you want to create a
844:01 - group of identical instances you must
844:04 - use an instance template to create a mig
844:07 - and is something you should always keep
844:08 - in the front of mind when using migs
844:11 - these two resources both instance
844:13 - templates and managed instance groups go
844:16 - hand in hand now some other things to
844:18 - note is that instance templates are
844:20 - designed to create instances with
844:23 - identical configurations so you cannot
844:26 - update an existing instance template or
844:28 - change an instance template after you
844:30 - create it if you need to make changes to
844:32 - the configuration
844:34 - create a new instance template you can
844:36 - create a template based on an existing
844:38 - instance template or based on an
844:40 - existing instance to use an existing vm
844:43 - to make a template you can save the
844:45 - configuration using the gcloud command
844:48 - gcloud instance dash templates create or
844:52 - to use the console you can simply go to
844:54 - the instance templates page click on the
844:56 - template that you want to update and
844:58 - click on create similar the last thing
845:00 - that i wanted to point out is that you
845:02 - can use custom or public images in your
845:06 - instance templates and so that's pretty
845:08 - much all i had to cover when it comes to
845:11 - instance groups and instance templates
845:13 - managed instance groups are great for
845:15 - when you're looking at high availability
845:17 - as a priority and letting migs do all
845:20 - the work of keeping your environment up
845:22 - and running and so you can now mark this
845:24 - lesson as complete and whenever you're
845:26 - ready join me in the next one where we
845:28 - go hands-on with instance groups
845:31 - instance templates and load balancers in
845:33 - a demo
845:38 - welcome back in this demo we're going to
845:41 - put everything that we've learned
845:43 - together in a hands-on demo called
845:45 - managing bow ties we're going to create
845:47 - an instance template and next we're
845:49 - going to use it to create an instance
845:51 - group we're then going to create a low
845:53 - balancer with a new back end and create
845:56 - some health checks along the way we're
845:58 - then going to verify that all instances
846:00 - are working by browsing to the load
846:02 - balancer ip and verifying the website
846:05 - application we're then going to stress
846:07 - test one of the instances to simulate a
846:10 - scale out using auto scaling and then
846:12 - we're going to simulate scaling the
846:14 - instance group back in now there's quite
846:16 - a bit to do here so with that being said
846:19 - let's dive in so here i am logged in as
846:22 - tony bowties at gmail.com under project
846:26 - bowtie inc and so the first thing that
846:28 - you want to do is you want to make sure
846:29 - that you have a default vpc network
846:32 - already created and so just to double
846:34 - check i'm going to go over to the
846:35 - navigation menu i'm going to scroll down
846:38 - to vpc network
846:41 - and yes i do have a default vpc network
846:44 - so i'm going to go ahead and start
846:45 - creating my resources and so now what i
846:48 - want to do is i want to create my
846:49 - instance template and so in order to do
846:51 - that i'm going to go back up to the
846:53 - navigation menu i'm going to go down to
846:55 - compute engine and go up to instance
846:58 - templates as you can see i currently
847:00 - have no instance templates and yours
847:02 - should look the same and so you can go
847:04 - ahead and click on create instance
847:06 - template and so just as a note there are
847:08 - no monthly costs associated with
847:11 - instance templates but this estimate
847:13 - here on the right is to show you the
847:15 - cost of each instance you will be
847:17 - creating with this template okay so
847:19 - getting right into it i'm going to name
847:21 - this instance template
847:23 - bowtie template and since we're spinning
847:26 - up a lot of vms you want to be conscious
847:28 - on costs and so under series you're
847:30 - going to click on the drop down and
847:32 - you're going to select n1 and under
847:34 - machine type you're going to select f1
847:37 - micro and this is the smallest instance
847:39 - type as well as the cheapest within
847:41 - google cloud you can go ahead and scroll
847:43 - down right to the bottom here under
847:45 - firewall you want to check off allow
847:47 - http traffic next you want to select
847:50 - management security disks networking and
847:52 - sold tenancy you scroll down a little
847:55 - bit and under startup script you're
847:57 - going to paste in the script that's
847:59 - available in the repo and you will find
848:02 - a link to this script and the repo in
848:04 - the lesson text and so you can leave all
848:06 - the other options as its default and
848:09 - simply click on create it's going to
848:10 - take a couple minutes here okay and the
848:12 - instance template is ready and so the
848:14 - next step that you want to do is create
848:17 - an instance group and as i said in a
848:19 - previous lesson in order to create an
848:21 - instance group you need an instance
848:23 - template hence why we made the instance
848:25 - template first okay and our instance
848:27 - template has been created and so now
848:29 - that you've created your instance
848:31 - template you can head on over to
848:33 - instance groups here in the left hand
848:35 - menu and as expected there are no
848:37 - instance groups and so you can go ahead
848:39 - and click on the big blue button and
848:41 - create an instance group you're going to
848:43 - make sure that new managed instance
848:45 - group stateless is selected and here you
848:47 - have the option of choosing a stateful
848:49 - instance group as well as an unmanaged
848:52 - instance group and so we're going to
848:53 - keep things stateless and so for the
848:55 - name of the instance group you can
848:57 - simply call this bowtie group i'm going
849:00 - to use the same name in the description
849:01 - and under location you want to check off
849:04 - multiple zones in under region you want
849:06 - to select us east one and if you click
849:09 - on configure zones you can see here that
849:11 - you can select all the different zones
849:13 - that's available in that region that you
849:16 - choose to have your instances in and so
849:18 - i'm going to keep it under all three
849:19 - zones i'm going to scroll down here a
849:21 - little bit and under instance template
849:24 - you should see bow tie template you can
849:26 - select that you can scroll down a little
849:28 - bit more and here under minimum number
849:30 - of instances you want to set the minimum
849:32 - number of instances to 3 and under
849:35 - maximum number of instances you want to
849:37 - set that to 6 and so this is going to be
849:39 - double the amount of the minimum number
849:42 - of instances so when you're scaled out
849:44 - you should have a maximum of 6 instances
849:46 - and when you're scaled in or you have
849:49 - very low traffic you should only have
849:51 - three instances so you can scroll down
849:53 - some more and under auto healing you
849:55 - want to select the health check and
849:57 - you're going to go ahead and create a
849:59 - new health check under name you can call
850:01 - this healthy bow ties i'm going to use
850:04 - the same for the description and i'm
850:06 - going to leave the rest as its default
850:08 - and go down and click on save and
850:10 - continue i'm going to scroll down some
850:11 - more and i'm going to leave the rest as
850:13 - is and simply click on create and it's
850:16 - going to take a couple minutes here and
850:18 - so i'm going to pause the video and i'll
850:19 - be back in a flash okay and my instance
850:22 - group has been created and so to get a
850:24 - better look at it i'm going to click on
850:26 - bow tie group and i can see here that
850:28 - three instances have been created if i
850:30 - go up to vm instances you can see here
850:33 - that i have three instances but under
850:35 - instance groups because i have health
850:38 - check enabled it shows that my instances
850:40 - are unhealthy and this is because i
850:42 - still need to create a firewall rule
850:44 - that will allow google's health check
850:46 - probes to reach my vm instances and so
850:49 - you're going to go ahead and create that
850:51 - firewall rule so you can bring the
850:53 - health check status up to healthy so i'm
850:55 - going to go over to the navigation menu
850:57 - and scroll down to vpc network and go
851:00 - over to firewall here under firewall as
851:03 - expected you have the default firewall
851:05 - rules from the default created vpc
851:07 - network and so i'm going to go up to
851:09 - create firewall and you can name this
851:11 - firewall rule allow health check i'm
851:13 - going to use the same for the
851:14 - description i'm going to scroll down
851:16 - here a little bit and under targets i'm
851:18 - going to select all instances in the
851:20 - network source filter i'm going to leave
851:22 - as i p ranges and so here under source i
851:25 - p ranges i want to enter in the ip
851:28 - addresses for the google cloud health
851:30 - check probes and you can find these in
851:32 - the documentation and i will also be
851:34 - supplying them in the instructions and
851:36 - there are two sets of ip addresses that
851:38 - need to be entered and just as a note
851:40 - you don't need to know this for the exam
851:43 - but it's always a good to know if you're
851:45 - ever adding health checks to any of your
851:47 - instances i'm going to scroll down a
851:48 - little bit to protocols and ports and
851:50 - under tcp i'm going to check it off and
851:53 - put in port 80. that's pretty much all
851:55 - you have to do here so whenever you
851:57 - entered all that information in you can
851:59 - simply click on create and so now i have
852:01 - a firewall rule that will allow health
852:04 - checks to be done and so it may take a
852:06 - minute or two but if i head back on over
852:09 - to my compute engine instances and go
852:11 - over to my instance groups
852:14 - i'll be able to see that all my
852:16 - instances are now healthy and so
852:18 - whenever you're creating instance groups
852:19 - and you're applying health checks this
852:22 - firewall rule is necessary so please be
852:24 - aware okay so now that we've created our
852:26 - instance templates we've created our
852:29 - instance groups and we created a
852:31 - firewall rule in order to satisfy health
852:34 - checks we can now move on to the next
852:36 - step which is creating the load balancer
852:38 - so i'm going to go back up to the
852:39 - navigation menu and i'm going to scroll
852:42 - down to network services and over to
852:44 - load balancing and as expected there are
852:47 - no load balancers created and so
852:49 - whenever you're ready you can click on
852:50 - the big blue button and create a new low
852:52 - balancer here you have the option of
852:55 - creating an http or https load balancer
852:58 - along with a tcp load balancer or a udp
853:02 - load balancer and because we're serving
853:04 - external traffic on port 80 we're going
853:07 - to use the http load balancer so you can
853:09 - click on start configuration and i'm
853:11 - being prompted to decide between
853:13 - internet facing or internal only and
853:16 - you're going to be accepting traffic
853:17 - from the internet to your load bouncer
853:19 - so make sure that from internet to my
853:21 - vms is checked off and simply click
853:23 - continue and so next you will be
853:25 - prompted with a page with a bunch of
853:28 - configurations that you can enter and so
853:30 - we'll get to that in just a second but
853:32 - first we need to name our load balancer
853:34 - and so i'm going to call this
853:36 - bowtie dash lb for low balancer and so
853:39 - next step for your load balancer is you
853:41 - need to configure a back end so you can
853:43 - click on back end configuration and here
853:45 - you have the option of selecting from
853:48 - back-end services or back-end buckets so
853:50 - you're going to go ahead and click on
853:52 - back-end services and create a back-end
853:54 - service and here you will be prompted
853:56 - with a bunch of fields to fill out in
853:59 - order to create your back-end service
854:01 - and you can go ahead and name the
854:02 - backend service as bowtie backend
854:05 - service back-end type is going to be
854:07 - instance group and you can leave the
854:09 - protocol named port and timeout as is as
854:12 - we're going to be using http under
854:14 - instance group in new back-end if you
854:16 - select the drop-down you should see your
854:18 - available bow tie group instance group
854:21 - select that
854:22 - scroll down a little bit and under port
854:24 - numbers you can enter in port 80 and you
854:27 - can leave all the other options as
854:28 - default and simply click on done and so
854:31 - if you're ever interested you can always
854:33 - add a cache using cloud cdn now i know
854:36 - we haven't gone through cloud cdn in
854:38 - this course but just know that this is
854:40 - google's content delivery network and it
854:42 - uses google's global edge network to
854:45 - serve content closer to users and this
854:48 - accelerates your websites and your
854:50 - applications and delivers a better user
854:52 - experience for your user okay and moving
854:55 - on here under health check if i click on
854:57 - the drop down you should see healthy bow
854:59 - ties you can select that for your health
855:02 - check and so just as a note here under
855:04 - advanced configurations you can set your
855:07 - session affinity your connection
855:09 - draining timeout as well as request and
855:11 - response headers and so we don't need
855:13 - any of that for this demo and so i'm
855:15 - going to go ahead and collapse this and
855:17 - once you've finished filling in all the
855:19 - fields you can simply click on create
855:21 - okay and so you should now have your
855:23 - back end configuration and your host and
855:26 - path rules configured and so the only
855:28 - thing that's left to configure is the
855:29 - front end so you can go up and click on
855:31 - front-end configuration and you can name
855:33 - your front-end bowtie front-end service
855:36 - gonna keep the protocols http and here
855:39 - is where you would select the network
855:41 - service tier choosing either premium or
855:43 - standard and if you remember in the load
855:45 - balancing lesson in order to use this as
855:48 - a global load balancer i need to use a
855:50 - premium tier okay and we're going to
855:52 - keep this as ipv4 with an ephemeral ip
855:55 - address on port 80 so once you've
855:57 - finished configuring the front end you
855:59 - can simply click on done and you can go
856:01 - and click on review and finalize and
856:03 - this will give you a summary on your
856:05 - configuration and so i'm happy with the
856:07 - way everything's configured and if you
856:09 - are as well you can simply click on
856:11 - create and this may take a minute or two
856:14 - but it will create your low balancer
856:16 - along with your back end and your front
856:18 - end so again i'm going to pause the
856:19 - video here for just a minute and i'll be
856:21 - back before you can say cat in the hat
856:23 - okay and my load balancer has been
856:25 - created and to get a little bit more
856:27 - details i'm going to drill down into it
856:29 - and i can see here the details of my
856:31 - load balancer along with my monitoring
856:34 - and any caching but i don't have any
856:36 - caching enabled and therefore nothing is
856:38 - showing so going back to the details i
856:40 - can see here that i have a new ip
856:42 - address for my load balancer and i'll be
856:45 - getting into that in just a minute i'm
856:47 - going to go back here and i'm going to
856:48 - check out my back ends click on bow tie
856:50 - back end service and here i can see the
856:53 - requests per second as well as my
856:55 - configuration and if you do see this
856:57 - caution symbol here showing that some of
857:00 - your instances are unhealthy it's only
857:02 - because the low balancer needs time to
857:04 - do a full health check on all the
857:06 - instances in the instance group and so
857:09 - this will take some time okay and so i'm
857:11 - going to go back over and check out my
857:13 - front end and there's nothing to drill
857:15 - down into with the front end service but
857:17 - it does show me my scope the address the
857:20 - protocol
857:21 - network tier and the low balancer itself
857:24 - so this is the end of part one of this
857:26 - demo it was getting a bit long so i
857:28 - decided to break it up this would be a
857:30 - great opportunity for you to get up have
857:33 - a stretch get yourself a coffee or tea
857:36 - and whenever you're ready part two will
857:38 - be starting immediately from the end of
857:40 - part one so you can now mark this as
857:43 - complete and i'll see you in part two
857:49 - this is part two of the managing bow
857:51 - ties demo and we will be starting
857:53 - exactly where we left off in part one so
857:56 - with that being said let's dive in and
857:58 - so before you move forward you want to
858:00 - make sure that all your instances are
858:02 - considered healthy by your load balancer
858:05 - and as i can see here all my instances
858:08 - in my instance group are considered
858:09 - healthy by the load balancer and so just
858:12 - to verify this i'm going to go ahead and
858:14 - copy the i p address and you can open up
858:17 - a new tab in your browser and simply
858:19 - paste it in
858:20 - and success as you can see here managing
858:23 - the production of many bow ties can be
858:26 - automated but managing the wearer of
858:28 - them definitely cannot another fine
858:31 - message from the people at bow tie inc
858:33 - now although this is a simple web page i
858:36 - used a couple variables just to show you
858:38 - the low balancing that happens in the
858:41 - background and traffic will be load
858:42 - balanced in between all of the instances
858:45 - in the instance group so if you click on
858:47 - refresh then you should see the machine
858:49 - name and the data center change so every
858:52 - time i click refresh the traffic will be
858:54 - routed to a different instance in a
858:56 - different zone and so a simple
858:58 - simulation on how traffic is low balance
859:01 - between the different instances in their
859:03 - different zones okay so now that we've
859:05 - verified the website application i'm
859:07 - going to close down this tab and so now
859:09 - that we've created our instance template
859:12 - we've created our instance group and
859:14 - we've created our low balancer with the
859:16 - back end and front end service and it
859:18 - looks like everything seems to be
859:20 - working together nicely we're going to
859:22 - go ahead and simulate a scale out using
859:25 - auto scaling and so in order to simulate
859:27 - this we're going to do a stress test on
859:29 - one of the instances so i'm going to
859:31 - head back on over to the navigation menu
859:34 - scroll down to compute engine and here
859:36 - you can ssh into any one of these
859:38 - instances and run the stress test from
859:40 - there so i'm going to pick here the one
859:42 - at the top and so whenever you're logged
859:44 - in you can simply paste in the command
859:46 - that i've included in the instructions
859:48 - that will run the stress test and so
859:50 - this is a stress test application called
859:52 - stress that was included in the startup
859:55 - script and this again will put stress on
859:57 - the server itself and trigger a scale
860:00 - out to handle the load and it'll do this
860:02 - for 30 seconds so you can go ahead and
860:05 - hit enter and head back over to the
860:07 - console and in about a minute or two you
860:09 - should see some new instances that will
860:11 - be created by your instance group in
860:14 - order to handle the load okay and after
860:16 - about a couple minutes it's showing here
860:18 - that instances are being created and it
860:20 - will be scaling out to the maximum
860:22 - amount of instances that i've set it to
860:25 - which is six i'm going to drill down
860:27 - into this
860:28 - and yes a scale out is happening and
860:30 - some new instances are being created to
860:32 - handle the load so i'm going to give it
860:34 - just a minute here okay and as you can
860:36 - see here all the instances have been
860:38 - created they've been added to the
860:40 - instance group and all of them are
860:42 - marked as healthy and so just to verify
860:44 - that all the instances are working i'm
860:46 - going to go ahead and open up a new tab
860:48 - i'm going to plug in the ip address on
860:50 - my load balancer and i'm going to simply
860:52 - cycle through all these instances to
860:55 - make sure that all them are working and
860:57 - it looks like i have no issues and so
861:00 - now that you've simulated a scale out i
861:02 - wanted to go ahead and run a scale in
861:05 - and so i'm first going to close up these
861:06 - tabs now with regards to scaling there
861:10 - is a 10 minute stabilization period that
861:12 - cannot be adjusted for scaling and this
861:15 - is a built-in feature into google cloud
861:17 - now because i respect your time as a
861:19 - student i'm going to show you a work
861:21 - around to trigger a scale in sooner
861:24 - strictly for this demo and i also wanted
861:26 - to caution that this should never be
861:29 - done in a production or production-like
861:31 - environment you should always wait for
861:34 - the scaling to happen on its own and
861:36 - never force it this method is being used
861:39 - strictly for learning purposes to save
861:42 - you some time and so i'm going to go
861:44 - ahead to the top menu and click on
861:46 - rolling restart and replace and this
861:48 - will bring up a new page where you will
861:50 - have the option to either restart or
861:52 - replace any instances in your instance
861:55 - group and so for your purposes under
861:58 - operation make sure that you have
862:00 - restart checked off and this will
862:02 - restart all of your instances and only
862:04 - bring up the ones that are needed so i'm
862:06 - going to go ahead and click on restart
862:08 - i'm going to go back to my instance
862:09 - group console and i'm just going to give
862:11 - this a few minutes to cook and i'll be
862:13 - right back in a flash okay so it looks
862:15 - like the instance group has scaled in
862:17 - and we are now down left to three
862:19 - instances the minimum that we configured
862:22 - for our instance group and so that
862:24 - pretty much covers the managing bow ties
862:27 - demo so i wanted to congratulate you on
862:29 - making it through this demo and i hope
862:32 - that this has been extremely useful in
862:34 - excelling your knowledge on managing
862:36 - instance templates managed instance
862:38 - groups and creating load balancers with
862:41 - back-end and front-end services now this
862:44 - was a jam-packed demo and there was a
862:46 - lot to pack in with everything you've
862:48 - learned from the last few lessons and so
862:51 - just as a recap you created an instance
862:54 - template with your startup script you
862:55 - then created a new instance group with a
862:58 - health check to go with it configuring
863:00 - auto scaling for a minimum of three
863:02 - instances you then created a firewall
863:05 - rule so that the health check probes
863:07 - were able to connect to the application
863:09 - and you then created a load balancer
863:12 - with its back end and front-end service
863:14 - and verified that the website
863:16 - application was indeed up and running
863:19 - you then ran a stress test to allow a
863:21 - simulation of a scale out of your
863:23 - instance group and then simulated a
863:25 - scale in of your instance group great
863:28 - job and so now that we've completed this
863:30 - demo you want to make sure that you're
863:32 - not accumulating any unnecessary costs
863:35 - and so i'm going to go ahead and walk
863:37 - you through the breakdown of deleting
863:39 - all these resources so first you're
863:41 - going to go ahead and delete the load
863:43 - balancer go back up to the navigation
863:45 - menu and scroll down to network services
863:48 - and go over to load balancing so i'm
863:50 - going to go ahead and check off bow tie
863:52 - lb and simply go up to the top and click
863:54 - on delete it's going to ask me if i'm
863:56 - sure i want to do this i'm also going to
863:58 - select bow tie back end service and i
864:01 - can delete my load balancer and my back
864:03 - end service all at once i'm going to go
864:05 - ahead and delete load balancer and the
864:07 - selected resources
864:10 - and this should clear up within a few
864:12 - seconds okay and our load balancer has
864:14 - been deleted i'm going to just go up
864:16 - here to the back end make sure
864:18 - everything's good yeah we're all clean
864:20 - same thing with front end and so now you
864:22 - can move on to instance groups so i'm
864:24 - going to head back up to the navigation
864:26 - menu go down a compute engine and go up
864:29 - to instance groups and here you can just
864:31 - simply check off bow tie group and
864:33 - simply click on delete
864:35 - you're going to be prompted with a
864:36 - notification to make sure you want to
864:37 - delete bow tie group yes i want to
864:40 - delete and again this should take about
864:42 - a minute okay it actually took a couple
864:44 - minutes but my instance group has been
864:46 - deleted and so now i'm going to go over
864:48 - to instance templates and i'm going to
864:50 - delete my template and check off bow tie
864:52 - template and simply click delete you're
864:55 - going to get a prompt to make sure you
864:56 - want to delete your instance template
864:58 - yes you want to delete
865:00 - and success you've now deleted all your
865:03 - resources although there is one more
865:05 - resource that you will not be billed for
865:07 - but since we're cleaning everything up
865:09 - we might as well clean that up as well
865:11 - and this is the firewall rule that we
865:12 - created and go over to the navigation
865:14 - menu and scroll down to vpc network
865:18 - i'm going to go to firewall here on the
865:20 - left hand menu and here i'm going to
865:22 - check off the allow health check
865:24 - firewall rule and simply click on delete
865:27 - i'm going to get a prompt to make sure
865:28 - that i want to delete it yes you want to
865:31 - delete i'm going to quickly hit refresh
865:33 - and yes we've deleted it and so this
865:36 - concludes the end of this demo so you
865:38 - can now mark this as complete and i'll
865:40 - see you in the next one
865:45 - welcome back in this next section we
865:48 - will be focusing on google cloud's
865:50 - premier container orchestration service
865:53 - called kubernetes but before we can dive
865:56 - right into kubernetes and the benefits
865:58 - that it gives to containers you'll need
866:00 - an understanding as to what containers
866:03 - are and what value containers provide in
866:06 - this lesson i will be covering the
866:08 - difference between virtual machines and
866:10 - containers what containers are how they
866:13 - work and the value proposition they
866:16 - bring so with that being said let's dive
866:18 - in
866:19 - now for those of you who didn't know
866:22 - container technology gets its name from
866:24 - the shipping industry products get
866:26 - placed into standardized shipping
866:29 - containers which are designed to fit
866:31 - into the ship that accommodates the
866:33 - container's standard size instead of
866:36 - having various sizes of packaging now by
866:40 - standardizing this process and keeping
866:42 - the items together the container can be
866:45 - moved as a unit and it costs less to do
866:48 - it this way as well the standardization
866:51 - allows for consistency when packing and
866:54 - moving the containers placing them on
866:57 - ships and docks as well as storage no
867:00 - matter where the container is it always
867:03 - stays the same size and the contents
867:06 - stay isolated from all the other
867:08 - containers that they are stacked with
867:10 - and so now before we get into the
867:12 - details of containers i wanted to cover
867:15 - how we got here and why
867:18 - so a great way to discuss containers is
867:21 - through their comparison to virtual
867:23 - machines now as we discussed in a
867:25 - previous lesson when it comes to vms the
867:29 - systems are virtualized through a
867:31 - hypervisor that sits on top of the
867:33 - underlying host infrastructure the
867:36 - underlying hardware is virtualized so
867:39 - that multiple operating system instances
867:42 - can run on the hardware each vm runs its
867:45 - own operating system and has access to
867:49 - virtualized resources representing the
867:52 - underlying hardware due to this process
867:55 - vms come with the cost of large overhead
867:59 - in cpu memory and disk as well can be
868:03 - very large due to the fact that each vm
868:06 - needs its own individual operating
868:08 - system there also lacks standardization
868:11 - between each vm making them unique due
868:15 - to the os configuration the software
868:18 - installed and the software libraries
868:20 - thus not making it very portable to be
868:24 - able to run in any environment now when
868:27 - dealing with containers things are run
868:29 - very differently the underlying host
868:31 - infrastructure is still there but
868:33 - instead of just using a hypervisor and
868:36 - abstracting the underlying hardware
868:38 - containerization takes it one step
868:40 - further and abstracts the operating
868:43 - system
868:44 - thus
868:45 - leaving the application with all of its
868:47 - dependencies in a neatly packaged
868:51 - standardized container this is done by
868:53 - installing the operating system on top
868:56 - of the host infrastructure
868:58 - and then a separate layer on top of the
869:00 - host operating system called the
869:02 - container engine now instead of having
869:05 - their own operating system the
869:07 - containers share the operating system
869:09 - kernel with other containers
869:12 - while operating independently
869:14 - running just the application code and
869:17 - the dependencies needed to run that
869:19 - application this allows each container
869:22 - to consume very little memory or disk
869:25 - making containers very lightweight
869:28 - efficient and portable containerized
869:31 - applications can start in seconds and
869:33 - many more instances of the application
869:36 - can fit onto the machine compared to a
869:39 - vm environment this container can now be
869:42 - brought over to other environments
869:44 - running docker and able to run without
869:47 - having the worries of running into
869:50 - issues of compatibility now although
869:52 - there are a few different container
869:54 - engines out there the one that has
869:56 - received the most popularity is docker
869:59 - and this is the engine that we will be
870:01 - referring to for the remainder of this
870:03 - course now a docker image is a
870:06 - collection or stack of layers that are
870:09 - created from sequential instructions on
870:11 - a docker file so each line in the
870:14 - dockerfile is run line by line and a
870:17 - unique read-only layer is written to the
870:20 - image what makes docker images unique is
870:23 - that each time you add another
870:25 - instruction in the docker file a new
870:28 - layer is created now going through a
870:30 - practical example here shown on the
870:33 - right is a docker file and we will be
870:35 - able to map each line of code to a layer
870:38 - shown on the docker image on the left
870:41 - the line marked from
870:43 - shows the base image that the image will
870:45 - be using the example shown here shows
870:48 - that the ubuntu image version 12.04
870:52 - will be used next the run instruction is
870:55 - used which will perform a general update
870:59 - install apache 2 and output a message to
871:02 - be displayed that is written to the
871:04 - index.html file next up is the working
871:07 - directories and these are the
871:09 - environment variables set by using an
871:12 - env instruction and this will help run
871:14 - the apache runtime next layer is the
871:17 - expose instruction and this is used to
871:19 - expose the container's port on 8080 and
871:23 - lastly the command layer is an
871:26 - instruction that is executing the apache
871:29 - web server from its executable path and
871:32 - so this is a great example of how a
871:34 - docker file is broken down from each
871:36 - line to create the layers of this image
871:39 - and so just as a note here each docker
871:42 - image starts with a base image as well
871:45 - each line in a docker file creates a new
871:48 - layer that is added to the image and
871:51 - finally all the layers in a docker image
871:54 - are read only and cannot be changed
871:56 - unless the docker file is adjusted to
871:59 - reflect that change
872:00 - so now how do we get from a docker image
872:03 - to a container well a running docker
872:06 - container is actually an instantiation
872:09 - of an image so containers using the same
872:12 - image are identical to each other in
872:15 - terms of their application code and
872:17 - runtime dependencies so i could use the
872:20 - same image for multiple copies of the
872:23 - same container that have different tasks
872:26 - what makes each individual container
872:28 - different
872:29 - is that running containers include a
872:32 - writable layer on top of the read-only
872:35 - content runtime changes including any
872:38 - rights and updates to data and files are
872:41 - saved in this read write layer so in
872:43 - this example when using the command
872:46 - docker run fashionista a docker
872:49 - container will be instantiated from the
872:52 - docker image and a read write layer is
872:54 - always added on top of the read-only
872:57 - layers when a container is created
873:00 - writing any necessary files that's
873:02 - needed for the application and so just
873:05 - as a note here docker containers are
873:08 - always created from docker images and
873:10 - containers can use the same image yet
873:13 - will always have a different read write
873:15 - layer no matter the amount of containers
873:18 - running on a given host so now when your
873:21 - containers have been created you need a
873:23 - place to store them and so this is where
873:25 - a container registry comes into play now
873:28 - a container registry is a single place
873:31 - for you to store and manage docker
873:33 - images now when you create your docker
873:35 - file and then build your image
873:38 - you want to store that image in a
873:40 - central image repository whether it be a
873:43 - private one or a public one a popular
873:46 - public container registry is docker hub
873:49 - and this is a common registry where many
873:52 - open source images can be found
873:54 - including those used for the base layer
873:57 - images like the ubuntu example that i
874:00 - showed you earlier and so once you have
874:02 - your containers in a container registry
874:05 - you need to be able to run these
874:06 - containers so in order to run these
874:08 - containers you need docker hosts and
874:11 - these can consist of any machine running
874:14 - the docker engine and this could be your
874:16 - laptop
874:17 - server or you can run them in provided
874:20 - hosted cloud environments now this may
874:22 - have been a refresher for some but for
874:25 - those of you who are new to containers i
874:27 - hope this has given you a lot more
874:29 - clarity on what containers are what they
874:32 - do and the value that they bring to any
874:35 - environment and so that's pretty much
874:37 - all i wanted to cover on this short
874:39 - lesson of an introduction to containers
874:42 - so you can now mark this lesson as
874:43 - complete and let's move on to the next
874:45 - one
874:50 - welcome back so now that you've gotten
874:52 - familiar with what containers are and
874:55 - how they work i wanted to dive into
874:57 - google cloud's platform as a service
874:59 - offering for containers called google
875:02 - kubernetes engine also known as short as
875:05 - gke now although the exam goes into a
875:08 - more operational perspective with
875:10 - regards to gke knowing the foundation of
875:13 - kubernetes and the different topics of
875:16 - kubernetes is a must in order to
875:18 - understand the abstractions that take
875:20 - place with gke from regular kubernetes
875:24 - in this lesson i will be getting into
875:26 - key topics with regards to kubernetes
875:29 - and we'll be touching on the
875:30 - architecture
875:31 - components and how they all work
875:34 - together to achieve the desired state
875:36 - for your containerized workloads now
875:39 - there's a lot to get into so with that
875:41 - being said let's dive in now before i
875:45 - can get into gke i need to set the stage
875:48 - on explaining what kubernetes is put
875:51 - simply kubernetes is an orchestration
875:54 - platform for containers which was
875:57 - invented by google and eventually open
876:00 - source it is now maintained by the cncf
876:04 - short for the cloud native computing
876:06 - foundation and has achieved incredible
876:09 - widespread adoption kubernetes provides
876:11 - a platform to automate schedule and run
876:15 - containers on clusters of physical or
876:18 - virtual machines
876:19 - thus eliminating many of the manual
876:22 - processes involved in deploying and
876:25 - scaling containerized applications
876:27 - kubernetes manages the containers that
876:29 - run the applications and ensure that
876:32 - there is no downtime in a way that you
876:35 - the user can define
876:37 - for example if you define that when a
876:40 - container goes down and another
876:42 - container needs to start kubernetes
876:44 - would take care of that for you
876:46 - automatically and seamlessly kubernetes
876:49 - provides you with the framework to run
876:51 - distributed systems resiliently it takes
876:54 - care of scaling and failover for your
876:56 - application provides deployment patterns
876:59 - and allows you to manage your
877:01 - applications with tons of flexibility
877:04 - reliability and power it works with a
877:07 - range of container tools including
877:09 - docker now although this adoption was
877:11 - widespread it did come with its various
877:14 - challenges this included scaling at cd
877:17 - load balancing availability auto scaling
877:21 - networking
877:22 - rollback on faulty deployments and so
877:24 - much more
877:26 - so now google cloud has since developed
877:29 - a managed offering for kubernetes
877:31 - providing a managed environment for
877:34 - deploying managing and scaling your
877:37 - containerized applications using google
877:40 - infrastructure the gke environment
877:42 - consists of compute engine instances
877:45 - grouped together to form a cluster and
877:48 - it provides all the same benefits as
877:50 - on-premises kubernetes yet has
877:52 - abstracted the complexity of having to
877:55 - worry about the hardware and to top it
877:57 - off it has the benefits of advanced
878:00 - cluster management features that google
878:03 - cloud provides
878:04 - with things like cloud load balancing
878:07 - and being able to spread traffic amongst
878:09 - clusters and nodes node pools to
878:12 - designate subnets of nodes within a
878:14 - cluster for additional flexibility
878:16 - automatic scaling of your cluster's node
878:18 - instance count and automatic upgrades
878:21 - for your clusters node software it also
878:23 - allows you to maintain node health and
878:25 - availability with node auto repair and
878:28 - takes care of logging and monitoring
878:31 - with google cloud's operation suite for
878:33 - visibility into your cluster so as you
878:36 - can see here gke holds a lot of benefits
878:39 - when it comes to running kubernetes in
878:42 - google cloud so i wanted to take a
878:44 - moment now to dive into the cluster
878:47 - architecture and help familiarize you
878:50 - with all the components involved in a
878:52 - cluster so a cluster is the foundation
878:55 - of google kubernetes engine and
878:58 - kubernetes as a whole the kubernetes
879:00 - objects that represent your
879:02 - containerized applications all run on
879:05 - top of the cluster in gke a cluster
879:08 - consists of at least one control plane
879:11 - and multiple worker machines called
879:13 - nodes the control plane and node
879:15 - machines run the kubernetes cluster the
879:18 - control plane is responsible to
879:21 - coordinate the entire cluster and this
879:23 - can include scheduling workloads like
879:26 - containerized applications and managing
879:28 - the workload's life cycle scaling and
879:31 - upgrades the control plane also manages
879:34 - network and storage resources for those
879:36 - workloads and most importantly it
879:39 - manages the state of the cluster and
879:41 - make sure it is at the desired state now
879:44 - the nodes are the worker machines that
879:47 - run your containerized applications and
879:49 - other workloads the nodes are compute
879:51 - engine vm instances that gke creates on
879:55 - your behalf when you create a cluster
879:57 - each node is managed from the control
880:00 - plane which receives updates on each
880:03 - node's self-reported status a node also
880:06 - runs the services necessary to support
880:09 - the docker containers that make up your
880:11 - cluster's workloads these include the
880:13 - docker runtime and the kubernetes node
880:16 - agent known as the cubelet which
880:18 - communicates with the control plane and
880:21 - is responsible for starting and running
880:23 - docker containers scheduled on that node
880:27 - now diving deeper into the architecture
880:30 - there are components within the control
880:32 - plane and nodes that you should
880:34 - familiarize yourself with as these
880:36 - components are what ties the cluster
880:39 - together and helps manage the
880:41 - orchestration as well as the state now
880:44 - the control plane is the unified
880:46 - endpoint for your cluster the control
880:48 - plane's components make global decisions
880:51 - about the cluster for example scheduling
880:54 - as well as detecting and responding to
880:56 - cluster events all interactions with the
880:59 - cluster are done via kubernetes api
881:02 - calls and the control plane runs the
881:05 - kubernetes api server process to handle
881:08 - those requests you can make kubernetes
881:10 - api calls directly via http or grpc or
881:16 - can also be done indirectly by running
881:18 - commands from the kubernetes command
881:20 - line client called cubectl and of course
881:24 - you can interact with the ui in the
881:26 - cloud console the api server process is
881:30 - the hub for all communications for the
881:33 - cluster moving on to the next component
881:35 - is cube scheduler the cube scheduler is
881:38 - a component that discovers and assigns
881:41 - newly created pods to a node for them to
881:44 - run on so any new pods that are created
881:47 - will automatically be assigned to its
881:49 - appropriate node by the cube scheduler
881:52 - taking into consideration any
881:54 - constraints that are in place next up is
881:57 - the cube controller manager and this is
882:00 - the component that runs controller
882:02 - processes and is responsible for things
882:05 - like noticing and responding when nodes
882:07 - go down
882:08 - maintaining the correct number of pods
882:11 - populating the services and pods as well
882:13 - as creating default accounts and api
882:16 - access tokens for new namespaces it is
882:20 - these controllers that will basically
882:22 - look to make changes to the cluster when
882:25 - the current state does not meet the
882:27 - desired state now when it comes to the
882:29 - cloud controller manager this is what
882:32 - embeds cloud-specific control logic the
882:35 - cloud controller manager lets you link
882:37 - your cluster into any cloud providers
882:40 - api
882:41 - and separates out the components that
882:43 - interact with that cloud platform from
882:47 - components that just interact with your
882:48 - cluster the cloud controller manager
882:51 - only runs controllers that are specific
882:54 - to your cloud provider in this case
882:56 - google cloud and lastly we have fcd and
883:00 - this component is responsible to store
883:02 - the state of the cluster at cd is a
883:05 - consistent and highly available key
883:07 - value store that only interacts with the
883:11 - api server it saves all the
883:13 - configuration data along with what nodes
883:16 - are part of the cluster and what pods
883:18 - they are running so now the control
883:20 - plane needs a way to interact with the
883:22 - nodes of the cluster thus the nodes
883:25 - having components themselves for this
883:27 - communication to occur this component is
883:31 - called a cubelet and this is an agent
883:33 - that runs on each node in the cluster
883:35 - that communicates with the control plane
883:38 - it is responsible for starting and
883:40 - running docker containers scheduled on
883:43 - that node it takes a set of pod specs
883:46 - that are provided to it and ensures that
883:49 - the containers described in those pod
883:51 - specs are running and healthy and i will
883:53 - be diving into pod specs in a later
883:55 - lesson next up is cube proxy and this is
883:58 - the component that maintains network
884:00 - connectivity to the pods in a cluster
884:03 - and lastly the container runtime is the
884:06 - software that is responsible for running
884:09 - containers kubernetes supports container
884:12 - runtimes like docker and container d and
884:15 - so these are the main components in a
884:17 - cluster covering the control plane and
884:20 - nodes with regards to communication
884:23 - within the cluster now before i end this
884:26 - lesson there is one more topic i wanted
884:28 - to touch on with regards to the
884:30 - architecture of a gke cluster and that
884:33 - is the abstraction that happens and what
884:36 - exactly does gke manage with regards to
884:39 - kubernetes well gke manages all the
884:42 - control plane components the endpoint
884:45 - exposes the kubernetes api server that
884:49 - cubectl uses to communicate with your
884:52 - cluster control plane the endpoint
884:54 - exposes the kubernetes api server that
884:57 - cubectl uses to communicate with your
885:00 - cluster control plane the endpoint ip is
885:02 - displayed in cloud console and this ip
885:06 - will allow you to interact with the
885:08 - cluster when you run the command gcloud
885:10 - container clusters get dash credentials
885:14 - you see that the command gets the
885:16 - cluster endpoint as part of updating
885:18 - cubeconfig an ip address for the cluster
885:22 - is then exposed to interact with and is
885:25 - responsible for provisioning and
885:27 - managing all the infrastructure that is
885:29 - needed for the control plane gke also
885:32 - automates the kubernetes nodes by
885:35 - launching them as compute engine vms
885:37 - under the hood but still allows the user
885:40 - to change the machine type and access
885:42 - upgrade options by default google
885:45 - kubernetes engine clusters and node
885:47 - pools are upgraded automatically by
885:50 - google but you can also control when
885:52 - auto upgrades can and cannot occur by
885:55 - configuring maintenance windows and
885:57 - exclusions and just as a note a clusters
886:00 - control plane and nodes do not
886:02 - necessarily run the same version at all
886:05 - times and i will be digging more into
886:07 - that in a later lesson and so i know
886:09 - this is a lot of theory to take in but
886:12 - is as i said before a necessity to
886:15 - understanding kubernetes and gke and as
886:18 - we go further along into kubernetes and
886:20 - get into demos i promise that this will
886:23 - start to make a lot more sense and you
886:25 - will start becoming more comfortable
886:27 - with gke and the underlying components
886:30 - of kubernetes knowing kubernetes is a
886:33 - must when working in any cloud
886:35 - environment as it is a popular and
886:38 - growing technology that is not slowing
886:40 - down so knowing gke will put you in a
886:43 - really good position for your career as
886:46 - an engineer in google cloud as well will
886:49 - give you a leg up on diving into other
886:52 - cloud vendors implementation of
886:54 - kubernetes and so that's pretty much all
886:57 - i wanted to cover when it comes to
886:59 - google kubernetes engine and kubernetes
887:02 - so you can now mark this lesson as
887:03 - complete and let's move on to the next
887:05 - one
887:10 - welcome back in this lesson i will be
887:13 - covering cluster and node management in
887:15 - gke as it refers to choosing different
887:18 - cluster types for your workloads cluster
887:21 - versions
887:22 - node pools as well as upgrades and the
887:25 - many different options to choose from it
887:28 - is good to familiarize yourself with
887:30 - these options as they may be the
887:31 - deciding factor of having to keep your
887:34 - workloads highly available and your
887:36 - tolerance to risk within your
887:38 - environment so with that being said
887:40 - let's dive in now in the last lesson we
887:43 - touched on nodes and how they are the
887:46 - workers for the kubernetes cluster so
887:48 - now that you are familiar with nodes i
887:51 - wanted to touch on a concept that builds
887:53 - on it called node pools now a node pool
887:56 - is a group of nodes within a cluster
887:58 - that all have the same configuration and
888:01 - using node config specification to
888:04 - achieve this a node pool can also
888:06 - contain one or multiple nodes when you
888:09 - first create a cluster the number and
888:11 - type of nodes that you specify becomes
888:14 - the default node pool as shown here in
888:17 - the diagram then you can add additional
888:20 - custom node pools of different sizes and
888:23 - types to your cluster all nodes in any
888:25 - given node pool are identical to one
888:28 - another now custom node pools are really
888:31 - useful when you need to schedule pods
888:33 - that require more resources than others
888:36 - such as more memory more disk space or
888:39 - even different machine types you can
888:41 - create upgrade and delete node pools
888:44 - individually without affecting the whole
888:47 - cluster and just as a note you cannot
888:50 - configure a single node in any node pool
888:53 - any configuration changes affect all
888:56 - nodes in the node pool and by default
888:59 - all new node pools run the latest stable
889:01 - version of kubernetes existing node
889:04 - pools can be manually upgraded or
889:07 - automatically upgraded you can also run
889:10 - multiple kubernetes node versions on
889:12 - each node pool in your cluster update
889:14 - each node pool independently and target
889:17 - different node pools for specific
889:19 - deployments in that node now with gke
889:22 - you can create a cluster tailored to
889:25 - your availability requirements and your
889:27 - budget the types of available clusters
889:30 - include zonal both single zone or
889:33 - multi-zonal and regional zonal clusters
889:36 - have a single control plane in a single
889:39 - zone depending on what kind of
889:41 - availability you want you can distribute
889:43 - your nodes for your zonal cluster in a
889:46 - single zone or in multiple zones now
889:49 - when you decide to deploy a single zone
889:52 - cluster it again has a single control
889:55 - plane running in one zone this control
889:57 - plane manages workloads on nodes running
890:01 - in the same zone a multi-zonal cluster
890:03 - on the other hand has a single replica
890:06 - of the control plane running in a single
890:09 - zone and has nodes running in multiple
890:12 - zones during an upgrade of the cluster
890:14 - or an outage of the zone where the
890:16 - control plane runs
890:18 - workloads still run however the cluster
890:22 - its nodes and its workloads cannot be
890:24 - configured until the control plane is
890:27 - available multi-zonal clusters are
890:30 - designed to balance availability and
890:32 - cost for consistent workloads and just
890:35 - as a note the same number of nodes will
890:38 - be deployed to each selected zone and
890:41 - may cost you more than budgeted so
890:43 - please be aware and of course when
890:46 - you're looking to achieve high
890:47 - availability for your cluster regional
890:50 - clusters are always the way to go a
890:52 - regional cluster has multiple replicas
890:55 - of the control plane running in multiple
890:57 - zones within a given region nodes also
891:00 - run in each zone where a replica of the
891:03 - control plane runs because a regional
891:06 - cluster replicates the control plane and
891:08 - nodes it consumes more compute engine
891:11 - resources than a similar single zone or
891:14 - multi-zonal cluster the same number of
891:17 - nodes will be deployed to each selected
891:19 - zone and the default when selecting
891:22 - regional clusters is three zones now if
891:25 - you're dealing with more sensitive
891:27 - workloads that require more strict
891:29 - guidelines private clusters give you the
891:32 - ability to isolate nodes from having
891:35 - inbound and outbound connectivity to the
891:37 - public internet this isolation is
891:40 - achieved as the nodes have internal ip
891:43 - addresses only if you want to provide
891:45 - outbound internet access for certain
891:47 - private nodes you can use cloudnat or
891:50 - manage your own nat gateway by default
891:53 - private google access is enabled in
891:55 - private clusters and their workloads
891:57 - with limited outbound access to google
892:00 - cloud apis and services over google's
892:03 - private network in private clusters the
892:06 - control plane's vpc network is connected
892:09 - to your clusters vpc network with vpc
892:12 - network peering your vpc network
892:15 - contains the cluster nodes and a
892:17 - separate google cloud vpc network
892:20 - contains your cluster's control plane
892:22 - the control plane's vpc network is
892:24 - located in a project controlled by
892:27 - google traffic between nodes and the
892:29 - control plane is routed entirely using
892:33 - internal ip addresses the control plane
892:35 - for a private cluster has a private
892:38 - endpoint in addition to a public
892:41 - endpoint the control plane for a
892:42 - non-private cluster only has a public
892:45 - endpoint the private endpoint is an
892:48 - internal ip address in the control
892:50 - plane's vpc network the public endpoint
892:53 - is the external ip address of the
892:55 - control plane and you can control access
892:58 - to this endpoint using authorized
893:01 - networks or you can disable access to
893:04 - the public endpoint as shown here in the
893:06 - diagram you can disable the public
893:08 - endpoint and connect to your network
893:11 - using an internal ip address using cloud
893:14 - interconnect or cloud vpn and you always
893:17 - have the option of enabling or disabling
893:20 - this public endpoint now when you create
893:22 - a cluster you can choose the cluster
893:24 - specific kubernetes version or you can
893:27 - mix the versions for flexibility on
893:30 - features either way it is always
893:33 - recommended that you enable auto upgrade
893:36 - for the cluster and its nodes now when
893:38 - you have auto upgrade enabled you are
893:40 - given the choice to choose from what are
893:42 - called release channels when you enroll
893:45 - a new cluster in a release channel
893:47 - google automatically manages the version
893:49 - and upgrade cadence for the cluster and
893:52 - its node pools all channels offer
893:54 - supported releases of gke and are
893:56 - considered in general availability you
893:59 - can choose from three different release
894:01 - channels for automatic management of
894:03 - your cluster's version and upgrade
894:05 - cadence as shown here the available
894:07 - release channels are rapid regular and
894:11 - stable release channels the rapid
894:13 - release channel gets the latest
894:15 - kubernetes release as early as possible
894:18 - and be able to use new gka features the
894:20 - moment that they go into general
894:22 - availability with the regular release
894:25 - channel you have access to gke and
894:28 - kubernetes features reasonably soon
894:30 - after they are released but on a version
894:33 - that has been qualified two to three
894:35 - months after releasing in the rapid
894:37 - release channel and finally we have the
894:40 - stable release channel where stability
894:43 - is prioritized over new functionality
894:45 - changes and new versions in this channel
894:48 - are rolled out last after being
894:50 - validated two to three months in the
894:53 - regular release channel and so if you're
894:55 - looking for more direct management of
894:57 - your cluster's version choose a static
894:59 - version when you enroll a cluster in a
895:02 - release channel that cluster is upgraded
895:05 - automatically when a new version is
895:07 - available in that channel now if you do
895:09 - not use a release channel or choose a
895:12 - cluster version the current default
895:14 - version is use the default version is
895:17 - selected based on usage and real world
895:20 - performance and is changed regularly
895:23 - while the default version is the most
895:25 - mature one
895:26 - other versions being made available are
895:29 - generally available versions that pass
895:32 - internal testing and qualification
895:34 - changes to the default version are
895:36 - announced in a release note now if you
895:38 - know that you need to use a specific
895:41 - supported version of kubernetes for a
895:43 - given workload you can specify it when
895:46 - creating the cluster if you do not need
895:48 - to control the specific patch version
895:50 - you use consider enrolling your cluster
895:53 - in a release channel instead of managing
895:55 - its version directly now when it comes
895:57 - to upgrading the cluster please be aware
896:00 - that control plane and nodes do not
896:03 - always run the same version at all times
896:06 - as well a control plane is always
896:08 - upgraded before its nodes when it comes
896:11 - to zonal clusters you cannot launch or
896:14 - edit workloads during that upgrade and
896:17 - with regional clusters each control
896:19 - plane is upgraded one by one as well
896:22 - with control planes auto upgrade is
896:25 - enabled by default and this is google
896:28 - cloud's best practice now again if you
896:30 - choose you can do a manual upgrade but
896:33 - you cannot upgrade the control plane
896:36 - more than one minor version at a time so
896:39 - please be aware as well with any cluster
896:42 - upgrades maintenance windows and
896:44 - exclusions are available and so this way
896:47 - you can choose the best times for your
896:49 - upgrades and so like cluster upgrades by
896:52 - default a clusters nodes have auto
896:55 - upgrade enabled and it is recommended
896:57 - that you do not disable it again this is
897:00 - best practice by google cloud and again
897:03 - like the cluster upgrades a manual
897:05 - upgrade is available and maintenance
897:07 - windows and exclusions are available for
897:10 - all of these upgrades now when a no pool
897:13 - is upgraded gke upgrades one node at a
897:15 - time
897:16 - while a node is being upgraded gke stops
897:19 - scheduling new pods onto it and attempts
897:22 - to schedule its running pods onto other
897:25 - nodes the node is then recreated at the
897:27 - new version but using the same name as
897:30 - before this is similar to other events
897:33 - that recreate the node such as enabling
897:35 - or disabling a feature on the node pool
897:38 - and the upgrade is only complete when
897:41 - all nodes have been recreated and the
897:43 - cluster is in the desired state when a
897:46 - newly upgraded node registers with the
897:48 - control plane gke marks the node as
897:51 - schedulable upgrading a no pool may
897:54 - disrupt workloads running in that pool
897:56 - and so in order to avoid this you can
897:58 - create a new node pool with the desired
898:01 - version and migrate the workload then
898:04 - after migration you can delete the old
898:06 - node pool now surge upgrades let you
898:09 - control the number of nodes gke can
898:12 - upgrade at a time and control how
898:15 - disruptive upgrades are to your
898:16 - workloads you can change how many nodes
898:19 - gke attempts to upgrade at once by
898:22 - changing the surge upgrade parameters on
898:24 - a no pool surge upgrades reduce
898:27 - disruption to your workloads during
898:29 - cluster maintenance and also allow you
898:31 - to control the number of nodes upgraded
898:34 - in parallel surge upgrades also work
898:37 - with the cluster auto scaler to prevent
898:40 - changes to nodes that are being upgraded
898:42 - now surge upgrade behavior is determined
898:45 - by two settings max surge upgrade and
898:48 - max unavailable upgrade
898:50 - now with max surge upgrade this is the
898:52 - number of additional nodes that can be
898:55 - added to the no pool during an upgrade
898:58 - increasing max surge upgrade raises the
899:01 - number of nodes that can be upgraded
899:03 - simultaneously and when it comes to the
899:05 - max unavailable upgrade this is the
899:08 - number of nodes that can be
899:10 - simultaneously unavailable during an
899:13 - upgrade increasing max unavailable
899:15 - upgrade raises the number of nodes that
899:18 - can be upgraded in parallel so with max
899:21 - surge upgrade the higher the number the
899:23 - more parallel upgrades which will end up
899:26 - costing you more money with max
899:28 - unavailable upgrade the higher the
899:30 - number the more disruptive it is and so
899:33 - the more risk you are taking and so
899:35 - during upgrades gke brings down at most
899:39 - the sum of the max surge upgrade added
899:42 - with the max unavailable upgrade so as
899:44 - you can see here there are a slew of
899:47 - options when it comes to deciding on the
899:49 - type of cluster you want as well as the
899:51 - type of upgrades that are available
899:54 - along with when you want them to occur
899:56 - and so your deciding factor in the end
899:58 - will be the workload that you are
900:00 - running and your risk tolerance and this
900:02 - will play a big factor in keeping up
900:05 - time for your cluster as well as saving
900:07 - money in any type of environment and so
900:10 - that's pretty much all i wanted to cover
900:12 - when it comes to gke cluster and node
900:14 - management so you can now mark this
900:16 - lesson as complete and let's move on to
900:18 - the next one
900:19 - [Music]
900:23 - welcome back and in this lesson i will
900:26 - be diving into some more theory within
900:29 - kubernetes and gke this time touching on
900:32 - objects and how objects are managed pods
900:36 - are only one type of object but there
900:38 - are many other parts that are involved
900:40 - in the management of these objects and
900:43 - this is what this lesson is set out to
900:45 - teach you now there's quite a bit to
900:46 - cover here so with that being said let's
900:49 - dive in
900:50 - now kubernetes objects are persistent
900:53 - entities in kubernetes kubernetes uses
900:56 - these entities to represent the state of
900:59 - your cluster for example it can describe
901:01 - things like what containerized
901:03 - applications are running and on which
901:05 - nodes and what resources are available
901:08 - to those applications a kubernetes
901:11 - object is a record of intent once you
901:14 - create the object kubernetes will
901:16 - constantly work to ensure that object
901:18 - exists by creating an object you're
901:21 - effectively telling kubernetes what you
901:24 - want your cluster's workload to look
901:26 - like and this is your cluster's desired
901:28 - state and you've heard me speak about
901:30 - this many times before and this is what
901:33 - i was referring to now almost every
901:35 - kubernetes object includes two nested
901:38 - object fields that govern the object's
901:41 - configuration the object spec and the
901:44 - object's status for objects that have a
901:46 - spec you have to set this when you
901:48 - create the object providing a
901:50 - description of the characteristics you
901:53 - want the resource to have its desired
901:56 - state the status describes the current
901:59 - state of the object supplied and updated
902:02 - by kubernetes and its components the
902:04 - kubernetes control plane continually and
902:08 - actively manages every object's actual
902:11 - state to match the desired state you
902:13 - supplied now each object in your cluster
902:16 - has a name that is unique for that type
902:19 - of resource every kubernetes object also
902:22 - has a uid that is unique across your
902:25 - whole cluster only one object of a given
902:28 - kind can have a given name at a time
902:31 - however if you delete the object you can
902:33 - make a new object with that same name
902:36 - every object created over the whole
902:38 - lifetime of a kubernetes cluster has a
902:41 - distinct uid these distinct uids are
902:45 - also known as uuids which we discussed
902:48 - earlier on in the course now when
902:50 - creating updating or deleting objects in
902:53 - kubernetes this is done through the use
902:56 - of a manifest file where you would
902:58 - specify the desired state of an object
903:01 - that kubernetes will maintain when you
903:04 - apply the manifest each configuration
903:06 - file can contain multiple manifests and
903:09 - is common practice to do so when
903:12 - possible a manifest file is defined in
903:15 - the form of a yaml file or a json file
903:18 - and it is recommended to use yaml now in
903:21 - each yaml file for the kubernetes object
903:24 - that you want to create there are some
903:26 - required values that need to be set the
903:29 - first one is the api version and this
903:32 - defines which version of the kubernetes
903:35 - api you're using to create this object
903:38 - the kind described in this example as a
903:41 - pod is the kind of object you want to
903:44 - create next up is the metadata and this
903:47 - is the data that helps uniquely identify
903:50 - the object including a string name
903:53 - a uid and an optional namespace and the
903:56 - last required value is the spec
903:59 - and this is what state you desire for
904:02 - the object and the spec in this example
904:05 - is a container by the name of bow tie
904:07 - dash web server and is to be built with
904:10 - the latest nginx web server image as
904:12 - well as having port 80 open on the
904:15 - container now when it comes to objects
904:17 - pods are the smallest most basic
904:20 - deployable objects in kubernetes a pod
904:23 - represents a single instance of a
904:26 - running process in your cluster pods
904:28 - contain one or more containers such as
904:31 - docker containers and when a pod runs
904:34 - multiple containers the containers are
904:36 - managed as a single entity and share the
904:39 - pods resources which also includes
904:42 - shared networking and shared storage for
904:46 - their containers generally one pod is
904:49 - meant to run a single instance of an
904:51 - application on your cluster which is
904:54 - self-contained and isolated
904:56 - now although a pod is meant to run a
904:59 - single instance of your application on
905:01 - your cluster
905:02 - it is not recommended to create
905:05 - individual pods directly instead you
905:08 - generally create a set of identical pods
905:11 - called replicas to run your application
905:14 - a set of replicated pods are created and
905:17 - managed by a controller such as a
905:20 - deployment controllers manage the life
905:22 - cycle of their pods as well as
905:25 - performing horizontal scaling changing
905:27 - the number of pods is necessary now
905:30 - although you might occasionally interact
905:32 - with pods directly to debug troubleshoot
905:35 - or inspect them it's recommended that
905:38 - you use a controller to manage your pods
905:41 - and so once your pods are created they
905:43 - are then run on nodes in your cluster
905:46 - which we discussed earlier the pod will
905:48 - then remain on its node until its
905:50 - process is complete the pot is deleted
905:53 - the pod is evicted from the node due to
905:55 - lack of resources or the node fails if a
905:58 - node fails pods on the node are
906:01 - automatically scheduled for deletion now
906:04 - a single gke cluster should be able to
906:06 - satisfy the needs of multiple users or
906:09 - groups of users and kubernetes
906:12 - namespaces help different projects teams
906:15 - or customers to share a kubernetes
906:18 - cluster you can think of a namespace as
906:20 - a virtual cluster inside of your
906:23 - kubernetes cluster and you can have
906:25 - multiple namespaces logically isolated
906:28 - from each other they can help you and
906:30 - your teams with organization and
906:32 - security now you can name your
906:34 - namespaces whatever you'd like but
906:37 - kubernetes starts with four initial
906:39 - namespaces the first one is the default
906:42 - namespace and this is for objects with
906:45 - no other namespace
906:46 - so when creating new objects without a
906:49 - namespace your object will automatically
906:52 - be assigned to this namespace cube dash
906:54 - system is the next one and these are for
906:57 - objects created by kubernetes
906:59 - cube-public is created automatically and
907:03 - is readable by all users but is mostly
907:05 - reserved for cluster usage in case that
907:08 - some resources should be visible and
907:11 - readable publicly throughout the whole
907:13 - cluster and finally cube node lease is
907:16 - the namespace for the lease objects
907:19 - associated with each node which improves
907:22 - the performance of the node heartbeats
907:24 - as the cluster scales and so like most
907:27 - resources in google cloud labels are key
907:30 - value pairs that help you organize your
907:33 - resources in this case kubernetes
907:36 - objects labels can be attached to
907:38 - objects at creation time and can be
907:40 - added or modified at any time each
907:43 - object can have a set of key value
907:46 - labels defined and each key must be
907:49 - unique for a given object and labels can
907:52 - be found under metadata in your manifest
907:55 - file and so the one thing to remember
907:57 - about pods is that they are ephemeral
908:00 - they are not designed to run forever and
908:03 - when a pod is terminated it cannot be
908:05 - brought back in general pods do not
908:07 - disappear until they are deleted by a
908:10 - user or by a controller pods do not heal
908:14 - or repair themselves for example if a
908:17 - pod is scheduled on a node which later
908:19 - fails the pod is deleted as well if a
908:23 - pod is evicted from a node for any
908:25 - reason the pod does not replace itself
908:27 - and so here is a diagram of a pod life
908:30 - cycle that shows the different phases of
908:33 - its running time to give you some better
908:35 - clarity of its ephemeral nature when
908:38 - first creating the pod the pod will
908:40 - start impending and this is the pod's
908:43 - initial phase and is waiting for one or
908:46 - more of the containers to be set up and
908:49 - made ready to run this includes the time
908:51 - a pod spends waiting to be scheduled as
908:54 - well as the time spent downloading
908:57 - container images over the network once
908:59 - the pod has completed the pending phase
909:02 - it is moved on to be scheduled and once
909:04 - it is scheduled it will move into the
909:06 - running phase and this is the phase
909:08 - where the pod has been bound to a node
909:11 - and all of the containers have been
909:13 - created the running phase has at least
909:16 - one container in the pod running or is
909:19 - in the process of starting or restarting
909:22 - and once the workload is complete the
909:24 - pod will move into the succeeded phase
909:27 - and this is where all the containers in
909:29 - the pod have terminated in success and
909:32 - will not be restarted now if all the
909:34 - containers in the pod have not
909:36 - terminated successfully the pod will
909:39 - move into a failed phase and this is
909:41 - where all the containers in the pod have
909:43 - terminated and at least one container
909:46 - has terminated in failure now there's
909:48 - one more phase in the pod life cycle
909:50 - that i wanted to bring up which is the
909:52 - unknown phase and this is the state of
909:55 - the pod that could not be obtained this
909:57 - phase typically occurs due to an error
910:00 - in communicating with the node where the
910:03 - pod should be running so now when you're
910:05 - creating pods using a deployment is a
910:09 - common way to do this a deployment runs
910:12 - multiple replicas of your application
910:14 - and automatically replaces any instances
910:17 - that fail or become unresponsive
910:20 - deployments help ensure that one or more
910:23 - instances of your application are
910:25 - available to serve user requests
910:28 - deployments use a pod template which
910:31 - contains a specification for its pods
910:34 - the pod specification determines how
910:37 - each pod should look like for instance
910:39 - what applications should run inside its
910:42 - containers which volumes the pods should
910:44 - mount its labels and more and so when a
910:47 - deployments pod template is changed new
910:50 - pods are automatically created one at a
910:53 - time now i wanted to quickly bring up
910:55 - replica sets for just a moment you'll
910:58 - hear about replica sets and i wanted to
911:00 - make sure that i covered it replica sets
911:03 - ensures that a specified number of pod
911:06 - replicas are running at any given time
911:09 - however a deployment is a higher level
911:12 - concept that manages replica sets and
911:15 - provides updates to pods along with
911:18 - other features and so using deployments
911:22 - is recommended over using replica sets
911:25 - unless your workload requires it and i
911:27 - will be including a link to replica sets
911:30 - in the lesson text so speaking of
911:33 - workloads in kubernetes workloads are
911:36 - objects that set deployment rules four
911:39 - pods based on these rules kubernetes
911:42 - performs the deployment and updates the
911:44 - workload with the current state of the
911:47 - application workloads let you define the
911:50 - rules for application scheduling scaling
911:53 - and upgrading now deployments which we
911:56 - just discussed is a type of workload and
911:59 - as we've seen a deployment runs multiple
912:02 - replicas of your application and
912:04 - automatically replaces any instances
912:07 - that fail or become unresponsive
912:10 - deployments are best used
912:12 - for stateless applications another type
912:15 - of workload is stateful sets and in
912:17 - contrast to deployments these are great
912:20 - for when your application needs to
912:22 - maintain its identity and store data so
912:25 - basically any application that requires
912:29 - some sort of persistent storage daemon
912:31 - sets is another common workload that
912:34 - ensures every node in the cluster runs a
912:36 - copy of that pod and this is for use
912:38 - cases where you're collecting logs or
912:41 - monitoring node performance now jobs is
912:44 - a workload that launches one or more
912:46 - pods and ensures that a specified number
912:49 - of them successfully terminate jobs are
912:52 - best used to run a finite task to
912:54 - completion as opposed to managing an
912:57 - ongoing desired application state and
913:00 - cron jobs are similar to jobs however
913:03 - cron jobs runs to completion on a
913:06 - cron-based schedule and so the last
913:08 - workload that i wanted to cover are
913:10 - config maps and these store general
913:13 - configuration information and so after
913:15 - you upload a config map any workload can
913:18 - reference it as either an environment
913:20 - variable or a volume mount and so just
913:23 - as a note config maps are not meant to
913:26 - store sensitive data if you're planning
913:28 - to do this please use secrets now i know
913:31 - this lesson has been extremely heavy in
913:34 - theory but these are fundamental
913:36 - concepts to know when dealing with
913:38 - kubernetes and gke as well as the
913:41 - objects that it supports so i recommend
913:43 - that if you need to go back and review
913:46 - this lesson if things aren't making
913:48 - sense so that you can better understand
913:50 - it as these concepts all tie in together
913:54 - and will come up in the exam and so
913:56 - that's pretty much all i wanted to cover
913:58 - in this lesson on pods and object
914:01 - management within gke so you can now
914:04 - mark this lesson as complete
914:06 - and let's move on to the next one
914:08 - [Music]
914:12 - welcome back and in this lesson i'm
914:14 - going to be diving into kubernetes
914:16 - services now services are a major
914:19 - networking component when it comes to
914:21 - working in kubernetes and can play a
914:23 - major factor when it comes to deciding
914:26 - on how you want to route your traffic
914:28 - within your kubernetes cluster as well
914:31 - in my experience services show up on the
914:34 - exam and so an understanding of how they
914:36 - work and the different types to use are
914:39 - essential to understanding the big
914:41 - picture of kubernetes this lesson will
914:44 - cover an overview on what services are
914:47 - what they do and the different types
914:49 - that are available along with their use
914:51 - cases now there's a lot to cover here so
914:54 - with that being said let's dive in now
914:56 - as i had discussed earlier kubernetes
914:59 - pods are ephemeral pods are created and
915:02 - destroyed to match the state of your
915:04 - cluster so these resources are never
915:07 - permanent a perfect example of this is
915:09 - by using a deployment object so you can
915:12 - create and destroy pods dynamically now
915:15 - when it comes to networking in
915:16 - kubernetes each pod gets its own ip
915:19 - address however in a deployment a pod
915:22 - that is running once destroyed will be
915:25 - recreated with a new ip address and
915:28 - there is no real way to keep track of
915:30 - these i p addresses for communication as
915:33 - they change very frequently and this is
915:36 - where services come into play now a
915:39 - service is an abstraction in the sense
915:41 - that it is not a process that listens on
915:44 - some network interface
915:46 - a service can be defined as a logical
915:48 - set of pods an abstraction on top of the
915:51 - pod which provides a single persistent
915:54 - ip address and dns name by which pods
915:57 - can be accessed it allows for routing
916:00 - external traffic into your kubernetes
916:02 - cluster and used inside your cluster for
916:05 - more intelligent routing with services
916:08 - it is also very easy to manage load
916:10 - balancing configuration for traffic
916:13 - between replicas it helps pods scale
916:16 - quickly and easily as the service will
916:19 - automatically handle the recreation of
916:22 - pods and their new ip addresses the main
916:25 - goal of services in kubernetes is to
916:27 - provide persistent access to its pods
916:31 - without the necessity to look for a
916:33 - pod's ip
916:34 - each time when the pod is recreated and
916:37 - again services also allow for external
916:41 - access from users to the applications
916:44 - inside the cluster without having to
916:46 - know the ip address of the individual
916:49 - pod in order to reach that application
916:52 - now in order for a service to route
916:54 - traffic to the correct pod in the
916:56 - cluster there are some fields in the
916:58 - manifest file that will help determine
917:00 - the end points on where traffic should
917:02 - be routed shown here on the right is the
917:05 - deployment manifest for reference and on
917:08 - the left is the services manifest now as
917:10 - you can see here in the service manifest
917:13 - on the left the kind is clearly defined
917:16 - as service under metadata is the name of
917:19 - the service and this will be the dns
917:21 - name of the service when it is created
917:24 - so when it comes to the spec there is a
917:26 - field here called a selector and this is
917:29 - what defines what pods should be
917:32 - included in the service and it is the
917:34 - labels under the selector that define
917:37 - which pods and labels are what we
917:39 - discussed in the last lesson as
917:41 - arbitrary key value pairs so any pod
917:44 - with these matching labels is what will
917:46 - be added to the service as shown here in
917:49 - the deployment file this workload will
917:52 - be a part of the service and its labels
917:54 - match that of the selector in the
917:56 - services file for type this is the type
918:00 - of service that you will want to use in
918:02 - this example type cluster ip is used but
918:06 - depending on the use case you have a few
918:08 - different ones to choose from now at the
918:11 - bottom here is a list of port
918:13 - configurations protocol being the
918:15 - network protocol to use with the port
918:17 - port being the port that incoming
918:19 - traffic goes to and finally the target
918:22 - port which is the port on the pod that
918:25 - traffic should be sent to and this will
918:27 - make more sense as we go through the
918:29 - upcoming diagrams so touching on
918:32 - selectors and labels for a moment
918:34 - kubernetes has a very unique way of
918:36 - routing traffic and when it comes to
918:38 - services it's not any different services
918:42 - select pods based on their labels now
918:45 - when a selector request is made to the
918:47 - service it selects all pods in the
918:50 - cluster matching the key value pair
918:52 - under the selector it chooses one of the
918:55 - pods if there are more than one with the
918:57 - same key value pair and forwards the
918:59 - network request to it and so here in
919:02 - this example you can see that the
919:04 - selector specified for the service has a
919:07 - key value pair of app inventory you can
919:10 - see the pod on node 1 on the left holds
919:13 - the label of app inventory as well which
919:16 - matches the key value pair of the
919:18 - selector and so traffic will get routed
919:21 - to that pod because of it if you look at
919:23 - the label for the pod in node 2 on the
919:25 - right the label does not match that of
919:28 - the selector and so it will not route
919:30 - traffic to that pod and so to sum it up
919:33 - the label on the pod matching the
919:35 - selector in the service determines where
919:38 - the network request will get routed to
919:41 - and so now i will be going through the
919:43 - many different service types that are
919:45 - available for routing network traffic
919:48 - within gke starting with cluster ip
919:52 - now a cluster ip service is the default
919:55 - kubernetes service it gives you a
919:57 - service inside your cluster that other
919:59 - apps inside your cluster can access the
920:02 - service is not exposed outside the
920:04 - cluster but can be addressed from within
920:06 - the cluster when you create a service of
920:09 - type cluster ip kubernetes creates a
920:12 - stable ip address that is accessible
920:15 - from nodes in the cluster clients in the
920:18 - cluster call the service by using the
920:21 - cluster ip address and the port value
920:23 - specified in the port field of the
920:26 - service manifest the request is
920:28 - forwarded to one of the member pods on
920:31 - the port specified in the target port
920:33 - field and just as a note this ip address
920:37 - is stable for the lifetime of the
920:39 - service so for this example a client
920:42 - calls the service at 10.176
920:48 - on tcp port 80. the request is forwarded
920:52 - to one of the member pods on tcp port
920:55 - 80. note that the member pod must have a
920:58 - container that is listening on tcp port
921:01 - 80. if there is no container listening
921:03 - on port 80 clients will see a message
921:05 - like fail to connect or this site can't
921:08 - be reached think of the case when you
921:11 - have a dns record that you don't want to
921:13 - change and you want the name to resolve
921:16 - to the same ip address or you merely
921:18 - want a static ip address for your
921:20 - workload this would be a great use case
921:23 - for the use of the cluster ip service
921:26 - now although the service is not
921:27 - accessible by network requests outside
921:30 - of the cluster
921:31 - if you need to connect to the service
921:33 - you can still connect to it with the
921:35 - cloud sdk or cloud shell by using the
921:39 - exposed ip address of the cluster and so
921:41 - i wanted to take a moment to show you
921:43 - what a cluster ip manifest actually
921:46 - looks like and i will be going through
921:48 - the manifest for each service type for
921:51 - you to familiarize yourself with we
921:53 - first have the name of the service which
921:55 - is cluster ip dash service
921:57 - we then have the label used for the
922:00 - selector which is the key value pair of
922:02 - app inventory and then we have the
922:05 - service type which is cluster ip and we
922:08 - have the port number exposed internally
922:10 - in the cluster which is port 80 along
922:13 - with the target port that containers are
922:16 - listening on which again is port 80. and
922:18 - so the next service type we have is node
922:21 - port
922:22 - so when you create a service of type
922:24 - node port you specify a node port value
922:28 - the node port is a static port and is
922:30 - chosen from a pre-configured range
922:33 - between 30 000 and 32
922:36 - 760
922:38 - you can specify your own value within
922:40 - this range but please note that any
922:43 - value outside of this range will not be
922:45 - accepted by kubernetes as well if you do
922:48 - not choose a value a random value within
922:51 - the range specified will be assigned
922:54 - once this port range has been assigned
922:55 - to the service then the service is
922:58 - accessible by using the ip address of
923:01 - any node along with the no port value
923:04 - the service is then exposed on a port on
923:07 - every node in the cluster the service
923:10 - can then be accessed externally at the
923:12 - node ip along with the node port when
923:15 - using node port services you must make
923:17 - sure that the selected port is not
923:19 - already open on your nodes and so just
923:22 - as a note the no port type is an
923:24 - extension of the cluster i p type so a
923:27 - service of type node port naturally has
923:31 - a cluster i p address and so this method
923:34 - isn't very secure as it opens up each
923:36 - node to external entry as well this
923:40 - method relies on knowing the ip
923:42 - addresses of the nodes which could
923:44 - change at any time and so going through
923:47 - the manifest of type node port service
923:49 - we start off with the name of the
923:51 - service which is node port dash service
923:54 - the label used for the selector which
923:56 - uses the key value pair of app inventory
924:00 - the type which is node port and notice
924:02 - the case sensitivity here which you will
924:05 - find in most service types along with
924:07 - the port number exposed internally in
924:09 - the cluster which is port 80 and again
924:12 - the port that the containers are
924:14 - listening on which is the target port
924:16 - which is port 80 as well and lastly and
924:19 - most importantly we have the no port
924:22 - value which is marked as you saw in the
924:24 - diagram earlier as port
924:27 - 32002 the next service type we have up
924:30 - is low balancer and this service is
924:33 - exposed as a load balancer in the
924:35 - cluster low balancer services will
924:38 - create an internal kubernetes service
924:41 - that is connected to a cloud provider's
924:43 - load balancer and in this case google
924:46 - cloud this will create a static publicly
924:49 - addressable ip address and a dns name
924:52 - that can be used to access your cluster
924:54 - from an external source the low balancer
924:57 - type is an extension of the no port type
925:00 - so a service of type load balancer
925:02 - naturally has a cluster ip address if
925:05 - you want to directly expose a service
925:08 - this is the default method all traffic
925:10 - on the port you specify
925:12 - will be forwarded to the service there
925:14 - is no filtering or routing and it means
925:17 - you can send many different types of
925:19 - traffic to it like http https tcp or udp
925:24 - and more the downside here is that for
925:27 - each service you expose with a low
925:30 - balancer you pay for that load balancer
925:32 - and so you can really rack up your bill
925:35 - if you're using multiple load balancers
925:37 - and shown here is the manifest for type
925:40 - load balancer it shows the name of the
925:42 - service load balancer dash service the
925:45 - label which is used for the selector
925:47 - which is the key value pair of app
925:49 - inventory
925:51 - the service type which is low balancer
925:54 - again notice the case sensitivity along
925:56 - with the port and the target port which
925:59 - are both port 80. and so this is the end
926:01 - of part one of this lesson it was
926:03 - getting a bit long so i decided to break
926:06 - it up this would be a great opportunity
926:08 - for you to get up and have a stretch get
926:10 - yourself a coffee or tea and whenever
926:13 - you're ready part two will be starting
926:15 - immediately from the end of part one so
926:18 - go ahead and mark this as complete and
926:20 - i'll see you in the next one
926:21 - [Music]
926:25 - welcome back this is part two of the
926:28 - kubernetes services lesson and we're
926:30 - going to continue immediately from the
926:32 - end of part one so whenever you're ready
926:34 - let's dive in and so the next service
926:36 - type we have is multiport services
926:39 - now for some services there is the need
926:42 - to expose more than one port kubernetes
926:45 - lets you configure multiple port
926:46 - definitions on a service object so when
926:49 - using multiple ports for a service you
926:52 - must give all your ports names and if
926:54 - you have multiple service ports these
926:56 - names must be unique in this example if
926:59 - a client calls the service at 10.176.1
927:05 - on tcp port 80 the request is forwarded
927:09 - to a member pod on tcp port 80 on either
927:12 - node 1 or node 2. but if a client calls
927:15 - the service at 10.176.133.7
927:20 - on tcp port 9752 the request is
927:24 - forwarded to the pod on tcp port 9752
927:28 - that resides on node 1. each member pod
927:31 - must have a container listening on tcp
927:34 - port 80 and a container listening on tcp
927:37 - port 9752 this could be a single
927:40 - container with two threads or two
927:43 - containers running in the same pod and
927:45 - of course as shown here is a manifest
927:48 - showing the multi-port services
927:50 - the name of the service
927:52 - the label used for the selector
927:55 - as well as the service type the port
927:57 - node exposed internally for each
927:59 - separate workload as well as the port
928:02 - that containers are listening on for
928:05 - each workload as well and as you saw
928:07 - before nginx was using target port 80
928:11 - where appy was using port 9752 moving on
928:15 - to another service type is external name
928:18 - now a service of type external name
928:20 - provides an internal alias for an
928:23 - external dns name internal clients make
928:26 - requests using the internal dns name and
928:29 - the requests are redirected to the
928:31 - external name when you create a service
928:34 - kubernetes creates a dns name that
928:36 - internal clients can use to call the
928:39 - service in this example the internal dns
928:42 - name is bowtie.sql when an internal
928:45 - client makes a request to the internal
928:47 - dns name of bowtie.sql the request gets
928:51 - redirected to bowtie.sql2
928:54 - dot bow tie inc dot private the external
928:57 - name service type is a bit different
928:59 - than other service types as it's not
929:02 - associated with a set of pods or an ip
929:05 - address it is a mapping from an internal
929:08 - dns name to an external dns name this
929:11 - service does a simple cname redirection
929:14 - and is a great use case for any external
929:17 - service that resides outside of your
929:20 - cluster and again here is a view of a
929:22 - manifest for type external name here
929:26 - showing the internal dns name along with
929:29 - the external dns name redirect and
929:32 - moving on to the last service type we
929:34 - have the headless service type now
929:36 - sometimes you don't need or want low
929:38 - balancing and a single service ip in
929:41 - this case you can create headless
929:43 - services by specifying none as the
929:46 - service type in the manifest file this
929:48 - option also allows you to choose other
929:51 - service discovery mechanisms without
929:53 - being tied to kubernetes implementation
929:56 - applications can still use a
929:58 - self-registration pattern with this
930:00 - service and so a great use case for this
930:02 - is when you don't need any low balancing
930:04 - or routing you only need the service to
930:07 - patch the request to the back end pod no
930:10 - ips needed headless service is typically
930:13 - used with stateful sets where the name
930:16 - of the pods are fixed this is useful in
930:18 - situations like when you're setting up a
930:20 - mysql cluster where you need to know the
930:23 - name of the master and so here is a
930:26 - manifest for the headless service again
930:29 - the service type is marked as none and
930:32 - so to sum it up kubernetes services
930:34 - provides the interfaces through which
930:36 - pods can communicate with each other
930:39 - they also act as the main gateway for
930:41 - your application services use selectors
930:44 - to identify which pods they should
930:46 - control they expose an ip address and a
930:49 - port that is not necessarily the same
930:51 - port at which the pod is listening and
930:53 - services can expose more than one port
930:57 - and can also route traffic to other
930:59 - services external ip addresses or dns
931:02 - names services make it really easy to
931:05 - create network services in kubernetes
931:08 - each service can be backed with as many
931:10 - pods as needed without having to make
931:13 - your code aware of how each service is
931:15 - backed also please note that there are
931:18 - many other features and use cases within
931:20 - the services that have been mentioned
931:22 - that i've not brought up i will also
931:24 - include some links in the lesson text
931:27 - for those who are interested in diving
931:29 - deeper into services this lesson was to
931:32 - merely summarize the different service
931:34 - types and knowing these service types
931:36 - will put you in a great position on the
931:38 - exam for any questions that cover
931:41 - services within gke now i know this has
931:44 - been another lesson that's been
931:46 - extremely heavy in theory and has been a
931:49 - tremendous amount to take in but not to
931:52 - worry next up is a demo that will put
931:55 - all this theory into practice and we'll
931:57 - be going ahead and building a cluster
931:59 - along with touching on much of the
932:01 - components discussed within the past few
932:04 - lessons and so that's pretty much all i
932:06 - wanted to cover when it comes to
932:08 - kubernetes service types so you can now
932:10 - mark this lesson as complete and
932:12 - whenever you're ready join me in the
932:14 - console
932:15 - [Music]
932:19 - welcome back
932:20 - in this lesson i'll be going over
932:22 - ingress for gke
932:24 - an object within gke that defines rules
932:28 - for routing traffic to specific services
932:32 - ingress is a well-known topic that comes
932:35 - up in the exam
932:36 - as well as being a common resource that
932:39 - is used in many gke clusters that you
932:42 - will see in most environments
932:45 - something that you will get very
932:46 - familiar with while diving deeper into
932:49 - more complex environments
932:52 - so whenever you're ready let's dive in
932:55 - now in gke an ingress object defines
932:58 - rules for routing http and https traffic
933:03 - to applications running in a cluster
933:06 - an ingress object is associated with one
933:10 - or more service objects
933:12 - each of which is associated with a set
933:14 - of pods
933:16 - when you create an ingress object the
933:18 - gke ingress controller
933:21 - creates a google cloud
933:23 - http or https load balancer and
933:27 - configures it according to the
933:29 - information in the ingress and its
933:32 - associated services
933:34 - gke ingress is a built-in
933:37 - and managed ingress controller
933:39 - this controller implements ingress
933:42 - resources as google cloud load balancers
933:46 - for http and https workloads in gke
933:51 - also the load balancer is given a stable
933:54 - ip address that you can associate with a
933:57 - domain name each external http
934:00 - and https load balancer or internal http
934:05 - or https load balancer uses a single url
934:09 - map
934:10 - which references one or more back-end
934:13 - services
934:14 - one back-end service corresponds to each
934:18 - service referenced by the ingress in
934:21 - this example assume that you have
934:23 - associated the load balancers ip address
934:26 - with the domain name bowtieinc.co
934:29 - when a client sends a request to
934:31 - bowtieinc.co
934:33 - the request is routed to a kubernetes
934:36 - service named products on port 80. and
934:39 - when a client sends a request to
934:40 - bowtieinc.co
934:42 - forward slash discontinued the request
934:45 - is routed to a kubernetes service named
934:48 - discontinued on port 21337
934:52 - ingress is probably the most powerful
934:55 - way to expose your services but can also
934:58 - be very complex
935:00 - as there are also many types of ingress
935:03 - controllers to choose from
935:06 - along with plugins for ingress
935:08 - controllers
935:09 - ingress is the most useful and cost
935:12 - effective if you want to expose
935:15 - multiple services under the same ip
935:18 - address
935:19 - as you only pay
935:20 - for one load balancer if you are using
935:23 - the native gcp integration
935:26 - and comes with a slew of features
935:30 - and so shown here is the ingress
935:32 - manifest which is a bit different from
935:34 - the other manifest that you've seen
935:37 - as it holds rules for different paths
935:40 - explain in the previous diagram in the
935:43 - manifest shown here one path directs all
935:46 - traffic
935:47 - to the product's service name
935:50 - while the other path redirects traffic
935:53 - from discontinued to the back end
935:55 - service name of discontinued
935:58 - and note that each of these service
936:00 - names have their own independent
936:03 - manifest
936:04 - as it is needed to create the service
936:07 - and are referenced within the ingress
936:09 - manifest
936:10 - so the more rules you have for different
936:13 - paths or ports the more services you
936:16 - will need
936:17 - now i wanted to touch on network
936:20 - endpoint groups or any g's for short for
936:23 - just a second
936:24 - now this is a configuration object that
936:27 - specifies a group of back-end endpoints
936:30 - or services
936:32 - negs are useful
936:34 - for container native load balancing
936:37 - where each container can be represented
936:40 - as an endpoint to the load balancer the
936:43 - negs are used to track pod endpoints
936:46 - dynamically so the google low balancer
936:50 - can route traffic to its appropriate
936:52 - back ends
936:54 - so traffic is low balanced from the load
936:56 - balancer directly to the pod ip
937:00 - as opposed to traversing the vm ip and
937:04 - coupe proxy networking in these
937:06 - conditions
937:07 - services will be annotated automatically
937:10 - indicating that a neg
937:12 - should be created to mirror the pod ips
937:16 - within the service the neg is what
937:19 - allows compute engine load balancers to
937:22 - communicate directly with pods the
937:25 - diagram shown here is the ingress to
937:28 - compute engine resource mappings of the
937:31 - manifest that you saw earlier where the
937:34 - gke ingress controller
937:36 - deploys and manages compute engine low
937:39 - balancer resources based on the
937:42 - ingressed resources
937:43 - that are deployed in the cluster
937:46 - now touching on health checks for just a
937:48 - minute if there are no specified health
937:51 - check parameters for a corresponding
937:54 - service
937:55 - using a back-end custom resource
937:57 - definition
937:58 - a set of default and inferred parameters
938:01 - are used health check parameters for a
938:04 - back-end service should be explicitly
938:07 - defined by creating a back-end config
938:11 - custom resource definition for the
938:13 - service and this should be done if
938:15 - you're using anthos
938:16 - a backend config custom resource
938:19 - definition should also be used if you
938:22 - have more than one container in the
938:24 - serving pods as well if you need control
938:27 - over the port that's used for the low
938:30 - balancers health checks now you can
938:33 - specify the backend services health
938:35 - check parameters
938:37 - using the health check parameter of a
938:39 - back-end config custom resource
938:42 - definition referenced by the
938:44 - corresponding service
938:46 - this gives you more flexibility and
938:48 - control
938:49 - over health checks for a google cloud
938:52 - external http or https load balancer or
938:58 - internal http or https load balancer
939:02 - created by an ingress
939:04 - and lastly i wanted to touch on ssl
939:07 - certificates and there are three ways to
939:09 - provide ssl certificates to an http or
939:14 - https load balancer the first way is
939:17 - google managed certificates
939:19 - and these are provisioned deployed
939:22 - renewed
939:23 - and managed for your domains and just as
939:26 - a note
939:27 - managed certificates do not support
939:30 - wildcard domains the second way to
939:33 - provide ssl certificates is through
939:36 - self-managed certificates that are
939:38 - shared with google cloud you can
939:40 - provision your own ssl certificate
939:43 - and create a certificate resource
939:46 - in your google cloud project you can
939:48 - then list the certificate resource in an
939:51 - annotation on an ingress to create an
939:55 - http or https load balancer that uses
939:59 - the certificate and the last way to
940:02 - provide ssl certificates is through
940:04 - self-managed certificates as secret
940:07 - resources so you can provision your own
940:10 - ssl certificate and create a secret to
940:13 - hold it you can then refer to the secret
940:16 - as an ingress specification to create an
940:20 - http or https load balancer that uses
940:24 - this certificate and just as a note you
940:27 - can specify multiple certificates in an
940:30 - ingress manifest the load balancer
940:33 - chooses a certificate if the common name
940:36 - in the certificate
940:37 - matches the host name
940:39 - used in the request and so that pretty
940:42 - much covers
940:43 - all the main topics in this short lesson
940:46 - on ingress for gke
940:49 - so you can now mark this lesson as
940:50 - complete
940:51 - and let's move on to the next one
940:53 - [Music]
940:57 - welcome back
940:59 - in this lesson i'll be going over gke
941:02 - storage options now kubernetes currently
941:06 - offers a slew of different storage
941:08 - options
941:09 - and is only enhanced by the added
941:12 - features available in google cloud for
941:15 - gke we'll also be getting into the
941:18 - different abstractions that kubernetes
941:21 - offers to manage storage
941:23 - and how they can be used for different
941:26 - types of workloads
941:27 - now there's quite a bit to go over here
941:29 - so with that being said let's dive in
941:32 - now as i stated before there are several
941:35 - storage options for applications running
941:38 - on gke
941:40 - the choices vary in terms of flexibility
941:43 - and ease of use
941:45 - google cloud offers several storage
941:47 - options that can be used for your
941:50 - specific workload
941:52 - kubernetes also provides storage
941:54 - abstractions which i will be getting
941:56 - into in just a bit the easiest storage
941:59 - options are google cloud's managed
942:02 - storage products if you need to connect
942:04 - a database to your cluster you can
942:06 - consider
942:08 - using cloud sql datastore or cloud
942:11 - spanner and when it comes to object
942:13 - storage cloud storage would be an
942:15 - excellent option to fill the gap file
942:18 - store is a great option for when your
942:20 - application requires
942:22 - managed network attached storage and if
942:25 - your application requires block storage
942:28 - the best option is to use persistent
942:31 - disks
942:32 - and can be provisioned manually or
942:34 - provisioned dynamically through
942:36 - kubernetes now i wanted to first start
942:39 - off with kubernetes storage abstractions
942:42 - but in order to understand kubernetes
942:44 - storage abstractions i wanted to take a
942:47 - moment to explain how storage is mounted
942:50 - in the concept of docker now docker has
942:52 - a concept of volumes
942:54 - though it is somewhat looser and less
942:57 - managed than kubernetes
942:59 - a docker volume is a directory on disk
943:02 - or in another container
943:04 - docker provides volume drivers but the
943:07 - functionality is somewhat limited a
943:10 - docker container has a writable layer
943:13 - and this is where the data is stored by
943:15 - default
943:16 - making the data ephemeral and so data is
943:19 - not persisted when the container is
943:22 - removed so storing data inside a
943:25 - container is not always recommended now
943:28 - there are three ways to mount data
943:30 - inside a docker container the first way
943:33 - is a docker volume and sits inside the
943:36 - docker area within the host's file
943:39 - system and can be shared amongst other
943:42 - containers
943:44 - this volume is a docker object and is
943:46 - decoupled from the container
943:49 - they can be attached and shared across
943:51 - multiple containers as well bind
943:54 - mounting is the second way to mount data
943:56 - and is coming directly from the host's
943:59 - file system
944:00 - bind mounts are great for local
944:03 - application development
944:05 - yet cannot be shared across containers
944:08 - and the last way to mount data is by
944:10 - using temp-fs
944:12 - and is stored in the host's memory this
944:15 - way is great for ephemeral data and
944:17 - increases performance as it no longer
944:20 - lies in the container's writable layer
944:23 - now with kubernetes storage abstractions
944:27 - file system and block based storage are
944:29 - provided to your pods but are different
944:32 - than docker in nature volumes are the
944:35 - basic storage unit in kubernetes that
944:38 - decouples the storage from the container
944:41 - and tie it to the pod and not the
944:43 - container like in docker a regular
944:46 - volume simply called volume is basically
944:49 - a directory that the containers in a pod
944:53 - have access to the particular volume
944:55 - type used is what will determine its
944:58 - purpose
944:59 - some volume types are backed by
945:01 - ephemeral storage
945:03 - like empty dir
945:05 - config map and secrets and these volumes
945:08 - do not persist after the pod ceases to
945:11 - exist
945:12 - volumes are useful for caching temporary
945:16 - information
945:17 - sharing files between containers
945:19 - or to load data into a pod other volume
945:23 - types are backed by durable storage and
945:26 - persist beyond the lifetime of a pod
945:29 - like persistent volumes and persistent
945:32 - volume claims a persistent volume is a
945:35 - cluster resource
945:37 - that pods can use for durable storage a
945:40 - persistent volume claim
945:42 - can be used to dynamically provision a
945:45 - persistent volume
945:46 - backed by persistent disks persistent
945:49 - volume claims can also be used to
945:52 - provision other types of backing storage
945:56 - like nfs
945:57 - and i will be getting more into
945:59 - persistent volumes and persistent volume
946:02 - claims in just a bit
946:05 - now as you saw in docker on disk files
946:08 - in a container are the simplest place
946:11 - for an application to write data but
946:14 - files are lost when the container
946:16 - crashes or stops for any other reason
946:20 - as well as being unaccessible to other
946:22 - containers running in the same pod in
946:26 - kubernetes the volume source declared in
946:29 - the pod specification determines how the
946:33 - directory is created
946:35 - the storage medium used
946:37 - and the directory's initial contents
946:40 - a pod specifies what volumes it contains
946:44 - and the path where containers mount the
946:46 - volume ephemeral volume types
946:49 - live the same amount of time as the pods
946:52 - they are connected to these volumes are
946:55 - created when the pod is created and
946:58 - persist through container restarts only
947:01 - when the pod terminates or is deleted
947:04 - are the volumes terminated as well other
947:07 - volume types are interfaces to durable
947:11 - storage that exist independently of a
947:14 - pod like ephemeral volumes data in a
947:17 - volume backed by durable storage
947:20 - is preserved when the pod is removed
947:23 - the volume is merely unmounted and the
947:26 - data can be handed off to another pod
947:30 - now volumes differ in their storage
947:32 - implementation
947:34 - and their initial contents you can
947:36 - choose the volume source that best fits
947:39 - your use case
947:40 - and i will be going over some common
947:42 - volume sources
947:43 - that are used and you will see
947:46 - in many gke implementations the first
947:49 - volume that i want to bring up is empty
947:52 - dir
947:53 - now an empty dir volume provides an
947:56 - empty directory that containers in the
947:59 - pod can read and write from when the pod
948:02 - is removed from a node for any reason
948:05 - the data in the empty dir is deleted
948:08 - forever an empty dir volume is stored on
948:11 - whatever medium is backing the node
948:14 - which might be a disk
948:16 - ssd or network storage
948:19 - empty der volumes are useful for scratch
948:22 - space and sharing data between multiple
948:26 - containers in a pod
948:28 - the next type of volume that i wanted to
948:30 - go over is config map
948:32 - and config map is a resource that
948:34 - provides a way to inject configuration
948:37 - data into pods
948:39 - the data stored in a config map object
948:42 - can be referenced in a volume of type
948:45 - config map
948:47 - and then consumed through files running
948:50 - in a pod the next volume type is secret
948:53 - and a secret volume is used to make
948:55 - sensitive data such as passwords oauth
948:59 - tokens and ssh keys
949:01 - available to applications
949:04 - the data stored in a secret object can
949:06 - be referenced in a volume of type secret
949:10 - and then consumed through files running
949:13 - in a pod
949:14 - next volume type is downward api and
949:17 - this volume makes downward api data
949:20 - available to applications
949:22 - so this data includes information
949:25 - about the pod and container in which an
949:28 - application is running in
949:30 - an example of this would be to expose
949:33 - information about the pods namespace and
949:37 - ip address to applications and the last
949:40 - volume type that i wanted to touch on is
949:42 - persistent volume claim now a persistent
949:45 - volume claim volume can be used to
949:48 - provision durable storage so that they
949:51 - can be used by applications a pod uses a
949:54 - persistent volume claim
949:56 - to mount a volume that is backed by this
949:59 - durable storage and so now that i've
950:02 - covered volumes i wanted to go into a
950:05 - bit of detail about persistent volumes
950:08 - persistent volume resources are used to
950:11 - manage durable storage in a cluster in
950:14 - gke a persistent volume
950:17 - is typically backed by a persistent disk
950:20 - or file store can be used as an nfs
950:22 - solution unlike volumes the persistent
950:26 - volume life cycle is managed by
950:28 - kubernetes and can be dynamically
950:31 - provisioned
950:32 - without the need to manually create and
950:35 - delete the backing storage persistent
950:38 - volume resources are cluster resources
950:41 - that exist independently of pods and
950:44 - continue to persist as the cluster
950:46 - changes and as pods are deleted and
950:50 - recreated moving on to persistent volume
950:53 - claims this is a request for
950:55 - and claim to
950:57 - a persistent volume resource persistent
950:59 - volume claim objects request a specific
951:03 - size
951:04 - access mode and storage class for the
951:07 - persistent volume if an existing
951:09 - persistent volume can satisfy the
951:12 - request or can be provisioned the
951:15 - persistent volume claim is bound to that
951:18 - persistent volume and just as a note
951:21 - pods use claims
951:22 - as volumes the cluster inspects the
951:25 - claim to find the bound volume and
951:28 - mounts that volume for the pod
951:31 - now i wanted to take a moment to go over
951:33 - storage classes and how they apply to
951:36 - the overall storage in gke
951:39 - now these volume implementations
951:42 - such as gce persistent disk are
951:45 - configured through storage class
951:47 - resources
951:48 - gke creates a default storage class for
951:50 - you which uses the standard persistent
951:53 - disk type of ext4 as shown here the
951:57 - default storage class is used when a
952:00 - persistent volume claim doesn't specify
952:03 - a storage class name
952:05 - and can also be replaced with one of
952:08 - your choosing you can even create your
952:11 - own storage class resources to describe
952:14 - different classes of storage
952:16 - and is helpful when using windows node
952:19 - pools
952:20 - now as i stated before persistent volume
952:23 - claims can automatically provision
952:26 - persistent disks for you
952:28 - when you create this persistent volume
952:30 - claim object
952:32 - kubernetes dynamically creates a
952:35 - corresponding persistent volume object
952:38 - due to the gke default storage class
952:41 - this persistent volume
952:43 - is backed by a new empty compute engine
952:46 - persistent disk you use this disk in a
952:49 - pod by using the claim as a volume when
952:52 - you delete a claim the corresponding
952:55 - persistent volume object and the
952:57 - provision compute engine persistent disk
953:00 - are also deleted now to prevent deletion
953:03 - you can set the reclaim policy of the
953:06 - persistent disk resource
953:08 - or its storage class resource to retain
953:12 - now deployments as shown here in this
953:14 - diagram are designed for stateless
953:17 - applications
953:18 - so all replicas of a deployment
953:21 - share the same persistent volume claim
953:24 - which is why stateful sets are the
953:26 - recommended method of deploying stateful
953:29 - applications
953:31 - that require a unique volume per replica
953:34 - by using stateful sets with persistent
953:37 - volume claim templates
953:39 - you can have applications that can scale
953:42 - up automatically
953:44 - with unique persistent volume claims
953:47 - associated to each replica pod
953:50 - now lastly i wanted to touch on some
953:53 - topics that will determine the storage
953:55 - access that is available for any gke
953:59 - cluster in your environment now i first
954:02 - wanted to start off with access modes
954:04 - and there are three supported modes for
954:06 - your persistent disks that allow read
954:09 - write access and are listed here read
954:12 - write once is where the volume can be
954:15 - mounted as read write by a single node
954:18 - read only many is where the volume can
954:21 - be mounted as a read only by many nodes
954:25 - and lastly read write many is where the
954:28 - volume can be mounted as read write by
954:31 - many nodes and just as a note
954:34 - read write once is the most common use
954:36 - case for persistent disks
954:39 - and works as the default access mode for
954:42 - most applications next i wanted to touch
954:45 - on the type of persistent disks that are
954:48 - available and the benefits and caveats
954:51 - of access for each now going through the
954:54 - persistent disks lesson of this course
954:57 - you probably know by now about the
955:00 - available persistent disks when it comes
955:03 - to zonal versus regional availability
955:06 - and so this may be a refresher for some
955:09 - now going into regional persistent disks
955:12 - these are multi-zonal resources that
955:14 - replicate data between two zones in the
955:18 - same region and can be used similarly to
955:21 - zonal persistent disks in the event of a
955:24 - zonal outage kubernetes can fail over
955:27 - workloads using the volume to the other
955:30 - zone regional persistent disks are great
955:34 - for highly available solutions for
955:37 - stateful workloads on gke now zonal
955:40 - persistent disks
955:42 - are zonal resources
955:44 - and so unless a zone is specified
955:47 - gke assigns the disk to a single zone
955:51 - and chooses the zone at random once a
955:54 - persistent disk is provisioned
955:56 - any pods referencing the disk are
955:59 - scheduled to the same zone as the disk
956:03 - and just as a note
956:04 - using anti-affinity on zones
956:07 - allows stateful set pods to be spread
956:11 - across zones
956:13 - along with the corresponding disks and
956:15 - the last point that i wanted to cover
956:18 - when it comes to persistent volume
956:20 - access
956:21 - is the speed of access
956:23 - now as stated in an earlier lesson the
956:26 - size of persistent disks determine the
956:29 - iops and throughput of the disk gke
956:33 - typically uses persistent disks
956:35 - as boot disks and to back kubernetes
956:39 - persistent volumes
956:40 - so whenever possible use larger and
956:44 - fewer disks
956:45 - to achieve higher iops and throughput
956:48 - and so that pretty much covers
956:50 - everything that i wanted to go over
956:53 - in this lesson on gke storage options
956:56 - so you can now mark this lesson as
956:58 - complete
956:59 - and let's move on to the next one
957:01 - [Music]
957:05 - welcome back in these next few demos i'm
957:08 - going to be doing a complete walkthrough
957:10 - and putting all the theory we learned
957:13 - into practice through building and
957:15 - interacting with gke clusters and you'll
957:18 - be building and deploying your own
957:21 - containerized application on this
957:22 - cluster called box of bowties so in this
957:26 - demo we're going to be setting up our
957:28 - own gke cluster in the console along
957:31 - with going through all the options that
957:33 - are available when deploying it we're
957:35 - also going to use the command line to
957:38 - configure the cubectl command line tool
957:40 - so that we can interact with the cluster
957:43 - so with that being said let's dive in
957:45 - and so here in the console i am logged
957:47 - in as tonybowties gmail.com
957:51 - under the project of bow tie inc and so
957:53 - before launching the cluster i need to
957:55 - make sure that my default vpc has been
957:58 - created so i'm going to go over to the
957:59 - navigation menu and i'm going to scroll
958:02 - down to vpc network
958:04 - and as expected the default network is
958:06 - here so i can go ahead and create my
958:08 - cluster and so in order to get to my
958:10 - kubernetes engine console i'm going to
958:13 - go up to the navigation menu and i'm
958:15 - going to scroll down under compute and
958:17 - you will find here kubernetes engine and
958:20 - you'll see a few different options to
958:22 - choose from and over here on the left
958:24 - hand menu i will be going through these
958:26 - options in the upcoming demos but for
958:29 - now i want to concentrate on creating
958:31 - our cluster now gk makes things pretty
958:33 - easy
958:34 - as i have the option to create a cluster
958:37 - to deploy a container or even taking the
958:39 - quick start and so we're going to go
958:41 - ahead and click on create our cluster
958:43 - and so here we are prompted with our
958:45 - cluster basics now if i really wanted to
958:48 - i can simply fill out all the fields
958:50 - that you see here and click on create
958:53 - and it will use all the defaults to
958:55 - build my cluster but we're going to
958:57 - customize it a little bit so we're going
958:59 - to go ahead and go through all these
959:00 - options so first under name we're going
959:03 - to name this cluster bowtie dash cluster
959:06 - and so under location type we want to
959:08 - keep things as zonal and if i check off
959:11 - the specify default node locations i'll
959:14 - be able to make this a multi-zonal
959:16 - cluster as i have the option of
959:19 - selecting from multiple zones where i
959:22 - can situate my nodes and so i can select
959:24 - off a bunch of different zones if i
959:26 - choose but we want to keep it as a
959:29 - single zonal cluster and so i'm going to
959:31 - check these all off
959:33 - and under zone i'm going to click on the
959:34 - drop down menu and i'm going to select
959:37 - us east 1b and just as a note for each
959:40 - zone that you select this is where the
959:43 - control plane will live so if i was to
959:45 - create a multi-zonal cluster as you can
959:48 - see the master zone is the zone where
959:50 - the control plane will be created and is
959:53 - selected as us east 1b as that is the
959:56 - zone that i had selected and so if i
959:58 - change this
960:00 - to let's say us east 1d you can see that
960:03 - the control plane will change with it so
960:05 - i'm going to change it back to us east
960:07 - 1b and you also have the option of
960:09 - creating a regional cluster and the
960:12 - location selection will change from zone
960:15 - to region and here you will have to
960:17 - specify at least one zone to select but
960:20 - please also remember that the same
960:22 - number of nodes will be deployed to each
960:25 - selected zone so if i have three nodes
960:27 - in this cluster and i decide to select
960:30 - three zones then i will have nine nodes
960:33 - in this cluster and so doing something
960:35 - like this could get quite pricey when
960:38 - you're looking to be cost conscious okay
960:40 - so moving on i'm going to uncheck
960:42 - specify default node locations i'm going
960:44 - to change the location type back to
960:46 - zonal and make sure that my zone is at
960:49 - us east 1b moving down to the master
960:51 - version this is where we would select
960:54 - either a static version or opt-in to a
960:57 - release channel for the version of
960:59 - kubernetes that you want for your
961:01 - cluster and so with the static version i
961:03 - can choose from a bunch of different
961:05 - versions here all the way back from
961:07 - 1.14.10
961:10 - all the way to the latest version and so
961:12 - with the release channel i have the
961:14 - release channel selection here and i can
961:16 - choose from the rapid channel the
961:18 - regular channel or the stable channel
961:21 - and so i'm going to keep things as the
961:22 - default with the regular channel as well
961:25 - i'm going to keep the default version as
961:27 - the version of my choice now i could go
961:30 - ahead and simply click on create here
961:32 - but as this demo is a walkthrough i'm
961:34 - going to go ahead and go through all the
961:36 - available options so i'm going to start
961:38 - by going over to the left hand menu and
961:41 - clicking on default pool under no pools
961:44 - now here i have one node pool already
961:46 - with three nodes and this is the default
961:49 - node pool that comes with any cluster
961:51 - but if i was doing something specific i
961:54 - could add another node pool and
961:56 - configure it from here but because i
961:58 - don't have a need for two node pools i'm
962:01 - gonna go ahead and remove nodepool1 so
962:04 - i'm going to go up here to remove
962:06 - nodepool and as you can see gke makes it
962:09 - really easy for me to add or remove node
962:12 - pools so i'm going to go back to the
962:14 - default pool and i'm going to keep the
962:15 - name as is i'm gonna keep my number of
962:18 - nodes as three and if i wanted to change
962:20 - the number of nodes i can simply select
962:23 - this i can choose six or however many
962:25 - nodes you need for your workload and so
962:28 - because we're not deploying a large
962:29 - workload i'm gonna keep this number at 3
962:32 - and moving right along we do want to
962:34 - check off enable auto scaling and so
962:37 - this way we don't have to worry about
962:39 - scaling up or scaling down and here i'm
962:42 - going to put the minimum number of nodes
962:44 - as one and i'm going to keep my maximum
962:46 - number of nodes at 3. and so here i'm
962:48 - given the option to select the zone
962:51 - location for my nodes but again for each
962:54 - zone that i select it will run the same
962:56 - amount of nodes so basically i have
962:59 - another option in order to choose from
963:01 - having a zonal or multi-zonal cluster
963:04 - and because we're creating our cluster
963:06 - in a single zone i'm going to uncheck
963:08 - this and under automation as you can see
963:11 - enable auto upgrade and enable auto
963:14 - repair are both checked off and this is
963:16 - due to the fact that the auto upgrade
963:19 - feature is always enabled for the
963:21 - release channel that i selected but as i
963:24 - pointed out in a previous lesson that
963:26 - this is google's best practice
963:28 - to have auto upgrade and auto repair
963:31 - enabled and so moving down to the bottom
963:33 - are some fields to change the surge
963:35 - upgrade behavior and so just as a
963:37 - refresher surge upgrades allow you to
963:40 - control the number of nodes gke can
963:42 - upgrade at a time and control how
963:44 - disruptive those upgrades are to your
963:47 - workloads so max surge being the number
963:49 - of additional nodes that can be added to
963:51 - the node pool during an upgrade and max
963:53 - unavailable being the number of nodes
963:55 - that can be simultaneously unavailable
963:58 - during that upgrade and because we're
964:00 - not worried about disruptions we'll just
964:02 - leave it set as the default and so
964:04 - moving on we're going to move back over
964:06 - to the left hand menu and under no pools
964:08 - we're going to click on nodes and here
964:10 - is where i can choose the type of
964:12 - instance that i want to be using for my
964:15 - nodes and so i'm going to keep the image
964:17 - type as container optimize os and this
964:20 - is the default image type but i also
964:22 - have the option of choosing from others
964:24 - like ubuntu or windows and so i'm going
964:27 - to keep it as the default and under
964:29 - machine configuration i'm going to keep
964:32 - it under general purpose
964:34 - with series e2 but i do want to change
964:36 - the machine type to e2 micro just to be
964:40 - cost conscious and under boot disk size
964:43 - i want to keep it as 10 gigabytes as we
964:46 - don't really need 100 gigabytes for what
964:48 - we're doing here and you also have the
964:50 - option of choosing from a different boot
964:53 - disk type you can change it from
964:55 - standard persistent disk to ssd but i'm
964:57 - going to keep things as standard as well
965:00 - i also have the option here to use
965:02 - customer manage keys for encryption on
965:04 - my boot disk as well as selecting from
965:06 - preemptable nodes for some cost savings
965:09 - and so i'm going to now move down to
965:11 - networking and here if i wanted to get
965:13 - really granular i can add a maximum pods
965:16 - per node as well as some network tags
965:19 - but our demo doesn't require this so i'm
965:21 - going to leave it as is and i'm going to
965:23 - go back over to the left hand menu and
965:26 - click on security and under node
965:28 - security you have the option of changing
965:30 - your service account along with the
965:33 - access scopes and so for this demo we
965:35 - can keep things as the default service
965:38 - account and the access scopes can be
965:40 - left as is i'm going to go back over to
965:42 - the left hand menu and click on metadata
965:45 - and here i can add kubernetes labels as
965:48 - well as the instance metadata and so i
965:50 - know i didn't get into node taints but
965:53 - just to fill you in on no taints when
965:55 - you submit a workload to run in a
965:57 - cluster the scheduler determines where
966:00 - to place the pods associated with the
966:02 - workload and so the scheduler will place
966:05 - a pod on any node that satisfies the
966:08 - resource requirements for that workload
966:11 - so no taints will give you some more
966:13 - control over which workloads can run on
966:16 - a particular pool of nodes and so they
966:19 - let you mark a node so that the
966:21 - scheduler avoids or prevents using it
966:24 - for certain pods so for instance if you
966:26 - had a node pool that is dedicated to
966:29 - gpus you'd want to keep that node pool
966:32 - specifically for the workload that
966:34 - requires it and although it is in beta
966:37 - this is a great feature to have and so
966:39 - that pretty much covers no pools as we
966:42 - see it here and so this is the end of
966:44 - part one of this demo it was getting a
966:46 - bit long so i decided to break it up
966:49 - this would be a great opportunity for
966:51 - you to get up and have a stretch get
966:53 - yourself a coffee or a tea and whenever
966:56 - you're ready part two will be starting
966:58 - immediately from the end of part one so
967:01 - you can now mark this as complete and
967:03 - i'll see you in the next one
967:05 - [Music]
967:09 - this is part two of creating a gke
967:11 - cluster part 2 will be starting
967:14 - immediately from the end of part 1. so
967:17 - with that being said let's dive in and
967:19 - so i'm going to go back over to the left
967:21 - hand menu and under cluster i'm going to
967:24 - click on automation and here i have the
967:26 - option of enabling a maintenance window
967:29 - for aligning times when auto upgrades
967:31 - are allowed i have the option of adding
967:34 - the window here and i can do it at
967:36 - specified times during the week or i can
967:39 - create a custom maintenance window and
967:41 - so we don't need a maintenance window
967:43 - right now so i'm going to uncheck this
967:45 - and as well you have the option of doing
967:48 - maintenance exclusions for when you
967:50 - don't want maintenance to occur ngk
967:53 - gives you the option of doing multiple
967:55 - maintenance exclusions for whenever you
967:58 - need them and because we don't need any
968:00 - maintenance exclusions i'm going to
968:02 - delete these and here you have the
968:04 - option to enable vertical pod auto
968:06 - scaling and this is where gke will
968:09 - automatically schedule pods onto other
968:12 - nodes that satisfy the resources
968:14 - required for that workload as well here
968:18 - i can enable my node auto provisioning
968:21 - and enabling this option allows gke to
968:24 - automatically manage a set of node pools
968:27 - that can be created and deleted as
968:29 - needed and i have a bunch of fields that
968:31 - i can choose from the resource type the
968:34 - minimum and maximum for cpu and memory
968:37 - the service account
968:39 - as well as adding even more resources
968:41 - like gpus but our workload doesn't
968:44 - require anything this fancy so i'm going
968:46 - to delete this and i'm going to uncheck
968:49 - enable auto provisioning and lastly we
968:51 - have the auto scaling profile and i have
968:54 - the option from choosing the balance
968:56 - profile which is the default as well as
968:58 - the optimize utilization which is still
969:01 - in beta and so i'm going to keep things
969:03 - as the default and i'm going to move
969:05 - back on over to the left hand menu over
969:07 - to networking and so here i can get
969:09 - really granular with my cluster when it
969:12 - comes to networking i have the option of
969:14 - choosing from a public or a private
969:17 - cluster as well i can choose from a
969:19 - different network and since we only have
969:21 - the default that's what shows up but if
969:23 - you had different networks here you can
969:25 - choose from them as well as the subnets
969:28 - i can also choose from other networking
969:30 - options like pod address range maximum
969:33 - pods per node and there's a bunch of
969:35 - other options which i won't get into any
969:37 - detail with but i encourage you if
969:40 - you're very curious to go through the
969:42 - docs and to check out these different
969:44 - options now the one thing that i wanted
969:46 - to note here is the enable http low
969:49 - balancing and this is a add-on that is
969:51 - required in order to use google cloud
969:54 - load balancer and so as we discussed
969:56 - previously in the services lesson when
969:59 - you enable service type load balancer a
970:02 - load balancer will be created for you by
970:05 - the cloud provider and so google
970:07 - requires you to check this off so that a
970:09 - controller can be installed in the
970:12 - cluster upon creation and will allow a
970:15 - load balancer to be created when the
970:17 - service is created and so i'm going to
970:19 - leave this checked as we will be
970:21 - deploying a load balancer a little bit
970:23 - later and so moving back over to the
970:25 - left hand menu i'm going to now click on
970:27 - security and there are many options here
970:29 - to choose from that will allow you to
970:32 - really lock down your cluster and again
970:34 - this would all depend on your specific
970:36 - type of workload now i'm not going to go
970:38 - through all these options here but i did
970:41 - want to highlight it for those who are
970:42 - looking to be more security focused with
970:45 - your cluster and so moving down the list
970:47 - in the menu i'm going to click on
970:48 - metadata and so here i can enter a
970:51 - description for my cluster as well as
970:53 - adding labels and so the last option on
970:56 - the cluster menu is features and here i
970:58 - have the option of running cloud run for
971:01 - anthos which will allow you to deploy
971:03 - serverless workloads to anthos clusters
971:06 - and runs on top of gke and here you can
971:09 - enable monitoring for gke and have it be
971:12 - natively monitored by google cloud
971:14 - monitoring and if i was running a
971:16 - third-party product to monitor my
971:18 - cluster i can simply uncheck this and
971:21 - use my third-party monitoring and
971:23 - there's a whole bunch of other features
971:25 - that i won't dive into right now but if
971:28 - you're curious you can always hover over
971:30 - the question mark and get some more
971:32 - information about what it does and so
971:34 - now i've pretty much covered all the
971:36 - configuration that's needed for this
971:39 - cluster and so now i'm going to finally
971:41 - head down to the bottom and click on
971:43 - create and so it may take a few minutes
971:45 - to create this cluster so i'm going to
971:47 - go ahead and pause this video here and
971:50 - i'll be back faster than you can say cat
971:52 - in the hat okay and the cluster has been
971:54 - created as you can see it's in the
971:56 - location of us east 1b with three nodes
971:59 - six vcpus and three gigabytes of memory
972:03 - and i can drill down and see exactly the
972:05 - details of the cluster as well if i
972:08 - wanted to edit any of these options i
972:10 - can simply go up to the top click on
972:12 - edit and make the necessary changes and
972:15 - so now you're probably wondering what
972:17 - will i need to do in order to create
972:19 - this cluster through the command line
972:21 - well it's a bit simpler than what you
972:23 - think and i'm going to show you right
972:25 - now i'm going to simply go over to the
972:26 - right hand menu and activate cloud shell
972:29 - and bring this up for better viewing and
972:31 - i'm going to paste in my command gcloud
972:34 - container clusters create bow tie dash
972:37 - cluster with the flag num nodes and the
972:40 - number of nodes that i choose which is
972:42 - three and so like i said before if i
972:44 - wanted to simply create a simple cluster
972:47 - i can do so like this but if i wanted to
972:50 - create the cluster exactly how i built
972:52 - my last cluster then i can use this
972:54 - command which has all the necessary
972:57 - flags that i need to make it customize
972:59 - to my liking a not so very exciting
973:02 - demonstration but at the same time shows
973:05 - you how easy yet powerful gke really is
973:08 - and so i'm not going to launch this
973:10 - cluster as i already have one and so now
973:13 - i wanted to show you how to interact
973:15 - with your new gke cluster so i'm going
973:17 - to simply clear my screen and so now in
973:20 - order for me to interact with my cluster
973:22 - i'm going to be using the cube ctl
973:25 - command line tool and this is the tool
973:27 - that is used to interact with any
973:29 - kubernetes cluster no matter the
973:31 - platform now i could use the gcloud
973:34 - container commands but they won't allow
973:36 - me to get very granular as the cubectl
973:39 - tool and so a caveat of creating your
973:41 - cluster through the console is that you
973:44 - need to run a command in order to
973:46 - retrieve the cluster's credentials and
973:49 - configure the cubectl command line tool
973:52 - and i'm going to go ahead and paste that
973:53 - in now and the command is gcloud
973:56 - container clusters get dash credentials
974:00 - and the name of my cluster which is bow
974:02 - tie dash cluster along with the zone
974:04 - flag dash dash zone followed by the zone
974:07 - itself which is us east 1b i'm going to
974:10 - go ahead and hit enter and as you can
974:12 - see cubectl has now been configured and
974:15 - so now i'm able to interact with my
974:17 - cluster so just to verify i'm going to
974:20 - run the command cubectl
974:22 - getpods and naturally as no workloads
974:26 - are currently deployed in the cluster
974:28 - there are no pods so i'm going to run
974:30 - the command cube ctl get nodes
974:34 - and as you can see the cubectl command
974:36 - line tool is configured correctly and so
974:39 - now this cluster is ready to have
974:40 - workloads deployed to it and is also
974:42 - configured with the cubectl command line
974:45 - tool so that you're able to manage the
974:48 - cluster and troubleshoot if necessary
974:50 - now i know that there has been a ton of
974:52 - features that i covered but i wanted to
974:55 - give you the full walkthrough so that
974:57 - you are able to tie in some of the
974:59 - theory from the last few lessons and get
975:01 - a feel for the gke cluster as we will be
975:05 - getting more involved with it over the
975:07 - next couple of demos and so that's
975:09 - pretty much all i wanted to cover when
975:11 - it comes to creating and setting up a
975:14 - gke cluster so you can now mark this as
975:16 - complete and whenever you're ready join
975:19 - me in the console in the next one where
975:21 - you will be building your box a bow ties
975:23 - container to deploy to your new cluster
975:26 - but if you are not planning to go
975:27 - straight into the next demo i do
975:29 - recommend that you delete your cluster
975:32 - to avoid any unnecessary costs and
975:34 - recreate it when you are ready to go
975:36 - into the next demo
975:38 - [Music]
975:42 - welcome back now in the last lesson you
975:45 - built a custom gke cluster and
975:47 - configured the cube ctl command line
975:50 - tool to interact with the cluster in
975:52 - this lesson you're going to be building
975:54 - a docker image for a box of bow ties
975:58 - using cloud build which will then be
976:00 - pushed over to google cloud container
976:03 - registry so that you can deploy it to
976:05 - your current gke cluster and so as you
976:08 - can see there's a lot to do here so with
976:10 - that being said let's dive in so now the
976:13 - first thing that you want to do is to
976:15 - clone your repo within cloud shell so
976:18 - you can run the necessary commands to
976:20 - build your image so i'm going to go up
976:22 - here to the top right and i'm going to
976:24 - open up cloud shell i'm going to make
976:25 - sure that i'm in my home directory so
976:27 - i'm going to run the command cd space
976:30 - tilde
976:31 - hit enter and i'm in my home directory
976:33 - if i run the command ls i can see that i
976:36 - only have cloud shell.txt and so now i'm
976:39 - going to clone my github repository and
976:42 - i'll have a link in the instructions in
976:44 - the github repo as well as having it in
976:47 - the lesson text below and so the command
976:49 - would be git clone along with the https
976:52 - address of the github repo and i'm going
976:55 - to hit enter
976:56 - and it's finished cloning my repo i'm
976:58 - going to quickly clear my screen
977:00 - and i'm going to run the command ls and
977:03 - i can see my repo here and now i'm going
977:05 - to drill down into the directory by
977:07 - running cd google cloud associate cloud
977:10 - engineer if i run an ls i can see all my
977:13 - clone files and folders and so now the
977:15 - files that we need are going to be found
977:18 - in the box of bowties folder under
977:20 - kubernetes engine and containers so i'm
977:23 - going to change directories to that
977:24 - location and run ls and under box of bow
977:28 - ties is a folder called container which
977:31 - will have all the necessary files that
977:33 - you need in order to build your image we
977:36 - have the jpeg for box of bow ties we
977:39 - have the docker file and we have our
977:41 - index.html and so these are the three
977:44 - files that we need in order to build the
977:46 - image and so as i said before we are
977:48 - going to be using a tool called cloud
977:51 - build which we have not discussed yet
977:53 - cloudbuild is a serverless ci cd
977:56 - platform that allows me to package
977:58 - source code into containers and you can
978:01 - get really fancy with cloud build but
978:03 - we're not going to be setting up any ci
978:05 - cd pipelines we're merely using cloud
978:08 - build to build our image and to push it
978:11 - out to container registry as well
978:13 - container registry is google cloud's
978:16 - private docker repository where you can
978:18 - manage your docker images and integrates
978:21 - with cloud build gke app engine cloud
978:25 - functions
978:26 - and other repos like github or bitbucket
978:29 - and it allows for an amazing build
978:31 - experience with absolutely no heavy
978:34 - lifting and because you're able to build
978:36 - images without having to leave google
978:38 - cloud i figured that this would be a
978:40 - great time to highlight these services
978:43 - so getting back to it we've cloned the
978:45 - repo and so we have our files here in
978:48 - cloud shell and so what you want to do
978:50 - now is you want to make sure the cloud
978:52 - build api has been enabled as this is a
978:55 - service that we haven't used before now
978:57 - we can go through the console and enable
978:59 - the api there but i'm going to run it
979:01 - here from cloud shell and i'm going to
979:03 - paste in the command gcloud services
979:06 - enable cloudbuild.googleapis.com
979:09 - i'm going to hit enter and you should
979:11 - get a prompt asking you to authorize the
979:14 - api call you definitely want to
979:16 - authorize
979:17 - should take a few seconds all right and
979:19 - the api has been enabled for cloud build
979:22 - so now i'm going to quickly clear my
979:24 - screen and so because i want to show you
979:26 - exactly what cloud build is doing i want
979:28 - to head on over there through the
979:30 - console and so i'm going to go over to
979:31 - the navigation menu and i'm going to
979:34 - scroll down to tools until you come to
979:36 - cloud build
979:38 - and as expected there is nothing here in
979:40 - the build history as well not a lot here
979:43 - to interact with and so now you're going
979:45 - to run the command that builds the image
979:47 - and so you're going to paste that
979:48 - command into the cloud shell which is
979:50 - gcloud builds submit dash dash tag
979:54 - gcr.io which is the google cloud
979:56 - container registry our variable for our
979:59 - google cloud project along with the
980:01 - image name of box bow ties version 1.0.0
980:06 - and please don't forget the trailing dot
980:08 - at the end i'm going to go ahead and hit
980:10 - enter cloud build will now compress the
980:12 - files and move them to a cloud storage
980:15 - bucket and then cloud build takes those
980:17 - files from the bucket and uses the
980:20 - docker file to execute the docker build
980:22 - process and so i'm going to pause the
980:24 - video here till the build completes and
980:27 - i'll be back in a flash okay and the
980:29 - image is complete and is now showing up
980:32 - in the build history in the cloud build
980:34 - dashboard and so if i want to drill down
980:36 - into the actual build right beside the
980:39 - green check mark you will see the hot
980:41 - link so you can just simply click on
980:42 - that and here you will see a build
980:45 - summary with the build log
980:48 - the execution details along with the
980:51 - build artifacts and as well the
980:53 - compressed files are stored in cloud
980:55 - storage and it has a hot link right here
980:57 - if i wanted to download the build log i
981:00 - can do so here and i conveniently have a
981:03 - hot link to the image of box of bow ties
981:07 - and this will bring me to my container
981:09 - registry so you can go ahead and click
981:11 - on the link
981:12 - it should open up another tab and bring
981:15 - you right to the page of the image that
981:17 - covers a lot of its details now the
981:19 - great thing i love about container
981:21 - registry is again it's so tightly
981:24 - coupled with a lot of the other
981:25 - resources within google cloud that i am
981:28 - able to simply deploy right from here
981:31 - and i can deploy to cloud run to gke as
981:35 - well as compute engine now i could
981:37 - simply deploy this image right from here
981:39 - but i wanted to do it from gke so i'm
981:42 - going to go back over to gke in the
981:44 - other tab i'm going to go to the
981:45 - navigation menu go down to kubernetes
981:48 - engine
981:48 - and i'm going to go up to the top menu
981:51 - and click on deploy it's going to ask
981:53 - for the image you want to deploy and you
981:55 - want to click on select to select a new
981:57 - container image and you should have a
981:59 - menu pop up from the right hand side of
982:01 - your screen and under container registry
982:04 - you should see box of bow ties you can
982:06 - expand the node here and simply click on
982:09 - the image and then hit select
982:11 - and so now the container image has been
982:13 - populated into my image path and you
982:16 - want to scroll down and if i wanted to i
982:18 - could add another container and even add
982:21 - some environment variables and so we're
982:23 - not looking to do that right now so you
982:25 - can simply click on continue and you're
982:27 - going to be prompted with some fields to
982:29 - fill out for your configuration on your
982:31 - deployment and so the application name
982:34 - is going to be called box of bow ties
982:37 - i'm going to keep it in the default
982:38 - namespace as well i'm going to keep the
982:41 - key value pair as app box of bow ties
982:45 - for my labels and because this
982:47 - configuration will create a deployment
982:49 - file for me you can always have a look
982:52 - at the manifest by clicking on the view
982:54 - yaml button before it's deployed and
982:56 - this is always good practice before you
982:59 - deploy any workload so as you can see
983:01 - here at the top i have the kind as
983:04 - deployment the name as well as the
983:06 - namespace my labels
983:09 - replicas of three as well as my selector
983:12 - and my spec down here at the bottom as
983:15 - well this manifest also holds another
983:17 - kind
983:18 - of horizontal pod auto scaler and is
983:21 - coupled with the deployment in this
983:22 - manifest due to the reference of the
983:25 - deployment itself and so it's always
983:27 - common practice to try and group the
983:29 - manifest together whenever you can and
983:32 - so this is a really cool feature to take
983:34 - advantage of on gke so i'm going to
983:36 - close this now and i'm actually going to
983:38 - close cloud shell as i don't need it
983:40 - right now as well you can see here that
983:43 - it's going to deploy to my kubernetes
983:45 - cluster of bow tie cluster in us east 1b
983:49 - and if i wanted to i can deploy it to a
983:52 - new cluster and if i had any other
983:54 - clusters in my environment they would
983:57 - show up here and i'd be able to select
983:59 - from them as well but bow tie cluster is
984:02 - the only one that i have and so now that
984:04 - you've completed your configuration for
984:06 - your deployment you can simply click on
984:08 - deploy this is just going to take a
984:10 - couple minutes so i'm just going to
984:12 - pause the video here and i'll be back as
984:14 - soon as the deployment is done okay the
984:16 - workload has been deployed and i got
984:19 - some default messages that popped up i
984:21 - can set an automated pipeline for this
984:23 - workload but we're not going to do that
984:25 - for this demo but feel free to try it on
984:27 - your own later if you'd like and we will
984:29 - want to expose our service as we want to
984:32 - see if it's up and running and we're
984:34 - going to take care of that in just a bit
984:36 - and so if i scroll through some of the
984:37 - details here i can see that i have some
984:40 - metrics here for cpu memory and disk the
984:43 - cluster
984:44 - namespace
984:45 - labels and all the pods that it's
984:48 - running on basically a live visual
984:50 - representation of my deployment if i
984:53 - scroll back up to the top i can dive
984:55 - into some details events
984:58 - and even my manifest i can also copy my
985:01 - manifest and download it if i'd like so
985:03 - as you can see a lot of different
985:05 - options and so now i want to verify my
985:08 - deployment and so i'm going to use the
985:10 - cube ctl command line tool to run some
985:13 - commands to verify the information so
985:16 - i'm going to open back up my cloud shell
985:18 - and make this a little bit bigger for
985:19 - better viewing and i'm going to run the
985:21 - command cubectl get all
985:24 - and as you can see here i have a list of
985:27 - all the pods that are running the name
985:29 - of the service the deployment the
985:32 - replica set everything about my cluster
985:34 - and my deployment and you should be
985:36 - seeing the same when running this
985:38 - command and so next you want to pull up
985:40 - the details on your deployments in the
985:42 - cluster and so the command for that is
985:44 - cube ctl get deployments and it came out
985:48 - kind of crammed at the bottom so i'm
985:50 - going to simply clear my screen and run
985:52 - that command again
985:54 - and as you can see the box of bowties
985:56 - deployment is displayed how many
985:58 - replicas that are available how many of
986:00 - those replicas achieve their desired
986:02 - state and along with how long the
986:04 - application has been running and so now
986:07 - i want to dive into my pods and in order
986:09 - to do that i'm going to run the command
986:12 - cube ctl get pods and here i can see all
986:16 - my pods now if i wanted to look at a
986:18 - list of events
986:20 - for a specific pod the command for that
986:23 - would be cubectl describe pod and then
986:27 - the name of one of the pods so i'm going
986:30 - to pick this first one copy that i'm
986:32 - going to paste it and i'm going to hit
986:34 - enter and here i can see all the events
986:36 - that have occurred for this pod as well
986:39 - i also have access to some other
986:41 - information with regards to volumes
986:44 - conditions and even the container and
986:47 - image ids and this is a great command to
986:50 - use for when you're troubleshooting your
986:52 - pods and you're trying to get to the
986:54 - bottom of a problem and so now the final
986:56 - step that you want to do is you want to
986:58 - be able to expose your application so
987:01 - you can check to see if it's running
987:03 - properly and so we're going to go ahead
987:05 - and do that through the console so i'm
987:06 - going to close down cloud shell and i'm
987:08 - going to go to overview and scroll down
987:11 - to the bottom click on the button that
987:12 - says expose and if i wanted to i can do
987:15 - it from up here in the top right hand
987:17 - corner where it says expose deployment
987:20 - so i'm going to click on expose and this
987:22 - probably looks very familiar to you as
987:25 - this is a graphical representation of
987:27 - the services manifest and so the port
987:30 - mapping here will cover the ports
987:32 - configuration of the services manifest
987:34 - starting here with port target port as
987:37 - well as protocol for target port i'm
987:40 - going to open up port 80. here under
987:42 - service type you have the option of
987:44 - selecting cluster ip
987:46 - node port or load balancer and the
987:49 - service type you want to use is going to
987:51 - be low balancer and we can keep the
987:53 - service name as box of bowties service
987:56 - and again you can view the manifest file
987:59 - for this service and you can copy or
988:01 - download it if you need to but we don't
988:03 - need this right now so i'm going to
988:04 - close it in a pretty simple process
988:07 - so all i need to do is click on expose
988:10 - and within a minute or two you should
988:12 - have your service up and running with
988:14 - your shiny new low balancer okay and the
988:17 - service has been created and as you can
988:19 - see we're under the services and ingress
988:22 - from the left hand menu and if i go back
988:24 - to the main page of services in ingress
988:26 - you can see that box a bow tie service
988:29 - is the only one that's here i also have
988:31 - the option of creating a service type
988:34 - ingress but we don't want to do that
988:36 - right now so i'm going to go back to
988:37 - services and here you will see your
988:40 - endpoint and this is the hot link that
988:42 - should bring you to your application so
988:45 - you can click on it now you'll get a
988:46 - redirect notice as it is only http and
988:50 - not https so it's safe to click on it so
988:53 - i'm going to click on it now and success
988:56 - and here is your box of bow ties
988:59 - what were you expecting and so i wanted
989:01 - to congratulate you on deploying your
989:03 - first application box of bow ties on
989:06 - your gke cluster and so just as a recap
989:09 - you've cloned your repo into your cloud
989:11 - shell environment you then built a
989:13 - container image using cloud build and
989:16 - pushed the image to container registry
989:18 - you then created a deployment using this
989:21 - image and verified the deployment using
989:24 - the cube ctl command line tool you then
989:26 - launched a service of type low balancer
989:29 - to expose your application and verified
989:32 - that your application was working so
989:35 - fantastic job on your part and that's
989:37 - pretty much all i wanted to cover in
989:39 - this part of the demo
989:40 - so you can now mark this as complete and
989:43 - whenever you're ready join me in the
989:45 - console for the next part of the demo
989:47 - where you will manage your workload on
989:49 - the gke cluster so please be aware of
989:52 - the charges incurred on your currently
989:54 - deployed cluster if you plan to do the
989:57 - next demo at a later date again you can
990:00 - mark this as complete and i'll see you
990:02 - in the next
990:07 - welcome back in the last couple of demo
990:09 - lessons you built a custom gke cluster
990:13 - and deployed the box of bowties
990:14 - application in this lesson you will be
990:17 - interacting with this workload on gke by
990:20 - scaling the application editing your
990:23 - application and rebuilding your docker
990:25 - image so you can do a rolling update to
990:28 - the current workload in your cluster now
990:30 - there's a lot to do here so with that
990:32 - being said let's dive in so continuing
990:35 - where we left off you currently have
990:38 - your box of bow ties workload deployed
990:41 - on your gke cluster and so the first
990:43 - thing you want to do is scale your
990:45 - deployment and you are looking to scale
990:48 - down your cluster to one pod and then
990:51 - back up again to three and this is just
990:53 - to simulate scaling your workload so
990:56 - whether it be ten pods or one the action
990:59 - is still the same so now we can easily
991:02 - do it through the console by drilling
991:04 - down into the box of bowties workload
991:07 - going up to the top menu and clicking on
991:09 - actions and clicking on scale and here i
991:12 - can indicate how many replicas i'd like
991:14 - and scale it accordingly and so i wanted
991:16 - to do this using the command line so i'm
991:19 - going to cancel out of here and then i'm
991:21 - going to open up cloud shell instead
991:23 - okay and now that you have cloud shell
991:25 - open up you want to run the command cube
991:28 - ctl get pods to show the currently
991:31 - running available pods for the box of
991:34 - bowties workload and you may get a
991:36 - pop-up asking you to authorize the api
991:38 - call using your credentials and you
991:40 - definitely want to authorize and here
991:42 - you will get a list of all the pods that
991:44 - are running your box of bow ties
991:46 - workload and so now since you want to
991:49 - scale your replicas down to one you can
991:52 - run this command cube ctl scale
991:55 - deployment and your workload which is
991:57 - box of bowties dash dash replicas is
992:00 - equal to one you can hit enter and it is
992:03 - now scaled
992:05 - and in order to verify that i'm going to
992:07 - run cube ctl get pods and notice that
992:11 - there is only one pod running with my
992:13 - box of bow ties workload and in order
992:16 - for me to scale my deployment back up to
992:18 - three replicas i can simply run the same
992:20 - command but change the replicas from 1
992:23 - to 3. hit enter it's been scaled i'm
992:26 - going to run cube ctl get pods and
992:29 - notice that i am now back up to 3
992:31 - replicas and so as you can see
992:33 - increasing or decreasing the number of
992:35 - replicas in order to scale your
992:38 - application is pretty simple to do okay
992:41 - so now that you've learned how to scale
992:42 - your application you're gonna learn how
992:44 - to perform a rolling update but in order
992:47 - to do that you need to make changes to
992:49 - your application and so what you're
992:51 - going to do is edit your application
992:54 - then rebuild your docker image and apply
992:56 - a rolling update and in order to do that
992:58 - we can stay here in cloud shell as
993:01 - you're going to edit the file in cloud
993:02 - shell editor i'm going to first clear my
993:05 - screen i'm going to change directory
993:07 - into my home directory and now you want
993:09 - to change directories to your container
993:11 - folder where the files are that i need
993:13 - to edit i'm going to run ls and here's
993:16 - the files that i need and so what you're
993:18 - going to do now is edit the
993:20 - index.html file and the easiest way to
993:23 - do that is to simply type in edit
993:26 - index.html and hit enter and this will
993:29 - open up your editor so you can edit your
993:32 - index.html file and if you remember when
993:35 - we launched our application it looked
993:37 - exactly like this and so instead of what
993:40 - were you expecting we're going to
993:42 - actually change that text to something a
993:44 - little different and so i'm going to go
993:46 - back to the editor in my other tab and
993:48 - where it says what were you expecting
993:51 - i'm going to actually change this to
993:53 - well i could always use something to eat
993:56 - then i'm going to go back up to the menu
993:58 - click on file and click on save and so
994:00 - now in order for me to deploy this i
994:03 - need to rebuild my container and so i'm
994:05 - going to go back to my terminal i'm
994:07 - going to clear the screen and i'm going
994:09 - to run the same command that i did the
994:10 - last time which is gcloud build submit
994:13 - dash dash tag gcr dot io with the
994:17 - variable for your google cloud project
994:19 - followed by the image box of bowties
994:22 - colon
994:23 - 1.0.1 and so this will be a different
994:26 - version of the image also don't forget
994:28 - that trailing dot at the end and you can
994:30 - hit enter and again this is the process
994:33 - where cloud build compresses the files
994:35 - moves them to a cloud storage bucket and
994:38 - then takes the files from the bucket and
994:40 - uses the docker file to execute the
994:43 - docker build process and this will take
994:45 - a couple minutes so i'm going to pause
994:47 - the video here and i'll be back before
994:49 - you can say cat in the hat okay and my
994:51 - new image has been created and so i want
994:53 - to head over to cloud build just to make
994:56 - sure that there are no errors so i'm
994:57 - going to close down cloud shell because
994:59 - i don't need it right now i'm going to
995:01 - head back up to the navigation menu and
995:03 - scroll down to cloud build and under
995:05 - build history you should see your second
995:07 - build and if you drill down into it you
995:10 - will see that the build was successful
995:12 - and heading over to build artifacts you
995:15 - should now see your new image as version
995:18 - 1.0.1 and so now i'm going to head over
995:20 - to the registry and verify the image
995:22 - there and it seems like everything looks
995:24 - okay so now i'm gonna head back on over
995:26 - to my gke cluster
995:28 - i'm gonna go to the navigation menu down
995:31 - to kubernetes engine and here i'm gonna
995:33 - click on workloads i'm gonna select box
995:35 - of bowties and up at the top menu you
995:38 - can click on actions and select a
995:40 - rolling update and here you are prompted
995:42 - with a pop-up where you can enter in
995:44 - your minimum seconds ready your maximum
995:47 - search percentage as well as your
995:49 - maximum unavailable percentage and so
995:51 - here under container images i am
995:53 - prompted to enter in the sha-256 hash of
995:57 - this docker image now a docker image's
996:00 - id is a digest which contains a sha-256
996:04 - hash of the image's configuration and if
996:07 - i go back over to the open tab for
996:09 - container registry you can see here the
996:11 - digest details to give you a little bit
996:14 - more context along with the sha 256 hash
996:18 - for the image that i need to deploy and
996:20 - so you can copy this digest by simply
996:22 - clicking on the copy button and then you
996:24 - can head back on over to the gke console
996:27 - head over to the container images
996:29 - highlight the hash and paste in the new
996:32 - hash and so when you copy it in make
996:34 - sure it's still in the same format of
996:36 - gcr dot io forward slash your project
996:40 - name forward slash box of bow ties the
996:44 - at symbol followed by the hash and so
996:46 - once you've done that you can click on
996:48 - the update button and this will schedule
996:50 - an update for your application and as
996:53 - you can see here at the top it says that
996:55 - pods are pending
996:57 - as well if i go down to active revisions
997:00 - you can see here that there is a summary
997:02 - and the status that pods are pending and
997:05 - so just as a note
997:06 - rolling updates allow the deployments
997:09 - update to take place with zero downtime
997:12 - by incrementally updating pods instances
997:15 - with new ones so the pods will be
997:17 - scheduled on nodes with available
997:20 - resources and if the nodes do not have
997:22 - enough resources the pods will stay in a
997:25 - pending state but i don't think we're
997:27 - going to have any problems with these
997:28 - nodes as this application is very light
997:31 - in resources and if i open up cloud
997:33 - shell
997:34 - and run a cube ctl get pods command
997:39 - you will see that new pods have started
997:41 - and you can tell this by the age of the
997:43 - pod as well if you ran the command keep
997:46 - ctl describe pod along with the pod name
997:49 - you could also see the event logs when
997:51 - the pod was created and if i close cloud
997:54 - shell i can see up here at the top of my
997:57 - deployment details it shows that my
997:59 - replicas have one updated four ready
998:03 - three available and one unavailable and
998:05 - if i click on refresh i can see now that
998:08 - my replicas are all updated and
998:11 - available and so now in order to check
998:13 - your new update you can simply go down
998:15 - to exposing services and click on the
998:18 - endpoints link you'll get that redirect
998:20 - notice you can simply click on the link
998:23 - and because the old site may be cached
998:25 - in your browser you may have to refresh
998:27 - your web page
998:29 - and success and you have now completed a
998:32 - rolling update in gke so i wanted to
998:34 - congratulate you on making it to the end
998:37 - of this multi-part demo and hope that
998:40 - it's been extremely useful in excelling
998:42 - your knowledge in gke and so just as a
998:45 - recap you scaled your application to
998:48 - accommodate both less and more replicas
998:51 - you edited your application in the cloud
998:53 - shell editor and rebuilt your container
998:56 - image using cloud build you then applied
998:58 - the new digest to your rolling update
999:01 - and applied that rolling update to your
999:03 - deployment while verifying it all in the
999:06 - end fantastic job on your part as this
999:09 - was a pretty complex and long multi-part
999:13 - demo and you can expect things like what
999:15 - you've experienced in this demo to pop
999:18 - up in your role of being a cloud
999:20 - engineer when dealing with gke and so
999:23 - that's pretty much all i wanted to cover
999:25 - with this multi-part demo working with
999:27 - gke so before you go i wanted to take a
999:31 - few moments to delete all the resources
999:33 - you've created one by one so i'm going
999:35 - to go up to the top i'm going to close
999:37 - all my tabs i'm going to head on over to
999:39 - clusters and so i don't want to delete
999:42 - my cluster just yet but the first thing
999:44 - that i want to do is delete my container
999:46 - images so i'm going to head up to the
999:48 - top and open up cloud shell
999:50 - and i'm going to use the command gcloud
999:53 - container images delete gcr dot io
999:56 - forward slash your google cloud project
999:58 - variable forward slash along with your
1000:01 - first image of box of bow ties colon
1000:04 - 1.0.0
1000:06 - hit enter it's going to prompt you if
1000:08 - you want to continue you want to hit y
1000:09 - for yes and it has now deleted the image
1000:12 - as well you want to delete your latest
1000:14 - image which is
1000:16 - 1.0.1 so i'm going to change the zero to
1000:18 - one hit enter it's going to ask if you
1000:20 - want to continue yes and so the
1000:22 - container images have now been deleted
1000:25 - and so now along with the images you
1000:27 - want to delete the artifacts as well and
1000:29 - those are stored in cloud storage so i'm
1000:31 - going to close down cloud shell i'm
1000:33 - going to head on up to the navigation
1000:35 - menu and i'm going to head down to
1000:36 - storage and you want to select your
1000:38 - bucket that has your project name
1000:40 - underscore cloud build select the source
1000:42 - folder and click on delete and you're
1000:44 - going to get a prompt asking you to
1000:45 - delete the selected folder but in order
1000:48 - to do this you need to type in the name
1000:50 - of the folder so i'm going to type it in
1000:51 - now
1000:52 - you can click on confirm and so now the
1000:54 - folder has been deleted along with the
1000:56 - artifacts and so now that we've taken
1000:59 - care of the images along with the
1001:01 - artifacts we need to clean up our gke
1001:04 - cluster so i'm going to head back on up
1001:06 - to the navigation menu and i'm going to
1001:08 - head on over to kubernetes engine and
1001:10 - the first thing that i want to delete is
1001:12 - the low balancer so i'm going to head on
1001:14 - up to services and ingress and you can
1001:16 - select box of bow tie service and go up
1001:19 - to the top and click on delete you're
1001:21 - going to get a confirmation and you want
1001:23 - to click on delete and it's going to
1001:24 - take a couple minutes you do quick
1001:26 - refresh and the service has finally been
1001:28 - deleted i now want to delete my workload
1001:31 - so i'm going to go over to the left hand
1001:32 - menu click on workloads select the
1001:35 - workload box of bowties and go up to the
1001:38 - top and click on delete and you want to
1001:40 - delete all resources including the
1001:42 - horizontal pod auto scaler so you can
1001:44 - simply click on delete and it may take a
1001:46 - few minutes to delete gonna go up to the
1001:48 - top and hit refresh and my workload has
1001:50 - been deleted and so now all that's left
1001:53 - to delete is the gke cluster itself so
1001:55 - i'm going to go back to clusters so
1001:57 - you're going to select the cluster and
1001:59 - go up to the top and click on delete and
1002:01 - you're going to get a prompt asking you
1002:03 - if you want to delete these storage pods
1002:05 - and these are default storage pods that
1002:07 - are installed with the cluster as well
1002:09 - you can delete the cluster while the
1002:11 - workload is still in play but i have
1002:13 - this habit of being thorough so i wanted
1002:15 - to delete the workload before deleting
1002:18 - the cluster and so you want to go ahead
1002:20 - and click on delete and so that's pretty
1002:22 - much all i have for this demo
1002:24 - and this section on google kubernetes
1002:27 - engine and again congrats on the great
1002:29 - job you can now mark this as complete
1002:32 - and i'll see you in the next one
1002:34 - [Music]
1002:38 - welcome back and in this lesson i will
1002:40 - be covering the features of cloud vpn an
1002:43 - essential service for any engineer to
1002:46 - know about when looking to connect
1002:48 - another network to google cloud whether
1002:50 - it be your on-premises network another
1002:53 - cloud provider
1002:54 - or even when connecting to vpcs
1002:57 - this service is a must know for any
1002:59 - engineer and for the exam so with that
1003:02 - being said let's dive in
1003:04 - now cloudvpn securely connects your peer
1003:07 - network to your vpc network through an
1003:10 - ipsec vpn connection when i talk about a
1003:13 - peer network this is referring to an
1003:16 - on-premises vpn device or vpn service a
1003:20 - vpn gateway hosted by another cloud
1003:22 - provider such as aws or azure or another
1003:26 - google cloud vpn gateway and so this is
1003:29 - an ipsec or encrypted tunnel from your
1003:32 - peer network to your vpc network that
1003:35 - traverses the public internet and so for
1003:38 - those who don't know ipsec being short
1003:40 - for internet security protocol and this
1003:43 - is a set of protocols using algorithms
1003:46 - allowing the transport of secure data
1003:49 - over an ip network ipsec operates at the
1003:52 - network layer so layer 3 of the osi
1003:55 - model which allows it to be independent
1003:58 - of any applications although it does
1004:00 - come with some additional overhead so
1004:02 - please be aware and so when creating
1004:05 - your cloud vpn
1004:06 - traffic traveling between the two
1004:08 - networks is encrypted by one vpn gateway
1004:12 - and then decrypted by the other vpn
1004:15 - gateway now moving on to some details
1004:17 - about cloud vpn this is a regional
1004:20 - service and so please take that into
1004:22 - consideration when connecting your
1004:24 - on-premises location to google cloud for
1004:27 - the least amount of latency it also
1004:30 - means that if that region were to go
1004:31 - down you would lose your connection
1004:33 - until the region is back up and running
1004:36 - now cloud vpn is also a site-to-site vpn
1004:39 - only and therefore it does not support
1004:42 - site-to-client so this means that if you
1004:44 - have a laptop or a computer at home you
1004:47 - cannot use this option with a vpn client
1004:50 - to connect to google cloud cloudvpn can
1004:53 - also be used in conjunction with private
1004:56 - google access for your on-premises hosts
1004:59 - so if you're using private google access
1005:02 - within gcp you can simply connect to
1005:05 - your data center with vpn and have
1005:08 - access as if you were already in gcp so
1005:11 - if you're looking to extend private
1005:13 - google access to your on-premises data
1005:15 - center cloud vpn would be the perfect
1005:18 - choice and so when it comes to speeds
1005:20 - each cloud vpn tunnel can support up to
1005:23 - three gigabits per second total for
1005:26 - ingress and egress as well routing
1005:29 - options that are available are both
1005:31 - static and dynamic but are only
1005:33 - available as dynamic for aha vpn and
1005:37 - lastly cloudvpn supports ik version 1
1005:40 - and ike version 2 using shared secret
1005:44 - and for those of you who are unaware ike
1005:46 - stands for internet key exchange and
1005:48 - this helps establish a secure
1005:50 - authenticated communication channel by
1005:53 - using a key exchange algorithm to
1005:55 - generate a shared secret key to encrypt
1005:58 - communications so know that when you
1006:00 - choose cloudvpn that your connection is
1006:03 - both private and secure so now there are
1006:06 - two types of vpn options that are
1006:08 - available in google cloud one being the
1006:11 - classic vpn and the other being h a vpn
1006:14 - and i'm going to take a moment to go
1006:16 - through the differences now with classic
1006:18 - vpn this provides a service level
1006:20 - agreement of 99.9 percent also known as
1006:24 - an sla of three nines while h a vpn
1006:28 - provides a four nines sla when
1006:31 - configured with two interfaces and two
1006:33 - external ips now when it comes to
1006:35 - routing classic vpn supports both static
1006:39 - and dynamic routing whereas havpn
1006:42 - supports dynamic routing only and this
1006:45 - must be done through bgp using cloud
1006:48 - router classic vpn gateways have a
1006:51 - single interface
1006:52 - and a single external ip address and
1006:55 - support tunnels using static routing as
1006:58 - well as dynamic routing and the static
1007:00 - routing can be either route based or
1007:02 - policy based whereas with havpn it can
1007:06 - be configured for two interfaces and two
1007:09 - external ips for true ha capabilities
1007:12 - and as mentioned earlier when it comes
1007:14 - to routing for havpn dynamic routing is
1007:18 - the only available option now the one
1007:20 - thing about classic vpn is that google
1007:23 - cloud is deprecating certain
1007:24 - functionality on october 31st of 2021
1007:29 - and is recommending all their customers
1007:32 - to move to h a vpn and so know that this
1007:35 - has not been reflected in the exam and
1007:37 - not sure if and when it will be but know
1007:40 - that when you are creating a cloud vpn
1007:42 - connection in your current environment h
1007:45 - a vpn is the recommended option and so
1007:48 - now i wanted to dive into some
1007:49 - architecture of how cloud vpn is set up
1007:53 - for these two options starting with
1007:55 - classic vpn
1007:57 - now as i said before classic vpn is a
1008:00 - cloud vpn solution
1008:02 - that lets you connect your peer network
1008:04 - to your vpc network through an ipsec vpn
1008:08 - connection in a single region now unlike
1008:11 - h a vpn classic vpn offers no redundancy
1008:15 - out of the box you would have to create
1008:17 - another vpn connection and if the
1008:20 - connection were to go down you would
1008:22 - have to manually switch over the
1008:23 - connection from one to the other now as
1008:26 - you can see here when you create a vpn
1008:28 - gateway google cloud automatically
1008:31 - chooses only one external ip address for
1008:34 - its interface and the diagram shown here
1008:37 - shows that of a classic vpn network
1008:41 - connected from the bowtie dash network
1008:44 - vpc in bowtie project to an on-premises
1008:47 - network configured using a static route
1008:50 - to connect now moving on to h-a-v-p-n
1008:54 - again this is a highly available cloud
1008:56 - vpn solution that lets you connect your
1008:59 - peer network to your vpc network using
1009:02 - an ipsec vpn connection in a single
1009:05 - region exactly like classic vpn where
1009:09 - havpn differs is that it provides four
1009:12 - nines sla and as you can see here it
1009:14 - supports double the connections so when
1009:17 - you create an h a vpn gateway google
1009:20 - cloud automatically chooses two external
1009:23 - ip addresses
1009:24 - one for each of its fixed number of two
1009:27 - interfaces each ip address is
1009:30 - automatically chosen from a unique
1009:32 - address pool to support high
1009:34 - availability each of these ha vpn
1009:36 - gateway interfaces supports multiple
1009:39 - tunnels which allows you to create
1009:41 - multiple h a vpn gateways and you can
1009:44 - configure an h a vpn gateway with only
1009:47 - one active interface and one public ip
1009:50 - address however this configuration does
1009:53 - not provide a four nines sla now for h a
1009:57 - vpn gateway you configure an external
1010:00 - peer vpn gateway resource that
1010:03 - represents your physical peer gateway in
1010:05 - google cloud you can also create this
1010:07 - resource as a standalone resource and
1010:09 - use it later in this diagram the two
1010:12 - interfaces of an h a vpn gateway in the
1010:16 - bowtie network vpc living in bowtie
1010:19 - project are connected to two peer vpn
1010:22 - gateways in an on-premises network and
1010:25 - this connection is using dynamic routing
1010:28 - with bgp connecting to a cloud router in
1010:31 - google cloud now when it comes to the
1010:33 - times when using cloudvpn makes sense
1010:37 - one of the first things you should think
1010:38 - about is whether or not you need public
1010:41 - internet access so when you're sharing
1010:43 - files or your company needs a specific
1010:46 - sas product that's only available on the
1010:48 - internet vpn would be your only option
1010:51 - as well when you're looking to use
1010:53 - interconnect and your peering location
1010:56 - is not available so you're not able to
1010:58 - connect your data center to the
1011:00 - colocation facility of your choice vpn
1011:03 - would be the only other option that you
1011:05 - have as well if budget constraints come
1011:08 - into play when deciding on connecting to
1011:10 - your peer network vpn would always be
1011:13 - the way to go as cloud interconnect is
1011:16 - going to be the more expensive option
1011:18 - and lastly if you don't need a high
1011:20 - speed network and low latency is not
1011:23 - really a concern for you and you only
1011:25 - have regular outgoing traffic coming
1011:28 - from google cloud then vpn would suffice
1011:31 - for your everyday needs and so the
1011:33 - options shown here are also the deciding
1011:36 - factors to look for when it comes to
1011:39 - questions in the exam that refer to
1011:41 - cloudvpn or connecting networks and so
1011:44 - that's pretty much all i have for this
1011:46 - short lesson on cloudvpn so you can now
1011:49 - mark this lesson as complete and let's
1011:52 - move on to the next one
1011:53 - [Music]
1011:57 - welcome back and in this lesson i'm
1011:59 - going to go over another connection type
1012:02 - that allows for on-premises connectivity
1012:05 - to your google cloud vpcs which is cloud
1012:08 - interconnect other than vpn this is the
1012:11 - other connection type that allows
1012:13 - connectivity from your on-premises
1012:15 - environment to your google cloud vpc
1012:18 - cloud interconnect is the most common
1012:20 - connection for most larger organizations
1012:23 - and are for those that demand fast low
1012:26 - latency connections this lesson will
1012:28 - cover the features of cloud interconnect
1012:31 - and the different types that are
1012:32 - available so with that being said let's
1012:34 - dive in so getting right into it cloud
1012:37 - interconnect is a low latency highly
1012:40 - available connection between your
1012:42 - on-premises data center and google cloud
1012:45 - vpc networks also cloud interconnect
1012:48 - connections provide internal ip address
1012:51 - connection which means internal ip
1012:54 - addresses are directly accessible from
1012:57 - both networks and so on premises hosts
1013:00 - can use internal ip addresses and take
1013:03 - advantage of private google access
1013:06 - rather than external ip addresses to
1013:09 - reach google apis and services traffic
1013:12 - between your on-premises network and
1013:14 - your vpc network doesn't traverse the
1013:17 - public internet traffic traverses a
1013:19 - dedicated connection or through a
1013:21 - service provider with a dedicated
1013:23 - connection your vpc network's internal
1013:26 - ip addresses are directly accessible
1013:29 - from your on-premises network now unlike
1013:32 - vpn this connection is not encrypted if
1013:35 - you need to encrypt your traffic at the
1013:37 - ip layer you can create one or more
1013:39 - self-managed vpn gateways in your vpc
1013:43 - network and assign a private ip address
1013:46 - to each gateway now although this may be
1013:48 - a very fast connection it also comes
1013:51 - with a very high price tag now unlike
1013:53 - vpn this connection type is not
1013:56 - encrypted if you need to encrypt your
1013:58 - traffic at the ip layer you can create
1014:01 - one or more self-managed vpn gateways in
1014:04 - your vpc network and assign a private ip
1014:07 - address to each gateway now although
1014:10 - this may be a very fast connection it
1014:12 - also comes with a very high price tag
1014:15 - and is the highest price connection type
1014:17 - cloud interconnect offers two options
1014:20 - for extending your on-premises network
1014:22 - dedicated interconnect which provides a
1014:25 - direct physical connection between your
1014:27 - on-premises network and google's network
1014:30 - as well as partner interconnect which
1014:32 - provides connectivity between your
1014:34 - on-premises and vpc networks through a
1014:38 - supported service provider and so i
1014:40 - wanted to take a moment to highlight the
1014:42 - different options for cloud interconnect
1014:44 - starting with dedicated interconnect now
1014:47 - dedicated interconnect provides a direct
1014:50 - physical connection between your
1014:52 - on-premises network and google's network
1014:55 - dedicated interconnect enables you to
1014:58 - transfer large amounts of data between
1015:00 - your network and google cloud which can
1015:03 - be more cost effective than purchasing
1015:06 - additional bandwidth over the public
1015:08 - internet for dedicated interconnect you
1015:10 - provision a dedicated interconnect
1015:13 - connection between the google network
1015:15 - and your own router in a common location
1015:18 - the following example shown here shows a
1015:21 - single dedicated interconnect connection
1015:24 - between a vpc network and an on-premises
1015:27 - network for this basic setup a dedicated
1015:30 - interconnect connection is provisioned
1015:32 - between the google network and the
1015:34 - on-premises router in a common
1015:37 - co-location facility when you create a
1015:39 - vlan attachment you associate it with a
1015:42 - cloud router this cloud router creates a
1015:44 - bgp session for the vlan attachment and
1015:48 - its corresponding on-premises peer
1015:50 - router these routes are added as custom
1015:53 - dynamic routes in your vpc network and
1015:55 - so for dedicated interconnect connection
1015:57 - capacity is delivered over one or more
1016:01 - 10 gigabits per second or 100 gigabits
1016:04 - per second ethernet connections with the
1016:06 - follow-on maximum capacity supported per
1016:09 - interconnect connection so with your 10
1016:12 - gigabit per second connections you can
1016:14 - get up to eight connections totaling a
1016:17 - speed of 80 gigabits per second with the
1016:19 - 100 gigabit per second connection you
1016:22 - can connect two of them together to have
1016:24 - a total speed of 200 gigabits per second
1016:27 - and so for dedicated interconnect your
1016:30 - network must physically meet google's
1016:32 - network in a supported co-location
1016:35 - facility also known as an interconnect
1016:37 - connection location this facility
1016:40 - is where a vendor the co-location
1016:42 - facility provider provisions a circuit
1016:45 - between your network and a google edge
1016:47 - point of presence also known as a pop
1016:50 - the setup shown here is suitable for
1016:52 - non-critical applications that can
1016:55 - tolerate some downtime but for sensitive
1016:57 - production applications at least two
1017:00 - interconnect connections in two
1017:03 - different edge availability domains are
1017:05 - recommended now partner interconnect
1017:08 - provides connectivity between your
1017:10 - on-premises network and your vpc network
1017:13 - through a supported service provider so
1017:15 - this is not a direct connection from
1017:17 - your on-premises network to google as
1017:20 - the service provider provides a conduit
1017:22 - between your on-premises network and
1017:25 - google's pop now a partner interconnect
1017:27 - connection is useful if a dedicated
1017:30 - interconnect co-location facility is
1017:33 - physically out of reach or your
1017:35 - workloads don't warrant an entire 10
1017:38 - gigabit per second connection for
1017:40 - partner interconnect 50 megabits per
1017:42 - second to 50 gigabits per second vlan
1017:45 - attachments are available with the
1017:47 - maximum supported attachment size of 50
1017:51 - gigabits per second now service
1017:53 - providers have existing physical
1017:55 - connections to google's network that
1017:57 - they make available for their customer
1017:59 - to use so in this example shown here you
1018:02 - would provision a partner interconnect
1018:05 - connection with a service provider and
1018:07 - connecting your on-premises network to
1018:10 - that service provider after connectivity
1018:12 - is established with the service provider
1018:15 - a partner interconnect connection is
1018:17 - requested from the service provider and
1018:19 - the service provider configures your vln
1018:22 - attachment for use once your connection
1018:24 - is provisioned you can start passing
1018:26 - traffic between your networks by using
1018:28 - the service providers network now there
1018:31 - are many more detailed steps involved to
1018:33 - get a connection established along with
1018:35 - traffic flowing but i just wanted to
1018:38 - give you a high level summary of how a
1018:40 - connection would be established with a
1018:42 - service provider now as well to build a
1018:45 - highly available topology you can use
1018:48 - multiple service providers as well you
1018:50 - must build redundant connections for
1018:53 - each service provider in each
1018:55 - metropolitan and so now there's a couple
1018:57 - more connection types that run through
1018:59 - service providers that are not on the
1019:01 - exam but i wanted you to be aware of
1019:03 - them if ever the situation arises in
1019:06 - your role as a cloud engineer so the
1019:08 - first one is direct peering and direct
1019:10 - peering enables you to establish a
1019:13 - direct peering connection between your
1019:15 - business network and google's edge
1019:18 - network and exchange high throughput
1019:20 - cloud traffic this capability is
1019:23 - available at any of more than 100
1019:26 - locations in 33 countries around the
1019:28 - world when established direct peering
1019:31 - provides a direct path from your
1019:33 - on-premises network to google services
1019:36 - including google cloud products that can
1019:39 - be exposed through one or more public ip
1019:42 - addresses traffic from google's network
1019:45 - to your on-premises network also takes
1019:47 - that direct path
1019:49 - including traffic from vpc networks in
1019:52 - your projects now you can also save
1019:54 - money and receive direct egress pricing
1019:57 - for your projects after they have
1019:59 - established direct peering with google
1020:02 - direct peering exists outside of google
1020:04 - cloud unless you need to access google
1020:07 - workspace applications the recommended
1020:09 - methods of access to google cloud are
1020:12 - dedicated interconnect or partner
1020:15 - interconnect establishing a direct
1020:17 - peering connection with google is free
1020:19 - and there are no costs per port and no
1020:22 - per hour charges you just have to meet
1020:25 - google's technical peering requirements
1020:27 - and can then be considered for the
1020:29 - direct peering service
1020:31 - and moving on to the last connection
1020:33 - type is cdn interconnect now i know we
1020:36 - haven't gotten into cdns in the course
1020:39 - as the exam does not require you to know
1020:41 - it but cdn standing for content delivery
1020:45 - network is what caches content at the
1020:48 - network edge to deliver files faster to
1020:51 - those requesting it one of the main ways
1020:53 - to improve website performance now
1020:56 - moving on to cdn interconnect this
1020:58 - connection type enables select
1021:01 - third-party cdn providers like akamai
1021:04 - and cloudflare along with others to
1021:07 - establish and optimize your cdn
1021:09 - population costs by using direct peering
1021:12 - links with google's edge network and
1021:15 - enables you to direct your traffic from
1021:17 - your vpc networks to the provider's
1021:20 - network and so your egress traffic from
1021:23 - google cloud through one of these links
1021:26 - benefits from the direct connectivity to
1021:29 - the cdn provider and is billed
1021:31 - automatically with reduced pricing
1021:34 - typical use cases for cdn interconnect
1021:37 - is if you're populating your cdn with
1021:40 - large data files from google cloud or
1021:42 - you have frequent content updates stored
1021:45 - in different cdn locations and so
1021:47 - getting into the use cases of when to
1021:50 - use cloud interconnect a big purpose for
1021:53 - it would be to prevent traffic from
1021:55 - traversing the public internet it is a
1021:58 - dedicated physical connection right to
1022:00 - google's data centers so when you need
1022:02 - an extension of your vpc network to your
1022:05 - on-premises network interconnect is
1022:08 - definitely the way to go now in speed
1022:10 - and low latencies of extreme importance
1022:13 - interconnect is always the best option
1022:16 - and will support up to 200 gigabits per
1022:18 - second as well when you have heavy
1022:21 - outgoing traffic or egress traffic
1022:24 - leaving google cloud cloud interconnect
1022:26 - fits the bill perfectly and lastly when
1022:29 - it comes to private google access this
1022:31 - travels over the backbone of google's
1022:34 - network and so when you are connected
1022:35 - with interconnect this is an extension
1022:38 - of that backbone and therefore your
1022:40 - on-premises hosts will be able to take
1022:43 - advantage of private google access and
1022:45 - so i hope this has given you some
1022:47 - clarity on the differences between the
1022:49 - different connection types and how to
1022:52 - extend your google cloud network to a
1022:55 - peer or on-premises network so that's
1022:58 - pretty much all i had to cover when it
1023:00 - comes to cloud interconnect so you can
1023:02 - now mark this lesson as complete and
1023:05 - let's move on to the next one
1023:06 - [Music]
1023:10 - welcome back in this lesson i'm going to
1023:13 - be covering an overview of app engine
1023:16 - now this is not a deep dive lesson for
1023:18 - app engine as there is so much to cover
1023:21 - with this service but i will be listing
1023:23 - a lot of the features of app engine to
1023:25 - give you a good feel for what it can do
1023:28 - and what you will need to know for the
1023:30 - exam so with that being said let's dive
1023:33 - in now app engine is a fully managed
1023:36 - serverless platform for developing and
1023:39 - hosting web applications at scale this
1023:42 - is google's platform as a service
1023:44 - offering that was designed for
1023:46 - developers so that they can develop
1023:49 - their application and let app engine do
1023:52 - all the heavy lifting by taking care of
1023:54 - provisioning the servers and scaling the
1023:57 - instances needed based on demand app
1024:00 - engine gives you the flexibility of
1024:02 - launching your code as is or you can
1024:04 - launch it as a container and uses
1024:07 - runtime environments of a variety of
1024:09 - different programming languages like
1024:11 - python java node.js go ruby php or net
1024:18 - applications deployed on app engine that
1024:21 - experience regular traffic fluctuations
1024:24 - or newly deployed applications where
1024:26 - you're simply unsure about the load are
1024:28 - auto scaled accordingly and
1024:30 - automatically your apps scale up to the
1024:33 - number of instances that are running to
1024:36 - provide consistent performance or scale
1024:39 - down to minimize idle instances and
1024:42 - reduces costs app engine also has the
1024:45 - capabilities of being able to deal with
1024:48 - rapid scaling for sudden extreme spikes
1024:51 - of traffic having multiple versions of
1024:53 - your application within each service
1024:56 - allows you to quickly switch between
1024:58 - different versions of that application
1025:00 - for rollbacks testing or other temporary
1025:04 - events you can route traffic to one or
1025:07 - more specific versions of your
1025:08 - application by migrating or splitting
1025:11 - traffic and you can use traffic
1025:13 - splitting to specify a percentage
1025:16 - distribution of traffic across two or
1025:19 - more of the versions within a service
1025:21 - and allows you to do a b testing or blue
1025:24 - green deployment between your versions
1025:27 - when rolling out new features app engine
1025:30 - supports connecting to back-end storage
1025:32 - services such as cloud firestore cloud
1025:35 - sql and cloud storage along with
1025:38 - connecting to on-premises databases and
1025:41 - even external databases that are hosted
1025:43 - on other public clouds app engine is
1025:46 - available in two separate flavors
1025:48 - standard and flexible environments and
1025:51 - each environment offers their own set of
1025:54 - features that i will get into in just a
1025:56 - sec
1025:57 - now as i mentioned before app engine is
1025:59 - available in standard and flexible
1026:02 - environments and depending on your
1026:04 - application needs either one will
1026:06 - support what you need for your workload
1026:09 - or you could even use both
1026:11 - simultaneously the features shown here
1026:14 - will give you a feel for both types of
1026:16 - environments and i'm going to be doing a
1026:18 - quick run through summarizing the
1026:20 - features of each starting with the
1026:22 - standard environment now with the
1026:24 - standard environment applications run in
1026:27 - a secure sandboxed environment allowing
1026:31 - app engine standard to distribute
1026:33 - requests across multiple servers and
1026:36 - scaling servers to meet traffic demands
1026:39 - your application runs with its own
1026:41 - secure reliable environment that is
1026:44 - independent of the hardware
1026:47 - operating system or physical location of
1026:49 - the server the source code is written in
1026:52 - specific versions
1026:54 - of the supported programming languages
1026:56 - and with app engine standard it is
1026:59 - intended to run for free or at a very
1027:01 - low cost where you pay only for what you
1027:04 - need and when you need it with app
1027:07 - engine standard your application can
1027:09 - scale to zero instances when there is no
1027:12 - traffic app engine standard is designed
1027:16 - for sudden and extreme spikes of traffic
1027:19 - which require immediate scaling and
1027:21 - pricing for standard app engine is based
1027:24 - on instance hours and so when it comes
1027:27 - to features for app engine flexible the
1027:30 - application instances run within docker
1027:33 - containers that includes a custom
1027:35 - runtime or source code written in other
1027:38 - programming languages these docker
1027:40 - containers are then run on compute
1027:43 - engine vms app engine flexible will run
1027:46 - any source code that is written in a
1027:48 - version of any of the supported
1027:50 - programming languages for app engine
1027:52 - flexible and unlike the standard
1027:55 - environment unfortunately there is no
1027:57 - free quota for app engine flexible as
1028:00 - well app engine flexible is designed for
1028:03 - consistent traffic or for applications
1028:06 - that experience regular traffic
1028:08 - fluctuations and pricing is based on the
1028:11 - vm resources and not on instance hours
1028:14 - like app engine standard and so where
1028:16 - app engine flexible really shines over
1028:19 - app engine standard are how the vms are
1028:22 - managed so instances are health checked
1028:25 - healed as necessary and co-located with
1028:27 - other services within the project the
1028:30 - vm's operating system is updated and
1028:32 - applied automatically as well vms are
1028:35 - restarted on a weekly basis to make sure
1028:38 - any necessary operating system and
1028:40 - security updates are applied ssh along
1028:43 - with root access are available to the vm
1028:46 - instances running your containers now
1028:48 - deploying applications to app engine is
1028:51 - as simple as using the gcloud app deploy
1028:53 - command
1028:54 - this command automatically builds a
1028:57 - container image from your configuration
1028:59 - file by using the cloud build service
1029:02 - and then deploys that image to app
1029:04 - engine now an app engine application is
1029:08 - made up of a single application resource
1029:11 - that consists of one or more services
1029:14 - each service can be configured to use
1029:16 - different runtimes and to operate with
1029:19 - different performance settings
1029:21 - services and app engine are used to
1029:23 - factor your large applications into
1029:26 - logical components
1029:28 - that can securely share app engine
1029:30 - features and communicate with one
1029:32 - another these app engine services become
1029:35 - loosely coupled behaving like
1029:37 - microservices now within each service
1029:40 - you deploy versions of that service and
1029:43 - each version then runs within one or
1029:47 - more instances depending on how much
1029:49 - traffic you configured it to handle
1029:52 - having multiple versions of your
1029:53 - application within each service allows
1029:56 - you to quickly switch between different
1029:58 - versions of that application for
1030:00 - rollbacks testing or other temporary
1030:03 - events you can route traffic to one or
1030:06 - more specific versions of your
1030:08 - application by migrating traffic to one
1030:11 - specific version
1030:12 - or splitting your traffic between two
1030:14 - separate versions and so the versions
1030:17 - within your services run on one or more
1030:20 - instances by default app engine scales
1030:23 - your application to match the load your
1030:26 - applications will scale up the number of
1030:28 - instances that are running to provide
1030:31 - consistent performance or scale down to
1030:34 - minimize idle instances and reduce costs
1030:37 - now when it comes to managing instances
1030:40 - app engine can automatically create and
1030:43 - shut down instances as traffic
1030:45 - fluctuates or you can specify a number
1030:48 - of instances to run
1030:49 - regardless of the amount of traffic you
1030:52 - can also configure how and when new
1030:54 - instances are created by specifying a
1030:57 - scaling type for your application and
1031:00 - how you do this is you specify the
1031:02 - scaling type in your application's
1031:04 - app.yaml file now there are three
1031:07 - different types of scaling choices to
1031:09 - choose from and the first one being
1031:11 - automatic scaling and this scaling type
1031:14 - creates instances based on request rate
1031:17 - response latencies and other application
1031:19 - metrics you can specify thresholds for
1031:22 - each of these metrics as well as a
1031:25 - minimum number instances to keep running
1031:27 - at all times if you use automatic
1031:30 - scaling each instance in your
1031:32 - application has its own queue for
1031:35 - incoming requests before the queues
1031:37 - become long enough to have a visible
1031:39 - effect on your app's latency app engine
1031:42 - automatically creates one or more new
1031:45 - instances to handle the load the second
1031:48 - type is basic scaling and this creates
1031:51 - instances when your application receives
1031:53 - requests each instance is shut down when
1031:56 - the application becomes idle basic
1031:59 - scaling is fantastic for intermittent
1032:02 - workloads or if you're looking to drive
1032:04 - your application by user activity app
1032:07 - engine will try to keep your costs low
1032:09 - even though it might result in higher
1032:11 - latency as the volume of incoming
1032:14 - requests increase and so the last
1032:16 - scaling type is manual scaling and this
1032:18 - is where you specify the number of
1032:20 - instances that continuously run
1032:23 - regardless of the load so these are
1032:25 - instances that are constantly running
1032:27 - and this allows complex startup tasks on
1032:30 - the instances to have already been
1032:33 - completed when receiving requests and
1032:35 - applications that rely on the state of
1032:38 - the memory over time so this is ideal
1032:41 - for instances whose configuration
1032:43 - scripts require some time to fully run
1032:46 - their course so now that i've gone over
1032:48 - managing the instances i wanted to take
1032:51 - a few moments to go over how app engine
1032:54 - manages traffic starting with traffic
1032:57 - migration now traffic migration switches
1032:59 - the request routing between the versions
1033:02 - within a service of your application
1033:05 - moving traffic from one or more versions
1033:08 - to a single new version so when
1033:10 - deploying a new version with the same
1033:12 - name of an existing version it causes an
1033:15 - immediate traffic migration all
1033:17 - instances of the old version are
1033:19 - immediately shut down in app engine
1033:22 - standard you can choose to route
1033:24 - requests to the target version either
1033:27 - immediately or gradually you can also
1033:30 - choose to enable warm-up requests if you
1033:32 - want the traffic gradually migrated to a
1033:35 - version gradual traffic migration is not
1033:38 - supported in app engine flexible and
1033:41 - traffic is migrated immediately now one
1033:44 - thing to note is that when you
1033:46 - immediately migrate traffic to a new
1033:48 - version without any running instances
1033:51 - then your application will have a spike
1033:53 - in latency for loading requests
1033:56 - while instances are being created and so
1033:59 - another way to manage traffic on app
1034:01 - engine is through traffic splitting now
1034:03 - you can use traffic splitting to specify
1034:06 - a percentage distribution of traffic
1034:09 - across two or more of the versions
1034:11 - within a service so in this example if
1034:14 - i'm deploying a new version of my
1034:16 - service i can decide on how i want to
1034:19 - distribute traffic to each version of my
1034:22 - application and so i decide that i want
1034:24 - to keep my current version in play but
1034:26 - roll out the new version of my
1034:28 - application to 10 of my users leaving
1034:32 - the old version was still 90 of the
1034:34 - traffic going to that version and so
1034:37 - splitting traffic allows you to conduct
1034:39 - a b testing between your versions and
1034:42 - provides control over the pace when
1034:45 - rolling out features and just as a note
1034:48 - when you've specified two or more
1034:50 - versions for splitting you must choose
1034:52 - whether to split traffic by either by
1034:55 - either ip address http cookie or do it
1034:59 - randomly now again this has not been a
1035:02 - deep dive lesson on app engine but i
1035:05 - hope this has given you an overview of
1035:07 - the features that are available as the
1035:09 - exam touches on these features i also
1035:12 - wanted to give you some familiarity with
1035:14 - the service itself as coming up next i
1035:17 - will be going into a demo where we will
1035:19 - be launching an application using app
1035:22 - engine and trying on some of these
1035:24 - features for yourself and so that's
1035:26 - pretty much all i wanted to cover
1035:28 - when it comes to app engine so you can
1035:30 - now mark this lesson as complete and
1035:32 - whenever you're ready join me in the
1035:34 - console where you will deploy an
1035:36 - application on app engine and try out
1035:38 - some of these features for yourself
1035:40 - [Music]
1035:44 - welcome back and in this demo you're
1035:47 - going to build another application to
1035:49 - deploy on app engine called serverless
1035:52 - bowties this demo will run you through
1035:55 - the ins and outs of deploying a website
1035:57 - application on app engine along with
1036:00 - managing it while experiencing no
1036:02 - downtime so there's quite a bit of work
1036:04 - to do here so with that being said let's
1036:07 - dive in and so here in my console i am
1036:09 - logged in as tonybowtieace gmail.com
1036:13 - under project bowtie inc and so the
1036:15 - first thing i want to do here is i want
1036:18 - to head on over to app engine so in
1036:20 - order to do that i'm going to go to the
1036:22 - top left-hand navigation menu and i'm
1036:24 - going to go down to app engine and
1036:26 - because i haven't created any
1036:27 - applications i'm going to be brought to
1036:29 - this splash page now in order to deploy
1036:32 - this application we're not going to be
1036:34 - doing it through the console but we will
1036:36 - be doing it through the command line and
1036:38 - so to get started with that i'm going to
1036:39 - go up to the top and open up cloud shell
1036:42 - i'm going to make this bigger for better
1036:43 - viewing and so in order for me to get
1036:45 - the code to launch this application i'm
1036:47 - going to be cloning my github repository
1036:50 - into cloud shell and so for those of you
1036:52 - who haven't deleted your repository from
1036:55 - the last demo you can go ahead and skip
1036:58 - the cloning step for those of you who
1037:00 - need to clone your repository you will
1037:02 - find a link to the instructions in the
1037:04 - lesson text and there you'll be able to
1037:06 - retrieve the command which will be git
1037:08 - clone along with the address of the repo
1037:11 - i'm going to hit enter and because i've
1037:13 - already cloned this repo i'm receiving
1037:15 - this error i'm going to do an ls and as
1037:17 - you can see here the google cloud
1037:19 - associate cloud engineer repo has
1037:21 - already been cloned so i'm going to cd
1037:23 - into that directory and in order to get
1037:26 - the code i'm going to simply run the
1037:28 - command git pull
1037:30 - to get the latest and i'm going to
1037:32 - simply clear my screen and so now that
1037:34 - i've retrieved all the code that i need
1037:36 - in order to deploy it i need to go to
1037:38 - that directory and that directory is
1037:40 - going to be 11 serverless services
1037:44 - forward slash 0 1 serverless bowties and
1037:47 - hit enter you're going to run ls and
1037:49 - here you will find two versions of the
1037:51 - website application site v1 and site v2
1037:55 - along with the instructions if you want
1037:57 - to follow straight from here and so i
1037:59 - want to go ahead and deploy my first
1038:01 - website application so i'm going to cd
1038:04 - into site v1 ls and here you will see
1038:08 - the app.yaml which is the configuration
1038:11 - file that you will need in order to run
1038:13 - the application on app engine and so
1038:15 - before i go ahead and deploy this i
1038:18 - wanted to take a moment to show you the
1038:20 - application configuration so i'm going
1038:22 - to go ahead and open it up in cloud
1038:24 - shell editor so i'm going to type in
1038:25 - edit app.yaml enter and as you can see
1038:29 - here my runtime is python 3.7 and as you
1038:32 - can see i have a default expiration of
1038:35 - two seconds along with an expiration
1038:37 - underneath each handler and this is due
1038:40 - to the caching issue that happens with
1038:42 - app engine and so in order to simulate
1038:44 - traffic splitting between the two
1038:46 - website applications in order to make
1038:48 - things easy i needed to expire the cash
1038:51 - and this is an easy way to do it now
1038:54 - there may be applications out there that
1038:56 - do need that caching and so the
1038:58 - expiration may be a lot higher but for
1039:00 - the purposes of this demo two seconds
1039:02 - expiration should suffice as well
1039:05 - explain the two handlers here the first
1039:08 - one showing the files that will be
1039:09 - uploaded to the cloud storage bucket as
1039:12 - well as the second stating what static
1039:14 - files will be presented and so i'm going
1039:16 - to go ahead back over to my terminal and
1039:18 - i'm going to go ahead and clear my
1039:20 - screen and i'm going to go ahead and run
1039:22 - the command gcloud app deploy with the
1039:25 - flag dash dash version and this is going
1039:28 - to be version one so i'm going to go
1039:30 - ahead and hit enter and you may get a
1039:32 - pop-up asking you to authorize this api
1039:34 - call using your credentials and you want
1039:36 - to click on authorize and you're going
1039:38 - to be prompted to enter in a region that
1039:41 - you want to deploy your website
1039:43 - application to we want to keep this in
1039:45 - us east one so i'm going to type in 15
1039:48 - hit enter
1039:49 - and you're going to be prompted to
1039:51 - verify your configuration for your
1039:53 - application before it's deployed you're
1039:55 - also going to be prompted if you want to
1039:56 - continue definitely yes so i'm going to
1039:59 - hit y enter and so now as you've seen
1040:01 - the files have been uploaded to cloud
1040:03 - storage and app engine is going to take
1040:06 - a few minutes to create the service
1040:08 - along with the version so i'm going to
1040:10 - let it do the needful and i'll be back
1040:12 - before you know it okay and my
1040:13 - application has been deployed now
1040:15 - although you don't see it here in the
1040:17 - console it has been deployed all i need
1040:20 - to do is refresh my screen but i wanted
1040:22 - to just point out a couple things that
1040:24 - is shown here in the terminal the first
1040:26 - one being the default service now the
1040:29 - first time you deploy a version of your
1040:31 - application it will always deploy to the
1040:34 - default service initially and only then
1040:36 - will you be able to deploy another named
1040:39 - service to app engine now here where it
1040:42 - says setting traffic split for service
1040:44 - this is referring to the configuration
1040:46 - for traffic splitting being applied in
1040:49 - the background which i will be getting
1040:51 - into a little bit later and lastly the
1040:54 - url shown for the deployed service will
1040:56 - always start with the name of your
1040:58 - project followed by.ue.r.appspot.com
1041:03 - which is why in production google
1041:06 - recommends to run app engine in a
1041:08 - completely separate project
1041:10 - before this demo running it in the same
1041:12 - project that we've been using will
1041:14 - suffice okay so let's go ahead and take
1041:17 - a look at the application so i'm going
1041:19 - to go back up to the top here to the
1041:21 - navigation menu and i'm gonna go down to
1041:24 - app engine and go over to services and
1041:26 - so here you will see the default service
1041:29 - with version one and if i go over to
1041:31 - versions i will see here my version the
1041:34 - status
1041:35 - the traffic allocation along with any
1041:38 - instances that it needs the run time the
1041:41 - specific environment and i'll have some
1041:44 - diagnostic tools here that i could use
1041:46 - and so because this is a static website
1041:48 - application we won't be using any
1041:51 - instances and so this will always show a
1041:53 - zero so now i want to head back on over
1041:56 - to services and i'm going to launch my
1041:58 - application by simply clicking on this
1042:00 - hot link
1042:01 - and success serverless bow ties for all
1042:04 - and so it looks like my application has
1042:06 - been successfully deployed so i'm going
1042:08 - to close down this tab now there's a
1042:10 - couple of things that i wanted to run
1042:12 - through here on the left hand menu just
1042:14 - for your information so here i can click
1042:17 - on instances and if i was running any
1042:19 - instances i am able to see a summary of
1042:22 - those instances and i can click on the
1042:25 - drop down here and choose a different
1042:27 - metric and find out any information that
1042:29 - i need as well i can click on this drop
1042:32 - down and select a version if i had
1042:34 - multiple versions which i do not
1042:37 - clicking on task queues here is where i
1042:39 - can manage my task queues but this is a
1042:41 - legacy service that will soon be
1042:43 - deprecated clicking on cron jobs here i
1042:46 - can schedule any tasks that i need to
1042:48 - run at a specific time on a recurring
1042:51 - basis i can edit or add any firewall
1042:54 - rules if i need to and as you can see
1042:56 - the default firewall rule is open to the
1042:59 - world now you probably noticed memcache
1043:02 - as being one of the options here in the
1043:04 - menu but this is a legacy service that
1043:07 - will soon be deprecated
1043:09 - memcache is a distributed in-memory data
1043:11 - store that is bundled into the python to
1043:14 - runtime acting as a cache for specific
1043:18 - tasks and google recommends moving to
1043:21 - memory store for redis if you're
1043:23 - planning on applying caching for your
1043:25 - app engine application and so i'm not
1043:27 - sure how much longer this will be here
1043:30 - and lastly under settings here is where
1043:32 - you can change your settings for your
1043:34 - application i can add any custom domains
1043:37 - any ssl certificates as well as setting
1043:40 - up email for any applications that want
1043:42 - to send email out to your users okay and
1043:45 - now that we've done that walkthrough i
1043:47 - want to go ahead and deploy my second
1043:49 - version of the application and so i'm
1043:51 - going to go ahead back down to cloud
1043:53 - shell i'm going to quickly clear my
1043:55 - screen and i want to move into the site
1043:57 - v2 directory so i'm going to hit cd dot
1044:00 - dot which will bring you back one
1044:02 - directory you do an ls and i'm going to
1044:05 - change directories into site v2 and do
1044:08 - an ls just to verify and yes you will
1044:10 - see serverless bow ties too i'm going to
1044:12 - quickly clear my screen and i'm going to
1044:15 - run the same command as before which is
1044:17 - gcloud app deploy with the version flag
1044:20 - dash dash version and instead of one i'm
1044:23 - going to launch version 2. so i'm going
1044:25 - to hit enter i'm going to be prompted if
1044:27 - i want to continue yes i do and as you
1044:29 - can see the files have been uploaded to
1044:31 - cloud storage for version 2 of the
1044:34 - website application and app engine is
1044:36 - going to take a few minutes to create
1044:39 - the service along with the version so
1044:41 - i'm going to let it cook here for a
1044:42 - couple minutes and i'll be back before
1044:44 - you can say cat in the hat okay so
1044:47 - version 2 has been deployed and so if i
1044:49 - go up here to the console and i click on
1044:52 - refresh you should see version 2 of your
1044:54 - service and as you can see 100 of the
1044:57 - traffic has been allocated to version 2
1045:00 - automatically and this is the default
1045:02 - behavior for whenever you launch a new
1045:05 - version of your service the only way to
1045:07 - avoid this is to deploy your new version
1045:10 - with the no promote flag and so if i go
1045:13 - back to services here on the left and i
1045:15 - click on the default service
1045:17 - you should see success for version two
1045:20 - and so i know that my website
1045:21 - application for version 2 has been
1045:24 - deployed successfully so i'm going to
1045:25 - close down this tab again and i'm going
1045:27 - to go back to versions and so what i
1045:29 - want to do now is i want to simulate an
1045:32 - a b test or blue green deployment by
1045:35 - migrating my traffic back to the old
1045:37 - version in this case being version one
1045:40 - so in production let's say that you
1045:42 - would release a new version and the
1045:45 - version doesn't go according to plan you
1045:47 - can always go back to the previous
1045:49 - version and app engine allows you to do
1045:52 - that very easily and so i'm going to
1045:54 - click on version 1 and i'm going to go
1045:56 - up to the top menu and click on migrate
1045:58 - traffic you'll be prompted if you want
1046:00 - to migrate traffic yes i do so i'm going
1046:03 - to click on migrate and it should take a
1046:05 - minute here and traffic should migrate
1046:07 - over to version one and success traffic
1046:10 - has been migrated and so we want to
1046:12 - verify that this has happened i'm gonna
1046:14 - go back to services i'm gonna click on
1046:16 - the default service and yes the traffic
1046:19 - has been allocated to version one okay
1046:21 - so i'm going to shut down this tab i'm
1046:24 - going to go back to versions and so now
1046:26 - what i want to do is i want to simulate
1046:28 - splitting the traffic between the two
1046:30 - versions and so in order for you to do
1046:32 - this you can go up to the top menu
1046:35 - click on split traffic and you'll be
1046:37 - prompted with a new menu here and here i
1046:40 - can choose from different versions and
1046:42 - because i only have two versions i'm
1046:44 - going to add version 2 and in order to
1046:47 - allocate the traffic between the two i
1046:49 - can either use this slider
1046:52 - and as you can see the allocation
1046:53 - percentage will change or i can simply
1046:56 - just type it in and so i'm going to
1046:58 - leave this at 50 percent so fifty
1047:00 - percent of version one fifty percent of
1047:02 - version two i'm going to split traffic
1047:04 - randomly i'm gonna move this down just a
1047:07 - little bit and so that's exactly how you
1047:09 - wanna allocate your traffic and so once
1047:11 - you've completed that you can simply
1047:12 - click on save it's going to take a
1047:14 - moment to update the settings and it's
1047:16 - been successful so if i head back on
1047:18 - over to the previous page you can see
1047:20 - here that traffic has been allocated to
1047:22 - both versions and so now in order to
1047:25 - verify this what you're going to do is
1047:27 - go over to services and click on the
1047:29 - default hot link and you'll see version
1047:32 - one but if i continuously refresh my
1047:34 - screen i can see that here i have
1047:36 - version two so because it's random i
1047:39 - have a 50 chance of getting version 1
1047:42 - and a 50 chance of getting version 2.
1047:44 - and so this is a simulation of splitting
1047:47 - traffic to different versions and
1047:49 - usually with a b testing only a small
1047:52 - percentage of the traffic is routed to
1047:54 - the new version until verification can
1047:56 - be made that the new version deployed
1047:59 - has indeed been successful and this can
1048:02 - be done by receiving feedback from the
1048:04 - users and so now i wanted to take a
1048:06 - quick moment to congratulate you on
1048:08 - making it through this demo and hope
1048:10 - that it has been extremely useful in
1048:13 - excelling your knowledge in deploying
1048:15 - and managing applications on app engine
1048:18 - so just as a recap you've cloned the
1048:20 - repo to cloud shell you then deployed
1048:22 - version one of your application into app
1048:25 - engine you verified its launch and then
1048:27 - you deployed version two of the
1048:29 - application and verified its launch as
1048:31 - well you then migrated traffic from
1048:34 - version two over to version one and then
1048:36 - you went ahead and split traffic between
1048:39 - both versions and allotted 50 of the
1048:42 - traffic allocation to each version and
1048:45 - so now before you go i want to make sure
1048:47 - that we clean up any resources that
1048:49 - we've deployed so that we don't incur
1048:51 - any unnecessary costs and so the way to
1048:54 - do this is very simple so first step you
1048:57 - want to go over to the left hand menu
1048:59 - and click on settings and simply click
1049:02 - on disable application you're going to
1049:03 - be prompted to type in the app's id for
1049:06 - me it's bowtie inc so i'm going to type
1049:08 - that in and i'm going to click on
1049:09 - disable now unfortunately with app
1049:11 - engine you can't actually delete the
1049:13 - application it can only be disabled and
1049:16 - so now here i'm going to hit the hot
1049:18 - link to go over to the cloud storage
1049:20 - bucket and as you can see here i have no
1049:22 - files but i'm going to move back to my
1049:24 - buckets
1049:25 - and i'm going to move into the staging
1049:27 - bucket which is appended with your
1049:29 - project id.appspot.com
1049:31 - and as you can see here there's a whole
1049:33 - bunch of different files as well if i
1049:35 - drill down into the directory marked as
1049:37 - ae for app engine i can see here that i
1049:40 - have some more directories along with
1049:42 - the manifest and so now if you want to
1049:44 - keep your application in order to run it
1049:47 - later you don't need to delete this
1049:48 - bucket but because i don't need it i'm
1049:51 - going to go ahead and delete the bucket
1049:53 - hit delete paste in my bucket name hit
1049:56 - delete as well under us.artifacts
1049:59 - you will find a directory called
1050:00 - containers and as explained in the last
1050:03 - lesson code build builds a container for
1050:05 - your application before deploying it to
1050:08 - app engine so i'm going to drill down
1050:10 - into images so here's all the container
1050:12 - digests and i don't need any of these so
1050:15 - i'm gonna go ahead and delete this
1050:17 - bucket as well and so this is the last
1050:19 - step in order to delete all the
1050:21 - directories and files that we use to
1050:23 - deploy our application in an app engine
1050:25 - okay and so i'm gonna head back on over
1050:28 - to app engine and so now that cleanup
1050:31 - has been taken care of that's pretty
1050:33 - much all i wanted to cover in this demo
1050:35 - for deploying and managing applications
1050:38 - on app engine so you can now mark this
1050:40 - as complete and i'll see you in the next
1050:42 - one and again congrats on a job well
1050:45 - done
1050:46 - [Music]
1050:50 - welcome back in this lesson i will be
1050:52 - diving into another serverless product
1050:55 - from google cloud by the name of cloud
1050:57 - functions an extremely useful and
1050:59 - advanced service that can be used with
1051:02 - almost every service on the platform now
1051:04 - there's quite a bit to cover here so
1051:06 - with that being said let's dive in now
1051:09 - cloud functions as i said before are a
1051:11 - serverless execution environment and
1051:14 - what i mean by this is like app engine
1051:16 - there is no need to provision any
1051:18 - servers or updating vms as the
1051:21 - infrastructure is all handled by google
1051:24 - but unlike app engine you will never see
1051:26 - the servers so the provisioning of
1051:28 - resources happens when the code is
1051:30 - executed now cloud functions are a
1051:33 - function as a service offering and this
1051:35 - is where you upload code that is
1051:38 - purposefully written in a supported
1051:40 - programming language and when your code
1051:42 - is triggered it is executed in a fully
1051:45 - managed environment and your billed for
1051:48 - when that code is executed cloud
1051:50 - functions run in a runtime environment
1051:53 - and support many different runtimes like
1051:55 - python java node.js go and net core
1052:01 - cloud functions are event driven so when
1052:03 - something happens in your environment
1052:05 - you can choose whether or not you'd like
1052:07 - to respond to this event if you do then
1052:10 - your code can be executed in response to
1052:12 - the event these triggers can be one of a
1052:15 - few different types such as http
1052:18 - pub sub cloud storage and now firestore
1052:22 - and firebase which are in beta and have
1052:24 - yet to be seen in the exam cloud
1052:27 - functions are priced according to how
1052:29 - long your function runs and how many
1052:31 - resources you provision for your
1052:33 - function if your function makes an
1052:35 - outbound network request there are also
1052:38 - additional data transfer fees cloud
1052:40 - functions also include a perpetual free
1052:43 - tier which allows you 2 million
1052:45 - invocations or executions of your
1052:48 - function now cloud functions themselves
1052:50 - are very simple but have a few steps to
1052:53 - execute before actually running so i
1052:56 - wanted to give you a walkthrough on
1052:58 - exactly how cloud functions work now
1053:01 - after selecting the name and region you
1053:03 - want your function to live in you would
1053:05 - then select the trigger you wish to use
1053:08 - and you can choose from the many i
1053:10 - listed earlier being http cloud storage
1053:13 - pub sub cloud firestore and firebase a
1053:16 - trigger is a declaration that you are
1053:19 - interested in a certain event or set of
1053:21 - events binding a function to a trigger
1053:25 - allows you to capture and act on these
1053:27 - events authentication configuration is
1053:29 - the next step and can be selected with
1053:32 - public access
1053:34 - or configured through iam now there are
1053:37 - some optional settings that can be
1053:39 - configured where you would provide the
1053:41 - amount of memory the function will need
1053:43 - to run
1053:44 - networking preferences and even
1053:46 - selection for a service account now once
1053:49 - all the settings have been solidified
1053:51 - your written code can then be put into
1053:53 - the function now the functions code
1053:55 - supports a variety of languages as
1053:58 - stated before like python java node.js
1054:01 - or go now when writing your code there
1054:04 - are two distinct types of cloud
1054:06 - functions that you could use http
1054:09 - functions and background functions with
1054:12 - http functions you invoke them from
1054:14 - standard http requests these http
1054:18 - requests wait for the response and
1054:20 - support handling of common http request
1054:24 - methods like get put
1054:26 - post delete and options when you use
1054:29 - cloud functions a tls certificate is
1054:32 - automatically provisioned for you so all
1054:35 - http functions can be invoked via a
1054:38 - secure connection now when it comes to
1054:41 - background functions
1054:42 - these are used to handle events from
1054:44 - your gcp infrastructure such as messages
1054:48 - on a pub sub topic or changes in a cloud
1054:51 - storage bucket now once you have put all
1054:53 - this together you are ready to deploy
1054:55 - your code now there are two things that
1054:57 - will happen when deploying your code the
1055:00 - first one is the binding of your trigger
1055:02 - to your function once you bind a trigger
1055:05 - you cannot bind another one to the same
1055:07 - function
1055:08 - only one trigger can be bound to a
1055:10 - function at a time now the second thing
1055:12 - that will happen when you deploy your
1055:14 - function's source code to cloud
1055:16 - functions is that source code is stored
1055:18 - in a cloud storage bucket as a zip file
1055:21 - cloud build then automatically builds
1055:24 - your code into a container image that
1055:27 - pushes that image to container registry
1055:30 - cloud functions accesses this image when
1055:32 - it needs to run the container to execute
1055:35 - your function the process of building
1055:37 - the image is entirely automatic and
1055:40 - requires no manual intervention and so
1055:42 - at this point of the process the
1055:44 - building of your function is now
1055:46 - complete now that the function has been
1055:48 - created we now wait for an event to
1055:51 - happen and events are things that happen
1055:54 - within your cloud environment that you
1055:56 - might want to take action on these might
1055:58 - be changes to data in cloud sql files
1056:01 - added to cloud storage or a new vm being
1056:04 - created currently cloud functions
1056:06 - supports
1056:08 - events from the same services used for
1056:10 - triggers that i have just mentioned
1056:13 - including other google services like
1056:15 - bigquery cloud sql and cloud spanner now
1056:19 - when an event triggers the execution of
1056:21 - your cloud function
1056:23 - data associated with the event is passed
1056:26 - via the functions parameters the type of
1056:28 - event determines the parameters that are
1056:31 - passed to your function cloud functions
1056:33 - handles incoming requests by assigning
1056:36 - them to instances of your function now
1056:38 - depending on the volume of requests
1056:40 - as well as the number of existing
1056:42 - function instances cloud functions may
1056:44 - assign a request to an existing instance
1056:48 - or create a new one so the cloud
1056:50 - function will grab the image from cloud
1056:52 - registry and hand off the image along
1056:55 - with the event data to the instance for
1056:57 - processing now each instance of a
1056:59 - function handles only one concurrent
1057:02 - request at a time this means that while
1057:04 - your code is processing one request
1057:06 - there is no possibility of a second
1057:09 - request being routed to the same
1057:11 - instance thus the original request can
1057:14 - use the full amount of resources that
1057:17 - you requested and this is the memory
1057:19 - that you assign to your cloud function
1057:21 - when deploying it now to allow google to
1057:24 - automatically manage and scale the
1057:26 - functions they must be stateless
1057:28 - functions are not meant to be persistent
1057:31 - nor is the data that is passed on to the
1057:33 - function and so once the function has
1057:36 - run and all data has been processed by
1057:38 - the server it is then passed on to
1057:40 - either a vpc or to the internet now by
1057:44 - default functions have public internet
1057:46 - access unless configured otherwise
1057:49 - functions can also be private and used
1057:52 - within your vpc but must be configured
1057:55 - before deployment now there are so many
1057:57 - use cases for cloud functions and there
1058:00 - are many that have already been created
1058:02 - by google for you to try out
1058:04 - and can be located in the documentation
1058:07 - that i've supplied in the lesson text
1058:09 - below now the exam doesn't go into too
1058:12 - much depth on cloud functions but i did
1058:14 - want to give you some exposure to this
1058:16 - fantastic serverless product from google
1058:19 - as it is so commonly used in many
1058:22 - production environments in a simple and
1058:25 - easy way to take in data process it and
1058:28 - return a result from any event you are
1058:30 - given and i have no doubt that once you
1058:33 - get the hang of deploying them that you
1058:35 - will be a huge fan of them as well and
1058:37 - so that's pretty much all i had to cover
1058:40 - when it comes to cloud functions so you
1058:42 - can now mark this lesson as complete and
1058:44 - whenever you're ready join me in the
1058:46 - next one where we go hands-on in the
1058:49 - console creating and deploying your very
1058:51 - first function
1058:57 - welcome back and in this demo we will be
1059:00 - diving into creating and deploying our
1059:02 - very first cloud function we're going to
1059:04 - take a tour of all the options in the
1059:07 - console but we're going to do most of
1059:09 - the work in cloud shell to get a good
1059:11 - feel for doing it in the command line so
1059:13 - with that being said let's dive in and
1059:16 - so i'm logged in here as tony bowties
1059:19 - gmail.com
1059:21 - and i'm in the project of bowtie inc and
1059:23 - so the first thing i want to do is head
1059:26 - on over to cloud functions in the
1059:27 - console so i'm going to go up to the top
1059:29 - left to the navigation menu and i'm
1059:32 - going to scroll down to cloud functions
1059:34 - and as you can see here cloud functions
1059:36 - is getting ready and this is because
1059:38 - we've never used it before and the api
1059:40 - is being enabled okay and the api has
1059:43 - been enabled and we can go ahead and
1059:45 - start creating our function so you can
1059:47 - go ahead and click create function and
1059:49 - you will be prompted with some fields to
1059:51 - fill out for the configuration of your
1059:54 - cloud function and so under basics for
1059:56 - function name i'm going to name this
1059:59 - hello underscore world for region i'm
1060:02 - going to select us east one and under
1060:04 - trigger for trigger type we're gonna
1060:06 - keep this as http although if i click on
1060:09 - the drop down menu you can see that i
1060:11 - will have options for cloud pub sub
1060:14 - cloud storage and the ones that i
1060:16 - mentioned before that are in beta so
1060:18 - we're going to keep things as http and
1060:21 - here under url is the url for the actual
1060:24 - cloud function under authentication i
1060:27 - have the option of choosing require
1060:29 - authentication or allow unauthenticated
1060:32 - invocations and as you can see this is
1060:35 - clearly marked saying that check this if
1060:37 - you are creating a public api or website
1060:40 - which we are and so this is the
1060:42 - authentication method that you want to
1060:44 - select and so now that we have all the
1060:46 - fields filled out for the basic
1060:48 - configuration i'm going to go ahead and
1060:50 - click on save and just to give you a
1060:52 - quick run through of what else is
1060:54 - available i'm going to click on the drop
1060:56 - down here and this will give me access
1060:58 - to variables networking and advanced
1061:01 - settings the first field here memory
1061:03 - allocated i can actually add more memory
1061:07 - depending what i am doing with my cloud
1061:09 - function but i'm going to keep it as the
1061:10 - default if you have a cloud function
1061:12 - that runs a little bit longer and you
1061:15 - need more time to run the cloud function
1061:17 - you can add additional time for the
1061:19 - timeout and as well i have the option of
1061:22 - choosing a different service account for
1061:24 - this cloud function and so moving on
1061:26 - under environment variables you will see
1061:29 - the options to add build environment
1061:31 - variables along with runtime environment
1061:34 - variables and the last option being
1061:36 - connections here you can change the
1061:38 - different networking settings for
1061:40 - ingress and egress traffic under ingress
1061:43 - settings i can allow all traffic which
1061:45 - is the default i can allow internal
1061:47 - traffic only as well i can allow
1061:50 - internal traffic and traffic from cloud
1061:52 - low balancing now as well when it comes
1061:55 - to the egress settings as i said before
1061:58 - by default your cloud function is able
1062:00 - to send requests to the internet but not
1062:03 - to resources in your vpc network and so
1062:05 - this is where you would create a vpc
1062:07 - connector to send requests from your
1062:10 - cloud function to resources in your vpc
1062:13 - so if i click on create a connector
1062:15 - it'll open up a new tab and bring me to
1062:18 - vpc network to add serverless vpc access
1062:21 - and so i don't want to do that right now
1062:23 - so i'm going to close down this tab and
1062:25 - i'm going to go ahead and leave
1062:26 - everything else as is and click on next
1062:30 - and so now that the configuration is
1062:32 - done i can dive right into the code and
1062:34 - so google cloud gives you a inline
1062:37 - editor right here along with the
1062:38 - different runtime environments so if i
1062:40 - click on the drop down menu you can see
1062:43 - i have the options
1062:44 - of net core go java node.js and python
1062:49 - 3.7 and 3.8 and so for this demo i'm
1062:53 - going to keep it as node.js 10. the
1062:55 - entry point will be hello world and i'm
1062:57 - going to keep the code exactly as is and
1063:00 - this is a default cloud function that is
1063:02 - packaged with any runtime whenever you
1063:05 - create a function from the console and
1063:07 - so if i had any different code i can
1063:09 - change it here but i'm not going to do
1063:11 - that i'm going to leave everything else
1063:13 - as is and click on deploy and it'll take
1063:15 - a couple minutes here to create my cloud
1063:18 - function and so i'm going to pause the
1063:19 - video here for just a quick sec and i'll
1063:22 - be back in a flash okay and my cloud
1063:24 - function has been deployed and i got a
1063:26 - green check mark which means that i'm
1063:28 - all good and so i want to dive right
1063:30 - into it for just a second so i can get
1063:32 - some more details here i have the
1063:34 - metrics for my cloud function the
1063:36 - invocations per second execution time
1063:39 - memory utilization and active instances
1063:42 - i have my versions up here at the top
1063:44 - but since i only have one version only
1063:47 - one version shows up if i click on
1063:49 - details it'll show me the general
1063:51 - information along with the networking
1063:53 - settings the source will show me the
1063:56 - code for this cloud function as well as
1063:58 - the variables the trigger permissions
1064:01 - logs and testing and here i can write in
1064:03 - some code and test the function and so
1064:05 - in order for me to invoke this function
1064:08 - i can simply go to trigger and it'll
1064:10 - show me the url but a quick way to do
1064:12 - this through the command line is to
1064:14 - simply open up cloud shell and make this
1064:16 - a little bigger for better viewing and
1064:18 - i'm going to paste in the command gcloud
1064:20 - functions describe along with the
1064:23 - function name which is hello underscore
1064:25 - world along with the region flag dash
1064:27 - dash region with the region that my
1064:29 - cloud function has been deployed in
1064:31 - which is us east one and i'm going to
1064:33 - hit enter
1064:34 - it's going to ask me to authorize my api
1064:37 - call yes i want to authorize it and this
1064:39 - command should output some information
1064:41 - on your screen and so what we're looking
1064:43 - for here is the http trigger which you
1064:46 - will find here under https trigger and
1064:49 - it is the same as what you see here in
1064:51 - the console and so just know if you want
1064:54 - to grab the http url trigger you can
1064:57 - also do it from the command line and so
1064:59 - i'm going to now trigger it by going to
1065:00 - this url and you should see in the top
1065:03 - left hand side of your screen hello
1065:05 - world not as exciting as spinning bow
1065:07 - ties but this example gives you an idea
1065:10 - of what an http function can do and so
1065:12 - i'm going to close down this tab and so
1065:14 - now what i want to do is i want to
1065:16 - deploy another function but i want to do
1065:18 - it now through the command line and so
1065:20 - i'm going to now quickly clear my screen
1065:22 - and so since i've already uploaded the
1065:24 - code to the repo i'm going to simply
1065:26 - clone that repo and run it from here so
1065:29 - i'm going to simply do a cd tilde to
1065:32 - make sure i'm in my home directory for
1065:34 - those of you who haven't deleted the
1065:35 - directory you can simply cd into it so
1065:38 - i'm going to run cd google cloud
1065:41 - associate cloud engineer hit enter and
1065:43 - i'm going to run a get pull command
1065:45 - and it pull down all the files that i
1065:47 - needed i'm going to quickly clear my
1065:49 - screen and so i'm going to change
1065:51 - directories into the directory that has
1065:53 - my code and so you're going to find it
1065:55 - under 11 serverless services under zero
1065:58 - to you called hit enter and again i will
1066:01 - have a link in the lesson text for the
1066:04 - full instructions on this demo and it
1066:06 - will list the directory where you can
1066:08 - find this code okay so moving forward
1066:11 - i'm going to run ls and you should see
1066:13 - three files here
1066:15 - main.py
1066:17 - requirements.txt and the text file with
1066:19 - the instructions and so now that i have
1066:21 - everything in place in order to deploy
1066:23 - my code i'm going to paste in the
1066:25 - command to actually deploy my function
1066:27 - which is gcloud functions deploy the
1066:30 - name of the function which is you
1066:32 - underscore called the flag for the
1066:34 - runtime dash dash runtime and the
1066:36 - runtime is going to be python 3.8 the
1066:39 - flag for the trigger which is going to
1066:40 - be http and because i'm a nice guy and i
1066:43 - want everyone to have access to this i'm
1066:45 - going to tag it with the flag dash dash
1066:48 - allow unauthenticated so i'm going to
1066:50 - hit enter okay and this function should
1066:52 - take a couple minutes to deploy so i'm
1066:54 - going to sit here and let it cook and
1066:55 - i'll be back before you can say cat in
1066:57 - the hat okay and our function has been
1066:59 - deployed i'm going to do a quick refresh
1067:01 - here in the console and it deployed
1067:03 - successfully as you can see the green
1067:06 - check mark is here okay and so now that
1067:08 - it's been deployed we want to trigger
1067:10 - our function and so because i just
1067:11 - deployed this function the url trigger
1067:14 - is conveniently located here in my
1067:16 - screen so you can go ahead and click on
1067:18 - it and hello lover of bow ties you
1067:21 - called now although this may be similar
1067:23 - to the hello world demo but i did add a
1067:26 - small feature that might spice things up
1067:28 - and so if you go up to the url and you
1067:30 - type in question mark name equals and
1067:34 - your name and since my name is anthony
1067:36 - i'm going to type in anthony
1067:38 - hit enter and hello anthony you called
1067:42 - and so this is a perfect example of the
1067:44 - many different ways you can use
1067:46 - functions and although i've only
1067:47 - highlighted some very simple
1067:49 - demonstrations there are many different
1067:51 - ways that you can use functions such as
1067:54 - running pipelines running batch jobs and
1067:57 - even event driven security now although
1067:59 - the exam doesn't go into too much depth
1068:02 - on cloud functions it's always good to
1068:04 - know its use cases and where its
1068:06 - strengths lie for when you do decide to
1068:09 - use it in your role as a cloud engineer
1068:12 - now before you go be sure to delete all
1068:14 - the resources you've created by deleting
1068:16 - the functions and the storage buckets
1068:19 - that house the code for the cloud
1068:20 - functions and i will walk you through
1068:23 - the steps right now okay so first i'm
1068:25 - going to close down this tab and next
1068:27 - you're going to select all the functions
1068:29 - and you're going to simply click on
1068:30 - delete you're going to get a prompt to
1068:32 - delete the functions you're going to
1068:33 - click on delete and it's going to take a
1068:35 - minute or two and the functions are
1068:37 - deleted i'm going to close down my cloud
1068:39 - shell and i'm going to head over to
1068:41 - cloud storage
1068:42 - and as you can see here both these
1068:44 - buckets that start with gcf standing for
1068:46 - google cloud functions can be safely
1068:49 - deleted as inside them are the files
1068:52 - that were used for the cloud function so
1068:54 - i'm going to go back out i'm going to
1068:55 - select both of these and i'm going to
1068:57 - click on delete you get a prompt to
1068:59 - delete two buckets you can simply type
1069:02 - in delete and click on delete and the
1069:04 - buckets have now been deleted and you've
1069:06 - pretty much finished your cleanup and so
1069:08 - just as a recap you created a default
1069:11 - cloud function that was available from
1069:13 - the console and then verified it by
1069:16 - triggering the http url you then
1069:19 - deployed another function from the
1069:20 - command line by pulling the code from
1069:23 - the repo and using it for deployment and
1069:26 - then you verified that function by
1069:28 - triggering it using the http url as well
1069:32 - and then you modify the url for a
1069:35 - different output great job on another
1069:37 - successful demo so you can now mark this
1069:39 - as complete and let's move on to the
1069:42 - next one
1069:42 - [Music]
1069:46 - welcome back in this lesson we're going
1069:49 - to dive into cloud storage the go to
1069:52 - storage service from google cloud if
1069:54 - you're an engineer working in google
1069:56 - cloud you've probably used this many
1069:58 - times as a storage solution and if you
1070:01 - haven't this is definitely a service
1070:03 - that you will need to know for both the
1070:05 - exam and your day-to-day role as a cloud
1070:08 - engineer now there's quite a bit to
1070:10 - cover here so with that being said let's
1070:12 - dive in now cloud storage is a
1070:15 - consistent scalable large capacity
1070:18 - highly durable object storage and this
1070:21 - is unlimited storage for objects with no
1070:24 - minimum object size but please remember
1070:27 - that this is object storage and is not
1070:29 - designed to store an operating system on
1070:31 - but to store whole objects like pictures
1070:34 - or videos cloud storage has worldwide
1070:36 - accessibility and worldwide storage
1070:39 - locations so anywhere that there is a
1070:41 - region or zone cloud storage is
1070:44 - available from there and can be accessed
1070:47 - at any time through an internet
1070:49 - connection cloud storage is great for
1070:51 - storing data from data analytics jobs
1070:54 - text files with code pictures of the
1070:57 - latest fashion from paris and videos of
1070:59 - your favorite house dj at the shelter
1071:02 - cloud storage excels for content
1071:04 - delivery big data sets and backups and
1071:07 - are all stored as objects in buckets and
1071:10 - this is the heart of cloud storage that
1071:13 - i will be diving into so starting with
1071:15 - buckets these are the basic containers
1071:18 - or construct that holds your data
1071:21 - everything that you store in cloud
1071:23 - storage must be contained in a bucket
1071:26 - you can use buckets to organize your
1071:28 - data and control access to your data but
1071:31 - unlike directories and folders you
1071:34 - cannot nest buckets and i'll get into
1071:36 - that in just a minute now when you
1071:38 - create a bucket you must specify a
1071:40 - globally unique name as every bucket
1071:44 - resides in a single cloud storage
1071:46 - namespace as well as a name you must
1071:48 - specify a geographic location where the
1071:52 - bucket and its contents are stored and
1071:54 - you have three available geography
1071:56 - choices to choose from from region dual
1071:59 - region and multi-region and so just as a
1072:02 - note choosing dual region and
1072:04 - multi-region is considered geo-redundant
1072:07 - for dual region geo-redundancy is
1072:10 - achieved using a specific pair of
1072:12 - regions for multi-region geo-redundancy
1072:15 - is achieved using a continent that
1072:18 - contains two or more geographic places
1072:21 - basically the more regions your data is
1072:23 - available in the greater your
1072:25 - availability for that data after you've
1072:28 - chosen a geographic location a default
1072:31 - storage class must be chosen and this
1072:33 - applies to objects added to the bucket
1072:36 - that don't have a storage class
1072:38 - explicitly specified and i'll be diving
1072:40 - into storage classes in just a bit and
1072:42 - so after you create a bucket you can
1072:45 - still change its default storage class
1072:47 - to any class supported in the buckets
1072:50 - location with some stipulations
1072:52 - you can only change the bucket name
1072:55 - and location by deleting and recreating
1072:57 - the bucket as well once dual region is
1073:00 - selected it cannot be changed to
1073:02 - multi-region and when selecting
1073:04 - multi-region you will not be able to
1073:06 - change the bucket to be dual region and
1073:09 - lastly you will need to choose what
1073:11 - level of access you want others to have
1073:13 - on your bucket whether you want to apply
1073:15 - permissions using uniform or fine
1073:18 - grained access uniform bucket level
1073:21 - access allows you to use iam alone to
1073:24 - manage permissions iam applies
1073:26 - permissions to all the objects contained
1073:29 - inside the bucket or groups of objects
1073:32 - with common name prefixes the find green
1073:35 - option enables you to use iam and access
1073:38 - control lists or acls
1073:40 - together to manage permissions acls are
1073:44 - a legacy access control system for cloud
1073:46 - storage designed for interoperability
1073:49 - with amazon s3 for those of you who use
1073:52 - aws you can specify access and apply
1073:55 - permissions at both the bucket level and
1073:58 - per individual object and i will also be
1074:01 - diving more into depth with access
1074:03 - control
1074:04 - in just a bit and just as a note labels
1074:07 - are an optional item for bucket creation
1074:10 - like every other resource creation
1074:12 - process in gcp now that we've covered
1074:14 - buckets i wanted to cover what is stored
1074:17 - in those buckets which is objects and
1074:20 - objects are the individual pieces of
1074:22 - data or data chunks that you store in a
1074:25 - cloud storage bucket and there is no
1074:27 - limit on the number of objects that you
1074:29 - can create in a bucket so you can think
1074:31 - of objects kind of like files objects
1074:34 - have two components object data and
1074:36 - object metadata
1074:38 - object data is typically a file that you
1074:41 - want to store in cloud storage and in
1074:43 - this case it is the picture of the plaid
1074:46 - bow tie and object metadata is a
1074:48 - collection of name value pairs that
1074:51 - describe the various properties of that
1074:53 - object an object's name is treated as a
1074:56 - piece of object metadata in cloud
1074:58 - storage and must be unique within the
1075:00 - bucket cloud storage uses a flat
1075:03 - namespace to store objects which means
1075:05 - that cloud storage isn't a file system
1075:08 - hierarchy but sees all objects in a
1075:10 - given bucket as independent with no
1075:13 - relationship towards each other for
1075:15 - convenience
1075:16 - tools such as the console and gsutil
1075:20 - work with objects that use the slash
1075:23 - character as if they were stored in a
1075:25 - virtual hierarchy for example you can
1075:28 - name one object slash bow ties slash
1075:31 - spring 2021 slash plaid bowtie.jpg when
1075:35 - using the cloud console you can then
1075:37 - navigate to these objects as if they
1075:39 - were in a hierarchical directory
1075:41 - structure under the folders bow ties and
1075:44 - spring 2021 now i mentioned before that
1075:47 - the part of the bucket creation is the
1075:49 - selection of a storage class the storage
1075:51 - class you set for an object affects the
1075:54 - object's availability and pricing model
1075:56 - so when you create a bucket you can
1075:58 - specify a default storage class for the
1076:01 - bucket when you add objects to the
1076:02 - bucket they inherit this storage class
1076:05 - unless explicitly set otherwise now i
1076:08 - wanted to touch on these four storage
1076:09 - classes now to give you a better
1076:11 - understanding of the differences between
1076:13 - them the first one is standard storage
1076:16 - and is considered best for hot data or
1076:19 - frequently accessed data and is best for
1076:22 - short-term use as it does not have any
1076:25 - specified storage duration and this is
1076:27 - excellent for use in analytical
1076:29 - workloads and transcoding and the price
1076:32 - for this storage class comes in at two
1076:34 - cents per gigabyte per month next up is
1076:37 - near line storage and this is considered
1076:39 - hot data as well and is a low-cost
1076:42 - storage class for storing in frequently
1076:45 - accessed data nearline storage has a
1076:48 - slightly lower availability a 30-day
1076:50 - minimum storage duration and comes with
1076:53 - the cost for data access nearline
1076:55 - storage is ideal if you're looking to
1076:57 - continuously add files but only plan to
1077:00 - access them once a month and is perfect
1077:03 - for data backup and data archiving the
1077:06 - price for this storage class comes in at
1077:08 - a penny per gigabyte per month now cold
1077:11 - line storage is considered cold data as
1077:14 - it enters into more of the longer term
1077:16 - storage classes and is a very low cost
1077:20 - storage class for storing and frequently
1077:22 - accessed data it comes with slightly
1077:24 - lower availability than nearline storage
1077:27 - a 90-day minimum storage duration and
1077:30 - comes with the cost for data access that
1077:32 - is higher than the retrieval cost for
1077:35 - nearline storage coldline storage is
1077:37 - ideal for data you plan to read or
1077:40 - modify at most once a quarter and is
1077:43 - perfect for data backup and data
1077:46 - archiving the price for this storage
1077:47 - class comes in at less than half of a
1077:50 - penny per gigabyte per month and finally
1077:53 - archive storage is the lowest cost
1077:56 - highly durable storage service for data
1077:58 - archiving online backup and disaster
1078:01 - recovery and even coming in at a lowest
1078:04 - cost the data access is still available
1078:08 - within milliseconds archive storage
1078:10 - comes in at a higher cost for data
1078:12 - retrieval as well as a
1078:15 - day minimum storage duration and is the
1078:18 - best choice for data that you plan to
1078:20 - access less than once a year archive
1078:23 - storage also comes with the highest
1078:25 - price for data retrieval and it is ideal
1078:28 - for archive data storage that's used for
1078:31 - regulatory purposes or disaster recovery
1078:34 - data in the event that there is an
1078:36 - oopsies in your environment the price of
1078:38 - the storage class comes in at a
1078:40 - ridiculously low price per gigabyte per
1078:43 - month at a fraction of a penny per
1078:46 - gigabyte per month now when it comes to
1078:48 - choosing your geographic location this
1078:51 - will determine the availability of your
1078:53 - data here as you can see the highest
1078:56 - availability is the standard
1078:58 - multi-region whereas archive has the
1079:01 - lowest availability when stored in a
1079:03 - regional setting now when it comes to
1079:05 - the durability of your data meaning the
1079:08 - measurement of how healthy and resilient
1079:10 - your data is from data loss or data
1079:12 - corruption google cloud boasts 11 9's
1079:16 - durability annually on all data stored
1079:20 - in any storage class on cloud storage so
1079:23 - know that your data is stored safely and
1079:26 - will be there holding the same integrity
1079:28 - from the day you stored it now when it
1079:30 - comes to granting permissions to your
1079:32 - cloud storage buckets and the objects
1079:35 - within them there are four different
1079:37 - options to choose from the first is iam
1079:40 - permissions and these are the standard
1079:42 - permissions that control all your other
1079:44 - resources in google cloud and follow the
1079:47 - same top-down hierarchy that we
1079:49 - discussed earlier the next available
1079:51 - option are access control list or acls
1079:55 - and these define who has access to your
1079:57 - buckets and objects as well as what type
1080:00 - of access they have and these can work
1080:03 - in tandem with im permissions moving on
1080:06 - to sign urls these are time limited
1080:09 - reader write access urls that can be
1080:12 - created by you to give access to the
1080:15 - object in question for the duration that
1080:17 - you specify and lastly is sign policy
1080:20 - documents and these are documents to
1080:23 - specify what can be uploaded to a bucket
1080:25 - and i will be going into each one of
1080:27 - these in a bit of detail now cloud
1080:30 - storage offers two systems for granting
1080:32 - users permission to access your buckets
1080:35 - and objects iam and access control lists
1080:39 - these systems act in parallel in order
1080:42 - for a user to access a cloud storage
1080:44 - resource only one of the systems needs
1080:47 - to grant the user permission im is
1080:49 - always the recommended method when it
1080:51 - comes to giving access to buckets and
1080:54 - the objects within those buckets
1080:56 - granting roles at the bucket level does
1080:58 - not affect any existing roles that you
1081:00 - granted at the project level and vice
1081:03 - versa giving you two levels of
1081:05 - granularity to customize your
1081:07 - permissions so for instance you can give
1081:09 - a user permission to read objects in any
1081:12 - bucket but permissions to create objects
1081:15 - only in one specific bucket the roles
1081:17 - that are available through iam are the
1081:19 - primitive standard storage roles or the
1081:22 - legacy roles which are equivalent to
1081:24 - acls now acls are there if you need to
1081:28 - customize access and really get granular
1081:31 - with individual objects within a bucket
1081:33 - and are used to define who has access to
1081:36 - your buckets and objects as well as what
1081:38 - level of access they have each acl
1081:41 - consists of one or more entries and
1081:43 - gives a specific user or group the
1081:46 - ability to perform specific actions each
1081:49 - entry consists of two pieces of
1081:51 - information a permission which defines
1081:54 - what actions can be performed and a
1081:56 - scope which defines who can perform the
1081:58 - specified actions now acls should be
1082:01 - used with caution as iam roles and acls
1082:04 - overlap cloud storage will grant a
1082:07 - broader permission so if you allow
1082:09 - specific users access to an object in a
1082:11 - bucket and then an acl is applied to
1082:14 - that object to make it public then it
1082:16 - will be publicly accessible so please be
1082:19 - aware now a signed url is a url that
1082:22 - provides limited permission and time to
1082:24 - make a request sign urls contain
1082:27 - authentication information allowing
1082:30 - users without credentials to perform
1082:32 - specific actions on a resource when you
1082:35 - generate a signed url you specify a user
1082:38 - or service account which must have
1082:41 - sufficient permission to make the
1082:43 - request that the sign url will make
1082:45 - after you generate a signed url anyone
1082:48 - who possesses it can use the sign url to
1082:51 - perform specified actions such as
1082:54 - reading an object within a specified
1082:56 - period of time now if you want to
1082:58 - provide public access to a user who
1083:00 - doesn't have an account you can provide
1083:03 - a signed url to that user which gives
1083:05 - the user read write or delete access to
1083:08 - that resource for a limited time you
1083:11 - specify an expiration date when you
1083:13 - create the sign url so anyone who knows
1083:16 - the url can access the resource until
1083:19 - the expiration time for the url is
1083:22 - reached or the key used to sign the url
1083:25 - is rotated and the command to create the
1083:27 - sign url is shown here and as you can
1083:30 - see has been assigned for a limited time
1083:33 - of 10 minutes so as you've seen when it
1083:36 - comes to cloud storage there are so many
1083:38 - configuration options to choose from and
1083:41 - lots of different ways to store and give
1083:43 - access and this makes this resource from
1083:46 - google cloud such a flexible option and
1083:49 - full of great potential for many
1083:51 - different types of workloads this is
1083:53 - also a service that comes up a lot in
1083:56 - the exam as one of the many different
1083:58 - storage options to choose from and so
1084:01 - knowing the features storage classes
1084:03 - pricing and access options will
1084:06 - definitely give you a leg up when you
1084:08 - are presented with questions regarding
1084:10 - storage and so that's pretty much all i
1084:12 - wanted to cover when it comes to this
1084:14 - overview on cloud storage so you can now
1084:16 - mark this lesson as complete and let's
1084:19 - move on to the next one
1084:20 - [Music]
1084:24 - welcome back and in this lesson i will
1084:27 - be covering object versioning and life
1084:29 - cycle management a feature within cloud
1084:31 - storage that is used to manage and sort
1084:34 - through older files that need to be
1084:36 - deleted along with files that are not in
1084:39 - high need of regular access knowing the
1084:42 - capabilities of these two features
1084:44 - can really help organize accumulated
1084:47 - objects in storage buckets and cut down
1084:50 - on costs so without further ado let's
1084:53 - dive in now to understand a bit more
1084:55 - about objects i wanted to dive into
1084:58 - immutability and versioning now objects
1085:01 - are immutable which means that an
1085:03 - uploaded object cannot change throughout
1085:05 - its storage lifetime an object's storage
1085:08 - lifetime is the time between a
1085:11 - successful object creation or upload and
1085:14 - successful object deletion this means
1085:16 - that you cannot edit objects in place
1085:19 - instead objects are always replaced with
1085:22 - a new version so after the upload of the
1085:25 - new object completes the new version of
1085:27 - the object is served to readers this
1085:30 - replacement marks the end of one
1085:32 - object's life cycle and the beginning of
1085:34 - a new one now to support the retrieval
1085:37 - of objects that are deleted or replaced
1085:39 - cloud storage offers the object
1085:41 - versioning feature object versioning
1085:43 - retains a non-current object version
1085:46 - when the live object version gets
1085:48 - replaced or deleted enabling object
1085:51 - versioning increases storage costs which
1085:53 - can be partially mitigated by
1085:56 - configuring object lifecycle management
1085:58 - to delete older object versions but more
1086:01 - on that in just a bit cloud storage uses
1086:03 - two properties that together identify
1086:06 - the version of an object the generation
1086:09 - which identifies the version of the
1086:11 - object's data
1086:12 - and the meta generation which identifies
1086:15 - the version of the object's metadata
1086:17 - these properties are always present with
1086:20 - every version of the object even if
1086:22 - object versioning is not enabled these
1086:25 - properties can be used to enforce
1086:27 - ordering of updates so in order to
1086:29 - enable object versioning you would do
1086:32 - that by enabling it on a bucket once
1086:34 - enabled older versions remain in your
1086:36 - bucket when a replacement or deletion
1086:38 - occurs so by default when you replace an
1086:41 - object cloud storage deletes the old
1086:44 - version and adds a new version these
1086:46 - older versions retain the name of the
1086:48 - object but are uniquely identified by
1086:51 - their generation number when object
1086:53 - versioning has created an older version
1086:55 - of an object you can use the generation
1086:58 - number to refer to the older version
1087:01 - this allows you to restore a replaced
1087:03 - object in your bucket or permanently
1087:06 - delete older object versions that you no
1087:08 - longer need and so touching back on cost
1087:11 - for just a minute these versions can
1087:13 - really add up and start costing you some
1087:16 - serious money if you have thousands of
1087:18 - files with hundreds of versions and this
1087:21 - is where life cycle management comes
1087:23 - into play now cloud storage offers the
1087:25 - object lifecycle management feature in
1087:28 - order to support some common use cases
1087:30 - like setting a time to live or ttl for
1087:33 - objects
1087:34 - retaining non-current versions of
1087:36 - objects or downgrading storage classes
1087:39 - of objects to help manage costs now in
1087:42 - order to apply this feature to your
1087:44 - objects you would assign a lifecycle
1087:46 - management configuration to a bucket the
1087:49 - configuration contains a set of rules
1087:51 - which apply to current and feature
1087:53 - objects in the bucket when an object
1087:55 - meets the criteria of one of the rules
1087:58 - cloud storage automatically performs the
1088:00 - specified action on the object and so
1088:03 - some example use cases are shown here so
1088:06 - if you're looking to downgrade the
1088:07 - storage class
1088:08 - of objects older than 365 days to cold
1088:12 - line storage for compliance purposes
1088:15 - along with saving money life cycle
1088:17 - management is perfect for this another
1088:19 - use case is when you want to delete
1088:21 - objects created before january 1st of
1088:24 - 2020 and this is another great use case
1088:26 - to save money as well with keeping only
1088:29 - the three most recent versions of each
1088:32 - object in a bucket with versioning
1088:34 - enabled to keep from version objects
1088:36 - building up object lifecycle management
1088:39 - has so many other use cases across a
1088:41 - myriad of industries and when used
1088:43 - correctly is a great way to achieve
1088:46 - object management along with saving
1088:48 - money now i wanted to take a moment to
1088:51 - dive into the lifecycle management
1088:53 - configuration each lifecycle management
1088:55 - configuration contains a set of
1088:58 - components these are a set of rules
1089:00 - conditions and the action when the
1089:02 - conditions are met rules are any set of
1089:05 - conditions for any action conditions is
1089:08 - something an object must meet before the
1089:11 - action defined in the rule occurs on the
1089:14 - object and there are various conditions
1089:16 - to choose from that allows you to get
1089:18 - pretty granular and finally the action
1089:21 - which is where you would have the option
1089:23 - to delete or set storage class now when
1089:26 - you delete current versions this will
1089:28 - move the current version into a
1089:30 - non-current state and when you delete a
1089:33 - non-current version you will permanently
1089:36 - delete the version and cannot get it
1089:38 - back and so when you set the storage
1089:40 - class it will transition the object to a
1089:43 - different storage class so when defining
1089:45 - a rule you can specify any set of
1089:48 - conditions for any action if you specify
1089:51 - multiple conditions in a rule an object
1089:54 - has to match all of the conditions for
1089:57 - the action to be taken so if you have
1089:59 - three conditions and one of those
1090:01 - conditions have not been met then the
1090:03 - action will not take place if you
1090:05 - specify multiple rules that contain the
1090:08 - same action the action is taken when an
1090:11 - object matches the conditions in any of
1090:13 - these rules now if multiple rules have
1090:16 - their conditions satisfied
1090:18 - simultaneously for a single object cloud
1090:21 - storage will either perform the delete
1090:23 - action as it takes precedence over the
1090:25 - set storage class action or the set
1090:28 - storage class action that switches the
1090:30 - object to the storage class with the
1090:33 - lowest at rest storage pricing takes
1090:35 - precedence so for example if you have
1090:38 - one rule that deletes an object and
1090:41 - another rule that changes the object
1090:43 - storage class but both rules use the
1090:46 - exact same condition the delete action
1090:49 - always occurs when the condition is met
1090:51 - or if you have one rule that changes the
1090:54 - object storage class to near line
1090:56 - storage and another rule that changes
1090:58 - the object storage class to cold line
1091:00 - storage but both rules use the exact
1091:03 - same condition the object storage class
1091:06 - always changes to cold line storage when
1091:08 - the condition is met and so some
1091:10 - considerations that i wanted to point
1091:12 - out when it comes to cloud storage is
1091:15 - that when it comes to object life cycle
1091:17 - management
1091:18 - changes are in accordance to object
1091:20 - creation date as well once an object is
1091:23 - deleted it cannot be undeleted so please
1091:26 - be careful when permanently deleting a
1091:29 - version as well life cycle rules can
1091:31 - take up to 24 hours to take effect so be
1091:35 - aware when setting them and always be
1091:37 - sure to test these life cycle rules in
1091:40 - development first before rolling them
1091:42 - out into production and so that's pretty
1091:45 - much all i had to cover when it comes to
1091:47 - versioning and object life cycle
1091:49 - management and so you can now mark this
1091:51 - lesson as complete and whenever you're
1091:53 - ready join me in the console where we go
1091:56 - hands-on with versioning object life
1091:58 - cycle management and cloud storage as a
1092:01 - whole
1092:02 - [Music]
1092:06 - welcome back in this demo we're going to
1092:08 - cement the knowledge that we learned
1092:10 - from the past couple lessons on cloud
1092:12 - storage and really dive into the nitty
1092:15 - gritty when it comes to the features and
1092:17 - configuration you're first going to
1092:19 - create a cloud storage bucket and upload
1092:22 - some files to it and then interact with
1092:24 - the bucket and the files using the
1092:26 - console as well you're going to get your
1092:29 - hands dirty using the gsutil command
1092:32 - line tool and this is the tool for
1092:34 - managing cloud storage from the command
1092:36 - line now there's quite a bit of work to
1092:38 - do here so with that being said let's
1092:41 - dive in and so i am logged in here as
1092:44 - tony bowties at gmail.com along with
1092:47 - being in project bowtie inc and so the
1092:50 - first thing i want to do is i want to
1092:52 - create a cloud storage bucket so in
1092:54 - order for me to do that i'm going to
1092:55 - head over to the navigation menu and i'm
1092:58 - going to scroll down to storage
1093:00 - and here i already have a couple of
1093:02 - buckets that i created from earlier
1093:04 - lessons and you may have a couple
1093:06 - buckets as well but you're going to go
1093:08 - ahead and create a new bucket by going
1093:10 - up to the top here and click on create
1093:12 - bucket now i know that we've gone
1093:14 - through this before in previous lessons
1093:17 - but this time i wanted to go through all
1093:19 - the configuration options that are
1093:21 - available and so the first thing that
1093:23 - you're prompted to do here is to name
1093:25 - your bucket as explained in an earlier
1093:27 - lesson it needs to be a globally unique
1093:30 - name and so you can pick any name you
1093:32 - choose and so for me i'm going to call
1093:34 - this bucket bowtie inc dash 2021 i'm
1093:38 - going to hit continue and if it wasn't a
1093:40 - globally unique name it would error out
1093:43 - and you would have to enter in a new
1093:45 - name but since this bucket name is
1093:47 - globally unique i'm able to move forward
1093:49 - for location type you can select from
1093:52 - region dual region and multi region with
1093:55 - multi region under location you can
1093:57 - select from either the americas europe
1094:00 - or asia pacific and under dual region
1094:03 - you have the options of again choosing
1094:06 - from america's europe and asia pacific
1094:09 - and you will be given the regions for
1094:11 - each and so for this demo we're going to
1094:13 - go ahead and choose region and we're
1094:15 - going to keep the location as u.s east
1094:17 - one and once you've selected that you
1094:19 - can go ahead and hit continue and you're
1094:21 - going to be prompted to choose a default
1094:23 - storage class and here you have the
1094:25 - option of selecting from the four
1094:27 - storage classes that we discussed in an
1094:29 - earlier lesson and so for this demo you
1094:31 - can keep it as standard and simply click
1094:34 - on continue and so here you're prompted
1094:36 - to choose access control and because
1094:38 - we're going to be diving into acls you
1094:41 - can keep this as the default fine grain
1094:43 - access control you can go ahead and
1094:45 - click continue and under encryption you
1094:47 - can keep it as the default google manage
1094:50 - key but know that you always have the
1094:52 - option of choosing a customer manage key
1094:54 - and once you've uploaded your customer
1094:56 - manage key you can select it from here
1094:58 - and because i have no customer managed
1095:00 - keys no other keys show up so i'm going
1095:02 - to click on google manage keys and here
1095:04 - under retention policy i know i haven't
1095:07 - touched into that but just to give you
1095:09 - some context when placing a retention
1095:12 - policy on a bucket it ensures that all
1095:15 - current and future objects in the bucket
1095:17 - can't be deleted or replaced until they
1095:20 - reach the age that you define in the
1095:23 - retention policy so if you try to delete
1095:26 - or replace objects where the age is less
1095:28 - than the retention period it will
1095:30 - obviously fail and this is great for
1095:33 - compliance purposes in areas where logs
1095:36 - need to be audited by regulators every
1095:38 - year or where government required
1095:40 - retention periods apply as well with the
1095:43 - retention policy you have the option of
1095:46 - locking that retention policy and when
1095:48 - you lock a retention policy on a bucket
1095:50 - you prevent the policy from ever being
1095:53 - removed or the retention period from
1095:56 - ever being reduced and this feature is
1095:59 - irreversible so please be aware if
1096:01 - you're ever experimenting with lock
1096:03 - retention policies so if i set a
1096:06 - retention policy here i can retain
1096:08 - objects for a certain amount of seconds
1096:11 - days months and years and for this demo
1096:14 - we're not going to set any retention
1096:15 - policies so i'm going to check that off
1096:17 - and i'm going to go ahead and add a
1096:19 - label with the key being environment and
1096:22 - the value being test and just as a note
1096:24 - before you go ahead and click on create
1096:26 - over on the right hand side you will see
1096:28 - a monthly cost estimate and you will be
1096:31 - given an estimate with storage and
1096:33 - retrieval as well as how much it costs
1096:35 - for operations your sla and your
1096:38 - estimated monthly cost and so before
1096:40 - creating any buckets you can always do a
1096:43 - price check to see how much it'll cost
1096:46 - for storage size retrieval to get a good
1096:49 - idea of how much it'll cost you monthly
1096:51 - okay so once you're all done here you
1096:53 - can simply click on create
1096:56 - and it'll go ahead and create your
1096:57 - bucket and so now that your bucket is
1096:59 - created we want to add some files and so
1097:02 - we first want to go into copying files
1097:05 - from an instance to your cloud storage
1097:07 - bucket and so in order to do that we
1097:10 - need to create an instance and so we're
1097:12 - gonna go back over to the navigation
1097:13 - menu we're gonna scroll down to compute
1097:16 - engine and we're gonna create our
1097:17 - instance and for those who do not have
1097:20 - your default vpc set up please be sure
1097:22 - to create one before going ahead and
1097:24 - creating your instance i'm going to go
1097:26 - ahead and click on create i'm going to
1097:28 - name this instance
1097:29 - bowtie instance going to give it a label
1097:32 - of environment test click on save
1097:35 - the region is going to be
1097:37 - east one and you can keep the default
1097:39 - zone as us east 1b the machine type
1097:42 - we're going to change it to e2micro and
1097:44 - you're going to scroll down to access
1097:46 - scopes and here your instance is going
1097:48 - to need access to your cloud storage
1097:51 - bucket and so it's going to need cloud
1097:53 - storage access so you're going to click
1097:54 - on set access for each api scroll down
1097:57 - to storage and for this demo we'll
1097:59 - select full gonna leave everything else
1098:01 - as the default and simply click on
1098:03 - create and so we'll give it a couple
1098:05 - minutes here for instance to create okay
1098:08 - and my instance has been created and so
1098:10 - now i want to create some files and copy
1098:12 - them over to cloud storage so i'm going
1098:14 - to first navigate over to cloud storage
1098:17 - and into my bucket and this way you can
1098:19 - see the files that you upload and so
1098:22 - next you're going to open up cloud shell
1098:23 - and make this a little bigger for better
1098:25 - viewing and so now you're going to ssh
1098:27 - into your instance by using the command
1098:30 - gcloud compute ssh along with your
1098:32 - instance name the zone flag dash dash
1098:35 - zone with the zone of us east 1b i'm
1098:38 - going to go ahead and hit enter and you
1098:40 - may be prompted with a message asking to
1098:42 - authorize this api call and you want to
1098:44 - hit authorize and you're going to be
1098:46 - prompted to enter a passphrase for your
1098:48 - key pair enter it in again
1098:51 - and one more time
1098:52 - and success we're logged into the
1098:54 - instance i'm going to quickly clear my
1098:56 - screen and so i know i could have sshed
1098:59 - into the instance from the compute
1099:01 - engine console but i wanted to display
1099:03 - both the console and the shell on the
1099:06 - same screen to make viewing a bit easier
1099:08 - as i add and remove files to and from
1099:11 - the bucket okay and so now that you're
1099:13 - logged in you want to create your first
1099:15 - file that you can copy over to your
1099:17 - bucket so you can enter in the command
1099:20 - sudo nano file a bow ties dot text hit
1099:23 - enter and this will allow you to open up
1099:25 - the nano editor to edit the file of
1099:28 - bowties.txt and here you can enter in
1099:30 - any message that you'd like for me i'm
1099:32 - going to enter in learning to tie a bow
1099:35 - tie takes time okay and i'm going to hit
1099:38 - ctrl o to save hit enter to verify the
1099:41 - file name to right and ctrl x to exit
1099:44 - and so now i want to copy this file up
1099:46 - to my bucket and so here is where i'm
1099:48 - going to use the gsutil command so i'm
1099:50 - going to type in gsutil cp for copy the
1099:54 - name of the file which is file of
1099:55 - bowties
1099:57 - text along with gs colon forward slash
1100:00 - forward slash and the name of your
1100:02 - bucket which in my case is bow tie ink
1100:05 - dash 2021 and this should copy my file
1100:09 - file a bowties.txt up to my bucket of
1100:12 - bow tie inc
1100:13 - 2021 i'm gonna hit enter
1100:16 - okay and it's finished copying over and
1100:18 - if i go up here to the top right and
1100:20 - click on refresh i can see that my file
1100:23 - successfully uploaded and this is a
1100:25 - great and easy method to upload any
1100:28 - files that you may have to cloud storage
1100:30 - okay and so now that you've copied files
1100:33 - from your instance to your bucket you're
1100:35 - going to now copy some files from the
1100:37 - repo to be uploaded to cloud storage for
1100:40 - our next step so you're gonna go ahead
1100:42 - and exit out of the instance by just
1100:45 - simply typing in exit i'm gonna quickly
1100:47 - clear the screen and so here i need to
1100:49 - clone my repo if you already have clone
1100:52 - the repo then you can skip this step i'm
1100:55 - going to cd tilde to make sure i'm in my
1100:57 - home directory i'm going to do an ls and
1100:59 - so i can see here that i've already
1101:01 - cloned my repo so i'm going to cd into
1101:03 - that directory and i'm going to run the
1101:05 - command git pull to get the latest files
1101:08 - fantastic i'm going to now clear my
1101:09 - screen and i'm going to cd back to my
1101:12 - home directory and so now i want to copy
1101:14 - up the files that i want to work with to
1101:16 - my cloud storage bucket and they are two
1101:19 - jpegs by the name of pink
1101:21 - elephant-bowtie as well as plaid bowtie
1101:24 - and these files can be found in the repo
1101:26 - marked 12 storage services under zero
1101:30 - one cloud storage management and i will
1101:32 - be providing this in the lesson text as
1101:34 - well as can be found in the instructions
1101:36 - and so i'm going to simply cd into that
1101:38 - directory by typing in cd google cloud
1101:41 - associate cloud engineer 12 storage
1101:43 - services and 0 1 cloud storage
1101:46 - management i'm going to list all the
1101:47 - files in the directory and as you can
1101:49 - see here pink elephant dash bow tie and
1101:52 - plaid bow tie are both here and so i'm
1101:54 - going to quickly clear my screen and so
1101:56 - now for me to copy these files i'm going
1101:58 - to use the command gsutil
1102:01 - cp for copy star.jpg which is all the
1102:05 - jpegs that are available along with gs
1102:07 - colon forward slash forward slash and
1102:10 - the bucket name which is bow tie inc
1102:12 - dash 2021 i'm going to hit enter and it
1102:15 - says that it's successfully copied the
1102:17 - files i'm going to simply go up to the
1102:19 - top right hand corner and do another
1102:21 - refresh and success the files have been
1102:24 - successfully uploaded another perfect
1102:26 - example of copying files from another
1102:29 - source to your bucket using the gsutil
1102:32 - command line tool and so this is the end
1102:34 - of part one of this demo it was getting
1102:36 - a bit long so i decided to break it up
1102:38 - and this would be a great opportunity
1102:40 - for you to get up and have a stretch get
1102:43 - yourself a coffee or tea and whenever
1102:45 - you're ready part two will be starting
1102:47 - immediately from the end of part one so
1102:50 - you can complete this video and i will
1102:52 - see you in part two
1102:53 - [Music]
1102:57 - this is part two of the managing cloud
1103:00 - storage access demo and we'll be
1103:02 - starting exactly where we left off in
1103:04 - part 1. so with that being said let's
1103:06 - dive in and so now that we've uploaded
1103:09 - all these files we next want to make
1103:11 - this bucket publicly available now
1103:13 - please know that leaving a bucket public
1103:16 - is not common practice and should only
1103:19 - be used on the rare occasion that you
1103:21 - are hosting a static website from your
1103:23 - bucket and should always be kept private
1103:26 - whenever possible especially in a
1103:28 - production environment so please note
1103:31 - that this is only for the purposes of
1103:33 - this demo and so i'm going to quickly
1103:35 - show this to you in the console so i'm
1103:37 - going to shut down the cloud shell for
1103:39 - just a minute and i'm going to go to the
1103:40 - top menu and click on permissions and
1103:42 - under permissions i'm going to click on
1103:44 - add here you can add new members and
1103:47 - because you want to make it publicly
1103:48 - available you want to use the all users
1103:51 - member so you type in all and you should
1103:53 - get a pop-up bringing up all users and
1103:56 - all authenticated users you want to
1103:58 - click on all users and the role that you
1104:00 - want to select for this demo is going to
1104:03 - be storage object viewer so i'm going to
1104:05 - type in storage object viewer and here
1104:08 - it should pop up and select that and
1104:10 - then you can click on save you're going
1104:12 - to be prompted to make sure that this is
1104:14 - what you want to do that you want to
1104:15 - make this bucket public and so yes we do
1104:18 - so you can simply click on allow public
1104:20 - access and you will get a banner up here
1104:22 - at the top saying that this bucket is
1104:24 - public to internet and is a great fail
1104:27 - safe to have in case you were to ever
1104:29 - mistakenly make your bucket public and
1104:31 - if i head back over to objects you can
1104:34 - see that public access is available to
1104:37 - all the files in the bucket and so just
1104:39 - to verify this i'm going to copy the
1104:41 - public url for pink elephant dash bowtie
1104:44 - i'm going to open up a new tab paste in
1104:46 - the url hit enter and as you can see i
1104:48 - have public access to this picture and
1104:51 - close this tab and so now that we've
1104:53 - done our demo to make the bucket
1104:54 - publicly accessible we should go ahead
1104:57 - and remove public access so in order to
1105:00 - remove public permissions i can simply
1105:02 - go up to permissions and simply click on
1105:05 - remove public permissions i'm going to
1105:07 - get a prompt to make sure this is
1105:08 - exactly what i want to do and yes it is
1105:11 - so you can click on remove public
1105:13 - permissions a very simple and elegant
1105:15 - solution in order to remove public
1105:18 - access from your bucket and if you go
1105:20 - back to objects you'll see that all the
1105:22 - public access has been removed from all
1105:24 - the files and so now that you've
1105:26 - experienced how to add public access to
1105:29 - a bucket i wanted to get a little bit
1105:31 - more granular and so we're going to go
1105:33 - ahead and apply acl permissions for one
1105:36 - specific object and because i like pink
1105:39 - elephants let's go ahead and select pink
1105:41 - elephant dash bow tie and so here i can
1105:43 - go up to the top menu and click on edit
1105:46 - permissions and i'll be prompted with a
1105:48 - new window for permissions that are
1105:50 - currently available for this object you
1105:52 - can click on add entry click on the drop
1105:54 - down and select public from the
1105:57 - drop-down and it will automatically auto
1105:59 - populate the name which is all users and
1106:02 - the access which will be reader i'm
1106:04 - going to go ahead and click on save and
1106:06 - a public url will be generated and so
1106:08 - just to verify this i'm going to click
1106:10 - on the public url and success i now have
1106:13 - public access to this picture yet once
1106:15 - again i'm going to close down this tab
1106:17 - and so now that you've configured this
1106:19 - object for public access i want to show
1106:22 - you how to remove public access using
1106:24 - the command line this time so you're
1106:26 - going to go up to the top right hand
1106:27 - corner and open up cloud shell i'm going
1106:29 - to quickly clear my screen and i'm going
1106:31 - to paste in the command here which is
1106:33 - gsutil acl ch for change minus d which
1106:38 - is delete the name of the user which is
1106:40 - all users and if this was a regular user
1106:43 - you could enter in their email address
1106:45 - along with gs colon forward slash
1106:47 - forward slash the bucket name which in
1106:50 - my case is bow tie ink dash 2021 and the
1106:53 - name of the file which is pink elephant
1106:55 - bow tie dot jpeg i'm going to hit enter
1106:58 - and it says that it's been successfully
1106:59 - updated and so if i go back up here to
1107:02 - the console and i back out and go back
1107:04 - into the file i can see here that the
1107:06 - public url has been removed okay and now
1107:09 - there's one last step that we need to do
1107:11 - before ending this demo and this is to
1107:14 - create a signed url for the file so in
1107:17 - order to create a signed url we first
1107:19 - need to create a private key and so
1107:21 - we're gonna do this using a service
1107:23 - account and so i'm gonna head on over to
1107:24 - iam so i'm going to go up to the
1107:26 - navigation menu i'm going to go to i am
1107:28 - an admin and here with the menu on the
1107:30 - left i'm going to click on service
1107:32 - accounts here up at the top menu you're
1107:34 - going to click on create service account
1107:36 - and under service account name you can
1107:38 - enter in any name
1107:40 - but for me i'm going to enter in signed
1107:43 - url i'm going to leave everything else
1107:45 - as is i'm going to simply click on
1107:47 - create i'm going to close down cloud
1107:48 - shell because i don't really need it
1107:50 - right now just select a role and i'm
1107:52 - going to give it the role of storage
1107:54 - object viewer
1107:56 - i'm going to click on continue and i'm
1107:58 - going to leave the rest blank and simply
1108:00 - click on done and you should see a
1108:02 - service account with the name of signed
1108:04 - url and so in order to create a key i'm
1108:06 - going to simply go over to actions and
1108:09 - i'm going to click on the three dots and
1108:11 - i'm going to select create key from the
1108:13 - drop down menu and here i'm going to be
1108:15 - prompted with what type of key that i
1108:17 - want to create and you want to make sure
1108:18 - that json is selected and simply click
1108:21 - on create and this is where your key
1108:23 - will be automatically downloaded to your
1108:25 - downloads folder i'm going to click on
1108:27 - close and so once you have your key
1108:29 - downloaded you're able to start the
1108:31 - process of generating a signed url and
1108:34 - so i'm going to go ahead and use cloud
1108:36 - shell in order to generate this signed
1108:38 - url so i'm going to go ahead back up to
1108:40 - the top and open up cloud shell again
1108:42 - and then you can open up the cloud shell
1108:44 - editor going to go up to the top menu in
1108:46 - editor and click on file and you're
1108:48 - going to select upload files and here's
1108:51 - where you upload your key from your
1108:53 - downloads folder and i can see my key
1108:55 - has been uploaded right here and you can
1108:57 - rename your key file to something a
1108:59 - little bit more human readable so i'm
1109:01 - going to right click i'm going to click
1109:02 - on rename and you can rename this file
1109:05 - as privatekey.json hit ok and so once
1109:08 - you have your key uploaded and renamed
1109:10 - you can now go back into the terminal to
1109:13 - generate a signed url i'm going to
1109:15 - quickly clear the screen i'm going to
1109:16 - make sure that the private key is in my
1109:18 - path by typing in ls and as you can see
1109:21 - here privatekey.json
1109:23 - is indeed in my path and so before i
1109:25 - generate this key i'm going to head back
1109:27 - on over to cloud storage i'm going to
1109:29 - drill down into my bucket and as you can
1109:31 - see here pink elephant dash bow tie does
1109:34 - not have a public url and so when the
1109:36 - sign url is generated you will get a
1109:39 - public url that will not be shown here
1109:41 - in the console and will be private to
1109:44 - only the user that generated it and the
1109:47 - users that the url has been distributed
1109:49 - to okay and once you have everything in
1109:52 - place you can then go ahead and paste in
1109:54 - the command gsutil sign url minus d the
1109:58 - allotted time which is 10 minutes the
1110:01 - private key which is private key dot
1110:03 - json along with gs colon forward slash
1110:06 - forward slash your bucket name which in
1110:08 - my case is bow tie ink dash 2021 along
1110:11 - with the file name of
1110:14 - pinkelephant-bowtie.jpg i'm going to hit
1110:16 - enter and so i purposely left this error
1110:18 - here so you can see that when you
1110:20 - generate a signed url you need pi open
1110:23 - ssl in order to generate it and so the
1110:26 - caveat here is that because python 2 is
1110:28 - being deprecated the command pip install
1110:31 - pi openssl will not work pi open ssl
1110:35 - needs to be installed with python3 and
1110:37 - so to install it you're going to run the
1110:39 - command pip3 install pi open ssl and hit
1110:44 - enter and so once it's finished
1110:45 - installing you can now generate your
1110:48 - signed url i'm going to quickly clear my
1110:50 - screen paste in the command again hit
1110:52 - enter and success you've now generated a
1110:56 - sign url for the object pink elephant
1110:59 - bowtie.jpg and because this is a signed
1111:02 - url you will see under public url there
1111:05 - is no url there available even though it
1111:08 - is publicly accessible and so just to
1111:10 - verify this i'm going to highlight the
1111:12 - link here i'm going to copy it i'm going
1111:14 - to open up a new tab i'm going to paste
1111:16 - in this url hit enter and success this
1111:20 - sign url is working and anyone who has
1111:23 - access to it has viewing permissions of
1111:25 - the file for 10 minutes and so again
1111:28 - this is a great method for giving
1111:30 - someone access to an object who doesn't
1111:33 - have an account and will give them a
1111:35 - limited time to view or edit this object
1111:38 - and so i wanted to congratulate you on
1111:40 - making it through this demo and hope
1111:42 - that it has been extremely useful in
1111:45 - excelling your knowledge on managing
1111:47 - buckets files and access to the buckets
1111:50 - and files in cloud storage and so just
1111:52 - as a recap you created a cloud storage
1111:55 - bucket you then created an instance and
1111:57 - copied a file from that instance to the
1112:00 - bucket you then clone your repo to cloud
1112:02 - shell and copy two jpeg files to your
1112:06 - cloud storage bucket you then assigned
1112:08 - and then removed public access to your
1112:11 - bucket and then applied an acl to a file
1112:14 - in the bucket making it public as well
1112:17 - as removing public access right after
1112:19 - you then created a service account
1112:21 - private key and generated a signed url
1112:25 - to an object in that bucket
1112:27 - congratulations again on a job well done
1112:29 - and so that's pretty much all i wanted
1112:31 - to cover in this demo on managing cloud
1112:35 - storage access so you can now mark this
1112:37 - as complete and let's move on to the
1112:39 - next one
1112:40 - [Music]
1112:44 - welcome back in this demo we're going to
1112:46 - be getting into the weeds with object
1112:49 - versioning and life cycle management
1112:51 - using both the console and the command
1112:53 - line we're going to go through how
1112:55 - versioning works and what happens when
1112:57 - objects get promoted along with creation
1113:00 - configuration and editing these life
1113:03 - cycle policies and so with that being
1113:05 - said let's dive in so we're going to be
1113:08 - starting off from where we left off in
1113:10 - the last demo with all the resources
1113:12 - intact that we created before and we're
1113:15 - going to go ahead and dive right into
1113:17 - versioning and so the first thing that
1113:19 - you want to do is turn on versioning for
1113:21 - your current bucket so in my case for
1113:24 - bow tie ink dash 2021 and we're going to
1113:27 - do this through the command line so i'm
1113:29 - going to first go up to the top right
1113:30 - hand corner and open up cloud shell and
1113:33 - so you first want to see if versioning
1113:36 - is turned on for your bucket and you can
1113:38 - do this by using the command gsutil
1113:41 - versioning get along with gs colon
1113:44 - forward slash forward slash with your
1113:46 - bucket name and hit enter and you may be
1113:49 - prompted with a message asking you to
1113:51 - authorize this api call you definitely
1113:53 - want to authorize and as expected
1113:55 - versioning is not turned on on this
1113:57 - bucket hence the return of suspended and
1114:00 - so in order to turn versioning on we're
1114:02 - going to use a similar command gsutil
1114:05 - versioning and instead of get we're
1114:07 - going to use set on gs colon forward
1114:10 - slash forward slash and the bucket name
1114:12 - and hit enter and versioning has been
1114:15 - enabled and so if i run the command
1114:17 - gsutil version in get again i'll get a
1114:20 - response of enabled okay great now that
1114:23 - we have versioning enabled we can go
1114:25 - ahead with the next step which is to
1114:27 - delete one of the files in the bucket
1114:29 - and so you can go ahead and select plaid
1114:31 - bowtie.jpg
1114:33 - and simply click on delete you can
1114:35 - confirm the deletion and the file has
1114:37 - been deleted now technically the file
1114:39 - has not been deleted it is merely been
1114:42 - converted to a non-current version and
1114:44 - so in order to check the current and
1114:47 - non-current versions i'm going to use
1114:49 - the command
1114:50 - gsutil
1114:52 - ls minus a along with the bucket name of
1114:55 - g s colon forward slash forward slash
1114:58 - bow tie inc dash 2021 i'm gonna hit
1115:01 - enter
1115:02 - and as you can see here plaid bow tie
1115:04 - still shows up the ls minus a command is
1115:08 - a linux command to show all files
1115:10 - including the hidden files and so what's
1115:13 - different about these files is right
1115:15 - after the dot text or dot jpg you will
1115:18 - see a hashtag number and this is the
1115:20 - generation number and this determines
1115:22 - the version of each object and so what i
1115:25 - want to do now is bring back the
1115:27 - non-current version and make it current
1115:30 - so i'm going to promote the non-current
1115:32 - version of plaid bowtie.jpg
1115:34 - to the current version and so in order
1115:36 - to do this i'm going to run the command
1115:38 - gsutil and v for move along with the
1115:41 - bucket of gs colon forward slash forward
1115:44 - slash bowtie inc hyphen 2021 and the
1115:48 - name of the file of plaid bow tie dot
1115:51 - jpeg along with the generation number
1115:53 - and i'm going to copy it from the
1115:55 - currently listed i'm going to paste it
1115:57 - in and so now we need to put in the
1115:59 - target which is going to be the same
1116:01 - without the generation number and paste
1116:03 - that in then hit enter
1116:05 - okay operation completed and so if i go
1116:08 - up to the top right hand corner and
1116:10 - click on refresh i can see that now
1116:13 - there is a current version for plaid bow
1116:15 - tie now just know that using the move
1116:18 - command actually deletes the non-current
1116:20 - version and gives the new current
1116:22 - version a new generation number and so
1116:25 - in order to verify this i'm going to
1116:27 - quickly clear my screen and i'm going to
1116:29 - run the command gsutil ls minus a along
1116:33 - with the bucket name a bow tie inc dash
1116:35 - 2021
1116:38 - and the generation number here is
1116:40 - different than that of the last now if i
1116:43 - use the cp or copy command it would
1116:46 - leave the non-current version and create
1116:48 - a new version on top of that leaving two
1116:51 - objects with two different generation
1116:53 - numbers okay so with that step being
1116:55 - done you now want to log into your linux
1116:58 - instance and we're going to be doing
1117:00 - some versioning for file of bowties.text
1117:03 - so i'm going to go ahead and clear my
1117:05 - screen again and i'm going to run the
1117:06 - command gcloud compute ssh bowtie
1117:10 - instance which is the name of my
1117:12 - instance along with the zone flag dash
1117:14 - dash zone of the zone us east 1b i'm
1117:18 - going to hit enter
1117:19 - and you should be prompted for the
1117:21 - passphrase of your key
1117:23 - and i'm in and so here you want to edit
1117:25 - file a bowties.txt to a different
1117:28 - version so you can go ahead and run the
1117:30 - command sudo nano file a bow ties dot
1117:33 - text and hit enter and you should have
1117:36 - learning to tie a bow tie takes time and
1117:38 - what you want to do is append version 2
1117:42 - right at the end ctrl o to save enter to
1117:45 - verify the file name to right and
1117:47 - control x to exit and so now we want to
1117:49 - copy file a bow ties dot text to your
1117:52 - current bucket mine being bow tie ink
1117:55 - dash 2021 so i'm going to go ahead and
1117:57 - run the command gsutil cp the name of
1118:00 - the file which is file of bowties dot
1118:03 - text and the target which is going to be
1118:06 - bowtie inc
1118:08 - 2021 and hit enter
1118:10 - and it's copied the file to the bucket
1118:13 - and so if i hit refresh in the console
1118:15 - you can see that there is only one
1118:17 - version of file of bowties.text and so
1118:20 - to check on all the versions that i have
1118:22 - i'm going to go back to my cloud shell
1118:24 - i'm going to quickly clear my screen and
1118:26 - i'm going to run the command gsutil ls
1118:30 - minus a along with the target bucket
1118:33 - hit enter and as you can see here there
1118:36 - are now two versions of file of
1118:38 - bowties.text and if i quickly open this
1118:41 - up
1118:42 - i'm gonna click on the url you can see
1118:44 - here that this is version two and so
1118:46 - this should be the latest generation of
1118:49 - file of bowties.txt that you edited over
1118:52 - in your instance i'm going to close this
1118:54 - tab now and so what i want to do now is
1118:56 - i want to promote the non-current
1118:58 - version to be the current version in
1119:01 - essence making version 2 the non-current
1119:03 - version and so i'm going to run the
1119:05 - command gsutil cp and i'm going to take
1119:08 - the older generation number and i'm
1119:10 - going to copy it and paste it here and
1119:13 - the target is going to be the same
1119:15 - without the generation number and paste
1119:17 - it and hit enter okay and the file has
1119:20 - been copied over so i'm going to do a
1119:22 - quick refresh in the console i'm going
1119:24 - to drill down into file a bowties.txt
1119:27 - and when i click on the url link it
1119:29 - should come up as version 1. and so this
1119:31 - is a way to promote non-current versions
1119:34 - to current versions using the gsutil
1119:37 - copy command or the gsutil move command
1119:40 - i'm going to close on this tab now i'm
1119:42 - going to quickly clear my screen and if
1119:44 - i run the command gsutil ls minus a
1119:47 - again you can see that i have even more
1119:50 - files and so these files and versions of
1119:53 - files will eventually accumulate and
1119:55 - continuously take up space along with
1119:58 - costing you money and so in order to
1120:00 - mitigate this a good idea would be to
1120:02 - put life cycle policies into place and
1120:05 - so you're gonna go ahead now and add a
1120:07 - life cycle policy to the bucket and this
1120:10 - will help manage the ever-growing
1120:12 - accumulation of files as more files are
1120:14 - being added to the bucket and more
1120:17 - versions are being produced something
1120:19 - that is very common that is seen in many
1120:21 - different environments and so we're
1120:23 - going to go ahead and get this done in
1120:24 - the console so i'm going to close down
1120:26 - cloud shell and i'm going to go back to
1120:28 - the main page of the bucket and under
1120:30 - the menu you can click on lifecycle and
1120:33 - here you'll be able to add the lifecycle
1120:35 - rules and so here you're going to click
1120:37 - on add a rule and the first thing that
1120:39 - you're prompted to do is to select an
1120:41 - action and so the first rule you're
1120:43 - going to apply is to delete non-current
1120:46 - objects after seven days so you're gonna
1120:49 - click on delete object you're gonna be
1120:51 - prompted with a warning gonna hit
1120:52 - continue and you'll be prompted to
1120:54 - select object conditions and as
1120:57 - discussed in an earlier lesson there are
1120:59 - many conditions to choose from and
1121:01 - multiple conditions can be selected so
1121:04 - here you're going to select days since
1121:06 - becoming non-current and in the empty
1121:08 - field you're going to type in 7. you can
1121:10 - click on continue and before you click
1121:12 - on create i wanted just to note that any
1121:15 - life cycle rule can take up to 24 hours
1121:18 - to take effect so i'm going to click on
1121:20 - create and here you can see the rule has
1121:23 - been applied to delete objects after
1121:26 - seven days when object becomes
1121:28 - non-current and so now that we added a
1121:30 - delete rule we're going to go ahead and
1121:32 - add another rule to move current files
1121:35 - that are not being used to a storage
1121:37 - class that can save the company money
1121:40 - and so let's go ahead and create another
1121:43 - lifecycle rule but this time to use this
1121:46 - set storage class action and so the
1121:48 - files that accumulate that have been
1121:50 - there for over 90 days you want to set
1121:52 - the storage class the cold line so this
1121:54 - way it'll save you some money and so
1121:56 - you're going to click on add a rule
1121:58 - you're going to select set storage class
1121:59 - to cold line and as a note here it says
1122:02 - archive objects will not be changed to
1122:04 - cold line so you can move forward with
1122:06 - the storage class but you can't move
1122:08 - backwards in other words i can't move
1122:10 - from cold line to near line or archive
1122:13 - the cold line i can only move from near
1122:15 - line to cold line or cold line to
1122:17 - archive so i'm going to go ahead and
1122:19 - click continue for the object conditions
1122:21 - you want to select age and in the field
1122:24 - you want to enter 90 days and here you
1122:26 - want to hit continue and finally click
1122:29 - on create and so in order to actually
1122:31 - see these rules take effect like i said
1122:33 - before it'll take up to 24 hours and so
1122:36 - before we end this demo i wanted to show
1122:39 - you another way to edit a life cycle
1122:41 - policy by editing the json file itself
1122:44 - so you can head on up to the top right
1122:47 - and open up cloud shell i'm going to
1122:48 - bring this down a little bit and you're
1122:50 - going to run the command gsutil
1122:52 - lifecycle get along with the bucket name
1122:55 - and output it to a file called
1122:57 - lifecycle.json and hit enter
1123:00 - and no errors so that's a good sign next
1123:03 - i'm going to run the command ls and as
1123:05 - you can see here the lifecycle.json file
1123:08 - has been written and so i'd like to edit
1123:10 - this file where it changes the set to
1123:12 - cold line rule from 90 days to 120 days
1123:16 - as tony bowtie's manager thinks that
1123:19 - they should keep the files a little bit
1123:20 - longer before sending it to coldline and
1123:23 - so in order to edit this file you're
1123:25 - going to run the command sudo nano
1123:28 - along with the name of the file of
1123:30 - lifecycle.js you hit enter and it's
1123:33 - going to be a long string but if you use
1123:36 - your arrow keys and move down and then
1123:39 - back you'll see the set to cold line
1123:41 - rule with the age of 90 days so i'm
1123:44 - going to move over here and i'm going to
1123:45 - edit this to 120 and i'm going to hit
1123:48 - ctrl o to save enter to verify file name
1123:50 - to write and ctrl x to exit and just
1123:53 - know that you can also edit this file in
1123:56 - cloud shell editor and so in order for
1123:58 - me to put this lifecycle policy in place
1124:01 - i need to set this as the new lifecycle
1124:03 - policy and so in order for me to do that
1124:06 - i'm going to run the command gsutil
1124:08 - lifecycle set along with the name of the
1124:10 - json file which is
1124:12 - lifecycle.json along with the bucket
1124:14 - name and hit enter and it looks like it
1124:17 - said it and i'm going to do quick
1124:18 - refresh in the console just to verify
1124:21 - and success the rule has been changed
1124:24 - from 90 days to 120 days congratulations
1124:28 - on completing this demo now a lot of
1124:30 - what you've experienced here is more of
1124:32 - what you will see in the architect exam
1124:35 - as the cloud engineer exam focuses on
1124:38 - more of the high level theory of these
1124:40 - cloud storage features
1124:42 - but i wanted to show you some real life
1124:44 - scenarios and how to apply the theory
1124:47 - that was shown in previous lessons into
1124:49 - practice and so just as a recap you set
1124:52 - versioning on the current bucket that
1124:54 - you are working in and you deleted a
1124:56 - file and made it non-current you then
1124:58 - brought it back to be current again you
1125:00 - then edited a file on your instance and
1125:03 - copied it over to replace the current
1125:05 - version of that file in your bucket you
1125:08 - then promoted the non-current version as
1125:10 - the new one and moved into lifecycle
1125:12 - rules where you created two separate
1125:14 - rules you created a rule to delete files
1125:18 - along with the rule to set storage class
1125:20 - after a certain age of the file and the
1125:22 - last step you took was to copy the
1125:25 - lifecycle policy to your cloud shell and
1125:27 - edited that policy and set it to a newer
1125:30 - edited version and so that pretty much
1125:32 - covers this demo on object versioning
1125:36 - and lifecycle management congratulations
1125:38 - again on a job well done and so before
1125:40 - you go
1125:41 - make sure you delete all the resources
1125:44 - you've created for the past couple of
1125:46 - demos as you want to make sure that
1125:48 - you're not accumulating any unnecessary
1125:51 - costs and so i'm going to do a quick run
1125:53 - through on deleting these resources and
1125:55 - so i'm going to quickly close down cloud
1125:56 - shell and i'm going to head on over to
1125:58 - the navigation menu go to compute engine
1126:01 - i'm going to delete my instance and i'm
1126:03 - going to head back on over to cloud
1126:05 - storage and delete the bucket there i'm
1126:07 - going to confirm the deletion i'm going
1126:09 - to click on delete and so that covers
1126:11 - the deletion of all the resources so you
1126:14 - can now mark this as complete and i'll
1126:16 - see you in the next one
1126:21 - welcome back and in this lesson i'm
1126:24 - going to be covering cloud sql one of
1126:26 - google cloud's many database offerings
1126:29 - that offers reliable secure and scalable
1126:32 - sql databases without having to worry
1126:35 - about the complexity to set it all up
1126:37 - now there's quite a bit to cover here so
1126:39 - with that being said let's dive in now
1126:42 - cloud sql is a fully managed cloud
1126:45 - native relational database service that
1126:48 - offers mysql postgres and sql server
1126:52 - engines with built-in support for
1126:54 - replication cloud sql is a database as a
1126:57 - service offering from google where
1126:59 - google takes care of all the underlying
1127:02 - infrastructure for the database along
1127:04 - with the operating system and the
1127:06 - database software now because there are
1127:08 - a few different types of database
1127:10 - offerings from google cloud sql was
1127:13 - designed for low latency transactional
1127:16 - and relational database workloads it's
1127:19 - also available in three different
1127:21 - flavors of databases mysql postgres and
1127:25 - the newest edition is sql server and all
1127:28 - of them support standard apis for
1127:30 - connectivity cloud sql offers
1127:33 - replication using different types of
1127:35 - read replicas which i will get into a
1127:38 - little bit later and offers capabilities
1127:40 - for high availability for continuous
1127:43 - access to your data cloud sql also
1127:46 - offers backups in two different flavors
1127:49 - and allows you to restore your database
1127:52 - from these backups with the same amount
1127:54 - of ease now along with your backups
1127:57 - comes point in time recovery for when
1127:59 - you want to restore a database from a
1128:02 - specific point in time cloud sql storage
1128:05 - relies on connected persistent disks in
1128:07 - the same zone that are available in
1128:10 - regular hard disk drives or ssds that
1128:14 - currently give you up to 30 terabytes of
1128:16 - storage capacity and because the same
1128:19 - technologies lie in the background for
1128:21 - persistent disks
1128:23 - automatic storage increase is available
1128:26 - to resize your disks for more storage
1128:29 - cloud sql also offers encryption at rest
1128:32 - and in transit for securing data
1128:34 - entering and leaving your instance and
1128:37 - when it comes to costs you are billed
1128:39 - for cpu memory and storage of the
1128:42 - instance along with egress traffic as
1128:45 - well please be aware that there is a
1128:47 - licensing cost when it comes to windows
1128:50 - instances now cloud sql instances are
1128:52 - not available in the same instance types
1128:55 - as compute engine and are only available
1128:58 - in the shared core
1128:59 - standard and high memory cpu types and
1129:03 - when you see them they will be clearly
1129:05 - marked with a db on the beginning of the
1129:08 - cpu type you cannot customize these
1129:10 - instances like you can with compute
1129:12 - engine and so memory will be pre-defined
1129:15 - when choosing the instance type now
1129:17 - storage types for cloud sql are only
1129:20 - available in hard disk drives and ssds
1129:23 - you are able to size them according to
1129:25 - your needs and as stated earlier can be
1129:28 - sized up to 30 terabytes in size and
1129:31 - when entering the danger zone of having
1129:33 - a full disk you do have the option of
1129:35 - enabling automatic storage increase so
1129:38 - you never have to worry about filling up
1129:40 - your disk before that 30 terabyte limit
1129:43 - now when it comes to connecting to your
1129:45 - cloud sql instance you can configure it
1129:48 - with a public or private ip but know
1129:52 - that after configuring the instance with
1129:54 - a private ip it cannot be changed
1129:56 - although connecting with the private ip
1129:59 - is preferred when connecting from a
1130:01 - client on a resource with access to a
1130:04 - vpc as well it is always best practice
1130:08 - to use private i p addresses for any
1130:11 - database in your environment whenever
1130:13 - you can now moving on to authentication
1130:15 - options the recommended method to
1130:18 - connecting to your cloud sql instance is
1130:21 - using cloud sql proxy the cloud sql
1130:23 - proxy allows you to authorize and secure
1130:26 - your connections using iam permissions
1130:29 - unless using the cloud sql proxy
1130:32 - connections to an instance's public ip
1130:34 - address are only allowed if the
1130:36 - connection comes from an authorized
1130:39 - network authorized networks are ip
1130:41 - addresses or ranges that the user has
1130:44 - specified as having permission to
1130:46 - connect once you are authorized you can
1130:48 - connect to your instance through
1130:50 - external clients or applications and
1130:53 - even other google cloud services like
1130:56 - compute engine gke app engine cloud
1131:00 - functions and cloud run now i wanted to
1131:03 - focus a moment here on the recommended
1131:05 - method for connecting to your instance
1131:08 - which is cloud sql proxy now as
1131:10 - mentioned before the cloud sql proxy
1131:13 - allows you to authorize and secure your
1131:15 - connections using iam permissions the
1131:18 - proxy validates connections using
1131:21 - credentials for a user or service
1131:23 - account and wrapping the connection in
1131:26 - an ssl tls layer that is authorized for
1131:30 - a cloud sql instance using the cloud sql
1131:33 - proxy is the recommended method for
1131:36 - authenticating connections to a cloud
1131:38 - sql instance as it is the most secure
1131:41 - the client proxy is an open source
1131:43 - library distributed as an executable
1131:46 - binary and is available for linux macos
1131:50 - and windows the client proxy acts as an
1131:53 - intermediary server that listens for
1131:55 - incoming connections wraps them in ssl
1131:58 - or tls and then passes them to a cloud
1132:01 - sql instance the cloud sql proxy handles
1132:04 - authentication with cloud sql providing
1132:07 - secure access to cloud sql instances
1132:10 - without the need to manage allowed ip
1132:13 - addresses or configure ssl connections
1132:16 - as well this is also the best solution
1132:18 - for applications that hold ephemeral
1132:20 - eyepiece and while the proxy can listen
1132:23 - on any port
1132:24 - it only creates outgoing connections to
1132:27 - your cloud sql instance on port 3307 now
1132:31 - when it comes to database replication
1132:33 - it's more than just copying your data
1132:36 - from one database to another the primary
1132:38 - reason for using replication is to scale
1132:41 - the use of data in a database without
1132:44 - degrading performance other reasons
1132:46 - include migrating data between regions
1132:48 - and platforms and from an on-premises
1132:51 - database to cloud sql you could also
1132:53 - promote a replica if the original
1132:56 - instance becomes corrupted and i'll be
1132:58 - getting into promoting replicas a little
1133:00 - bit later now when it comes to a cloud
1133:03 - sql instance the instance that is
1133:05 - replicated is called a primary instance
1133:08 - and the copies are called read replicas
1133:11 - the primary instance and read replicas
1133:14 - all reside in cloud sql read replicas
1133:17 - are read-only and you cannot write to
1133:19 - them the read replica processes queries
1133:22 - read requests and analytics traffics
1133:24 - thus reducing the load on the primary
1133:27 - instance read replicas can have more
1133:30 - cpus in memory than the primary instance
1133:33 - but they cannot have any less and you
1133:35 - can have up to 10 read replicas per
1133:38 - primary instance and you can connect to
1133:40 - a replica directly using its connection
1133:43 - name and ip address cloud sql supports
1133:46 - the following types of replicas
1133:49 - read replicas cross region read replicas
1133:52 - external read replicas and cloud sql
1133:55 - replicas when replicating from an
1133:58 - external server now when it comes to
1134:00 - read replicas you would use it to
1134:02 - offload work from a cloud sql instance
1134:05 - the read replica is an exact copy of the
1134:08 - primary instance and data and other
1134:10 - changes on the primary instance are
1134:13 - updated in almost real time on the read
1134:16 - replica a read replica is created in a
1134:19 - different region from the primary
1134:21 - instance and you can create a cross
1134:23 - region read replica the same way as you
1134:25 - would create an in-region replica this
1134:28 - improves read performance by making
1134:30 - replicas available closer to your
1134:32 - application's region it also provides
1134:34 - additional disaster recovery capability
1134:37 - to guard you against a regional failure
1134:39 - it also lets you migrate data from one
1134:42 - region to another with minimum downtime
1134:45 - and lastly when it comes to external
1134:47 - read replicas these are external mysql
1134:50 - instances that replicate from a cloud
1134:53 - sql primary instance
1134:55 - for example a mysql instance running on
1134:58 - compute engine is considered an external
1135:01 - instance and so just as a quick note
1135:03 - here before you can create a read
1135:05 - replica of a primary cloud sql instance
1135:08 - the instance must meet the following
1135:10 - requirements automated backups must be
1135:13 - enabled binary logging must be enabled
1135:16 - which requires point-in-time recovery to
1135:19 - be enabled and at least one backup must
1135:22 - have been created after binary logging
1135:24 - was enabled and so when you have read
1135:27 - replicas in your environment it gives
1135:29 - you the flexibility of promoting those
1135:31 - replicas if needed now promoting
1135:33 - replicas is a feature that can be used
1135:36 - for when your primary database becomes
1135:38 - corrupted or unreachable now you can
1135:41 - promote an in-region read replica or
1135:44 - cross-region re-replica depending on
1135:46 - where you have your read replicas hosted
1135:49 - so when you promote a read replica the
1135:52 - instance stops replication and converts
1135:55 - the instance to a standalone cloud sql
1135:57 - primary instance with read and write
1136:00 - capabilities please note that this
1136:02 - cannot be undone and also note that when
1136:05 - your new primary instance has started
1136:08 - your other read replicas are not
1136:10 - transferred over from the old primary
1136:12 - instance you will need to reconnect your
1136:15 - other read replicas to your new primary
1136:18 - instance and as you can see here
1136:20 - promoting a replica is done manually and
1136:23 - intentionally whereas high availability
1136:26 - has a standby instance that
1136:28 - automatically becomes the primary in
1136:31 - case of a failure horizontal outage now
1136:34 - when it comes to promoting cross-region
1136:35 - replicas there are two common scenarios
1136:38 - for promotion
1136:40 - regional migration which performs a
1136:42 - planned migration of a database to a
1136:44 - different region and disaster recovery
1136:47 - and this is where you would fail over a
1136:49 - database to another region in the event
1136:52 - that the primary instances region
1136:54 - becomes unavailable both use cases
1136:57 - involve setting up cross-region
1136:58 - replication and then promoting the
1137:01 - replica the main difference between them
1137:03 - is whether the promotion of the replica
1137:06 - is planned or unplanned now if you're
1137:08 - promoting your replicas for a regional
1137:11 - migration you can use a cross region
1137:13 - replica to migrate your database to
1137:16 - another region with minimal downtime and
1137:18 - this is so you can create a replica in
1137:21 - another region wait until the
1137:23 - replication catches up promote it and
1137:25 - then direct your applications to the
1137:27 - newly promoted instance the steps
1137:29 - involved in promotion are the same as
1137:31 - for promoting an in-region replica and
1137:34 - so when you're promoting replicas for
1137:36 - disaster recovery cross-region replicas
1137:39 - can be used as part of this disaster
1137:41 - recovery procedure you can promote a
1137:43 - cross-region replica to fail over to
1137:46 - another region should the primary
1137:48 - instances region become unavailable for
1137:51 - an extended period of time so in this
1137:54 - example the entire u.s east 1 region has
1137:57 - gone down yet the reed replica in the
1137:59 - europe region is still up and running
1138:01 - and although there may be a little bit
1138:03 - more latency for your customers in north
1138:05 - america i'm able to promote this read
1138:08 - replica connect it to the needed
1138:10 - resources and get back to business now
1138:13 - moving along to high availability cloud
1138:16 - sql offers aha capabilities out of the
1138:19 - box the aha configuration sometimes
1138:22 - called a cluster provides data
1138:24 - redundancy so a cloud sql instance
1138:27 - configured for ha is also called a
1138:30 - regional instance and is located in a
1138:32 - primary and secondary zone within the
1138:35 - configured region within a regional
1138:38 - instance the configuration is made up of
1138:40 - a primary instance and a standby
1138:43 - instance and through synchronous
1138:45 - replication to each zone's persistent
1138:47 - disk all rights made to the primary
1138:49 - instance are also made to the standby
1138:52 - instance each second the primary
1138:55 - instance writes to a system database as
1138:57 - a heartbeat signal if multiple
1139:00 - heartbeats aren't detected
1139:01 - failover is initiated and so if an
1139:04 - ha-configured instance becomes
1139:06 - unresponsive cloud sql automatically
1139:09 - switches to serving data from the
1139:11 - standby instance and this is called a
1139:13 - failover in this example the primary
1139:16 - instance or zone fails and failover is
1139:19 - initiated so if the primary instance is
1139:22 - unresponsive for approximately 60
1139:25 - seconds or the zone containing the
1139:27 - primary instance experiences an outage
1139:30 - failover will initiate the standby
1139:33 - instance immediately starts serving data
1139:36 - upon reconnection through a shared
1139:38 - static ip address with the primary
1139:41 - instance and the standby instance now
1139:44 - serves data from the secondary zone and
1139:46 - now when the primary instance is
1139:48 - available again a fail back will happen
1139:51 - and this is when traffic will be
1139:53 - redirected back to the primary instance
1139:56 - and the standby instance will go back
1139:59 - into standby mode as well the regional
1140:02 - persistent disk will pick up replication
1140:04 - to the persistent disk in that same zone
1140:07 - and with regards to billing an ha
1140:10 - configured instance is charged at double
1140:13 - the price of a standalone instance
1140:15 - and this includes cpu ram and storage
1140:18 - also note that the standby instance
1140:21 - cannot be used for read queries and this
1140:23 - is where it differs from read replicas
1140:26 - as well a very important note here is
1140:28 - that automatic backups and point in time
1140:31 - recovery must be enabled for high
1140:33 - availability and so the last topic that
1140:36 - i wanted to touch on is backups
1140:38 - and backups help you restore lost data
1140:41 - to your cloud sql instance you can also
1140:43 - restore an instance that is having
1140:45 - problems from a backup you enable
1140:48 - backups for any instance that contains
1140:50 - necessary data backups protect your data
1140:53 - from loss or damage enabling automated
1140:56 - backups along with binary logging is
1140:59 - also required for some operations such
1141:02 - as clone and replica creation by default
1141:05 - cloud sql stores backup data in two
1141:08 - regions for redundancy one region can be
1141:10 - the same region that the instance is in
1141:13 - and the other is a different region if
1141:15 - there are two regions in a continent the
1141:17 - backup data remains on the same
1141:20 - continent cloud sql also lets you select
1141:22 - a custom location for your backup data
1141:25 - and this is great if you need to comply
1141:27 - with data residency regulations for your
1141:30 - business now cloud sql performs two
1141:33 - types of backups on-demand backups and
1141:36 - automated backups now with on-demand
1141:39 - backups you can create a backup at any
1141:41 - time and this is useful for when you're
1141:44 - making risky changes that may go
1141:46 - sideways you can always create on-demand
1141:49 - backups for any instance whether the
1141:52 - instance has automatic backups enabled
1141:54 - or not and these backups persist until
1141:57 - you delete them or until their instance
1142:00 - is deleted now when it comes to
1142:02 - automated backups these use a four hour
1142:05 - backup window these backups start during
1142:07 - the backup window and just as a note
1142:10 - when possible you should schedule your
1142:12 - backups when your instance has the least
1142:14 - activity automated backups occur every
1142:17 - day when your instance is running at any
1142:20 - time in the 36 hour window and by
1142:23 - default up to seven most recent backups
1142:26 - are retained you can also configure how
1142:28 - many automated backups to retain from 1
1142:31 - to
1142:32 - 365. now i've touched on this topic many
1142:35 - times in this lesson and i wanted to
1142:37 - highlight it for just a second and this
1142:39 - is point-in-time recovery so
1142:41 - point-in-time recovery helps you recover
1142:43 - an instance to a specific point in time
1142:47 - for example if an error causes a loss of
1142:49 - data you can recover a database to its
1142:52 - state before the error happened a point
1142:55 - in time recovery always creates a new
1142:57 - instance and you cannot perform a point
1143:00 - in time recovery to an existing instance
1143:03 - and point in time recovery is enabled by
1143:06 - default when you create a new cloud sql
1143:08 - instance and so when it comes to billing
1143:11 - by default cloud sql retains seven days
1143:14 - of automated backups plus all on-demand
1143:18 - backups for an instance and so i know
1143:20 - there is a lot to retain in this lesson
1143:22 - on cloud sql but be sure that these
1143:25 - concepts and knowing the difference
1143:27 - between them as well as when to use each
1143:29 - feature will be a sure help in the exam
1143:33 - along with giving you the knowledge you
1143:35 - need to use cloud sql in your role as a
1143:38 - cloud engineer and so that's pretty much
1143:40 - all i had to cover when it comes to
1143:42 - cloud sql so you can now mark this
1143:45 - lesson as complete and let's move on to
1143:47 - the next one
1143:52 - welcome back and in this lesson i wanted
1143:55 - to touch on google cloud's global
1143:57 - relational database called cloud spanner
1144:01 - now cloud spanner is the same in some
1144:03 - ways as cloud sql when it comes to asset
1144:06 - transactions sql querying and strong
1144:09 - consistency but differs in the way that
1144:12 - data is handled under the hood than
1144:14 - cloud sql and so knowing this database
1144:17 - only at a high level is needed for the
1144:19 - exam but i'll be going into a bit more
1144:22 - detail just to give you a better
1144:24 - understanding on how it works so with
1144:26 - that being said let's dive in now cloud
1144:29 - spanner is a fully managed relational
1144:32 - database service that is both strongly
1144:35 - consistent and horizontally scalable
1144:37 - cloud spanner is another database as a
1144:39 - service offering from google and so it
1144:42 - strips away all the headaches of setting
1144:44 - up and maintaining the infrastructure
1144:47 - and software needed to run your database
1144:50 - in the cloud now being strongly
1144:52 - consistent in this context is when data
1144:55 - will get passed on to all the replicas
1144:58 - as soon as a write request comes to one
1145:00 - of the replicas of the database cloud
1145:02 - spanner uses truetime a highly available
1145:06 - distributed atomic clock system that is
1145:09 - provided to applications on all google
1145:11 - servers it applies a time stamp to every
1145:15 - transaction on commit and so
1145:17 - transactions in other regions are always
1145:20 - executed sequentially cloud spanner can
1145:22 - distribute and manage data at a global
1145:25 - scale and support globally consistent
1145:28 - reads along with strongly consistent
1145:30 - distributed transactions now being fully
1145:33 - managed cloud spanner handles any
1145:36 - replicas that are needed for
1145:38 - availability of your data and optimizes
1145:41 - performance by automatically sharding
1145:43 - the data based on request load and size
1145:46 - of the data part of why cloud spanner's
1145:48 - high availability is due to its
1145:51 - automatic synchronous data replication
1145:54 - between all replicas in independent
1145:57 - zones cloud spanner scales horizontally
1145:59 - automatically within regions but it can
1146:02 - also scale across regions for workloads
1146:05 - that have higher availability
1146:07 - requirements making data available
1146:10 - faster to users at a global scale along
1146:13 - with node redundancy quietly added for
1146:16 - every node deployed in the instance and
1146:19 - when you quickly add up all these
1146:21 - features of cloud spanner it's no wonder
1146:24 - that it's available to achieve five
1146:26 - nines availability on a multi-regional
1146:28 - instance and four nines availability on
1146:32 - a regional instance cloud spanner is
1146:34 - highly secure and offers data layer
1146:37 - encryption audit logging and iam
1146:40 - integration cloud spanner was designed
1146:42 - to fit the needs of specific industries
1146:45 - such as financial services
1146:47 - ad tech retail and global supply chain
1146:51 - along with gaming and pricing for cloud
1146:53 - spanner comes in at 90 cents per node
1146:56 - per hour with the cost of storage coming
1146:58 - in at 30 cents per gigabyte per month
1147:02 - definitely not cheap but the features
1147:04 - are plentiful now this isn't in the exam
1147:07 - but i did want to take a moment to dive
1147:09 - into the architecture for a bit more
1147:11 - context as to why this database is of a
1147:14 - different breed than the typical sql
1147:17 - database now to use cloud spanner you
1147:20 - must first create a cloud spanner
1147:22 - instance this instance is an allocation
1147:25 - of resources that is used by cloud
1147:28 - spanner databases created in that
1147:30 - instance instance creation includes two
1147:33 - important choices the instance
1147:35 - configuration and the node count and
1147:38 - these choices determine the location and
1147:40 - the amount of the instances cpu and
1147:43 - memory along with its storage resources
1147:46 - your configuration choice is permanent
1147:48 - for an instance and only the node count
1147:51 - can be changed later if needed an
1147:53 - instance configuration defines the
1147:56 - geographic placement and replication of
1147:58 - the database in that instance either
1148:01 - regional or multi-region and please note
1148:04 - that when you choose a multi-zone
1148:06 - configuration it allows you to replicate
1148:09 - the databases data not just in multiple
1148:12 - zones but in multiple zones across
1148:14 - multiple regions and when it comes to
1148:17 - the node count this determines the
1148:19 - number of nodes to allocate to that
1148:21 - instance these nodes allocate the amount
1148:23 - of cpu memory and storage needed for
1148:26 - your instance to either increase
1148:28 - throughput or storage capacity there is
1148:31 - no instance types to choose from like
1148:33 - cloud sql and so when you need more
1148:36 - power you simply add another node now
1148:38 - for any regional configuration cloud
1148:41 - spanner maintains exactly three read
1148:44 - write replicas each within a different
1148:46 - zone in that region each read write
1148:48 - replica contains a full copy of your
1148:51 - operational database that is able to
1148:54 - serve rewrite and read only requests
1148:57 - cloud spanner uses replicas in different
1148:59 - zones so that if a single zone failure
1149:02 - occurs your database remains available
1149:05 - in a multi-region instance configuration
1149:08 - the instance is allotted a combination
1149:10 - of four read write and read only
1149:12 - replicas and just as a note a three node
1149:16 - configuration minimum is what is
1149:18 - recommended for production by google and
1149:21 - as cloud spanner gets populated with
1149:23 - data
1149:24 - sharding happens which is also known as
1149:26 - a split and cloud spanner creates
1149:29 - replicas of each database split to
1149:31 - improve performance and availability all
1149:34 - of the data in a split is physically
1149:36 - stored together in a replica and cloud
1149:39 - spanner serves each replica out of an
1149:41 - independent failure zone and within each
1149:44 - replica set
1149:45 - one replica is elected to act as the
1149:48 - leader leader replicas are responsible
1149:50 - for handling rights while any read write
1149:54 - or read only replica can serve a read
1149:56 - request without communicating with the
1149:58 - leader and so this is the inner workings
1150:01 - of cloud spanner at a high level and not
1150:04 - meant to confuse you but to give you a
1150:06 - better context of how cloud spanner
1150:09 - although it is a relational sql database
1150:12 - is so different than its cloud sql
1150:14 - cousin now before ending this lesson i
1150:16 - wanted to touch on node performance for
1150:18 - a quick moment and so each cloud spanner
1150:21 - node can provide up to 10 000 queries
1150:24 - per second or qps of reads or 2000 qps
1150:29 - of writes each node provides up to two
1150:32 - terabytes of storage and so if you need
1150:35 - to scale up the serving and storage
1150:37 - resources in your instance you add more
1150:40 - nodes to that instance
1150:42 - and remember as noted earlier that
1150:44 - adding a node does not increase the
1150:46 - number of replicas but rather increases
1150:49 - the resources each replica has in the
1150:52 - instance adding nodes gives each replica
1150:55 - more cpu and ram which increases the
1150:58 - replicas throughput and so if you're
1151:00 - looking to scale up automatically you
1151:02 - can scale the numbers of nodes in your
1151:05 - instance based on the cloud monitoring
1151:07 - metrics on cpu or storage utilization
1151:11 - in conjunction with using cloud
1151:13 - functions to trigger and so when you are
1151:15 - deciding on a relational database that
1151:18 - provides global distribution and
1151:20 - horizontally scalable that handles
1151:23 - transactional workloads in google cloud
1151:25 - cloud spanner will always be the obvious
1151:28 - choice over cloud sql and so that's
1151:31 - pretty much all i have to cover when it
1151:33 - comes to this overview on cloud spanner
1151:35 - so you can now mark this lesson as
1151:37 - complete and let's move on to the next
1151:39 - one
1151:39 - [Music]
1151:43 - welcome back and in this lesson we will
1151:46 - be going over the available nosql
1151:48 - databases available in google cloud this
1151:52 - lesson is meant to be another overview
1151:54 - just to familiarize you with the nosql
1151:56 - database options as they show up in the
1151:59 - exam this lesson is not meant to go in
1152:02 - depth on databases but an overview and
1152:04 - will give you a good understanding on
1152:06 - what features are available for each
1152:09 - and their use cases so with that being
1152:11 - said let's dive in now there are four
1152:15 - managed nosql databases available in
1152:18 - google cloud and i will be briefly going
1152:20 - over them and i'll be starting this off
1152:22 - by discussing bigtable
1152:24 - now cloud bigtable is a fully managed
1152:27 - wide column nosql database designed for
1152:31 - terabyte and petabyte scale workloads
1152:34 - that offers low latency and high
1152:36 - throughput bigtable is built for
1152:38 - real-time application serving workloads
1152:41 - as well as large-scale analytical
1152:44 - workloads cloud bigtable is a regional
1152:47 - service and if using replication a copy
1152:50 - is stored in a different zone or region
1152:53 - for durability cloud bigtable is
1152:55 - designed for storing very large amounts
1152:58 - of single keyed data while still being
1153:01 - able to provide very low latency and
1153:04 - because throughput scales linearly you
1153:06 - can increase the queries per second by
1153:09 - adding more bigtable nodes when you need
1153:12 - them bigtable throughput can be
1153:14 - dynamically adjusted by adding or
1153:16 - removing cluster nodes without
1153:18 - restarting meaning you can increase the
1153:21 - size of a bigtable cluster for just a
1153:24 - few hours to handle a large load and
1153:26 - then reduce the cluster size again and
1153:28 - do it all without any downtime bigtable
1153:31 - is an ideal source
1153:33 - for map reduce operations and integrates
1153:36 - easily with all the existing big data
1153:38 - tools such as hadoop dataproc and
1153:42 - dataflow along with apache hbase and
1153:45 - when it comes to price bigtable is
1153:47 - definitely no joke pricing for bigtable
1153:50 - starts at 65 cents per hour per node
1153:53 - or over 450 dollars a month for a one
1153:57 - node configuration with no data now you
1153:59 - can use bigtable to store and query all
1154:02 - of the following types of data such as
1154:04 - cpu and memory usage over time for
1154:07 - multiple servers marketing data such as
1154:10 - purchase histories and customer
1154:11 - preferences financial data such as
1154:14 - transaction histories stock prices and
1154:17 - currency exchange rates iot data or
1154:20 - internet of things such as usage reports
1154:23 - from energy meters and home appliances
1154:25 - and lastly graph data such as
1154:28 - information about how users are
1154:30 - connected to one another cloud bigtable
1154:32 - excels as a storage engine as it can
1154:35 - batch mapreduce operations
1154:37 - stream processing or analytics as well
1154:40 - as being used for storage for machine
1154:42 - learning applications now moving on to
1154:45 - the next nosql database is cloud
1154:47 - datastore and cloud datastore is a
1154:50 - highly scalable nosql document database
1154:53 - built for automatic scaling high
1154:56 - performance and ease of application
1154:58 - development datastore is redundant
1155:00 - within your location
1155:02 - to minimize impact from points of
1155:04 - failures and therefore can offer high
1155:07 - availability of reads and rights cloud
1155:10 - datastore can execute atomic
1155:12 - transactions where a set of operations
1155:15 - either all succeed or none occur cloud
1155:18 - datastore uses a distributed
1155:20 - architecture to automatically manage
1155:23 - scaling so you never have to worry about
1155:25 - scaling manually as well what's very
1155:28 - unique about cloud datastore is that it
1155:30 - has a sql-like query language that's
1155:33 - available called gql also known as gql
1155:37 - gql maps roughly to sql however a sql
1155:41 - role column lookup is limited to a
1155:43 - single value whereas in gql a property
1155:46 - can be a multiple value property this
1155:49 - consistency model allows an application
1155:52 - to handle large amounts of data and
1155:54 - users while still being able to deliver
1155:57 - a great user experience data is
1155:59 - automatically encrypted before it is
1156:01 - written to disk and automatically
1156:03 - decrypted when read by an authorized
1156:06 - user now this does not reflect in the
1156:08 - exam as of yet and i will be updating
1156:11 - this lesson if and when it happens but
1156:14 - firestore is the newest version of
1156:16 - datastore and introduces several
1156:19 - improvements over datastore existing
1156:21 - datastore users can access these
1156:23 - improvements by creating a new firestore
1156:26 - database instance in datastore mode and
1156:29 - in the near future all existing
1156:31 - datastore databases will be
1156:33 - automatically upgraded to firestore in
1156:36 - datastore mode now moving right along
1156:39 - cloud datastore holds a really cool
1156:41 - feature for developers that's called
1156:44 - datastore emulator and this provides
1156:46 - local emulation of the production
1156:49 - datastore environment so that you can
1156:51 - use to develop and test your application
1156:54 - locally this is a component of the
1156:56 - google cloud sdks gcloud tool and can be
1157:00 - installed by using the gcloud components
1157:02 - install command that we discussed
1157:04 - earlier on in the course and so moving
1157:06 - on to use cases for datastore
1157:09 - it is ideal for applications that rely
1157:12 - on highly available structured data at
1157:14 - scale you can use datastore for things
1157:17 - like product catalogs that provide
1157:19 - real-time inventory and product details
1157:22 - for a retailer user profiles that
1157:25 - deliver a customized experience based on
1157:28 - the user's past activities and
1157:30 - preferences
1157:31 - as well as transactions based on asset
1157:34 - properties for example transferring
1157:36 - funds from one bank account to another
1157:39 - next up we have firestore for firebase
1157:43 - and so this is a flexible scalable nosql
1157:46 - cloud database to store and sync data
1157:49 - for client and server side development
1157:51 - and is available for native c plus unity
1157:55 - node.js java go and python sdks
1158:00 - in addition to rest and rpc apis pretty
1158:03 - much covering the gamut of most major
1158:05 - programming languages now with cloud
1158:07 - firestore you store data in documents
1158:11 - that contain fields mapping to values
1158:14 - these documents are stored in
1158:15 - collections which are containers for
1158:18 - your documents that you can use to
1158:20 - organize your data and build queries
1158:23 - documents support many different data
1158:25 - types as well you can also create sub
1158:28 - collections within documents and build
1158:31 - hierarchical data structures cloud
1158:33 - firestore is serverless with absolutely
1158:36 - no servers to manage update or maintain
1158:39 - and with automatic multi-region
1158:41 - replication and strong consistency
1158:44 - google is able to hold
1158:46 - a five nines availability guarantee and
1158:49 - so when it comes to querying in cloud
1158:51 - firestore it is expressive efficient and
1158:55 - flexible you can create shallow queries
1158:57 - to retrieve data at the document level
1159:00 - without needing to retrieve the entire
1159:02 - collection or any nested subcollections
1159:05 - cloud firestore uses data
1159:07 - synchronization to update data in real
1159:10 - time for any connected device as well it
1159:14 - also caches data that your application
1159:16 - is actively using so that the
1159:18 - application can write read listen to and
1159:22 - query data even if the device is offline
1159:25 - when the device comes back online cloud
1159:28 - firestore synchronizes any local changes
1159:31 - back to cloud firestore you can also
1159:33 - secure your data in cloud firestore with
1159:36 - firebase authentication and cloud
1159:38 - firestore security rules for android ios
1159:42 - and javascript or you can use iam for
1159:45 - server side languages and when it comes
1159:47 - to costs firestore falls into the always
1159:51 - available free tier where you can use
1159:54 - one database holding five gigabytes or
1159:57 - if you need more you can move into their
1159:59 - paid option now firebase also has
1160:02 - another database sharing similar
1160:04 - features like having no servers to
1160:06 - deploy and maintain real-time updates
1160:09 - along with the free tier in this
1160:11 - database is called real time database
1160:14 - and is used for more basic querying
1160:16 - simple data structure and keeping things
1160:19 - to one database
1160:21 - it's something i like to call firestore
1160:23 - lite real time database does not show up
1160:26 - in the exam but i wanted to bring it to
1160:28 - light as it is part of the firebase
1160:30 - family just know that you can use both
1160:32 - databases within the same firebase
1160:35 - application or project as both can store
1160:37 - the same types of data client libraries
1160:40 - work in a similar manner and both hold
1160:43 - real-time updates now although firebase
1160:46 - is a development platform and not a
1160:48 - database service i wanted to give it a
1160:50 - quick mention for those of you who are
1160:52 - unfamiliar with the tie-in to firestore
1160:55 - with firebase firebase is a mobile
1160:58 - application development platform that
1161:00 - provides tools and cloud services to
1161:03 - help enable developers to develop
1161:05 - applications faster and more easily and
1161:08 - since it ties in nicely with firestore
1161:10 - it becomes the perfect platform for
1161:13 - mobile application development okay so
1161:16 - moving on to our last nosql database is
1161:19 - memorystore and memorystore is a fully
1161:21 - managed service from google cloud for
1161:23 - either redis or memcached in memory
1161:27 - datastore to build application caches
1161:30 - and this is a common service used in
1161:32 - many production environments
1161:34 - specifically when the need for caching
1161:36 - arises memory store automates the
1161:38 - administration tasks for redis and
1161:41 - memcached like enabling high
1161:43 - availability failover patching and
1161:46 - monitoring so you don't have to and when
1161:48 - it comes to memory store for redis
1161:50 - instances in the standard tier these are
1161:52 - replicated across zones
1161:55 - monitored for health and have fast
1161:57 - automatic failover standard tier
1162:00 - instances also provide an sla of three
1162:03 - nines availability memory store for
1162:06 - redis also provides the ability to scale
1162:09 - instant sizes seamlessly so that you can
1162:12 - start small and increase the size of the
1162:14 - instance as needed memory store is
1162:16 - protected from the internet using vpc
1162:19 - networks and private ip and also comes
1162:22 - with iam integration systems are
1162:24 - monitored around the clock ensuring that
1162:27 - your data is protected at all times and
1162:30 - know that the versions are always kept
1162:32 - up to date with the latest critical
1162:34 - patches ensuring your instances are
1162:37 - secure now when it comes to use cases of
1162:40 - course the first thing you will see is
1162:42 - caching and this is the main reason to
1162:44 - use memory store as it provides low
1162:47 - latency access and high throughput for
1162:50 - heavily accessed data compared to
1162:52 - accessing the data from a disk common
1162:54 - examples of caching is session
1162:56 - management frequently accessed queries
1162:58 - scripts or pages so when using memory
1163:01 - store for leaderboards and gaming this
1163:03 - is a common use case in the gaming
1163:05 - industry as well as using it for player
1163:08 - profiles memory store is also a perfect
1163:10 - solution for stream processing combined
1163:13 - with data flow memory store for redis
1163:16 - provides a scalable fast in memory store
1163:19 - for storing intermediate data that
1163:22 - thousands of clients can access with
1163:24 - very low latency and so when it comes to
1163:26 - nosql databases these are all the
1163:29 - available options on google cloud and as
1163:32 - i said before it will only show up on
1163:34 - the exam at merely a high level and so
1163:37 - knowing what each of these databases are
1163:39 - used for
1163:40 - will be a huge benefit along with being
1163:43 - an entry to diving deeper into possibly
1163:46 - using these services within your
1163:48 - day-to-day job as a cloud engineer and
1163:50 - so that's pretty much all i wanted to
1163:52 - cover when it comes to nosql databases
1163:55 - available in google cloud so you can now
1163:58 - mark this lesson as complete and let's
1164:00 - move on to the next one
1164:01 - [Music]
1164:05 - welcome back and in this lesson we'll be
1164:08 - going over the big data ecosystem in an
1164:11 - overview just to familiarize you with
1164:14 - the services that are available in
1164:16 - google cloud and are the services that
1164:18 - will show up in the exam this lesson is
1164:21 - not meant to go in depth but is an
1164:24 - overview and will give you a good
1164:26 - understanding on what these services can
1164:28 - do and how they all work together to
1164:31 - make sense of big data as a whole
1164:33 - so getting right into it i wanted to
1164:36 - first ask the question what is big data
1164:38 - i mean many people talk about it but
1164:40 - what is it really well big data refers
1164:43 - to massive amounts of data that would
1164:46 - typically be too expensive to store
1164:48 - manage and analyze using traditional
1164:51 - database systems either relational or
1164:54 - monolithic as the amount of data that we
1164:56 - have been seeing over the past few years
1164:59 - has started to increase these systems
1165:01 - have become very inefficient because of
1165:04 - their lack of flexibility for storing
1165:07 - unstructured data such as images text or
1165:10 - video as well as accommodating high
1165:13 - velocity or real-time data or scaling to
1165:16 - support very large
1165:18 - petabyte scale data volumes for this
1165:21 - reason the past few years has seen the
1165:23 - mainstream adoption of new approaches to
1165:26 - managing and processing big data
1165:29 - including apache hadoop and nosql
1165:32 - database systems however those options
1165:35 - often prove to be complex to deploy
1165:38 - manage and use in an on-premises
1165:40 - situation
1165:42 - now the ability to consistently get
1165:44 - business value from data
1165:46 - fast and efficiently is now becoming the
1165:49 - de facto of successful organizations
1165:52 - across every industry the more data a
1165:54 - company has access to the more business
1165:57 - insights and business value they're able
1165:59 - to achieve
1166:00 - like gain useful insights increase
1166:03 - revenue
1166:04 - get or retain customers and even improve
1166:07 - operations and because machine learning
1166:09 - models get more efficient as they are
1166:11 - trained with more data machine learning
1166:14 - and big data are highly complementary
1166:17 - all in all big data brings some really
1166:19 - great value to the table that is
1166:22 - impossible for any organization to turn
1166:24 - down and so now that we've gone through
1166:26 - that overview of what big data is i
1166:29 - wanted to dive into some shorter
1166:31 - overviews of the services available for
1166:34 - the big data ecosystem on google cloud
1166:37 - and so the first service that i'd like
1166:39 - to start with is bigquery now bigquery
1166:42 - is a fully managed serverless data
1166:44 - warehouse that enables scalable analysis
1166:48 - over petabytes of data this service
1166:51 - supports querying using sql and holds
1166:54 - built-in machine learning capabilities
1166:56 - you start by ingesting data into
1166:58 - bigquery and then you are able to take
1167:00 - advantage of all the power it provides
1167:03 - so big data would ingest that data by
1167:05 - doing a batch upload or by streaming it
1167:08 - in real time and you can use any of the
1167:11 - currently available google cloud
1167:13 - services to load data into bigquery you
1167:16 - can take a manual batch ingestion
1167:18 - approach
1167:19 - or stream using pub sub etl data and
1167:22 - with bigquery data transfer service you
1167:24 - can automatically transfer data from
1167:27 - external google data sources and partner
1167:30 - sas applications to bigquery on a
1167:33 - scheduled and fully managed basis and
1167:36 - the best part is batch and export is
1167:38 - free bigquery's high-speed streaming api
1167:42 - provides an incredible foundation for
1167:44 - real-time analytics making business data
1167:47 - immediately available for analysis and
1167:50 - you can also leverage pub sub and data
1167:52 - flow to stream data into bigquery
1167:56 - bigquery transparently and automatically
1167:59 - provides highly durable replicated
1168:01 - storage in multiple locations for high
1168:04 - availability as well as being able to
1168:07 - achieve easy resource bigquery keeps a
1168:10 - seven day history of changes in case
1168:13 - something were to go wrong bigquery
1168:15 - supports standard sql querying which
1168:18 - reduces the need for code rewrites you
1168:20 - can simply use it as you would for
1168:22 - querying any other sql compliant
1168:25 - database and with dataproc and dataflow
1168:28 - bigquery provides integration with the
1168:31 - apache big data ecosystem allowing
1168:34 - existing hadoop spark and beam workloads
1168:37 - to read or write data directly from
1168:40 - bigquery using the storage api bigquery
1168:43 - also makes it very easy to access this
1168:45 - data by using the cloud console using
1168:48 - the bq command line tool or making calls
1168:51 - to the bigquery rest api using a variety
1168:55 - of client libraries such as java.net or
1168:58 - python there are also a variety of
1169:01 - third-party tools that you can use to
1169:03 - interact with bigquery when visualizing
1169:06 - the data or loading the data bigquery
1169:09 - provides strong security and governance
1169:11 - controls with fine-grained controls
1169:14 - through integration with identity and
1169:16 - access management bigquery gives you the
1169:18 - option of geographic data control
1169:21 - without the headaches of setting up and
1169:23 - managing clusters and other computing
1169:26 - resources in different zones and regions
1169:29 - bigquery also provides fine grain
1169:31 - identity and access management and rest
1169:33 - assured that your data is always
1169:35 - encrypted at rest and in transit now the
1169:38 - way that bigquery calculates billing
1169:41 - charges is by queries and by storage
1169:44 - storing data in bigquery is comparable
1169:46 - in price with storing data in cloud
1169:48 - storage which makes it an easy decision
1169:51 - for storing data in bigquery there is no
1169:54 - upper limit to the amount of data that
1169:55 - can be stored in bigquery so if tables
1169:58 - are not edited for 90 days the price of
1170:01 - storage for that table drops by 50
1170:04 - percent query costs are also available
1170:06 - as on-demand and flat rate pricing and
1170:09 - when it comes to on-demand pricing you
1170:12 - are only charged for bytes read not
1170:15 - bytes returned in the end bigquery
1170:18 - scales seamlessly to store and analyze
1170:21 - petabytes to exabytes of data with ease
1170:24 - now there are so many more features to
1170:26 - list but if you are interested feel free
1170:29 - to dive into the other features with the
1170:31 - supplied link in the lesson text now
1170:34 - moving on to the next service is pub sub
1170:37 - and pub sub is a fully managed real-time
1170:40 - messaging service that allows you to
1170:42 - send and receive messages between
1170:45 - independent applications it acts as
1170:48 - messaging oriented middleware or event
1170:51 - ingestion and delivery for streaming
1170:54 - analytics pipelines and so a publisher
1170:57 - application creates and send messages to
1171:00 - a topic subscriber applications create a
1171:03 - subscription to a topic and receives
1171:05 - messages from it and so i wanted to take
1171:07 - a moment to show you exactly how it
1171:09 - works
1171:10 - so first the publisher creates messages
1171:13 - and sends them to the messaging service
1171:15 - on a specified topic a topic is a named
1171:18 - entity that represents a feed of
1171:21 - messages a publisher application creates
1171:24 - a topic in the pub sub service and sends
1171:27 - messages to that topic a message
1171:29 - contains a payload and optional
1171:31 - attributes that describe the content the
1171:34 - service as a whole ensures that
1171:36 - published messages are retained on
1171:38 - behalf of subscriptions and so a
1171:41 - published message is retained for a
1171:43 - subscription in a message queue shown
1171:46 - here as message storage until it is
1171:49 - acknowledged by any subscriber consuming
1171:51 - messages from that subscription pub sub
1171:54 - then forwards messages from a topic to
1171:57 - all of its subscriptions individually a
1172:00 - subscriber then receives messages either
1172:03 - by pub sub pushing them to the
1172:05 - subscriber's chosen endpoint or by the
1172:07 - subscriber pulling them from the service
1172:10 - the subscriber then sends an
1172:12 - acknowledgement to the pub sub service
1172:15 - for each received message the service
1172:17 - then removes acknowledged messages from
1172:20 - the subscriptions message queue and some
1172:22 - of the use cases for pub sub is
1172:25 - balancing large task queues distributing
1172:28 - event notifications and real-time data
1172:31 - streaming from various sources and so
1172:33 - the next service that i wanted to get
1172:35 - into is composer now composer is a
1172:38 - managed workflow orchestration service
1172:41 - that is built on apache airflow this is
1172:43 - a workflow automation tool for
1172:46 - developers that's based on the open
1172:48 - source apache airflow project similar to
1172:51 - an on-premises deployment cloud composer
1172:54 - deploys multiple components to run
1172:57 - airflow in the cloud airflow is a
1172:59 - platform
1173:00 - created by the community to
1173:02 - programmatically author schedule and
1173:05 - monitor workflows the airflow scheduler
1173:07 - as you see here executes the tasks on an
1173:11 - array of workers while following the
1173:13 - specified dependencies and storing the
1173:16 - data in a database and having a ui
1173:19 - component for easy management now
1173:21 - breaking down these workflows for just a
1173:23 - sec in data analytics a workflow
1173:27 - represents a series of tasks for
1173:30 - ingesting transforming analyzing or
1173:33 - utilizing data in airflow workflows are
1173:36 - created using dags which are a
1173:39 - collection of tasks that you want to
1173:41 - schedule and run
1173:43 - and organizes these tasks to ensure that
1173:46 - each task is executed at the right time
1173:49 - in the right order or with the right
1173:52 - issue handling now in order to run the
1173:54 - specialized workflows
1173:56 - provision environments are needed and so
1173:58 - composer deploys these self-contained
1174:01 - environments on google kubernetes engine
1174:04 - that work with other google cloud
1174:06 - services using connectors built into
1174:09 - airflow the beauty of composer is that
1174:12 - you can create one or more of these
1174:14 - environments in a single google cloud
1174:16 - project using any supported region
1174:20 - without having to do all the heavy
1174:22 - lifting of creating a full-blown apache
1174:25 - airflow environment now when it comes to
1174:27 - data flow dataflow is a serverless fully
1174:31 - managed processing service for executing
1174:34 - apache beam pipelines for batch and
1174:37 - real-time data streaming the apache beam
1174:39 - sdk is an open source programming model
1174:43 - that enables you to develop both batch
1174:46 - and streaming pipelines using one of the
1174:48 - apache beam sdks you build a program
1174:51 - that defines the pipeline then one of
1174:54 - apache beam's supported distributed
1174:56 - processing back-ends such as data flow
1174:59 - executes that pipeline the data flow
1175:01 - service then takes care of all the
1175:03 - low-level details like coordinating
1175:06 - individual workers sharding data sets
1175:09 - auto scaling and exactly once processing
1175:12 - now in its simplest form google cloud
1175:15 - data flow reads the data from a source
1175:18 - transforms it and then writes the data
1175:21 - back to a sink now getting a bit more
1175:23 - granular with how this pipeline works
1175:26 - data flow reads the data presented from
1175:29 - a data source
1175:30 - once the data has been read it is put
1175:32 - together into a collection of data sets
1175:35 - called a p collection and this allows
1175:38 - the data to be read distributed and
1175:40 - processed across multiple machines now
1175:43 - at each step in which the data is
1175:45 - transformed a new p collection is
1175:48 - created and once the final collection
1175:50 - has been created it is written to async
1175:53 - and this is the full pipeline of how
1175:55 - data goes from source to sync this
1175:58 - pipeline within data flow is called a
1176:01 - job and finally here is a high-level
1176:03 - overview of what a data flow job would
1176:06 - look like when you involve other
1176:08 - services within google cloud and put
1176:10 - together in an end-to-end solution from
1176:13 - retrieving the data to visualizing it
1176:16 - and finally when it comes to pricing
1176:18 - data flow jobs are billed in per second
1176:21 - increments so you're only charged for
1176:23 - when you are processing your data now
1176:26 - moving on to data proc this is a fast
1176:28 - and easy way to run spark hadoop hive or
1176:33 - pig on google cloud in an on-premises
1176:36 - environment it takes 5 to 30 minutes to
1176:39 - create spark and hadoop clusters data
1176:42 - proc clusters take 90 seconds or less on
1176:45 - average to be built in google cloud
1176:48 - dataproc has built-in integration with
1176:51 - other google cloud platform services and
1176:53 - use spark and hadoop clusters without
1176:56 - any admin assistance so when you're done
1176:58 - with the cluster you can simply turn it
1177:00 - off so you don't spend money on an idle
1177:03 - cluster as well there's no need to worry
1177:06 - about data loss because data proc is
1177:08 - integrated with cloud storage bigquery
1177:11 - and cloud bigtable the great thing about
1177:14 - dataproc is you don't need to learn new
1177:16 - tools or apis to use it
1177:18 - spark hadoop pig and hive are all
1177:22 - supported and frequently updated and
1177:24 - when it comes to pricing you are billed
1177:26 - at one cent per vcpu in your cluster per
1177:30 - hour on top of the other resources you
1177:33 - use you also have the flexibility of
1177:36 - using preemptable instances for even
1177:38 - lower compute cost now although cloud
1177:41 - data proc and cloud data flow can both
1177:44 - be used to implement etl data
1177:46 - warehousing solutions they each have
1177:49 - their strengths and weaknesses and so i
1177:52 - wanted to take a quick moment to point
1177:53 - them out now with dataproc you can
1177:56 - easily spin up clusters through the
1177:57 - console the sdk or the api and turn it
1178:01 - off when you don't need it with dataflow
1178:04 - it is serverless and fully managed so
1178:07 - there are never any servers to worry
1178:09 - about and when it comes to having any
1178:11 - dependencies to tools in the hadoop or
1178:13 - spark ecosystem data proc would be the
1178:16 - way to go but if you're looking to make
1178:18 - your jobs more portable across different
1178:21 - execution engines apache beam allows you
1178:24 - to do this and is only available on data
1178:27 - flow moving on to the next service is
1178:29 - cloud data lab now cloud data lab is an
1178:32 - interactive developer tool created to
1178:35 - explore analyze transform and visualize
1178:39 - data and build machine learning models
1178:42 - from your data data lab uses open
1178:44 - sourced jupyter notebooks a well-known
1178:47 - format used in the world of data science
1178:49 - it runs on compute engine and connects
1178:52 - to multiple cloud services easily so you
1178:55 - can focus on your data science tasks it
1178:58 - also integrates with all of the google
1179:00 - services that help you simplify data
1179:03 - processing like bigquery and cloud
1179:06 - storage cloud data lab is packaged as a
1179:08 - container and run in a vm instance cloud
1179:11 - data lab uses notebooks instead of text
1179:14 - files containing code notebooks bring
1179:17 - together code documentation written as
1179:19 - markdown and the results of code
1179:21 - execution whether it's text image or
1179:25 - html or javascript like a code editor or
1179:28 - ide notebooks help you write code and
1179:32 - they allow you to execute code in an
1179:34 - interactive and iterative manner
1179:37 - rendering the results alongside the code
1179:39 - cloud data lab notebooks can be stored
1179:42 - in google cloud source repository this
1179:44 - git repository is cloned onto persistent
1179:47 - disk when attached to the vm now when it
1179:50 - comes to prepping your data before
1179:52 - consumption whether it be data cleansing
1179:55 - cleaning prepping or alteration this is
1179:58 - where data prep hits it out of the park
1180:01 - dataprep is a serverless intelligent
1180:03 - data service for visually exploring
1180:06 - cleaning and preparing structured and
1180:09 - unstructured data for analysis reporting
1180:12 - and machine learning it automatically
1180:14 - detects schemas data types possible
1180:18 - joins and anomalies such as missing
1180:20 - values outliers and duplicates so you
1180:23 - don't have to the architecture that i'm
1180:25 - about to show you is how data prep
1180:28 - shines the raw data that's available
1180:30 - from various different sources is
1180:32 - ingested into cloud data prep to clean
1180:36 - and prepare the data data prep then
1180:38 - sends the data off to cloud data flow to
1180:40 - refine that data and then sent off to
1180:43 - cloud storage or bigquery for storage
1180:46 - before being analyzed by one of the many
1180:49 - available bi tools now these big data
1180:52 - services are used by many data analysts
1180:54 - in the field and it's great to know what
1180:57 - services that can be used to help
1180:59 - process the data needed for their
1181:01 - specific job as well for the exam you
1181:04 - only need to know these services at a
1181:06 - high level and not to know them in depth
1181:09 - but if you seem interested in diving
1181:11 - into any of these services to know more
1181:14 - about them i highly encourage you to
1181:16 - dive in after the course and really take
1181:19 - a look at them and that's pretty much
1181:21 - all i have to cover in this lesson on
1181:23 - the services that are available for the
1181:25 - big data ecosystem in google cloud so
1181:28 - you can now mark this lesson as complete
1181:30 - and let's move on to the next one
1181:32 - [Music]
1181:36 - welcome back
1181:37 - this lesson is going to be based on the
1181:40 - foundation of machine learning i'm going
1181:43 - to go over what machine learning is what
1181:45 - it can do for us the machine learning
1181:48 - ecosystem on google cloud
1181:50 - and hopefully answer any questions along
1181:52 - the way this lesson will be a high level
1181:55 - overview of the services available on
1181:58 - google cloud yet these services that are
1182:00 - available are a need to know as they
1182:03 - come up in the exam and hopefully will
1182:06 - give you some really cool ideas on the
1182:09 - possibilities of building something
1182:11 - truly fantastic on google cloud so what
1182:15 - is machine learning
1182:17 - well machine learning is functionality
1182:20 - that helps enable software to perform
1182:23 - tasks without any explicit programming
1182:26 - or rules traditionally considered a
1182:28 - subcategory of artificial intelligence
1182:31 - machine learning involves statistical
1182:34 - techniques such as deep learning also
1182:36 - known as neural networks that are
1182:39 - inspired by theories about how the human
1182:42 - brain processes information it is
1182:44 - trained to recognize patterns in
1182:47 - collected data using algorithmic models
1182:50 - and this collected data includes video
1182:53 - images speech or text and because
1182:56 - machine learning is very expensive to
1182:59 - run on-premises
1183:01 - is an efficient place
1183:03 - for machine learning due to the use of
1183:06 - massive computation at scale
1183:08 - and as explained before machine learning
1183:11 - is always better with big data so now i
1183:14 - wanted to touch on what can machine
1183:16 - learning do for us
1183:18 - well it can categorize images such as
1183:21 - photos faces or satellite imagery
1183:24 - it can look for keywords in text
1183:26 - documents or emails
1183:28 - it can flag potentially fraudulent
1183:31 - transactions when it comes to credit
1183:33 - cards or debit cards it can enable
1183:35 - software to respond accurately to voice
1183:38 - commands it can also translate languages
1183:41 - in text or audio and these are just some
1183:44 - of the common functions that machine
1183:46 - learning can do for us so getting into
1183:48 - google's machine learning platform
1183:50 - itself machine learning has been a
1183:53 - cornerstone of google's internal systems
1183:56 - for years primarily because their need
1183:59 - to automate data-driven systems on a
1184:02 - massive scale
1184:04 - and doing this has provided unique
1184:06 - insight into the right techniques
1184:09 - infrastructure and frameworks that help
1184:12 - their customers get optimal value out of
1184:14 - machine learning the originally
1184:16 - developed open source framework for use
1184:19 - inside of google
1184:20 - called tensorflow
1184:22 - is now the standard in the data science
1184:24 - community in addition to heavily
1184:27 - contributing to the academic and open
1184:30 - source communities
1184:31 - google's machine learning researchers
1184:34 - helped bring that functionality into
1184:36 - google products such as g suite search
1184:40 - and photos in addition to google's
1184:43 - internal operations when it comes to
1184:46 - data center automation
1184:48 - now here is an overview of all the
1184:50 - machine learning services that we will
1184:52 - be covering and that you will need to
1184:54 - know
1184:55 - only at a high level for the exam and
1184:58 - we'll start off with the site api
1185:00 - services
1185:02 - starting with the vision api
1185:04 - the vision api offers powerful
1185:06 - pre-trained machine learning models
1185:09 - that allow you to assign labels to
1185:12 - images
1185:13 - and quickly classify them into millions
1185:16 - of pre-defined categories
1185:18 - vision api
1185:19 - can read printed and handwritten text it
1185:23 - can detect objects and faces
1185:25 - and build metadata into an image catalog
1185:28 - of your choice now when it comes to
1185:30 - video intelligence
1185:32 - it has pre-trained machine learning
1185:35 - models that automatically recognizes
1185:38 - more than 20 000 objects
1185:41 - places and actions in stored and
1185:45 - streaming video you can gain insights
1185:47 - from video in near real time using the
1185:50 - video intelligence streaming video apis
1185:54 - and trigger events based on objects
1185:56 - detected you can easily search a video
1185:58 - catalog the same way you search text
1186:00 - documents and extract metadata that can
1186:04 - be used to index organize and search
1186:07 - video content
1186:09 - now moving on to the language apis
1186:12 - we start off with the natural language
1186:14 - api and this uses machine learning to
1186:17 - reveal the structure and meaning of text
1186:20 - you can extract information about people
1186:23 - places and events
1186:25 - and better understand social media
1186:27 - sentiment and customer conversations
1186:30 - natural language enables you to analyze
1186:33 - text and also integrate it with your
1186:36 - document storage on cloud storage now
1186:39 - with the translation api it enables you
1186:42 - to dynamically translate between
1186:44 - languages using google's pre-trained or
1186:48 - custom machine learning models
1186:50 - translation api
1186:52 - instantly translates text into more than
1186:55 - 100 languages for your website and apps
1186:59 - with optional customization features
1187:03 - following another grouping of machine
1187:04 - learning is the conversation apis first
1187:08 - up we have dialog flow dialog flow is a
1187:11 - natural language understanding platform
1187:14 - that makes it easy to design and
1187:17 - integrate a conversational user
1187:19 - interface into your application or
1187:21 - device it could be a mobile app a web
1187:24 - application a bot or an interactive
1187:27 - voice response system using dialogflow
1187:30 - you can provide new and engaging ways
1187:33 - for users to interact with your product
1187:36 - dialogflow can analyze multiple types of
1187:38 - input from your customers
1187:40 - including text or audio inputs
1187:43 - like from a phone or voice recording and
1187:45 - it can also respond to your customers in
1187:48 - a couple of ways either through text or
1187:51 - with synthetic speech now with the
1187:53 - speech-to-text api this api accurately
1187:57 - converts speech into text it can
1188:00 - transcribe content with accurate
1188:02 - captions and deliver better user
1188:05 - experience in products through voice
1188:08 - commands going the other way from text
1188:10 - to speech this api enables developers to
1188:14 - synthesize natural sounding speech with
1188:17 - over a hundred different voices
1188:19 - available in multiple languages and
1188:22 - variants text to speech
1188:24 - allows you to create lifelike
1188:26 - interactions with their users across
1188:29 - many applications and devices and to
1188:32 - finish off our machine learning segment
1188:35 - i wanted to touch on auto ml automl is a
1188:39 - suite of machine learning products that
1188:41 - enables developers with very limited
1188:44 - machine learning expertise
1188:46 - to train high quality models specific to
1188:50 - their business needs in other words
1188:52 - using automl allows making deep learning
1188:55 - easier to use and relies on google's
1188:58 - state-of-the-art transfer learning and
1189:00 - neural architecture search technology so
1189:03 - you can now generate high quality
1189:04 - training data and be able to deploy new
1189:07 - models based on your data in minutes
1189:10 - automl is available for vision
1189:14 - video intelligence translation
1189:16 - natural language tables
1189:19 - inference and recommendation apis
1189:23 - now i know this has been a lot to cover
1189:26 - for this machine learning lesson and the
1189:28 - ecosystem around it but is a necessity
1189:31 - for the exam and will also help you
1189:34 - build really cool products when it comes
1189:37 - to your role as an engineer again all
1189:40 - the services that i have discussed in
1189:42 - this lesson should be known at a high
1189:45 - level only although my recommendation
1189:48 - would be to dive deeper into these
1189:50 - services by checking out the links in
1189:53 - the lesson text below and having some
1189:55 - fun with these products getting to know
1189:58 - these services will really help up your
1190:01 - game when it comes to getting to know
1190:03 - these services a little bit more in
1190:05 - depth and will really help you gain more
1190:08 - momentum when it comes to building any
1190:10 - applications or applying them to any
1190:13 - currently running applications i
1190:16 - personally found it extremely valuable
1190:18 - and really cemented my knowledge when it
1190:21 - came to machine learning i also had a
1190:23 - ton of fun doing it and so that's all i
1190:26 - have for this lesson on machine learning
1190:28 - so you can now mark this lesson as
1190:30 - complete and let's move on to the next
1190:32 - one
1190:33 - [Music]
1190:37 - welcome back and in this lesson we'll be
1190:40 - diving into a suite of tools used on the
1190:43 - google cloud platform that allow you to
1190:45 - operate monitor and troubleshoot your
1190:48 - environment known as operation suite and
1190:51 - previously known as stackdriver this
1190:53 - lesson will be mostly conceptual and
1190:55 - gear more towards what the suite of
1190:58 - tools do as it plays a big part not only
1191:01 - in the exam but for the needs of gaining
1191:04 - insight from all the resources that
1191:06 - exist in your environment now there are
1191:08 - a few tools to cover here so with that
1191:11 - being said let's dive in
1191:13 - now the operation suite is a suite of
1191:15 - tools for logging monitoring and
1191:18 - application diagnostics
1191:20 - operation suite ingests this data and
1191:23 - generates insights using dashboards
1191:26 - charts and alerts this suite of tools
1191:30 - are available for both gcp and aws you
1191:34 - can connect to aws using an aws role and
1191:38 - a gcp service account you can also
1191:41 - monitor vms with specific agents that
1191:44 - again both run on gcp for compute engine
1191:48 - and aws ec2 operation suite also allows
1191:52 - the added functionality of monitoring
1191:54 - any applications that's running on those
1191:57 - vms operation suite is also available
1192:00 - for any on-premises infrastructure or
1192:03 - hybrid cloud environments operation
1192:06 - suite has a native integration within
1192:09 - gcp out of the box so there's no real
1192:12 - configurations that you need to do and
1192:14 - integrates with almost all the resources
1192:17 - on google cloud such as the previously
1192:20 - mentioned compute engine gke app engine
1192:24 - and bigquery and you can find and fix
1192:26 - issues faster due to the many different
1192:29 - tools an operation suite can reduce
1192:32 - downtime with real-time alerting you can
1192:35 - also find support from a growing partner
1192:38 - ecosystem of technology integration
1192:40 - tools to expand your operations security
1192:44 - and compliance capabilities now the
1192:47 - operation suite comprises of six
1192:50 - available products that covers the gamut
1192:53 - of all the available tools you will need
1192:55 - that allows you to monitor troubleshoot
1192:58 - and improve application performance on
1193:00 - your google cloud environment and i will
1193:03 - be going over these products in a bit of
1193:05 - detail starting with monitoring now
1193:08 - cloud monitoring collects measurements
1193:10 - or metrics to help you understand how
1193:13 - your applications and system services
1193:16 - are performing giving you the
1193:18 - information about the source of the
1193:20 - measurements time stamped values and
1193:23 - information of those values that can be
1193:26 - broken down through time series data
1193:28 - cloud monitoring can then take the data
1193:31 - provided and use pre-defined dashboards
1193:35 - that require no setup or configuration
1193:38 - effort cloud monitoring also gives you
1193:40 - the flexibility to create custom
1193:43 - dashboards that display the content you
1193:45 - select you can use the widgets available
1193:48 - or you can install a dashboard
1193:50 - configuration that is stored in github
1193:52 - now in order for you to start using
1193:54 - cloud monitoring you need to configure a
1193:57 - workspace
1193:58 - now workspaces organize monitoring
1194:01 - information in cloud monitoring this is
1194:03 - a single pane of glass where you can
1194:06 - view everything that you're monitoring
1194:08 - in your environment it is also best
1194:10 - practice to use a multi-project
1194:12 - workspace so you can monitor multiple
1194:15 - projects from a single pane of glass now
1194:18 - as i mentioned earlier cloud monitoring
1194:21 - has an agent and this gathers system and
1194:23 - application metrics from your vm and
1194:26 - sends them to cloud monitoring you can
1194:28 - monitor your vms without the agent but
1194:31 - you will only get specific metrics such
1194:33 - as cpu
1194:34 - disk traffic network traffic and uptime
1194:38 - using the agent is optional but is
1194:40 - recommended by google and with the agent
1194:43 - it allows you to monitor many
1194:45 - third-party applications and just as a
1194:47 - note cloud logging has an agent as well
1194:51 - and works well together with cloud
1194:53 - monitoring to create visualize and alert
1194:57 - on metrics based on log data but more on
1195:00 - that a little bit later cloud monitoring
1195:02 - is also available for gke and this will
1195:05 - allow you to monitor your clusters as it
1195:08 - manages the monitoring and logging
1195:10 - together and this will monitor clusters
1195:12 - infrastructure its workloads and
1195:15 - services as well as your nodes pods and
1195:18 - containers so when it comes to alerting
1195:21 - this is defined by policies and
1195:24 - conditions so an a learning policy
1195:26 - defines the conditions under which a
1195:29 - service is considered unhealthy when
1195:31 - these conditions are met the policy is
1195:33 - triggered and it opens a new incident
1195:36 - and sends off a notification a policy
1195:38 - belongs to an individual workspace and
1195:41 - each workspace can contain up to 500
1195:43 - policies now conditions determine when
1195:46 - an alerting policy is triggered so all
1195:49 - conditions watch for three separate
1195:51 - things the first one is a metric the
1195:53 - second one is a behavior in some way and
1195:55 - the third one is for a period of time
1195:58 - describing a condition includes a metric
1196:01 - to be measured and a test for
1196:03 - determining when the metric reaches a
1196:05 - state that you want to know about so
1196:07 - when an alert is triggered you could be
1196:09 - notified using notification channels
1196:12 - such as email
1196:13 - sms
1196:14 - as well as third party tools such as
1196:17 - pagerduty and slack now moving on to
1196:19 - cloud logging cloud logging is a central
1196:22 - repository for log data from multiple
1196:25 - sources and as described earlier logging
1196:29 - can come not just from google but with
1196:32 - aws as well as on-premises environments
1196:35 - cloud logging handles real-time log
1196:37 - management and analysis and has tight
1196:41 - integration with cloud monitoring it
1196:43 - collects platform system and application
1196:47 - logs and you also have the option of
1196:49 - exporting logs to other sources such as
1196:52 - long-term storage like cloud storage or
1196:55 - for analysis like bigquery you can also
1196:58 - export to third-party tools as well now
1197:01 - diving into the concepts of cloud
1197:03 - logging these are associated primarily
1197:06 - with gcp projects so logs viewer only
1197:10 - shows logs from one specific project now
1197:13 - when it comes to log entries log entry
1197:15 - records a status or an event a project
1197:18 - receives log entries when services being
1197:20 - used produce log entries and to get down
1197:23 - to the basics
1197:25 - logs are a named collection of log
1197:27 - entries within a google cloud resource
1197:30 - and just as a note each log entry
1197:33 - includes the name of its log logs only
1197:35 - exist if they have log entries and the
1197:38 - retention period is the length of time
1197:41 - for which your logs are kept so digging
1197:44 - into the types of logs that cloud
1197:46 - logging handles there are three
1197:48 - different types of logs there are audit
1197:50 - logs transparency logs and agent logs
1197:54 - now with audit logs these are logs that
1197:56 - define who did what where and when they
1198:00 - also show admin activity and data access
1198:03 - as well as system events continuing on
1198:06 - to access transparency logs these are
1198:08 - logs for actions taken by google so when
1198:11 - google staff is accessing your data due
1198:14 - to a support ticket the actions that are
1198:16 - taken by the google staff are logged
1198:19 - within cloud logging now when it comes
1198:21 - to agent logs these are the logs that
1198:24 - come from agents that are installed on
1198:26 - vms
1198:27 - the logging agent sends system and
1198:30 - third-party logs on the vm instance to
1198:32 - cloud logging moving on to error
1198:34 - reporting this looks at real-time error
1198:37 - monitoring and alerting it counts
1198:39 - analyzes and aggregates the errors that
1198:42 - happen in your gcp environment and then
1198:45 - alerts you when a new application error
1198:47 - occurs details of the error can be sent
1198:50 - through the api and notifications are
1198:53 - still in beta error reporting is
1198:56 - integrated into cloud functions and
1198:58 - google app engine standard which is
1199:01 - enabled automatically error reporting is
1199:03 - in beta for compute engine kubernetes
1199:06 - engine and app engine flexible as well
1199:09 - as aws ec2 air reporting can be
1199:13 - installed in a variety of languages such
1199:16 - as go java.net
1199:19 - node.js python php and ruby now moving
1199:23 - into debugger this tool debugs a running
1199:26 - application without slowing it down it
1199:29 - captures and inspects the call stack and
1199:33 - local variables in your application this
1199:36 - tool debugs a running application
1199:38 - without slowing it down it captures and
1199:41 - inspects the call stack and local
1199:43 - variables in your application this is
1199:45 - also known as taking a snapshot once the
1199:48 - snapshot has been taken
1199:50 - a log point can be injected to allow you
1199:53 - to start debugging debugger can be used
1199:56 - with or without access to your
1199:58 - application source code and if your repo
1200:01 - is not local it can be hooked into a
1200:04 - remote git repo such as github git lab
1200:07 - or bitbucket debugger is integrated with
1200:10 - google app engine automatically and can
1200:13 - be installed on google compute engine
1200:16 - gke
1200:17 - and google app engine debugger is
1200:19 - integrated with google app engine
1200:21 - automatically and can be installed on
1200:23 - gke debugger is integrated with google
1200:26 - app engine automatically and can be
1200:28 - installed on google compute engine
1200:31 - google kubernetes engine google app
1200:33 - engine and cloud run and just as a note
1200:36 - installation on these products is all
1200:38 - dependent on the library and again
1200:41 - debugger can be installed like trace on
1200:44 - non-gcp environments and is available to
1200:47 - be installed using a variety of
1200:49 - different languages next up is trace and
1200:53 - trace helps you understand how long it
1200:55 - takes your application to handle
1200:57 - incoming requests from users and
1201:00 - applications trace collects latency data
1201:02 - from app engine https load balancers and
1201:06 - applications using the trace api this is
1201:09 - also integrated with google app engine
1201:12 - standard and is applied automatically so
1201:14 - you would use trace for something like a
1201:16 - website that is taking forever to load
1201:19 - to troubleshoot that specific issue
1201:21 - trace can be installed on google compute
1201:24 - engine google kubernetes engine and
1201:26 - google app engine as well it can also be
1201:29 - installed on non-gcp environments and it
1201:32 - can be installed using a variety of
1201:34 - different languages as shown here and
1201:37 - coming up on the last tool of the bunch
1201:39 - is profiler now profiler gathers cpu
1201:42 - usage and memory allocation information
1201:45 - from your applications continuously and
1201:48 - this helps you discover patterns of
1201:50 - resource consumption to help you better
1201:53 - troubleshoot profiler is low profile and
1201:56 - therefore won't take up a lot of memory
1201:58 - or cpu on your system as well in order
1202:02 - to use profiler an agent needs to be
1202:04 - installed profiler can be installed on
1202:07 - compute engine kubernetes engine and app
1202:10 - engine as well and of course it can be
1202:13 - installed on non-gcp environments and
1202:16 - profiler can be installed using the
1202:18 - following languages just go
1202:21 - java node.js and python and so just as a
1202:25 - note for the exam only a high level
1202:27 - overview of these tools are needed and
1202:29 - so this concludes this lesson on a high
1202:32 - level overview of operation suite so you
1202:35 - can now mark this lesson as complete and
1202:38 - let's move on to the next one

Cleaned transcript:

hey this is anthony tavelos your cloud instructor at exam pro bringing you a complete study course for the google cloud associate cloud engineer made available to you here on free code camp and so this course is designed to help you pass and achieve google issued certification the way we're going to do that is to go through lots of lecture content follow alongs and using my cheat sheets on the day of the exam so you pass and you can take that certification and put it on your resume or linkedin so you can get that cloud job or promotion that you've been looking for and so a bit about me is that i have 18 years industry experience seven of it specializing in cloud and four years of that as a cloud trainer i previously been a cloud and devops engineer and i've also published multiple cloud courses and i'm a huge fan of the cartoon looney tunes as well as a coffee connoisseur and so i wanted to take a moment to thank viewers like you because you make these free courses possible and so if you're looking for more ways of supporting more free courses just like this one the best way is to buy the extra study material at co example.com in particular for this certification you can find it at gcp hyphen ace there you can get study notes flash cards quizlets downloadable lectures which are the slides to all the lecture videos downloadable cheat sheets which by the way are free if you just go sign up practice exams and you can also ask questions and get learning support and if you want to keep up to date with new courses i'm working on the best way is to follow me on twitter at antony's cloud and i'd love to hear from you if you passed your exam and also i'd love to hear on what you'd like to see next welcome back in this lesson i wanted to quickly go over how to access the course resources now the resources in this course are designed to accompany the lessons and help you understand not just the theory but to help with the demo lessons that really drive home the component of handson learning these will include study notes lesson files scripts as well as resources that are used in the demo lessons these files can be found in a github repository that i will be including below that are always kept uptodate and it is through these files that you will be able to follow along and complete the demos on your own to really cement the knowledge learned it's a fairly simple process but varies through the different operating systems i'll be going through this demo to show you how to obtain access through the three major operating systems being windows mac os and ubuntu linux so i'm first going to begin with windows and the first step would be to open up the web browser and browse to this url which i will include in the notes below and this is the course github repository which will house all the course files that i have mentioned before keeping the course up to date will mean that files may need to be changed and so as i update them they will always be reflected and uploaded here in the repo so getting back to it there are two ways to access this repository so the easiest way to obtain a copy of these files will be to click on the clone or download button and click on download zip once the file has been downloaded you can then open it up by clicking on it here and here are the files here in downloads and this will give you a snapshot of all the files and folders as you see them from this repository now although this may seem like the simple way to go this is not the recommended method to download as if any files have changed you will not be up to date with the latest files and will only be current from the date at which you've downloaded them now the way that is recommended is using a source control system called git and so the easiest way to install it would be to go to this url https colon forward slash forward slash git dash scm.com and this will bring you to the git website where you can download the necessary software for windows or any other supported operating system and so i'm going to download it here and this should download the latest version of git for windows and it took a few seconds there but it is done and no need to worry about whether or not you've got the proper version usually when you click that download button it will download the latest version for your operating system so i'm going to go over here and open this up you'll get a prompt where you would just say yes and we're going to go ahead and accept all the defaults here this is where it's going to install it let's hit next these are all the components that they're going to be installed let's click on next and again we're going to go through everything with all the defaults and once we've reached installing all the defaults it's gonna take a couple minutes to install and again it took a minute or so we're going to just click on next and it's going to ask if you want to view the release notes and we don't really need those so we can click on ok and simply close that and we're just going to go over and see if git is installed we're going to run the command prompt and i'm going to just zoom in here so we can see a little better and there we go and we are just going to type in git and as you can see it's been installed and so now that we've installed git we want to be able to pull down all the folders and the files within them from the repository to our local system and so i'm just going to clear the screen here and we're going to do a cd to make sure that i'm in my home directory and then we're going to make a directory called repos and in order to do that we're going to do mkdir space repos and then we're going to move into that directory so cd space repos and so again here we want to clone those files that are in the repository to our local system so in order to do that we're going to use the command git clone so get space clone and then we're going to need our location of the git repository so let's go back to the browser and we're going to go over here to clone or download and here you will see clone with https so make sure that this says https and you can simply click on this button which will copy this to the clipboard and then we'll move back to our command prompt and paste that in and once that's pasted just hit enter and it will clone your repository into the repos directory and so just to verify that we've cloned all the necessary files we're going to cd into the master directory that we had just cloned and we're going to do a dir and there you have it all of the files are cloned exactly as it is here in the repository now just as a note in order to keep these files up to date we need to run a different command which would be a git pull and this can be run at any time in order to pull down any files or folders that have been updated since you did the first pull which in this case would be cloning of the repository again this will provide you with the latest and most uptodate files at any given moment in time and in this case since nothing has changed i have been prompted with a message stating that i'm up to date if nothing is changed you will always be prompted with this message if there was it will pull your changes down to your synced local copy and the process for windows is completed and is similar in mac os and i'll move over to my mac os virtual machine and log in and once you've logged in just going to go over here to the terminal and i'm just going to cd to make sure i'm in my home directory then i'm going to do exactly what we did in windows so i'm going to run the command mk dir space repos and create the repos directory and i'm going to move in to the repos directory and then i'm going to run git now for those of you who do not have get installed you will be prompted with this message to install it and you can go ahead and just install you'll be prompted with this license agreement you can just hit agree and depending on your internet connection this will take a few minutes to download and install so as this is going to take a few minutes i'm going to pause the video here and come back when it's finished installing okay and the software was successfully installed so just to do a double check i'm going to run git and as you can see it's been installed so now that we have git installed we want to clone all the directories and the files from the github repository to our local repos folder so i'm going to open up my browser and i'm going to paste my github repository url right here and you'll see the clone button over here so we're going to click on this button and here we can download zip but like i said we're not going to be doing that we're going to go over here and copy this url for the github repository again make sure it says https and we're going to copy this to our clipboard and we're going to go back to our terminal and we are going to run the command git space clone and we're going to paste in our url and as you can see here i've cloned the repository and all the files and folders within it and so as is my best practice i always like to verify that the files have been properly cloned and so i'm going to run the command ls just to make sure and go into the master directory and do a double check and as you can see the clone was successful as all the files and folders are here and again to download any updates to any files or directories we can simply run the command git space poll and because we've already cloned it it's already up to date and so the process is going to be extremely similar on linux so i'm going to simply move over to my linux machine and log in i'm going to open up a terminal and i'm going to make my terminal a little bit bigger for better viewing and so like the other operating systems i want to clone all the files and directories from the github repository to my machine and so i'm going to cd here to make sure i'm in my home directory and like we did before we want to create a directory called repos so i'm going to run the command mkdir space repos and we're going to create the repos directory we're now going to move into the repos directory and here we're going to run the git command and because git is not installed on my machine i've been prompted with the command in order to install it so i'm going to run that now so the command is sudo space apt space install space get and i'm going to enter in my password and install it and just to verify i'm going to run the command git and i can see here it's been installed so now i'm going to go over here to my browser and i'm going to paste in the url to my repository and over here we'll have the same clone button and when i click on it i can get the url for the github repository in order to clone it again make sure before you clone that this says https if it doesn't say https you'll have the option of clicking on a button that will allow you to do so once it says https then you can simply copy this url to your clipboard by clicking on the button and then move over back to the terminal and we are going to clone this repository by typing in the get space clone command along with the url of the repository and when we hit enter it'll clone it right down to our directory so i'm just going to move into the master directory just to verify that the files are there and again they're all here so again if you're looking to update your repository with any new updated changes you can simply run the get space pull command to update those files and so that's the linux setup so you have a local copy of the lesson files now there's just one more thing that i highly recommend you do and to demonstrate it i'm going to move back over to my windows virtual machine now i'm going to open up the web browser again open up a new tab and i'm going to browse to this url https colon forward slash forward slash code.visualstudio.com and i'll make sure that the url is in the text below there is a version of this code editor available for windows mac os and linux you can simply click on this drop down and you'll find the link to download it for your operating system but in most cases it should automatically show the correct version so just go ahead and click on download and it should start downloading automatically and you should be able to run it right away now the reason behind me asking you to install this utility is for editing code of different sorts whether you're adjusting yaml or python documents for deployment manager or even managing scripts a code editor will give you the ease of use when it comes to managing editing and even syntactical highlighting of code as shown here below it will highlight the code to make it easier to understand now if you have your own editor that you would prefer to use go ahead and use that but for those that don't my recommendation will be to use visual studio code so to install visual studio code we're just going to accept this license agreement and then we're going to click on next and we're just going to follow all the defaults to install it it's going to take a minute or two and for those running windows you want to make sure that this box is checked off so that you can launch it right away let's hit finish another recommendation would be to go over here to the task bar so you can pin it in place so that it's easier to find and so now you have access to all the resources that's needed for this course but with that that's everything that i wanted to cover for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i wanted to discuss the various certifications available for google cloud as this number keeps on growing and i am looking to keep this lesson as up to date as possible so with that being said let's dive in now google cloud has released a slew of certifications in many different areas of expertise as well as different experience levels now there are two levels of difficulty when it comes to the google cloud certifications starting off with the associate level we see that there is only the one certification which is the cloud engineer the associate level certification is focused on the fundamental skills of deploying monitoring and maintaining projects on google cloud this is a great starting point for those completely new to cloud and google recommends the associate cloud engineer as the starting point to undergoing your certification journey this was google cloud's very first certification and to me was the entry point of wanting to learn more as an engineer in cloud in my personal opinion no matter your role this certification will cover the general knowledge that is needed to know about starting on google cloud and the services within it which is why i labeled it here as the foundational level course i also consider this the stepping stone into any other professional level certifications which also happens to be a recommended path by google with a great course and some dedication i truly believe that anyone with even a basic skill level in it should be able to achieve this associate level certification now it is recommended from google themselves that prior to taking this exam that you should have over six months experience building on google cloud for those of you with more of an advanced background in google cloud or even other public clouds this certification should be an easy pass as it covers the basics that you should be familiar with adding a google twist to it at the time of this lesson this exam is two hours long and the cost is 125 us dollars the exam is a total of 50 questions which consists of both multiple choice and multiple answer questions each of the questions contain three to four line questions with single line answers that by the time you finish this course you should have the confidence to identify the incorrect answers and be able to select the right answers without a hitch moving into the professional level certifications there are seven certifications that cover a variety of areas of specialty depending on your role you might want to take one or maybe several of these certifications to help you gain more knowledge in google cloud or if you love educating yourself and you're really loving your journey in gcp you will probably want to consider pursuing them all in my personal opinion the best entry point into the professional level would be the cloud architect it is a natural step up from the associate cloud engineer and it builds on top of what is learned through that certification with a more detailed and more thorough understanding of cloud architecture that is needed for any other certification there is some overlap from the cloud engineer which is why in my opinion doing this certification right after makes sense it also brings with it the ability to design develop and manage secure scalable and highly available dynamic solutions it is a much harder exam and goes into great depth on services available the professional cloud architect is a great primer for any other professional level certification and can be really helpful to solidify the learning that is needed in any other technical role i find it the most common path that many take who look to learn google cloud which is why i personally recommend it to them and at the time of this lesson it also holds the highest return on investment due to the highest average wage over any other current cloud certification in the market google recommends over three years of industry experience including one year on google cloud before attempting these exams with regards to the exams in the professional tier they are much harder than the associate level and at the time of this course is two hours long and the cost is 200 us dollars these exams are a total of 50 questions which consists of both multiple choice and multiple answer questions it's the same amount of questions with the same amount of time but it does feel much harder each of the questions contain four to five line questions with one to three line answers it's definitely not a walk in the park and will take some good concentration and detailed knowledge on google cloud to solidify a pass after completing the cloud architect certification depending on your role my suggestion would be to pursue the areas that interest you the most to make your journey more enjoyable for me at the time i took the security engineer out as i am a big fan of security and i knew that i would really enjoy the learning and make it more fun for me this is also a great certification for those who are looking to excel their cloud security knowledge on top of any other security certifications such as the security plus or cissp now others may be huge fans of networking or hold other networking certifications such as the ccna and so obtaining the network engineer certification might be more up your alley and give you a better understanding in cloud networking now if you're in the data space you might want to move into the data engineer exam as well as taking on the machine learning engineer exam to really get some deeper knowledge in the areas of big data machine learning and artificial intelligence on google cloud now i know that there are many that love devops me being one of them and really want to dig deeper and understand sre and so they end up tackling the cloud developer and cloud devops engineer certifications so the bottom line is whatever brings you joy in the area of your choosing start with that and move on to do the rest all the professional certifications are valuable but do remember that they are hard and need preparation for study last but not least is the collaboration engineer certification and this certification focuses on google's core cloudbased collaboration tools that are available in g suite or what is now known as google workspaces such as gmail drive hangouts docs and sheets now the professional level collaboration engineers certification dives into more advanced areas of g suite such as mail routing identity management and automation of it all using tools scripting and apis this certification is great for those looking to build their skill set as an administrator of these tools but gives very little knowledge of google cloud itself so before i move on there is one more certification that i wanted to cover that doesn't fall under the associate or professional certification levels and this is the google cloud certified fellow program now this is by far one of the hardest certifications to obtain as there are very few certified fellows at the time of recording this lesson it is even harder than the professional level certifications and this is due to the sheer level of competency with hybrid multicloud architectures using google cloud anthos google's recommended experience is over 10 years with a year of designing enterprise solutions with anthos then a fourstep process begins first step is to receive a certified fellow invitation from google and once you've received that invitation then you need to submit an application with some work samples that you've done showing google your competency in hybrid multicloud once that is done the third step is a series of technical handson labs that must be completed and is a qualifying assessment that must be passed in order to continue and after all that the last step is a panel interview done with google experts in order to assess your competency of designing hybrid and multicloud solutions with anthos so as you can see here this is a very difficult and highly involved certification process to achieve the title of certified fellow this is definitely not for the faint of heart but can distinguish yourself as a technical leader in anthos and a hybrid multicloud expert in your industry now i get asked many times whether or not certifications hold any value are they easy to get are they worth more than the paperwork that they're printed on and does it show that people really know how to use google cloud and my answer is always yes as the certifications hold benefits beyond just the certification itself and here's why targeting yourself for a certification gives you a milestone for learning something new with this new milestone it allows you to put together a study plan in order to achieve the necessary knowledge needed to not only pass the exam but the skills needed to progress in your everyday technical role this new knowledge helps keep your skills up to date therefore making you current instead of becoming a relic now having these uptodate skills will also help advance your career throughout my career in cloud i have always managed to get my foot in the door with various interviews due to my certifications it gave me the opportunity to shine in front of the interviewer while being able to confidently display my skills in cloud it also allowed me to land the jobs that i sought after as well as carve out the career path that i truly wanted on top of landing the jobs that i wanted i was able to achieve a higher salary due to the certifications i had i have doubled and tripled my salary since i first started in cloud all due to my certifications and i've known others that have obtained up to five times their salary because of their certifications now this was not just from achieving the certification to put on my resume and up on social media but from the knowledge gained through the process and of course i personally feel that having your skills constantly up to date advancing your career and getting the salary that you want keeps you motivated to not only get more certifications but continue the learning process i am and always have been a huge proponent of lifelong learning and as i always say when you continue learning you continue to grow so in short google cloud certifications are a great way to grow and so that about covers everything that i wanted to discuss in this lesson so you can now mark this lesson as complete and i'll see you in the next one welcome back and in this lesson i'm going to be talking about the fictitious organization called bow tie inc that i will be using throughout the course now while going through the architectures and demos in this course together i wanted to tie them to a real world situation so that the theory and practical examples are easy to understand tying it to a scenario is an easy way to do this as well it makes things a lot more fun so the scenario again that i will be using is based on bow tie ink so before we get started with the course i'd like to quickly run through the scenario and don't worry it's going to be very high level and i will keep it brief so bow tie ink is a bow tie manufacturing company that designs and manufactures bow ties within their own factories they also hold a few retail locations where they sell their bow ties as well as wholesale to other thai and men's fashion boutiques and department stores across the globe being in the fashion business they mainly deal with commerce security and big data sets bow tie inc is a global company and they are headquartered in montreal canada they employ about 300 people globally with a hundred of them being in sales alone to support both the brick and mortar stores and wholesale branches there are many different departments to the company that make it work such as instore staff i.t marketing for both instore and online sales manufacturing finance and more the types of employees that work in bow tie inc vary greatly due to the various departments and consists of many people such as sales for both instore and wholesale managers that run the stores and sewers that work in the manufacturing plant and many more that work in these various departments the business has both offices and brick and mortar stores in montreal london and los angeles now due to the thrifty mindset of management concentrating all their efforts on commerce and almost none in technical infrastructure has caused years of technical debt and is now a complete disaster within the brick and mortar location there contains two racks with a few servers and some networking equipment the global inventory of bow ties are updated upon sales in both stores and wholesale as well as new stock that has been manufactured from the factory there are pointofsale systems in each store or office location these systems are all connected to each other over a vpn connection in order to keep updates of the inventory fresh all office and store infrastructure are connected to each other and the montreal headquarters and the point of sale systems and kiosk systems are backed up to tape in the montreal headquarters as well and like i said before management is extremely thrifty but they have finally come to the realization that they need to start spending money on the technical infrastructure in order to scale so diving into a quick overview of exactly what the architecture looks like the head office is located in montreal canada it has its main database for the crm and pointofsale systems as well as holding the responsibility of housing the equipment for the tape backups the tapes are then taken off site within montreal by a thirdparty company for storage the company has two major offices one in london covering the eu and the other in the west coast us in los angeles these major offices are also retail locations that consume i.t services from the headquarters in montreal again being in the fashion business bowtie inc employs a large amount of sales people and the managers that support them these employees operate the pointofsale systems so we're constantly looking to have the website sales and the inventory updated at all times each salesperson has access to email and files for updated forecasts on various new bowtie designs most sales people communicate over a voice over ip phone and chat programs through their mobile phone the managers also manually look at inventory on what's been sold versus what's in stock to predict the sales for stores in upcoming weeks this will give manufacturing a head start to making more bow ties for future sales now whatever implementations that we discuss throughout this course we'll need to support the daytoday operations of the sales people and the managers and because of the different time zones in play the backend infrastructure needs to be available 24 hours a day seven days a week any downtime will impact updated inventory for both online sales as well as store sales at any given time now let's talk about the current problems that the business is facing most locations hold on premise hardware that is out of date and also out of warranty the business looked at extending this warranty but became very costly as well management is on the fence about whether to buy new onpremise hardware or just move to the cloud they were told that google cloud is the way to go when it comes to the retail space and so are open to suggestions yet still very weary now when it comes to performance there seems to be a major lag from the vpn connecting from store to store as well as the head office that's responsible for proper inventory thus slowing down the point of sale systems and to top it all off backups taking an exorbitant amount of time is consuming a lot of bandwidth with the current vpn connection now bowtie inc has always struggled with the lack of highly available systems and scalability due to cost of new hardware this is causing extreme stress for online ecommerce whenever a new marketing campaign is launched as the systems are unable to keep up with the demand looking at the forecast for the next two quarters the business is looking to open up more stores in the eu as well as in the us and with the current database in place providing very inefficient high availability or scalability there is a major threat of the main database going down now when it comes to assessing the backups the tape backups have become very slow especially backing up from london and the offsite storage costs continuously go up every year the backups are consuming a lot of bandwidth and are starting to become the major pain point for connection issues between locations on top of all these issues the small it staff that is employed have outdated i.t skills and so there is a lot of manual intervention that needs to be done to top it all off all the running around that is necessary to keep the outdated infrastructure alive management is also now pushing to open new stores to supply bow ties globally given the evergrowing demand as well as being able to supply the demand of bow ties online through their ecommerce store now these are some realistic yet common scenarios that come up in reality for a lot of businesses that are not using cloud computing and throughout the course we will dive into how google cloud can help ease the pain of these current ongoing issues now at a high level with what the business wants to achieve and what the favorable results are they are all interrelated issues so bowtie inc requires a reliable and stable connection between all the locations of the stores and offices so that sales inventory and pointofsale systems are quick and uptodate at all times this will also allow all staff in these locations to work a lot more efficiently with a stable and reliable connection in place backups should be able to run smoothly and also eliminate the cost of offsite backup not to mention the manpower and infrastructure involved to get the job done while scaling up offices and stores due to increase in demand the business should be able to deploy stores in new regions using pay as you go billing while also meeting the requirements and regulations when it comes to gpdr and pci this would also give the business flexibility of having a disaster recovery strategy in place in case there was a failure of the main database in montreal now as mentioned before the business is extremely thrifty especially when it comes to spend on it infrastructure and so the goal is to have the costs as low as possible yet having the flexibility of scaling up when needed especially when new marketing campaigns are launched during high demand sales periods this would also give bowtie inc the flexibility of analyzing sales ahead of time using realtime analytics and catering to exactly what the customer is demanding thus making inventory a lot more accurate and reducing costs in manufacturing items that end up going on sale and costing the company money in the end finally when it comes to people supporting infrastructure automation is key removing manual steps and a lot of the processes can reduce the amount of manpower needed to keep the infrastructure alive and especially will reduce downtime when disaster arises putting automation in place will also reduce the amount of tedious tasks that all departments have on their plate so that they can focus on more important business needs now that's the scenario at a high level i wanted to really emphasize that this is a typical type of scenario that you will face as a cloud engineer and a cloud architect the key to this scenario is the fact that there are areas that are lacking in detail and areas that are fully comprehensible and this will trigger knowing when and where to ask relevant questions especially in your daytoday role as an engineer it will allow you to fill the gaps so that you're able to figure out what services you will need and what type of architecture to use this is also extremely helpful when it comes to the exam as in the exam you will be faced with questions that pertain to real life scenarios that will test you in a similar manner knowing what services and architecture to use based on the information given will always give you the keys to the door with the right answer and lastly when it comes to the demos this scenario used throughout the course will help put things in perspective as we will come to resolve a lot of these common issues real world scenarios can give you a better perspective on learning as it is tied to something that makes it easy to comprehend and again bow tie inc is the scenario that i will be using throughout the course to help you grasp these concepts so that's all i have to cover this scenario so you can now mark this lesson as complete and let's move on to the next one hey this is anthony cevallos and what i wanted to show you here is where you can access the practice exam on the exam pro platform so once you've signed up for your account you can head on over to the course and you can scroll down to the bottom of the curriculum list and you will see the practice exams here at the bottom now just as a quick note you should generally not attempt the practice exam unless you have completed all the lecture content including the follow alongs as once you start to see those questions you will get an urge to start remembering these questions and so i always recommend to use the practice exam as a serious attempt and not just a way to get to the final exam at a faster pace taking your time with the course will allow you to really prevail through these practice exams and allow you for a way better pass rate on the final exam looking here we can see two practice exams with 50 questions each and so i wanted to take a moment here and dive into the practice exam and show you what some of these questions will look like and so clicking into one of these exams we can get right into it and so as you can see i've already started on practice exam one and so i'm going to click into that right now and as you can see the exam is always timed and in this case will be 120 minutes for this specific exam there are 50 questions for this practice exam and you will see the breakdown in the very beginning of the types of questions you will be asked now for the google cloud exams at the associate level they are usually structured in a common format they generally start with one or two lines of sentences which will typically represent a scenario followed by the question itself this question tends to be brief and to the point immediately following that you will be presented with a number of answers usually four or five in nature and can sometimes be very very technical as they are designed for engineers like asking about which gcloud commands to use to execute in a given scenario as well as theoretical questions that can deal with let's say best practices or questions about the specific services themselves now these answers will come in two different styles either multichoice or multiselect the multichoice is usually about identifying the correct answer from a group of incorrect or less correct answers whereas the multiselect will be about choosing multiple correct solutions to identify the answer as well for this associate exam the overall structure is pretty simple in nature and typically will be either right or wrong now sometimes these questions can get tricky where there are multiple possible answers and you will have to select the most suitable ones now although most of these types of questions usually show up in the professional exam they can sometimes peek their heads into the associate and so a great tactic that i always like to use is to immediately identify what matters in the question itself and then to start ruling out any of the answers that are wrong and this will allow you to answer the question a lot more quickly and efficiently as it will bring the more correct answer to the surface as well as making the answer a lot more obvious and making the entire question less complex so for instance with this question here you are immediately asked about google's recommended practices when it comes to using cloud storage as backup for disaster recovery and this would be for a specific storage type and so quickly looking at the answers you can see that standard storage and near line storage will not be part of the answer and so that will leave cold line storage or archive storage as the two possible choices for the answer of this question and so these are the typical techniques that i always like to use for these exams and so provided that you've gone through all the course content you will be able to answer these technical questions with ease and following the techniques i've just given and applying them to each question can really help you in not only this practice exam but for the final exam landing you a passing grade getting you certified welcome back and in this section i wanted to really hone in on the basics of cloud computing the characteristics that make it what it is the different types of computing and how they differ from each other as well as the types of service models now in this lesson i wanted to dive into the definition of cloud computing and the essential characteristics that define it now for some advanced folk watching this this may be a review and for others this may fulfill a better understanding on what is cloud now cloud is a term that is thrown around a lot these days yet holds a different definition or understanding to each and every individual you could probably ask 10 people on their definition of cloud and chances are everyone would have their own take on it many see cloud as this abstract thing in the sky where files and emails are stored but it's so much more than that now the true definition of it can be put in very simple terms and can be applied to any public cloud being google cloud aws and azure moving on to the definition cloud computing is the delivery of a shared pool of ondemand computing services over the public internet that can be rapidly provisioned and released with minimal management effort or service provider interaction these computing services consist of things like servers storage networking and databases they can be quickly provisioned and accessed from your local computer over an internet connection now coupled with this definition are five essential characteristics that define the cloud model that i would like to go over with you and i believe that it would hold massive benefits to understanding when speaking to cloud this information can be found in the white paper published by the national institute of standards and technology i will include a link to this publication in the lesson notes for your review now these essential characteristics are as follows the first one is ondemand selfservice and this can be defined as being able to provision resources automatically without requiring human interaction on the provider's end so in the end you will never need to call up or interact with the service provider in order to get resources provisioned for you as well you have the flexibility of being able to provision and deprovision these resources whenever you need them and at any given time of the day the second characteristic is broad network access now this simply means that cloud computing resources are available over the network and can be accessed by many different customer platforms such as mobile phones tablets or computers in other words cloud services are available over a network moving into the third is resource pooling so the provider's computing resources are pooled together to support a multitenant model that allows multiple customers to share the same applications or the same physical infrastructure while retaining privacy and security over their information this includes things like processing power memory storage and networking it's similar to people living in an apartment building sharing the same building infrastructure like power and water yet they still have their own apartments and privacy within that infrastructure this also creates a sense of location independence in that the customer generally has no control or knowledge over the exact location of the provided resources but they may be able to specify location at a higher level of abstraction so in the end the customer does not really have the option of choosing exactly which server server rack or data center for that matter of where the provided resources are coming from they will only be able to have the option to choose things like regions or sections within that region the fourth essential characteristic is rapid elasticity this to me is the key factor of what makes cloud computing so great and so agile capabilities can be elastically provisioned and released in some cases automatically to scale rapidly outwards and inwards in response with demand to the consumer the capabilities available for provisioning often appear to be unlimited and can be provisioned in any quantity at any time and touching on the fifth and last characteristic cloud systems automatically control and optimize resource usage by leveraging a metering capability resource usage can be monitored controlled and reported providing transparency for both the provider and consumer of the service now what this means is that cloud computing resource usage is metered and you can pay accordingly for what you've used resource utilization can be optimized by leveraging payperuse capabilities and this means that cloud resource usage whether they are instances that are running cloud storage or bandwidth it all gets monitored measured and reported by the cloud service provider the cost model is based on pay for what you use and so the payment is based on the actual consumption by the customer so knowing these key characteristics of cloud computing along with their benefits i personally find can really give you a leg up on the exam as well as speaking to others in your daytoday role as more and more companies start moving to cloud i hope this lesson has explained to you on what is cloud computing and the benefits it provides so that's all i have for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i wanted to go over the four common cloud deployment models and distinguish the differences between public cloud multicloud private cloud and hybrid cloud deployment models this is a common subject that comes up a fair amount in the exam as well as a common theme in any organization moving to cloud knowing the distinctions between them can be critical to the types of architecture and services that you would use for the specific scenario you are given as well as being able to speak to the different types of deployment models as an engineer in the field getting back to the deployment models let's start with the public cloud model which we touched on a bit in our last lesson now the public cloud is defined as computing services offered by thirdparty providers over the public internet making them available to anyone who wants to use or purchase them so this means that google cloud will fall under this category as a public cloud there are also other vendors that fall under this category such as aws and azure so again public cloud is a cloud that is offered over the public internet now public clouds can also be connected and used together within a single environment for various use cases this cloud deployment model is called multicloud now a multicloud implementation can be extremely effective if architected in the right way one implementation that is an effective use of multicloud is when it is used for disaster recovery this is where your architecture would be replicated across the different public clouds in case one were to go down another could pick up the slack what drives many cases of a multicloud deployment is to prevent vendor lockin where you are locked into a particular cloud provider's infrastructure and unable to move due to the vendorspecific feature set the main downfall to this type of architecture is that the infrastructure of the public cloud that you're using cannot be fully utilized as each cloud vendor has their own proprietary resources that will only work in their specific infrastructure in other words in order to replicate the environment it needs to be the same within each cloud this removes each cloud's unique features which is what makes them so special and the resources so compelling so sometimes finding the right strategy can be tricky depending on the scenario now the next deployment model i wanted to touch on is private cloud private cloud refers to your architecture that exists on premise and restricted to the business itself with no public access yet it still carries the same five characteristics that we discussed with regards to what defines cloud each of the major cloud providers shown here all have their own flavor of private cloud that can be implemented on site google cloud has anthos aws has aws outposts and azures is azure stack they show the same characteristic and leverage similar technologies that can be found in the vendor's public cloud yet can be installed on your own onpremise infrastructure please be aware any organizations may have a vmware implementation which holds cloudlike features yet this is not considered a private cloud true private cloud will always meet the characteristics that make up cloud now it is possible to use private cloud with public cloud and this implementation is called hybrid cloud so hybrid cloud is when you are using public cloud in conjunction with private cloud as a single system a common architecture used is due to compliance where one cloud could help organizations achieve specific governance risk management and compliance regulations while the other cloud could take over the rest now i'd really like to make an important distinction here if your onpremise infrastructure is connected to public cloud this is not considered hybrid cloud this is what's known as hybrid environment or a hybrid network as the onpremises infrastructure holds no private cloud characteristics true hybrid cloud allows you to use the exact same interface and tooling as what's available in the public cloud so being aware of this can avoid a lot of confusion down the road so to sum up everything that we discussed when it comes to public cloud this is when one cloud provided by one vendor that is available over the public internet multicloud is two or more public clouds that are connected together to be used as a single system a private cloud is considered an onpremises cloud that follows the five characteristics of cloud and is restricted to the one organization with no accessibility to the public and finally hybrid cloud is private cloud connected to a public cloud and being used as a single environment again as a note onpremises architecture connected to public cloud is considered a hybrid environment and not hybrid cloud the distinction between the two are very different and should be observed carefully as gotchas may come up in both the exam and in your role as an engineer so these are all the different cloud deployment models which will help you distinguish on what type of architecture you will be using in any scenario that you are given and so this is all i wanted to cover when it comes to cloud deployment models so you can now mark this lesson as complete and let's move on to the next one welcome back so to finish up the nist definition of cloud computing i wanted to touch on cloud service models which is commonly referred to as zas now this model is usually called zas or xaas standing for anything as a service it includes all the services in a cloud that customers can consume and x can be changed to associate with the specific service so in order to describe the cloud service models i needed to touch on some concepts that you may or may not be familiar with this will make understanding the service models a little bit easier as i go through the course and describe the services available and how they relate to the model this lesson will make so much sense by the end it'll make the services in cloud easier to both describe and define now when it comes to deploying an application they are deployed in an infrastructure stack like the one you see here now a stack is a collection of needed infrastructure that the application needs to run on it is layered and each layer builds on top of the one previous to it to create what it is that you see here now as you can see at the top this is a traditional onpremises infrastructure stack that was typically used precloud now in this traditional model all the components are managed by the customer the purchasing of the data center and all the network and storage involved the physical servers the virtualization the licensing for the operating systems the staff that's needed to put it all together including racking stacking cabling physical security was also something that needed to be taken into consideration in other words for the organization to put this together by themselves they were looking at huge costs now the advantages to this is that it allowed for major flexibility as the organization is able to tune this any way they want to satisfy the application compliance standards basically anything that they wanted now when talking about the cloud service model concepts parts are always managed by you and parts are managed by the vendor now another concept i wanted to touch on is that unit of consumption is how the vendor prices what they are serving to their customer now just before cloud became big in the market there was a model where the data center was hosted for you so a vendor would come along and they would take care of everything with regards to the data center the racks the power to the racks the air conditioning the networking cables out of the building and even the physical security and so the unit of consumption here was the rack space within the data center so the vendor would charge you for the rack space and in turn they would take care of all the necessities within the data center now this is less flexible than the traditional onpremises model but the data center is abstracted for you so throughout this lesson i wanted to introduce a concept that might make things easier to grasp which is the pizza as a service so now the traditional onpremises model is where you would buy everything and make the pizza at home now as we go on in the lesson less flexibility will be available because more layers will be abstracted so the next service model that i wanted to introduce is infrastructure as a service or i as for short this is where all the layers from the data center up to virtualization is taken care of by the vendor this is the most basic model which is essentially your virtual machines in a cloud data center you set up configure and manage instances that run in the data center infrastructure and you put whatever you want on them on google cloud google compute engine would satisfy this model and so the unit of consumption here would be the operating system as you would manage all the operating system updates and everything that you decide to put on that instance but as you can see here you are still responsible for the container the run time the data and the application layers now bringing up the pizza as a service model is would be you picking up the pizza and you cooking it at home moving on to platform as a service or paz for short this is a model that is geared more towards developers and with pass the cloud provider provides a computing platform typically including the operating system the programming language execution environment the database and the web server now typically with pass you never have to worry about the operating system updates or managing the runtime and middleware and so the unit of consumption here would be the runtime now the runtime layer would be the layer you would consume as you would be running your code in the supplied runtime environment that the cloud vendor provides for you the provider manages the hardware and software infrastructure and you just use the service this is usually the layer on top of is and so all the layers between the data center and runtime is taken care of by the vendor a great example of this for google cloud is google app engine which we will be diving into a little bit later getting back to the pizza as a service model pass would fall under the pizza being delivered right to your door now with the past model explained i want to move into the last model which is sas which stands for software as a service now with sas all the layers are taken care of by the vendor so users are provided access to application software and cloud providers manage the infrastructure and platforms that run the applications g suite and microsoft's office 365 are great examples of this model now sas doesn't offer much flexibility but the tradeoff is that the vendor actually takes care of all these layers so again the unit of consumption here is the application itself and of course getting to the pizza as a service model sas is pretty much dining in the restaurant enjoying your pizza now to summarize when you have a data center on site you manage everything when it's infrastructure as a service part of that stack is abstracted by the cloud vendor with platform as a service you're responsible for the application and data everything else is abstracted by the vendor with software as a service again using the pizza as a service analogy on premise you buy everything and you make the pizza at home infrastructure as a service you pick up the pizza and you cook it at home when it comes to platform as a service the pizza is delivered and of course software as a service is dining in the restaurant now there will be some other service models coming up in this course such as function as a service and containers as a service and don't worry i'll be getting into those later but i just wanted to give you a heads up so now for some of you this may have been a lot of information to take in but trust me knowing these models will give you a better understanding of the services provided in google cloud as well as any other cloud vendor so that's all i wanted to cover in this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i wanted to discuss google cloud global infrastructure how data centers are connected how traffic flows when a request is done along with the overall structure of how google cloud geographic locations are divided for better availability durability and latency now google holds a highly provisioned low latency network where your traffic stays on google's private backbone for most of its journey ensuring high performance and a user experience that is always above the norm google cloud has been designed to serve users all around the world by designing their infrastructure with redundant cloud regions connected with high bandwidth fiber cables as well as subsea cables connecting different continents currently google has invested in 13 subsea cables connecting these continents at points of presence as you see here in this diagram hundreds of thousands of miles of fiber cables have also been laid to connect points of presence for direct connectivity privacy and reduced latency just to give you an idea of what a subsea cable run might look like i have included a diagram of how dedicated google is to their customers as there is so much that goes into running these cables that connect continents as you can see here this is the north virginia region being connected to the belgium region from the u.s over to europe a cable is run from the north virginia data center as well as having a point of presence in place going through a landing station before going deep into the sea on the other side the landing station on the french west coast picks up the other side of the cable and brings it over to the data center in the belgium region and this is a typical subsea cable run for google so continents are connected for maximum global connectivity now at the time of recording this video google cloud footprint spans 24 regions 73 zones and over 144 points of presence across more than 200 countries and territories worldwide and as you can see here the white dots on the map are regions that are currently being built to expand their network for wider connectivity now to show you how a request is routed through google's network i thought i would demonstrate this by using tony bowtie now tony makes a request to his database in google cloud and google responds to tony's request from a pop or edge network location that will provide the lowest latency this point of presence is where isps can connect to google's network google's edge network receives tony's request and passes it to the nearest google data center over its private fiber network the data center generates a response that's optimized to provide the best experience for tony at that given moment in time the app or browser that tony is using retrieves the requested content with a response back from various google locations including the google data centers edge pops and edge nodes whichever is providing the lowest latency this data path happens in a matter of seconds and due to google's global infrastructure it travels securely and with the least amount of latency possible no matter the geographic location that the request is coming from now i wanted to take a moment to break down how the geographic areas are broken out and organized in google cloud we start off with the geographic location such as the united states of america and it's broken down into multiregion into regions and finally zones and so to start off with i wanted to talk about zones now a zone is a deployment area for google cloud resources within a region a zone is the smallest entity in google's global network you can think of it as a single failure domain within a region now as a best practice resources should always be deployed in zones that are closest to your users for optimal latency now next up we have a region and regions are independent geographic areas that are subdivided into zones so you can think of a region as a collection of zones and having a region with multiple zones is designed for fault tolerance and high availability the intercommunication between zones within a region is under five milliseconds so rest assured that your data is always traveling at optimal speeds now moving on into a multiregion now multiregions are large geographic areas that contain two or more regions and this allows google services to maximize redundancy and distribution within and across regions and this is for google redundancy or high availability having your data spread across multiple regions always reassures that your data is constantly available and so that covers all the concepts that i wanted to go over when it comes to geography and regions within google cloud note that the geography and regions concepts are fundamental not only for the exam but for your daytoday role in google cloud so just as a recap a zone is a deployment area for google cloud resources within a region a zone is the smallest entity of google's global infrastructure now a region is an independent geographic area that are subdivided into zones and finally when it comes to multiregion multiregions are large geographic areas that contains two or more regions again these are all fundamental concepts that you should know for the exam and for your daytoday role in google cloud and so that's all i had for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back this lesson is going to be an overview of all the compute service options that are available in google cloud how they differ from each other and where they fall under the cloud service model again this lesson is just an overview of the compute options as we will be diving deeper into each compute option later on in this course so google cloud gives you so many options when it comes to compute services ones that offer complete control and flexibility others that offer flexible container technology managed application platform and serverless environments and so when we take all of these compute options and we look at it from a service model perspective you can see that there's so much flexibility starting here on the left with infrastructure as a service giving you the most optimal flexibility moving all the way over to the right where we have function as a service offering less flexibility but the upside being less that you have to manage and we'll be going through these compute options starting on the left here with infrastructure as a service we have compute engine now compute engine is google's staple infrastructure the service product that offers virtual machines or vms called instances these instances can be deployed in any region or zone that you choose you also have the option of deciding what operating system you want on it as well as the software so you have the option of installing different types of flavors of linux or windows and the software to go with it google also gives you the options of creating these instances using public or private images so if you or your company have a private image that you'd like to use you can use this to create your instances google also gives you the option to use public images to create instances and are available when you launch compute engine as well there are also preconfigured images and software packages available in the google cloud marketplace and we will be diving a little bit deeper into the google cloud marketplace in another lesson just know that there are slew of images out there that's available to create instances giving you the ease to deploy now when it comes to compute engine and you're managing multiple instances these are done using instance groups and when you're looking at adding or removing capacity for those compute engine instances automatically you would use auto scaling in conjunction with those instance groups compute engine also gives you the option of attaching and detaching disks as you need them as well google cloud storage can be used in conjunction with compute engine as another storage option and when connecting directly to compute engine google gives you the option of using ssh to securely connect to it so moving on to the next compute service option we have google kubernetes engine also known as gke now gke is google's flagship container orchestration system for automating deploying scaling and managing containers gke is also built on the same open source kubernetes project that was introduced by google to the public back in 2014 now before google made kubernetes a managed service there was many that decided to build kubernetes on premise in their data centers and because it is built on the same platform gke offers the flexibility of integrating with these onpremise kubernetes deployments now under the hood gke uses compute engine instances as nodes in a cluster and as a quick note a cluster is a group of nodes or compute engine instances and again we'll be going over all this in much greater detail in a different lesson so if you haven't already figured it out google kubernetes engine is considered container as a service now the next compute service option that i wanted to go over that falls under platform as a service is app engine now app engine is a fully managed serverless platform for developing and hosting web applications at scale now with app engine google handles most of the management of the resources for you for example if your application requires more computing resources because traffic to your website increases google automatically scales the system to provide these resources if the system software needs a security update as well that's handled for you too and so all you need to really take care of is your application and you can build your application in your favorite language go java.net and many others and you can use both preconfigured runtimes or use custom runtimes to allow you to write the code in any language app engine also allows you to connect with google cloud storage products and databases seamlessly app engine also offers the flexibility of connecting with thirdparty databases as well as other cloud providers and thirdparty vendors app engine also integrates with a wellknown security product in google cloud called web security scanner as to identify security vulnerabilities and so that covers app engine in a nutshell moving on to the next compute service option we have cloud functions and cloud functions fall under function as a service this is a serverless execution environment for building and connecting cloud services with cloud functions you write simple single purpose functions that are attached to events that are produced from your infrastructure and services in google cloud your function is triggered when an event being watched is fired your code then executes in a fully managed environment there is no need to provision any infrastructure or worry about managing any servers and cloud functions can be written using javascript python 3 go or java runtimes so you can take your function and run it in any of these standard environments which makes it extremely portable now cloud functions are a good choice for use cases that include the following data processing or etl operations such as video transcoding and iot streaming data web hooks that respond to http triggers lightweight apis that compose loosely coupled logic into applications as well as mobile backend functions again cloud functions are considered function as a service and so that covers cloud functions now moving to the far right of the screen on the other side of the arrow we have our last compute service option which is cloud run now cloud run is a fully managed compute platform for deploying and scaling containerized applications quickly and securely cloudrun was built on an open standard called k native and this enabled the portability of any applications that were built on it cloudrun also abstracts away all the infrastructure management by automatically scaling up and down almost instantaneously depending on the traffic now cloud run was google's response to abstracting all the infrastructure that was designed to run containers and so this is known as serverless for containers cloudrun has massive flexibility as you can write it in any language any library using any binary this compute service is considered a function as a service now at the time of recording this video i have not heard of cloud cloudrun being in the exam but since it is a compute service option i felt the need for cloudrun to have an honorable mention and so these are all the compute service options that are available on google cloud and we will be diving deeper into each one of these later on in this course again this is just an overview of all the compute service options that are available on the google cloud platform and so that's all i wanted to cover for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back now in the last lesson i covered all the different options for compute services in this lesson we're going to cover the options that are available that couple well with these compute services by diving deeper into the different storage types and the different databases available on google cloud again this is strictly an overview as i will be diving deeper into these services later on in the course now when it comes to storage options there are three services that are readily available to you in google cloud each of them have their own specific use case that i will be diving into in just a second the first one i wanted to go over is cloud storage now with cloud storage this is google's consistent scalable large capacity and highly durable object storage so when i refer to object storage this is not the type of storage that you would attach to your instance and store your operating system on i'm talking about managing data as objects such as documents or pictures and shouldn't be confused with block storage which manages data at a more granular level such as an operating system not to worry if you fully don't grasp the concept of object storage i will be going into further detail with that later on in the cloud storage lesson cloud storage has 11 9's durability and what i mean by durability is basically loss of files so just to give you a better picture on cloud storage durability if you store 1 million files statistically google would lose one file every 659 000 years and you are about over 400 times more likely to get hit by a meteor than to actually lose a file so as you can see cloud storage is a very good place to be storing your files another great feature on cloud storage is the unlimited storage that it has with no minimum object size so feel free to continuously put files in cloud storage now when it comes to use cases cloud storage is fantastic for content delivery data lakes and backups and to make cloud storage even more flexible it is available in different storage classes and availability which i will be going over in just a second now when it comes to these different storage classes there are four different classes that you can choose from the first one is the standard storage class and this storage class offers the maximum availability with your data with absolutely no limitations this is great for storage that you access all the time the next storage class is near line and this is lowcost archival storage so this storage class is cheaper than standard and is designed for storage that only needs to be accessed less than once a month and if you're looking for an even more cost effective solution cloud storage has cold line storage class which is an even lower cost archival storage solution this storage class is designed for storage that only needs to be accessed less than once every quarter and just when you thought that the prices couldn't get lower than cold line cloud storage has offered another storage class called archive and this is the lowest cost archival storage which offers storage at a fraction of a penny per gigabyte but is designed for archival or backup use that is accessed less than once a year now when it comes to cloud storage availability there are three options that are available there is region dual region and multiregion region is designed to store your data in one single region dual region is exactly how it sounds which is a pair of regions now in multiregion cloud storage stores your data over a large geographic area consisting of many different regions across that same selected geographic area and so that about covers cloud storage as a storage option the next storage option that i wanted to talk about is file store now file store is a fully managed nfs file server from google cloud that is nfs version 3 compliant you can store data from running applications from multiple vm instances and kubernetes clusters accessing the data at the same time file store is a great option for when you're thinking about accessing data from let's say an instance group and you need multiple instances to access the same data and moving on to the last storage option we have persistent disks now with persistent disks this is durable block storage for instances now as i explained before block storage is different than object storage if you remember previously i explained that object storage is designed to store objects such as data or photos or videos whereas block storage is raw storage capacity that is used in drives that are connected to an operating system in this case persistent disks are doing just that persistent disks come in two options the first one is the standard option which gives you regular standard storage at a reasonable price and the other option is solid state or ssd which gives you lower latency higher iops and is just all around faster than your standard persistent disk both of these options are available in zonal and regional options depending on what you need for your specific workload so now that i've covered all three storage options i wanted to touch into the database options that are available on google cloud these database options come in both the sql and nosql flavors depending on your use case now getting into the options themselves i wanted to start off going into a little bit of detail with the sql relational options so the first option is cloud sql and cloud sql is a fully managed database service that is offered in postgres mysql and sql server flavors cloud sql also has the option of being highly available across zones now moving into cloud spanner this is a scalable relational database service that's highly available not only across zones but across regions and if need be available globally cloud spanner is designed to support transactions strong consistency and synchronous replication moving into the nosql options there are four available services that google cloud offers moving into the first one is bigtable and bigtable is a fully managed scalable nosql database that has high throughput and low latency bigtable also comes with the flexibility of doing cluster resizing without any downtime the next nosql option available is datastore and this is google cloud's fast fully managed serverless nosql document database datastore is designed for mobile web and internet of things applications datastore has the capabilities of doing multiregion replication as well as acid transactions for those of you who don't know i will be covering acid transactions in a later lesson next up for nosql options is firestore and this is a nosql realtime database and is optimized for offline use if you're looking to store data in a database in real time firestore is your option and like bigtable you can resize the cluster in firestore without any downtime and the last nosql option is memorystore and this is google cloud's highly available in memory service for redis and memcached this is a fully managed service and so google cloud takes care of everything for you now i know this has been a short lesson on storage and database options but a necessary overview nonetheless of what's to come and so that's about all i wanted to cover in this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back now while there are some services in gcp that take care of networking for you there are still others like compute engine that give you a bit more flexibility in the type of networking you'd like to establish this lesson will go over these networking services at a high level and provide you with strictly an overview to give you an idea on what's available for any particular type of scenario when it comes to connecting and scaling your network traffic i will be going into further details on these networking services in later lessons now i wanted to start off with some core networking features for your resources and how to govern specific traffic traveling to and from your network this is where networks firewalls and routes come into play so first i wanted to start off with virtual private cloud also known as vpc now vpc manages networking functionality for your google cloud resources this is a virtualized network within google cloud so you can picture it as your virtualized data center vpc is a core networking service and is also a global resource that spans throughout all the different regions available in google cloud each vpc contains a default network as well additional networks can be created in your project but networks cannot be shared between projects and i'll be going into further depth on vpc in a later lesson so now that we've covered vpc i wanted to get into firewall rules and routes now firewall rules segment your networks with a global distributive firewall to restrict access to resources so this governs traffic coming into instances on a network each default network has a default set of firewall rules that have already been established but don't fret you can create your own rules and set them accordingly depending on your workload now when it comes to routes this specifies how traffic should be routed within your vpc to get a little bit more granular routes specify how packets leaving an instance should be directed so it's a basic way of defining which way your traffic is going to travel moving on to the next concept i wanted to cover a little bit about low balancing and how it distributes workloads across multiple instances now we have two different types of load balancing and both these types of load balancing can be broken down to even a more granular level now when it comes to http or https low balancing this is the type of load balancing that covers worldwide auto scaling and load balancing over multiple regions or even a single region on a single global ip https load balancing distributes traffic across various regions and make sure that the traffic is routed to the closest region or in case there's failures amongst instances or in instances being bombarded with traffic http and https load balancing can route the traffic to a healthy instance in the next closest region another great feature of this load balancing is that it can distribute traffic based on content type now when it comes to network load balancing this is a regional load balancer and supports any and all ports it distributes traffic among server instances in the same region based on incoming ip protocol data such as address port and protocol now when it comes to networking dns plays a big part and because dns plays a big part in networking google has made this service 100 available on top of giving any dns queries the absolute lowest latency with google cloud dns you can publish and maintain dns records by using the same infrastructure that google uses and you can work with your managed zones and dns records such as mx records tax records cname records and a records and you can do this all through the cli the api or the sdk now some of the advanced connectivity options that are available in google cloud are cloudvpn and direct interconnect now cloudvpn connects your existing network whether it be onpremise or in another location to your vbc network through an ipsec connection the traffic is encrypted and travels between the two networks over the public internet now when it comes to direct interconnect this connectivity option allows you to connect your existing network to your vpc network using a highly available low latency connection this connectivity option does not traverse the public internet and merely connects to google's backbone and this is what gives it the highly available low latency connection a couple of other advanced connectivity options is direct and carrier peering these connections allow your traffic to flow through google's edge network locations and pairing can be done directly or it can be done through a thirdparty carrier and so although this is a very short lesson i will be going into greater depth on all these concepts in later lessons in the course so that's all i had to cover for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson we're going to learn about how resources and entities are organized within google cloud and how permissions are inherited through this approach knowing this structure is a fundamental concept that you should know while working in gcp at any capacity so before defining what the resource hierarchy is i'd like to take a little bit of time to define what is a resource now in the context of google cloud a resource can refer to the service level resources that are used to process your workloads such as compute instance vms cloud storage buckets and even cloud sql databases as well as the account level resources that sit above the services such as the organization itself the folders and the projects of course which we will be getting into a little bit deeper in just a minute the resource hierarchy is google's way to configure and grant access to the various cloud resources for your company within google cloud both at the service level and at the account level the resource hierarchy in google cloud can truly define the granular permissions needed for when you need to configure permissions to everyone in the organization that actually makes sense so now that we covered what is a resource i wanted to start digging into the resource hierarchy and the structure itself now google cloud resources are organized hierarchically using a parentchild relationship this hierarchy is designed to map an organization's operational structure to google cloud and to manage access control and permissions for groups of related resources so overall resource hierarchy will give organizations better management of permissions and access control the accessibility of these resources or policies are controlled by identity and access management also known as iam a big component of gcp which we will be digging into a little bit later on in this course and so when an iam policy is set on a parent the child will inherit this policy respectively access control policies and configuration settings on a parent resource are always inherited by the child also please note that each child object can only have exactly one parent and that these policies are again controlled by iam so now to understand a little bit more about how the gcp resource hierarchy works i wanted to dig into the layers that support this hierarchy so this is a diagram of exactly what the resource hierarchy looks like in all of its awesomeness including the billing account along with the payments profile but we're not going to get into that right now i'll actually be covering that in a later lesson so more on that later so building the structure from the top down we start off with the domain or cloud level and as you can see here the domain of bowtieinc.co is at the top this is the primary identity of your organization at the domain level this is where you manage your users in your organizations so users policies and these are linked to g suite or cloud identity accounts now underneath the domain level we have the organization level and this is integrated very closely with the domain so with the organization level this represents an organization and is the root node of the gcp resource hierarchy it is associated with exactly one domain here we have the domain set as bowtie inc all entities or resources belong to and are grouped under the organization all controlled policies applied to the organization are inherited by all other entities and resources underneath it so any folders projects or resources will get those policies that are applied from the organization layer now i know that we haven't dug into roles as of yet but the one thing that i did want to point out is that when an organization is created an organization admin role is created and this is to allow full access to edit any or all resources now moving on to the folders layer this is an additional grouping mechanism and isolation boundary between each project in essence it's a grouping of other folders projects and resources so if you have different departments and teams within a company this is a great way to organize it now a couple of caveats when it comes to folders the first one is you must have an organization node and the second one is while a folder can contain multiple folders or resources a folder or resource can have exactly one parent now moving into the projects layer this is a core organizational component of google cloud as projects are required to use service level resources these projects are the base level organizing entity in gcp and parent all service level resources just as a note any given resource can only exist in one project and not multiple projects at the same time and moving on to the last layer we have the resources layer and this is any service level resource created in google cloud everything from compute engine instances to cloud storage buckets to cloud sql databases apis users all these service level resources that we create in google cloud fall under this layer now giving the hierarchy a little bit more context i want to touch on labels for just a second labels help categorize resources by using a key value pair and you can attach them to any resource and so what labels help you do is to break down and organize costs when it comes to billing now to give you some more structure with regards to the hierarchy under the domain level everything underneath this is considered a resource and to break it down even further everything you see from the organization layer to the projects layer is considered an account level resource everything in the resource layer is considered a service level resource and so this is how the google cloud resource hierarchy is split up and organized and so before i finish off this lesson i wanted to give you a quick runthrough on how policies can be applied at a hierarchical level so i thought i'd bring in tony bowtie for a quick demo so just to give you an example tony bowtie is part of department b and tony's manager lark decides to set a policy on department b's folder and this policy grants project owner role to tony at bowtieinc.co so tony will have the project owner role for project x and for project y at the same time lark assigns laura at bowtieinc.co cloud storage admin role on project x and thus she will only be able to manage cloud storage buckets in that project this hierarchy and permission inheritance comes up quite a bit not only in the exam but is something that should be carefully examined when applying permissions anywhere within the hierarchy in your daytoday role as an engineer applying permissions or policies to resources with existing policies may not end up getting you the desired results you're looking for and may have a chance to be overlooked now i hope these diagrams have given you some good contacts with regards to resource hierarchy its structure and the permissions applied down the chain now that's all i have for this lesson on resource hierarchy so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i will be covering a few different topics that i will touch on when creating a new google cloud account i will be covering going over the free tier and the always free options the differences between them and a demo showing how you can create your own free tier account as well i'll also be going into what you will need in order to fulfill this demo so for the remainder of this course all the demos will run under the free tier now when i built this course i built it with budget in mind and having viewed on ways where i can keep the price to a minimum while still keeping the demos extremely useful and so the free tier falls within all these guidelines and will help you learn without the high ticket price and so getting into a quick overview of the differences between the free tier and the always free option i have broken them down here with their most significant differences in the free tier google cloud offers you a 12 month free trial with a 300 u.s credit this type of account ends when the credit is used or after the 12 months whichever happens first and so for those of you who are looking at taking advantage of this on a business level unfortunately the free tier only applies to a personal account and cannot be attached to a business account now moving over to the always free option the always free option isn't a special program but it's a regular part of your google cloud account it provides you limited access to many of the google cloud resources free of charge and once these limits have been hit then you are charged at the regular per second billing rate and i will show you a little bit later how to monitor these credits so that you don't go over using this in conjunction with the free tier account is not possible you have to have an upgraded billing account which can also include a business account now there are a bunch more stipulations in this program and i will include a link to both of them in the lesson text below for later viewing at your convenience now lastly before we get into the demo i wanted to go through a quick runthrough of exactly what's needed to open up your free tier account so we're going to start off with a fresh new gmail address so that it doesn't conflict with any current gmail address that you may have you're gonna need a credit card for verification and this is for google to make sure that you're an actual human being and not a robot and you won't be charged unless you go above the 300 credit limit as well i highly recommend going into a private browsing session so whether you're using chrome you would use an incognito session if you're using firefox you would use private browsing and in microsoft edge you would be using the in private mode and so in order to start with this free trial you can head on over to the url listed here and i'll also include this in the lesson text so head on over to this url and i'll see you there in just a second okay so here we are at the free trial url i'm here in google chrome in an incognito session and so we're not going to sign up we're going to go over here to create account you can just click on create account for myself because as i mentioned earlier you're not able to create a free trial account with your business so i'm going to click on for myself and it's going to bring you to this page where it says create your google account and you're going to go to create a new gmail address instead and now you're going to fill in all the necessary information that's needed in order to open up this new gmail account once you're finished typing your password you can hit next and now i got prompted for six digit verification code that i have to plug in but in order to do that google needs my telephone number so i'm gonna type that in now and just to let you know this verification is done to let google know that you're not a bot and you're a real human and google just sent me a verification code and this is a onetime verification code that i'm going to plug in and i'm going to hit verify and you can plug in the necessary information here for recovery email address your birthday and gender and this is so that google can authenticate you in case you accidentally misplace your password and then just hit next and here google gives you a little bit more information on what your number can be used for and so i'm going to go ahead and skip it and of course we're going to read through the terms of service and the privacy policy click on agree and as you can see we're almost there it shows here that we're signing up for the free trial i'm in canada so depending on your country this may change of course i read the terms of service and i'm going to agree to it and i don't really want any updates so you can probably skip that and just hit continue and so this is all the necessary information that needs to be filled out for billing and so here under account type be sure to click on individual as opposed to business and again fill in all the necessary information with regards to your address and your credit card details and once you fill that in you can click on start my free trial and once you've entered in all that information you should be brought to this page with a prompt asking you exactly what you need with regards to google cloud and you can just hit skip here and i'm going to zoom in here just see a little better and so here you're left with a checklist where you can go through all the different resources and it even gives you a checklist to go through but other than that we're in and so just to verify that we're signed up for a free tier account i'm going to go over to billing and i'm going to see here that i have my free trial credit and it says 411 dollars and due to the fact that my currency is in canadian dollars it's been converted from us dollars and so we'll be going through billing in a later lesson but right now we are actually logged in and so that's all i wanted to cover for this lesson on how to sign up for your free trial account so you can now mark this lesson as complete and you can join me in the next one where we will secure the account using a method called twostep verification welcome back so in the last lesson we went ahead and created a brand new gcp account in this lesson we'll be discussing how to secure that gcp account by following some best practices whenever any account is created in google cloud and this can be applied with regards to personal accounts as well as the super admin account as it's always good to keep safety as a priority this lesson may be a refresher for those who are a bit more advanced as for everyone else these steps could help you from an attack on your account i'd first like to run you through a scenario of the outcome on both secure and nonsecure accounts as well as the different options that reside in google cloud when it comes to locking down your account i'll then run through a handson demo in the console to show you how you can apply it yourself so in this specific scenario a username and password is used to secure the account here lark a trouble causing manager looks over the shoulder of tony bowtie while he plugs in his username and password so that he can later access his account to wreak havoc on tony's reputation as tony leaves for coffee lark decides to log in and send a companywide email from tony's account to change an already made decision about next season's store opening in rome italy that would not look good for tony it was that easy for lark to steal tony's password and in a real life scenario it would be that easy for someone to steal your password now when someone steals your password they could do even more devious things than what lark did not just sending out harmful emails they could lock you out of your account or even delete emails or documents this is where twostep verification comes in this can help keep bad people out even if they have your password twostep verification is an extra layer of security most people only have one layer to protect their account which is their password with twostep verification if a bad person hacks through your password they'll still need your phone or security key to get into your account so how twostep verification works is that signin will require something you know and something that you have the first one is to protect your account with something you know which will be your password and the second is something that you have which is your phone or security key so whenever you sign into google you'll enter your password as usual then a code will be sent to your phone via text voice call or google's mobile app or if you have a security key you can insert it into your computer's usb port codes can be sent in a text message or through a voice call depending on the setting you choose you can set up google authenticator or another app that creates a onetime verification code which is great for when you're offline you would then enter the verification code on the sign in screen to help verify that it is you another way for verification is using google prompts and this can help protect against sim swap or other phone number based hacks google prompts are push notifications you'll receive on android phones that are signed into your google account or iphones with the gmail app or google app that's signed into your google account now you can actually skip a second step on trusted devices if you don't want to provide a second verification step each time you sign in on your computer or your phone you can check the box next to don't ask again on this computer and this is a great added feature if you are the only user on this device this feature is not recommended if this device is being used by multiple users security keys are another way to help protect your google account from phishing attacks when a hacker tries to trick you into giving them your password or other personal information now a physical security key is a small device that you can buy to help prove it's you signing in when google needs to make sure that it's you you can simply connect your key to your computer and verify that it's you and when you have no other way to verify your account you have the option of using backup codes and these are onetime use codes that you can print or download and these are multiple sets of eightdigit codes that you can keep in a safe place in case you have no other options for verification i personally have found use in using these backup codes as i have used them in past when my phone died so ever since lark's last email tony not only changed his password but added a twostep verification to his account so that only he would have access and would never have to worry again about others looking over his shoulder to gain access to his account as tony leaves for coffee lark tries to log in again but is unsuccessful due to the twostep verification in place tony has clearly outsmarted the bad man in this scenario and lark will have to look for another way to foil tony's plan to bring greatness to bow ties across the globe and this is a sure difference between having a secure account and a not so secure account and so now that i've gone through the theory of the twostep verification process i'm going to dive into the console and implement it with the handson demo just be aware that you can also do this through the gmail console but we're going to go ahead and do it through the google cloud console using the url you see here so whenever you're ready feel free to join me in the console and so here we are back in the console and over here on the top right hand corner you will find a user icon and you can simply click on it and click over to your google account now i'm just going to zoom in for better viewing and so in order to enable twostep verification we're gonna go over here to the menu on the left and click on security and under signing into google you will find twostep verification currently it's off as well as using my phone to sign in is off so i'm going to click on this bar here for twostep verification and i definitely want to add an extra layer of security and i definitely want to keep the bad guys out so i'm going to go ahead and click on the get started button it'll ask me for my password and because i've entered my phone number when i first signed up for the account it actually shows up here this is i antony which is my iphone and so now i can get a twostep verification here on my iphone and again this is going to be a google prompt as it shows here but if i wanted to change it to something else i can simply click on show more options and here we have a security key as well as text message or voice call i highly recommend the google prompt as it's super easy to use with absolutely no fuss and so as i always like to verify what i've done i'm going to click on this try it now button and so because i wanted to show you exactly what a live google prompt looks like i'm going to bring up my phone here on the screen so that you can take a look and it actually sent me a google prompt to my phone and i'm just going to go ahead and open up my gmail app so i can verify that it is indeed me that wants to log in which i will accept and so once i've accepted the google prompt another window will pop up asking me about a backup option and so i'll simply need my phone number and i can either get a text message or a phone call and again you have other options as well so you can use the onetime backup codes which we discussed earlier and you can print or download them but i usually like to use a text message and so i'm going to use that i'm going to send it to my phone and so just to verify it i'm gonna now plug in the onetime code that was sent to me and then just hit next so the second step is the google prompt it's my default and my backup options if i can't get google prompt is a voice or text message and again this is for my account antony gcloud ace at gmail.com sending it to my i antony device so turn on twostep verification absolutely and so there you have it there is twostep verification enabled and if i wanted to change the available steps i can do so here i can also edit it i can edit my phone number and i can also set up any backup codes in case i need it in my personal opinion twostep verification is a musthave on any account best practice is to always do it for your super admin account which would be my gmail account that i am currently signed up with but i find is a necessity for any other users and always make it a policy for people to add twostep verification to their accounts i highly recommend that you make it your best practice to do this in your role as an engineer in any environment at any organization again twostep verification will allow to keep you safe your users safe and your environment safe from any malicious activities that could happen at any time and that's all i have for this lesson on twostep verification and securing your account so you can now mark this lesson as complete and let's move on to the next one welcome back now there are many different ways in which you can interact with google cloud services and resources this lesson is an overview of the gcp console and how you can interact with it using the graphical user interface and so for this handson demo i will be diving into how to navigate through the gcp console and point out some functions and features that you may find helpful so with that being said let's dive in and so here we are back in the console up here you can see the free trial status and then i still have 410 credit again this is canadian dollars so i guess consider me lucky so i'm going to go ahead over here and dismiss this don't activate it because otherwise this will kill your free trial status and you don't want to do that so i'm just going to hit dismiss so over here on the main page you have a bunch of cards here that will give you the status of your environment as well as the status of what's happening within google cloud with these cards you can customize them by hitting this button over here customize and you can turn them on or off and you can go ahead and move these around if you'd like and i'm going to put this up here as well i'm going to turn on my billing so i can keep track of exactly what my spend is i don't really need my get starting card so i'm going to turn that off as well as the documentation i'm going to turn that off as well and the apis is always nice to have as well up here on the project info this reflects the current project which is my first project and the project name here is the same the project id is showing and the project number and i'm going to dive deeper into that in another lesson also note that your cards will reflect exactly what it is that you're interacting with and so the more resources that you dive into the cards will end up showing up here and you can add them and turn them off at will so i'm going to go up here and click on done because i'm satisfied with the way that things look here on my home page and over here to your left i wanted to focus on all the services that are available in their own specific topics so for instance all of compute you will find app engine compute engine kubernetes and so on so note that anything compute related you'll find them all grouped together also another great feature is that you can pin exactly what it is that you use often so if i am a big user of app engine i can pin this and it will move its way up to the top this way it saves me the time from having to go and look for it every time i need it and if i'm using it constantly it's great to have a shortcut to unpin it i simply go back to the pin and click on it again as well if i'd like to move the menu out of the way to get more screen real estate i can simply click on this hamburger button here and make it disappear and to bring it back i can just click on that again and i'll bring it back again now i know that there's a lot of resources here to go through so if you're looking for something specific you can always go up to the search bar right here and simply type it in so if i'm looking for let's say cloud sql i can simply type in sql and i can find it right here i can find the api and if anything associated with the word sql if i'm looking for cloud sql specifically i can simply type in cloud sql and here it is another thing to note is that if you want to go back to your homepage you can simply go up to the left hand corner here and click on the google cloud platform logo and it'll bring you right back and right here under the google cloud platform logo you'll see another set of tabs we have dashboard we also have activity and this will show all the latest activity that's been done and because this is a brand new account i don't have much here now because this is my first time in activity this is going to take some time to index and in the meantime i wanted to show you filters if this were a long list to go through where activity has been happening for months i can filter through these activities either by user or by categories or by resource type as well as the date i can also combine these to search for something really granular and beside the activity tab we have recommendations which is based on the recommender service and this service provides recommendations and insights for using resources on google cloud these recommendations and insights are on a per product or per service basis and they are based on machine learning and current resource usage a great example of a recommendation is vm instance right sizing so if the recommender service detects that a vm instance is underutilized it will recommend changing the machine size so that i can save some money and because this is a fresh new account and i haven't used any resources this is why there is no recommendations for me so going back to the home page i want to touch on this projects menu for a second and as you can see here i can select a project now if i had many different projects i can simply search from each different one and so to cover the last part of the console i wanted to touch on this menu on the top right hand corner here so clicking on this present icon will reveal my free trial status which i dismissed earlier next to the present we have a cloud shell icon and this is where you can activate and bring up the cloud shell which i will be diving into deeper in a later lesson and right next to it is the help button in case you need a shortcut to any documentations or tutorials as well some keyboard shortcuts may help you be a little bit more efficient and you can always click on this and it'll show you exactly what you need to know and so i'm going to close this and to move over to the next part in the menu this is the notifications so any activities that happen you will be notified here and you can simply click on the bell and it'll show you a bunch of different notifications for either resources that are created or any other activities that may have happened now moving on over three buttons over here is the settings and utilities button and over here you will find the preferences and under communication you will find product notifications and updates and offers and you can turn them off or on depending on whether or not you want to receive these notifications as well you have your language and region and you can personalize the cloud console as to whether or not you want to allow google to track your activity and this is great for when you want recommendations so i'm going to keep that checked off getting back to some other options you will find a link to downloads as well as cloud partners and the terms of service privacy and project settings and so to cover the last topic i wanted to touch on is the actual google account button and here you can add other user accounts for when you log into the console with a different user as well as go straight to your google account and of course if you're using a computer that's used by multiple users you can sign out here as well and so that's just a quick runthrough of the console and so feel free to poke around and get familiar with exactly what's available in the console so that it's a lot easier for you to use and allow you to become more efficient and so that's all i have for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to be going through a breakdown of cloud billing and an overview of the various resources that's involved with billing billing is important to know and i'll be diving into the concepts around billing and billing interaction over the next few lessons as well i'll be getting into another demo going through the details on how to create edit and delete a cloud billing account now earlier on in the course i went over the resource hierarchy and how google cloud resources are broken down starting from the domain level down to their resource level this lesson will focus strictly on the billing account and payments profile and the breakdown are concepts that are comprised within them so getting right into it let's start with the cloud billing account a cloud billing account is a cloud level resource managed in the cloud console this defines who pays for a given set of google cloud resources billing tracks all of the costs incurred by your google cloud usage as well it is connected to a google payments profile which includes a payment method defining on how you pay for your charges a cloud billing account can be linked to one or more projects and not to any one project specifically cloud billing also has billing specific roles and permissions to control accessing and modifying billing related functions that are established by identity and access management cloud billing is offered in two different account types there is the selfservice or online account or you can also choose from the invoiced or offline payments when it comes to the selfservice option the payment method is usually a credit or debit card and costs are charged automatically to the specific payment method connected to the cloud billing account and when you need access to your invoices you can simply go to the cloud console and view them online now when it comes to the invoice account first you must be eligible for invoice billing once you are made eligible the payment method used can be check or wire transfer your invoices are sent by mail or electronically as well they're also available in the cloud console as well as the payment receipts now another cool feature of billing account is subaccounts and these are intended for resellers so if you are a reseller you can use subaccounts to represent your customers and make it easy for chargebacks cloud billing subaccounts allow you to group charges from projects together on a separate section of your invoice and is linked back to the master cloud billing account on which your charges appear subaccounts are designed to allow for customer separation and management so when it comes to ownership of a cloud billing account it is limited to a single organization it is possible though for a cloud billing account to pay for projects that belong to an organization that is different than the organization that owns the cloud billing account now one thing to note is that if you have a project that is not linked to a billing account you will have limited use of products and services available for your project that is projects that are not linked to a billing account cannot use google cloud services that aren't free and so now that we've gone through an overview of the billing account let's take a quick step into the payments profile now the payments profile is a google level resource managed at payments.google.com the payments profile processes payments for all google services and not just for google cloud it connects to all of your google services such as google ads as well as google cloud it stores information like your name address and who is responsible for the profile it stores your various payment methods like credit cards debit cards and bank accounts the payments profile functions as a single pane of glass where you can view invoices payment history and so on it also controls who can view and receive invoices for your various cloud billing accounts and products now one thing to note about payments profile is that there are two different types of payment profiles the first one is individual and that's when you're using your account for your own personal payments if you register your payments profile as an individual then only you can manage the profile you won't be able to add or remove users or change permissions on the profile now if you choose a business profile type you're paying on behalf of a business or organization a business profile gives you the flexibility to add other users to the google payments profile you manage so that more than one person can access or manage a payments profile all users added to a business profile can then see the payment information on that profile another thing to note is that once the profile type has been selected it cannot be changed afterwards and so now that we've quickly gone through an overview of all the concepts when it comes to billing i am now going to run through a short demo where i will create a new billing account edit that billing account and show you how to close a billing account so whenever you're ready join me in the console and so here i am back in the console and so the first thing i want to do is i want to make sure that i have the proper permissions in order to create and edit a new billing account so what i'm going to do is go over here to the hamburger menu up here in the top left hand corner and click on it and go over to i am an admin and over to iam now don't worry i'm not going to get really deep into this i will be going over this in a later section where i'll go through iam and roles but i wanted to give you a sense of exactly what you need with regards to permissions so now that i'm here i'm going to be looking for a role that has to do with billing so i'm simply going to go over here on the left hand menu and click on roles and you'll have a slew of roles coming up and what you can do is filter through them just by simply typing in billing into the filter table here at the top and as you can see here there is billing account administrator billing account creator and so on and so forth and just to give you a quick overview on these roles and so for the billing account administrator this is a role that lets you manage billing accounts but not create them so if you need to set budget alerts or manage payment methods you can use this role the billing account creator allows you to create new selfserve online billing accounts the billing account user allows you to link projects to billing accounts the billing account viewer allows you to view billing account cost information and transactions and lastly the project billing manager allows you to link or unlink the project to and from a billing account so as you can see these roles allow you to get pretty granular when it comes to billing so i'm going to go back over to the left hand menu over on iam and click on there and i want to be able to check my specific role and what permissions that i have or i will need in order to create a new billing account and so if i click on this pencil it'll show me exactly what my role is and what it does and as it says here i have full access to all resources which means that i am pretty much good to go so i'm going to cancel out here and i'm going to exit i am an admin so i'm going to click on the navigation menu and go over to billing and so this billing account is tied to the current project and because it's the only billing account it's the one that shows up and so what i want to do is i want to find out a little bit more information with regards to this billing account so i'm going to move down the menu and click on account management here i can see the billing account which is my billing account i can rename it if i'd like and i can also see the projects that are linked to this billing account so now that we've viewed all the information with regards to the my billing account i'm going to simply click on this menu over here and click on the arrow and go to manage billing accounts and here it will bring me to all my billing accounts and because i only have one is shown here my billing account but if i had more than one they would show up here and so now in order for me to create this new billing account i'm going to simply click on create account and i will be prompted with a name a country and a currency for my new billing account and i'm actually going to rename this billing account and i'm going to rename it to gcloud ace dash billing i'm going to leave my country as canada and my currency in canadian dollars and i'm going to simply hit continue and it's giving me the choice in my payments profile and because i want to use the same payments profile i'm just going to simply leave everything as is but for demonstration purposes over here you can click on the payments profile and the little arrow right beside the current profile will give me the option to create a new payments profile and we're going to leave that as is under customer info i have the option of changing my address and i can click on this pencil icon and change it as well i can go to payment methods and click on the current payment method with that little arrow and add a new credit or debit card and as i said before we're going to keep things the way they are and just hit submit and enable billing now as you can see here i got a prompt saying that a confirmation email will be sent within 48 hours now usually when you're setting up a brand new billing profile with an already created payments profile you'll definitely get a confirmation email in less than 48 hours now in order for me to finish up this demo i'm gonna wait until the new billing account shows up and continue with the demo from then and so here i am back in the billing console and it only took about 20 minutes and the gcloud ace billing account has shown up and so with part of this demo what i wanted to show is how you can take a project and attach it to a different billing account and so currently my only project is attached to the my billing account so now if i wanted to change my first project to my gcloud ace dash billing account i can simply go over here to actions click on the hamburger menu and go to change billing here i'll be prompted to choose a billing account and i can choose g cloud a stash billing and then click on set account and there it is my first project is now linked to g cloud a stash billing so if i go back over to my billing accounts you can see here that my billing account currently has zero projects and g cloud a stash billing has one project now just as a quick note and i really want to emphasize this is that if you're changing a billing account for a project and you are a regular user you will need the role of the billing account administrator as well as the project owner role so these two together will allow a regular user to change a billing account for a project and so now what i want to do is i want to take the gcloud a stash billing and i want to close that account but before i do that i need to unlink this project and bring it back to another billing account which in this case would be my billing account so i'm going to go back up here to the menu click on my projects and we're going to do the exact same thing that we did before under actions i'm going to click on the hamburger menu and change billing i'm going to get the prompt again and under billing account i'm going to choose my billing account and then click on set account so as you can see the project has been moved to a different billing account i'm going to go back to my billing accounts and as you can see here the project is back to my billing account and so now that the project is unlinked from the gcloud a stash billing account i can now go ahead and close out that account now in order to do that i'm going to click on gcloud a stash billing i'm going to go down here on the hand menu all the way to the bottom to account management click on there and at the top here you will see close billing account i'm going to simply click on that and i'll get a prompt that i've spent zero dollars and is linked to zero projects now if i did have a project that was linked to this billing account i would have to unlink the project before i was able to close this billing account so as a failsafe i'm being asked to type close in order to close this billing account so i'm going to go ahead and do that now and click on close billing account just as a note google gives me the option to reopen this billing account in case i did this by mistake and i really needed it i can reopen this billing account so now moving back over to billing you'll see here that i'm left with my single billing account called my billing account with the one project that's linked to it and so that covers my demo on creating editing and closing a new billing account as well as linking and unlinking a project to and from a different billing account so i hope you found this useful and you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to be going over controlling costs in google cloud along with budget alerts i will be touching on all the available discounts the number of ways to control costs and go over budget alerts to get a more granular and programmatic approach so starting off i wanted to touch on committed use discounts now committed use discounts provide discounted prices in exchange for your commitment to use a minimum level of resources for a specified term the discounts are flexible cover a wide range of resources and are ideal for workloads with predictable resource needs when you purchase google cloud committed use discounts you commit to a consistent amount of usage for a one or three year period there are two commitment types available and as you can see here they are spend based and resource based commitment types and unlike most other providers the commitment fee is billed monthly so going over the specific commitment types i wanted to start off with spend based commitment now for spend based commitment you commit to a consistent amount of usage measured in dollars per hour of equivalent ondemand spend for a one or three year term in exchange you receive a discounted rate on the applicable usage your commitment covers so you can purchase committed use discounts from any cloud billing account and the discount applies to any eligible usage in projects paid for by that cloud billing account any overage is charged at the ondemand rate spend based commitments can give you a 25 discount off ondemand pricing for a oneyear commitment and up to a 52 discount off of ondemand pricing for a threeyear commitment now spendbased commitments are restricted to specific resources which is cloud sql database instances and google cloud vmware engine and this commitment applies to the cpu and memory usage for these available resources now the other committed use discount is the resourcebased commitment so this discount is for a commitment to spend a minimum amount for compute engine resources in a particular region resourcebased commitments are ideal for predictable workloads when it comes to your vms when you purchase a committed use contract you purchase compute resources such as vcpus memory gpus and local ssds and you purchase these at a discounted price in return for committing to paying for those resources for one or three years the discount is up to 57 percent for most resources like machine types or gpus the discount is up to 70 percent for memory optimized machine types and you can purchase a committed use contract for a single project or purchase multiple contracts which you can share across many project by enabling shared discounts and sharing your committed use discounts across all your projects reduces the overhead of managing discounts on a per project basis and maximizes your savings by pooling all of your discounts across your project's resource usage if you have multiple projects that share the same cloud billing account you can enable committed use discount sharing so all of your projects within that cloud billing account share all of your committed use discount contracts and so your sustained use discounts are also pooled at the same time so touching on sustained use discounts these are automatic discounts for running specific compute engine resources a significant portion of the billing month sustained use discounts apply to the general purpose compute and memory optimize machine types as well as sole tenant nodes and gpus again sustained use discounts are applied automatically to usage within a project separately for each region so there's no action required on your part to enable these discounts so for example when you're running one of these resources for more than let's say 25 percent of the month compute engine automatically gives you a discount for every incremental minute that you use for that instance now sustained use discounts automatically apply to vms created by both google kubernetes engine and compute engine but unfortunately do not apply to vms created using the app engine flexible environment as well as data flow and e2 machine types now to take advantage of the full discount you would create your vm instances on the first day of the month as discounts reset at the beginning of each month and so the following table shows the discount you get at each usage level of a vm instance these discounts apply for all machine types but don't apply to preemptable instances and so sustained use discounts can save you up to a maximum of a 30 percent discount so another great way to calculate savings in google cloud is by using the gcp pricing calculator this is a quick way to get an estimate of what your usage will cost on google cloud so the gcp pricing calculator can help you identify the pricing for the resources that you plan to use in your future architecture so that you are able to calculate how much your architecture will cost you this calculator holds the pricing for almost all resources encapsulated within gcp and so you can get a pretty good idea of what your architecture will cost you without having to find out the hard way this calculator can be found at the url shown here and i will include this in the lesson text below now moving right along to cloud billing budgets so budgets enable you to track your actual spend against your plan spend after you've set a budget amount you set budget alert threshold rules that are used to trigger email notifications and budget alert emails help you stay informed about how your spend is tracking against your budget this example here is a diagram of a budget alert notification and is the default functionality for any budget alert notifications now to get a little bit more granular you can define the scope of the budget so for example you can scope the budget to apply to the spend of an entire cloud billing account or get more granular to one or more projects and even down to a specific product you can set the budget amount to a total that you specify or base the budget amount on the previous month's spend when costs exceed a percentage of your budget based on the rules that you set by default alert emails are sent to billing account administrators and billing account users on the target cloud billing account and again this is the default behavior of a budget email notification now as said before the default behavior of a budget is to send alert emails to billing account administrators and billing account users on the target cloud billing account when the budget alert threshold rules trigger an email notification now these email recipients can be customized by using cloud monitoring to specify other people in your organization to receive these budget alert emails a great example of this would be a project manager or a director knowing how much spend has been used up in your budget and the last concept i wanted to touch on when it comes to cloud billing budgets is that you can also use pub sub for programmatic notifications to automate your cost control response based on the budget notification you can also use pub sub in conjunction with billing budgets to automate cost management tasks and this will provide a realtime status of the cloud billing budget and allow you to do things like send notifications to slack or disable billing to stop usage as well as selectively control usage when budget has been met and so these are all the concepts that i wanted to cover when it came to cloud billing budgets now i know this lesson may have been a bit dry and not the most exciting service to dive into but it is very important to know both for the exam and for your role as an engineer when it comes to cutting costs in environments where your business owners deem necessary and so that's all i had for this lesson so you can now mark this lesson as complete and please join me in the next one where i dive into the console and do some handson demos when it comes to committed use discounts budget alerts and editing budget alerts as well as adding a little bit of automation into the budgeting alerts welcome back in the last lesson i went over a few ways to do cost management and the behaviors of budget alerts in this lesson i will be doing a demo to show you committed use discounts and reservations along with how to create budget alerts and as well how to edit them so with that being said let's dive in so now i'm going to start off with committed use discounts in order to get there i'm going to find it in compute engine so i'm going to simply go up here on the top left hand corner back to the navigation menu i'm going to go down to compute engine and i'm going to go over here to committed use discounts and as we discussed earlier these commitments for compute engine are resource based and as you can see here we have hardware commitments and reservations now reservations i will get into just a little bit later but with regards to hardware commitments we're going to get into that right now and as expected i have no current commitments so i'm going to go up to purchase commitment and so i need to start off with finding a name for this commitment and so i'm going to name this commitment demo dash commitment it's going to ask me for a region i'm going to keep it in us central one with the commitment type here is where i can select the type of machine that i'm looking for so i can go into general purpose and 1 and 2 and 2d e2 as well as memory optimize and compute optimized and so i'm going to keep it at general purpose and one again the duration one or three years and we get down to cores i can have as many vcpus as i'd like so if i needed 10 i can do that and i'll get a popup here on the right showing me the estimated monthly total as well as an hourly rate for this specific vm with 10 cores i can also select the duration for three years and as expected i'll get a higher savings because i'm giving a bigger commitment so bring it back down to one year and let's put the memory up to 64 gigabytes here i can add gpus and i have quite a few to choose from as well as local ssds and here with the local ssds i can choose as many disks as i'd like as long as it's within my quota and each disk size is going to be 375 gigabytes so if you're looking into committed use discounts and using local ssds please keep that in mind again the reservation can be added here and i'll be getting into that in just a second and now i don't want to actually purchase it but i did want to show you exactly what a committed use discount would look like and how you would apply it again here on the right hand side it shows me the details of the estimated monthly total and the hourly rate so i'm going to go over here and hit cancel and if i were to have applied it the commitment would show up here in this table and give me all the specified configurations of that instance right here now touching on reservations reservations is when you reserve the vm instances you need so when the reservation has been placed the reservation ensures that those resources are always available for you as some of you might know when you go to spin up a new compute engine vm especially when it comes to auto scaling instance groups the instances can sometimes be delayed or unavailable now the thing with reservations is that a vm instance can only use a reservation if its properties exactly match the properties of the reservation which is why it's such a great pairing with committed use discounts so if you're looking to make a resourcebased commitment and you always want your instance available you can simply create a reservation attach it to the commitment and you will never have to worry about having the resources to satisfy your workload as they will always be there so again going into create reservation it'll show me here the name the description i can choose to use the reservation automatically or select a specific reservation the region and zone number of instances and here i can specify the machine type or specify an instance template and again this is another use case where if you need compute engine instances spun up due to auto scaling this is where reservations would apply so getting back to machine type i can choose from vcpus as well as the memory i can customize it i can add as many local ssds as my quotas will allow me and i can select my interface type and i'm going to cancel out of here now when it comes to committed use discounts and reservations as it pertains to the exam i have not seen it but since this is an option to save money i wanted to make sure that i included it in this lesson as this could be a great option for use in your environment so now that we covered resourcebased committed use discounts i wanted to move into spend based commitments and so where you would find that would be over in billing so again i'm going to go up to the navigation menu in the top left hand corner and go into billing now you'd think that you would find it here under commitments but only when you have purchased a commitment will it actually show up here but as you can see here it's prompting us to go to the billing overview page so going back to the overview page you'll find it down here on the right and so i can now purchase a commitment and as we discussed before a spend based commitment can be used for either cloud sql or for vmware engine i select my billing account the commitment name the period either one year or three years and it also shows me the discount which could help sway my decision as well as the region as well as the hourly ondemand commitment now you're probably wondering what this is and as explained here this commitment is based on the ondemand price and once this is all filled out the commitment summary will be populated and after you agree to all the terms and services you can simply hit purchase but i'm going to cancel out of here and so that is an overview for the spend based commitment and again these committed use discounts i have not seen on the exam but i do think that it's good to know for your daytoday environment if you're looking to save money and really break down costs so now that i've covered committed use discounts and reservations i wanted to move over to budgets and budget alerts and because i'm already on the billing page all i need to do is go over here to the left hand menu and click on budgets and alerts now setting up a budget for yourself for this course would be a great idea especially for those who are cost conscious on how much you're spending with regards to your cloud usage and so we're to go ahead and create a new budget right now so let's go up here to the top to create budget and i'm going to be brought to a new window where i can put in the name of the budget and i'm going to call this ace dash budget and because i want to monitor all projects and all products i'm going to leave this as is but if you did have multiple projects you could get a little bit more granular and the same thing with products so i'm going to go ahead and leave it as is and just click on next now under budget type i can select from either a specified amount or the last month's spend and so for this demo i'm going to keep it at specified amount and because i want to be really conscious about how much i spend in this course i'm going to put in 10 for my target amount i'm going to include the credits and cost and then i'm going to click on next now these threshold rules are where billing administrators will be emailed when a certain percent of the budget is hit so if my spend happens to hit five dollars because i am a billing administrator i will be sent an email telling me that my spend has hit five dollars i also have the option of changing these percentages so if i decided to change it to forty percent now my amount goes to four dollars and this is done automatically so no need to do any calculations but i'm going to keep this here at 50 percent and vice versa if i wanted to change the amount the percentage of budget will actually change now with the trigger i actually have the option of selecting forecasted or actual and so i'm going to keep it on actual and if i want i can add more threshold rules now i'm going to leave everything as is and just click on finish and now as you can see here i have a budget name of ace budget now because the budget name doesn't have to be globally unique in your environment you can name your budget exactly the same and again it'll give me all the specific configurations that i filled out shows me how much credits i've used and that's it and that's how you would create a budget alert now if i needed to edit it i can always go back to ace budget and here i can edit it but i'm not going to touch it and i'm just going to hit cancel and so the last thing i wanted to show you before we end this lesson is how to create another budget but being able to send out the trigger alert emails to different users and so in order to do that i'm going to go back up here to create budget i'm going to name this to ace dash budget dash users i'm going to leave the rest as is i'm going to click on next again i'm going to leave the budget type the way it is the target amount i'm going to put ten dollars leave the include credits and cost and just click on next and so here i'm going to leave the threshold rules the way they are and right here under manage notifications i'm going to click off link monitoring email notification channels to this budget now because the email notification channel needs cloud monitoring in order to work i am prompted here to select a workspace which is needed by cloud monitoring so because i have none i'm going to go ahead and create one and so clicking on managing monitoring workspaces will bring you to the documentation but in order for me to get a workspace created i need to go to cloud monitoring now workspace is the top level container that is used to organize and control access to your monitoring notification channels in order for your notification channels to work they must belong to a monitoring workspace so you need to create at least one workspace before adding monitoring notification channels and don't worry we'll be getting into greater depth with regards to monitoring in a later section in this course so i'm going to go ahead and cancel this and i'm going to go up to the navigation menu click on there and scroll down to monitoring and then overview and this may take a minute to start up as the apis are being enabled and the default workspace for cloud monitoring is being built okay and now that the monitoring api has been enabled we are now in monitoring the workspace that was created is my first project so now that we have our monitoring workspace created i need to add the emails to the users that i want the alerts to be sent out to and added to the notification channel so in order to do that i'm going to go over here to alerting and up here at the top i'm going to click on edit notification channels and here as you can see are many notification channels that you can enable by simply clicking on add new over here on the right so now what i'm looking for is under email i'm going to click on add new now here i can add the new email address and so for me i'm going to add antony at antonyt.com and you can add whatever email address you'd like and under display name i'm going to add billing admin notification and just click on save and as you can see my email has been added to the notification channel and so this is all i needed to do in order to move on to the next step and so now that i've covered creating my monitoring workspace as well as adding another email to my email notification channels i can now go back to billing and finish off my budget alert let's go over here to budgets and alerts create budget and we're gonna go through the same steps call this billing alert users leave everything else as is and click on next i'm just going to change the target amount to 10 click on next i'm going to leave everything here as is and i'm going to go back to click on link monitoring email notification channels to this budget now if you notice when i click on select workspace my first project shows up and here it will ask me for my notification channels and because i've already set it up i can simply click on it and you'll see the billing admin notification channel and so if i didn't have this set up i can always go to manage notification channels and it'll bring me back to the screen which you saw earlier and so now that that's set up i can simply click on finish and so now that i have a regular budget alert i also have another budget alert that can go to a different email so if you have a project manager or a director that you want to send budget alerts to this is how you would do it and so that about covers this demo on committed use discounts reservations budgets and budget alerts and so that's all i wanted to cover for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back in this short lesson i will be covering the exporting of your billing data so that you're able to analyze that data and understand your spend at a more granular level i will also be going through a short demo where i will show you how to enable the export billing feature and bring it into bigquery to be analyzed now cloud billing export to bigquery enables you to export granular google cloud billing data such as usage cost details and pricing data automatically to a bigquery data set that you specify then you can access your cloud billing data from bigquery for detailed analysis or use a tool like data studio to visualize your data just a quick note here that billing export is not retroactive and this should be taken into consideration when planning for analysis on this data and so there are two types of cloud billing data that you can export there's the daily cost detail data and the pricing data and these can be selected right within the console depending on your use case and so now that we've gone through exactly what billing export is i wanted to get into a demo and show you how to export your cloud billing data to bigquery and go through all the necessary steps to get it enabled so when you're ready join me in the console and so here we are back in the console and so in order to enable billing export i'm going to be going to the billing page so i'm going to move up to the top left hand corner to the navigation menu and click on billing here in the left hand menu you'll see billing export and you can just click on there and so for those just coming to billing export for the first time there's a quick summary of exactly what the bigquery export is used for and as we discussed earlier there is an option for the daily cost detail and for pricing and i'm going to use the daily cost detail in this demo and export that data to bigquery so the first step i'm going to do is to click on edit settings and it's going to bring me to a new page where it will ask me for my project and this is where my billing data is going to be stored but as you can see here i'm getting a prompt that says you need to create a bigquery data set first now the bigquery data set that is asking for is where the billing data is going to be stored so in order to move forward with my billing export i need to go to bigquery and set up a data set so i'm going to simply click on this button here that says go to bigquery and it's going to bring me to the bigquery page where i'll be prompted with a big welcome note you can just click on done and over here in the right hand side where it says create data set i'm just going to click on there and i'm going to create my new data set and so for my data set id i'm going to call this billing export and just as a note with the data set id you can't use any characters like hyphens commas or periods and therefore i capitalize the b and the e now with the data location the default location is the us multi region but i can simply click on the drop down and have an option to store my data in a different location but i'm going to keep it at default i have the option of expiring this table in either a certain amount of days or to never expire as well when it comes to encryption i'm going to leave it as google manage key as opposed to a customer manage key and i'll get into encryption and key management a little later on in this course i'm going to go ahead and move right down to the bottom and click on create data set and now my data set has been created i can now see it over here on the left hand side menu where subtle poet 28400 is the id for my project if i simply click on the arrow beside it it'll show my billing export data set because there's nothing in it nothing is showing and so now that the data set is set up i can now go back to the billing export page and finish setting up my billing export so with that being said i'm going to go back up to the navigation menu head over to billing and go to billing export under daily cost detail i'm going to click on edit settings and because i have a data set already set up and since it's the only one it has been propagated in my billing export data set field if i had more data sets then i would be able to select them here as well so i'm going to leave the data set at billing export and simply click on save and so now that billing export has been enabled i'll be able to check on my billing as it is updated each day as it says here and to go right to the data set i can simply click on this hot link and it'll bring me right to bigquery and so there is one last step that still needs to be done to enable the billing export to work and that is to enable the bigquery data transfer service api so in order to do that we need to go back to the navigation menu go into apis and services into the dashboard and now i'm going to do a search for the bigquery data transfer service and i'm going to simply go up here to the top search bar and simply type in bigquery and here it is bigquery data transfer api i'm going to simply click on that and hit enable and this might take a minute and you may be asked to create credentials over here on the top right and you can simply ignore that as they are not currently needed and so now that the bigquery data transfer service api has been enabled i'm now able to go over to bigquery and take a look at my billing export data without any issues now it's going to take time to propagate but by the time i come here tomorrow the data will be fully propagated and i'll be able to query the data as i see fit and so although this is a short demo this is necessary to know for the exam as well being an engineer and looking to query your billing data you will now have the knowledge in order to take the steps necessary that will allow you to do so and so that's all i have for this lesson and demo on export billing data so you can now mark this lesson as complete and let's move on to the next one welcome back in this handson demo i'm going to go over apis in google cloud now the google cloud platform is pretty much run on apis whether it's in the console or the sdk under the hood it's hitting the apis now some of you may be wondering what is an api well this is an acronym standing for application programming interface and it's a standard used amongst the programming community in this specific context it is the programming interface for google cloud services and as i said before both the cloud sdk and the console are using apis under the hood and it provides similar functionality now when using the apis directly it allows you to enable automation in your workflow by using the software libraries that you use for your favorite programming language now as seen in previous lessons to use a cloud api you must enable it first so if i went to compute engine or when i was enabling monitoring i had to enable the api so no matter the service you're requesting here in google cloud and some of them may be even linked together it always has to be enabled in order to use it now getting a little bit more granular when using an api you need to have a project so when you enable the api you enable it for your project using the permissions on the project and permissions on the api to enable it now since this is a demo i want to go over to the navigation menu and go straight into apis and services and so here is the dashboard of the apis and services you can see the traffic here the errors and the latency with regards to these apis as well up here it has a time frame for the median latency that you can select for a more granular search now when it comes to what is enabled already you can see list here of the apis that are enabled and since we haven't done much there's only a few apis that are enabled now this handson demo is not meant to go into depth with apis but is merely an overview so that you understand what the apis are used for in context with google cloud if you'd like to go more in depth with regards to apis and possibly get certified in it the apogee certification with its corresponding lessons would be a great way to get a little bit more understanding but for this demo we're going to stick with this overview and so in order to search for more apis that need to be enabled or if you're looking for something specific you can come up here to enable apis and services or you can do a quick search on the search bar at the top of the page but just as a quick glance i'm going to go into enable apis and services and so you will be brought to a new page where you will see the api library on the left you will see a menu where the apis are categorized and all the apis that are available when it comes to google cloud and other google services so as you saw before when i needed to enable the api for bigquery i would simply type in bigquery and i can go to the api and since the api is enabled there's nothing for me to do but if i needed to enable it i could do that right there and just as a quick note when going to a service that's available in the console the api automatically gets enabled when you go and use it for the first time and so again this is just a quick overview of apis and the api library with regards to google cloud a short yet important demo to understand the under workings of the cloud sdk and the console so just remember that when using any service in google cloud again you must enable the api in order to start using it and so that about wraps up this demo for cloud apis so you can now mark this lesson as complete and let's move on to the next one welcome back in this demo i'll be creating and setting up a new gmail user as an admin user for use moving ahead in this course as well as following google's best practices we need a user that has lesser privileges than the user account that we set up previously and i'll be going through a full demo to show you how to configure it now in a google cloud setup that uses a g suite or cloud identity account a super administrator account is created to administer the domain this super admin account has irrevocable administrative permissions that should not be used for daytoday administration this means that no permissions can be taken away from this account and has the power to grant organization admin role or any other role for that matter and recover accounts at the domain level which makes this account extremely powerful now since i do not have a domain setup or using a g suite or cloud identity account i don't need to worry about a super admin account in this specific environment as gmail accounts are standalone accounts that are meant to be personal and hold no organization and usually start at the project level and so to explain it in a bit more detail i have a diagram here showing the two different accounts i will be using and the structure behind it now as we discussed before billing accounts have the option of paying for projects in a different organization so when creating new projects using the two different gmail accounts they were created without any organization and so each account is standalone and can create their own projects now what makes them different is that the antony gcloud ace account owns the billing account and is set as a billing account administrator and the tony bowtie ace account is a billing account user that is able to link projects to that billing account but does not hold full access to billing so in the spirit of sticking to the principle of lease privilege i will be using the tony bowtie ace account that i had created earlier with lesser privileges on billing it will still give me all the permissions i need to create edit and delete resources without all the powerful permissions needed for billing i will be assigning this new gmail user the billing account user role and it will allow you to achieve everything you need to build for the remainder of the course so just as a review i will be using a new google account that i have created or if you'd like you can use a preexisting google account and as always i recommend enabling twostep verification on your account as this user will hold some powerful permissions to access a ton of different resources in google cloud so now that we've gone over the details of the what and why for setting up this second account let's head into the demo and get things started so whenever you're ready join me over in the console and so here i am back in the console and so before switching over to my new user i need to assign the specific roles that i will need for that user which is the billing account user role so to assign this role to my new user i need to head over to billing so i'm going to go back up here to the lefthand corner and click on the navigation menu and go to billing again in the lefthand menu i'm going to move down to account management and click on there and over here under my billing account you will see that i have permissions assigned to one member of the billing account administrator and as expected i am seeing anthony g cloud ace gmail.com and so i want to add another member to my billing account so i'm going to simply click on add members and here i will enter in my new second user which is tony bowtie ace gmail.com and under select a role i'm going to move down to billing and over to billing account user and as you can see here this role billing account user will allow permissions to associate projects with billing accounts which is exactly what i want to do and so i'm going to simply click on that and simply click on save and so now that i've assigned my second user the proper permissions that i needed i am now going to log out and log in as my new user by simply going up to the right hand corner in the icon clicking on the icon and going to add account by adding the account i'll be able to switch back and forth between the different users and i would only recommend this if you are the sole user of your computer if you are on a computer that has multiple users simply sign out and sign back in again with your different user and here i'm asked for the email which would be tony bowtie ace gmail.com i'm gonna plug in my password and it's going to ask me for my twostep verification i'm going to click on yes and i should be in and because it's my first time logging into google cloud with this user i get a prompt asking me to agree to the terms of service i'm going to agree to them and simply click on agree and continue and so now i'm going to move back up to overview and as you can see here i don't have the permissions to view costs for this billing account and so all the permissions assigned for the billing account administrator which is antony g cloud ace is not applied to tony bowtie ace and therefore things like budgets and alerts even billing exports i do not have access to so moving forward in the course if you need to access anything in billing that you currently don't have access to like budgets and alerts you can simply switch over to your other account and take care of any necessary changes but what i do have access to is if i go up here to my billing account click on the drop down menu and click on manage billing accounts but as you can see here i do have access to view all the billing accounts along with the projects that are linked to them now because these gmail accounts are standalone accounts this project here that is owned by antony gcloud ace i do not have access to in order to access the project i would have to have permissions assigned to me directly in order for me to actually view the project or possibly creating any resources within that project now if i go back to my home page i can see here that i have no projects available and therefore no resources within my environment and so to kick it off i'm going to create a new project and so under project name i am going to call this project tony and you can name your project whatever you'd like under location i don't have any organization and so therefore i'm just going to click on create and this may take a minute to create and here we are with my first project named project tony as well as my notification came up saying that my project has been created and so now that this project has been created it should be linked to my billing account so in order to verify this i'm going to go over into billing and under the drop down i'm going to click on manage billing accounts and as you can see here the number of projects has gone from one to two and if i click on the menu up here under my projects you can see that project tony is a project that is linked to my billing account i also have the permissions to either disable billing or change billing for this specific project yet in order to change billing i will have to have another billing account but there are no other billing accounts available and so moving forward i will only have this one billing account and so any projects i decide to create will be linked to this billing account and so this is a great example of trimming down the permissions needed for different users and even though this is not a domain owned account but a personal account it's always recommended to practice the principle of lease privilege whenever you come across assigning permissions to any user now as i said before any billing related tasks that you decide to do moving forward you can simply switch over to your other user and do the necessary changes and so that's all i have for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back in this short lesson i'm going to be covering an overview of the cloud sdk and the command line interface as it is an essential component of interacting with google cloud for the exam you will need to get familiar with the command line and the commands needed in order to create modify and delete resources this is also an extremely valuable tool for your tool belt in the world of being a cloud engineer as i have found that is a very common and easy way to implement small operations within google cloud as well as automating the complex ones so what exactly is the cloud sdk well the cloud sdk is a set of command line tools that allows you to manage resources through the terminal in google cloud and includes commands such as gcloud gsutil bq and cubectl using these commands allow you to manage resources such as compute engine cloud storage bigquery kubernetes and so many other resources these tools can be run interactively or through automated scripts giving you the power and flexibility that you need to get the job done the cloud sdk is so powerful that you can do everything that the console can do yet has more options than the console you can use it for infrastructure as code autocompletion helps you finish all of your command line statements and for those of you who run windows the cloud sdk has got you covered with availability for powershell now in order to access google cloud platform you will usually have to authorize google cloud sdk tools so to grant authorization to cloud sdk tools you can either use a user account or a service account now a user account is a google account that allows end users to authenticate directly to your application for most common use cases on a single machine using a user account is best practice now going the route of a service account this is a google account that is associated with your gcp project and not a specific user a service account can be used by providing a service account key to your application and is recommended to script cloud sdk tools for use on multiple machines now having installed the cloud sdk it comes with some builtin commands that allow you to configure different options using gcloud init this initializes and authorizes access and performs other common cloud sdk setup steps using some optional commands gcloud auth login authorizes your access for gcloud with google user credentials and sets the current account as active gcloud config is another optional configuration that allows you to configure accounts and projects as well gcloud components allow you to install update and delete optional components of the sdk that give you more flexibility with different resources now after having installed the cloud sdk almost all gcloud commands will follow a specific format shown here is an example of this format and is broken down through component entity operation positional arguments and flags and i'll be going through some specific examples in the demonstration a little bit later on and so that's all i wanted to cover in this overview of the cloud sdk and the cli so you can now mark this lesson as complete and you can join me in the next one where i go ahead and demonstrate installing the cloud sdk back in this demonstration i will show you how to download install and configure the cloud sdk and i will be using the quick start guide that lies in the cloud sdk documentation which holds all the steps for installing the cloud sdk on different operating systems and i will make sure to include it in the lesson text below this demo will show you how to install the cloud sdk on each of the most common operating systems windows mac os and ubuntu linux all you need to do is follow the process on each of the pages and you should be well on your way so with that being said let's get this demo started and bring the cloud sdk to life by getting it all installed and configured for your specific operating system so as i explained before i'm gonna go ahead and install the cloud sdk on each of the three different operating systems windows mac os and ubuntu linux and i will be installing it with the help of the quick start guide that you see here and as i said before i'll be including this link in the lesson text and so to kick off this demo i wanted to start by installing the cloud sdk on windows so i'm going to move over to my windows virtual machine and i'm going to open up a browser and i'm going to paste in the link for the quick start guide and you can click on either link for the quick start for windows and each quick start page will give me the instructions of exactly what i need to do for each operating system so now it says that we need to have a project created which i did in the last lesson which is project tony so next i'm going to download the cloud sdk installer so i'm going to click on there and i'll see a prompt in the bottom left hand corner that the installer has been downloaded i'm going to click on it to open the file and i'm going to be prompted to go through this wizard and so i'm just going to click on next i'm going to agree to the terms of the agreement it's going to be for just me anthony and my destination folder i'll keep it as is and here's all the components that it's going to install i'm going to keep the beta commands unchecked as i don't really need them and if i need them later then i can install that component for those who are more experienced or even a bit curious you could click on the beta commands and take it for a test drive but i'm going to keep it off and i'm going to click install and depending on the power of your machine it should take anywhere from two to five minutes to install and the google cloud sdk has been installed and so i'm just going to click on next and as shown here in the documentation you want to make sure that you have all your options checked off is to create a start menu shortcut a desktop shortcut you want to start the google cloud sdk shell and lastly you want to run gcloud init in order to initialize and configure the cloud sdk now i'm going to click on finish to exit the setup and i'm going to get a command shell that pops up and i'm just going to zoom in for better viewing and so it says here my current configuration has been set to default so when it comes to configuration this is all about selecting the active account and so my current active account is going to be set as the default account it also needed to do a diagnostic check just to make sure that it can connect to the internet so that it's able to verify the account and so now the prompt is saying you must log in to continue would you like to log in yes you can just click on y and then enter and it's going to prompt me with a new browser window where i need to log in using my current account so that i can authorize the cloud sdk so i'm going to log in with my tony bowtie ace account click on next type in my password again it's going to ask me for my twostep verification and i'm going to get a prompt saying that the google sdk wants to access my google account i'm going to click on allow and success you are now authenticated with the google cloud sdk and if i go back to my terminal i am prompted to enter some values so that i can properly configure the google cloud sdk so i'm going to pick a cloud project to use and i'm going to use project tony that i created earlier so i'm going to enter 1 and hit enter and again whatever project that you've created use that one for your default configuration and it states here that my current project has been set to project tony and again this configuration is called default so if i have a second configuration that i wanted to use i can call it a different configuration but other than that my google cloud sdk is configured and ready to use so just to make sure that it's working i'm going to run a couple commands i'm going to run the gcloud help command and as you can see it's given me a list of a bunch of different commands that i can run and to exit you can just hit ctrl c i'm going to run gcloud config list and this will give me my properties in my active configuration so my account is tony bowtie ace gmail.com i've disabled usage reporting and my project is project tony and my active configuration is set as default now don't worry i'm going to be covering all these commands in the next lesson and i'm going to be going into detail on how you can configure and add other users within your cloud sdk configuration so as we go deeper into the course i'm going to be using a lot more command line just so you can get familiar with the syntax and become a bit more comfortable with it so now that i've installed the cloud sdk on windows the process will be a little bit different when it comes to installation on the other operating systems but will be very similar when it comes to the configuration so now let's head over to mac os and install the cloud sdk there and so here we are in mac os and so the first thing i want to do is i want to open up a web browser and i want to go to the cloud sdk quick start page so i'm just going to paste in the url here and we're looking for the quick start for mac os and so you can either click on the menu from the left hand side or the menu here on the main page and so like i said before this installation is going to be a little bit different than what it was in windows and so there's a few steps here to follow and so the first step asks us if we have a project already created which we've already done and is project tony and so the next step tells us that the cloud sdk requires python and so we want to check our system to see if we have a supported version so in order to check our version we're going to use this command here python minus v and i'm going to copy that to my clipboard and then open up a terminal and i'm going to zoom in for better viewing and so i'm going to paste the command in here and simply click on enter and as you can see here i'm running python 2.7 but the starred note here says that the cloud sdk will soon move to python 3 and so in order to avoid having to upgrade later you'd want to check your version for python 3 and so you can use a similar command by typing in python 3 space minus capital v and as you can see i'm running version 3.7.3 and so moving back to the guide i can see here that it is a supportive version if you do not have a supportive version i will include a link on how to upgrade your version in the lesson text below and so now that i've finished off this step let's move on to the next one where i can download the archive file for the google cloud sdk again most machines will run the 64bit package so if you do have the latest operating system for mac os you should be good to go so i'm going to click on this package and it'll start downloading for me and once it's finished you can click on downloads and click on the file itself and it should extract itself in the same folder with all the files and folders within it and so just as another quick note google prefers that you keep the google cloud sdk in your home directory and so following the guide i'm going to do exactly that and so the easiest way to move the folder into your home directory is to simply drag and drop it into the home folder on the left hand menu it should be marked with a little house icon and nested under favorites i can now move into my home folder and confirm that it is indeed in here and so now moving to the last step which shows as optional the guide asks us to install a script to add cloud sdk tools to our path now i highly recommend that you install this script so that you can add the tools for command completion and i will get into command completion a little bit later on in the next couple of lessons and so here is the command that i need to run so i'm going to copy that to my clipboard again and i'm going to move back over to my terminal i'm going to clear my screen and so to make sure i'm in my home directory where the cloud sdk folder is i'm going to simply type ls and so for those who don't know ls is a linux command that will list all your files and folders in your current path and as you can see here the google cloud sdk is in my path and therefore i can run that script so i'm going to paste it in here and i'm going to hit enter and so a prompt comes up asking me whether or not i want to disable usage reporting and because i want to help improve the google cloud sdk i'm going to type in y for yes and hit enter and so as i was explaining before the cloud sdk tools will be installed in my path and so this is the step that takes care of it and so i'm going to type y and enter for yes to continue and usually the path that comes up is the right one unless you've changed it otherwise so i'm going to leave this blank and just hit enter and that's it i've installed the tools so now in order for me to run gcloud init i have to start a new shell as it says here for the changes to take effect so i'm going to go up here to the top left hand menu click on terminal and quit terminal and so now i can restart the terminal again i'm going to zoom in for better viewing and now i'm able to run gcloud init in order to initialize the installation again the prompt to do the diagnostic tests and i can see i have no network issues but it shows me that i have to login to continue i would like to log in so i'm going to type y for yes and hit enter and so a new browser has popped open prompting me to enter my email and password and so i'm going to do that now i'm going to authorize my account with twostep verification i'm not going to save this password and yes i want to allow the google cloud sdk to access my google account so i'm going to click on allow and it shows that i've been authenticated so now i'm going to move back to my terminal and so just as a note before we move forward in case you don't get a browser popup for you to log into your google account you can simply highlight this url copy it into your browser and it should prompt you just the same so moving right ahead it shows that i'm logged in as tonybowtieace gmail.com which is exactly what i wanted and it's asking me to pick a cloud project to use now i want to use project tony so i'm going to type in 1 and enter and that's it the cloud sdk has been configured and just to double check i'm going to run the gcloud config list command to show me my configuration and as you can see here my account is tonybowties gmail.com my disable usage reporting is equal to false and my project is project tony and again my active configuration is set as default and so that about covers the cloud sdk install for mac os and so finally i'm going to move over to ubuntu linux and configure the cloud sdk there and so here we are in ubuntu and like i did in the other operating systems i'm going to open up the browser and i'm going to paste in the url for the quick start guide and so we want to click on the quick start for debian and ubuntu and so again you have your choice from either clicking on the link on the left hand menu or the one here in the main menu and so following the guide it is telling us that when it comes to an ubuntu release it is recommended that the sdk should be installed on an ubuntu release that has not reached end of life the guide also asks to create a project if we don't have one already which we have already done and so now we can continue on with the steps and so since we are not installing it inside a docker image we're gonna go ahead and use the commands right here now you can copy all the commands at once by copying this to the clipboard but my recommendation is to install each one one by one so i'm going to copy this and i'm going to open up my terminal i'm going to zoom in for better viewing and i'm going to paste that command in and click on enter it's going to prompt me for my password and it didn't come up with any errors so that means it was successfully executed and so i'm going to move on to the next command i'm going to copy this go back over to my terminal and paste it in now for those of you who do not have curl installed you will be prompted to install it and given the command to run it so i'm going to copy and paste this command and click on enter i'm going to type in y for yes to continue and it's going to install it after a couple of minutes okay now that curl has been installed i'm able to run that command again i'm going to clear the screen first and that executed with no errors as well and so now moving on to the last command this command will download and install the google cloud sdk i am prompted to install some packages and so i'm going to type y for yes to continue so now it's going to download and install the necessary packages needed for the google cloud sdk and depending on the speed of your internet and the speed of your machine this could take anywhere from two to five minutes okay and the google cloud sdk has been installed and so now that the cloud sdk has been installed we can now initialize the configuration so i'm going to type in gcloud init again the prompt with the network diagnostics i'm going to type y for yes to log in and i'm going to get the prompt for my email and password i'm going to take care of my twostep verification and i'm going to allow the google cloud sdk to access my google account and success i am now authenticated and moving back to the terminal just to verify it and again i'm going to pick project tony as the cloud project to use and the cloud sdk has been configured as always i'm going to do a double check by running a gcloud config list and as expected the same details has come up and so this is a quick run through on all three operating systems windows mac os and ubuntu linux on how to install the google cloud sdk and this will help you get started with becoming more familiar and more comfortable using the command line interface and so that about wraps up for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back in the last demo we went through a complete install of the cloud sdk and configured our admin account to be used within it in this demonstration i will be walking through how to manage the cloud sdk and this will involve how to utilize it and how to customize it to your environment as well as configuring our other user account so that we are able to apply switching configurations from one user to another and so i will be going through initializing and authorization configurations and properties installing and removing components as well as a full run through of the gcloud interactive shell so let's kick off this demo by diving into a preconfigured terminal with the sdk installed and configured with my second user tony bowtie ace gmail.com and so here i am in the mac os terminal and just be aware that it doesn't matter which operating system you're running as long as the sdk is installed and you have your user configured and so as you saw in the last lesson after you install the cloud sdk the next step is typically to initialize the cloud sdk by running the gcloud init command and this is to perform the initial setup tasks as well as authorizing the cloud sdk to use your user account credentials so that it can access google cloud and so in short it sets up a cloud sdk configuration and sets a base set of properties and this usually covers the active account the current project and if the api is enabled the default google compute engine region and zone now as a note if you're in a remote terminal session with no access to a browser you can still run the gcloud init command but adding a flag of dash dash console dash only and this will prevent the command from launching a browserbased authorization like you saw when setting up your last user so now even though i have a user already set up i can still run gcloud init and it will give me a couple different options to choose from so i can reinitialize this configuration with some new settings or i can create a new configuration now for this demo since we already have two users and to demonstrate how to switch between different users i want to create a new configuration with my very first user so i'm going to type in 2 and hit enter and it's going to ask me for a configuration name now it asks me for a configuration name because when setting up your first configuration it's set as default and because i know that this user account has full access to billing as well as administration privileges i'm going to call this configuration master and i'm going to hit enter it did the necessary network checks and now it's asking me for which account i want to use this configuration for now if tony bowtie ace had access to two different google cloud accounts i would be able to add a different configuration here and so because i'm going to log in with a new account i'm going to put in two and hit enter and so again it brought me to my browser window and i'm going to log in using another account and so here you can type in the first account that you created and for me it was antony gcloud ace gmail.com i hit next and i'm going to enter my password it's going to ask me for my twostep verification and i don't want to save this password and i'm going to allow the google cloud sdk to access my google account and i am now authenticated so moving back to the console you can see here that i am currently logged in and it's asking me to pick a cloud project to use now since i only have one project in that google cloud account which is subtle poet i'm going to choose one and since i have the compute engine api enabled i am now able to configure a default compute region and zone and so i'm going to hit y for yes to configure it and as you can see there are 74 different options to choose from and if you scroll up a little bit you should be able to find the zone that you're looking for and so for this course we are going to be using us central one dash a and so this is number eight so i'm going to scroll back down and type in eight and so now my master configuration has been configured with my antony g cloud ace account using us central 1a as the compute engine zone now touching back on authorization if i didn't want to set up a whole configuration i can simply type in gcloud auth login and this will allow me to authorize just the user account only so gcloud init would authorize access and perform the cloud sdk setup steps and gcloud auth login will authorize the access only now as i mentioned in a previous lesson you can use a service account for authorization to the cloud sdk tools and this would be great for a compute instance or an application but would need a service account key file in order to authorize it and so moving back to our user accounts when running the cloud sdk you can only have one active account at any given time and so to check my active account i can type in the command gcloud auth list and this will give me a list of all the accounts that have been authorized and so whenever you run a gcloud init it will use that account as the active account and as you can see here the antony gcloud ace gmail.com has a star beside it and this is marked as the active account and so in essence the account with the star beside it is the active account and so i'm looking to change my active account back to tony bowtie ace and in order for me to do that the command is conveniently shown here and so i'm going to go ahead and run that and the account would be the user shown above and so when i do a gcloud auth list i can see that my active account is now back to tony bowtie bowtieace gmail.com now if you wanted to switch the account on a per command basis you can always do that using the flag dash dash account after the command and put in the user account that you want to use and so let's say i wanted to revoke credentials from an account that i don't need anymore i can simply use the command gcloud auth revoke followed by the username and it will revoke the credentials for that account and so doing this would remove your credentials and any access tokens for any specific account that you choose that's currently on your computer and so if we're looking for that specific account we can always use the gcloud info command and it will give us the path for the user config directory and it is this directory that holds your encrypted credentials and access tokens alongside with your active configurations and any other configurations as well now as you can see here running the gcloud info command will also give you some other information everything from the account the project the current properties and where the logs can be found so now moving on to configurations a configuration is a named set of gcloud cli properties and it works kind of like a profile and so earlier on i demonstrated how to set up another configuration through gcloud init so now if i run a gcloud config list command it would give me all the information of the active configuration so as you can see here my user has changed but my configuration has stayed the same now as seen previously in a different lesson tony bow tie ace does not have access to the project subtle poet this project belongs to antony g cloud ace and the configuration was set for that account now if tony bowtie ace did have access to the subtle poet project then i could use this configuration but it doesn't and so i want to switch back to my other configuration and how i would do this is type in the command gcloud config configurations activate and the configuration that i set up for tony bowtie ace is the default configuration and so now that it has been activated i can now run a gcloud config list and as you can see here the configuration is back to default setup during the initialization process for tony bowtie ace now if i wanted to create multiple configurations for the same user account i can simply type in the command gcloud config configurations create but if i wanted to just view the configuration properties i can always type in the command gcloud config configurations describe and as you can see after the describe i needed the configuration name to complete the command and so i'm going to do that now and i've been given all the properties for this configuration now another thing that i wanted to share when it comes to properties is that you can change the project or the compute region and zone by simply typing in the command gcloud config set now if i wanted to change the project i can simply type in project and the project name if it was for the compute instance i can simply type in compute forward slash zone for the specific zone and just as a note only the properties that are not in the core property section are the ones that can be set as well when you are setting the properties this only applies to the active configuration if you want to change the configuration of one that is not active then you'd have to switch to it and run the gcloud config set command and so moving on i wanted to touch on components which are the installable parts of the sdk and when you install the sdk the components gcloud bq gsutil and the core libraries are installed by default now you probably saw a list of components when you ran the gcloud init command and so to see all the components again you can simply type in the gcloud components list command and if you scroll up you're able to see all the components that are available that you can install at your convenience and so if i wanted to install the cubectl component i can type in the command gcloud components install cubectl and a prompt will come up asking me if i want to continue with this i want to say yes and now it will go through the process of installing these components and so just to verify if i run the command gcloud components list you can see here that i have the cube ctl component installed now if i wanted to remove that component i can simply type in gcloud components remove and then the component that i want to remove which is cubectl i'm going to be prompted if i want to do this i'm going to say yes and it's going to go through the stages of removing this component and it's been successfully uninstalled and so if you're working with a resource that you need a component for you can simply install or uninstall it using the gcloud components command and so one last thing about components before we move on is that you can update your components to make sure you have the latest version and so in order to update all of your installed components you would simply run the command gcloud components update and so before i go ahead and finish off this demonstration i wanted to touch on the gcloud interactive shell the gcloud interactive shell provides a richer shell experience simplifying commands and documentation discovery with as you type autocompletion and help text snippets below it produces suggestions and autocompletion for gcloud bq gsutil and cubectl command line tools as well as any command that has a man page sub commands and flags can be completed along with online help as you type the command and because this is part of the beta component i need to install it and so i'm going to run the command gcloud components install beta and i want to hit yes to continue and this will go ahead and kick off the installation of the gcloud beta commands and so now that it's installed i'm going to simply clear the screen and so now in order to run the gcloud interactive shell i need to run the command gcloud beta interactive and so now for every command that i type i will get auto suggestions that will help me with my commands and so to see it in all of its glory i'm going to start typing and as you can see it's giving me the option between g cloud or gsutil and i can use the arrow to choose either one and below it it'll also show me the different flags that i can use for these specific commands and how to structure them and so for now i'm going to run gsutil version minus l and as you can see here it's giving me all the information about this command and what it can do and so i'm going to hit enter and as you can see my gsutil version is 4.52 and along with the version number i'm also given all the specific information with regards to this gsutil version and this can be used with absolutely any command used on the google cloud platform and so i'm going to go ahead and do that again but running a different command so i'm just going to first clear the screen and i'm going to type gcloud compute instances and as you can see the snippet on the bottom of the screen is showing me not only the command and how it's structured but also the url for the documentation so continuing on gcloud compute instances i'm going to do a list and i'm going to filter it by using the flag dash dash filter and i'm going to filter the us east one a zone and i'm going to hit enter and as expected there are no instances in us east 1a and as you've just experienced this is a great tool and i highly recommend that you use it whenever you can now i know this is a lot to take in and a lot of these commands will not show up on the exam but again getting comfortable with the command line and the sdk will help you on your path to becoming a cloud engineer as well it will help you get really comfortable with the command line and before you know it you'll be running commands in the command line and prefer it over using the console and so that's all i have for this demo on managing the cloud sdk so you can now mark this lesson as complete and let's move on to the next one welcome back in this demonstration i'm going to be talking about the always available browserbased shell called cloud shell cloud shell is a virtual machine that is loaded with development tools and offers a persistent five gigabyte home directory that runs on google cloud cloud shell is what provides you command line access to your google cloud resources within the console cloud shell also comes with a builtin code editor that i will be diving into and allows you to browse file directories as well as view and edit files while still accessing the cloud shell the code editor is available by default with every cloud shell instance and is based on the open source editor thea now cloud shell is available from anywhere in the console by merely clicking on the icon showed here in the picture and is positioned in the top right hand corner of the console in the blue toolbar so let's get started with the cloud shell by getting our hands dirty and jumping right into it and so here we are back in the console and i am logged in as tony bowtie ace gmail.com and as you can see up here in the right hand corner as mentioned earlier you will find the cloud shell logo and so to open it up you simply click on it and it'll activate the cloud shell here at the bottom and because it's my first time using cloud shell i'll get this prompt quickly explaining an overview of what cloud shell is and i'm going to simply hit continue and i'm going to make the terminal a little bit bigger by dragging this line up to the middle of the screen and so when you start cloud shell it provisions an e2 small google compute engine instance running a debianbased linux operating system now this is an ephemeral preconfigured vm and the environment you work with is a docker container running on that vm cloud shell instances are provisioned on a per user per session basis the instance persists while your cloud shell session is active and after an hour of inactivity your session terminates and the vm is discarded you can also customize your environment automatically on boot time and it will allow you to have your preferred tools when cloud shell boots up so when your cloud shell instance is provision it's provisioned with 5 gigabytes of free persistent disk storage and it's mounted at your home directory on the virtual machine instance and you can check your disk storage by simply typing in the command df minus h and here where it shows dev disk by id google home part one it shows here the size as 4.8 gigabytes and this would be the persistent disk storage that's mounted on your home directory now if you've noticed it shows here that i'm logged in as tony bowtie ace at cloud shell and that my project id is set at project tony so the great thing about cloud shell is that you're automatically authenticated as the google account you're logged in with so here you can see i'm logged in as tony bowtie ace and so picture it like running gcloud auth login and specifying your google account but without having to actually do it now when the cloud shell is started the active project in the console is propagated to your gcloud configuration inside cloud shell so as you can see here my project is set at project tony now if i wanted to change it to a different project i could simply use the command stated up here gcloud config set project along with the project id and this will change me to a different project now behind the scenes cloud shell is globally distributed across multiple regions so when you first connect to cloud shell you'll be automatically assigned to the closest available region and thus avoiding any unnecessary latency you do not have the option to choose your own region and so cloud shell does that for you by optimizing it to migrate to a closer region whenever it can so if you're ever curious where your cloud shell session is currently active you can simply type in this command curl metadata slash compute metadata slash version one slash instance slash zone and this will give me the zone where my instance is located and as shown here it is in us east 1b now as you've probably been seeing every time i highlight something that there is a picture of scissors coming up the cloud shell has some automated and available tools that are built in and so one of those available tools is that whenever i highlight something it will automatically copy it to the clipboard for me cloud shell also has a bunch of very powerful preinstalled tools that come with it such as the cloud sdk bash vim helm git docker and more as well cloud shell has support for a lot of major different programming languages like java go python node.js ruby and net core for those who run windows now if you're looking for an available tool that is not preinstalled you can actually customize your environment when your instance boots up and automatically run a script that will install the tool of your choice and the script runs as root and you can install any package that you please and so in order for this environment customization to work there needs to be a file labeled as dot customize underscore environment now if we do an ls here you can see that all we have is the readme dash cloud shell text file if we do ls space minus al to show all the hidden files as well you can see that the dot customize underscore environment file does not exist and this is because we need to create it ourselves and so for this example i want terraform installed as an available tool when my instance boots up and so i have to create this file so i'm going to do so by using the touch command and then the name of the file dot customize underscore environment hit enter and if i clear the screen and do another ls space minus al i can see that my dot customize underscore environment file has been created and so now i'm going to need the script to install terraform which means i would have to edit it and so another great feature of cloud shell is that it comes with a code editor and i can do it one of two ways i can either come up here and click on the open editor button which will open up a new tab or i can simply use the edit command with the file name and i'm going to do just that so edit dot customize underscore environment and i'm just going to hit enter and as you can see i got a prompt saying that it's unable to load the code editor and this is because when using code editor you need cookies enabled on your browser and because i am using a private browser session cookies are disabled and because my cloud shell environment persists i'm going to open up a regular browser window and i'm going to continue where i left off and so here i am back with a new browser window again logged in as tony bowtie ace and so just to show you the persistence that happens in cloud shell i'm going to run the command ls space minus al and as you can see here the customize environment is still here and so again i wanted to install terraform as an extra tool to have in my environment and so i'm going to open up the editor by typing in edit dot customize underscore environment and i'm going to hit enter and here is the editor that popped up as you can see here it's built with eclipse thea and this is an open source code editor that you can download from eclipse and this is what the editor is built on now this menu here on the left i can make it a little bit bigger and because the only viewable file on my persistent disk is the readme cloud shell dot text file i'm not able to see my dot customize underscore environment so in order to open it and edit it i'm going to go to the menu at the top of the editor and click on file open and here i'll be able to select the file that i need so i'm going to select customize environment and click on open and so i'm going to paste in my script to install terraform and i'm just going to paste in my script from my clipboard and i'll be including the script in the github repo for those of you who use terraform and i'm going to move over to the menu on the left click on file and then hit save and so now in order for me to allow this to work the customize environment needs to be loaded into my cloud shell so i'm going to have to restart it and so in order to accomplish this i'm going to move over to the menu on the right i'm going to click on the icon with the three dots and click on restart and you'll be presented with a prompt it's saying that it will immediately terminate my session and then a new vm will be provisioned for me and you'll also be presented with an optional response from google telling them why you're restarting the vm and this is merely for statistical purposes so i'm going to click on restart and i'm going to wait till a new cloud shell is provisioned and my new cloud shell is provisioned and up and running and so i want to double check to see if terraform has been installed so i'm going to go over here to the open terminal button on the right hand side toolbar and i'm going to move back to my terminal and i'm going to simply run the command terraform dash dash version and so it looks like terraform has been installed and as you can see i'm running version.12 but it says my terraform version is out of date and that the latest version is dot 13. and so because i really want to be up to date with terraform i want to be able to go into my customize environment file and edit my version of terraform so that when my cloud shell is initiated terraform.13 can be installed and so i'm going to simply type in the command edit dot customize underscore environment and i'm back to my editor and i'm going to change the terraform version from dot 12 to dot 13 and then go over here to the lefthand menu click on file and then save and now i'm going to restart my machine again and come back when it's fully provisioned and i'm back again my machine has been provisioned and i'm going to go back to my terminal by clicking on the open terminal button and so i'm going to type in the command terraform dash dash version and as you can see i'm at version dot 13 and i'm going to run a simple terraform command to see if it's working and as you can see i am successful in running terraform on cloud shell now customizing the environment is not on the exam but it is such an amazing feature that i wanted to highlight it for you with a real world example like terraform in case you're away from your computer and you're logged into a browser and you need some special tools to use in cloud shell this is the best way to do it now as i mentioned before the cloud sdk is preinstalled on this and so everything that i've showed you in the last lesson with regards to cloud sdk can be done in the cloud shell as well so if i run the command gcloud beta interactive i'd be able to bring up the interactive cloud shell and i'll be able to run the same commands so now if i go ahead and run the command gcloud components list i'll be able to see all the components installed and as you can see with the cloud shell there are more components installed than what's installed on the default installation of the sdk i can also run the gcloud config list command to see all the properties in my active configuration and so this goes to show you that the sdk installation that's on cloud shell is just as capable as the one that you've installed on your computer the only difference here is that the sdk along with all the other tools that come installed in cloud shell is updated every week and so you can always depend that they're up to date and so moving on to a few more features of cloud shell i wanted to point out the obvious ones up here in the cloud shell toolbar right beside the open terminal i can open brand new tabs opening up different projects or even the same project but just a different terminal and moving over to the right hand menu of cloud shell this keyboard icon can send key combinations that you would normally not have access to moving on to the gear icon with this you're able to change your preferences and looking at the first item on the list when it comes to color themes you can go from a dark theme to a light theme or if you prefer a different color in my case i prefer the dark theme as well you have the options of changing your text size we can go to largest but i think we'll just keep things back down to medium and as well we have the different fonts the copy settings from which i showed you earlier as well as keyboard preferences you also have the option of showing your scroll bar now moving on to this icon right beside the gear is the web preview button and so the web preview button is designed so that you can run any web application that listens to http requests on the cloud shell and be able to view it in a new web browser tab when running these web applications web preview also supports applications run in app engine now mind you these ports are only available to the secure cloud shell proxy service which restricts access over https to your user account only and so to demonstrate this feature i am going to run a simple http server running a hello world page so first i'm going to clear my screen and then i'm going to exit the interactive shell and again i'm going to paste in for my clipboard a simple script that will run my simple http server and as you can see it's running on port 8080 and now i'm able to click on the web preview button and i'm able to preview it on port 8080 and a new web browser tab will open up and here i'll see my hello world page now this is just a simple example and so i'm sure that many of you can find great use for this and so i'm going to stop this http server now by hitting ctrl c and just as a quick note web preview can also run on a different port anywhere from port 2000 all the way up to 65 000. now moving on to the rest of the features hitting on the more button here with the three dots starting from the top we covered restart earlier when we had to restart our cloud shell you're able to both upload and download a file within cloud shell when the demands are needed as well if i have a misconfigured configuration i can boot into safe mode and fix the issue instead of having to start from scratch again moving on to boost cloud shell also known as boost mode is a feature that increases your cloud shell vm from the default e2 small to an e2 medium so in essence a memory bump from 2 gigabytes to 4 gigabytes and once it's activated all your sessions will be boosted for the next 24 hours and just as a quick note enabling boost mode restarts your cloud shell and immediately terminates your session but don't worry the data in your home directory will persist but any of the processes that you are running will be lost now when it comes to usage quota cloud shell has a 50 hour weekly usage limit so if you reach your usage limit you'll need to wait until your quota is reset before you can use cloud shell again so it's always good to keep your eyes on this in case you're a heavy user of cloud shell and moving back to the menu again you have your usage statistics which collects statistics on commands that come preinstalled in the vm and you can turn them on or off and as well help for cloud shell is available here as well if you wanted to give feedback to the google cloud team with regards to cloud shell this is the place to do it and so one last thing about cloud shell before we end this demo is that if you do not access cloud shell for 120 days your home disk will be deleted now don't worry you'll receive an email notification before its deletion and if you just log in and start up a session you'll prevent it being removed now moving ahead in this course i will be using cloud shell quite a bit and so feel free to use either cloud shell or the cloud sdk installed on your computer or feel free to follow along with me in the cloud shell within your google cloud environment and so if you are following along please make sure that you keep an eye on your quota and so i hope this demonstration has given you some really good insight as to what you can do with cloud shell and its limitations and so that's pretty much all i wanted to cover in this demonstration of cloud shell so you can now mark this as complete and let's move on to the next one welcome back in this lesson and demonstration i am going to go over limits and quotas and how they affect your cloud usage within google cloud i'm going to quickly go over some theory followed by a demonstration on where to find the quotas and how to edit them accordingly so google cloud enforces quotas on resource usage for project owners setting a hard limit on how much of a particular google cloud resource your project can use and so there are two types of resource usage that google limits with quota the first one is rate quota such as api requests per day this quota resets after a specified time such as a minute or a day the second one is allocation quota an example is the number of virtual machines or load balancers used by your project and this quota does not reset over time but must be explicitly released when you no longer want to use the resource for example by deleting a gke cluster now quotas are enforced for a variety of reasons for example they protect other google cloud users by preventing unforeseen usage spikes quotas also help with resource management so you can set your own limits on service usage within your quota while developing and testing your applications each quota limit is expressed in terms of a particular countable resource from requests per day to an api to the number of load balancers used by your application not all projects have the same quotas for the same services and so using this free trial account you may have very limited quota compared to a higher quota on a regular account as well with your use of google cloud over time your quotas may increase accordingly and so you can also request more quota if you need it and set up monitoring and alerts and cloud monitoring to warn you about unusual quota usage behavior or when you're actually running out of quota now in addition to viewing basic quota information in the console google cloud lets you monitor quota usage limits and errors in greater depth using the cloud monitoring api and ui along with quota metrics appearing in the metrics explorer you can then use these metrics to create custom dashboards and alerts letting you monitor quota usage over time and receive alerts when for example you're near a quota limit only your services that support quota metrics are displayed and so popular supported services include compute engine data flow cloud spanner cloud monitoring and cloud logging common services that are not supported include app engine cloud storage and cloud sql now as a note be aware that quota limits are updated once a day and hence new limits may take up to 24 hours to be reflected in the google cloud console if your project exceeds a particular quota while using a service the platform will return an error in general google cloud will return an http 429 error code if you're using http or rest to access the service or resource exhausted if you're using grpc if you're using cloud monitoring you can use it to identify the quota associated with the error and then create custom alerts upon getting a quota error and we will be going into greater depth with regards to monitoring later on in the course now there are two ways to view your current quota limits in the google cloud console the first is using the quotas page which gives you a list of all of your project's quota usage and limits the second is using the api dashboard which gives you the quota information for a particular api including resource usage over time quota limits are also accessible programmatically through the service usage api and so let's head into the console where i will provide a demonstration on where to look for quotas and how to increase them when you need to and so here we are back in the console and so as i explained before there are two main ways to view your current quota limits in the console and so the first one is using the quotas page and so in order to get to the quotas page i need to go to iam so i'm going to do that now by going up to the navigation menu in the top left hand corner i'm going to go to i am and admin and over to quotas and so here i am shown all the quotas of the current apis that i have enabled as you can see here it shows me the service the limit name the quota status and the details in this panel here on the right hand side shows me a little bit more information with regards to the service and the quota itself and so let's say i wanted to increase my quota on the compute engine api within networks so i'm going to select this service and over here on the right hand panel i'm going to tick the box that says global and i'm going to go back over here to the top left and click on the edit quotas button and a panel will pop up and i am prompted to enter a new quota limit along with a description explaining to google why i need this quota limit increase and so once i've completed my request i can click on done and then submit request and like i said before once the request has been submitted it will go to somebody at google to evaluate the requests for approval and don't worry these quota limit increases are usually approved within two business days and can often times be sooner than that also a great way to enter multiple quota changes is to click on the selected apis let's do bigquery api and cloud data store api and so i've clicked off three and now i can go back up to the top and click on the edit quotas button and as you can see in the panel i have all three apis that i want to increase my quotas on so i can enter all my new limit requests for each api and then i can submit it as a bulk request with all my new quota limit changes and so doing it this way would increase the efficiency instead of increasing the quotas for each service one by one and because i'm not going to submit any quota changes i'm going to close this panel and so again using the quotas page will give you a list of all your project quota usage and its limits and allow you to request changes accordingly and so now moving on to the second way which you can view your current quota limits i'm going to go to the api dashboard which will give me a more granular view including the resource usage over time so to get there i'm going to go back up to the left hand side to the navigation menu i'm going to go to apis and services and click on dashboard and here i will see all the names of the apis and i'm going to click on compute engine api for this demonstration and over here on the left hand menu you will see quotas and in here as i said before you can get some really granular data with regards to queries read requests list requests and a whole bunch of other requests i'm going to drill down into queries here and i can see my queries per day per 100 seconds per user and per 100 seconds and i can see here that my queries per 100 seconds is at a limit of 2 000 so if i wanted to increase that limit i can simply click on the pencil icon and a panel on the right hand side will prompt me to enter a new quota limit but i currently see that my quota limit is at its maximum and that i need to apply for a higher quota so when i click on the link it will bring me back to my iam page where my services are filtered and i can easily find the service that i was looking at to raise my quota limit and i can increase the quota by checking off this box and clicking on the edit quotas button at the top of the page and so as you can see the quotas page as well as the api dashboard work in tandem so that you can get all the information you need with regards to quotas and limits and to edit them accordingly and so i hope this gave you a good idea and some great insight on how you can view and edit your quotas and quota limits according to the resources you use and so that about wraps up this brief yet important demo on limits and quotas so you can now mark this as complete and let's move on to the next section welcome back and in this section we're going to be going through in my opinion one of the most important services in google cloud identity and access management also known as iam for short and i'll be diving into identities roles and the architecture of policies that will give you a very good understanding of how permissions are granted and how policies are inherited so before i jump into i am i wanted to touch on the principle of least privilege just for a second now the principle of least privilege states that a user program or process should have access to the bare minimum privileges necessary or the exact resources it needs in order to perform its function so for example if lisa is performing a create function to a cloud storage bucket lisa should be restricted to create permissions only on exactly one cloud storage bucket she doesn't need read edit or even delete permissions on a cloud storage bucket to perform her job and so this is a great illustration of how this principle works and this is something that happens in not only google cloud but in every cloud environment as well as any onpremises environment so note that the principle of least privilege is something that i have previously and will continue to be talking about a lot in this course and this is a key term that comes up quite a bit in any major exam and is a rule that most apply in their working environment to avoid any unnecessary granted permissions a wellknown and unsaid rule when it comes to security hence me wanting to touch on this for a brief moment so now with that out of the way i'd like to move on to identity and access management or i am for short so what is it really well with iam you manage access control by defining who the identity has what access which is the role for which resource and this also includes organizations folders and projects in iam permission to access a resource isn't granted directly to the end user instead permissions are grouped into roles and roles are then granted to authenticated members an iam policy defines and enforces what roles are granted to which members and this policy is attached to a resource so when an authenticated member attempts to access a resource iam checks the resources policy to determine whether the action is permitted and so with that being said i want to dive into the policy architecture breaking it down by means of components in this policy architecture will give you a better understanding of how policies are put together so now what is a policy a policy is a collection of bindings audit configuration and metadata now the binding specifies how access should be granted on resources and it binds one or more members with a single role and any contact specific conditions that change how and when the role is granted now the metadata includes additional information about the policy such as an etag and version to facilitate policy management and finally the audit config field specifies the configuration data of how access attempts should be audited and so now i wanted to take a moment to dive deeper into each component starting with member now when it comes to members this is an identity that can access a resource so the identity of a member is an email address associated with a user service account or google group or even a domain name associated with a g suite or cloud identity domains now when it comes to a google account this represents any person who interacts with google cloud any email address that is associated with a google account can be an identity including gmail.com or other domains now a service account is an account that belongs to your application instead of an individual end user so when you run your code that is hosted on gcp this is the identity you would specify to run your code a google group is a named collection of google accounts and can also include service accounts now the advantages of using google groups is that you can grant and change permissions for the collection of accounts all at once instead of changing access one by one google groups can help you manage users at scale and each member of a google group inherits the iam roles granted to that group the inheritance means that you can use a group's membership to manage users roles instead of granting iam roles to individual users moving on to g suite domains this represents your organization's internet domain name such as antonyt.com and when you add a user to your g suite domain a new google account is created for the user inside this virtual group such as antony antonyt.com a g suite domain in actuality represents a virtual group of all of the google accounts that have been created like google groups g suite domains cannot be used to establish identity but they simply enable permission management now a cloud identity domain is like a g suite domain but the difference is that domain users don't have access to g suite applications and features so a couple more members that i wanted to address is the all authenticated users and the all users members the all authenticated users is a special identifier that represents anyone who is authenticated with a google account or a service account users who are not authenticated such as anonymous visitors are not included and finally the all users member is a special identifier that represents anyone and everyone so any user who is on the internet including authenticated and unauthenticated users and this covers the slew of the different types of members now touching on the next component of policies is roles now diving into roles this is a named collection of permissions that grant access to perform actions on google cloud resources so at the heart of it permissions are what determines what operations are allowed on a resource they usually but not always correspond onetoone with rest methods that is each google cloud service has an associated permission for each rest method that it has so to call a method the caller needs that permission now these permissions are not granted to the users directly but grouped together within the role you would then grant roles which contain one or more permissions you can also create a custom role by combining one or more of the available iam permissions and again permissions allow users to perform specific actions on google cloud resources so you will typically see a permission such as the one you see here compute.instances.list and within google cloud iam permissions are represented in this form service.resource.verb so just as a recap on roles this is a collection of permissions and you cannot grant a permission directly to the user but you grant a role to a user and all the permissions that the role contains so an example is shown here where the compute instances permissions are grouped together in a role now you can grant permissions by granting roles to a user a group or a service account so moving up into a more broader level there are three types of roles in iam there are the primitive roles the predefined roles and the custom roles with the primitive roles these are roles that existed prior to the introduction of iam and they consist of three specific roles owner editor and viewer and these roles are concentric which means that the owner role includes the permissions in the editor role and the editor role includes the permissions in the viewer role and you can apply primitive roles at the project or service resource levels by using the console the api and the gcloud tool just as a note you cannot grant the owner role to a member for a project using the iam api or the gcloud command line tool you can only add owners to a project using the cloud console as well google recommends avoiding these roles if possible due to the nature of how much access the permissions are given in these specific roles google recommends that you use predefined roles over primitive roles and so moving into predefined roles these are roles that give granular and finergrained access control than the primitive roles to specific google cloud resources and prevent any unwanted access to other resources predefined roles are created and maintained by google their permissions are automatically updated as necessary when new features or services are added to google cloud now when it comes to custom roles these are user defined and allow you to bundle one or more supported permissions to meet your specific needs unlike predefined roles custom roles are not maintained by google so when new permissions features or services are added to google cloud your custom roles will not be updated automatically when you create a custom role you must choose an organization or project to create it in you can then grant the custom role on the organization or project as well as any resources within that organization or project and just as a note you cannot create custom roles at the folder level if you need to use a custom role within a folder define the custom role on the parent of that folder as well the custom roles user interface is only available to users who have permissions to create or manage custom roles by default only project owners can create new roles now there is one limitation that i wanted to point out and that is that some predefined roles contain permissions that are not permitted in custom roles so i highly recommend that you check whether you can use a specific permission when making a custom role custom roles also have a really cool feature that includes a launch stage which is stored in the stage property for the role the stage is informational and helps you keep track of how close each role is to being generally available and these launch stages are available in the stages shown here alpha which is in testing beta which is tested and awaiting approval and of course ga which is generally available and i'll be getting handson later with these roles in an upcoming demonstration so now moving on to the next component is conditions and so a condition is a logic expression and is used to define and enforce conditional attributebased access control for google cloud resources conditions allow you to choose granting resource access to identities also known as members only if configured conditions are met for example this could be done to configure temporary access for users that are contractors and have been given specific access for a certain amount of time a condition could be put in place to remove the access they needed once the contract has ended conditions are specified in the role bindings of a resources im policy so when a condition exists the access request is only granted if the condition expression is true so now moving on to metadata this component carries both e tags and version so first touching on e when multiple systems try to write to the same im policy at the same time there is a risk that those systems might overwrite each other's changes and the risk exists because updating an im policy involves multiple operations so in order to help prevent this issue iam supports concurrency control through the use of an etag field in the policy the value of this field changes each time a policy is updated now when it comes to a version this is a version number that is added to determine features such as a condition and for future releases of new features it is also used to avoid breaking your existing integrations on new feature releases that rely on consistency in the policy structure also when new policy schema versions are introduced and lastly we have the auditconfig component and this is used in order to configure audit logging for the policy it determines which permission types are logged and what identities if any are exempted from logging and so to sum it up this is a policy in all its entirety each component as you can see plays a different part and i will be going through policies and how they are assembled in statements in a later lesson and so there is one more thing that i wanted to touch on before ending this lesson and that is the policy inheritance when it comes to resource hierarchy and so as explained in an earlier lesson you can set an im policy at any level in the resource hierarchy the organization level the folder level the project level or the resource level and resources inherit the policies of all their parent resources the effective policy for a resource is the union of the policy set on that resource and the policies inherited from higher up in the hierarchy and so again i wanted to reiterate that this policy inheritance is transitive in other words resources inherit policies from the project which inherit policies from folders which inherit policies from the organization therefore the organization level policies also apply at the resource level and so just a quick example if i apply a policy on project x on any resources within that project the effective policy is going to be a union of these policies as the resources will inherit the policy that is granted to project x so i hope this gave you a better understanding of how policies are granted as well as the course structure and so that's all i have for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i wanted to build on the last lesson where we went through iam and policy architecture and dive deeper into policies and conditions when it comes to putting them together in policy statements as cloud engineers you should be able to read and decipher policy statements and understand how they're put together by using all the components that we discussed earlier so just as a refresher i wanted to go over the policy architecture again now as i discussed previously a policy is a collection of statements that define who has what type of access it is attached to a resource and is used to enforce access control whenever that resource is accessed now the binding within that policy binds one or more members with a single role and any context specific conditions so in other words the member roles and conditions are bound together using a binding combined with the metadata and audit config we have a policy so now taking all of this and putting it together in a policy statement shown here you can see the bindings which have the role the members and conditions the first member being tony beauties gmail.com holding the role of storage admin and the second member as larkfetterlogin at gmail.com holding the role of storage object viewer now because lark only needs to view the files for this project in cloud storage till the new year a condition has been applied that does not grant access for lark to view these files after january the 1st an e tag has been put in and the version is numbered 3 due to the condition which i will get into a little bit later this policy statement has been structured in json format and is a common format used in policy statements moving on we have the exact same policy statement but has been formatted in yaml as you can see the members roles and conditions in the bindings are exactly the same as well as the etag and version but due to the formatting it is much more condensed so as you can see policy statements can be written in both json or yaml depending on your preference my personal preference is to write my policy statements in yaml due to the shorter and cleaner format so i will be moving ahead in this course with more statements written in yaml when you are looking to query your projects for its granted policies an easy way to do this would be to query it from the command line as shown here here i've taken a screenshot from tony bowtie ace in the cloud shell and have used the command gcloud projects get dash iam policy with the project id and this brought up all the members and roles within the bindings as well as the etag and version for the policy that has been attached to this project and as you can see here i have no conditions in place for any of my bindings and so again using the command gcloud projects get dash iam dash policy along with the project id will bring up any policies that are attached to this resource and the resource being the project id if the resource were to be the folder id then you could use the command gcloud resource dash manager folders get dash iampolicy with the folder id and for organizations the command would be gcloud organizations get dash iampolicy along with the organization id now because we don't have any folders or organizations in our environment typing these commands in wouldn't bring up anything and just as a note using these commands in the cloud shell or in the sdk will bring up the policy statement formatted in yaml so now i wanted to just take a second to dive into policy versions now as i haven't covered versions in detail i wanted to quickly go over it and the reasons for each numbered version now version one of the i am syntax schema for policies supports binding one role to one or more members it does not support conditional role bindings and so usually with version 1 you will not see any conditions version 2 is used for google's internal use and so querying policies usually you will not see a version 2. and finally with version 3 this introduces the condition field in the role binding which constrains the role binding via contact space and attributes based rules so just as a note if your request does not specify a policy version iam will assume that you want a version 1 policy and again if the policy does not contain any conditions then iam always returns a version one policy regardless of the version number in the request so moving on to some policy limitations each resource can only have one policy and this includes organizations folders and projects another limitation is that each iam policy can contain up to 1500 members and up to 250 of these members can be google groups now when making policy changes it will take up to seven minutes to fully propagate across the google cloud platform this does not happen instantaneously as iam is global as well there is a limit of 100 conditional role bindings per policy now getting a little bit deeper into conditions these are attributes that are either based on resource or based on details about the request and this could vary from time stamp to originating or destination ip address now as you probably heard me use the term earlier conditional role bindings are another name for a policy that holds a condition within the binding conditional role bindings can be added to new or existing iam policies to further control access to google cloud resources so when it comes to resource attributes this would enable you to create conditions that evaluate the resource in the access request including the resource type the resource name and the google cloud service being used request attributes allow you to manage access based on days or hours of the week a conditional role binding can be used to grant time bounded access to a resource ensuring that a user can no longer access that resource after the specified expiry date and time and this sets temporary access to google cloud resources using conditional role bindings in iam policies by using the date time attributes shown here you can enforce timebased controls when accessing a given resource now showing another example of a timebased condition it is possible to get even more granular and scope the geographic region along with the day and time for access in this policy lark only has access during business hours to view any objects within cloud storage lark can only access these objects from monday to friday nine to five this policy can also be used as a great example for contractors coming into your business yet only needing access during business hours now an example of a resourcebased condition shown here a group member has a condition tied to it where dev only access has been implemented any developers that are part of this group will only have access to vm resources within project cat bowties and tied to any resources that's name starts with the word development now some limitations when it comes to conditions is that conditions are limited to specific services primitive roles are unsupported and members cannot be of the all users or all authenticated users members conditions also hold a limit of 100 conditional role bindings per policy as well as 20 role bindings for the same role and same member and so for the last part of the policy statements i wanted to touch on audit config logs and this specifies the audit configuration for a service the configuration determines which permission types are logged and what identities if any are exempted from logging and when specifying audit configs they must have one or more audit log configs now as shown here this policy enables data read data write and admin read logging on all services while exempting tony bowtie ace gmail.com from admin read logging on cloud storage and so that's pretty much all i wanted to cover in this lesson on policies policy statements and conditions and so i highly recommend as you come across more policy statements take the time to read through it and get to know exactly what the statement is referring to and what type of permissions that are given and this will help you not only in the exam but will also help you in reading and writing policy statements in future and so that's all i have for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back and in this demonstration i'm going to do a handson tour working with iam here in the google cloud console we're going to go through the available services in the iam console as well as touching on the command line in the cloud shell to show how policies can be both added and edited we're also going to be bringing in another new user to really bring this demo to life and to show you how to edit existing policies so with that being said let's dive in so if i go over here to my user icon in the top right hand corner i can see that i am logged in as tony bowtie ace gmail.com and as you can see at the top i'm here in project tony so now to get to iam i'm going to go over to the navigation menu and i'm going to go to i am in admin and over to iam now moving over here to the menu on the left i wanted to go through the different options that we have in iam so under iam itself this is where you would add or edit permissions with regards to members and roles for the policy added to your given project which in my case is project tony and i'll be coming back in just a bit to go greater in depth with regards to adding and editing the policy permissions moving on to identity and organization now although we haven't touched on cloud identity yet i will be covering this in high level detail in a different lesson but for now know that cloud identity is google cloud's identity as a service solution and it allows you to create and manage users and groups within google cloud now if i was signed into cloud identity i would have a whole bunch of options here but since this is a personal account i cannot create or manage any users as well i do not have a domain tied to any cloud identity account as well as any g suite account so just know that if you had cloud identity or g suite set up you would have a bunch of different options to choose from in order to help you manage your users and groups and here under organization policies i'm able to manage organization policies but since i am not an organization policy administrator and i don't have an organization there's not much that i can do here just know that when you have an organization set up you are able to come here in order to manage and edit your organization policies now moving under quotas we went over this in a little bit of detail in a previous lesson and again this is to edit any quotas for any of your services in case you need a limit increase moving on to service accounts i will be covering this topic in great depth in a later lesson and we'll be going through a handson demonstration as well now i know i haven't touched much on labels as of yet but know that labels are a key value pair that helps you organize and then filter your resources based on their labels these same labels are also forwarded to your billing system so you can then break down your billing charges by label and you can also use labels based on teams cost centers components and even environments so for example if i wanted to label my virtual machines by environment i can simply use environment as the key and as the value i can use anything from development to qa to testing to production and i could simply add this label and add all the different environments and later i'd be able to query based on these specific labels now a good rule of thumb is to label all of your resources so that this way you're able to find them a lot easier and you're able to query them a lot easier so moving forward with any of your resources that you are creating be sure to add some labels to give you maximum flexibility so i'm going to discard these changes and we're going to move on to settings and we touched on settings in an earlier lesson with regards to projects and so here i could change the project name it'll give me the project id the project number and i'm able to migrate or shut down the project now when it comes to access transparency this provides you with logs that capture the actions that google personnel take when they're accessing your content for troubleshooting so they're like cloud audit logs but for google support now in order to enable access transparency for your google cloud organization your google cloud account must have a premium support plan or a minimum level of a 400 a month support plan and because i don't have this i wouldn't be able to enable access transparency now although access transparency is not on the exam this is a great feature to know about in case you are working in any bigger environments that have these support plans and compliance is of the utmost importance now moving into privacy and security this is where google supplies all of their clients of google cloud the compliance that they need in order to meet regulations across the world and across various industries such as health care and education and because google has a broad base in europe google provides capabilities and contractual commitments created to meet data protection recommendations which is why you can see here eu model contract clauses and eu representative contacts as well under transparency and control i'm able to disable the usage data that google collects in order to provide better data insights and recommendations and this is done at the project level and as well i have the option of going over to my billing account and i could select a different billing account that's linked to some other projects that you can get recommendations on and so continuing forward identity aware proxy is something that i will be covering in a later lesson and so i won't be getting into any detail about that right now and so what i really wanted to dig into is roles now this may look familiar as i touched on this very briefly in a previous lesson and here's where i can create roles i can create some custom roles from different selections and here i have access to all the permissions and if i wanted to i can filter down from the different types the names the permissions even the status so let's say i was looking for a specific permission and i'm looking all the permissions for projects this could help me find exactly what it is that i'm looking for and these filters allow me to get really granular so i can find the exact permission and so you can get really granular with regards to your permissions and create roles that are custom to your environment now moving on to audit logs here i can enable the auto logs without having to use a specific policy by simply clicking on default autoconfig and here i can turn on and off all the selected logging as well as add any exempted users now i don't recommend that you turn these on as audit logging can create an extremely large amount of data and can quickly blow through all of your 300 credit so i'm going to keep that off move back to the main screen of the audit logs and as well here i'm able to get really granular about what i want to log now quickly touching on audit logs in the command line i wanted to quickly open up cloud shell and show you an example of how i can edit the policy in order to enable audit logging just going to make this a little bit bigger and i'm going to paste in my command gcloud projects get dash iam dash policy with the project id which is project tony 286016 and i'm gonna just hit enter and as you can see here this is my current policy and as well as expected audit logs are not enabled due to the fact that the audit config field is not present so in order for me to enable the audit config logs i'm going to have to edit the policy and so the easiest way for me to do that is for me to run the same command and output it to a file where i can edit it and i'm going to call this new dash policy dot yaml and so now that my policy has been outputted to this file i'm going to now go into the editor and as you can see my new policy.yaml is right here and so for me to enable the autoconfig logs i'm going to simply append it to the file and then i'm going to go over here to the top menu and click on file and save and so now for me to apply this new policy i'm going to go back over to the terminal and now i'm going to paste in the command gcloud projects set dash iampolicy with the project id and the file name new dash policy dot yaml and i'm just going to hit enter and as you can see the audit log configs have been enabled for all services and because this may take some time to reflect in the console it will not show up right away but either way audit logs usually take up a lot of data and i don't want to blow through my 300 credit and so i'm going to disable them now the easiest way for me to do this is to output this policy to another file edit it and set it again and so i'm going to go ahead and do that i'm going to first clear the screen and then i'm going to paste in my command while outputting it to a new file called updated dash policy dot yaml and i'm gonna hit enter and now i'm gonna go into the editor so i can edit the file now the one thing i wanted to point out is that i could have overwritten the file new dash policy but if you look here in the updated policy the etag is different than the etag in the old policy and so this allowed me to highlight etags when it comes to editing and creating new policies and so when editing policies make sure that the etag is correct otherwise you will receive an error and not be able to set the new policy so going back to the updated policy file i'm going to take out the audit log configs and i'm going to leave the auto configs field there and i'm going to go to the menu click on file and then save now i'm going to go back to the terminal and i'm going to paste in the new command and this will update my policy and as you can see the audit config logs have been disabled and the policy has been updated now this is the same process that you can use when you want to update any parts of the policy when it comes to your members or roles and even adding any conditions so now moving on to the last item on the menu is groups and as you can see here because i do not have an organization i'm not able to view any groups and so if i did have an organization i could manage my groups right here in this page now moving back over to iam i wanted to dig into policies in a little bit of further detail now what we see here are the permissions and roles that have been granted to selected members in this specific project which is project tony now remember an im policy is a total collection of members that have roles granted to them in what's known as a binding and then the binding is applied to that layer and all other layers underneath it and since i'm at the project layer this policy is inherited by all the resources underneath it and so just to verify through the command line i'm going to open up cloud shell and i'm going to paste in the command gcloud projects get dash iampolicy with my project id and i'm going to hit enter and as you can see here the policy is a reflection of exactly what you see here in the console so as you can see here here's the service agent which you will find here and the other two service accounts which you will find above as well as tony bowtie ace gmail.com and all the other roles that accompany those members so as i mentioned earlier i've gone ahead and created a new user and so for those who are following along you can go ahead and feel free to create a new gmail user now going ahead with this demonstration the user i created is named laura delightful now tony needed an extra hand and decided to bring her onto the team from another department now unfortunately in order for laura to help tony on the project she needs access to this project and as you can see she doesn't have any access and so we're going to go ahead and change that and give her access to this project so i'm going to go back over to my open tab for tony bowtie ace and we're gonna go ahead and give laura permissions and so i'm gonna go ahead and click on this add button at the top of the page and the prompt will ask me to add a new member so i'm gonna add laura in here now and here she is and i'm going to select the role as project viewer i'm not going to add any conditions and i'm simply going to click on save and the policy has been updated and as you can see here laura has been granted the role of project viewer so i'm going to move over to the other open tab where laura's console is open and i'm going to simply do a refresh and now laura has access to view all the resources within project tony now laura is able to view everything in the project but laura isn't actually able to do anything and so in order for laura to get things done a big part of her job is going to be creating files with new ideas for the fall winter line of bow ties in 2021 and so because laura holds the project viewer role she is able to see everything in cloud storage but she is unable to create buckets to upload edit or delete any files or even folders and as you can see here there is a folder marked bowtie inc fallwinter 2021 ideas but laura cannot create any new buckets because she doesn't have the required permissions as well drilling down into this bucket laura is unable to create any folders as explained earlier and the same stands for uploading any files and so i'm going to cancel out of this and so in order to give laura the proper permissions for her to do her job we're going to give laura the storage admin role and so moving back over to the open console for tony bowtie i'm going to give laura access by using the command line so i'm going to go up to the top right and open up cloud shell and so the command i need to run to give laura the role of storage admin would be the following gcloud projects add dash iam dash policy dash binding with the project id dash dash member user followed by colon and then the user name which is laura delightful gmail.com dash dash role and the role which is storage admin and i'm going to go ahead and hit enter and as you can see it has been executed successfully so if i do a refresh of the web page here i'm going to be able to see the changes reflected in the console and after a refresh you can see here storage admin has been added to the role for laura delightful gmail.com and so if i go over to the open tab where laura has her console open i can simply do a refresh and if i go back to the home page for cloud storage you can see here that laura now has the permissions to create a bucket laura also now has permissions to create new folders create edit and delete new files on top of being able to create new storage buckets and so that about wraps up this demonstration on getting handson with iam in both the console and the command line and i also hope that this demo has given you a bit more confidence on working in the shell running the commands needed in order to create new bindings along with editing existing policies and this will get you comfortable for when you need to assign roles to new and existing users that are added to your gcp environment and so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to take a deep dive into service accounts now service accounts play a powerful part in google cloud and can allow a different approach for application interaction with the resources in google cloud now service accounts being both an identity and a resource can cause some confusion for some and so i really wanted to spend some time breaking it down for better understanding and so i'm first going to start off by explaining what exactly is a service account and so a service account is a special kind of account that is used by an application or a virtual machine instance and not a person an application uses the service account to authenticate between the application and gcp services so that the users aren't directly involved in short it is a special type of google account intended to represent a nonhuman user that needs to authenticate and be authorized to access data in google apis this way the service account is the identity of the service and the service accounts permissions control which resources the service can access and as a note a service account is identified by its email address which is unique to the account now the different service account types come in three different flavors user managed default and google managed service accounts when it comes to the user managed service accounts these are service accounts that you create you're responsible for managing and securing these accounts and by default you can create up to 100 user managed service accounts in a project or you can also request a quota increase in case you need more now when you create a user managed service account in your project it is you that chooses a name for the service account this name appears in the email address that identifies the service account which uses the following format seen here the service account name at the project id dot iam.gserviceaccount.com now moving on to the default service accounts when you use some google cloud services they create user managed service accounts that enable the service to deploy jobs that access other google cloud resources these accounts are known as default service accounts so when it comes to production workloads google strongly recommends that you create your own user managed service accounts and grant the appropriate roles to each service account when a default service account is created it is automatically granted the editor role on your project now following the principle of lease privilege google strongly recommends that you disable the automatic role grant by adding a constraint to your organization policy or by revoking the editor role manually the default service account will be assigned an email address following the format you see here project id at appspot.gserviceaccount.com for any service accounts created by app engine and project number dash compute at developer.gserviceaccount.com for compute engine and so lastly when it comes to google managed service accounts these are created and managed by google and they are used by google services the display name of most google managed service accounts ends with a gserviceaccount.com address now some of these service accounts are visible but others are hidden so for example google api service agent is a service account named with an email address that uses the following format project number at cloudservices.gerisa and this runs internal google processes on your behalf and this is just one example of the many google managed services that run in your environment and just as a warning it is not recommended to change or revoke the roles that are granted to the google api service agent or to any other google managed service accounts for that matter if you change or revoke these roles some google cloud services will no longer work now when it comes to authentication for service accounts they authenticate using service account keys so each service account is associated with two sets of public and private rsa key pairs that are used to authenticate to google they are the google manage keys and the user manage keys with the google manage keys google stores both the public and private portion of the key rotates them regularly and the private key is always held in escrow and is never directly accessible iam provides apis to use these keys to sign on behalf of the service account now when using user managed key pairs this implies that you own both the public and private portions of a key pair you can create one or more user managed key pairs also known as external keys that can be used from outside of google cloud google only stores the public portion of a user managed key so you are responsible for the security of the private key as well as the key rotation private keys cannot be retrieved by google so if you're using a user manage key please be aware that if you lose your key your service account will effectively stop working google recommends storing these keys in cloud kms for better security and better management user managed keys are extremely powerful credentials and they can represent a security risk if they are not managed correctly and as you can see here a user managed key has many different areas that need to be addressed when it comes to key management now when it comes to service account permissions in addition to being an identity a service account is a resource which has im policies attached to it and these policies determine who can use the service account so for instance lark can have the editor role on a service account and laura can have a viewer role on a service account so this is just like granting roles for any other google cloud resource just as a note the default compute engine and app engine service accounts are granted editor roles on the project when they are created so that the code executing in your app or vm instance has the necessary permissions now you can grant the service account user role at both the project level for all service accounts in the project or at the service account level now granting the service account user role to a user for a project gives the user access to all service accounts in the project including service accounts that may be created in the future granting the service account user role to a user for a specific service account gives a user access to only that service account so please be aware when granting the service account user role to any member now users who are granted the service account user role on a service account can use it to indirectly access all the resources to which the service account has access when this happens the user impersonates the service account to perform any tasks using its granted roles and permissions and is known as service account impersonation now when it comes to service account permissions there is also another method use called access scopes service account scopes are the legacy method of specifying permissions for your instance and they are used in substitution of iam roles these are used specifically for default or automatically created service accounts based on enabled apis now before the existence of iam roles access scopes were the only way for granting permissions to service accounts and although they are not the primary way of granting permissions now you must still set service account scopes when configuring an instance to run as a service account however when you are using a custom service account you will not be using scopes rather you will be using iam roles so when you are using a default service account for your compute instance it will default to using scopes instead of iam roles and so i wanted to quickly touch on how service accounts are used now one way of using a service account is to attach this service account to a resource so if you want to start a longrunning job that authenticates as a service account you need to attach a service account to the resource that will run the job and this will bind the service account to the resource now the other way of using a service account is directly impersonating a service account which i had explained a little bit earlier so once granted they require permissions a user or a service can directly impersonate the identity of a service account in a few common scenarios you can impersonate the service account without requiring the use of a downloaded external service account key as well a user may get artifacts signed by the google managed private key of the service account without ever actually retrieving a credential for the service account and this is an advanced use case and is only supported for programmatic access now although i'm going to be covering best practices at the end of this section i wanted to go over some best practices for service accounts specifically so you should always look at auditing the service accounts and their keys using either the service account dot keys dot list method or the logs viewer page in the console now if your service accounts don't need external keys you should definitely delete them you should always grant the service account only the minimum set of permissions required to achieve the goal service accounts should also be created for each specific service with only the permissions required for that service and finally when it comes to implementing key rotation you should take advantage of the iam service account api to get the job done and so that's all i have for this lesson on service accounts so you can now mark this lesson as complete and please join me in the next one where we go handson in the console welcome back so in this demonstration i'm going to take a handson tour diving through various aspects of working with both default and custommade service accounts we're going to start off fresh observing a new service account being automatically created along with viewing scopes observing how to edit them and creating custom service accounts that get a little bit more granular with the permissions assigned so with that being said let's dive in so as you can see here from the top right hand corner that i am logged in under tony bowtie ace gmail.com and looking over here from the top drop down menu you can see that i am in the project of cat bow ties fall 2021 and this is a brand new project that i had created specifically for this demo and so i currently have no resources created along with no apis enabled so now i want to navigate over to iam so i'm going to go up to the left hand corner to the navigation menu and i'm going to go to i am an admin and over to iam and as expected i have no members here other than myself tony bowtie ace gmail.com with no other members and if i go over here to the left hand menu under service accounts you can see that i have no service accounts created so now in order to demonstrate a default service account i'm going to go over to the navigation menu and go into compute engine and as you can see the compute engine api is starting up and so this may take a couple minutes to get ready okay and the compute engine api has been enabled so now if i go back over to iam to take a look at my service accounts as expected i have my compute engine default service account now again i did not create this manually this service account was automatically created when i had enabled the compute engine api along with the api's service agent and the compute engine service agent and the same would happen to other various apis that are enabled as well and so now that i have my default service account i want to go back over to compute engine and i'm going to go ahead and create a vm instance so i'm going to just click on create i'm going to keep everything as the default except i'm going to change the machine type from an e2 medium to an e2 micro and so now i'm going to scroll down to where it says identity and api access now here under service account you can see that the compute engine default service account has been highlighted and this is because i don't have any other service accounts that i am able to select from now when a default service account is the only service account you have access to access scopes are the only permissions that will be available for you to select from now remember access scopes are the legacy method of specifying permissions in google cloud now under access scopes i can select from the allow default access allow full access to all cloud apis and set access for each api and so i want to click on set access for each api for just a second and so as you can see here i have access to set permissions for each api the difference being is that i only have access to primitive roles and so now that i'm looking to grant access to my service account i'm going to grant access to cloud storage on a readonly capacity and so now that i have granted permissions for my service account i'm going to now create my instance by simply clicking on the create button and so now that my instance is created i want to head over to cloud storage to see exactly what my service account will have access to so i'm going to go over to my navigation menu and scroll down and click on storage and as you can see here i have created a bucket in advance called bow tie ink fall winter 2012 designs and this is due to bow tie ink bringing back some old designs from 2012 and making them relevant for today and within that bucket there are a few files of different design ideas that were best sellers back in 2012 that tony bowtie wanted to rerelease for the fall winter 2012 collection and so with the new granted access to my default service account i should have access to view these files so in order to test this i'm going to go back over to the navigation menu and go back to compute engine and i'm going to ssh into my instance and so now that i've sshed into my virtual machine i wanted to first check to see who is it that's running the commands is it my user account or is it my service account and so i'll be able to do this very easily by checking the configuration and i can do this by running the command gcloud config list and as you can see my current configuration is showing that my service account is the member that is being used to run this command in the project of cat bow ties fall 2021 now if i wanted to run any commands using my tony bowtie ace gmail.com user account i can simply run the command gcloud auth login and it will bring me through the login process that we've seen earlier on in the course for my tony bowtie ace gmail.com account but now since i'm running all my commands using my service account from this compute engine instance i'm using the permissions granted to that service account that we saw earlier and so since i set the storage scope for the service account to read only we should be able to see the cloud storage bucket and all the files within it by simply running the gsutil command so to list the contents of the bucket i'm going to type in the command gsutil ls for list and the name of the bucket and the syntax for that would be gs colon forward slash forward slash followed by the name of the bucket which would be bowtie inc fw2012 designs and as you can see we're able to view all the files that are in the bucket and so it is working as expected and so now because i've only granted viewing permissions for this service account i cannot create any files due to the lack of permissions so for instance if i was to create a file using the command touch file one i have now created that file here on the instance so now i want to copy this file to my bucket and so i'm going to run the gsutil command cp for copy file 1 which is the name of my file and gs colon forward slash forward slash along with the name of the bucket which is bow tie inc fw 2012 designs and as expected i am getting an access denied exception with a prompt telling me that i have insufficient permissions and so now that i've shown you how to create a default service account and give it permissions using access scopes let's now create a custom service account and assign it proper permissions to not only read files from cloud storage but be able to write files to cloud storage as well so i'm going to now close down this tab and i'm going to go back over to the navigation menu and go back to iam where we can go in and create our new service account under service accounts and so as you can see here this is the default service account and since we want to create a custom one i'm going to go ahead and go up to the top here and click on the button that says create service account and so now i'm prompted to enter some information with regards to details of this service account including the service account name the account id along with a description and so i'm going to call this service account sa hyphen bowtie hyphen demo and as you can see it automatically propagated the service account id and i'm going to give this service account a description storage read write access and i'm going to click on the button create and so now i've been prompted to grant permissions to the service account and i can do that by simply clicking on the drop down and selecting a roll but i'm looking to get a little bit more granular and so i'm going to simply type in storage and as you can see i'm coming up with some more granular roles as opposed to the primitive roles that i only had access to prior to the search so i'm going to click on storage object viewer for read access to cloud storage i'm not going to add any conditions and i'm going to add another role and this time i'm going to add storage object creator and so those are all the permissions i need for read write access to cloud storage and so now i can simply click on continue and so now i'm being prompted to add another user to act as a service account and this is what we discussed in the last lesson about service accounts being both a member and a resource now notice that i have an option for both the service account users role and the service account admins role now as discussed earlier the service account and men's role has the ability to grant other users the role of service account user and so because we don't want to do that i'm going to leave both of these fields blank and simply click on done now i know in the last lesson i talked about creating custom keys for authentication in case you're hosting your code on premise or on another cloud and so if i wanted to do that i can simply go to the actions menu and click on create key and it'll give me the option on creating a private key either using json or p12 format and because i'm not creating any keys i'm going to simply click on cancel and so in order for me to apply this service account to our vm instance i'm going to now go back over to the navigation menu and go back into compute engine and so now in order for me to change this service account that's currently assigned to this instance i'm going to go ahead and check off this instance and click on stop now please note that in order to change service accounts on any instance you must stop it first before you can edit the service account and so now that the instance has stopped i'm going to drill down into this instance one and i'm going to click on edit now i'm going to scroll down to the bottom and at the bottom you will find the service account field and clicking on the drop down i'll find my custom service account as a bow tie demo so i want to select this and simply click on save and so now that i've selected my new service account to be used in this vm instance i can now start up the instance again to test out the permissions that were granted and so just as a quick note here i wanted to bring your attention to the external ip whenever stopping and starting an instance with an ephemeral ip in other words it is not assigned a static ip your vm instance will receive a new ip address and i'll be getting into this in a lot deeper detail in the compute engine section of the course and so now i'm going to ssh into this instance now i'm going to run the same gsutil command that i did previously to list all the files in the bucket so i'm going to run the command gsutil ls for list and gs colon forward slash forward slash bow tie inc fw 2012 designs and as you can see i'm able to read all the files in the bucket now the difference in the permissions granted for the service account is that i'm able to write files to cloud storage and so in order to test that i'm going to use the touch command again and i'm going to name the file file2 and so now i'm going to copy this file to the cloud storage bucket by using the command gsutil cp file2 and the bucket name gs colon forward slash forward slash bow tie inc fw 2012 designs and as expected the file copied over successfully as we do have permissions to write to cloud storage and so before i end this demonstration i wanted to quickly go over exactly how to create service accounts using the command line and so i'm going to close down this tab and i'm going to head up to the top right hand corner and activate my cloud shell i'm going to make this window a little bit bigger and so now in order to view the service accounts i currently have i'm going to run the command gcloud iam service dash accounts list and so as expected the compute engine default service account along with the custom service account that i created earlier called sa bowtie demo is now displaying and in order to just verify that i'm going to go over to iam under service accounts and as you can see it is reflecting exactly the same in the console so now in order for me to create a new service account using the command line i'm going to run the command gcloud iam service accounts create and the name of the service account which i'm going to call satony bowtie along with the display name as essay tony bowtie as well and i'm going to hit enter and my service account has been created so now if i run the command gcloud i am service accounts list i should see my new service account and as well if i did a refresh here on the console i can see that it is reflecting the same so now that we've created our new service account we need to assign some permissions to it in order for us to be able to use it and so if i go over here to iam in the console i can see here that my service account has not been assigned any permissions and so in order to do that i am going to simply run the command gcloud projects add dash iampolicybinding so we're adding a policy binding and then the name of the project catbow ties fall 2021 we need to add the member which is the new service account email address along with the role of storage object viewer i'm going to hit enter and as you can see my member sa tony bowtie has been assigned the storage object viewer role and so if i wanted to grant some other roles to the service account i can do that as well and so if i did a refresh here i can see that the console reflects exactly the same and so in order for me to use this account in my instance i'm going to first have to stop my instance attach my service account and then start up my instance again so i'm going to go over to my cloud shell i'm just going to clear the screen and i'm going to paste in the command gcloud compute instances stop the name of the instance along with the zone and now that the instance has stopped i can now add my surface account to the instance and so i'm going to use the command gcloud compute instances set service account instance 1 along with the zone and the service account email address i'm going to go ahead and hit enter and it has now been successfully added and so now that that's done i can now start up the instance by using the command gcloud compute instances start along with the instance name and the zone and so now if i go over to my navigation menu and go over to compute engine and drill down on the instance if i scroll down to the bottom i'll be able to see that my new service account has been added and so this is a great demonstration for when you want to add different service accounts for your different applications on different instances or even on different resources and so that's pretty much all i wanted to cover in this demonstration so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to dive into cloud identity google's identity as a service offering for google cloud that maximizes end user efficiency protect company data and so much more now cloud identity as i said before is an identity as a service solution that centrally manages users and groups this would be the sole system for authentication and that provides a single signon experience for all employees of an organization to be used for all your internal and external applications cloud identity also gives you more control over the accounts that are used in your organization for example if developers in your organization use personal accounts such as gmail accounts those accounts are outside of your control so when you adopt cloud identity you can manage access and compliance across all the users in your domain now when you adopt cloud identity you create a cloud identity account for each of your users and groups you can then use iam to manage access to google cloud resources for each cloud identity account and you can also configure cloud identity to federate identities between google and other identity providers such as active directory and azure active directory and i'll be getting more into that a little bit later so now when it comes to cloud identity it gives you so much more than just user and group management it provides a slew of features such as device management security single signon reporting and directory management and i will be diving deeper into each one of these features of cloud identity now starting with device management this lets people in any organization access their work accounts from mobile devices while keeping the organization's data more secure in today's world employees want to access business applications from wherever they are whether at home at work or even traveling and many even want to use their own devices which is also known as bring your own device or byod for short using mobile device management there are several ways that you can provide the business applications employees need on their personal devices while implementing policies that keep the corporate data safe you can create a white list of approved applications where users can access corporate data securely through those applications you can enforce work profiles on android devices and requiring managed applications on ios devices policies can also be pushed out on these devices to protect corporate data and identities as well as keeping inventory of devices with corporate data present then when these devices are either no longer being used for corporate use or stolen the device can then be wiped of all its corporate data device management also gives organizations the power to enforce passcodes as well as auditing now moving into the security component of cloud identity this is where twostep verification steps in now as explained earlier twostep verification or to sv is a security feature that requires users to verify their identity through something they know such as a password plus something they have such as a physical key or access code and this can be anything from security keys to google prompt the authenticator app and backup codes so cloud identity helps by applying security best practices along with being able to deploy twostep verification for the whole company along with enforcement controls and can also manage passwords to make sure they are meeting the enforced password requirements automatically so single signon is where users can access many applications without having to enter their username and password for each application single signon also known as sso can provide a single point of authentication through an identity provider also known as idp for short you can set up sso using google as an identity provider to access a slew of thirdparty applications as well as any onpremise or custom inhouse applications you can also access a centralized dashboard for conveniently accessing your applications so now when lisa logs in with her employee credentials she will then have access to many cloud applications that bowtie inc it department has approved through a catalog of sso applications and this will increase both security and productivity for lisa and bowtie inc as lisa won't have to enter a separate username and password for separate applications now getting into reporting this covers audit logs for logins groups devices and even tokens you're even able to export these logs to bigquery for analysis and then you can create reports from these logs that cover security applications and activity now moving on to the last component of cloud identity is directory management and this provides profile information for users in your organization email and group addresses and shared external contacts in the directory using google cloud directory sync or gcds you can synchronize the data in your google account with your microsoft active directory or ldap server gcds doesn't migrate any content such as your email your calendar events or your files to your google account gcds is used to synchronize all your users groups and shared contacts to match the information in your ldap server which could be your active directory server or your azure active directory domain now getting deeper into google cloud directory sync i'd like to touch on active directory for just a minute now active directory is a very common directory service developed by microsoft and is a cornerstone in most big corporate onpremises environments it authenticates and authorizes all users and computers in a windows domain type network signing and enforcing security policies for all computers and installing or updating software as necessary now as you can see here in the diagram the active directory forest contains the active directory domain a bowtieinc.co and the active directory federation services of bowtieinc.co where the active directory forest is the hierarchical structure for active directory the active directory domain is responsible for storing information about members of the domain including devices and users and it verifies their credentials and defines their access rights active directory federation services or adfs is a single signon service where federation is the means of linking a person's electronic identity and attributes stored across multiple distinct identity management systems so you can think of it as a subset of sso as it relates only to authentication technologies used for federated identity include some common terms that you may hear me or others in the industry use from time to time such as saml which stands for security assertion markup language oauth open id and even security tokens such as simple web tokens json web tokens and saml assertions and so when you have identities already in your onpremises environment that live in active directory you need a way to tie these identities to the cloud and so here's where you would use google cloud directory sync to automatically provision users and groups from active directory to cloud identity or g suite google cloud directory sync is a free google provided tool that implements the synchronization process and can be run on google cloud or in your onpremises environment synchronization is one way so that active directory remains the source of truth cloud identity or g suite uses active directory federation services or adfs for single signon any existing corporate applications and other sas services can continue to use your adfs as an identity provider now i know this may be a review for some who are advanced in this topic but for those who aren't this is a very important topic to know as google cloud directory sync is a big part of cloud identity and is a common way that is used in many corporate environments to sync active directory or any other ldap server to google cloud especially when you want to keep your active directory as the single source of truth and so that's pretty much all i wanted to cover when it comes to cloud identity and google cloud directory sync so you can now mark this lesson as complete and let's move on to the next one welcome back now i wanted to close out this section by briefly going over the best practices to follow when working with identity and access management so the phrase that was discussed in the beginning of this lesson that will continuously come up in the exam is the principle of least privilege and again this is where you would apply only the minimal access level required for what is needed to be done and this can be done using predefined roles which is a more granular level role than using primitive roles which are very wide scoped roles that are applied to the whole project roles should also be granted at the smallest scope necessary so for instance when assigning somebody the permissions needed for managing preexisting compute instances assigning a compute instance admin role might be sufficient for what they need to do as opposed to assigning them the compute instance role that has full control of all compute engine instance resources now when it comes to child resources they cannot restrict access granted on its parent so always remember to check the policy granted on every resource and make sure you understand the hierarchical inheritance you also want to make sure that you restrict access to members abilities to create and manage service accounts as users who are granted the service account actor role for a service account can access all the resources for which the service account has access and granting someone with the owner role should be used with caution as they will have access to modify almost all resources projectwide including iam policies and billing granting an editor role might be more sufficient for the needs of most when using primitive roles now when dealing with resource hierarchy to make it easy on how to structure your environment you should look at mirroring your google cloud resource hierarchy structure to your organizational structure in other words the google cloud resource hierarchy should reflect how your company is organized you should also use projects to group resources that share the same trust boundary as well as setting policies at the organization level and at the project level rather than at the resource level now going back to what we discussed earlier about the principle of least privilege you should use this guideline to grant iam roles that is only give the least amount of access necessary to your resources and when granting roles across multiple projects it is recommended to grant them at the folder level instead of at the project level now diving back into service accounts a separate trust boundary should always be applied for any given application in other words create a new service account when multiple components are involved in your application you also want to make sure that you don't delete any service accounts that are in use by running instances as your application is likely to fail so you will want to schedule this during plan down time to avoid any outages now earlier on in this section we discussed service account keys and how they interact with google cloud and that is the main authentication mechanism used for keys so you want to make sure that any user managed keys are rotated periodically to avoid being compromised you can rotate a key by creating a new key switching applications to use the new key and then deleting the old key but be sure to create the new key first before deleting the old one as this will result in parts or even your entire application failing and also when working with service account keys it's always good practice to name your service keys and this will reflect your use for those keys and permissions for those keys so you know what they are used for when you're looking at them now when you are giving access to service accounts you want to make sure that only those who truly need access are the ones that have it others in your environment should be restricted to avoid any misuse now when it comes to keeping your service account keys safe i can't stress this enough you never want to check in these keys source code or leave them in your downloads directory as this is a prime way of not only getting your keys compromised but compromising your entire environment to be accessed publicly now we touched a bit on auditing but we haven't really gone into it in detail and we'll be going into it later on in the course but touching on best practices you want to be sure to check your cloud audit logs regularly and audit all i am policy changes whenever you edit any iam policies a log is generated that records that change and so you always want to periodically check these logs to make sure that there are no changes that are out of your security scope you also want to check to see who has editing permissions on these iam policies and make sure that those who hold them have the rights to do so point being is that you want to restrict who has the ability to edit policies and once these audit logs have been generated you want to export them to cloud storage so that you're able to store them for long term retention as these logs are typically held for weeks and not years getting back to service account keys service account key access should be periodically audited for viewing of any misuse or unauthorized access and lastly audit logs should also be restricted to only those who need access and others should have no permissions to view them and this can be done by adding a role to be able to view these logs now when touching on policy management you want to grant access to all projects in your organization by using an organization level policy you also want to grant roles to a google group instead of individual users as it is easier to add or remove members from a google group instead of updating an im policy and finally when you need to grant multiple roles to a task you should create a google group as it is a lot easier to grant the roles to that group and then add the users to that group as opposed to adding roles to each individual user and so that's all i wanted to cover on this short yet very important lesson on best practices when it comes to iam now i know this is not the most exciting topic but will become extremely necessary when you are dealing with managing users groups and policies in environments that require you to use iam securely and so please keep this in mind whenever you are working in any environment as it will help you grant the proper permissions when it comes to these different topics so now i highly recommend that you take a break grab a tea or coffee before moving on into the next section and so for now you can mark this lesson as complete and whenever you're ready please join me in the next section welcome back now i wanted to make this as easy as possible for those students who do not have a background in networking or any networking knowledge in general which is why i wanted to add this quick networking refresher to kick off the networking section of this course so with that being said let's dive in so before the internet computers were standalone and didn't have the capabilities to send emails transfer files or share any information fast forward some time people started to connect their computers together to share and be able to do the things that modern networks can do today part of being in this network is being able to identify each computer to know where to send and receive files this problem was solved by using an address to identify each computer on the network like humans use a street address to identify where they live so that mail and packages can be delivered to them an ip address is used to identify a computer or device on any network so communication between machines was done by the use of an ip address a numerical label assigned to each device connected to a computer network that uses the internet protocol for communication also known as ip for short so for this system to work a communication system was put in place that defined how the network would function this system was put together as a consistent model of protocol layers defining interoperability between network devices and software in layers to standardize how different protocols would communicate in this stack this stack is referred to as the open systems interconnection model or you may hear many refer to it as the seven layer osi model now this is not a deep dive networking course but i did feel the need to cover that which is necessary for the understanding of the elements taught in this course for those wanting to learn more about the osi model and the layers within it please check out the links that i have included in the lesson text below so for this lesson and the next i will be covering the specific layers with its protocols that are highlighted here and will help you understand the networking concepts in this course with a bit better clarity so i'll be covering a layer 3 being the network layer layer 4 being the transport layer and layer 7 being the application layer so first up i will be covering layer 3 which is the networking layer along with the internet protocol now there are two versions of the internet protocol and are managed globally by the regional internet registries also known as the rir the first one which is ipv4 is the original version of the internet protocol that first came on the scene in 1981 the second version is ipv6 which is a newer version designed in 2017 to deal with the problem of ipv4 address exhaustion meaning that the amount of usable ips were slowly being used up and i will be covering both versions of the internet protocol in a little bit of depth so let's first dive into ipv version 4. so ipv4 can be read in a human readable notation represented in dotted decimal notation consisting of four numbers each ranging from 0 to 255 separated by dots each part between the dots represents a group of 8 bits also known as an octet a valid range for an ip address starts from 0.0.0.0 and ends in 255.255.255.255. and this would give you a total number of over 4.2 billion ip addresses now this range was viewed as extremely large back then until the number of ip addresses available were quickly dwindling due to the many ipconnected devices that we have today and this is when a new addressing architecture was introduced called classful addressing where the address was split into smaller ranges and this was originally assigned to you when you needed an ip address by one of the registries noted before so for any given ip address they're typically made of two separate components the first part of the address is used to identify the network that the address is a part of the part that comes afterwards is used to specify a specific host within that network now the first part was assigned to you and your business by the registries and the second part was for you to do it as you'd like and so these ip addresses were assigned from the smaller ranges explained earlier called classes the first range of classes is class a and it started at 0.0.0.0 and ended at 127.255 and this would give a total number of over 2.1 billion addresses with 128 different networks class a ip addresses can support over 16 million hosts per network and those who were assigned addresses in this class had a fixed value of the first octet the second third and fourth octet was free for the business to assign as they choose class a ip addresses were to be used by huge networks like those deployed by internet service providers and so when ips started to dwindle many companies return these class a network blocks back to the registries to assist with extending addressing capacity and so the next range is class b and this is half the size of the class a network the class b network range started at one at 128.0.0.0 and end it at 191.255.255.255 and carries a total number of over 1 billion ip addresses with over 16 000 networks the fixed value in this class is of the first and second octet the third and fourth octet can be done with as you like ip addresses in this class were to be used for medium and large size networks in enterprises and organizations the next range is class c and this is half the size of the class b network the class c network range starts at 192 and ends at 223.255.255.255 and carries a total of over half a billion addresses with over two million networks and can support up to 256 hosts the fixed value of this class is the first second and third octet and the fourth can be done with as you like ip addresses in this class were the most common class and were to be used in small business and home networks now there's a couple more classes that were not commonly used called class d and class e and this is beyond the scope of this course so we won't be discussing this and so this was the way that was used to assign public ip addresses to devices on the internet and allowed communication between devices now the problem with classful addressing was that with businesses that needed larger address blocks than a class c network provided they received a class b block which in most cases was much larger than required and the same thing happened with requiring more ips than class b and getting a class a network block this problem introduced a lot of wasted ips as there was no real middle ground and so this was a way to address any publicly routable ips now there were certain ranges that were allocated for private use and were designed to be used in private networks whether onpremises or in cloud and again they are not designed for public use and also didn't have the need to communicate over the public internet and so these private ip address spaces were standardized using the rfc standard 1918 and again these ip addresses are designed for private use and can be used anywhere you like as long as they are still kept private chances are a network that you've come across whether it be a cloud provider your home network or public wifi will use one of these classes to define their network and these are split into three ranges first one being single class a with 10.0.0 ending in 10.255.255.255. the class b range ranging from 172.16.0.0 to 172.31 dot and lastly class c which was ranging from 192.168.0.0 to 192.168.255.255. now for those networks that use these private ips over the public internet the process they would use is a process called network address translation or nat for short and i will be covering this in a different lesson later on in the section this method of classful addressing has been replaced with something a bit more efficient where network blocks can be defined more granularly and was done due to the internet running out of ipv4 addresses as we needed to allocate these ips more efficiently now this method is called classless inter domain routing or cider for short now with cider based networks you aren't limited to only these three classes of networks class a b and c have been removed for something more efficient which will allow you to create networks in any one of those ranges cider ranges are represented by its starting ip address called a network address followed by what is called a prefix which is a slash and then a number this slash number represents the size of the network the bigger the number the smaller the network and the smaller the number the bigger the network given the example here 192.168.0.0 is the network address and the prefix is a slash 16. now at this high level it is not necessary to understand the math behind this but i will include a link in the lesson text for those of you who are interested in learning more about it all you need to keep in mind is as i said before the bigger the prefix number the smaller the network and the smaller the prefix number the bigger the network so just as an example the size of this slash 16 network is represented here by this circle its ip range is 192.168.0.0 ending in 192.168.255.255. and once you understand the math you will be able to tell that a slash 16 range means that the network is the fixed value in the first and second octet the hosts on the network or the range are the values of anything in the third or fourth octets so this network in total will provide us with 65 536 ip addresses now let's say you decided to create a large network such as this and you wanted to allocate part of it to another part of your business you can simply do so by splitting it in two and be left with two slash 17 networks so instead of one slash 16 network you will now have 2 17 networks and each network will be assigned 32 768 ip addresses so just to break it down the previous network which was 192.16 forward slash 16 with the first two octets being the network which is 192.168 it leaves the third and fourth octet to distribute as you like and these third and fourth octets are what you're having to create these two networks so looking at the blue half the address range will start at 0.0 and will end at 127.255. the green half will start halfway through the slash 16 network which will be 128.0 and end at 255.255. so now what if i was looking to break this network down even further and break it into four networks well using cider ranges this makes things fairly easy as i can have it again and as shown here i would split the two slash 17 networks to create four slash 18 networks so if i took the blue half circle and split it into two and then splitting the green half circle into this would leave me with four slash 18 networks as seen here the blue quarter would start from 192.168.0.0 ending with the last two octets of 63.255 and the red quarter which starts from where the blue left off starting at the last two octets of 64.0 and ending in 127.255. the green quarter again starting off with the previously defined 128.0 network which is where the red quarter left off and ending with the last two octets being 191.255 and lastly the yellow quarter starting off from where the green quarter left off at 192.0 with the last two octets ending with 255.255 and so this would leave us with four smaller slash 18 networks broken down from the previous two 17 networks with each of these networks consisting of 16 384 ip addresses and we can continue this process continuously having networks and breaking them down into smaller networks this process of dividing each network into two smaller networks is known as subnetting and each time you subnet a network and create two smaller networks the number in the prefix will increase and so i know this is already a lot to take in so this would be a perfect time for you to grab a coffee or a tea and i will be ending part one here and part two will be continuing immediately after part one so you can now mark this lesson as complete and i'll see you in the next one for part two welcome back and in this lesson i'm going to be covering the second part of the networking refresher now part two of this lesson is starting immediately from the end of part one so with that being said let's dive in now i know this network refresher has been filled with a ton of numbers with an underlying current of math but i wanted you to focus on the why so that things will make sense later i wanted to introduce the hard stuff first so that over the length of this course you will be able to digest this information and understand where this fits into when discussing the different network parts of google cloud this will also help you immensely in the real world as well as the exam when configuring networks and knowing how to do the job of an engineer so getting right into it i wanted to just do a quick review on classless interdomain routing or cider so as discussed in the first refresher an ipv4 address is referenced in dotted decimal notation alongside the slash 16 is the prefix and defines how large the network is and so before i move on i wanted to give you some references that i found helpful in order to determine the size of a network and so here i have referenced three of the most common prefixes that i continuously run into that i think would be an extremely helpful reference for you so if you look at the first i p address 192.168.0.0 with slash 8 as the prefix slash 8 would fall under a class a network 192 being the first octet as well as being the network part of the address would be fixed and so the host part of it would be anything after that so the address could be 192 dot anything and this cider range would give you over 16 million ip addresses the second most common network that i see is a slash 16 network and this would make this ip fall under a class b network making the first two octets fixed and being the network part meaning that anything after 192.168 would be the host part meaning that the address could be 192.168.anything and this would give you 65536 ip addresses and so for the third ip address which is probably the most common one that i see is a slash 24 network which falls under a class c network meaning that the first three octets are fixed and the fourth octet could be anything from zero to two five five and this would give you 256 ip addresses and another common one which is the smallest that you will see is a slash 32 prefix and this is one that i use constantly for white listing my ip address and because a slash 32 is one ip address this is a good one to know when you are configuring vpn for yourself or you're whitelisting your ip address from home or work and for the last reference as well as being the biggest network is the ip address of 0.0.0.1 forward slash 0 which covers all ip addresses and you will see this commonly used for the internet gateway in any cloud environment and so these are some common prefixes that come up very frequently and so i hope this reference will help you now moving back to the osi model i've covered ipv4 in the network layer and so now it's time to discuss ipv6 now as i noted earlier ipv4 notation is called dotted decimal and each number between the dots is an octet with a value of 0 to 255. now underneath it all each octet is made up of an 8bit value and having four numbers in an ip address that would make it a 32bit value ipv6 is a much longer value and is represented in hexadecimal and each grouping is two octets which is 16 bits and is often referred to as a hextet now as these addresses are very long as you can see you're able to abbreviate them by removing redundant zeros so this example shown here is the same address as the one above it so if there is a sequence of zeros you can simply replace them with one zero so in this address each grouping of four zeros can be represented by one zero and if you have multiple groups of zeros in one address you can remove them all and replace them with double colons so each of these ipv6 addresses that you see here are exactly the same now each ipv6 address is 128 bits long and is represented in a similar way to ipv4 starting with the network address and ending with the prefix each hextet is 16 bits and the prefix number is the number of bits that represent the network with this example slash 64 refers to the network address underlined in green which is 2001 colon de3 each hextet is 16 bits and the prefix is 64. so that's four groups of 16 and so this is how we know which part is the network part of the address and which is the host part of the address again notice the double colon here and as i explained previously any unneeded zeros can be replaced by a double colon and so this address would represent a slew of zeros and so adding in all the zeros the ipv6 starting network address would look like this now because the network address starts at 2001 colon de3 with another two hextets of zeros as the network address that was determined by the slash 64 prefix which is four hextets it means a network finishes at that network address followed by all fs and so that's the process of how we can determine the start and end of every ipv6 network now as i've shown you before with all ipv4 addresses they are represented with a 0.0.0.0.0 and because ipv6 addresses are represented by the same network address and prefix we can represent ipv6 addresses as double colon slash zero and you will see this frequently when using ipv6 and so i know this is really complicated but i just wanted to give you the exposure of ipv6 i don't expect you to understand this right away in the end it should become a lot clearer as we go through the course and i promise you it will become a lot easier i had a hard time myself trying to understand this network concept but after a few days i was able to digest it and as i went back and did some practice it started to make a lot more sense to me and so i know as we move along with the course that it will start making sense to you as well so now that we've discussed layer 3 in the osi model i wanted to get into layer 4 which is the transport layer with ip packets discussing tcp and udp and so in its simplest form a packet is the basic unit of information in network transmission so most networks use tcpip as the network protocol or set of rules for communication between devices and the rules of tcpip require information to be split into packets that contain a segment of data to be transferred along with the protocol and its port number the originating address and the address of where the data is to be sent now udp is another protocol that is sent with ip and is used in specific applications but mostly in this course i will be referring to tcpip and so as you can see in this diagram of the ip packet this is a basic datagram of what a packet would look like again with this source and destination ip address the protocol port number and the data itself now this is mainly just to give you a high level understanding of tcpip and udpip and is not a deep dive into networking now moving on to layer 7 of the osi model this layer is used by networked applications or applications that use the internet and so there are many protocols that fall under this layer now these applications do not reside in this layer but use the protocols in this layer to function so the application layer provides services for networked applications with the help of protocols to perform user activities and you will see many of these protocols being addressed as we go through this course through resources in google cloud like http or https for load balancing dns that uses udp on port 53 and ssh on port 22 for logging into hosts and so these are just a few of the many scenarios where layer 7 and the protocols that reside in that layer come up in this course and we will be diving into many more in the lessons to come and so that about wraps up this networking refresher lesson and don't worry like i said before i'm not expecting you to pick things up in this first go things will start to make more sense as we go through the course and we start putting these networking concepts into practice also feel free to go back and review the last couple of lessons again if things didn't make sense to you the first time or if you come across some networking challenges in future lessons and so that's everything i wanted to cover so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson we will be discussing the core networking service of gcp virtual private cloud or vpc for short it is the service that allows you to create networks inside google cloud with both private and public connectivity options both for incloud deployments and onpremise hybrid cloud deployments this is a service that you must know well as there are many questions that come up on the exam with regards to vpcs so with that being said let's dive in now vpcs are what manages the networking functionality for your google cloud resources this is a software defined network and is not confined to the physical limitations of networking in a data center this has been abstracted for you vpc networks including their associated routes and firewall rules are global resources they are not associated with any particular region or zone they are global resources and span all available regions across the globe as explained earlier vpcs are also encapsulated within projects projects are the logical container where your vpcs live now these vpcs do not have ip ranges but are simply a construct of all of the individual ip addresses and services within that network the ip addresses and ranges are defined within the subnetworks that i will be diving into a bit later as well traffic to and from instances can be controlled with network firewall rules rules are implemented on the vms themselves so traffic can be controlled and logged as it leaves or arrives at a vm now resources within a vpc network can communicate with one another by using internal or private ipv4 addresses and these are subject to applicable network firewall rules these resources must be in the same vpc for communication otherwise they must traverse the public internet with an assigned public ip or use a vpc peering connection or establish a vpn connection another important thing to note is that vpc networks only support ipv4 unicast traffic they do not support ipv6 traffic within the network vms in the vpc network can only send to ipv4 destinations and only receive traffic from ipv4 sources however it is possible to create an ipv6 address for a global load balancer now unless you choose to disable it each new project starts with a default network in a vpc the default network is an auto mode vpc network with predefined subnets a subnet is allocated for each region with nonoverlapping cider blocks also each default network has a default firewall rule these rules are configured to allow ingress traffic for icmp rdp and ssh traffic from anywhere as well as ingress traffic from within the default network for all protocols and ports and so there are two different types of vpc networks auto mode or custom mode an auto mode network also has one subnet per region the default network is actually an auto mode network as explained earlier now these automatically created subnets use a set of predefined ip ranges with a slash 20 cider block that can be expanded to a slash 16 cider block all of these subnets fit within the default 10.128.0.0 ford slash 9 cider block and as new gcp regions become available new subnets in those regions are automatically added to auto mode networks using an ip range on that block now a custom owned network does not automatically create subnets this type of network provides you with complete control over its subnets and ip ranges as well as another note an auto mode network can be converted to a custom mode network to gain more control but please be aware this conversion is one way meaning that custom networks cannot be changed to auto mode networks so when deciding on the different types of networks you want to use make sure that you review all of your considerations now custom mode vpc networks are more flexible and better suited to production and google recommends that you use custom mode vpc networks in production so here is an example of a project that contains three networks all of these networks span multiple regions across the globe as you can see here on the right hand side and each network contains separate vms and so this diagram is to demonstrate that vms that are in the same network or vpc can communicate privately even when placed in separate regions because vms in network a are in the same network they can communicate over internal ip addresses even though they're in different regions essentially your vms can communicate even if they exist in different locations across the globe as long as they are within the same network the vms in network b and network c are not in the same network therefore by default these vms must communicate over external ips even though they're in the same region as no internal ip communication is allowed between networks unless you set up vpc network peering or use a vpn connection now i wanted to bring back the focus to the default vpc for just a minute unless you create an organizational policy that prohibits it new projects will always start with a default network that has one subnet in each region and again this is an auto mode vpc network in this particular example i am showing a default vpc with seven of its default regions displayed along with their ip ranges and again i want to stress that vpc networks along with their associated routes and firewall rules are global resources they are not associated with any particular region or zone so the subnets within them are regional and so when an auto mode vpc network is created one subnet from each region is automatically created within it these automatically created subnets use a set of predefined ip ranges that fit within the cider block that you see here of 10.128.0.049 and as new google cloud regions become available new subnets in those regions are automatically added to auto mode vpc networks by using an ip range from that block in addition to the automatically created subnets you can add more subnets manually to auto mode vpc networks in regions that you choose by using ip ranges outside of 10.128.0.049 now if you're using a default vbc or have already created an auto mode vpc you can switch the vpc network from auto mode to custom mode and this is a oneway conversion only as custom mode vpc networks cannot be changed to auto mode vpc networks now bringing this theory into practice with regards to the default vpc i wanted to take the time to do a short demo so whenever you're ready join me in the console and so here we are back in the console and if i go here in the top right hand corner i am logged in as tony bowties at gmail.com and in the top drop down project menu i'm logged in under project tony and because this demo is geared around the default vpc i want to navigate to vpc networks so i'm going to go over here to the top left hand corner to the navigation menu and i'm going to click on it and scroll down to vpc network under networking and so as you can see here in the left hand menu there are a bunch of different options that i can choose from but i won't be touching on any of these topics as i have other lessons that will deep dive into those topics so in this demo i'd like to strictly touch on the default vpc and as you can see in project tony it has created a default vpc for me with a one subnet in every region having its own ip address range and so just as a reminder whenever you create a new project a default vpc will be automatically created for you and when these subnets were created each of them have a route out to the public internet and so the internet gateway is listed here its corresponding firewall rules along with global dynamic routing and flow logs are turned off and again i will be getting deeper into routing and flow logs in later lessons in the section now earlier i had pointed out that an auto mode vpc can be converted to a custom vpc and it's as simple as clicking this button but we don't want to do that just yet and what i'd like to do is drill down into the default vbc and show you all the different options as you can see here the dns api has not been enabled and so for most of you a good idea would be to enable it and so i'm going to go ahead and do that now as well you can see here that i can make adjustments to each of the different subnets or i can change the configuration of the vpc itself so if i click on this edit button here at the top i'm able to change the subnet creation mode along with the dynamic routing mode which i will get into in a later lesson and the same thing with the dns server policy and so to make this demo a little bit more exciting i want to show you the process on how to expand a subnet so i'm going to go into us central one i'm going to drill down here and here's all the configuration settings for the default subnet in the us central one region and so for me to edit this subnet i can simply click on the edit button up here at the top and so right below the ip address range i am prompted with a note saying that the ip ranges must be unique and nonoverlapping as we stated before and this is a very important point to know when you're architecting any vpcs or its corresponding sub networks and so i'm going to go ahead and change the subnet from a cider range of 20 and i'm going to change it to 16. i'm not going to add any secondary ip ranges i'm going to leave private google access off and so i'm going to leave everything else as is and simply click on save and so once this has completed i'll be able to see that my subnet range will go from a slash 20 to a slash 16. and so here you can see the ip address range has now changed to a slash 16. if i go back to the main page of the vpc network i can see that the ip address range is different from all the other ones now you're probably asking why can't i just change the ip address range on all the subnets at once and so even though i'd love to do that unfortunately google does not give you the option each subnet must be configured one by one to change the ipa address range now i wanted to quickly jump into the default firewall rules and as discussed earlier the rules for incoming ssh rdp and icmp have been prepopulated along with a default rule that allows incoming connections for all protocols and ports among instances within the same network so when it comes to routes with regards to the vpc network the only one i really wanted to touch on is the default route to the internet and so without this route any of the subnets in this vpc wouldn't have access to route traffic to the internet and so when the default vpc is created the default internet gateway is also created and so now going back to the main page of the vpc network i wanted to go through the process of making the ip address range bigger but doing it through the command line and so i'm going to go up to the right hand corner and open up cloud shell i'm going to make this a little bit bigger and so for this demo i'm going to increase the address range for the subnet in us west one from a slash 20 to a slash 16 and so i'm going to paste in the command which is gcloud compute networks subnets expand ip dash range and then the name of the network which is default as well as the region and i'm going to do uswest1 along with the prefix length which is going to be 16. so i'm going to hit enter i've been prompted to make sure that this is what i want to do and so yes i do want to continue so i'm going to type in y for yes and hit enter and so within a few seconds i should get some confirmation and as expected my subnet has been updated and so because i like to verify everything i'm going to now clear the screen and i'm going to paste in the command gcloud compute networks subnets describe and then the subnet name which is default along with the region which would be uswest1 i'm going to click on enter and as you can see here the ipsider range is consistent with what we have changed and if i do a quick refresh on the browser i'll be able to see that the console has reflected the same thing and as expected the ip address range here for us west one in the console reflects that which we see here in cloud shell and so now to end this demo i wanted to quickly show you how i can delete the default vpc and recreate it so all i need to do is to drill into the settings and then click on delete vpc network right here at the top i'm going to get a prompt to ask me if i'm sure and i'm going to simply click on delete now just as a note if you have any resources that are in any vpc networks you will not be able to delete the vpc you would have to delete the resources first and then delete the vpc afterwards okay and it has been successfully deleted and as you can see there are no local vpc networks in this current project and so i want to go ahead and recreate the default vpc so i'm going to simply click on create vpc network and so here i'm prompted to enter in a bunch of information for creating this new vpc network and so keeping with the spirit of default vpcs i'm going to name this vpc default i'm going to put default in the description and under subnet creation mode i'm going to click on automatic and as you can see a prompt came up telling me these ip address ranges will be assigned to each region in your vpc network and i'm able to review the ip address ranges for each region and as stated before the ip address ranges for each region will always be the same every time i create this default vpc or create a vpc in the automatic subnet creation mode now as a note here under firewall rules if i don't select these firewall rules none will actually be created so if you're creating a new default vpc be sure to check these off and so i'm going to leave everything else as is and i'm going to simply go to the bottom and click on the create button and within about a minute i should have the new default vpc created okay and we are back in business the default vpc has been recreated with all of these subnets in its corresponding regions all the ip address ranges the firewall rules everything that we saw earlier in the default vpc and so that's pretty much all i wanted to cover in this demo on the default vpc network along with the lesson on vpcs so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i'm going to be discussing vpc network subnets now the terms subnet and sub network are synonymous and are used interchangeably in google cloud as you'll hear me using either one in this lesson yet i am referring to the same thing now when you create a resource in google cloud you choose a network and a subnet and so because a subnet is needed before creating resources some good knowledge behind it is necessary for both building and google cloud as well as in the exam so in this lesson i'll be covering subnets at a deeper level with all of its features and functionality so with that being said let's dive in now each vpc network consists of one or more useful ip range partitions called subnets also known in google cloud as sub networks each subnet is associated with the region and vpc networks do not have any ip address ranges associated with them ip ranges are defined for the subnets a network must have at least one subnet before you can use it and as mentioned earlier when you create a project it will create a default vpc network with subnets in each region automatically auto mode will run under this same functionality now custom vpc networks on the other hand start with no subnets giving you full control over subnet creation and you can create more than one subnet per region you cannot change the name or region of a subnet after you've created it you would have to delete the subnet and replace it as long as no resources are using it primary and secondary ranges for subnets cannot overlap with any allocated range any primary or secondary range of another subnet in the same network or any ip ranges of subnets in peered networks in other words they must be a unique valid cider block now when it comes to ip addresses of a subnet google cloud vpc has an amazing feature that lets you increase the ip space of any subnets without any workload shutdown or downtime as demonstrated earlier in the previous lesson and this gives you the flexibility and growth options to meet your needs but unfortunately there are some caveats the new subnet must not overlap with other subnets in the same vpc network in any region also the new subnets must stay inside the rfc 1918 address space the new network range must be larger than the original which means the prefix length must be smaller in number and once a subnet has been expanded you cannot undo an expansion now auto mode network starts with a slash 20 range that can be expanded to a 16 ip range but not larger you can also convert the auto mode network to a custom mode network to increase the ip range even further and again this is a oneway conversion custom mode vpc networks cannot be changed to auto mode vpc networks now in any network that is created in google cloud there will always be some ip addresses that you will not be able to use and these are reserved for google and so every subnet has four reserved ip addresses in its primary ip range and just as a note there are no reserved ip addresses in the secondary ip ranges and these reserved ips can be looked at as the first two and the last two ip addresses in the cider range now the first address in the primary ip range for the subnet is reserved for the network the second address in the primary ip range for the subnet is reserved for the default gateway and allows you access to the internet the second to last address in the primary ip range for the subnet is reserved for google cloud for potential future use and the last address and the ip range for the subnet is for broadcast and so that about covers this short yet important lesson on vpc network subnets these features and functionalities of subnets that have been presented to you will help you make better design decisions that will give you a bit more knowledge and flexibility when it comes to assigning ipspace within your vpc networks and so that's all i have to cover for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i'm going to be going through routing and private google access now although routing doesn't really show up in the exam i wanted to give you an inside look on how traffic is routed so when you're building in google cloud you'll know exactly what you will need to do if you need to edit these routes in any way or if you need to build new ones to satisfy your particular need now private google access does pop its head in the exam but only at a high level but i wanted to get just a bit deeper with the service and get into the data flow of when the service is enabled so with that being said let's dive in now google cloud routes define the paths that network traffic takes from a vm instance to other destinations these destinations can be inside your google cloud vpc network for example in another vm or outside it in a vpc network a route consists of a single destination and a single next hop when an instance in a vpc network sends a packet google cloud delivers the packet to the route's next hop if the packet's destination address is within the route's destination range and so all these routes are stored in the routing table for the vpc now for those of you who are not familiar with a routing table in computer networking a routing table is a data table stored in a router or a network host that lists the routes to particular network destinations and so in this case the vpc is responsible for storing the routing table as well each vm instance has a controller that is kept informed of all applicable routes from the network's routing table each packet leaving a vm is delivered to the appropriate next hop of an applicable route based on a routing order now i wanted to take a couple minutes to go through the different routing types that are available on google cloud now in google cloud there are two types of routing there is the system generated which offers the default and subnet route and then there are the custom routes which support static routes and dynamic routes and so i first wanted to cover system generated routes in a little bit of depth and so every new network whether it be an automatic vpc or a custom vpc has two types of system generated routes a default route which you can remove or replace and one subnet route for each of its subnets now when you create a vpc network google cloud creates a system generated default route and this route serves two purposes it defines the path out of the vpc network including the path to the internet in addition to having this route instances must meet additional requirements if they need internet access the default route also provides a standard path for private google access and if you want to completely isolate your network from the internet or if you need to replace the default route with the custom route you can delete the default route now if you remove the default route and do not replace it packets destined to ip ranges that are not covered by other routes are dropped lastly the system generated default route has a priority of 1000 because its destination is the broadest possible which covers all ip addresses in the 0.0.0.0.0 range google cloud only uses it if a route with a more specific destination does not apply to a packet and i'll be getting into priorities in just a little bit and so now that we've covered the default route i wanted to get into the subnet route now subnet routes are system generated routes that define paths to each subnet in the vpc network each subnet has at least one subnet route whose destination matches the primary ip range of the subnet if the subnet has secondary ip ranges google cloud creates a subnet route with a corresponding destination for each secondary range no other route can have a destination that matches or is more specific than the destination of a subnet route you can create a custom route that has a broader destination range that contains the subnet route's destination range now when a subnet is created a corresponding subnet route for the subnet's primary and secondary ip range is also created auto mode vpc networks create a subnet route for the primary ip ranges of each of their automatically created subnets you can delete these subnets but only if you convert the auto mode vpc network to custom mode and you cannot delete a subnet route unless you modify or delete the subnet so when you delete a subnet all subnet routes for both primary and secondary ranges are deleted automatically you cannot delete the subnet route for the subnet's primary range in any other way and just as a note when networks are connected by using vpc network peering which i will get into a little bit later some subnet routes from one network are imported into the other network and vice versa and cannot be removed unless you break the peering relationship and so when you break the peering relationship all imported subnet routes from the other network are automatically removed so now that we've covered the system generated routes i wanted to get into custom routes now custom routes are either static routes that you can create manually or dynamic routes maintained automatically by one or more of your cloud routers and these are created on top of the already created system generated routes destinations for custom routes cannot match or be specific than any subnet route in the network now static routes can use any of the static route next hops and these can be created manually if you use the google cloud console to create a cloud vpn tunnel that uses policybased routing or one that is a route based vpn static routes for the remote traffic selectors are created for you and so just to give you a little bit more clarity and a little bit of context i've included a screenshot here for all the different routes that are available for the next hop we have the default internet gateway to define a path to external ip addresses specify an instance and this is where traffic is directed to the primary internal ip address of the vm's network interface in the vpc network where you define the route specify ip address is where you provide an internal ip address assigned to a google cloud vm as a next hop for cloud vpn tunnels that use policy based routing and routebased vpns you can direct traffic to the vpn tunnel by creating routes whose next hops refer to the tunnel by its name and region and just as a note google cloud ignores routes whose next hops are cloud vpn tunnels that are down and lastly for internal tcp and udp low balancing you can use a load balancer's ip address as a next hop that distributes traffic among healthy backend instances custom static routes that use this next hop cannot be scoped to specific instances by network tags and so when creating static routes you will always be asked for different parameters that are needed in order to create this route and so here i've taken a screenshot from the console to give you a bit more context with regards to the information that's needed so first up is the name and description so these fields identify the route a name is required but a description is optional and every route in your project must have a unique name next up is the network and each route must be associated with exactly one vpc network in this case it happens to be the default network but if you have other networks available you're able to click on the drop down arrow and choose a different network the destination range is a single ipv4 cider block that contains the ip addresses of systems that receive incoming packets and the ip range must be entered as a valid ipv4 cider block as shown in the example below the field now if multiple routes have identical destinations priority is used to determine which route should be used so a lower number would indicate a higher priority for example a route with a priority value of 100 has a higher priority than one with a priority value of 200 so the highest route priority means the smallest possible nonnegative number as well another great example is if you look back on your default routes all your subnet routes are of a priority of zero and the default internet gateway is of a priority of 1000 and therefore the subnet routes will take priority over the default internet gateway and this is due to the smaller number so remember a good rule of thumb is that the lower the number the higher the priority the higher the number the lower the priority now to get a little bit more granular you can specify a list of network tags so that the route only applies to instances that have at least one of the listed tags and if you don't specify any tags then google cloud applies the route to all instances in the network and finally next hop which was shown previously this is dedicated to static routes that have next hops that point to the options shown earlier so now that i've covered static routes in a bit of detail i want to get into dynamic routes now dynamic routes are managed by one or more cloud routers and this allows you to dynamically exchange routes between a vpc network and an onpremises network with dynamic routes their destinations always represent ip ranges outside of your vpc network and their next hops are always bgp peer addresses a cloud router can manage dynamic routes for cloud vpn tunnels that use dynamic routing as well as cloud interconnect and don't worry i'll be getting into cloud routers in a bit of detail in a later lesson now i wanted to take a minute to go through routing order and the routing order deals with priorities that i touched on a little bit earlier now subnet routes are always considered first because google cloud requires that subnet routes have the most specific destinations matching the ip address ranges of their respective subnets if no applicable destination is found google cloud drops the packet and replies with a network unreachable error system generated routes apply to all instances in the vpc network the scope of instances to which subnet routes apply cannot be altered although you can replace the default route and so just as a note custom static routes apply to all instances or specific instances so if the route doesn't have a network tag the route applies to all instances in the network now vpc networks have special routes that are used for certain services and these are referred to as special return paths in google cloud these routes are defined outside of your vpc network in google's production network they don't appear in your vpc network's routing table you cannot remove them or override them or if you delete or replace a default route in your vpc network although you can control traffic to and from these services by using firewall rules and the services that are covered are load balancers internet aware proxy or iap as well as cloud dns and so before i end this lesson i wanted to touch on private google access now vm instances that only have internal ip addresses can use private google access and this allows them to reach the external ip addresses of google's apis and services the source ip address of the packet can be the primary internal ip address of the network interface or an address in an alias ip range that is assigned to the interface if you disable private google access the vm instances can no longer reach google apis and services and will only be able to send traffic within the vpc network private google access has no effect on instances that have external ip addresses and can still access the internet they don't need any special configuration to send requests to the external ip addresses of google apis and services you enable private google access on a subnet by subnet basis and it's a setting for subnets in a vpc network and i will be showing you this in an upcoming demo where we'll be building our own custom vpc network now even though the next hop for the required routes is called the default internet gateway and the ip addresses for google apis and services are external requests to google apis and services from vms that only hold internal ip addresses in subnet 1 where private google access is enabled are not sent through the public internet those requests stay within google's network as well vms that only have internal ip addresses do not meet the internet access requirements for access to other external ip addresses beyond those for google apis and services now touching on this diagram here firewall rules in the vpc network have been configured to allow internet access vm1 can access google apis and services including cloud storage because its network interface is located in subnet 1 which has private google access enabled and because this instance only has an internal ip address private google access applies to this instance now with vm2 it can also access google apis and services including cloud storage because it has an external ip address private google access has no effect on this instance as it has an external ip address and private google access has not been enabled on that subnet and because both of these instances are in the same network they are still able to communicate with each other over an internal subnet route and so this is just one way where private google access can be applied there are some other options for private access as well you can use private google access to connect to google apis and services from your onpremises network through a cloud vpn tunnel or cloud interconnect without having any external ip addresses you also have the option of using private google access through a vpc network peering connection which is known as private services access and finally the last option available for private google access is connecting directly from serverless google services through an internal vpc connection now i know this has been a lot of theory to take in but i promise it'll become a lot easier and concepts will become less complicated when we start putting this into practice coming up soon in the demo of building our own custom vpc and so that's pretty much all i wanted to cover when it comes to routing and private google access so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i'm going to be discussing ip addressing now in the network refresher lesson i went into a bit of depth on how i p addresses are broken down and used for communication in computer networks in this lesson i'll be getting into the available types of ip addressing in google cloud and how they are used in each different scenario please note for the exam a high level overview will be needed to know when it comes to ip addressing but the details behind it will give you a better understanding on when to use each type of ip address so with that being said let's dive in now ip addressing in google cloud holds quite a few categories and really start by determining whether you are planning for communication internally within your vpc or for external use to communicate with the outside world through the internet once you determine the type of communication that you're looking to apply between resources some more decisions need to be made with regards to the other options and i will be going through these options in just a sec now in order to make these options a little bit more digestible i wanted to start off with the options available for internal ip addresses now internal ip addresses are not publicly advertised they are used only within a network now every vpc network or onpremises network has at least one internal ip address range resources with internal ip addresses communicate with other resources as if they're all on the same private network now every vm instance can have one primary internal ip address that is unique to the vpc network and you can assign a specific internal ip address when you create a vm instance or you can reserve a static internal ip address for your project and assign that address to your resources if you don't specify an address one will be automatically assigned to the vm in either case the address must belong to the ip range of the subnet and so if your network is an auto mode vpc network the address comes from the region subnet if your network is a custom mode vpc network you must specify which subnet the ip address comes from now all subnets have a primary sider range which is the range of internal ip addresses that define the subnet each vm instance gets its primary internal ip address from this range you can also allocate alias ip ranges from that primary range or you can add a secondary range to the subnet and allocate alias ip ranges from the secondary range use of alias ip ranges does not require secondary subnet ranges these secondary subnet ranges merely provide an organizational tool now when using ip aliasing you can configure multiple internal ip addresses representing containers or applications hosted in a vm without having to define a separate network interface and you can assign vm alias ip ranges from either the subnet's primary or secondary ranges when alias ip ranges are configured google cloud automatically installs vpc network routes for primary and alias ip ranges for the subnet of your primary network interface your container orchestrator or gke does not need to specify vpc network connectivity for these routes and this simplifies routing traffic and managing your containers now when choosing either an auto mode vpc or a custom vpc you will have the option to choose either an ephemeral ip or a static ip now an ephemeral ip address is an ip address that doesn't persist beyond the life of the resource for example when you create an instance or forwarding rule without specifying an ip address google cloud will automatically assign the resource an ephemeral ip address and this ephemeral ip address is released when you delete the resource when the ip address is released it is free to eventually be assigned to another resource so is never a great option if you depend on this ip to remain the same this ephemeral ip address can be automatically assigned and will be assigned from the selected region subnet as well if you have ephemeral ip addresses that are currently in use you can promote these addresses to static internal ip addresses so that they remain with your project until you actively remove them and just as a note before you reserve an existing ip address you will need the value of the ip address that you want to promote now reserving a static ip address assigns the address to your project until you explicitly release it this is useful if you are dependent on a specific ip address for a specific service and need to prevent another resource from being able to use the same address static addresses are also useful if you need to move an ip address from one google cloud resource to another and you also have the same options when creating an internal load balancer as you do with vm instances and so now that we've covered all the options for internal ip addresses i would like to move on to cover all the available options for external ip addresses now you can assign an external ip address to an instance or a forwarding rule if you need to communicate with the internet with resources in another network or need to communicate with a public google cloud service sources from outside a google cloud vpc network can address a specific resource by the external ip address as long as firewall rules enable the connection and only resources with an external ip address can send and receive traffic directly to and from outside the network and like internal ip addresses external ip addresses have the option of choosing from an ephemeral or static ip address now an ephemeral external ip address is an ip address that doesn't persist beyond the life of the resource and so follows the same rules as ephemeral internal ip addresses so when you create an instance or forwarding rule without specifying an ip address the resource is automatically assigned an ephemeral external ip address and this is something that you will see quite often ephemeral external ip addresses are released from a resource if you delete the resource for vm instances the ephemeral external ip address is also released if you stop the instance so after you restart the instance it is assigned a new ephemeral external ip address and if you have an existing vm that doesn't have an external ip address you can assign one to it forwarding rules always have an ip address whether external or internal so you don't need to assign an ip address to a forwarding rule after it is created and if your instance has an ephemeral external ip address and you want to permanently assign the ip to your project like ephemeral internal ip addresses you have the option to promote the ip address from ephemeral to static and in this case promoting an ephemeral external ip address to a static external ip address now when assigning a static ip address these are assigned to a project long term until they are explicitly released from that assignment and remain attached to a resource until they are explicitly detached for vm instances static external ip addresses remain attached to stopped instances until they are removed and this is useful if you are dependent on a specific ip address for a specific service like a web server or a global load balancer that needs access to the internet static external ip addresses can be either a regional or global resource in a regional static ip address allows resources of that region or resources of zones within that region to use the ip address and just as a note you can use your own publicly routable ip address prefixes as google cloud external ip addresses and advertise them on the internet the only caveat is that you must own and bring at the minimum a 24 cider block and so now that we've discussed internal and external ip addressing options i wanted to move into internal ip address reservations now static internal ips provide the ability to reserve internal ip addresses from the ip range configured in the subnet then assign those reserved internal addresses to resources as needed reserving an internal ip address takes that address out of the dynamic allocation pool and prevents it from being used for automatic allocations with the ability to reserve static internal ip addresses you can always use the same ip address for the same resource even if you have to delete and recreate the resource so when it comes to internal ip address reservation you can either reserve a static internal ip address before creating the associated resource or you can create the resource with an ephemeral internal ip address and then promote that ephemeral ip address to a static internal ip address and so just to give you a bit more context i have a diagram here to run you through it so in the first example you would create a subnet from your vpc network you would then reserve an internal ip address from that subnet's primary ip range and in this diagram is marked as 10.12.4.3 and will be held as reserved for later use with a resource and then when you decide to create a vm instance or an internal load balancer you can use the reserved ip address that was created in the previous step that i p address then becomes marked as reserved and in use now touching on the second example you would first create a subnet from your vpc network you would then create a vm instance or an internal load balancer with either an automatically allocated ephemeral ip address or a specific ip address that you've chosen from within that specific subnet and so once the ephemeral ip address is in use you can then promote the ephemeral ip address to a static internal ip address and would then become reserved and in use now when it comes to the external ip address reservation you are able to obtain a static external ip address by using one of the following two options you can either reserve a new static external ip address and then assign the address to a new vm instance or you can promote an existing ephemeral external ip address to become a static external ip address now in the case of external ip addresses you can reserve two different types a regional ip address which can be used by vm instances with one or more network interfaces or by network load balancers these ip addresses can be created either in the console or through the command line with the limitation that you will only be allowed to create ipv4 ip addresses the other type is a global ip address which can be used for global load balancers and can be created either in the console or through the command line as shown here the limitation here is that you must choose the premium network service tier in order to create a global ip address and after reserving the address you can finally assign it to an instance during instance creation or to an existing instance and so as you can see there is a lot to take in when it comes to understanding ip addressing and i hope this lesson has given you some better insight as to which type of ips should be used in a specific scenario now don't worry the options may seem overwhelming but once you start working with ip addresses more often the options will become so much clearer on what to use and when and as i said in the beginning only high level concepts are needed to know for the exam but knowing the options will allow you to make better decisions in your daily role as a cloud engineer and so that's pretty much all i wanted to cover when it comes to ip addressing in google cloud and so now that we've covered the theory behind ip addressing in google cloud i wanted to bring this into the console for a demo where we will get handson with creating both internal and external static ip addresses so as i explained before there was a lot to take in with this lesson so now would be a perfect opportunity to get up and have a stretch grab yourself a tea or a coffee and whenever you're ready join me back in the console so you can now mark this lesson as complete and i'll see you in the next welcome back in this demonstration i'm going to be going over how to create and apply both internal and external static ip addresses i'm going to show how to create them in both the console and the command line as well as how to promote ip addresses from ephemeral ips to static ips and once we're done creating all the ip addresses i'm going to show you the steps on how to delete them now there's a lot to get done here so let's dive in now for this demonstration i'm going to be using a project that has the default vpc created and so in my case i will be using project bowtieinc dev and so before you start make sure that your default vpc is created in the project that you had selected so in order to do that i'm going to head over to the navigation menu i'm going to scroll down to vpc network and we're going to see here that the default vpc has been created and so i can go ahead and start the demonstration and so the first thing i wanted to demonstrate is how to create a static internal ip address and so in order for me to demonstrate this i'm going to be using a vm instance and so i'm going to head over to the navigation menu again and i'm going to scroll down to compute engine and so here i'm going to create my new instance by simply clicking on create instance and so under name i'm going to keep it as instance 1. under region you want to select us east one and i'm going to keep the zone as the default selected under machine type i'm going to select the drop down and select e2 micro and i'm going to leave everything else as the default i'm going to scroll down here to management security disks networking and soul tenancy and i'm going to select the networking tab from there and so under here i'm going to select under network interfaces the default network interface and here is where i can create my static internal ip and so clicking on the drop down under primary internal ip you will see ephemeral automatic ephemeral custom and reserve static internal ip address and so you're going to select reserve static internal ip address and you'll get a popup prompting you with some fields to fill out to reserve a static internal ip address and so under name i'm going to call this static dash internal and for the purposes of this demo i'm going to leave the subnet and the static ip address as the currently selected if i wanted to select a specific ip address i can click on this drop down and select let me choose and this will give me the option to enter in a custom ip address with the subnet range that is selected for this specific sub network and so because i'm not going to do that i'm going to select assign automatically i'm going to leave the purpose as nonshared and i'm going to simply click on reserve and this is going to reserve this specific ip address and now as you can see here i have the primary internal ip marked as static internal and so this is going to be my first static internal ip address and so once you've done these steps you can simply click on done and you can head on down to the bottom and simply click on create to create the instance and when the instance finishes creating you will see the internal static ip address and as you can see here your static internal ip address has been assigned to the default network interface on instance 1. and so in order for me to view this static internal ip address in the console i can view this in vpc networks and drill down into the specific vpc and find it under static internal ip addresses but i wanted to show you how to view it by querying it through the command line and so in order to do this i'm going to simply go up to the menu bar on the right hand side and open up cloud shell and once cloud shell has come up you're going to simply paste in the command gcloud compute addresses list and this will give me a list of the internal ip addresses that are available and so now i'm going to be prompted to authorize this api call using my credentials and i definitely do so i'm going to click on authorize and as expected the static internal ip address that we created earlier has shown up it's marked as internal in the region of us east one in the default subnet and the status is in use and so as we discussed in the last lesson static ip addresses persist even after the resource has been deleted and so to demonstrate this i'm going to now delete the instance i'm going to simply check off the instance and go up to the top and click on delete you're going to be prompted to make sure if you want to delete this yes i do so i'm going to click on delete and so now that the instance has been deleted i'm going to query the ip addresses again by using the same command gcloud compute addresses list i'm going to hit enter and as you can see here the ip address static dash internal still persists but the status is now marked as reserved and so if i wanted to use this ip address for another instance i can do so by simply clicking on create instance up here at the top menu and then i can select static dash internal as my ip address so i'm going to quickly close down cloud shell and i'm going to leave the name as instance one the region can select us east one and we're going to keep the zone as the default selected under machine type you're going to select the e2 micro machine type going to scroll down to management security disks networking into soul tenancy and i'm going to select the networking tab from under here and under network interfaces i'm going to select the default network interface and under primary internal ip if i click on the drop down i have the option of selecting the static dash internal static ip address and so i wanted to move on to demonstrate how to promote an internal ephemeral ip address to an internal static ip address and so in order to do this i'm going to select on ephemeral automatic and i'm going to scroll down and click on done and i'm going to go ahead and create the instance and once the instance is ready i'll be able to go in and edit the network interface and so the instance is up and ready and so i'm going to drill down into the instance and i'm going to go up to the top and click on edit i'm going to scroll down to network interfaces and i'm going to edit the default network interface so i'm going to scroll down a little bit more and here under internal iptype i'm going to click on the drop down and i'm going to select static and so here you are taking the current ip address which is 10.142.0.4 and promoting it to a static internal ip address and so you're going to be prompted with a popup confirming the reservation for that static internal ip address and so notice that i don't have any other options and so all i'm going to do is type in a name and i'm going to call this promoted static and i'm going to click on reserve and this will promote the internal ip address from an ephemeral ip address to a static ip address and so now i'm just going to click on done and i'm going to scroll down and click on save and so now because i want to verify the ip address i'm going to go ahead and open up the cloud shell again and i'm going to use the same command that i used earlier which is gcloud compute addresses list and i'm going to hit enter as expected the promoted static ip address is showing as an internal ip address in the region of us east 1 in the default subnet and its status is in use and so just as a recap we've created a static internal ip address for the first instance and for the second instance we promoted an ephemeral internal ip address into a static internal ip address and we were able to verify this through cloud shell using the gcloud compute addresses list command and so this is the end of part one of this demo it was getting a bit long so i decided to break it up and this would be a great opportunity for you to get up and have a stretch get yourself a coffee or tea and whenever you're ready join me in part two where we will be starting immediately from the end of part one so you can now mark this as complete and i'll see you in the next one welcome back this is part two of the creating internal and external ip addresses demo and we will be starting immediately from the end of part one so with that being said let's dive in and so now that we've gone through how to both create static ip addresses and promote ephemeral ip addresses to static ip addresses for internal ips i want to go ahead and go through the same with external ips and so i'm going to first start off by deleting this instance i'm going to go ahead and click on delete and so instead of doing it through the compute engine interface i want to go into the external ip address interface which can be found in the vpc network menu so i'm going to go ahead up to the left hand corner click on the navigation menu and i'm going to scroll down to vpc network and from the menu here on the left hand side you can simply click on external ip addresses and here you will see the console where you can create a static external ip address and so to start the process you can simply click on reserve static address and so here you'll be prompted with a bunch of fields to fill out to create this new external static ip address and so for the name of this static ip address you can simply call this external dash static i'm going to use the same in the description now here under network service tier i can choose from either the premium or the standard and as you can see i'm currently using the premium network service tier and if i hover over the question mark over here it tells me a little bit more about this network service tier and as you can see the premium tier allows me higher performance as well as lower latency routing but this premium routing comes at a cost whereas the standard network service tier offers a lower performance compared to the premium network service tier and is a little bit more cost effective but still delivering performance that's comparable with other cloud providers and so i'm just going to leave it as the default selected and as we discussed in the previous lesson ipv6 external static ip addresses can only be used for global load balancers and so since we're only using it for an instance an ipv4 address will suffice and so just as a note for network service tier if i click on standard ipv6 is grayed out as well as the global selection and this is because in order to use global load balancing you need to be using the premium network service tier so whenever you're creating a global load balancer please keep this in mind as your cost may increase so i'm going to switch this back to premium and so under type i'm going to keep it as regional and under region i'm going to select the same region that my instance is going to be in which is us east 1 and because i haven't created the instance yet there is nothing to attach it to and so i'm going to click on the drop down and click on none and so just as another note i wanted to quickly highlight this caution point that the static ip addresses not attached to an instance or low balancer are still billed at an hourly rate so if you're not using any static ip addresses please remember to delete them otherwise you will be charged and so everything looks good here to create my external static ip address so i'm going to simply click on reserve and this will create my external static ip address and put the status of it as reserved so as you can see here the external static ip address has been created and you will find all of your external static ip addresses that you create in future right here in this menu and you will still be able to query all these external ip addresses from the command line and so now in order to assign this ip address to a network interface i'm going to go back over to the navigation menu and scroll down to compute engine and create a new instance so you can go ahead and click on create instance i'm going to go ahead and keep the name of this instance as instance one and in the region i'm going to select us east one i'm going to keep the zone as the selected default and under machine type i'm going to select the e2 micro machine type i'm going to scroll down to management security disks networking and soul tenancy and i'm going to select the networking tab and here under network interfaces i'm going to select the default network interface i'm going to scroll down a little bit here and under external ip ephemeral has been selected but if i click on the drop down i will have the option to select the ip that we had just created which is the external dash static ip and so i'm going to select that i'm going to click on done and you can go down and click on create and so now when the instance is created i will see the external ip address of external static as the assigned external ip and as expected here it is and because i always like to verify my work i'm going to go ahead and open up the cloud shell and verify it through the command line and so now i'm going to query all my available static ip addresses using the command gcloud compute addresses list i'm going to hit enter and as you can see here the external static ip address of 34.75.76 in the us east one region is now in use and this is because it is assigned to the network interface on instance one and so before we go ahead and complete this demo there's one more step that i wanted to go through and this is to promote an ephemeral external ip address to a static external ip address and so i'm going to go up here to the top menu and create a new instance i'm going to leave the name here as instance two under the region i'm going to select us east one i'm going to keep the zone as the selected default under machine type i'm going to select the e2 micro machine type i'm going to leave everything else as the default and i'm going to scroll down to management security disks networking and soul tenancy and select the networking tab and i'm going to verify that i'm going to be using an ephemeral external ip upon the creation of this instance if i scroll down here a little bit i can see that an external ephemeral ip address will be used upon creation and this will be the ip address that i will be promoting to a static ip through the command line so i'm going to go ahead and scroll down click on done and then i'm going to scroll down and click on create and once this instance is created then i can go ahead and promote the ephemeral external ip address okay and the instance has been created along with its external ephemeral ip address and so now i can go ahead and promote this ephemeral ip address so in order for me to do this i'm going to move back to my cloud shell and i'm going to quickly clear my screen and i'm going to use the command gcloud compute addresses create and then the name that we want to use for this static external ip address so i'm going to call this promoted external i'm going to use the flag dash dash addresses and so here i will need the external ip address that i am promoting which is going to be 104.196.219.42 and so i'm going to copy this to my clipboard and i'm going to paste it here in the command line and now i'm going to add the region flag along with the region of us east one and i'm going to go ahead and hit enter and success my ephemeral external ip address has been promoted to a static external ip address and of course to verify it i'm going to simply type in the gcloud compute addresses list command i'm going to hit enter and as expected here it is the promoted external ip of 104.196.219.42 marked as external in the u.s east one region and the status is marked as in use and so i wanted to take a moment to congratulate you on making it through this demonstration of creating internal and external ip addresses as well as promoting them so just as a recap you've created a static internal ip address in conjunction with creating a new instance and assigning it to that instance you then created another instance and used an ephemeral ip and then promoted it to a static internal ip address you then created an external static ip address using the console and assigned it to a brand new instance you then created another instance using an external ephemeral ip address and promoted it to a static external ip address and you did this all using both the console and the command line so i wanted to congratulate you on a great job now before we end this demonstration i wanted to go through the steps of cleaning up any leftover resources so the first thing you want to do is delete these instances so you can select them all and go up to the top and click on delete it's going to ask you if you want to delete the two instances yes we do click on delete and this will delete your instances and free up the external ip addresses so that you're able to delete them and so now that the instances have been deleted i'm going to go over to the vpc network menu and i'm going to head on over to the external ip address console and here i'm able to delete the external ip addresses and so i'm going to select all of them and i'm going to go up to the top menu and click on release static address and you should get a prompt asking you if you want to delete both these addresses the answer is yes click on delete and within a few seconds these external ip addresses should be deleted and so now all that's left to delete are the two static internal ip addresses and as i said before because there is no console to be able to view any of these static internal ip addresses i have to do it through the command line so i'm going to go back to my cloud shell i'm going to clear the screen and i'm going to list the ip addresses currently in my network and so here they are promoted static and static internal and so the command to delete any static ip addresses is as follows gcloud compute addresses delete the name of the ip address that i want to delete which is promoted static and then i will need the region flag and it'll be the region of us east one and i'm going to go ahead and hit enter it's going to prompt me if i want to continue with this and i'm going to type y for yes hit enter and success it has been deleted and so just a double check i'm going to do a quick verification and yes it has been deleted and so all that's left to delete is the static internal ip address and so i'm going to paste in the command gcloud compute addresses delete the name of the ip address that i want to delete which is static dash internal along with the region flag of us east one i'm going to go ahead and hit enter y for yes to continue and success and one last verification to make sure that it's all cleared up and as you can see i have no more static i p addresses and so this concludes this demonstration on creating assigning and deleting both static internal and static external ip addresses and so again i wanted to congratulate you on a great job and so that's pretty much all i wanted to cover in this demo on creating internal and external static ip addresses so you can now mark this as complete and i'll see you in the next one welcome back in this lesson i will be diving into some network security by introducing vpc firewall rules a service used to filter incoming and outgoing network traffic based on a set of userdefined rules a concept that you should be fairly familiar with for the exam and comes up extremely often when working as an engineer in google cloud it is definitely an essential security layer that prevents unwanted access to your cloud infrastructure now vpc firewall rules apply to a given project and network and if you'd like you can also apply firewall rules across an organization but i will be sticking to strictly vpc firewall rules in this lesson now vpc firewall rules let you allow or deny connections to or from your vm instances based on a configuration that you specify and these rules apply to either incoming connections or outgoing connections but never both at the same time enabled vpc firewall rules are always enforced regardless of their configuration and operating system even if they have not started up now every vpc network functions as a distributed firewall when firewall rules are defined at the network level connections are allowed or denied on a per instance basis so you can think of the vpc firewall rules as existing not only between your instances and other networks but also between individual instances within the same network now when you create a vpc firewall rule you specify a vpc network and a set of components that define what the rule does the components enable you to target certain types of traffic based on the traffic's protocol ports sources and destinations when you create or modify a firewall rule you can specify the instances to which it is intended to apply by using the target component of the rule now in addition to firewall rules that you create google cloud has other rules that can affect incoming or outgoing connections so for instance google cloud doesn't allow certain ip protocols such as egress traffic on tcp port 25 within a vpc network and protocols other than tcp udp icmp and gre to external ip addresses of google cloud resources are blocked google cloud always allows communication between a vm instance and its corresponding metadata server at 169.254 and this server is essential to the operation of the instance so the instance can access it regardless of any firewall rules that you configure the metadata server provides some basic services to the instance like dhcp dns resolution instance metadata and network time protocol or ntp now just as a note every network has two implied firewall rules that permit outgoing connections and block incoming connections firewall rules that you create can override these implied rules now the first implied rule is the allow egress rule and this is an egress rule whose action is allow and the destination is all ips and the priority is the lowest possible and lets any instance send traffic to any destination except for traffic blocked by google cloud the second implied firewall rule is the deny ingress rule and this is an ingress rule whose action is deny and the source is all ips and the priority is the lowest possible and protects all instances by blocking incoming connections to them now i know we touched on this earlier on in a previous lesson but i felt the need to bring it up as these are prepopulated rules and the rules that i'm referring to are with regards to the default vpc network and as explained earlier these rules can be deleted or modified as necessary the rules as you can see here in the table allow ingress connections from any source to any instance on the network when it comes to icmp rdp on port 3389 for windows remote desktop protocol and for ssh on port 22. and as well the last rule allows ingress connections for all protocols and ports among instances in the network and it permits incoming connections to vm instances from others in the same network and all of these have a rule priority of six five five four which is the second to lowest priority so breaking down firewall rules there are a few characteristics that google put in place that help define these rules and the characteristics are as follows each firewall rule applies to incoming or outgoing connections and not both firewall rules only support ipv4 connections so when specifying a source for an ingress rule or a destination for an egress rule by address you can only use an ipv4 address or ipv4 block insider notation as well each firewall rules action is either allow or deny you cannot have both at the same time and the rule applies to connections as long as it is enforced so for example you can disable a rule for troubleshooting purposes and then enable it back again now when you create a firewall rule you must select a vpc network while the rule is enforced at the instance level its configuration is associated with a vpc network this means you cannot share firewall rules among vpc networks including networks connected by vpc network peering or by using cloud vpn tunnels another major thing to note about firewall rules is that they are stateful and so that means when a connection is allowed through the firewall in either direction return traffic matching this connection is also allowed you cannot configure a firewall rule to deny associated response traffic return traffic must match the five tuple of the accepted request traffic but with the source and destination addresses and ports reversed so just as a note for those who may be wondering what a five tuple is i was referring to the set of five different values that comprise a tcpip connection and this would be source ip destination ip source port destination port and protocol google cloud associates incoming packets with corresponding outbound packets by using a connection tracking table google cloud implements connection tracking regardless of whether the protocol supports connections if a connection is allowed between a source and a target or between a target and a destination all response traffic is allowed as long as the firewalls connections tracking state is active and as well as a note a firewall rules tracking state is considered active if at least one packet is sent every 10 minutes now along with the multiple characteristics that make up a firewall rule there are also firewall rule components that go along with it here i have a screenshot from the console with the configuration components of a firewall rule and i wanted to take a moment to highlight these components for better clarity so now the first component is the network and this is the vpc network that you want the firewall rule to apply to the next one is priority which we discussed earlier and this is the numerical priority which determines whether the rule is applied as only the highest priority rule whose other components match traffic is applied and remember the lower the number the higher the priority the higher the number the lower the priority now the next component is the direction of traffic and these are the ingress rules that apply to incoming connections from specified sources to google cloud targets and this is where ingress rules apply to incoming connections from specified sources to google cloud targets and egress rules apply to connections going to specify destinations from targets and the next one up is action on match and this component either allows or denies which determines whether the rule permits or blocks the connection now a target is what defines which instances to which the rule applies and you can specify a target by using one of the following three options the first option are all instances in the network and this is the firewall rule that does exactly what it says it applies to all the instances in the network the second option is instances by target tags and this is where the firewall rule applies only to instances with a matching network tag and so i know i haven't explained it earlier but a network tag is simply a character string added to a tags field in a resource so let's say i had a bunch of instances that were considered development i can simply throw a network tag on them using a network tag of dev and apply the necessary firewall rule for all the development servers holding the network tag dev and so the third option is instances by target service accounts this is where the firewall rule applies only to instances that use a specific service account and so the next component is the source filter and this is a source for ingress rules or a destination for egress rules the source parameter is only applicable to ingress rules and it must be one of the following three selections source ip ranges and this is where you specify ranges of ip addresses as sources for packets either inside or outside of google cloud the second one is source tags and this is where the source instances are identified by a matching network tag and source service accounts where source instances are identified by the service accounts they use you can also use service accounts to create firewall rules that are a bit more granular and so one of the last components of the firewall rule is the protocols and ports you can specify a protocol or a combination of protocols and their ports if you omit both protocols and ports the firewall rule is applicable for all traffic on any protocol and any port and so when it comes to enforcement status of the firewall rule there is a drop down right underneath all the components where you can enable or disable the enforcement and as i said before this is a great way to enable or disable a firewall rule without having to delete it and is great for troubleshooting or to grant temporary access to any instances and unless you specify otherwise all firewall rules are enabled when they are created but you can also choose to create a rule in a disabled state and so this covers the vpc firewall rules in all its entirety and i will be showing you how to implement vpc firewall rules along with building a custom vpc custom routes and even private google access all together in a demo following this lesson to give you some handson skills of putting it all into practice and so that's pretty much all i wanted to cover when it comes to vpc firewall rules so you can now mark this lesson as complete and let's move on to the next one where we dive in and build our custom vpc so now is a perfect time to grab a coffee or tea and whenever you're ready join me in the console welcome back in this demonstration i want to take all the concepts that we've learned so far in this networking section and put it all into practice this diagram shown here is the architecture of exactly what we will be building in this demo we're going to start by creating a custom vpc and then we're going to create two subnets one public and one private in two separate regions we're then going to create a cloud storage bucket with some objects in it and then we will create some instances to demonstrate access to cloud storage as well as communication between instances and finally we're going to create some firewall rules for routing traffic to all the right places we're also going to implement private google access and demonstrate accessibility to the files in cloud storage from the private instance without an external ip so this may be a little bit out of your comfort zone for some but don't worry i'll be with you every step of the way and other than creating the instances all the steps here have been covered in previous lessons now there's a lot to get done here so whenever you're ready join me in the console and so here we are back in the console and as you can see up here in the right hand corner i am logged in as tony bowtie ace gmail.com and currently i am logged in under project tony and so in order to start off on a clean slate i'm going to create a new project and so i'm going to simply click on the project menu dropdown and click on new project i'm going to call this project bowtie inc and i don't have any organizations so i'm going to simply click on create and as well for those of you doing this lesson i would also recommend for you to create a brand new project so that you can start off anew again i'm going to go over to the project drop down and i'm going to select bow tie ink as the project and now that i have a fresh new project i can now create my vpc network so i'm going to go over to the left hand corner to the navigation menu and i'm going to scroll down to vpc network and so because vpc networks are tied in with the compute engine api we need to enable it before we can create any vpc networks so you can go ahead and enable this api so once this api has finished and is enabled we'll be able to create our vpc network ok and the api has been enabled and as you can see the default vpc network has been created with a subnet in every region along with its corresponding ip address ranges and so for this demo we're going to create a brand new vpc network along with some custom subnets and so in order to do that i'm going to go up here to the top and i'm going to click on create vpc network and so here i'm prompted with some fields to fill out so under name i'm going to think of a creative name that i can call my vpc network so i'm going to simply call it custom under description i'm going to call this custom vpc network and i'm going to move down here to subnets and because i'm creating custom subnets i'm going to keep it under custom under subnet creation mode and so i'm going to need a public subnet and a private subnet and you'll be able to get the values from the text file in the github repository within the sub networks folder under networking services and so i'm going to create my public subnet first and i'm going to simply call the public subnet public for region i'm going to use us east one and the ip address range will be 10.0.0.0 forward slash 24 and i'm going to leave private google access off and i'm going to simply click on done and now i can create the private subnet so underneath the public subnet you'll see add subnet you can simply click on that and the name of the new subnet will be as you guessed it private under region i'm going to use us east 4 and for the ip address range be sure to use 10.0.5.0.24 and we're going to leave private google access off for now and we'll be turning that on a little bit later in the demo and so you can now click on done and before we click on create we want to enable the dns api and clicking on enable will bring you to the dns api home page and you can click on enable to enable the api okay so now that we have our network configured along with our public and private subnets as well as dns being enabled we can now simply click on create but before i do that i wanted to give you some insight with regards to the command line so as i've shared before everything that can be done in the console can be done through the command line and so if ever you wanted to do that or you wanted to get to know the command line a little bit better after filling out all the fields with regards to creating resources in the console you will be given the option of a command line link that you can simply click on and here you will be given all the commands to create all the same resources with all the same preferences through the command line and i will be providing these commands in the lesson text so that you can familiarize yourself with the commands to use in order to build any networks using the command line but this is a great reference for you to use at any time and so i'm going to click on close and now i'm going to click on create and within a minute or two the custom vpc network will be created and ready to use okay and the custom vpc network has been created along with its public and private subnet and so just to get a little bit more insight with this custom vpc network i'm going to drill down into it and as you can see here the subnets are respectively labeled private and public along with its region ip address range the gateway and private google access the routes as you can see here are the system generated routes that i had discussed in an earlier lesson it has both the subnet routes to its respective ip range along with the default route with a path to the internet as well as a path for private google access now we don't have any firewall rules here yet but we'll be adding those in just a few minutes and so now that you've created the vpc network with its respective subnets we're going to head on over to cloud storage and create a bucket along with uploading the necessary files so i'm going to go again over to the navigation menu and i'm going to scroll down to storage and so as expected there are no buckets present here in cloud storage and so we're just going to go ahead and create our first bucket by going up here to the top menu and clicking on create bucket and so here i've been prompted to name my bucket and for those of you who are here for the first time when it comes to naming a storage bucket the name needs to be globally unique and this means that the name has to be unique across all of the google cloud platform now don't worry i'm going to get into further detail with this in the cloud storage lesson with all of these specific details when it comes to names storage classes and permissions and so in the meantime you can come up with a name for your bucket something that resonates with you and so for me i'm going to name my bucket bowtie inc dash file dash access and so now i'm going to simply click continue and so just as a note for those who are unable to continue through it is because the name for your bucket is not globally unique so do try to find one that is now when it comes to location type i'm just going to click on region and you can keep the default location as used one and i'm going to leave all the other options as default and i'm going to go down to the bottom and click create and so for those of you who have created your bucket you can now upload the files and those files can be found in the github repository in the cloud storage bucket folder under networking services and so now i'm going to click on upload files and under the networking services section under cloud storage bucket you will find these three jpeg files and you can simply select them and click on open and so they are now uploaded into the bucket and so now i'm ready to move on to the next step so you should now have created the vpc network with a private and public subnet along with creating your own bucket in cloud storage and have uploaded the three jpeg files so now that this is done we can now create the instances that will have access to these files and so again i will go over to the navigation menu in the top left hand corner and scroll down to compute engine and here i will click on create and so again i will be prompted with some fields to fill out and so for this instance i'm going to first create the public instance again i'm going to get really creative and call this public dash instance under labels i'm going to add a label under key i'm going to type environment and under value i'm going to type in public i'm going to go down to the bottom and click on save and under region i'm going to select us east1 and you can leave the zone as us east 1b moving down under machine type i'm going to select the e2 micro as the machine type just because i'm being cost conscious and i want to keep the cost down and so i'm going to scroll down to identity and api access and under service account you should have the compute engine default service account already preselected now under access scopes i want to be able to have the proper permissions to be able to read and write to cloud storage along with read and write access to compute engine and so you can click on set access for each api and you can scroll down to compute engine click on the drop down menu and select read write and this will give the public instance the specific access that it needs to ssh into the private instance and so now i'm going to set the access for cloud storage so i'm going to scroll down to storage i'm going to click on the drop down menu and select read write and this will give the instance read write access to cloud storage scrolling down a little bit further i'm going to go to management security disks networking and sold tenancy and i'm going to click on that scroll up here just a little bit and you can click on the networking tab which will prompt you for a bunch of options that you can configure for the networking of the instance so under network tags i want to type in public and you can click enter you can then scroll down to where it says network interfaces and click on the current interface which is the default and here it'll open up all your options and so under network you want to click on the drop down and set it from default to custom the public subnet will automatically be propagated so you can leave it as is and you also want to make sure that your primary internal ip as well as your external ip are set to ephemeral and you can leave all the other options as default and simply click on done and again before clicking on create you can click on the command line link and it will show you all the commands needed in order to create this instance through the command line so i'm going to go ahead and close this and so i'm going to leave all the other options as default and i'm going to click on create and so now that my public instance is being created i'm going to go ahead and create my private instance using the same steps that i did for the last instance so i'm going to go ahead and click on create instance here at the top and so the first thing i'm going to be prompted for is the name of the instance and so i'm going to call this instance private dash instance and here i'm going to add a label the key being environment and the value being private i'm going to go down here to the bottom and click on save and under region i'm going to select us east 4 and you can keep the zone as the default selected under machine type we're going to select the e2 micro and again scrolling down to the identity and api access under the access scopes for the default service account i'm going to click on the set access for each api and i'm going to scroll down to storage i'm going to click on the drop down menu and i'm going to select access for read write and for the last step i'm going to go into the networking tab under management security disks networking and soul tenancy and under network tags i'm going to give this instance a network tag of private and under network interfaces we want to edit this and change it from default over to the custom network and as expected it selected the private subnet by default and because this is going to be a private instance we are not going to give this an external ip so i'm going to click on the drop down and select none and with all the other options set as default i'm going to simply click on create and this will create my private instance along with having my public instance so just as a recap we've created a new custom vpc network along with a private and public subnet we've created a storage bucket and added some files in it to be accessed and we've created a private and public instance and assigning the service account on the public instance read write access to both compute engine and cloud storage along with a public ip address and assigning the service account on the private instance read write access only for cloud storage and no public ip and so this is the end of part one of this demo and this would be a great opportunity for you to get up and have a stretch get yourself a coffee or tea and whenever you're ready you can join me in part two where we will be starting immediately from the end of part one so you can go ahead and complete this video and i will see you in part two welcome back this is part two of the custom vpc demo and we will be starting exactly where we left off from part one so with that being said let's dive in and so now the last thing that needs to be done is to simply create some firewall rules and so with these firewall rules this will give me ssh access into the public instance as well as allowing private communication from the public instance to the private instance as well as giving ssh access from the public instance to the private instance and this will allow us to access the files in the bucket from the private instance and so in order to create these firewall rules i need to go back to my vpc network so i'm going to go up to the left hand corner again to the navigation menu and scroll down to vpc network over here on the left hand menu you'll see firewall i'm going to click on that and here you will see all the default firewall rules for the default network so for us to create some new ones for the custom vpc i'm going to go up here to the top and click on create firewall and so the first rule i want to create is for my public instance and i want to give it public access as well as ssh access and so i'm going to name this accordingly as public dash access i'm going to give this the same description always a good idea to turn on logs but for this demonstration i'm going to keep them off under network i'm going to select the custom network i'm going to keep the priority at 1000 the direction of traffic will be ingress and the action on match will be allow and so here is where the target tags come into play when it comes to giving access to the network so targets we're going to keep it as specified target tags and under target tags you can simply type in public under source filter you can keep it under ip ranges and the source ip range will be 0.0.0.0 forward slash 0. and we're not going to add a second source filter here so moving down to protocols and ports under tcp i'm going to click that off and add in port 22. and because i want to be able to ping the instance i'm going to have to add another protocol which is icmp and again as explained earlier the disable rule link will bring up the enforcement and as you can see it is enabled but if you wanted to create any firewall rules in future and have them disabled you can do that right here but we're gonna keep this enabled and we're gonna simply click on create and this will create the public firewall rule for our public instance in our custom vpc network and so we're going to now go ahead and create the private firewall rule and so i'm going to name this private dash access respectively i'm going to put the description as the same under network i'm going to select our custom network keep the priority at 1000 direction of traffic should be at ingress and the action on match should be allow for target tags you can type in private and then hit enter and because i want to be able to reach the private instance from the public instance the source ip range will be 10.0.0.1 forward slash 24. we're not going to add a second source filter and under protocols and ports we're going to simply add tcp port 22 and again i want to add icmp so that i'm able to ping the instance and i'm going to click on create and so we now have our two firewall rules private access and public access and if i go over to the custom vpc network and i drill into it i'll be able to see these selective firewall rules under the respective firewall rules tab and so now that we've created our vpc network along with the public and private subnet we've created the cloud storage bucket with the files that we need to access the instances that will access those files along with the firewall rules that will allow the proper communication we can now go ahead to test everything that we built and make sure that everything is working as expected so let's kick things off by first logging into the public instance so you can head on over to the navigation menu and scroll down to compute engine and you can ssh into the public instance by clicking on ssh under connect and this should open up a new tab or a new window logging you in with your currently authenticated credentials okay and we are logged into our instance and i'm going to zoom in for better viewing and so just to make sure that everything is working as expected we know that our firewall rule is correct because we are able to ssh into the instance and now i want to see if i have access to my files in the bucket and so in order to do that i'm going to run the gsutil command ls for list and then gs colon forward slash forward slash along with my bucket name which is bow tie inc hyphen file iphone access and i'm going to hit enter and as you can see i have access to all the files in the bucket and the last thing i wanted to check is if i can ping the private instance so i'm going to first clear my screen and i'm going to head on over back to the console i'm going to copy the ip address of the private instance to my clipboard and then i'm going to head back on over to my terminal and i'm going to type in ping i'm going to paste the ip address and success i am able to successfully ping the private instance from the public instance using the icmp protocol and you can hit control c to stop the ping so now that i know that my public instance has the proper permissions to reach cloud storage as well as being able to ping my private instance i want to be able to check if i can ssh into the private instance from my public instance and so i'm going to first clear my screen and next i'm going to paste in this command in order for me to ssh into the private instance g cloud compute ssh dash dash project and my project name which is bow tie inc dash dash zone and the zone that my instance is in which is us east 4c along with the name of the instance which is private dash instance and along with the flag dash dash internal dash ip stating that i am using the internal ip in order to ssh into the instance and i'm going to hit enter and so now i've been prompted for a passphrase in order to secure my rsa key pair as one is being generated to log into the private instance now it's always good practice when it comes to security to secure your key pair with a passphrase but for this demo i'm just going to leave it blank and so i'm just going to hit enter i'm going to hit enter again now i don't want to get too deep into it but i did want to give you some context on what's happening here so when you log into an instance on google cloud with os login disabled google manages the authorized keys file for new user accounts based on ssh keys in metadata and so the keys that are being generated that are being used for the first time are currently being stored within the instance metadata so now that i'm logged into my private instance i'm going to quickly clear my screen and just as a note you'll be able to know whether or not you're logged into your private instance by looking here at your prompt and so now i want to make sure that i can ping my public instance so i'm going to quickly type the ping command i'm going to head on over to the console i'm going to grab the ip address of the public instance i'm going to go back to my terminal and paste it in and as expected i'm able to ping my public instance from my private instance i'm just going to go ahead and hit control c to stop and i'm going to clear the screen so now we'd like to verify whether or not we have access to the files in the cloud storage bucket that we created earlier and so now i'm going to use the same command that i used in the public instance to list all the files in the cloud storage bucket so i'm going to use the gsutil command ls for list along with gs colon forward slash forward slash and the bucket name which is bow tie ink hyphen file if an access and i'm going to hit enter and as you can see here i'm not getting a response and the command is hanging and this is due to the fact that external access is needed in order to reach cloud storage and this instance only has an internal or private ip so accessing the files in the cloud storage bucket is not possible now in order to access cloud storage and the set of external ip addresses used by google apis and services we can do this by enabling private google access on the subnet used by the vms network interface and so we're going to go ahead and do that right now so i'm going to hit control c to stop and i'm going to go back into the console i'm going to go to the navigation menu and i'm going to scroll down to vpc network and then i'm going to drill down into the private subnet and i'm going to edit it under private google access i'm going to turn it on and i'm going to go down to the bottom and click on save and by giving this subnet private google access i will allow the private instance and any instances with private ip addresses to access any public apis such as cloud storage so now when i go back to my instance i'm going to clear the screen here and i'm going to run the gsutil command again and success we are now able to access cloud storage due to enabling private google access on the respective private subnet so i first wanted to congratulate you on making it to the end of this demo and hope that this demo has been extremely useful as this is a real life scenario that can come up and so just as a recap you've created a custom network with two custom subnets you've created a cloud storage bucket and uploaded some files to it you've created a public instance and a private instance and then created some firewall rules to route the traffic you then tested it all by using the command line for communication you also enable private google access for the instance with only the internal ip to access google's public apis so that it can access cloud storage and so again fantastic job on your part as this was a pretty complex demo and you can expect things like what you've experienced in this demo to pop up in your role of being a cloud engineer at any time so before you go be sure to delete all the resources you've created and again congrats on the great job so you can now mark this as complete and i'll see you in the next one welcome back in this lesson i will be going over vpc network peering and how you can privately communicate across vpcs in the same or different organization vpc network peering and vpc peering are used interchangeably in this lesson as they are used to communicate the same thing now for instances in one vpc to communicate with an instance in another vpc they would route traffic via the public internet however to communicate privately between two vpcs google cloud offers a service called vpc peering and i will be going through the theory and concepts of vpc peering throughout this lesson so with that being said let's dive in now vpc peering enables you to peer vpc networks so that workloads in different vpc networks can communicate in a private space that follows the rfc 1918 standard thus allowing private connectivity across two vpc networks traffic stays within google's network and never traverses the public internet vpc peering gives you the flexibility of peering networks that are of the same or different projects along with being able to peer with other networks in different organizations vpc peering also gives you several advantages over using external ip addresses or vpns to connect the first one is reducing network latency as all peering traffic stays within google's highspeed network vpc peering also offers greater network security as you don't need to have services exposed to the public internet and deal with greater risks of having your traffic getting compromised or if you're trying to achieve compliance standards for your organization vpc peering will allow you to achieve the standards that you need and finally vpc network peering reduces network costs as you save on egress costs for traffic leaving gcp so in a regular network google charges you for traffic communicating using public ips even if the traffic is within the same zone now you can bypass this and save money by using internal ips to communicate and keeping the traffic within the gcp network now there are certain properties or characteristics that peered vpcs follow and i wanted to point these out for better understanding first off peer vpc networks remain administratively separate so what exactly does this mean well it means that routes firewalls vpns and other traffic management tools are administered and applied separately in each of the vpc networks so this applies to each vpc independently which also means that each side of a peering association is set up independently as well so when you connect one vpc to the other you have to go into each vpc that you are connecting to both initiate and establish the connection peering becomes active only when the configuration from both sides match this also means that each vpc can delete the peering association at any given time now during vpc peering the vpc peers always exchange all subnet routes you also have the option of exchanging custom routes subnet and static routes are global and dynamic routes can be regional or global a given vpc network can peer with multiple vpc networks but there is a limit that you can reach in which you would have to reach out to google and ask the limit to be increased now when peering with vpc networks there are certain restrictions in place that you should be aware of first off a subnet cider range in one peered vpc network cannot overlap with a static route in another peered network this rule covers both subnet routes and static routes so when a vpc subnet is created or a subnet ip range is expanded google cloud performs a check to make sure that the new subnet range does not overlap with ip ranges of subnets in the same vpc network or in directly peered vpc networks if it does the creation or expansion will fail google cloud also ensures that no overlapping subnet ip ranges are allowed across vpc networks that have appeared network in common and again if it does the creation or expansion will fail now speaking of routing when you create a new subnet in appeared vpc network vpc network peering doesn't provide granular route controls to filter out which subnet cider ranges are reachable across pure networks these are handled by firewall rules so to allow ingress traffic from vm instances in a peer network you must create ingress allow firewall rules by default ingress traffic to vms is blocked by the implied deny ingress rule another key point to note is that transitive peering is not supported and only directly peered networks can communicate so they have to be peered directly in this diagram network a is peered with network b and network b is peered with network c and so if one instance is trying to communicate from network a to network c this cannot be done unless network a is directly peered with network c an extremely important point to note for vpc peering another thing to note is that you cannot use a tag or service account from one peered network in the other peered network they must each have their own as again they are each independently operated as stated earlier and so the last thing that i wanted to cover is that internal dns is not accessible for compute engine in peered networks as they must use an ip to communicate and so that about covers this short yet important lesson on the theory and concepts of vpc peering and so now that we've covered all the theory i'm going to be taking these concepts into a demo where we will be pairing two networks together and verifying the communication between them and so you can now mark this lesson as complete and whenever you're ready join me in the console welcome back in this handson demonstration we're going to go through the steps to create a peering connection from two vpcs in two separate projects as shown here in the diagram and then to verify that the connection works we're going to create two instances one in each network and ping one instance from the other instance this demo is very similar to the custom vpc demo that you had done earlier but we are adding in another layer of complexity by adding in vpc network peering and so there's quite a bit to do here so let's go ahead and just dive in okay so here we are back in the console as you can see up in the top right hand corner i am logged in as tony bowties gmail.com and for this specific demo i will be using two projects both project tony and project bowtie inc and if you currently do not have two projects you can go ahead and create yourself a new project or the two projects if you have none and so i'm going to continue here with project tony and the first thing i want to do is create the two networks in the two separate projects so i'm going to go up to the navigation menu in the top left hand corner and i'm going to scroll down to vpc network here i'm going to create my first vpc network and i'm going to name this bowtie ink dash a i'm going to give it the same description and then under subnets i'm going to leave the subnet creation mode under custom under the subnet name you can call this subnet dash a i'm going to use the us east one region and for the ip address range i'm going to use 10.0 that's 0.0 forward slash 20. and i'm going to leave all the other options as default and i'm going to go down to the bottom and click on create now as this network is being created i'm going to go over to the project bowtie inc and i'm going to create the vpc network there so under name i'm going to call this bowtie inc b and under description i'm going to use the same under subnets i'm going to keep subnet creation mode as custom and under new subnet i'm going to call this subnet subnet b the region will be used 4 and the ip address range will be 10.4.0.0 forward slash 20. you can leave all the other options as default and scroll down to the bottom and click on create as this network is being created i'm going to go back to project tony and i'm going to create the firewall rule for bow tie ink dash a in this firewall rule as explained in the last lesson we'll allow communication from one instance to the other and so i'm going to click on create firewall and under name i'm going to call this project tony dash a under description i'm going to use the same under the network i'm going to choose the source network which will be bowtie inc dash a priority i'm going to keep at 1000 direction of traffic should be ingress and action on match should be allow under targets i'm going to select all instances in the network and under source filter i'm going to keep ip ranges selected and the source ip range specifically for this demo is going to be 0.0.0.0 forward slash 0. and again this is specifically used for this demo and should never be used in a productionlike environment in production you should only use the source ip ranges that you are communicating with and under protocols and ports because i need to log into the instance to be able to ping the other instance i'm going to have to open up tcp on port 22. under other protocols you can add icmp and this will allow the ping command to be used i'm going to leave all the other options as default and i'm going to click on create and now that this firewall rule has been created i need to go back over to project bowtie inc and create the firewall rule there as well i'm going to call this firewall rule bowtie inc dash b i'm going to give it the same description under network i'm going to select bow tie ink dash b i'm going to keep the priority as 1000 and the direction of traffic should be ingress as well the action on match should be allow scrolling down under targets i'm going to select all instances in the network and again under source filter i'm going to keep ip ranges selected and under source ip ranges i'm going to enter in 0.0.0.0 forward slash 0. and under protocols and ports i'm going to select tcp with port 22 as well under other protocols i'm going to type in icmp i'm going to leave everything else as default and i'm going to click on create now once you've created both networks and have created both firewall rules you can now start creating the instances so because i'm already in project bowtie inc i'm going to go to the lefthand navigation menu and i'm going to scroll down to compute engine and create my instance so i'm just going to click on create and to keep with the naming convention i'm going to call this instance instance b i'm not going to add any labels for now under region i'm going to choose us east 4 and you can leave the zone as the default selection and under machine type i'm going to select e2 micro and i'm going to scroll down to the bottom and i'm going to click on management security disks networking and sold tenancy so that i'm able to go into the networking tab to change the network on the default network interface so i'm going to click on the default network interface and under network i'm going to select bowtie inc b and the subnet has already been selected for me and then i'm going to scroll down click on done and i'm going to leave all the other options as default and click on create and so as this is creating i'm going to go over to project tony and i'm going to create my instance there and i'm going to name this instance instance a under region i am going to select us east1 you can leave the zone as the default selected under machine type i'm going to select e2 micro and scrolling down here to the bottom i'm going to go into the networking tab under management security disks networking and soul and here i'm going to edit the network interface and change it from the default network to bow tie ink dash a and as you can see the subnet has been automatically selected for me so now i can just simply click on done i'm going to leave all the other options as default and i'm going to click on create so just as a recap we've created two separate networks in two separate projects along with its corresponding subnets and the firewall rules along with creating an instance in each network and so now that we have both environments set up it's now time to create the vbc peering connection and so because i'm in project tony i'm going to start off with this project and i'm going to go up to the navigation menu and scroll down to vpc network and under vpc network on the left hand menu you're going to click on vpc network peering and through the interface shown here we'll be able to create our vpc network peering so now you're going to click on create connection and i'm prompted with some information that i will need and because we are connecting to another vpc in another project you're going to need the project id as well as the name of the vpc network you want to peer with and just as explained in the earlier lesson the subnet ip ranges in both networks cannot overlap so please make sure that if you are using ip ranges outside of the ones that are given for this demonstration the ip ranges that you are using do not overlap so once you have that information you can then click continue and so here you will be prompted with some fields to fill out with the information that you were asked to collect in the previous screen and so since we have that information already we can go ahead and start filling in the fields so i'm going to call this peering connection peering a b and under vpc network i'm going to select bow tie ink dash a under peered vpc network we're going to select the other project which should be bowtie inc and the vpc network name will be bow tie inc dash b and i'm going to leave all the other options as default and so under vpc network name you will see exchange custom routes and here i can select to import and export custom routes that i have previously created so any special routes that i have created before the actual peering connection i can bring them over to the other network to satisfy my requirements and so i'm not going to do that right now i'm going to close this up and i'm going to simply click on create and so this is finished creating and is marked as inactive and this is because the corresponding peering connection in project bowtie has yet to be configured the status will change to a green check mark in both networks and marked as active once they are connected if this status remains as inactive then you should recheck your configuration and edit it accordingly so now i'm going to head on over to project bowtie inc and i'm going to create the corresponding peering connection i'm going to click on create connection once you have your project id and the vpc network you can click on continue and for the name of this peering connection i'm going to call this peering dash ba respectively under vpc network i'm going to select bowtie inc b and under peered vpc network i'm going to select in another project here you want to type in your project id for me i'm going to paste in my project tony project id and under vpc network name i'm going to type in bowtie inc a and i'm going to leave all the other options as default and i'm going to click on create and so now that we've established connections on each of the peering connections in each vpc if the information that we've entered is correct then we should receive a green check mark stating that the peering connection is connected and success here we have status as active and if i head on over to project tony i should have the same green check mark under status for the peering connection and as expected the status has a green check mark and is marked as active so now in order to do the pairing connectivity test i'm going to need to grab the internal ip of the instance in the other network that resides in project bowtie and so because it doesn't matter which instance i log into as both of them have ssh and ping access i'm going to simply go over to the navigation menu i'm going to head on over to compute engine and i'm going to record the internal ip of instance a and now i'm going to head over to project bowtie and log into instance b and ping instance a and so in order to ssh into this instance i'm going to click on the ssh button under connect and it should open a new browser tab for me logging me into the instance okay i'm logged in here and i'm going to zoom in for better viewing and so now i'm going to run a ping command against instance a using the internal ip that i had copied earlier and i'm going to hit enter and as you can see ping is working and so now we can confirm that the vpc peering connection is established and the two instances in the different vpc networks are communicating over their private ips and you can go ahead and hit control c to stop the ping and so just as a recap you've created two separate vpc networks with their own separate subnets in two separate projects you've created the necessary firewall rules in each of these networks along with creating instances in each of those networks you then established a vpc peering connection establishing the configuration in each vpc you then did a connectivity test by logging into one of the instances and pinging the other instance and so i hope this helps cement the theory of vpc peering that you learned in the previous lesson and has given you some context when it comes to configuring each end of the peering connection so i wanted to take a moment to congratulate you on completing this demo and so all that's left now is to clean up all the resources that we created throughout this demo and you can start by selecting the instances and deleting them in each network as well as the firewall rules and the networks themselves i'm going to go over to project tony and i'm going to do the same thing there and so you can do exactly what you did with the last instance here you can select it click on delete and delete the instance and so next we're going to delete the peering connection so we're going to go up to the navigation menu we're going to scroll down to vpc network and on the left hand menu we're going to scroll down to vpc network peering and so we're going to select appearing connection we're going to go to the top and click on delete and then delete the peering connection and so now we're going to delete the firewall rule so we're going to go up to firewall we're going to select the firewall rule at the top we're going to click delete and then delete the firewall rule and last but not least we want to delete the vpc network that we created so we're going to go up to vpc networks we're going to drill down into the custom vpc up at the top we're going to click on delete vpc network and then we're going to click on delete and so now that we've deleted all the resources in project tony we're going to go back over to our second project project bowtie and do the same thing and so we're first going to start off with the vpc peering connection so we're going to go over to vpc network peering we're going to select the appearing connection we're gonna click on delete at the top and delete the peering connection next we're gonna go into firewall we're gonna select the firewall rule go up to the top and click on delete and then delete the firewall rule and finally we're gonna go over to vpc networks we're going to drill down into the custom network we're going to click on delete vpc network at the top and delete the vpc network and so now that you've successfully deleted all your resources you can now mark this lesson as complete and i'll see you in the next one and congrats again on the great job of completing this demo welcome back and in this lesson i'm going to be discussing the concepts and terminology of shared vpcs i'm also going to go into some detailed use cases and how shared vpcs would be used in different scenarios so with that being said let's dive in now when a vpc is created it is usually tied to a specific project now what happens when you want to share resources across different projects but still have separate billing and access within the projects themselves this is where shared vpcs come into play shared vpcs allow an organization to connect resources from multiple projects to a common vpc network so that way they can communicate with each other securely and efficiently using internal ips from that network when you use shared vpcs you designate a project as a host project and attach one or more other service projects to it the vpc networks in the host project are considered the shared vpc networks so just as a reminder a project that participates in a shared vpc is either a host project or a service project a host project can contain one or more shared vpc networks a service project is any project that has been attached to a host project by a shared vpc admin this attachment allows it to participate in the shared vpc and just as a note a project cannot be both a host and a service project simultaneously it has to be one or the other and you can create and use multiple host projects however each service project can only be attached to a single host project it is also a common practice to have multiple service projects administered by different departments or teams in the organization and so just for clarity for those who are wondering a project that does not participate in a shared vpc is called a standalone project and this is to emphasize that it is neither a host project or a service project now when it comes to administering these shared vpcs we should be adhering to the principle of least privilege and only assigning the necessary access needed to specific users so here i broken down the roles that are needed to enable and administer the shared vpcs a shared vpc admin has the permissions to enable host projects attach service projects to host projects and delegate access to some or all of the subnets in shared vpc networks to service project admins when it comes to a service project admin this is a shared vpc admin for a given host project and is typically its project owner as well although when defining each service project admin a shared vpc admin can grant permission to use the whole host project or just some subnets and so when it comes to service project admins there are two separate levels of permissions that can be applied the first is project level permissions and this is a service project admin that can be defined to have permission to use all subnets in the host project when it comes to subnet level permissions a service project admin can be granted a more restrictive set of permissions to use only some subnets now i wanted to move into some use cases which will give you a bit more context on how shared vpcs are used in specific environments illustrated here is a simple shared vpc scenario here a host project has been created and attached to service projects to it the service project admin in service project a can be configured to access all or some of the subnets in the shared vpc network service project admin with at least subnet level permissions to the 10.0.2.0 24 subnet has created vm1 in a zone located in the us west one region this instance receives its internal ip address 10.0.2.15 from the 10.0.2.0 24 cider block now service project admins in service project b can be configured to access all or some of the subnets in the shared vpc network a service project admin with at least subnet level permissions to the 10.10.4.0 forward slash 24 subnet has created vm2 in a zone located in the us central 1 region this instance receives its internal ip address 10.10.4.1 from the 10.10.4.0 forward slash 24 cider block and of course the standalone project does not participate in the shared vpc at all as it is neither a host nor a service project and the last thing to note instances in service projects attached to a host project using the same shared vpc network can communicate with one another using either ephemeral or reserve static internal ip addresses and i will be covering both ephemeral and static ip addresses in a later section under compute engine external ip addresses defined in the host project are only usable by resources in that project they are not available for use in service projects moving on to the next use case is a multiple hosts project for this use case an organization is using two separate host projects development and production and each host project has two service projects attached to them both host projects have one shared vpc network with subnets configured to use the same cider ranges both the testing and production networks have been purposely configured in the same way so this way when you work with resources tied to a subnet range it will automatically translate over from one environment to the other moving on to the next use case is the hybrid environment now in this use case the organization has a single host project with a single shared vpc network the shared vpc network is connected via cloud vpn to an onpremises network some services and applications are hosted in gcp while others are kept on premises and this way separate teams can manage each of their own service projects and each project has no permissions to the other service projects as well each service project can also be billed separately subnet level or project level permissions have been granted to the necessary service project admins so they can create instances that use the shared vpc network and again instances in these service projects can be configured to communicate with internal services such as database or directory servers located on premises and finally the last use case is a twotier web service here an organization has a web service that is separated into two tiers and different teams manage each tier the tier one service project represents the externally facing component behind an http or https load balancer the tier 2 service project represents an internal service upon which tier 1 relies on and it is balanced using an internal tcp or udp load balancer the shared vpc allows mapping of each tier of the web service to different projects so that they can be managed by different teams while sharing a common vpc network to host resources that are needed for both tiers now we cover quite a bit in this lesson when it comes to all the concepts of shared vpcs we covered both host and service projects and the roles that they play and their limitations we also went over the different roles that are needed to administrate these shared vpcs and we went over different use cases on how to use shared vpcs for different scenarios and so that about covers everything i wanted to discuss in this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i'm going to be discussing vpc flow logs flow logs is an essential tool for monitoring and analyzing traffic coming in and going out of vpcs from vm instances flow logs are essential to know for the exam as you should know the capabilities and use cases and so with that being said let's dive in so vpc flow logs records a sample of network flows sent from and received by vm instances including instances used as google kubernetes engine nodes these logs can be used for network monitoring forensics realtime security analysis and expense optimization when you enable vpc flow logs you enable for all vms in a subnet so basically you would be enabling vpc flow logs on a subnet by subnet basis flow logs are aggregated by connection from compute engine vms and exported in real time these logs can be exported to cloud logging previously known as stackdriver for 30 days if logs need to be stored for longer than 30 days they can be exported to a cloud storage bucket for longer term storage and then read and queried by cloud logging google cloud samples packets that leave and enter a vm to generate flow logs now not every packet is captured into its own log record about one out of every 10 packets is captured but this sampling rate might be lower depending on the vm's load and just as a note you cannot adjust this rate this rate is locked by google cloud and cannot be changed in any way and because vpc flow logs do not capture every packet it compensates for missed packets by interpolating from the captured packets now there are many different use cases for vpc flow logs and i wanted to take a quick minute to go over them the first one i wanted to mention is network monitoring vpc flow logs provide you with realtime visibility into network throughput and performance so you can monitor the vpc network perform network diagnostics understand traffic changes and help forecast capacity for capacity planning you can also analyze network usage with vpc flow logs and you can analyze the network flows for traffic between regions and zones traffic to specific countries on the internet and based on the analysis you can optimize your network traffic expenses now a great use case for vpc flow logs is network forensics so for example if an incident occurs you can examine which ips talked with whom and when and you can also look at any compromised ips by analyzing all the incoming and outgoing network flows and lastly vpc flow logs can be used for realtime security analysis you can leverage the realtime streaming apis using pub sub and integrate them with a sim or security information in event management system like splunk rapid7 or logarithm and this is a very common way to add an extra layer of security to your currently existing environment as well as a great way to meet any compliance standards that are needed for your organization now vpc flow logs are recorded in a specific format log records contain base fields which are the core fields of every log record and meta data fields that add additional information metadata fields may be omitted to save storage costs but base fields are always included and cannot be omitted some log fields are in a multifield format with more than one piece of data in a given field for example the connection field that you see from the base is of the ip details format which contains the source and destination ip address and port plus the protocol in a single field flows that have an endpoint in a gke cluster can be annotated with gke annotations which can include details of the cluster pod and service of the endpoint gke annotations are only available with a custom configuration of metadata fields now when you enable vpc flow logs you can set a filter based on both base and metadata fields that only preserves logs that match the filter all other logs are discarded before being written to logging which saves you money and reduces the time needed to find the information you're looking for shown here is a sample from the console in both the classic logs viewer as well as the logs viewer in preview and so in the classic logs viewer you can simply select the sub network from the first pull down menu and from the second pull down menu you can select the compute.googleapis.com forward slash vpc underscore flows and this will give you the information that you need to pull up all your vpc flow logs in the logs viewer preview it is done in a similar way but the query is shown here in the query builder and can be adjusted accordingly pulling up any vpc flow logs must be done within the console when viewing them in google cloud and so the last thing i wanted to show you before ending this lesson is a sample of the log itself the log shown here is a sample of what a vpc flow log looks like and as you can see here beside each field you will see a small arrow clicking on these arrows will expand the field and reveal many of the subfields that you saw on the last slide and will give you the necessary information you need to analyze your vpc flow logs in this example of the connection field it shows the five tuple that describes this connection which you can clearly see up here at the top and if i were to go further down and expand more of these fields i would find more information that could help me better analyze more logging info for my given problem that i am trying to solve now i didn't want to go too deep into logging as i will be diving into a complete section on its own in a later section of the course but i did want you to get a feel for what type of data vpc flow logs can give you and how it can help you in your specific use case as well as on the exam and so that's pretty much all i wanted to cover with regards to vpc flow logs so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to cover a highlevel overview of a basic foundational service that supports the backbone of the internet as we know it today this foundation is called dns or the domain name system dns is used widely in google cloud from mostly an infrastructure perspective and is used in pretty much any other cloud environment or computer network on the planet now there is quite a bit to cover in this lesson with regards to dns so with that being said let's dive in now dns or domain name system is a global decentralized distributed database that lets you store ip addresses and other data and look them up by name this system uses human readable names like google.com and translates it into a language that computers understand which are numeric ip addresses for example humans access information online through a domain name like google.com computers use ip addresses to access information online like 172.217. now whether you type google.com or the ip address into a web browser both will connect to google.com dns translates the domain name to an ip address so that the web browser knows where to connect to and we know what to enter into the web browser through dns you can connect a domain name to web hosting mail and other services now getting a bit deeper into it as ip addresses are at the core of communicating between devices on the internet they are hard to memorize and can change often even for the same service to get around these problems we gave names to ip addresses for example when it comes to our computer communicating with www.google.com it will use the dns system to do this now in the dns database contains the information needed to convert the www.google.com domain name to the ip address and this piece of information is stored in a logical container called a zone the way that the zone is stored is through what's commonly known as a zone file now within this zone file is a dns record which links the name www and the ip address that your laptop needs to communicate with the specific website and this zone file is hosted by what's known as a name server or ns server for short and i will be going into further detail on this in just a minute so in short if you can query the zone for the record www.google.com then your computer can communicate with the web server and dns is what makes it all happen now i wanted to go into a bit of history of how dns came about so in early computer networks a simple text file called a host file was created that mapped hostnames to ip addresses and this enabled people to refer to other computers by the name and their computers translated that name to an ip address when it needed to communicate with it the problem is as network sizes increased the host file approach became impractical due to the fact that it needed to be stored on each computer as each computer would have to resolve the same host names as well updates were difficult to manage as all of the computers would need to be given an updated file all in all this system was not scalable now to overcome these and other limitations the dns system was developed and the dns system essentially provided for a way to organize the names using a domain name structure it also provided a dynamic system for protocols services and methods for storing updating and retrieving ip addresses for host computers now that i've covered what dns is and why we use it i wanted to dive into the structure of the dns system now the structure all begins with a dot the root if you will and this can be found after every domain name that you type into your browser you will almost never see it and this is because your browser will automatically put it in without your knowing you can try it with any domain in any browser and you will almost always come up with the same result this dot is put in for you and will provide the route for you and this is where we start to break down the dns system now the domain name space consists of a hierarchical data structure like the one you have on your computer each node has a label and zero or more resource records which hold information associated with the domain name the domain name itself consists of the label concatenated with the name of its parent node on the right separated by a dot so when it comes to dns the domain name is always assembled from right to left this hierarchy or tree is subdivided into zones beginning at the root zone a dns zone may consist of only one domain or may consist of many domains and sub domains depending on the administrative choices of the zone manager now getting right into it the root server is the first step in translating human readable hostnames into ip addresses the root domain is comprised of 13 dns systems dispersed around the world known collectively as the dns root servers they are indicated by the letters a through m operated by 12 organizations such as verisign cogent and nasa while there are 13 ip addresses that represent these systems there are actually more than 13 servers some of the ip addresses are actually a cluster of dns servers and so each of these dns servers also consists of the root zone file which contains the address of the authoritative name server for each top level domain and because this is such a big undertaking to keep updated iana or the internet assigned numbers authority was appointed as the authority that manages and administrates this file and i will include a link in the lesson text for those of you who are looking to dive deeper into the contents of this root zone file as well as getting to know a little bit more about the iana organization now while the dns root servers establish the hierarchy most of the name resolution process is delegated to other dns servers so just below the dns route in the hierarchy are the top level domain servers also known as tld for short the top level domain takes the tld provided in the user's query for example www.google and provides details for the dotcom tld name server the companies that administer these domains are named registries and they operate the authoritative name servers for these top level domains for example verisign is the registry for the dot com top level domain over a hundred million domains have been registered in the dot com top level domain and these top level dns servers handle top level domains such as com dot org dot net and dot io and this can also be referred to as the gtld which is the general top level domains and the cctld which is the country code top level domain like dot ca for canada dot uk for the united kingdom and dot it for italy the top level dns servers delegate to thousands of second level dns servers now second level domain names are sold to companies and other organizations and over 900 accredited registrars register and manage the second level domains in the dot com domain for end users the second level of this structure is comprised of millions of domain names second level dns servers can further delegate the zone but most commonly store the individual host records for a domain name this is the server at the bottom of the dns lookup chain where you would typically find resource records and it is these resource records that maps services and host names to ip addresses and will respond with the queried resource record ultimately allowing the web browser making the request to reach the ip address needed to access a website or other web resources now there is one more concept that i wanted to cover before we move on and this is the sub domain now some of you have noticed and wondered where does the sub domain come into play with regards to the dns structure well this is a resource record that falls under the second level domain and in dns hierarchy a sub domain is a domain that is a part of another main domain but i wanted to put it in here just to give you an understanding of where subdomains would fall so now that we understand how dns is structured i wanted to go through the breakdown of the data flow of dns to give you some better contacts now there are eight steps in a dns lookup first we start off with the dns client which is shown here as tony bowtie's laptop and this is a client device which could also be a phone or a tablet and is configured with software to send name resolution queries to a dns server so when a client needs to resolve a remote host name into its ip address in most cases it sends a request to the dns recursive resolver which returns the ip address of the remote host to the client a recursive resolver is a dns server that is configured to query other dns servers until it finds the answer to the question it will either return the answer or an error message to the client if it cannot answer the query and the query will eventually be passed off to the dns client the recursive resolver in essence acts as the middle man between a client and a dns name server which is usually the internet service provider a service carrier or a corporate network now to make sure that a resolver is able to properly run dns a root hints file is supplied with almost every operating system and this file holds the ip addresses for the root name servers this also includes the dns resolver but in case it is unable to answer the query the client will be able to still make the query to the dns name servers now after receiving a dns query from a client this recursive resolver will either respond with cache data or send a request to a root name server and in this case the resolver queries a dns root name server the root server then responds to the resolver with the address of a top level domain or tld dns server such as com or dot net which stores the information for its domains now when searching for google.com the request is pointed towards the dotcom tld so naturally the resolver then makes a request to the com tld then the tld name server then responds with the ip address of the domain's name server google.com and lastly the resolver then sends a query to the domain's name server the ip address for google.com is then returned to the resolver from the name server this ip address is cache for a period of time determined by the google.com name server and this process is so that a future request for this hostname could be resolved from its cache rather than performing the entire process from beginning to end and so for those of you who are unaware cache is a component that stores data so that future requests for that data can be served faster the purpose of this caching is to temporarily store data in a location that results in improvements in performance and reliability for data requests dns caching involves storing the data closer to the requesting client so that the dns query can be resolved earlier and additional queries further down the dns lookup chain can be avoided and thus improving load times dns data can be cached in a variety of locations down the chain each of which will store dns records for a set amount of time determined by a time to live also known as ttl for short and this value is the time to live for that domain record a high ttl for a domain record means that local dns resolvers will cache responses for longer and give quicker responses however making changes to dns records can take longer due to the need to wait for all cash records to expire alternatively domain records with low ttls can change much more quickly but dns resolvers will need to refresh their records more often and so in this final step the dns resolver then responds to the web browser with the ip address of the domain requested initially and once these eight steps of the dns lookup have returned the ip address for www.google.com the browser is able to make the request for the webpage and so the browser will reach out to the ip address of the server and request the web page which will be loaded up in the browser now i know this probably has been a review for those who are a bit more advanced when it comes to understanding dns but for others who are fairly new to the underpinnings of dns i hope this has given you a basic understanding of what it is why we use it and how it works moving forward in the course i will be discussing dns with regards to different services and the needed resource records within zones that are used by these given services and so that's pretty much all i wanted to cover when it comes to the fundamentals of dns so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to be diving into dns record types now dns resource records are the basic information elements of the domain name system they are entries in the dns database which provide information about hosts these records are physically stored in the zone files on the dns server this lesson will go through some of the most commonly used dns records that we will be coming across throughout this course so with that being said let's dive in now the first record that i wanted to touch on are the name server records also known as ns records for short this record identifies which dns server contains the current records for a domain these servers are usually found at a registrar internet service provider or hosting company ns records are created to identify the name server used for each domain name within a given zone in this example we have the dot co zone that will have multiple name server records for bowtieinc.co now these name server records are how the dot co delegation happens for bowtieinc.co and they point at servers that host the inc.co zone that is managed by bowtie inc and the flow shown here of the query starts from the root zone going to the dot co zone where the record lies for the name servers for bowtieinc.com and flows down to the bowtieinc.cozone that contain all the necessary records for bowtieinc.co the next record that i wanted to touch on are the a and aaa records and this is short for address records for ipv4 and ipv6 ip addresses respectively and this record points a domain name to an ip address for example when you type wwe in a web browser the dns system will translate that domain name to the ip address of 52.54.92.195 using the a record information stored in the bowtieinc.co dns zone file the a record links a website's domain name to an ipv4 address that points to the server where the website's files live now when it comes to an aaa record this links a website's domain to an ipv6 address that points to the same server where the website's files live a records are the simplest type of dns records and one of the primary records used in dns servers you can do a lot with a records including using multiple a records for the same domain in order to provide redundancy the same can be said for aaa records additionally multiple domains could point to the same address in which case each would have its own a or aaa record pointing to that same ip address moving on to cname records a c name record short for canonical name record is a type of resource record that maps one domain name to another this can be really convenient when running multiple services like an ftp server and an ecommerce server each running on different ports from a single ip address you can for example point ftp ftp.bowtieinc.co and shop.bowtieinc.co to the dns entry for bowtieinc.co which in turn has an a record which points to the ip address so if the ip address ever changes you only have to change the record in one place in the dns a record for bow tie inc dot co and just as a note cname records must always point to another domain name and never directly to an ip address next up are txt records a text record or txt for short is a type of resource record that provides text information to sources outside your domain that can be used for a number of arbitrary purposes the records value can be either human or machine readable text in many cases text records are used to verify domain ownership or even to provide human readable information about a server a network or a data center it is also often used in a more structured fashion to record small amounts of machine readable data into the dns system a domain may have multiple tax records associated with it provided the dns server implementation supports this each record can in turn have one or more character strings in this example google wants to verify the bowtieinc.co domain so that g suite can be set up and needs verification through the domain to google through creating a text record and adding it to the zone google will then supply a text verification record to add to the domain host's dns records and start to scan for the text record to verify the domain the supplied text record is then added by the domain administrator and behind the scenes google is doing a verification check at timed intervals when google finally sees the record exists the domain ownership is confirmed and g suite can be enabled for the domain and this is a typical example of how tax records are used now moving on to mx records a dns mx record also known as the mail exchange record is the resource record that directs email to a mail server the mx record indicates how email messages should be routed and to which server mail should go to like cname records an mx record must always point to another domain now mx records consist of two parts the priority and the domain name the priority are the numbers before the domains for these mx records and indicate the preference of the order in which the mail server should be used the lower the preference number the higher the priority so in this example laura is emailing tony bowtie at tony at bowtieinc.co the mx records are part of this process as dns needs to know where to send the mail to and we'll look at the domain attached to the email address which is bowtieinc.co so the dns client will run a regular dns query by first going to the root then to the cotld and finally to bowtieinc.co it will then receive the mx record which in this example is two of them the first one being mail representing mail.bowtieinc.co and then the second one is a different mail server outside the current domain and in this case is a google mail server of aspmx.l.google.com and this is a fully qualified domain name as the dot on the right of this record suggests so here the server will always try mail.bowtieinc.co first because 5 is lower than 10. and this will give mail.bowtieinc.co the higher priority in the result of a message send failure the server will default to aspmx.l.google.com if both values are the same then it would be low balanced across both servers whichever is used the server gets the result of the query back and it uses this to connect to the mail server for bowtieinc.co via the smtp protocol and it uses this protocol to deliver all email and this is how mx records are used for email the next record i wanted to cover are the pointer records also known as ptr records for short and this provides the domain name associated with an ip address so a dns pointer record is exactly the opposite of the a record which provides the ip address associated with the domain name dns pointer records are used in reverse dns lookups as we discussed earlier when a user attempts to reach a domain name in their browser a dns lookup occurs matching the domain name to the ip address a reverse dns lookup is the opposite of this process and it is a query that starts with the ip address and looks up the domain name while dnsa records are stored under the given domain name dns pointer records are stored under the ip address reverse and ending in dot i n a d d r dot arpa so in this example the pointer record for the iap address 52.54.90 would be stored under 195.92.54.52 dot in addr dot arpa ipv6 addresses are constructed differently from ipv4 addresses and ipv6 pointer records exist in a different namespace within.arpa ipv6 pointer records are stored under the ipv6 address reversed and converted into 4bit sections as opposed to 8bit sections as in ipv4 and as well the domain.ip6.arpa is added at the end pointer records are used most commonly in reverse dns lookups for antispam troubleshooting email delivery issues and logging and so the last record that i wanted to cover are the soa records also known as the start of authority records and this resource record is created for you when you create your managed zone and specifies the authoritative information including global parameters about a dns zone the soa record stores important information about a domain or zone such as the email address of the administrator when the domain was last updated and how long the server should wait between refreshes every dns zone registered must have an soa record as per the rfc 1035 and there is exactly one soa record per zone the soa record contains the core information about your zone so it is not possible for your zone to work without that information and i will include a link in the lesson text for those who are interested in diving deeper and understanding all the information that is covered under these soa records a properly optimized and updated soa record can reduce bandwidth between name servers increase the speed of website access and ensure the site is alive even when the primary dns server is down and so that about covers everything that i wanted to discuss when it comes to resource records within dns so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to be covering network address translation also known as nat for short this is a common process used in home business and any cloud networks that you will encounter knowing and understanding that will help you achieve why you would use it and what makes it such a necessary process now there's quite a bit to cover here so with that being said let's dive in now at a high level nat is a way to map multiple local private ip addresses to a public ip address before transferring the information this is done by altering the network address data in the ip header of the data packet while traveling through a network towards the destination as packets pass through a nat device either the source or destination ip address is changed then packets returning in the other direction are translated back to the original addresses and this is a process that is typically used in most home routers that are provided by your internet service provider now originally nat was designed to deal with the scarcity of free ipv4 addresses increasing the number of computers that can operate off a single publicly routable ip address and so because devices in the private ip space such as 192.168.0.0 cannot traverse the public internet that is needed for those devices to communicate with the public internet now ipv6 was designed to overcome the ipv4 shortage and has tons of available addresses and therefore there is no real need for nat when it comes to ipv6 now nat has an additional benefit of adding a layer of security and privacy by hiding the ip address of your devices from the outside world and only allowing packets to be sent and received from the originating private device and so this is a high level of what nat is now there are multiple types of not that i will be covering which at a high level do the same thing which is translate private i p addresses to public ip addresses yet different types of nat handles the process differently so first we have static nat which maps a single private ip address to a public ip address so a onetoone mapping that gives the device with the private ip address access to the public internet in both directions this is commonly used where one specific device with a private address needs access to the public internet the next type of nat is dynamic nan and this is similar to static nat but doesn't hold the same static allocation a private ip address space is mapped to a pool of public ip addresses and are allocated randomly as needed when the ip address is no longer needed the ip address is returned back to the pool ready to be used by another device this method is commonly used where multiple internal hosts with private i p addresses are sharing an equal or fewer amount of public i p addresses and is designed to be an efficient use of public ips and finally there is port address translation or pat where multiple private ip addresses are translated using a single public ip address and a specific port and this is probably what your home router is using and will cover all the devices you use in your home network this method uses ports to help distinguish individual devices and is also the method that is used for cloudnat in google cloud which i will be covering in a later lesson and so i wanted to get into a bit more detail on how these methods work starting with static not now to set the stage for static not i'm going to start off with a private network here on the left and the public ip space here on the right and the router or not device in the middle in this example there is a server on the left that needs access to external services and for this example the external service we are using is the bowtress service an image sharing site for all sorts of awesome bow ties so the server on the left is private with a private ip address of 192.168.0.5 and this means it has an address in the ip version 4 private address space meaning that it cannot route packets over the public internet because it only has a private ip the beautress service on the other hand has a public ip address which is 54.5.4.9 so the issue we run into is that the private address can't be routed over the public internet because it's private and the public address of the beau trust service can't directly communicate with any private address because public and private addresses can communicate over the public internet what we need is to translate the private address that the server on the left has to a public ip that can communicate with the service on the right and vice versa now then that device will map the private ip to public ip using and maintaining a nat table and in this case of static nat the nat device will have a onetoone mapping of the private ip address to a public ip address and can be allocated to the device specified which in this case is the server marked as 192.168.0.15 and so in order for the server on the left to communicate with the beautress service the server will generate a packet as normal with the source ip of the packet being the server's private ip address and the destination ip of the packet being the ip of the bowtrust service now the router in the middle is the default gateway for any destination so any ip packets which are destined for anything but the local network are sent to the router so as you can see here with the entry in the table it will contain the private i p address of 192.168.0.15 and mapped to the public address which in this case is 73.6.2.33 and these are statically mapped to one another and so as the packet passes through the nat device the source address of the packet is translated from the private address to the mapped public address and this results in a new packet so this new packet still has beautrest as the destination but now it has a valid public ip address as the source and so this is the translation that happens through nat now this process works in a similar way in the other direction so when the beautress service receives the packet it sees the source as this public ip so when it responds with data its packet has its ip address as the source and the previous server's public ip address as the destination so it sends this packet back to this public ip so when the packet arrives at the nat device the table is checked it recognizes then that the ip is for the server and so this time for incoming traffic the destination ip address is updated to the corresponding private ip address and then the packet is forwarded through to the private server and this is how static nat works the source i p address is translated from the mapped private ip to public ip and for incoming traffic the destination i p address is translated from the allocated public ip to the corresponding private ip all without having to configure a public ip on any private device as they always hold their private ip addresses now i wanted to supply an analogy for nat and so a very common analogy that is used is that of a phone service so in this example laura is the new manager of bow tie inc new location in montreal and has put in a new public phone number of 5145558437 although as you can see here laura also has a private extension of one three three seven now if george called laura at that public phone number he would reach laura without ever knowing her private extension so the private extension acts as that private ip address and the public phone number would act as the public ip address and this would be the telephone analogy for static nat and so this is the end of part one of this lesson it was getting a bit long so i decided to break it up this would be a great opportunity for you to get up and have a stretch get yourself a coffee or a tea and whenever you're ready you can join me in part two where we will be starting immediately from the end of part one so you can go ahead and complete this video and i will see you in part two welcome back this is part two of the network address translation lesson and we will be starting exactly where we left off from part 1. so with that being said let's dive in now moving on to dynamic nat this method is similar to static nat except that devices are not allocated a permanent public ip a public ip address is allocated from a pool of ip addresses as they are needed and the mapping of public to private is allocation base in this example there are two devices on the left and according to the nat table there are two public ip addresses available for use 73.6.2.33 and 73.6.2.34 so when the laptop on the left is looking to access the beautress service it will generate a packet where the source ip is the private address of 192.168.0.13 and the destination ip is 54.5.4.9 so it sends this packet and again the router in the middle is the default gateway for anything that isn't local as the packet passes through the router or the nat device it checks if the private ip has a current allocation of public addressing from the pool and if it doesn't and one is available it allocates one dynamically and in this case 73.6.2.34 is allocated so the packet's source i p address is translated to this address and the packets are sent to the beautress service and so this process is the same as static not thus far but because dynamic nat allocates these ip addresses dynamically multiple private devices can share a single public ip as long as the devices are not using the same public ip at the same time and so once the device is finished communication the ip is returned back to the pool and is ready for use by another device now just as a note if there's no public ip addresses available the router rejects any new connections until you clear the nat mappings but if you have as many public ip addresses as hosts in your network you won't encounter this problem and so in this case since the lower server is looking to access the fashion tube service there is an available public ip address in the pool of 73.6.2.33 thus giving it access to the public internet and access to fashion tube so in summary the nat device maps a private ip with the public ip in a nat table and public ips are allocated randomly and dynamically from a pool now this type of knot is used where multiple internal hosts with private ip addresses are sharing an equal or fewer amount of public ip addresses when all of those private devices at some time will need public access now an example of dynamic nat using the telephone analogy would be if laura and two other bow tie inc employees lisa and jane had private phone numbers and this would represent your private ips in this example bowtie inc has three public phone numbers now when any employee makes an outbound call they are routed to whichever public line is open at the time so the caller id on the receiver's end would show any one of the three public phone numbers depending on which one was given to the caller and this would represent the public ips in the public ip pool now the last type of nat which i wanted to talk about is the one which you're probably most familiar with and this is port address translation which is also known as not overload and this is the type of not you likely use on your home network port address translation is what allows a large number of private devices to share one public ip address giving it a many to one mapping architecture now in this example we'll be using three private devices on the left all wanting to access fashiontube on the right a popular video sharing website of the latest men's fashions shared by millions across the globe and this site has a public ip of 62.88.44.88 and accessed using tcp port 443 now the way that port address translation or pat works is to use both the ip addresses and ports to allow for multiple devices to share the same public ip every tcp connection in addition to a source and destination ip address has a source and destination port the source port is randomly assigned by the client so as long as the source port is always unique then many private clients can use the same public ip address and all this information is recorded in the nat table on the nat device in this example let's assume that the public ip address of this nat device is 73.6.2.33 so when the laptop in the top left generates a packet and the packet is going to fashion tube its destination ip address is 62.80 and its destination port is 443. now the source ip of this packet is the laptop's private ip address of 192.168.6 and the source port is 35535 which is a randomly assigned ephemeral port so the packet is routed through the nat device and in transit the nat device records the source ip and the original source private port and it allocates a new public ip address and a new public source port which in this case is 8844 it records this information inside the not table as shown here and it adjusts the pocket so that its source ip address is the public ip address that the nat device is using and the source port is this newly allocated source port and this newly adjusted packet is forwarded on to fashiontube now the process is very similar with the return traffic where the packet will verify the recorded ips and ports in the nat table before forwarding the packet back to the originating source now if the middle laptop with the ip of 192.168.0.14 did the same thing then the same process would be followed all of this information would be recorded in the nat table a new public source port would be allocated and would translate the packet adjusting the packet's source ip address and source port as well the same process would happen for the laptop on the bottom generating a packet with the source and destination ip with the addition of the source and destination ports and when routed through the nat device goes through its translation recording the information in the nat table and reaching its destination again return traffic will be verified by the recorded ips and ports in the nat table before forwarding the packet back to its originating source and so just as a summary when it comes to port address translation the nat device records the source ip and source port in a nat table the source ip is then replaced with a public ip and public source port and are allocated from a pool that allows overloading and this is a manytoone architecture and so for the telephone analogy for pat let's use a phone operator example so in this instance george is trying to call laura now george only knows lark laura's executive admin and only has lark's phone number george does not have laura's private line lark's public phone number is the equivalent to having a public ip address george calls lark who then connects george to laura the caveat here is that lark never gives out laura's phone number in fact laura doesn't have a public phone number and can only be called by lark and here's where nat can add an extra layer of security by only allowing needed ports to be accessed without allowing anyone to connect to any port now i hope this has helped you understand the process of network address translation how the translation happens and the process of using a nat table to achieve packet translation along with its destination this is so common in most environments that you will encounter and it's very important to fully understand the different types of not and how it can be used in these types of environments and so that's pretty much all i wanted to cover on this lesson of network address translation so you can now mark this lesson as complete and let's move on to the next one welcome back so now that we've covered the fundamentals of dns along with the different record types i wanted to focus in on google cloud's dns service called cloud dns now cloud dns is a fully managed service that manages dns servers for your specific zones and since cloud dns shows up on the exam only on a high level i will be giving an overview of what this service can do so with that being said let's dive in now cloud dns acts as an authoritative dns server for public zones that are visible to the internet or for private zones that are visible only within your network and is commonly referred to as google's dns as a service cloud dns has servers that span the globe making it a globally resilient service now while it is a global service there is no way to select specific regions to deploy your zones and dns server policies you simply add your zones records and policies and it is distributed amongst google's dns servers across the globe cloud dns is also one of the few google cloud services that offers 100 availability along with low latency access by leveraging google's massive global network backbone now in order to use cloud dns with a specific publicly available domain a domain name must be purchased through a domain name registrar and you can register a domain name through google domains or another domain registrar of your choice cloud dns does not provide this service and just as a note that to create private zones the purchasing of a domain name is not necessary now as stated earlier cloud dns offers the flexibility of hosting both public zones and privately managed dns zones now public zones are zones that are visible to the public internet and so when cloud dns is managing your public domain it has public authoritative name servers that respond to public zone dns queries for your specific domain now when it comes to private zones these enable you to manage custom domain names for your google cloud resources without exposing any dns data to the public internet a private zone can only be queried by resources in the same project where it is defined and as we discussed earlier a zone is a container of dns records that are queried by dns so from a private zone perspective these can only be queried by one or more vpc networks that you authorize to do so and just as a note the vpc networks that you authorize must be located in the same project as the private zone to query records hosted in manage private zones in other projects the use of dns peering is needed now i don't want to get too deep into dns peering but just know that vpc network peering is not required for the cloud dns peering zone to operate peering zones do not depend on vpc network peering now each managed zone that you create is associated with a google cloud project and once this zone is created it is hosted by google's managed name servers now these zones are always hosted on google's manage name servers within google cloud so you would create records and record sets and these servers would then become allocated to that specific zone hosting your records and record sets and just as a quick reminder a record set is the collection of dns records in a zone that have the same name and are of the same type most records contain a single record but it's not uncommon to see record sets a great example of this are a records or ns records which we discussed earlier and these records can usually be found in pairs and so now to give you a practical example of cloud dns i wanted to bring the theory into practice through a short demo where i'll be creating a managed private zone so whenever you're ready join me in the console and so here we are back in the console and i'm logged in as tonybowties gmail.com and i'm currently in project bowtie inc so now to get to cloud dns i'm going to go over to the navigation menu i'm going to scroll down to network services and go over to cloud dns and because i currently don't have any zones i'm prompted with only one option which is to create a zone and so i'm going to go ahead and create a zone and so here i've been prompted with a bunch of different options in order to create my dns zone and so the first option that i have is zone type and because i'm creating a private zone i'm going to simply click on private and i need to provide a zone name which i'm going to call tony bowtie next i'm going to have to provide a dns name which i will call tony bowtie dot private and under the description i'm just going to type in private zone for tony bowtie and so the next field i've been given is the options field where it is currently marked as default private and so if i go over here to the right hand side and open up the drop down menu i'm given the options to forward queries to another server dns peering manage reverse lookup zones and use a service directory namespace and so depending on your type of scenario one of these five options in most cases will suffice so i'm going to keep it under default private and under networks it says your private zone will be visible to the selected networks and so i'm going to click on the drop down and i'm giving only the option of the default network because it's the only network that i have and so i'm going to select it and i'm going to click on the white space and if i feel so inclined i can simply click on the shortcut for the command line and here i'm given this specific commands if i was to use the command line in order to create this dns zone so i'm going to click on close here and i'm going to click on create and as you can see here my zone has been created along with a couple of dns records the first one being my name server records as well as my start of authority records and so as a note to know for the exam when creating a zone these two records will always be created both the soa record and the ns record and moving on to some other options here i can add another record set if i choose to again the dns name the record type which i have a whole slew of record types to choose from it's ttl and the ip address but i'm not going to add any records so i'm just going to cancel and by clicking in use by i can view which vpc network is using this zone and as expected the default network shows up and i also have the choice of adding another network but since i don't have any other networks i can't add anything so i'm going to simply cancel i also have the option of removing any networks so if i click on this i can remove the network or i can also remove the network by clicking on the hamburger menu and so as you can see i have a slew of options to choose from when creating zones and record sets and so that about covers everything that i wanted to show you here in cloud dns but before i go i'm going to go ahead and clean up and i'm just going to click on the garbage can here on the right hand side of the zone and i'm going to be prompted if i want to delete the zone yes i do so i'm going to click on delete and so that pretty much covers everything that i wanted to show you with regards to cloud dns so you can now mark this lesson as complete and let's move on to the next one welcome back now before we step into the compute engine section of the course i wanted to cover a basic foundation of what makes these vms possible and this is where a basic understanding of virtualization comes into play now this is merely an introductory lesson to virtualization and i won't be getting too deep into the underpinnings it serves as just a basic foundation as to how compute engine gets its features under the hood and how they are possible through the use of virtualization for more indepth understanding on virtualization i will be including some links in the lesson text for those who are looking to learn more but for now this will provide just enough theory to help you understand how compute engine works so with that being said let's dive in so what exactly is virtualization well virtualization is the process of running multiple operating systems on a server simultaneously now before virtualization became popular a standard model was used where an operating system would be installed on a server so the server would consist of typical hardware like cpu memory network cards and other devices such as video cards usb devices and storage and then the operating system would run on top of the hardware now there is a middle layer of the operating system a supervisor if you will that is responsible for interacting with underlying hardware and this is known as the kernel the kernel manages the distribution of the hardware resources of the computer efficiently and fairly among all the various processes running on the computer now the kernel operates under what is called kernel mode or privilege mode as it runs privileged instructions that interacts with the hardware directly now the operating system allows other software to run on top of it like an application but cannot interact directly with the hardware it must interact with the operating system in user mode or nonprivileged mode so when lark decides to do something on an application that needs to use the system hardware that application needs to go through the operating system it needs to make what's known as a system call and this is the model of running one operating system on a single server now when passed servers would traditionally run one application on one server with one operating system in the old system the number of servers would continue to mount since every new application required its own server and its own operating system as a result expensive hardware resources were purchased but not used and each server would use approximately under 20 of its resources on average server resources were then known as underutilized now there came a time when multiple operating systems were installed on one computer isolated from each other with each operating system running their own applications this was a perfect model to consolidate hardware and keep utilization high but there is a major issue that arose each cpu at this given moment in time could only have one thing running as privileged so having multiple operating systems running on their own in an unmodified state and expecting to be running on their own in a privileged state running privileged instructions was causing instability in systems causing not just application crashes but system crashes now a hypervisor is what solved this problem it is a small software layer that enables multiple operating systems to run alongside each other sharing the same physical computing resources these operating systems come as virtual machines or vms and these are files that mimic an entire computing hardware environment in software the hypervisor also known as a virtual machine monitor or vmm manages these vms as they run alongside each other it separates virtual machines from each other logically assigning each its own slice of the underlying computing cpu memory and other devices like graphics network and storage this prevents the vms from interfering with each other so if for example one operating system suffers a crash or a security compromise the others will survive and continue running now the hypervisor was never as efficient as how you see it here it went through some major iterations that gave its structure as we know it today initially virtualization had to be done in software or what we now refer to as the host machine and the operating system with its applications put in logical containers known as virtual machines or guests the operating system would be installed on the host which included additional capabilities called a hypervisor and allowed it to make the necessary privileged calls to the hardware having full access to the host the hypervisor exposed the interface of the hardware device that is available on the host and allowed it to be mapped to the virtual machine and emulated the behavior of this device and this allowed the virtual machine using the operating system drivers that were designed to interact with the emulated device without installing any special drivers or tools as well as keeping the operating system unmodified the problem here is that it was all emulated and so every time the virtual machines made calls back to the host each instruction needed to be translated by the hypervisor using what's called a binary translation now without this translation the emulation wouldn't work and would cause system crashes bringing down all virtual machines in the process now the problem with this process is that it made the system painfully slow and it was this performance penalty that caused this process to not be so widely adopted but then another type of virtualization came on the scene called para virtualization now in this model a modified guest operating system is able to speak directly to the hypervisor and this involves having the operating system kernel to be modified and recompiled before installation onto the virtual machine this would allow the operating system to talk directly with the hypervisor without any performance hits as there is no translation going on like an emulation para virtualization replaces instructions that cannot be virtualized with hyper calls that communicate directly with the hypervisor so a hypercall is based on the same concept as a system call privileged instructions that accept instead of calling the kernel directly it calls the hypervisor and due to the modification in this guest operating system performance is enhanced as the modified guest operating system communicates directly with the hypervisor and emulation overhead is removed the guest operating system becomes almost virtualization aware yet there is still a process whereby software was used to speak to the hardware the virtual machines could still not access the hardware directly although things changed in the world of virtualization when the physical hardware on the host became virtualization aware and this is where hardware assisted virtualization came into play now hardware assisted virtualization is an approach that enables efficient full virtualization using help from hardware capabilities on the host cpu using this model the operating system has direct access to resources without any hypervisor emulation or operating system modification the hardware itself becomes virtualization aware the cpu contains specific instructions and capabilities so that the hypervisor can directly control and configure this support it also provides improved performance because the privileged instructions from the virtual machines are now trapped and emulated in the hardware directly this means that the operating system kernels no longer need to be modified and recompiled like in para virtualization and can run as is at the same time the hypervisor also does not need to be involved in the extremely slow process of binary translation now there is one more iteration that i wanted to discuss when it comes to virtualization and that is kernel level virtualization now instead of using a hypervisor kernel level virtualization runs a separate version of the linux kernel and sees the associated virtual machine as a user space process on the physical host this makes it easy to run multiple virtual machines on a single host a device driver is used for communication between the main linux kernel and the virtual machine every vm is implemented as a regular linux process scheduled by the standard linux scheduler with dedicated virtual hardware like a network card graphics adapter cpu memory and disk hardware support by the cpu is required for virtualization a slightly modified emulation process is used as the display and execution containers for the virtual machines in many ways kernel level virtualization is a specialized form of server virtualization and this is the type of virtualization platform that is used in all of google cloud now with this type of virtualization because of the kernel acting as the hypervisor it enables a specific feature called nested virtualization now with nested virtualization it is made possible to install a hypervisor on top of the already running virtual machine and so this is what google cloud has done now you're probably wondering after going through all the complexities involved with previous virtualization models what makes this scenario worthwhile well using nested virtualization it makes it easier for users to move their onpremises virtualized workloads to the cloud without having to import and convert vm images so in essence it eases the use when migrating to cloud a great use case for many but wouldn't be possible on google cloud without the benefit of running kernel level virtualization now this is an advanced concept that does not show up on the exam but i wanted you to understand virtualization at a high level so that you can understand nested virtualization within google cloud as it is a part of the feature set of compute engine and so that's pretty much all i wanted to cover when it comes to virtualization so you can now mark this lesson as complete and let's move on to the next one welcome back now earlier on in the course i discussed compute engine at a high level to understand what it is and what it does the goal for this section is to dive deeper into compute engine as it comes up heavily on the exam and so i want to make sure i expose all the nuances as well it is the goto service offering from google cloud when looking to solve any general computing needs with this lesson specifically i will be going into what makes up an instance and the different options that are available when creating the instance so with that being said let's dive in now compute engine lets you create and run virtual machines known as instances and host them on google's infrastructure compute engine is google's infrastructure as a service virtual machine offering so it being an is service google takes care of the virtualization platform the physical servers the network and storage along with managing the data center and these instances are available in different sizes depending on how much cpu and memory you might need as well compute engine offers different family types for the type of workload you need it for each instance is charged by the second after the first minute as this is a consumption based model and as well these instances are launched in a vpc network in a specific zone and these instances will actually sit on hosts in these zones and you will be given the option of using a multitenant host where the server that is hosting your machine is shared with others but please note that each instance is completely isolated from the other so no one can see each other's instances now you're also given the option of running your instance on a sole tenant node whereby your instance is on its own dedicated hosts that is reserved just for you and you alone you don't share it with anyone else and this is strictly for you only now although this option may sound really great it does come at a steep cost so only if your use case requires you to use a sole tenant node for security or compliance purposes i recommend that you stick with a multitenant host when launching your instances and this is usually the most common selection for most now compute engine instances can be configured in many different ways and allow you the flexibility to fulfill the requests for your specific scenario and as you can see here there are four different base options when it comes to configuration of the instance that you are preparing to launch and so i wanted to take time to go through them in just a bit of detail for context starting first with the machine type which covers vcpu and memory now there are many different predefined machine types that i will be covering in great depth in a different lesson but for now just know that they are available in different families depending on your needs and can be chosen from the general compute optimize and memory optimize machine types they are available in intel or amd flavors and if the predefined options doesn't fit your need you have the option of creating a custom machine that will suit your specific workload now when creating a vm instance on compute engine each virtual cpu or vcpu is implemented as a single hardware hyper thread on one of the available cpu processors that live on the host now when choosing the amount of vcpus on an instance you must take into consideration the desired network throughput as the amount of vcpus will determine this throughput as the bandwidth is determined per vm instance not per network interface or per ip address and so the network throughput is determined by calculating 2 gigabits per second for every vcpu on your instance so if you're looking for greater network throughput then you may want to select an instance with more vcpus and so once you've determined a machine type for your compute engine instance you will need to provide it an image with an operating system to boot up with now when creating your vm instances you must use an operating system image to create boot disks for your instances now compute engine offers many preconfigured public images that have compatible linux or windows operating systems and these operating system images can be used to create and start instances compute engine uses your selected image to create a persistent boot disk for each instance by default the boot disk for your instance is the same size as the image that you selected and you can use most public images at no additional cost but please be aware that there are some premium images that do add additional cost to your instances now moving on to custom images this is a boot disk image that you own and control access to a private image if you will custom images are available only to your cloud project unless you specifically decide to share them with another project or another organization you can create a custom image from boot disks or other images then use the custom image to create an instance custom images that you import to compute engine add no cost to your instances but do incur an image storage charge while you keep your custom image in your project now the third option that you have is by using a marketplace image now google cloud marketplace lets you quickly deploy functional software packages that run on google cloud you can start up a software package without having to manually configure the software the vm instances the storage or even the network settings this is a allinone instance template that includes the operating system and the software preconfigured and you can deploy a software package whenever you like and is by far the easiest way to launch a software package and i will be giving you a run through on these marketplace images in a later demo now once you've decided on your machine type as well as the type of image that you wanted to use moving into the type of storage that you want would be your next step now when configuring a new instance you will need to create a new boot disk for it and this is where performance versus cost comes into play as you have the option to pay less and have a slower disk speed or lower iops or you can choose to have fast disk speed with higher iops but pay a higher cost and so the slowest and most inexpensive of these options is the standard persistent disk which are backed by standard hard disk drives the balance persistent disks are backed by solid state drives and are faster and can provide higher iops than the standard option and lastly ssd is the fastest option which also brings with it the highest iops available for persistent disks now outside of these three options for persistent disks you also have the option of choosing a local ssd and these are solid state drives that are physically attached to the server that hosts your vm instances and this is why they have the highest throughput and lowest latency than any of the available persistent disks just as a note the data that you store on a local ssd persists only until the instance is stopped or deleted which is why local ssds are suited only for temporary storage such as caches or swap disk and so lastly moving into networking each network interface of a compute engine instance is associated with a subnet of a unique vpc network as you've seen in the last section you can do this with an auto a default or a custom network each network is available in many different regions and zones within that region we've also experienced routing traffic for our instance both in and out of the vpc network by use of firewall rules targeting ip ranges specific network tags or by instances within the network now load balancers are responsible for helping distribute user traffic across multiple instances either within the network or externally using a regional or global load balancer and i will be getting into low balancing in another section of the course but i wanted to stress that load balancers are part of instance networking that help route and manage traffic coming in and going out of the network and so this is a high level overview of the different configuration types that go into putting together an instance and i will be diving deeper into each in this section as well i will be putting a handson approach to this by creating an instance in the next lesson and focusing on the different available features that you can use for your specific use case and so this is all i wanted to cover for this lesson so you can now mark this lesson as complete and let's move on to the next one welcome back now i know in previous demonstrations we've built quite a few compute engine instances and have configured them accordingly in this demonstration we're going to go through a build of another instance but i wanted to dig deeper into the specific configurations that are available for compute engine so with that being said let's dive in and so i am now logged in under tony bowties gmail.com as well i am logged in under the bowtie inc project so in order to kick off this demo i'm going to head on over to the compute engine console so i'm going to go over to the navigation menu and i'm going to scroll down to compute engine and so here i'm prompted to either create or import a vm instance as well as taking the quick start and so i'm not going to import or take the quick start so i'm going to simply click on create and so i want to take a moment here to focus on the left hand menu where there are a bunch of different options to create any given instance so the first and default option allows me to create the instance from scratch choosing the new vm instance from template option allows me to create a new instance from an instance template and because i don't have any instance templates i am prompted here with the option to create one and so for those of you who are unfamiliar with instance templates templates are used in managed instance groups and define instance properties for when instances are launched within that managed instance group but don't worry i will be covering instance groups and instant templates in a later lesson the next option that's available is new vm instance from machine image and an image is a clone or a copy of an instance and again i will be covering this in a separate lesson and going through all the details of machine images but if i did have any machine images i would be able to create my instance from here but since i do not i am prompted with the option to create a new machine image now the last option that i wanted to show you is the marketplace and so the marketplace has existing machine images that are all preconfigured with its proper operating system as well as the software to accompany it so for instance if i'm looking to create a vm with a wordpress installation on it i can simply go up to the top to the search bar type in wordpress and i will be presented with many different options and i'm just going to choose the one here at the top and i am presented with 49 results of virtual machines with different types of wordpress installations on them and these are all different instances that have been configured specifically for wordpress by different companies like lightspeed analog innovation and cognosis inc and so for this demonstration i'm going to choose wordpress on centos 7 and here i'm giving an overview about the software itself i'm also given information about the company that configured this as well at the top i'm given a monthly estimated cost for this specific instance and if i scroll down the page i can get a little bit more information with regards to this image and as shown here on the right i can see my pricing the usage fee will cost me 109 a month along with the vm instance type that the software is configured for the amount of disk space and the sustained use discount i've also been given some links here for tutorials and documentation and i've also been given instructions for maintenance and support i've been given both an email and a link to live support and of course at the bottom we have the terms of service and this is a typical software package amongst many others that's available in the google cloud marketplace now i can go ahead and launch this if i choose but i'm going to choose not to launch this and i'm going to back out and so just to give you some context with regards to enterprise software software packages like f5 and jenkins are also available in the google cloud marketplace and again when i click on the first option it'll give me a bunch of available options on jenkins and its availability from different companies on different platforms now just as a note to update your existing deployment of a software package you have to redeploy the software package from marketplace in order to update it but other than that caveat the easiest way to deploy a software package is definitely through the marketplace and so now that we've gone through all the different options on how to create an instance i'm gonna go back and select new vm instance so i can create a new vm from scratch and so i am prompted here at the top with a note telling me that there was a draft that was saved from when i started to create in my new instance but i navigated away from it and i have the option to restore the configuration i was working on and so just know that when you are in the midst of creating an instance google cloud will automatically save a draft of your build so that you are able to continue working on it later now i don't really need this draft but i will just hit restore and for the name i'm going to keep it as instance 1 and for the sake of this demo i'm going to add a label the key is going to be environment and the value will be testing i'm going to go down to the bottom click save now when it comes to the geographic location of the instance using regions i can simply click on the drop down and i will have access to deploy this instance in any currently available region as regions are added they will be added here as well and so i'm going to keep it as us east one and under zone i have the availability of putting it in any zone within that region and so i'm going to keep it as us east 1b and just as another note once you've deployed the instance in a specific region you will not be able to move that instance to a different region you will have to recreate it using a snapshot in another region and i will be going over this in a later lesson now scrolling down to machine configuration there are three different types of families that you can choose from when it comes to machine types the general purpose the compute optimized and the memory optimized the general purpose machine family has a great available selection of different series types that you can choose from and is usually the go to machine family if you're unsure about which machine type to select so for this demo i'm going to keep my selection for series type as e2 and under machine type i'm given a very large selection of different sizes when it comes to vcpu and memory and so i can select from a shared core a standard type a high memory type or a high cpu type and i will be going over this in greater detail in another lesson on machine types now in case the predefined machine types do not fit my needs or the scope for the amount of vcpus and memory that i need fall in between those predefined machine types i can simply select the custom option and this will bring up a set of sliders where i am able to select both the amount of vcpus and amount of memory that i need for the instance that i am creating now as i change the course slider to either more vcpus or less my core to memory ratio for this series will stay the same and therefore my memory will be adjusted automatically i also have the option to change the memory as i see fit to either add more memory or to remove it and so this is great for when you're in between sizes and you're looking for something specific that fits your workload and so i'm going to change back the machine type to an e2 micro and as you can see in the top right i will find a monthly estimate of how much the instance will cost me and i can click on this drop down and it will give me a breakdown of the cost for vcpu in memory the cost for my disks as well as my sustained use discount and if i had any other resources that i was consuming like a static ip or an extra attached disk those costs would show up here as well and so if i went to a compute optimized you can see how the price has changed but i'm given the breakdown so that i know exactly what i'm paying for so i'm going to switch it back to general purpose and i wanted to point out here the cpu platform and gpu as you can add gpus to your specific machine configuration and so just as another note gpus can only be added to an n1 machine type as any other type will show the gpu selection as grayed out and so here i can add the gpu type as well as adding the number of gpus that i need but for the sake of this demonstration i'm not going to add any gpus and i'm going to select the e2 series and change it back to e2 micro scrolling down a little bit here when it comes to cpu platform depending on the machine type you can choose between intel or amd if you are looking for a specific cpu but just know that your configuration is permanent now moving down a little bit more you will see here display device now display device is a feature on compute engine that allows you to add a virtual display to a vm for system management tools remote desktop software and any application that requires you to connect to a display device on a remote server this is an especially great feature to have for when your server is stuck at boot patching or hardware failure and you can't log in and the drivers are already included for both windows and linux vms this feature works with the default vga driver right out of the box and so i'm going to keep this checked off as i don't need it and i'm going to move down to confidential vm service now confidential computing is a security feature to encrypt sensitive code and data that's in memory so even when it's being processed it is still encrypted and is a great use case when you're dealing with very sensitive information that requires strict requirements now compute engine also gives you the option of deploying containers on it and this is a great way to test your containers instead of deploying a whole kubernetes cluster and may even suffice for specific use cases but just note that you can only deploy one container per vm instance and so now that we've covered most of the general configuration options for compute engine i wanted to take a minute to dive into the options that are available for boot disk so i'm going to go ahead and click on change and here i have the option of choosing from a bunch of different public images with different operating systems that i can use for my boot disk so if i wanted to load up ubuntu i can simply select ubuntu and i can choose from each different version that's available as well i'm shown here the boot disk type which is currently selected as the standard persistent disk but i also have the option of selecting either a balanced persistent disk or ssd persistent disk and i'm going to keep it as standard persistent disk and if i wanted to i can increase the boot disk size so if i wanted 100 gigs i can simply add it and if i select it and i go back up to the top right hand corner i can see that my price for the instance has changed now i'm not charged for the operating system due to it being an open source image but i am charged more for the standard persistent disk because i'm no longer using 10 gigs but i'm using 100 gigabytes now let's say i wanted to go back and i wanted to change this image to a windows image i'm going to go down here to windows server and i want to select windows server 2016 i'm going to load up the data center version and i'm going to keep the standard persistent disk along with 100 gigabytes i'm going to select it if i scroll back up i can see that i'm charged a licensing fee for windows server and these images with these licensing fees are known as premium images so please make sure that you are aware of these licensing fees when launching your instances and because i want to save on money just for now i'm going to scroll back down to my boot disk and change it back to ubuntu and i'm going to change the size back down to 10 gigabytes as well before you move on i wanted to touch on custom images and so if i did have any custom images i could see them here and i would be able to create instances from my custom images using this method i also have the option of creating an instance from a snapshot and because i don't have any nothing shows up and lastly i have the option of using existing disks so let's say for instance i had a vm instance and i had deleted it but i decided to keep the attached boot disk it would show up as unattached and i am able to attach that to a new instance and so now that i've shown you all the available options when it comes to boot disk i'm going to go ahead and select the ubuntu operating system and move on to the next option here we have identity and api access which we've gone through in great depth in a previous demo as well i'm given an option to create a firewall rule automatically for http and https traffic and as for networking as we covered it in great depth in the last section i will skip that part of the configuration and simply launch it in the default vpc and so just as a quick note i wanted to remind you that down at the bottom of the page you can find the command line shortcut and when you click on it it will give you the gcloud command to run that you can use in order to create your instance and so i want to deploy this as is so i'm going to click here on close and i'm going to click on create and so i'm just going to give it a minute now so the instance can be created and it took a few seconds but the instance is created and this is regarded as the inventory page to view your instance inventory and to look up any correlating information on any of your instances and so this probably looks familiar to you from the previous instances that you've launched so here we have the name of the instance the zone the internal ip along with the external ip and a selection to connect to the instance as well i'm also given the option to connect to this instance in different ways you also have the option of adding more column information to your inventory dashboard with regards to your instance and you can do this by simply clicking on the columns button right here above the list of instances and you can select from creation time machine type preserve state and even the network and this may bring you more insight on the information available for that instance or even grouping of instances with common configurations this will also help you identify your instances visually in the console and so i'm just going to put the columns back to exactly what it was and so now i want to take a moment to dive right into the instance and have a look at the instance details so as you remember we selected the machine type of e2 micro which has two vcpus and one gigabyte of memory here we have the instance id as well scrolling down we have the cpu platform we have the display device that i was mentioning earlier along with the zone the labels the creation time as well as the network interface and scrolling down i can see here the boot disk with the ubuntu image as well as the name of the boot disk so there are quite a few configurations here and if i click on edit i can edit some of these configurations on the fly and with some configurations i need to stop the instance before editing them and there are some configurations like the network interface where i would have to delete the instance in order to recreate it so for instance if i wanted to change the machine type i need to stop the instance in order to change it and the same thing goes for my display device as well the network interface in order for me to change it from its current network or subnetwork i'm going to have to stop the instance in order to change it as well and so i hope this general walkthrough of configuring an instance has given you a sense of what can be configured on launch and allowed you to gain some insight on editing features of an instance after launch a lot of what you've seen here in this demo will come up in the exam and so i would recommend that before going into the exam to spend some time launching instances knowing exactly how they will behave and what can be edited after creation that can be done on the fly edits that need the instance to be shut down and edits that need the instance to be recreated and so that's pretty much all i wanted to cover when it comes to creating an instance so you can now mark this as complete and let's move on to the next one welcome back now in this lesson i'm going to be discussing compute engine machine types now a machine type is a set of virtualized hardware resources that's available to a vm instance including the system memory size virtual cpu count and persistent disks in compute engine machine types are grouped and curated by families for different workloads you must always choose a machine type when you create an instance and you can select from a number of predefined machine types in each machine type family if the predefined machine types don't meet your needs then you can create your own custom machine types in this lesson i will be going through all the different machine types their families and their use cases so with that being said let's dive in now each machine type family displayed here includes different machine types each family is curated for specific workload types the following primary machine types are offered on compute engine which is general purpose compute optimized and memory optimized and so i wanted to go through each one of these families in a little bit of detail now before diving right into it defining what type of machine type you are running can be overwhelming for some but can be broken down to be understood a bit better they are broken down into three parts and separated by hyphens the first part in this example shown here is the series so for this example the series is e2 and the number after the letter is the generation type in this case it would be the second generation now the series come in many different varieties and each are designed for specific workloads now moving on to the middle part of the machine type this is the actual type and types as well can come in a slew of different flavors and is usually coupled with a specific series so in this example the type here is standard and so moving on to the third part of the machine type this is the amount of vcp use in the machine type and so with vcpus they can be offered anywhere from one vcpu up to 416 vcpus and so for the example shown here this machine type has 32 vcpus and so there is one more aspect of a machine type which is the gpus but please note that gpus are only available for the n1 series and so combining the series the type and the vcpu you will get your machine type and so now that we've broken down the machine types in order to properly define them i wanted to get into the predefined machine type families specifically starting off with the general purpose predefined machine type and all the general purpose machine types are available in the standard type the high memory type and the high cpu type so the standard type is the balance of cpu and memory and this is the most common general purpose machine type general purpose also comes in high memory and this is a high memory to cpu ratio so very high memory a lower cpu and lastly we have the high cpu machine type and this is a high cpu to memory ratio so this would be the opposite of the high memory so very high cpu to lower memory so now digging into the general purpose machine family i wanted to start off with the e2 series and this is designed for daytoday computing at a low cost so if you're looking to do things like web serving application serving back office applications small to medium databases microservices virtual desktops or even development environments the e2 series would serve the purpose perfectly now the e2 machine types are cost optimized machine types that offer sizing between 2 to 32 vcpus and half a gigabyte to 128 gigabytes of memory so small to medium workloads that don't require as many vcpus and applications that don't require local ssds or gpus are an ideal fit for e2 machines e2 machine types do not offer sustained use discounts however they do provide consistently low ondemand and committed use pricing in other words they offer the lowest ondemand pricing across the general purpose machine types as well the e2 series machines are available in both predefined and custom machine types moving on i wanted to touch on all the machine types available in the nseries and these are a balanced machine type with price and performance across a wide range of vm flavors and these machines are designed for web servers application servers back office applications medium to large databases as well as caching and media streaming and they are offered in the standard high memory and high cpu types now the n1 machine types are compute engines first generation general purpose machine types now this machine type offers up to 96 vcpus and 624 gigabytes of memory and again as i mentioned earlier this is the only machine type that offers both gpu support and tpu support the n1 type is available as both predefined machine types and custom machine types and the n1 series offers a larger sustained use discount than n2 machine types speaking of which the n2 machine types are the second generation general purpose machine types and these offer flexible sizing between two 280 vcpus and half a gigabyte of memory to 640 gigabytes of memory and these machine types also offer an overall performance improvement over the n1 machine types workloads that can take advantage of the higher clock frequency of the cpu are a good choice for n2 machine types and these workloads can get higher per thread performance while benefiting from all the flexibility that a general purpose machine type offers and two machine types also offer the extended memory feature and this helps control per cpu software licensing costs now getting into the last n series machine type the n2d machine type is the largest general purpose machine type with up to 224 vcpus and 896 gigabytes of memory this machine type is available in predefined and custom machine types and this machine type as well has the extended memory feature which i discussed earlier that helps you avoid per cpu software licensing the n2d machine type supports the committed use and sustain use discounts now moving on from the general purpose machine type family i wanted to move into the compute optimize machine family now this series offers ultra high performance for compute intensive workloads such as high performance computing electronic design automation gaming and single threaded applications so anything that is designed for compute intensive workloads this will definitely be your best choice now compute engine optimized machine types are ideal for as i said earlier compute intensive workloads and these machine types offer the highest performance per core on compute engine compute optimized types are only available as predefined machine types and so they are not available for any custom machine types the c2 machine types offer a maximum of 60 vcpus and a maximum of 240 gigabytes of memory now although the c2 machine type works great for compute intensive workloads it does come with some caveats and so you cannot use regional persistent disks with compute optimized machine types and i will be getting into the details of persistent disks in a later lesson and they are only available in select zones and regions on select cpu platforms and so now moving into the last family is the memory optimize machine family and this is for ultra high memory workloads this family is designed for large in memory databases like sap hana as well as in memory analytics now the m series comes in two separate generations m1 and m2 the m1 offering a maximum of 160 vcpus and a maximum memory of 3844 gigabytes whereas the m2 offering again a maximum of 160 vcpus but offering a whopping 11 776 gigabytes of maximum memory and as i said before these machine types they're ideal for tasks that require intensive use of memory so they are suited for inmemory databases and in memory analytics data warehousing workloads genomics analysis and sql analysis services memory optimized machine types are only available as predefined machine types and the caveats here is that you cannot use regional persistent disks with memory optimized machine types as well they're only available in specific zones now i wanted to take a moment to go back to the general purpose machine type so that i can dig into the shared cord machine type and this is spread amongst the e2 and n1 series and these shared core machine types are used for burstable workloads are very cost effective as well they're great for nonresource intensive applications shared core machine types use context switching to share a physical core between vcpus for the purpose of multitasking different shared core machine types sustain different amounts of time on a physical core which allows google cloud to cut the price in general share core instances can be more cost effective for running small nonresource intensive applications than standard high memory or high cpu machine types now when it comes to cpu bursting these shared core machine types offer bursting capabilities that allow instances to use additional physical cpu for short periods of time bursting happens automatically when your instance requires more physical cpu than originally allocated during these spikes your instance will take advantage of available physical cpu in bursts and the e2 shared core machine type is offered in micro small and medium while the n1 series is offered in the f1 micro and the g1 small and both of these series have a maximum of two vcpus with a maximum of four gigabytes of memory now i wanted to take a moment to touch on custom machine types and these are available for any general purpose machine and so this is customer defined cpu and memory designed for custom workloads now if none of the general purpose predefined machine types cater to your needs you can create a custom machine type with a specific number of vcpus and amount of memory that you need for your instance these machine types are ideal for workloads that are not a good fit for the predefined machine types that are available they're also great for when you need more memory or more cpu but the predefined machine types don't quite fit exactly what you need for your workload just as a note it costs slightly more to use a custom machine type than a predefined machine type and there are limitations in the amount of memory and vcpu you can select and as i stated earlier when creating a custom machine type you can choose from the e2 n2 and 2d and n1 machine types and so the last part i wanted to touch on are the gpus that are available and these are designed for the graphic intensive workloads and again are only available for the n1 machine type and gpus come in five different flavors from nvidia showing here as the tesla k80 the tesla p4 the tesla t4 the tesla v100 and the tesla p100 and so these are all the families and machine types that are available for you in google cloud and will allow you to be a little bit more flexible with the type of workload that you need them for and so for the exam you won't have to memorize each machine type but you will need to know an overview of what each machine type does now i know there's been a lot of theory presented here in this lesson but i hope this is giving you a better understanding of all the available predefined machine types in google cloud and so that's pretty much all i wanted to cover in this lesson on compute engine machine types so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to be reviewing managing your instances now how you manage your instances is a big topic in the exam as well it's very useful to know for your work as a cloud engineer in the environments you are responsible for knowing both the features that are available as well as the best practices will allow you to make better decisions with regards to your instances and allow you to keep your environment healthy this lesson will dive into the many features that are available in order to better manage your instances using the specific features within google cloud so with that being said let's dive in now i wanted to start off this lesson discussing the life cycle of an instance within google cloud every instance has a predefined life cycle from its starting provisioning state to its deletion an instance can transition through many instant states as part of its life cycle when you first create an instance compute engine provisions resources to start your instance next the instance moves into staging where it prepares the first boot and then it finally boots up and is considered running during its lifetime a running instance can be repeatedly stopped and restarted or suspended and resumed so now i wanted to take a few minutes to go through the instance life cycle in a bit of detail starting with the provisioning state now this is where resources are being allocated for the instance the instance is not yet running and the instance is being allocated its requested amount of cpu and memory along with its root disk any additional disks that are attached to it and as well some additional feature sets that are assigned to this instance and when it comes to the cost while in the provisioning state there are no costs that are being incurred moving right along to the staging state after finishing the provisioning state the life cycle continues with the staging state and this is where resources have been acquired and the instance is being prepared for first boot both internal and external ips are allocated and can be either static or ephemeral in the system image that was originally chosen for this instance is used to boot up the instance and this can be either a public image or a custom image costs in the state are still not incurred as the instance is still in the preboot state now once the instance has left staging it will move on to the running state and this is where the instance is booting up or running and should allow you to log into the instance either using ssh or rdp within a short waiting period due to any startup scripts or any boot maintenance tasks for the operating system now during the running state you can reset your instance and this is where you would wipe the memory contents of the vm instance and reset the virtual machine to its initial state resetting an instance causes an immediate hard reset of the vm and therefore the vm does not do a graceful shutdown for the guest operating system however the vm retains all persistent disk data and none of the instance properties change the instance remains in running state through the reset now as well in the running state a repair can happen due to the instance encountering an internal error or the underlying machine is unavailable due to maintenance during this time the instance is unusable and if the repair is successful the instance returns back to the running state paying attention to costs this state is where the instance starts to occur them and is related to the resources assigned to the instance like the cpu and memory any static ips and any disks that are attached to the instance and i will be going into a bit of detail in just a bit with regards to this state and finally we end the life cycle with the stopping suspended and terminated states now when you are suspending an instance it is like closing the lid of your laptop suspending the instance will preserve the guest operating system memory and application state of the instance otherwise it'll be discarded and from this state you can choose either to resume or to delete it when it comes to stopping either a user has made a request to stop the instance or there was a failure and this is a temporary status and the instance will move to terminated touching on costs for just a second when suspending or stopping an instance you pay for resources that are still attached to the vm instance such as static ips and persistent disk data you do not pay the cost of a running vm instance ephemeral external ip addresses are released from the instance and will be assigned a new one when the instance is started now when it comes to stopping suspending or resetting an instance you can stop or suspend an instance if you no longer need it but want to keep the instance around for future use compute engine waits for the guest to finish shutting down and then transitions the instance to the terminated state so touching on the terminated state this is where a user either shuts down the instance or the instance encounters a failure you can choose to restart the instance or delete it as well as holding some reset options within the availability policy in this state you still pay for static ips and disks but like the suspending or stopping state you do not pay for the cpu and memory resources allocated to the instance and so this covers a high level overview of the instance lifecycle in google cloud and all of the states that make up this lifecycle now to get into some detail with regards to some feature sets for compute engine i wanted to revisit the states where those features apply now when creating your instance you have the option of using shielded vms for added security and when using them the instance would instantiate them as the instance boots and enters into the running state so what exactly is a shielded vm well shielded vms offer verifiable integrity of your compute engine vm instances so you can be sure that your instances haven't been compromised by boot or kernel level malware or rootkits and this is achieved through a fourstep process which is covered by secure boot virtual trusted platform module also known as vtpm measure boot which is running on vtpm and integrity monitoring so i wanted to dig into this for just a sec to give you a bit more context now the boot process for shielded vms start with secure boot and this helps ensure that the system only runs authentic software by verifying the digital signature for all boot components and stopping the boot process if signature verification fails so shielded vm instances run firmware that's signed and verified using google's certificate authority and on each and every boot any boot component that isn't properly signed or isn't signed at all is not allowed to run and so the first time you boot a vm instance measure boot creates the integrity policy baseline from the first set of these measurements and then securely stores this data each time the vm instance boots after that these measurements are taken again and stored in secure memory until the next reboot having these two sets of measurements enables integrity monitoring which is the next step and allows it to determine if there have been changes to a vm instance's boot sequence and this policy is loaded onto a virtualized trusted platform module again known as the vtpm for short which is a specialized computer chip that you can use to protect objects like keys and certificates that you use to authenticate access to your system with shielded vms vtpm enables measured boot by performing the measurements needed to create a known good boot baseline and this is called the integrity policy baseline the integrity policy baseline is used for comparison with measurements from subsequent vm boots to determine if anything has changed integrity monitoring relies on the measurements created by measured boot for both the integrity policy baseline and the most recent boot sequence integrity monitoring compares the most recent boot measurements to the integrity policy baseline and returns a pair of pass or failed results depending on whether they match or not one for the early boot sequence and one for the late boot sequence and so in summary this is how shielded vms help prevent data exfiltration so touching now on the running state when you start a vm instance using google provided public images a guest environment is automatically installed on the vm instance a guest environment is a set of scripts daemons and binaries that read the content of the metadata server to make a virtual machine run properly on compute engine a metadata server is a communication channel for transferring information from a client to the guest operating system vm instances created using google provided public images include a guest environment that is installed by default creating vm instances using a custom image will require you to manually install the guest environment this guest environment is available for both linux and windows systems and each supported operating system that is available on compute engine requires specific guest environment packages either google or the owner of the operating system builds these packages now when it comes to the linux guest environment it is either built by google or the owner of the operating system and there are some key components that are applicable to all builds which can be found in the link that i have included in the lesson text the base components of a linux guest environment is a python package that contains scripts daemons and packages for the supported linux distributions when it comes to windows a similar approach applies where a package is available with main scripts and binaries as a part of this guest environment now touching back on the metadata server compute engine provides a method for storing and retrieving metadata in the form of the metadata server this service provides a central point to set metadata in the form of key value pairs which is then provided to virtual machines at runtime and you can query this metadata server programmatically from within the instance and from the compute engine api this is great for use with startup and shutdown scripts or gaining more insight with your instance metadata can be assigned to projects as well as instances and project metadata propagates to all instances within the project while instance metadata only impacts that instance and you can access the metadata using the following url with the curl command you see here on the screen so if you're looking for the metadata for a project you would use the first url that ends in project and for any instance metadata you can use the second url that ends in instance now please note that when you make a request to get information from the metadata server your request and the subsequent metadata response never leaves the physical host running the virtual machine instance now once the instance has booted and has gone through the startup scripts you will then have the ability to login to your instance using ssh or rdp now there are some different methods that you can use to connect and access both your linux instances and your windows instances that i will be going over now when it comes to linux instances we've already gone through accessing these types of instances in previous lessons and demos but just as a refresher you would typically connect to your vm instance via ssh access on port 22. please note that you will require a firewall rule as we have done in previous demos to allow this access and you can connect to your linux instances through the google cloud console or the cloud shell using the cloud sdk now i know that the use of ssh keys are the defacto when it comes to logging into linux instances now in most scenarios on google cloud google recommends using os login over using ssh keys the os login feature lets you use compute engine iam roles to manage ssh access to linux instances and then if you'd like you can add an extra layer of security by setting up os login with twostep verification and manage access at the organization level by setting up organizational policies os login simplifies ssh access management by linking your linux user account to your google identity administrators can easily manage access to instances at either an instance or project level by setting iam permissions now if you're running your own directory service for managing access or are unable to set up os login you can manually manage ssh keys and local user accounts in metadata by manually creating ssh keys and editing the public ssh key metadata now when it comes to windows instances you would typically connect to your vm instance via rdp access on port 3389 and please note that you will also require a firewall rule as shown here to allow this access you can connect to your windows instances through the rdp protocol or through a powershell terminal now when logging into windows this requires setting a windows password and can be done either through the console or the gcloud command line tool and then after setting your password you can then log in from the recommended rdp chrome extension or using a thirdparty rdp client and i will provide a link to this rdp chrome extension in the lesson text now once the instance has booted up and your instance is ready to be logged into you always have the option of modifying your instance and you can do it manually by either modifying it on the fly or you can take the necessary steps to edit your instance like i showed you in a previous lesson by stopping it editing it and then restarting it although when it comes to google having to do maintenance on a vm or you merely want to move your instance to a different zone in the same region this has all become possible without shutting down your instance using a feature called live migration now when it comes to live migration compute engine migrates your running instances to another host in the same zone instead of requiring your vms to be rebooted this allows google to perform maintenance reliably without interrupting any of your vms when a vm is scheduled to be live migrated google provides a notification to the guest that a migration is coming soon live migration keeps your instances running during compute engine hosts that are in need of regular infrastructure maintenance and upgrades replacement of failed hardware and system configuration changes when google migrates a running vm instance from one host to another it moves the complete instance state from the source to the destination in a way that is transparent to the guest os and anyone communicating with it google also gives you the option of doing live migration manually from one zone to another within the same region either using the console or running the command line you see here gcloud compute instances move the name of the vm with the zone flag and the zone that it's currently in and then the destination zone flag with the zone that you wanted to go to and just as a note with some caveats instances with gpus attached cannot be live migrated and you can't configure a preemptable instance to live migrate and so instance lifecycle is full of different options and understanding them can help better coordinate moving editing and repairing vm instances no matter where they may lie in this life cycle now i hope this lesson has given you the necessary theory that will help better use the discuss feature sets and giving you some ideas on how to better manage your instances now there is a lot more to know than what i've shown you here to manage your instances but topics shown here are what shows up in the exam as well are some really great starting points to begin managing your instances and so that's pretty much all i wanted to cover when it comes to managing instances so you can now mark this lesson as complete and join me in the next one where i will cement the theory in this lesson with the handson demo welcome back in this demonstration i'm going to be cementing some of the theory that we learned in the last lesson with regards to the different login methods for windows and linux instances how to implement these methods are extremely useful to know both for the exam and for managing multiple instances in different environments now there's a lot to cover here so with that being said let's dive in so as you can see i am logged in here under tony bowtie ace gmail.com as well i am in the project of bowtie inc and so the first thing that i want to do is create both a linux instance and a windows instance and this is to demonstrate the different options you have for logging into an instance and so in order for me to do that i need to head on over to compute engine so i'm going to go over to the navigation menu and i'm going to scroll down to compute engine and so just as a note before creating your instances please make sure that you have a default vpc created before going ahead and creating these instances if you've forgotten how to create a default vpc please go back to the networking services section and watch the vpc lesson for a refresher and so i'm going to go ahead and create my first instance and i'm going to start with the windows instance so i'm going to simply click on create and so for the name of this instance you can simply call this windows dash instance and i'm not going to add any labels and for the region you should select us east1 and you can keep the zone as the default for us east 1b and scrolling down to the machine configuration for the machine type i'm going to keep it as is as it is a windows instance and i'm going to need a little bit more power scrolling down to boot disk we need to change this from debian over to windows so i'm going to simply click on the change button and under operating system i'm going to click on the drop down and select windows server for the version i'm going to select the latest version of windows server which is the windows server 2019 data center and you can keep the boot disk type and the size as its default and simply head on down and click on select and we're going to leave everything else as the default and simply click on create and success our windows instance has been created and so the first thing that you want to do is you want to set a windows password for this instance and so i'm going to head on over to the rdp button and i'm going to click on the dropdown and here i'm going to select set windows password and here i'm going to get a popup to set a new windows password the username has been propagated for me as tony bowties i'm going to leave it as is and i'm going to click on set and i'm going to be prompted with a new windows password that has been set for me so i'm going to copy this and i'm going to paste it into my notepad so be sure to record it somewhere either write it down or copy and paste it into a text editor of your choice i'm going to click on close and so now for me to log into this i need to make sure of a couple things the first thing is i need to make sure that i have a firewall rule open for port 3389 the second is i need to make sure that i have an rdp client and so in order to satisfy my first constraint i'm going to head on over to the navigation menu and go down to vpc network here i'm going to select firewall and as expected the rdp firewall rule has been already created due to the fact that upon creation of the default vpc network this default firewall rule is always created and so now that i've gotten that out of the way i'm going to head back on over to compute engine and what i'm going to do is i'm going to record the external ip so that i'll be able to log into it now i'm going to be logging into this instance from both a windows client and a mac client so starting with windows i'm going to head on over to my windows virtual machine and because i know windows has a default rdp client already built in i'm going to simply bring it up by hitting the windows key and typing remote desktop connection i'm going to click on that i'm going to paste in the public ip for the instance that i just recorded and i'm going to click on connect you should get a popup asking for your credentials i'm going to type in my username as tony bowtie ace as well i'm going to paste in the password and i'm going to click on ok i'm prompted to accept the security certificate and i'm going to select yes and success i'm now connected to my windows server instance and it's going to run all its necessary startup scripts you may get a couple of prompts that come up asking you if you want to connect to your network absolutely i'm going to close down server manager just for now and another thing that i wanted to note is that when you create a windows instance there will automatically be provisioned a google cloud shell with the sdk preinstalled and so you'll be able to run all your regular commands right from this shell without having to install it and this is due to the guest environment that was automatically installed on the vm instance upon creation and this is a perfect example of some of the scripts that are installed with the guest environment i'm going to go ahead and close out of this and i'm going to go ahead and close out of my instance hit ok and so being here in windows i wanted to show you an alternate way of logging into your instance through powershell so for those of you who are quite versed in windows and use powershell in your daytoday there is an easy way to log into your instance using powershell now in order for me to do that i need to open another firewall rule covering tcp port 5986 so i'm going to head on over back to the google cloud console i'm going to head over to the navigation menu and i'm going to scroll down to vpc network i'm going to go into firewall and i'm going to create a new firewall rule and under name i'm going to name this as allow powershell i'm going to use the same for the description i'm going to scroll down to targets and i'm going to select all instances in the network and under source ip ranges for this demonstration i'm going to use 0.0.0.0 forward slash 0. and again this should not be used in a production environment but is used merely for this demo i'm going to leave everything else as is and i'm going to go down to protocols and ports i'm going to click on tcp and i'm going to type in 5986 for the port and i'm going to click on create i'm going to give it a second just to create and it took a couple seconds but our firewall rule is now created and so now i'm gonna head over to my windows vm and i'm gonna open up a powershell command prompt and hit the windows key and type in powershell and so in order for me to not get constantly asked about my username and password i'm going to use a variable that will keep my password for me and so every time i connect to my windows instance i won't need to type it in all the time and so the command for that is dollar sign credentials equals get dash credential i'm going to hit enter and i'm going to get a prompt to type in my username and password so i'm going to simply type that in now along with my password and hit ok and if you don't get a prompt with any errors then chances are that you've been successful at entering your credentials and so now in order to connect to the instance you're going to need the public ip address again so i'm going to head on over back to the console i'm going to head on over to the navigation menu and back to compute engine here i'm going to record the external ip and i'm going to head on over back to my windows virtual machine and so you're going to enter this command which i will include in the lesson text and you'll also be able to find it in the github repository beside computer name you're going to put in your public ip address of your windows instance and make sure at the end you have your credentials variable i'm going to simply click enter and success i'm now connected to my windows instance in google cloud so as you can see here on the left is the public ip of my windows instance and so these are the various ways that you can connect to your windows instance from a windows machine and so now for me to connect to my windows instance on a mac i'm going to head on over there now and like i said before i need to satisfy the constraint of having an rdp client unfortunately mac does not come with an rdp client and so the recommended tool to use is the chrome extension but i personally like microsoft's rdp for mac application and so i'm going to go ahead and do a walkthrough of the installation so i'm going to start off by opening up safari and i'm going to paste in this url which i will include in the lesson text and microsoft has made available a microsoft remote desktop app available in the app store i'm going to go ahead and view it in the app store and i'm going to simply click on get and then install and once you've entered your credentials and you've downloaded and installed it you can simply click on open i'm going to click on not now and continue and i'm going to close all these other windows for better viewing i'm going to click on add pc i'm going to paste in the public ip address of my windows instance and under user account i'm going to add my user account type in my username paste in my password you can add a friendly name here i'm going to type in windows dash gc for google cloud and i'm going to click on add and then once you've pasted in all the credentials and your information you can then click on add and i should be able to connect to my windows instance by double clicking on this window it's asking me for my certificates i'm going to hit continue and success i'm connected to my windows instance and so this is how you would connect to a windows instance from a windows machine as well as from a mac as well there are a couple of other options that i wanted to show you over here on the drop down beside rdp i can download an rdp file which will contain the public ip address of the windows instance along with your username if i need to reset my password i can view the gcloud command to do it or i can set a new windows password if i forgotten my old one and so that's everything i had to show you with regards to connecting to a windows instance and so since this demo was getting kind of long i decided to split it up into two parts and so this is the end of part one of this demo and this would be a great opportunity to get up and have a stretch grab yourself a tea or a coffee and whenever you're ready you can join me in part two where we will be starting immediately from the end of part 1 so you can complete this video and i'll see you in part 2. welcome back this is part 2 of the connecting to your instances demo and we will be starting exactly where we left off in part one so with that being said let's dive in and so now that we've created our windows instance and went through all the methods of how to connect to it let's go ahead and create a linux instance i'm going to go up to the top menu here and click on create instance and i'm going to name this instance linux instance i'm not going to give it any labels under region i'm going to select the us east one region and the zone i'm going to leave it as its set default as us east 1b the machine configuration i'm going to leave it as is under boot disk i'm going to leave this as is with the debian distribution and i'm going to go ahead and click on create okay and our linux instance has been created and in order for me to connect to it i am going to ssh into it but first i need to satisfy the constraint of having a firewall rule with tcp port 22 open so i'm going to head on over to the navigation menu and i'm going to scroll down to vpc network i'm going to head on over to firewall and as expected the allow ssh firewall rule has been created alongside the default vpc network and so since i've satisfied that constraint i can head back on over to compute engine and so here i have a few different options that i can select from for logging into my linux instance i can open in a browser window if i decided i wanted to put it on a custom port i can use this option here if i provided a private ssh key to connect to this linux instance i can use this option here i have the option of viewing the gcloud command in order to connect to it and i've been presented with a popup with the command to use within the gcloud command line in order to connect to my instance i can run it now in cloud shell but i'm going to simply close it and so whether you are on a mac a windows machine or a linux machine you can simply click on ssh and it will open a new browser window connecting you to your instance now when you connect to your linux instance for the first time compute engine generates an ssh key pair for you this key pair by default is added to your project or instance metadata and this will give you the freedom of not having to worry about managing keys now if your account is configured to use os login compute engine stores the generated key pair with your user account now when connecting to your linux instance in most scenarios google recommends using os login this feature lets you use iam roles to manage ssh access to linux instances and this relieves the complexity of having to manage multiple key pairs and is the recommended way to manage many users across multiple instances or projects and so i'm going to go ahead now and show you how to configure os login for your linux instance and the way to do this will be very similar on all platforms so i'm going to go ahead and go back to my mac vm and i'm going to open up my terminal make this bigger for better viewing and i'm going to start by running the gcloud init command in order to make sure i'm using the right user and for the sake of this demonstration i'm going to reinitialize this configuration so i'm going to click on one hit enter number two for tony bowtie ace and i'm going to use project bow tie ink so 1 and i'm not going to configure a default compute region in zone and so if i run the gcloud config list command i can see that the account that i'm using is tony bowties gmail.com in project bowtie inc and so because os login requires a key pair i'm going to have to generate that myself so i'm going to go ahead and clear the screen and i'm going to use the command ssh keygen and this is the command to create a public and private key pair i'm going to use the default path to save my key and i'm going to enter a passphrase i'm going to enter it again and i recommend that you write down your passphrase so that you don't forget it as when you lose it you will be unable to use your key pair and so if i change directory to dot ssh and do an ls for list i can see that i now have my public and private key pair the private key lying in id underscore rsa and the public key lying in id underscore rsa.pub and so another constraint that i have is i need to enable os login for my linux instance so i'm going to go ahead and go back to the console and i'm going to go ahead and go into my linux instance i'm going to click on edit and if you scroll down you will come to some fields marked as custom metadata and under key you will type in enable dash os login and under value you will type in all caps true now i wanted to take a moment here to discuss this feature here under ssh keys for block project wide ssh keys now project wide public ssh keys are meant to give users access to all of the linux instances in a project that allow project projectwide public ssh keys so if an instance blocks projectwide public ssh keys as you see here a user can't use their projectwide public ssh key to connect to the instance unless the same public ssh key is also added to the instance metadata this allows only users whose public ssh key is stored in instance level metadata to access the instance and so this is an important feature to note for the exam and so we're going to leave this feature checked off for now and then you can go to the bottom and click on save now if i wanted to enable os login for all instances in my project i can simply go over to the menu on the left and click on metadata and add the metadata here with the same values so under key i type in enable dash os login and under value i type in in all caps true but i don't want to enable it for all my instances only for that one specific instance so with regards to projectwide public keys these keys can be managed through metadata and should only be used as a last resort if you cannot use the other tools such as ssh from the console or os login these are where the keys are stored and so you can always find them here when looking for them here as you can see there are a couple of keys for tony bowtie ace that i have used for previous instances and so i'm going to go back to metadata just to make sure that my key value pair for os login has not been saved and it is not and i'm going to head back on over to my instances and so now that my constraint has been fulfilled where i've enabled the os login feature by adding the unnecessary metadata i'm going to head on over back to my mac vm i'm going to go ahead and clear the screen so now i'm going to go ahead and log into my instance using os login by using the command gcloud compute os dash login ssh dash keys add and then the flag key dash file and then the path for my public key which is dot ssh forward slash id underscore rsa.pub i'm gonna hit enter and so my key has been successfully stored with my user account i'm gonna go ahead and make this a little bigger for better viewing and so in order to log into my instance i'm going to need my username which is right up here under username i'm going to copy that and i'm just going to clear my screen for a second here for better viewing and so in order for me to ssh into my instance i'm going to type in the command ssh minus i i'm going to have to provide my private key which is in dot ssh forward slash id underscore rsa and then my username that i had recorded earlier at and then i'm going to need my public ip address of my linux instance so i'm going to head back over to the console for just a sec i'm going to copy the ip address head back over to my mac vm paste it in and hit enter it's asking if i want to continue yes i do enter the passphrase for my key and success i am connected and so there is one caveat that i wanted to show you with regards to permissions for os login so i'm going to head back over to the console and i'm going to go up to the navigation menu and head over to i am an admin now as you can see here tony bowties gmail.com has the role of owner and therefore i don't need any granular specific permissions i have the access to do absolutely anything now in case i was a different user and i didn't hold the role of owner i would be looking for specific permissions that would be under compute os login and this would give me permissions as a standard user now if i wanted super user access or root access i would need to be given the compute os admin login role and as you can see it would allow me administrator user privileges so when using os login and the member is not an owner one of these two roles are needed so i'm going to exit out of here i'm going to hit cancel and so that about covers everything that i wanted to show you with regards to all the different methods that you can use for connecting to vm instances for both windows and linux instances now i know this may have been a refresher for some but for others knowing all the different methods of connecting to instances can come in very useful especially when coordinating many instances in bigger environments i want to congratulate you on making it to the end of this demo and gaining a bit more knowledge on this crucial part of managing your instances so before you go be sure to delete any resources that you've created and again congrats on the great job so you can now mark this as complete and i'll see you in the next one welcome back in this demonstration i'll be discussing metadata and how it can pertain to a project as well as an instance as well i'm going to touch on startup and shutdown scripts and it's real world use cases in the last lesson we touched the tip of the iceberg when it came to metadata and wanted to go a bit deeper on this topic as i personally feel that it holds so much value and give you some ideas on how you can use it i'm also going to combine the metadata using variables in a startup script and i'm going to bring to life something that's dynamic in nature so with that being said let's dive in so i am currently logged in as tony at bowtie ace gmail.com under the project of bow tie inc and so in order to get right into the metadata i'm going to head on over to my navigation menu and go straight to compute engine and over here on the left hand menu you will see metadata and you can drill down into there now as i explained in a previous lesson metadata can be assigned to both projects and instances while instance metadata only impacts a specific instance so here i can add and store metadata which will be used on a projectwide basis as well as mentioned earlier metadata is stored in key value pairs and can be added at any time now this is a way to add custom metadata but there is a default set of metadata entries that every instance has access to and again this applies for both project and instance metadata so here i have the option of setting my custom metadata for the entire project and so i'm going to dive into where to store custom metadata on an instance and so in order for me to show you this i'm going to first head over to vm instances and create my instance and so just as a note before creating your instance make sure that you have the default vpc created and so because i like to double check things i'm going to head over to the navigation menu i'm going to scroll down to vpc network and as expected i have the default vpc already created and so this means i can go ahead and create my instance so i'm going to head back on over to compute engine and i'm going to create my instance and i'm going to name this instance bowtie dash web server i'm not going to add any labels and under the region i'm going to select us east one and you can keep the zone as the default as us east 1b under machine type i want to keep things cost effective so i'm going to select the e2 micro i'm going to scroll down and under identity and api access i want to set access for each api and scroll down to compute engine i want to select it and i want to select on read write and i'm going to leave the rest as is and scrolling down to the bottom i want to click on management security disks networking and sold tenancy and under here you will find the option to add any custom metadata and you can provide it right here under metadata as a key value pair but we're not going to add any metadata right now so i'm just going to scroll down to the bottom i'm going to leave everything else as is and simply click on create and it should take a few moments for my instance to be created okay and now that my instance is up i want to go ahead and start querying the metadata now just as a note metadata must be queried from the instance itself and can't be done from another instance or even from the cloud sdk on your computer so i'm going to go ahead and log into the instance using ssh okay and now that i'm logged into my instance i want to start querying the metadata now normally you would use tools like wget or curl to make these queries in this demo i will use curl and for those who don't know curl is a command line tool to transfer data to or from a server using supported protocols like http ftp scp and many more this tool is fantastic for automation since it's designed to work without any user interaction and so i'm going to paste in the url that i am going to use to query the instance metadata and this is the default url that you would use to query any metadata on any instance getting a little deeper into it a trailing slash shown here shows that the instance value is actually a directory and will have other values that append to this url whether they are other directories or just endpoint values now when you query for metadata you must provide the following header in all of your requests metadata dash flavor colon google and should be put in quotations if you don't provide this header the metadata server will deny your request so i'm going to go ahead and hit enter and as you can see i've been brought up a lot of different values that i can choose from in order to retrieve different types of metadata and as stated before anything with a trailing slash is actually a directory and will have other values underneath it so if i wanted to query the network interfaces and because it's a directory i need to make sure that i add the trailing slash at the end and as you can see here i have the network interface of 0 and i'm going to go ahead and query that and here i will have access to all the information about the network interface on this instance so i'm going to go ahead and query the network on this interface and as expected the default network is displayed i'm going to quickly go ahead and clear my screen and i'm going to go ahead and query some more metadata this time i'm going to do the name of the server and as expected bowtie dash web server showed up and because it's an endpoint i don't need the trailing slash at the end i'm going to go ahead and do one more this time i'm going to choose machine type and again as expected the e2 micro machine type is displayed and so just as a note for those who haven't noticed any time that you query metadata it will show up to the left of your command prompt now what i've shown you here is what you can do with instance metadata and so how about if you wanted to query any project metadata well instead of instance at the end you would use project with the trailing slash i'm going to simply click on enter and as you can see here project doesn't give me a whole lot of options but it does give me some important values like project id so i'm going to simply query that right now and as expected bowtie inc is displayed and so this is a great example of how to query any default metadata for instances and for projects now you're probably wondering how do i query my custom metadata well once custom metadata has been set you can then query it from the attributes directory in the attributes directory can be found in both the instance and project metadata so i'm going to go ahead and show you that now but first i wanted to add some custom metadata and this can be set in either the console the gcloud command line tool or using the api and so i'm going to run the command here gcloud compute instances add dash metadata the name of your instance and when you're adding custom metadata you would add the flag dash dash metadata with the key value pair which in this example is environment equals dev and then i'm also going to add the zone of the instance which is us east 1a and i'm going to hit enter and because i had a typo there i'm going to go ahead and try that again using us east 1b i'm going to hit on enter and success and so to verify that this command has worked i'm going to go ahead and query the instance and i'm going to go under attributes i'm going to hit on enter and as you can see here the environment endpoint has been populated so i'm going to query that and as expected dev is displaying as the environment value now if i wanted to double check that in the console i can go over to the console i can drill down into bowtie web server and if i scroll down to the bottom under custom metadata you can see the key value pair here has m as the key and dev being the value and so these are the many different ways that you can query metadata for any instances or projects now i wanted to take a quick moment to switch gears and talk about startup and shutdown scripts now compute engine lets you create and run your own startup and shutdown scripts on your vm instance and this allows you to perform automation that can perform actions when starting up such as installing software performing updates or any other tasks that are defined in the script and when shutting down you can allow instances time to clean up on perform tasks such as exporting logs to cloud storage or bigquery or syncing with other systems and so i wanted to go ahead and show you how this would work while combining metadata into the script so i'm going to go ahead and drill down into bow tie web server i'm going to click on edit and i'm going to scroll down here to custom metadata i'm going to click on add item and under key i'm going to type in startup dash script and under value i'm going to paste in my script i'm going to just enlarge this here for a second and i will be providing the script in the github repository now just to break it down this is a bash script i'm pulling in a variable called name which will query the instance name as well i have a variable called zone which will query the instance zone i'm going to be installing an apache web server and it's going to display on a web browser both the server name and the zone that it's in and so in order for me to see this web page i also need to open up some firewall rules and so an easy way to do this would be to scroll up to firewalls and simply click on allow http and allow https traffic this will tag the instance with some network tags as http server and https server and create two separate firewall rules that will allow traffic for port 80 and port 443 so i'm going to leave everything else as is i'm going to scroll down to the bottom and click on save okay and it took a few seconds there but it did finish saving i'm going to go ahead and go up to the top and click on reset and this will perform a hard reset on the instance and will allow the startup script to take effect so i'm going to click on reset it's going to ask me if i really want to do this and for the purposes of this demonstration i'm going to click on reset please note you should never do this in production as it doesn't do a clean shutdown on the operating system but as this is an instance with nothing on it i'm going to simply click on reset now i'm going to head on back to the main console for my vm instances and i'm going to record my external ip i'm going to open up a new browser i'm going to zoom in for better viewing and i'm going to paste in my ip address and hit enter and as you can see here i've used my startup script to display not only this web page but i was able to bring in metadata that i pulled using variables and was able to display it here in the browser and so before i end this demonstration i wanted to show you another way of using a startup script but being able to pull it in from cloud storage so i'm going to go back to the navigation menu and i'm going to scroll down to storage here i will create a new bucket and for now find a globally unique name to name your bucket and i'm going to call my bucket bowtie web server site and i'm going to leave the rest as its default and i'm going to simply click on create and if you have a globally unique name for your bucket you will be prompted with this page without any errors and i'm going to go ahead and upload the script and you can find this script in the github repository so i'm going to go into my repo and i'm going to look for bow tie start up final sh i'm going to open it and now that i have the script uploaded i'm going to drill into this file so i can get some more information that i need for the instance and what i need from here is to copy the uri so i'm going to copy this to my clipboard and i'm going to head back on over to compute engine i'm going to drill down into my instance i'm going to click on edit at the top and i'm going to scroll down to where it says custom metadata and here i'm going to remove the startup script metadata and i'm going to add a new item and i'm going to be adding startup dash script dash url and in the value i'm going to paste in the uri that i had just copied over and this way on startup my instance will use this startup script that's in cloud storage so i'm going to scroll down to the bottom click on save and now i'm going to click on reset i'm going to reset here i'm going to go back to the main page for my vm instances and i can see that my external ip hasn't changed so i'm going to go back to my open web browser and i'm going to click on refresh and success and as you can see here i've taken a whole bunch of different variables including the machine name the environment variable the zone as well as the project and i've displayed it here in a simple website and although you may not find this website specifically useful in your production environment this is just an idea to get creative using default and custom metadata along with a startup script i've seen in some environments where people have multiple web servers and create a web page to display all the specific web servers in their different environments along with their ips their data and their configurations and so just as a recap we've gone through the default and custom metadata and how to query it in an instance we also went through startup scripts and how to apply them both locally and using cloud storage and so i hope you have enjoyed having fun with metadata and using them in startup scripts such as this one i also hope you find some fascinating use cases in your current environments and so before you go just a quick reminder to delete any resources that you've created to not incur any added costs and so that's pretty much all i wanted to cover with this demonstration so you can now mark this as complete and let's move on to the next one welcome back and in this lesson i'm going to be discussing compute engine billing now when it comes to pricing with regards to compute engine i've only gone over the fact that instances are charged by the second after the first minute but i never got into the depths of billing and the various ways to save money when using compute engine in this lesson i will be unveiling how both costs and discounts are broken down in google cloud as it refers to the resource based billing model and the various savings that can be had when using compute engine so with that being said let's dive in now each vcpu and each gigabyte of memory on compute engine is built separately rather than as part of a single machine type you are still creating instances using predefined machine types but your bill shows them as individual cpus and memory used per hour and this is what google refers to as resourcebased billing which i will get into in just a bit the billing model applies to all vcpus gpus and memory resources and are charged a minimum of one minute for example if you run your virtual machine for 30 seconds you will be billed for one minute of usage after one minute instances are charged in one second increments instance up time is another determining factor for cost and is measured as the number of seconds between when you start an instance and when you stop an instance in other words when your instance is in the terminated state if an instance is idle but still has a state of running it will be charged for instance uptime but again you will not be charged if your instance is in a terminated state now getting into reservations these are designed to reserve the vm instances you need so after you create a reservation the reservation ensures that those resources are always available for you to use during the creation process you can choose how a reservation is to be used for example you can choose for a reservation to be automatically applied to any new or existing instances that match the reservation's properties which is the default behavior or you can specify that reservation to be consumed by a specific instance in all cases a vm instance can only use a reservation if its properties exactly match the properties of the reservation after you create a reservation you begin paying for the reserved resources immediately and they remain available for your project to use indefinitely until the reservation is deleted reservations are great to ensure that your project has resources for future increases in demand including planned or unplanned spikes backup and disaster recovery or for a buffer when you're planning growth when you no longer need a reservation you can simply delete the reservation to stop incurring charges each reservation like normal vms are charged based on existing ondemand rates which include sustained use discounts and are eligible for committed use discounts which i will be getting into in just a bit now purchasing reservations do come with some caveats reservations apply only to compute engine data proc and google kubernetes engine as well reservations don't apply to shared core machine types preemptable vms sole tenant nodes cloud sql and data flow now as i explained before each vcpu and each gigabyte of memory on compute engine is built separately rather than as a part of a single machine type and is billed as individual cpus and memory used per hour resourcebased pricing allows compute engine to apply sustained use discounts to all of your predefined machine type usage in a region collectively rather than to individual machine types and this way vcpu and memory usage for each machine type can receive any one of the following discounts sustained use discounts committed use discounts and preemptable vms and i'd like to take a moment to dive into a bit of detail on each of these discount types starting with sustained use discounts now sustained use discounts are automatic discounts for running specific compute engine resources a significant portion of the billing month for example when you run one of these resources for more than 25 percent of a month compute engine automatically gives you a discount for every incremental minute that you use for that instance now the following tables show the discounts applied for the specific resources described here now for the table on the left for general purpose n2 and n2d predefined and custom machine types and for compute optimized machine types you can receive a discount of up to 20 percent the table on the right shows that for general purpose n1 predefined and custom machine types as well as sole tenant nodes and gpus you can get a discount of up to 30 percent sustained use discounts are applied automatically to usage within a project separately for each region so there is no action required on your part to enable these discounts now some notes that i wanted to cover here is that sustained use discounts automatically apply to vms created by both google kubernetes engine and compute engine as well they do not apply to vms created using the app engine flexible environment as well as data flow and the e2 machine types sustained use discounts are applied on incremental use after you reach certain usage thresholds this means that you pay only for the number of minutes that you use an instance and compute engine automatically gives you the best price google truly believes that there's no reason to run an instance for longer than you need it now sustained use discounts are applied on incremental use after you reach certain usage thresholds this means that you pay only for the number of minutes that you use an instance and compute engine automatically gives you the best price now consider a scenario where you have two instances or sole tenant nodes in the same region that have different machine types and run at different times of the month compute engine breaks down the number of vcpus and amount of memory used across all instances that use predefined machine types and combines the resources to qualify for the largest sustained usage discounts possible now in this example assume you run the following two instances in the us east one region during a month for the first half you run an n1 standard four instance with four vcpus and 15 gigabytes of memory for the second half of the month you run a larger and one standard 16 instance with 16 vcpus and 60 gigabytes of memory in this scenario compute engine reorganizes these machine types into individual vcpu and memory resources and combines their usage to create the following resources for vcpus so because four vcpus were being used for the whole month the discount here would be thirty percent the additional twelve vcpus were added on week two in the month and so for those 12 vcpus they would receive a 10 discount and this is how discounts are applied when it comes to sustained use discounts now moving on to the next discount type is committed use discounts so compute engine lets you purchase committed use contracts in return for deeply discounted prices for vm usage so when you purchase a committed use contract you purchase compute resource which is comprised of vcpus memory gpus and local ssds and you purchase these resources at a discounted price in return for committing to paying for those resources for one year or three years committed use discounts are ideal for workloads with predictable resource needs so if you know exactly what you're going to use committed use discounts would be a great option for this and the discount is up to 57 for most resources like machine types or gpus when it comes to memory optimized machine types the discount is up to 70 percent now when you purchase a committed use contract you can purchase it for a single project and applies to a single project by default or you can purchase multiple contracts which you can share across many projects by enabling shared discounts once purchased your billed monthly for the resources you purchased for the duration of the term you selected whether you use the services or not if you have multiple projects that share the same cloud billing account you can enable committed use discount sharing so that all of your projects within that cloud billing account share all of your committed use discount contracts your sustained use discounts are also pooled at the same time now some caveats when it comes to committed use discounts shared core machines are excluded on this as well you can purchase commitments only on a per region basis if a reservation is attached to a committed use discount the reservation can't be deleted for the duration of the commitment so please be aware now to purchase a commitment for gpus or local ssds you must purchase a general purpose and one commitment and lastly after you create a commitment you cannot cancel it you must pay the agreed upon monthly amount for the duration of the commitment now committed use discount recommendations give you opportunities to optimize your compute costs by analyzing your vm spending trends with and without a committed use discount contract by comparing these numbers you can see how much you can save each month with a committed use contract and this can be found under the recommendations tab on the home page in the console and so i wanted to move on to the last discount type which are preemptable vms now preemptable vms are up to eighty percent cheaper than regular instances pricing is fixed and you never have to worry about variable pricing these prices can be found on the link to instance pricing that i have included in the lesson text a preemptable vm is an instance that you can create and run at a much lower price than normal instances however compute engine might stop or preempt these instances if it requires access to those resources for other tasks as preemptable instances our access compute engine capacity so their availability varies with usage now generally compute engine avoids preempting instances but compute engine does not use an instant cpu usage or other behavior to determine whether or not to preempt it now a crucial characteristic to know about preemptable vms is that compute engine always stops them after they run for 24 hours and this is something to be aware of for the exam preemptable instances are finite compute engine resources so they might not always be available and if you happen to accidentally spin up a preemptable vm and you want to shut it down there is no charge if it's running for less than 10 minutes now another thing to note is that preemptable instances can't live migrate to a regular vm instance or be set to automatically restart when there is a maintenance event due to the limitations preemptable instances are not covered by any service level agreement and when it comes to the google cloud free tier credits for compute engine this does not apply to preemptable instances so you're probably asking when is a great time to use preemptable vms well if your apps are fault tolerant and can withstand possible instance preemptions then preemptable instances can reduce your compute engine costs significantly for example batch processing jobs can run on preemptable instances if some of those instances stop during processing the job slows down but does not completely stop preemptable instances create your batch processing tasks without placing any additional workload on your existing instances and without requiring for you to pay full price for additional normal instances and since containers are naturally stateless and fault tolerant this makes containers an amazing fit for preemptable vms so running preemptable vms for google kubernetes engine is another fantastic use case now it's really critical that you have an understanding for each different discount type and when is a good time to use each as you may be presented different costeffective solutions in the exam and understanding these discount types will prepare you to answer them understanding the theory behind this resourcebased pricing model all the available discount types along with the types of workloads that are good for each will guarantee that you will become familiar with what types of questions are being asked in the exam and will also make you a better cloud engineer as you will be able to spot where you can save money and be able to make the appropriate changes and so that's pretty much all i wanted to cover when it comes to compute engine billing and its discount types so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to be covering the fundamentals as it pertains to storage these concepts are needed to know in order to fully understand the different google cloud storage options that i will be diving into later as well the exam expects that you know the different types of storage that's available for all the various services and so before i get into the different types of storage i wanted to cover the underlying theory behind it so with that being said let's dive in so i wanted to start off by going through the three types of storage and how data is presented to a user or to the server there is block storage file storage and object storage these types of storage tie into the available services that are available in google cloud and they offer different options for different types of workloads and i will be going over each of these in a bit of depth and so the first one i wanted to touch on is block storage now block storage is sometimes referred to as block level storage and is a technology that is used to store data files on storage systems or cloudbased storage environments block storage is the fastest available storage type and it is also efficient and reliable with block storage files are split into evenly sized blocks of data each with its own unique identifier it is presented to the operating system as structureless raw data in the form of a logical volume or a hard drive and the operating system structures it with a file system like ext3 or ext4 on linux and ntfs for windows it would then mount this volume or drive as the root volume in linux or a c or d drive in windows block storage is usually delivered on physical media in the case of google cloud it is delivered as either spinning hard drives or solid state drives so in google cloud you're presented with block storage that consists of either persistent disks or local ssd which can both be mountable and bootable block storage volumes can then be used as your boot volumes for compute instances in google cloud installed with your operating system of choice and structured so that your operating system database or application will then be able to consume it now moving on to the second type of storage is file storage now file storage is also referred to as file level or file based storage and is normally storage that is presented to users and applications as a traditional network file system in other words the user or application receives data through directory trees folders and files file storage also allows you to do the same this functions similarly to a local hard drive however a structure has already been applied and cannot be adjusted after the fact this type of structure only has the capabilities of being mountable but not bootable you cannot install an operating system on file storage as i said before the structure has already been put in place for you and is ready for you or your application to consume due to this structure the service that is serving the file system has some underlying software that can handle access rights file sharing file locking and other controls related to file storage in google cloud this service that serves this type of storage is known as cloud file store and is usually presented over the network to users in your vpc network using the nfs protocol or in this case nfs version 3. but i'll be diving into that a little bit later and the last storage type that i wanted to cover is object storage now object storage also referred to as objectbased storage is a general term that refers to the way in which we organize and work with units of storage called objects and this is a storage type that is a flat collection of unstructured data and this type of storage holds no structure like the other two types of storage and is made up of three characteristics the first one is the data itself and this could be anything from movies songs and even photos of men in fancy bow ties the data could also be binary data as well the second characteristic is the metadata and this is usually related to any contextual information about what the data is or anything that is relevant to the data and the third characteristic is a globally unique identifier and this way it's possible to find the data without having to know the physical location of the data and this is what allows object storage to be infinitely scalable as it doesn't matter where the object is stored this type of storage can be found in google cloud and is known as cloud storage cloud storage is flat storage with a logical container called a bucket that you put objects into now although this type of storage is not bootable using an open source tool called fuse this storage type can be mounted in google cloud and i will be covering that a little bit later in the cloud storage lesson but in most cases object store is designed as the type of storage that is not bootable or mountable and because of the characteristics of this storage it allows object storage again to be infinitely scalable and so these are the three main types of storage that you will need to know and understand as each has its use cases so if you're looking for high performance storage you will always look to block storage to satisfy your needs if you're looking to share files across multiple systems or have multiple applications that need access to the same files and directories then file storage might be your best bet if you're looking to store terabytes of pictures for a web application and you don't want to worry about scaling object storage will allow you to read and write an infinite amount of pictures that will meet your requirements so now that we've covered these storage types let's take a few moments to discuss storage performance terms now when discussing storage performance there are some key terms to understand that when used together define the performance of your storage first there is io which stands for input output and is a single read write request and can be measured in block size and this block size can vary anywhere from one kilobyte to four megabytes and beyond depending on your workload now q depth when it comes to storage is the number of pending input output requests waiting to be performed on a disk io requests become queued when reads or writes are requested faster than they can be processed by the disk when io requests are queued the total amount of time it takes to read or write data to disk becomes significantly higher this is where performance degradation can occur and queue depth must be adjusted accordingly now the next term is a common touch point when it comes to discussing storage performance on gcp and on the exam which is iops and this is a metric that stands for input output operations per second this value indicates how many different input or output operations a device or group of devices can perform in one second more value in the iops signifies the capability of executing more operations per second and again this is a common touch point that i will be diving into a little bit later now next up is throughput and this is the speed at which the data is transferred in a second and is most commonly measured in megabytes per second this is going to be another common topic that comes up frequently when discussing storage on gcp as well latency is the measurement of delay between the time data is requested when the data starts being returned and is measured in milliseconds so the time each io request will take to complete results in being your average latency and the last two terms i wanted to bring up is sequential and random access sequential would be a large single file like a video and random access would be loading an application or an operating system so lots of little files that are all over the place it's obvious that accessing data randomly is much slower and less efficient than accessing it sequentially and this can also affect performance now why i bring up all these terms is not about calculating the average throughput but to give you a holistic view on storage performance as all these characteristics play a part in defining the performance of your storage there is not one specific characteristic that is responsible for disk performance but all have a role in achieving the highest performance possible for your selected storage now i know this is a lot of theory to take in but this will all start to make more sense when we dive into other parts of the course where we will discuss disk performance with all these characteristics as it relates to compute engine and other services that use storage it is crucial to know the storage types as well as the performance characteristics as it will bring clarity to questions in the exam and also give you a better sense on how to increase your storage performance in your work environment and so that's pretty much all i wanted to cover when it comes to storage types and storage performance as it pertains to storage as a whole so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i'm going to be covering persistent disks and local ssds i'm going to be getting into the detail with the most commonly used storage types for instances which are both persistent disks and local ssds this lesson will sift through all the different types of persistent disks and local ssds along with the performance of each knowing what type of disk to use for your instance and how to increase disk performance shows up on the exam and so i want to make sure to cover it in detail and leave no stone unturned so with that being said let's dive in now persistent disks and local ssds are the two available types of block storage devices available in google cloud and the determining factor of what you will use for your particular scenario will depend on your use case and the specific characteristics that you require from each storage medium now by default each compute engine instance has a single boot persistent disk that contains the operating system when you require additional storage space you can add one or more additional persistent disks or local ssds to your instance and i will be going through these storage options along with their characteristics now as you can see here persistent disks and local ssds come in a slew of different types as well with persistent disks they are available in both zonal and regional options so starting off with persistent disks you have three different types you can choose from as well you have the flexibility of choosing from two different geographic options when it comes to the redundancy of your persistent disks and i will be covering the zonal and regional options in detail in just a bit now persistent disks are durable network storage devices that your instances can access like physical disks in a computer so these are not physically attached disks but network disks that are connected over google's internal network persistent disks are independent of your instance and can persist after your instance has been terminated and this can be done by turning on this flag upon creation you can even detach your disk and move it to other instances when you need to scaling persistent disks can be done automatically and on the fly by using the disk resize feature and this gives you the flexibility to resize your current persistent disks with no downtime and even add additional disks to your instance for additional performance and storage persistent disks are also encrypted by default and google also gives you the option of using your own custom keys each persistent disk can be up to 64 terabytes in size and most instances can have up to 128 persistent disks and up to 257 terabytes of total persistent disk space attached and just as a note share core machine types are limited to 16 persistent disks and 3 terabytes of total persistent disk space and so now that i've gone through the details of persistent disks i wanted to dive into the two geographic options that's available for persistent disks first starting with zonal now zonal persistent disks are disks that are available in one zone in one region these disks are the most commonly used persistent disks for general daytoday usage and used for those whose workloads are not sensitive to specific zone outages they are redundant within the zone you've created them in but cannot survive an outage of that zone and may be subjected to data loss if that specific zone is affected and this is where snapshots should be a part of your high availability strategy when using zonal persistent disks snapshots are incremental and can be taken even if you snapshot disks that are attached to running instances and i'll be going into detail about snapshots in a later lesson zonal persistent disks can also be used with any machine type including predefined shared core and custom machine types now when it comes to regional persistent disks they have storage qualities that are similar to zonal persistent disks however regional persistent disks provide durable storage and replication of data between two zones in the same region if you are designing systems that require high availability on compute engine you should use regional persistent disks combined with snapshots for durability regional persistent disks are also designed to work with regional managed instance groups in the unlikely event of a zonal outage you can usually fail over your workload running on regional persistent disks to another zone by simply using the force attached flag regional persistent disks are slower than zonal persistent disks and should be taken into consideration when write performance is less critical than data redundancy across multiple zones now noting a couple of caveats here when it comes to disk limits regional persistent disks are similar to zonal persistent disks however regional standard persistent disks have a 200 gigabyte size minimum and may be a major factor when it comes to cost so please be aware as well you can't use regional persistent disks with memory optimized machine types or compute optimized machine types now these two geographic options are available for all three persistent disk types whose characteristics i will dive into now starting off with the standard persistent disk type also known in google cloud as pd standard now these persistent disks are backed by standard hard disk drives and these are your standard spinning hard disk drives and allows google cloud to give a cost effective solution for your specific needs standard persistent disks are great for large data processing workloads that primarily use sequential ios now as explained earlier sequential access would be accessing larger files and would require less work by the hard drive thus decreasing latency as there are physical moving parts in this hard drive this would allow the disc to do the least amount of work as possible and therefore making it the most efficient as possible and therefore sequential ios are best suited for this type of persistent disk and again this is the lowest price persistent disks out of all the persistent disk types now stepping into the performance of standard persistent disks for just a second please remember that iops and throughput performance depends on disk size instance vcpu count and i o block size among other factors and so this table here along with the subsequent tables you will see later are average speeds that google has deemed optimum for these specific disk types they cover the maximum sustained iops as well as the maximum sustained throughput along with the granular breakdown of each here you can see the differences between both the zonal and regional standard pd and as you can see here in the table the zonal standard pd and the regional standard pd are pretty much the same when it comes to most of these metrics but when you look closely at the read iops per instance this is where they differ where the zonal standard pd has a higher read iops per instance than the regional standard pd and this is because the regional standard pd is accessing two different disks in two separate zones and so the latency will be higher the same thing goes for right throughput per instance and so this would be a decision between high availability versus speed moving on to the next type of persistent disk is the balanced persistent disk in google cloud known as pd balance this disk type is the alternative to the ssd persistent disks that balance both performance and cost as this disk type has the same maximum iops as the ssd persistent disk type but holds a lower iops per gigabyte and so this disk is designed for general purpose use the price for this disk also falls in between the standard and the ssd persistent disks so this is basically your middle of the road disk when you're trying to decide between price and speed moving straight into performance i put the standard pd metric here so that you can see a sidebyside comparison between the balance pd and the standard pd and as you can see here when it comes to the metrics under the maximum sustained iops the balance pd is significantly higher than the standard pd in both the zonal and regional options as well looking at the maximum sustained throughput the read write throughput per gigabyte is a little over two times faster and the right throughput per instance is three times faster so quite a bit of jump from the standard pd to the balance pd and moving on to the last persistent disk type is the ssd persistent disk type also known in google cloud as a pd ssd and these are the fastest persistent disks that are available and are great for enterprise applications and high performance databases that demand lower latency and more iops so this would be great for transactional databases or applications that require demanding and near realtime performance the pd ssds have a single digit millisecond latency and because of this comes at a higher cost and therefore is the highest price persistent disk moving on to the performance of this persistent disk this disk type is five times faster when it comes to read iops per gigabyte than the balance pd as well as five times faster for the right iops per gigabyte and so the table here on the left shows the performance for the pd ssd and the table on the right shows the performance of both the standard pd and the balance pd and so here you can see the difference moving from the standard pd over to the ssd pd the read write throughput per instance stays the same from the standard pd all the way up to the ssd pd but where the ssd outperforms all the other ones is through the read write throughput per gigabyte it's one and a half times faster than the balance pd and four times faster than the standard pd and again you will also notice a drop in performance from the zonal option to the regional option and so this is the end of part one of this lesson as it started to get a little bit long and so whenever you're ready you can join me in part two where i will be starting immediately from the end of part one so you can complete this video and i will see you in the next welcome back this is part two of the persistent disks and local ssds lesson and we will be starting exactly where we left off in part one so with that being said let's dive in and so now that i've covered all the persistent disk types i wanted to move into discussing the characteristics of the local ssd local ssds are physically attached to the server that hosts your vm instance local ssds have higher throughput and lower latency than any of the available persistent disk options and again this is because it's physically attached and the data doesn't have to travel over the network now the crucial thing to know about local ssds is that the data you store on a local ssd persists only until the instance is stopped or deleted once the instance is stopped or deleted your data will be gone and there is no chance of getting it back now each local ssd is 375 gigabytes in size but you can attach a maximum of 24 local ssd partitions for a total of 9 terabytes per instance local ssds are designed to offer very high iops and very low latency and this is great for when you need a fast scratch disk or a cache and you don't want to use instance memory local ssds are also available in two flavors scuzzy and mvme now for those of you who are unaware scuzzy is an older protocol and made specifically for hard drives it also holds the limitation of having one queue for commands nvme on the other hand also known as nonvolatile memory express is a newer protocol and is designed for the specific use of flash memory and designed to have up to 64 000 qs as well each of those queues in turn can have up to 64 000 commands running at the same time and thus making nvme infinitely faster now although nvme comes with these incredible speeds it does come at a cost and so when it comes to the caveats of local ssd although compute engine automatically encrypts your data when it's written to local ssd storage space you can't use customer supplied encryption keys with local ssds as well local ssds are only available for the n1 n2 and compute optimized machine types now moving on to the performance of local ssds throughput is the same between scuzzy and nvme but the read write iops per instance is where nvme comes out on top and as you can see here the read iops per instance is a whopping two million four hundred thousand read iops per instance as well the right iops per instance is 1.2 million over the 800 000 for local ssd now before i end this lesson i wanted to cover a few points on performance scaling as it pertains to block storage on compute engine now persistent disk performance scales with the size of the disk and with the number of vcpus on your vm instance persistent disk performance scales linearly until it reaches either the limits of the volume or the limits of each compute engine instance whichever is lower now this may seem odd that the performance of your disk scales with cpu count but you have to remember persistent disks aren't physically attached to your vm they are independently located as such i o on a pd is a network operation and thus it takes cpu to do i o which means that smaller instances run out of cpu to perform disk io at higher rates so in order for you to get better performance you can increase the iops for your disk by resizing them to their maximum capacity but once that size has been reached you will have to increase the number of cpus on your instance in order to increase your disk performance a recommendation by google is that you have one available vcpu for every 2000 to iops of expected traffic so to sum it up performance scales until it reaches either the limits of the disk or the limits of the vm instance to which the disk is attached the vm instance limits are determined by the machine type and the number of vcpus of the instance now if you want to get more granular with regards to disk performance i've included a few links in the lesson text that will give you some more insight but for most general purposes and for the exam remember that persistent disk performance is based on the total persistent disk capacity attached to an instance and the number of vcpus that the instance has and so that's pretty much all i wanted to cover when it comes to persistent disks and local ssds so you can now mark this lesson as complete and let's move on to the next one welcome back in this demo i'm going to be covering how to manage and interact with your disks on compute engine this demo is designed to give you both experience and understanding on working with persistent disks and how you would interact with them we're going to start the demo off by creating an instance we're then going to create a separate persistent disk and attach it to the instance we're going to then interact with the disk and then resize the disk while afterwards we will delete it and we're going to do this all by both using the console and the command line so with that being said let's dive in so here i am in the console i'm logged in as tony bowties gmail.com and i am in project bowtie inc and so the first thing we need to do to kick off this demo is to create an instance that we can attach our disk to but first i always like to make sure that i have a vpc to deploy my instance into with its corresponding default firewall rules so i'm going to head on over to the navigation menu and i'm going to go down to vpc network and as expected my default vpc has been created and just to make sure that i have all my necessary firewall rules i'm going to drill down into the vpc and head on over to firewall rules i'm going to click on firewall rules and the necessary firewall rule that i need for ssh is created and so i can go ahead and create my instance so i'm going to go back up to the navigation menu and i'm going to go over to compute engine so i'm going to go ahead and click on create and i'm going to name this instance bowtie dash instance and for the sake of this demo i'll add in a label here the key is going to be environment and the value will be testing i'm going to go down to the bottom click on save with regards to the region i'm going to select us east 1 and i'm going to keep the zone as the default for us east 1b and under machine type to keep things cost effective i'm going to use an e2 micro shared core machine and i'm going to scroll down to service account and under service account you want to select the set access for each api you want to scroll down to compute engine and here you want to select read write and this will give us the necessary permissions in order to interact with our disk that we will be creating later so i'm going to scroll down to the bottom here and i'm going to leave everything else set at its default and just before creating the instance please do remember you can always click on the command line link where you can get the gcloud command to create this instance through the command line i'm going to close this up and i'm going to simply click on create i'm just going to wait a few seconds here for my instance to come up okay and my instance is up and so now what we want to do is we want to create our new disk so i'm going to go over here to the left hand menu and i'm going to click on disks and as you can see here the disk for the instance that i had just created has 10 gigabytes in us east 1b and we want to leave that alone and we want to create our new disk so i'm going to go up to the top here and simply click on create disk and so for the name of the disk i'm going to call this disk new pd for persistent disk and i'm going to give it the same description i'm going to keep the type as standard persistent disk and for the region i want to select us east one i'm going to keep the zone as its default in us east 1b and as the disk is in us east 1b i'll be able to attach it to my instance and so just as a note here there is a selection where you can replicate this disk within the region if i click that off i've now changed this from a zonal persistent disk to a regional persistent disk and over here in zones it'll give me the option to select any two zones that i prefer and so if you're looking at creating some regional persistent disks these are the steps you would need to take in order to get it done in the console now in order to save on costs i'm going to keep this as a zonal persistent disk so i'm going to click on cancel i'm going to uncheck the option and make sure your region is still set at us east 1 and your zone is selected as us east 1b we're going to leave the snapshot schedule alone and i'll be diving into snapshot schedules in a later lesson i'm going to scroll down here to source type i'm going to keep it as blank disk and the size here is set at 500 gigabytes and we want to set it to 100 gigabytes but before we do that i wanted to bring your attention to the estimated performance here you can see the sustain random iops limits as well as the throughput limit and so depending on the size of the disk that you want to add these limits will change accordingly so if i change this to 100 my sustained random iops limit on read went from 375 iops to 75 iops and so this is a great demonstration that the larger your disc the better your performance and so this is a great way to figure out on what your performance will be before you create your disk and i've also been prompted with a note here saying that because my disk is under 200 gigabytes that i will have reduced performance and so for this demo that's okay i'm going to keep my encryption as the google manage key and under labels i will add environment as the key and value is testing and so now that i've entered all my options i'm going to simply click on create and i'm going to give it a few seconds and my new disk should be created okay and my new disk has been created and you can easily create this disk through the command line and i will be supplying that in the lesson text i merely want to go through the console setup so that you are aware of all the different options and so now that i've created my disk and i've created my instance i want to now log into my instance and attach this new disk so i'm going to go back to vm instances and here i want to ssh into the bowtie instance and i'm going to give it a few seconds here to connect and i'm going to zoom in for better viewing i'm going to clear my screen and so the first thing i want to do is i want to list all my block devices that are available to me on this instance and the linux command for that is ls blk and as you can see my boot disk has been mounted and is available to me and so now i want to attach the new disk that we just created and just as a note i could as easily have done this in the console but i wanted to give you an idea of what it would look like doing it from the command line and so i'm going to paste in the command to attach the disk which is gcloud compute instances attach dash disk the name of the instance which is bow tie dash instance along with the flag dash dash disk the disk name which is new pd and the zone of the disk using the zone flag with us east 1b so i'm going to go ahead and hit enter and no errors came up so i'm assuming that this had worked and so just to double check i'm gonna run the lsblk command again and success as you can see here my block device sdb has been attached to my instance and is available to me with the size of 100 gigabytes and so now i want to look at the state that this roblox device is in and so the command for that will be sudo file dash s followed by the path of the block device which is forward slash dev forward slash sdb i'm going to hit on enter and as you can see it is showing data which means that it is just a raw data device and so in order for me to interact with it i need to format the drive with a file system that the operating system will be able to interact with and so the command to format the drive would be sudo mkfs which is make file system i'm going to use ext4 as the file system minus capital f along with the path of the new disk so i'm going to hit on enter and no errors so i'm assuming that it was successful so just to verify i'm going to run the sudo file minus s command and as you can see here because the disk now has a file system i've been given the information with regards to this disk whereas before it was simply raw data and so now that we've created our disk and we've formatted our disk to a file system that the operating system is able to read we need to now mount the disk and so in order to do that we need to create a mount point so i'm going to first clear the screen and i'm going to run the command sudo mkdir and the new mount point i'm going to call it slash new pd i'm going to hit enter and now i'm going to mount the disk and the command for that is sudo mount the path for the block device which is forward slash dev forward slash sdb and then the mount point which is forward slash new pd i'm going to hit enter no errors so i'm assuming that it had worked but just to verify i'm going to run the command lsblk and success as you can see sdb has now been mounted as new pd and so now i can interact with this disk so the first thing i want to do is i want to change directories to this mount point i'm in now new pd i'm going to do an ls and so just as a note for those of you who are wondering the lost and found directory is found on each linux file system and this is designed to place orphaned or corrupted files or any corrupted bits of data from the file system to be placed here and so it's not something that you would interact with but always a good to know so i'm going to now create a file in new pd so i'm going to run the command sudo nano file a bow ties dot text so file a bow ties is the file that i'm going to create nano is my text editor and so i'm going to hit on enter and so in this file i'm going to type in bow ties are so classy because after all they are i'm going to hit ctrl o to save i'm going to hit enter to verify it and ctrl x to exit so if i do another ls i can see the file of bow ties has been created also by running the command df minus k i'll be able to see the file system here as well and so this is the end of part one of this demo it was getting a bit long so i decided to break it up this would be a great opportunity for you to get up have a stretch get yourself a coffee or tea and whenever you're ready you can join me in the next one where part two will be starting immediately from the end of part one welcome back this is part two of this demo and we're gonna continue immediately from the end of part one so with that being said let's dive in and so what i want to do now is i want to reboot the instance in order to demonstrate the mounting of this device and i'm going to do that by using the command sudo reboot it's going to disconnect me i'm going to click on close and i'm going to wait about a minute for it to reboot okay and it's been about a minute so i'm going to now ssh into my instance okay and here i am back again logged into my instance i'm going to quickly clear the screen and i'm going to run the lsblk command now what i wanted to demonstrate here is that although i mounted the new device it did not stay mounted through the reboot and this is because there is a configuration file in linux that points to which partitions get mounted automatically upon startup that i need to edit in order to make sure that this device is mounted every time the instance reboots and so in order to do that i need to edit a file called fstab and i'm going to have to add the unique identifier for this partition also known as the device sdb and this will mount the partition automatically every time there happens to be a reboot so in order to do that i'm going to run the command sudo blk id and the path of the block device forward slash dev forward slash sdb i'm going to hit on enter and here is the identifier also known as the uuid that i need to append to the fstab file so i'm going to copy the uuid and i'm going to use the command sudo nano etc fs tab and i'm going to hit on enter and here you will find the uuid for your other partitions and so you're going to be appending a line here right at the end so i'm going to move my cursor down here i'm going to type in uuid equals and then the uuid that i had copied earlier the amount point which is going to be forward slash new pd the type of file system which is ext4 along with defaults comma no fail i'm going to hit control o to save hit enter to verify and control x to exit and so now i'm going to mount this device by running the command sudo mount dash a and hit enter and this command will mount all the partitions that are available in the fstab file and so when i run a lsblk i can see here that my block device sdb is now mounted on forward slash new pd now i know this may be a refresher for some but this is a perfect demonstration of the tasks that need to be done when creating and attaching a new disk to an instance and is a common task for many working on linux instances and working in cloud this can definitely be scripted but i wanted to show you the steps that need to be taken in order to get a new disk in a usable state okay so great we have created a new disk we had attached the disk created a file system and had mounted the disk along with editing the configuration file to make sure that the device mounts whenever the instance starts up so now that we've done all that i wanted to demonstrate resizing this disk from 100 gigabytes to 150 gigabytes and so just to show you where it is in the console i'm going to quickly go back to my console tab and so here i'm going to go to the left hand menu i'm going to click on disks i'm going to drill down into new pd and at the top i'm going to click on edit and so here i'm able to adjust the disk space size and simply click on save not much that i really need to do here but i did want to show you how to do this in the command line so i'm going to go back to the tab of my instance and i'm going to quickly clear the screen and i'm going to paste in the command gcloud compute disks resize the name of the disk which is new pd and the new size in gigabytes using the dash dash size flag 150 which is the new size of the disc along with the dash dash zone flag of us east 1b i'm going to hit enter it's going to ask me if i want to do this as this is not reversible and please remember when you resize a disk you can only make it bigger and never smaller so i'm going to hit y to continue and it took a few seconds there but it was successful so if i run a df minus k you can see here that i only have 100 gigabytes available to me and this is because i have to extend the file system on the disk so i've made the disk larger but i haven't allocated those raw blocks to the file system so in order for the file system to see those unallocated blocks that's available to it i need to run another command so i'm going to quickly clear my screen again and i'm going to run the command sudo resize to fs along with the block device i'm going to hit enter and as you can see it was successful showing the old blocks as 13 and the new blocks as 19. so if i run a df minus k i can now see my 150 gigabytes that's available to me and so just to demonstrate after resizing the disk along with mounting and then remounting the disk that the file that i've created still exists i'm going to run an ls minus al but first i will need to change directories into new pd clear my screen and run an ls and phyla bow ties is still there and so this is a great example demonstrating how the data on persistent disks persist through the lifetime of a disk even when mounting unmounting rebooting and resizing and so as you can see we've done a lot of work here and so just as a recap where we've created a new disk we attached this disk to an instance we formatted the disk into an ext4 file system we've mounted this disk we've written a file to it added its unique identifier to the configuration file so that it mounts on startup and then we've resized the disk along with extending the file system on the disk and so this is the end of the demo and i wanted to congratulate you on making it to the end and i hope this demo has been extremely useful and again fantastic job on your part now before you go i wanted to quickly walk through the steps of deleting all the resources you've created and so the first thing that i want to do is delete the disk that was created for this demo and so before i can delete the disk i'm going to first detach the disk from the instance and the easiest way to do that is through the command line so i'm going to quickly clear my screen and so i'm going to show you how to detach the disk from the instance and so i'm going to paste in this command gcloud compute instances detach disk the instance name which is bow tie dash instance along with the disc with the flag dash dash disc the name of the disc which is new pd along with the zone i'm going to hit enter and it's been successfully detached and so now that it's detached i can actually delete the disk and so i'm going to head on over back to the console and i'm going to go ahead and delete the new pd disk i'm going to click on delete i'm going to get a prompt asking me if i'm sure yes i am if i go back to the main menu for my disks and this should just take a moment and once it's deleted you will no longer see it here and i'm going to go back over to vm instances and i'm going to delete this as well and so there's no need to delete your default vpc unless you'd like to recreate it again but don't worry for those who decide to keep it you will not be charged for your vpc as we will be using it in the next demo and so that's pretty much all i wanted to cover when it comes to managing disks with compute engine so you can now mark this as complete and let's move on to the next one welcome back in this lesson i'll be discussing persistent disk snapshots now snapshots are a great way to backup data from any running or stopped instances from unexpected data loss snapshots are also a great strategy for use in a backup plan for any and all instances no matter where they are located and so as cloud engineers and architects this is a great tool for achieving the greatest uptime for your instances so diving right into it snapshots as i mentioned before are a great way for both backing up and restoring the data of your persistent disks you can create snapshots from disks even while they are attached to running instances snapshots are global resources so any snapshot is accessible by any resource within the same project you can also share snapshots across projects as well snapshots also support both zonal and regional persistent disks snapshots are incremental and automatically compressed so you can create regular snapshots on a persistent disk faster and at a much lower cost than if you regularly created a full image of a disk now when you create a snapshot you have the option of choosing a storage location snapshots are stored in cloud storage and can be stored in either a multiregional location or a regional cloud storage bucket a multiregional storage location provides higher availability but will drive up costs please be aware that the location of a snapshot affects its availability and can incur networking costs when creating the snapshot or restoring it to a new disk if you do not specify storage location for a snapshot google cloud uses the default location which stores your snapshot in a cloud storage multiregional location closest to the region of your source disk if you store your snapshot in the same region as your source disk there is no network charge when you access that snapshot from the same region if you access the snapshot from a different region you will incur a network cost compute engine stores multiple copies of each snapshot across multiple locations as well you cannot change the storage location of an existing snapshot once a snapshot has been taken it can be used to create a new disk in any region and zone regardless of the storage location of the snapshot now as i explained earlier snapshots are incremental and i wanted to take a moment to dive into that for just a minute so when creating snapshots the first successful snapshot of a persistent disk is a full snapshot that contains all the data on the persistent disk the second snapshot only contains any new data or modify data since the first snapshot data that hasn't changed since snapshot 1 isn't included instead snapshot 2 contains references to snapshot 1 for any unchanged data as shown here snapshot 3 contains any new or changed data since snapshot 2 but won't contain any unchanged data from snapshot 1 or 2. instead snapshot 3 contains references to blocks in snapshot 1 and snapshot 2 for any unchanged data this repeats for all subsequent snapshots of the persistent disk snapshots are always created based on the last successful snapshot taken and so now you're probably wondering what happens when you decide to delete a snapshot are they dependent on each other well when you delete a snapshot compute engine immediately marks the snapshot as deleted in the system if the snapshot has no dependent snapshots it is deleted outright however if the snapshot does have dependent snapshots then there are some steps that happen behind the scenes so shown here in this diagram snapshot 2 is deleted the next snapshot from the full snapshot no longer references the snapshot for deletion in this example snapshot 1 then becomes the reference for snapshot 3 and any data that is required for restoring other snapshots is moved into the next snapshot increasing its size shown here blocks that were unique to snapshot 2 are moved to snapshot 3 and the size of snapshot 3 increases any data that is not required for restoring other snapshots is deleted so in this case blocks that are already in snapshot 3 are deleted from snapshot 2 and the size of all snapshots are lower now because subsequent snapshots might require information stored in a previous snapshot please be aware that deleting a snapshot does not necessarily delete all the data on the snapshot if you're looking to make sure that your data has indeed been deleted from your snapshots you should delete all snapshots if your disk has a snapshot schedule you must detach the snapshot schedule from the disk before you can delete the schedule removing the snapshot schedule from the disk prevents further snapshot activity from occurring now touching on the topic of scheduled snapshots by far the best way to backup your data on compute engine is to use scheduled snapshots this way you will never have to worry about manually creating snapshots or even worry about using other tools to kick off those snapshots you can simply use this builtin tool by google which is why snapshot schedules are considered best practice to backup any compute engine persistent disks now in order to create any snapshot schedules you must create your snapshot schedule in the same region where your persistent disk resides now there are two ways to create a snapshot schedule the first one is to create a snapshot schedule and then attach it to an existing persistent disk the other way is to create a new persistent disk with a snapshot schedule you also have the option of setting up a snapshot retention policy that defines how long you want to keep your snapshots some options when creating snapshot schedules are both retention policies and source disk deletion rules now if you choose to set up a snapshot retention policy you must do it as part of your snapshot schedule when you create a snapshot schedule is when you can also set a source disk deletion rule the source disk deletion rule controls what happens to your snapshots if the source disk is deleted now a few caveats here on the scheduled snapshots is that a persistent disk can only have one snapshot schedule attached to it at a time also you cannot delete a snapshot schedule if it is attached to a disk you must detach the schedule from all disks then delete the schedule as well after you create a snapshot schedule you cannot edit it to update a snapshot schedule you must delete it and create a new one now before i end this lesson i wanted to touch on managing snapshots for just a minute so when managing snapshots there's a few things to remember in order to use snapshots to manage your data efficiently you can snapshot your disks at most once every 10 minutes you are unable to snapshot your disks at intervals less than 10 minutes so please keep that in mind when creating your schedules also you should create snapshots on a regular schedule to minimize data loss if there was an unexpected failure if you have existing snapshots of a persistent disk the system automatically uses them as a baseline for any subsequent snapshots that you create from that same disk so in order to improve performance you can eliminate excessive snapshots by creating an image and reusing it using this method would not only be ideal for storage and management of snapshots but also help to reduce costs and if you schedule regular snapshots for your persistent disks you can reduce the time that it takes to complete each snapshot by creating them during offpeak hours when possible and lastly for those of you who use windows for most situations you can use the volume shadow copy service to take snapshots of persistent disks that are attached to windows instances you can create vss snapshots without having to stop the instance or detach the persistent disk and so that's pretty much all i wanted to cover when it comes to the theory of persistent disk snapshots their schedules and how to manage them in the next lesson i'll be doing a handson demo demonstrating snapshots and putting this theory into practice and get a feel for how snapshots work and how they can be applied to persistent disks so you can now mark this lesson as complete and whenever you're ready join me in the console welcome back in this demonstration we're going to dive into snapshots and snapshot schedules this demo will give you the handson knowledge you need to create and delete snapshots along with how to manage snapshot schedules we're going to start the demo off by creating an instance we're going to interact with it and then take a snapshot of the disk we're going to then create another instance from the snapshot and then create some snapshot schedules for both of these instances by using both the console and the command line so there's a lot to do here so with that being said let's dive in and so i'm currently logged in as tony bowties gmail.com as well i'm in project bowtie inc so the first thing that we need to do to kick off this demo is to create an instance but first as always i like to make sure that i have a vpc to deploy my instance into with its corresponding default firewall rules and so i'm going to head on over to the navigation menu and scroll down to vpc network and because i didn't delete my default vpc from the last demo i still have it here i'm just going to drill down and make sure that i have my firewall rules i'm gonna go over to firewall rules and as expected the ssh firewall rule that i need has already been created and so now that i have everything in order i'm gonna go back over to the navigation menu and head on over to compute engine to create my instance now i figure for this demo i'd switch it up a little bit and create the instance by the command line so i'm going to head on over to cloud shell i'm going to open that up and it took a minute to provision and so what i'm going to do now is i'm going to open it up in a new tab i'm going to zoom in for better viewing and i'm going to paste in my command to create my instance and this gcloud command to create these instances will be available in the github repository and you will find all the instructions and the commands under managing snapshots in compute engine so i'm going to hit enter and you may get a prompt to authorize this api call and i'm going to click on authorize and success our instance has been created and is up and running and so now what i want to do is ssh into the instance and so i'm just going to run the command from here which is gcloud compute ssh dash dash zone the zone that i'm in which is used 1b and the instance which is bowtie dash instance i'm going to hit enter it's going to prompt me if i want to continue i'm going to say yes and i'm going to enter my passphrase and enter it again it's going to update my metadata and it's going to ask me again for my passphrase and i'm in so i'm going to just quickly clear my screen and so the first thing i want to do is i want to verify the name of my instance so i'm going to type in the command hostname and as expected bowtie dash instance shows up and so now i want to create a text file and so i'm going to run the command sudo nano file a text i'm going to hit enter and it's going to open up my nano text editor and you can enter a message of any kind that you'd like for me i'm going to enter more bow tie needed because you can never get enough bow ties i'm going to hit ctrl o to save press enter to verify the file name to write and then ctrl x to exit i'm going to run the command ls space minus al to list my files so i can verify that my file has been created and as you can see here file a bowties.txt has been created and so now that i've created my instance and i've written a file to disk i'm going to now head on over to the console and take a snapshot of this disk and because my session was transferred to another tab i can now close the terminal and you want to head over to the lefthand menu and go to disks and so now i want to show you two ways on how you can create this snapshot the first one is going to disks and choosing the disk that you want for me it's bowtie instance and under actions i'm going to click on the hamburger menu and here i can create snapshot and this will bring me straight to my snapshot menu but for this demo i'm going to go over to the left hand menu and i'm going to click on snapshots and here i'm going to click on create snapshot and so for the name of the snapshot i'm going to type in bowtie snapshot and i'm going to use the same for the description moving down on the source disk the only one that i can select is bow tie instance and that's the one that i want anyways so i'm going to click on that the location in order to cut down on costs we don't need multiregional we're going to just select regional and if you select on the location i'm able to select any other locations like tokyo and i can create my snapshot in tokyo but i want to keep my snapshot in the same region so i'm going to go back and select us east one where it is based on the source disk location and i'm going to add a label here with the key environment and the value of testing i'm going to leave my encryption type as google managed and i'm going to simply click on create and this will create a snapshot of the boot disk on bow tie instance and that took about a minute there and so just as a note if you have any bigger discs they will take a little bit longer to snapshot okay and now that i've created my snapshot i'm going to go back up to vm instances and i'm going to create a new instance from that snapshot and so i'm going to name this instance bowtie dash instance dash 2 and i'm going to give this a label i'm going to add a label here the key of environment and the value of testing and hit save the region is going to be used 1 and you can leave the zone as its default as us east 1b and under machine type you can select the e2 micro and you want to go down to boot disk and select the change button and here i'm going to select snapshots instead of using a public image so i'm going to click on snapshots and if i select the snapshot drop down menu i will see here my bowtie snapshot so i'm going to select this i'm going to leave the rest as default and i'm going to go down to select and i'm going to leave everything else as its default and i'm going to click on create i'm going to just give it a minute here so bowtie instance 2 can be created okay and it took a minute there so now i'm going to ssh into this instance and i'm going to zoom in for better viewing and even though i know the instance is named bowtie.instance2 i'm still going to run the hostname command and as expected the same name pops up but what i was really curious about is if i run the command ls space dash al i can see here my file of file of bowties.text and if i cat the file i'll be able to see the text that i inputted into that file and so although it was only one file and a text file at that i was able to verify that my snapshot had worked as there will be times where your snapshot can get corrupted and so doing some various spot checks on your snapshots is some good common practice and so now i want to create a snapshot schedule for both of these instances and so i'm going to go back to the console and on the left hand menu i'm going to head down to snapshots and if i go over to snapshot schedules you can see that i have no snapshot schedules so let's go ahead and create a new one by clicking on create snapshot schedule and so as mentioned in the last lesson we need to create this schedule first before we can attach it to a disk and so i'm going to name this snapshot schedule as bow tie dash disk schedule i'm going to use the same for the description the region i'm going to select it as us east one and i'm going to keep the snapshot location as regional under us east one you scroll down here and under schedule options you can leave the schedule frequency as daily and just as a note for start time this time is measured in utc so please remember this when you're creating your schedule in your specific time zone and so i'm going to put the start time as o 600 and this will be 1 am eastern standard time as backups are always best done when there is the least amount of activity and i'm going to keep the auto delete snapshots after 14 days i'm going to keep the deletion rule as keep snapshots as well i can enable the volume shadow copy service for windows but since we're running linux i don't need to enable this and since we labeled everything else i might as well give this a label i'm going to use the key as environment and the value of testing and once you've filled everything out then you can simply click on create and it took a minute there but the schedule was created and so now that i have my snapshot schedule i need to attach it to a disk so i'm going to head on over to the left hand menu and click on disks and here i'm going to drill down into bow tie instance i'm going to go up to the top and click on edit and under snapshot schedule i'm going to click on the drop down and here i will find bow tie disk schedule i'm going to select that i'm going to click on save and so now that i have my snapshot schedule attached to my disk for the bowtie instance instance i now want to create a snapshot schedule for my other instance and so instead of using the console i'm going to go ahead and do it through the command line so i'm going to go up to the top to my open shell and i'm going to quickly clear the screen and so in order to create my schedule i'm going to run this command gcloud compute resource policies create snapshot schedule the name of the snapshot schedule which is bow tie disk schedule 2 the region the maximum retention days the retention policy and the schedule followed by the storage location and like i said before these commands you will find in the github repository so i'm going to go ahead and hit enter and so i wanted to leave this error in here to show you that i needed the proper permissions in order to create this snapshot schedule a great reminder to always check if you have the right role for the task at hand and so i have two options i can either change users from my service account user to tony bowtie or i can simply head on over to my instance and edit the service account permissions and so the easiest way to do it would be to just switch users and so i'm going to go ahead and do that so i'm going to go ahead and run the command gcloud auth login and remember that this is something that you don't have to do i merely wanted to show you that you require the proper permissions on creation of specific resources okay and i quickly went through the authentication process i'm gonna just clear my screen and i'm going to go ahead and run the command again and as expected the snapshot schedule was created with no errors and so now that my schedule has been created i can now attach it to the disk so i'm going to run the command gcloud compute disks add resource policies the instance name which is bowtie instance 2 and the resource policy which is the snapshot schedule named as bowtie disk schedule 2 in the zone of us east 1b i'm going to hit enter and success and so just to verify that the snapshot schedule has been attached to my disk i'm going to go back to the console i'm going to head back on over to the main page of disks i'm going to drill down into bow tie instance 2 and here it is the snapshot schedule has been attached and so i want to congratulate you on making it to the end of this demo and i hope this demo has been useful as snapshots in the role of an engineer is a common task that can save you from any data loss once set into place and so just as a recap you've created an instance you created a file on that instance and then you've created a snapshot of the disk of that instance and used it to create another instance you then verified the snapshot and then created a snapshot schedule for both boot disks of the instances using the console and the command line well done on another great job now before you go i wanted to take a moment to clean up any resources we've used so we don't accumulate any costs and so the first thing we want to do is we want to detach the snapshot schedules from the disks and so since we're in bow tie instance 2 i'm going to go ahead and click on edit under snapshot schedule i'm going to select the no schedule hit save and i'm going to do the same thing with my other disk now i'm going to head back on over to snapshots i'm going to delete this snapshot and i'm going to head back on over to snapshot schedules i'm going to select all the snapshot schedules and i'm going to click on delete and now that everything's cleaned up with regards to snapshots and snapshot schedules i can now go over to vm instances and delete the instances i'm going to select them all and simply click on delete and so that's pretty much all i wanted to cover in this demo when it comes to snapshots and snapshot schedules so you can now mark this as complete and let's move on to the next one welcome back in this lesson we're going to switch gears and take an automated approach to deployment by diving into google's tool for infrastructure as code called deployment manager now deployment manager allows you to deploy update and tear down resources from within google cloud using yaml jinja and python code templates it allows you to automate the deployment of all the resources that are available in google cloud and deploy it in a fast easy and repeatable way for consistency and efficiency in this lesson we're going to explore the architecture of deployment manager and dive into all the different components that gives it its flexibility and the features that make this tool an easy solution for deploying complex environments so with that being said let's dive in now breaking down the components that i mentioned earlier i wanted to start off with the first component being the configuration now a configuration defines the structure of your deployment as you must specify a configuration to create a deployment a configuration describes all the resources you want for a single deployment and is written in yaml syntax that lists each of the resources you want to create and its respective resource properties a configuration must contain a resources section followed by the list of resources to create and so each resource must contain these three components the name the type and properties without these three components a deployment will not instantiate and so i wanted to take a moment to go over these three components in a bit of depth so the first component of the configuration is the name and the name is a user defined string to identify this resource and can be anything you choose from names like instance one myvm bowtie dash instance and you can even go as far to use larks dash instance dash don't dash touch and the syntax can be found here and must not contain any spaces or invalid characters next component in a configuration is type and there are a couple of different types that you can choose from a type can represent a single api source known as a base type or a set of resources known as a composite type and either one of these can be used to create part of your deployment the type of the resource being deployed here in this diagram is shown as a base type of compute.v1.instance and there are many other api resources that can be used such as compute.v1.disk app engine dot v1 as well as bigquery.v2 and the syntax is shown here as api dot version dot resource now a composite type contains one or more templates that are preconfigured to work together these templates expand to a set of base types when deployed in a deployment composite types are essentially hosted templates that you can add to deployment manager the syntax is shown here as gcp dash types forward slash provider colon resource and to give you an example of what a composite type looks like here is shown the creation of a reserved ip address using the compute engine v1 api and you could also use composite types with other apis in the same way such as gcp dash types forward slash app engine dash v1 colon apps or bigquery v2 colon data sets and for the last component in a configuration is properties and this is the parameters for the resource type this includes all the parameters you see here in this example including the zone machine type the type of disk along with its parameters pretty much everything that gives detail on the resource type now just as a note they must match the properties for this type so what do i mean by this so let's say you entered a zone but that particular zone doesn't exist or that compute engine machine type doesn't exist in that zone you will end up getting an error as deployment manager will not be able to parse this configuration and thus failing deployment so make sure when you add your properties that they match those of the resource now a configuration can contain templates which are essentially parts of the configuration file that have been abstracted into individual building blocks a template is a separate file that is imported and used as a type in a configuration and you can use as many templates as you want in a configuration and allow you to separate your configuration out into different pieces that you can use and reuse across different deployments templates can be as generalized or specific as you need and they also allow you to take advantage of features like template properties environment variables and modules to create dynamic configuration as shown here templates can be written in a couple of different ways they can be written in either ginger 2.1 or python 3. the example shown on the left has been written in ginger and is very similar to the yaml syntax so if you're familiar with yaml this might be better for you the example on the right has been written in python and is pretty amazing as you can take advantage of programmatically generating parts of your templates if you are familiar with python this might be a better format for you now one of the advantages of using templates is the ability to create and define custom template properties template properties are arbitrary variables that you define in template files any configuration file or template file that uses the template in question can provide a value for the template property without changing the template directly this lets you abstract the property so that you can change the property's value for each unique configuration without updating the underlying template and just as a note deployment manager creates predefined environment variables that you can use in your deployment in this example the project variable will use the project id for this specific project and so combining all these components together will give you a deployment and so a deployment is a collection of resources that are deployed and managed together using a configuration you can then deploy update or delete this deployment by merely changing some code or at the click of a button now when you deploy you provide a valid configuration in the request to create the deployment a deployment can contain a number of resources across a number of google cloud services when you create a deployment deployment manager creates all of the described resources to deploy a configuration it must be done through the command line and cannot be done through the console you can simply use the syntax shown here and a deployment will be instantiated from the configuration file that you have entered where bow tie deploy is the name of the deployment and the file after the dash dash config is your configuration file google cloud also offers predefined templates that you can use to deploy from the gcp marketplace and can be found right in the console of deployment manager this way all the configuration and template creation is handled for you and you just deploy the solution through the console now after you've created a deployment you can update it whenever you need to you can update a deployment by adding or removing resources from a deployment or updating the properties of existing resources in a deployment a single update can contain any combination of these changes so you can make changes to the properties of existing resources and add new resources in the same request you update your deployment by first making changes to your configuration file or you can create a configuration file with the changes you want you will then have the option to pick the policies to use for your updates or you can use the default policies and finally you then make the update request to deployment manager and so once you've launched your deployment each deployment has a corresponding manifest as the example shown here a manifest is a readonly property that describes all the resources in your deployment and is automatically created with each new deployment manifests cannot be modified after they have been created as well it's not the same as a configuration file but is created based on the configuration file and so when you delete a deployment all resources that are part of the deployment are also deleted if you want to delete specific resources from your deployment and keep the rest delete those resources from your configuration file and update the deployment instead and so as you can see here deployment manager gives you a slew of different options to deploy update or delete resources simultaneously in google cloud now like most services in gcp there are always some best practices to follow note that there are many more best practices to add to this and can be found in the documentation which i will be providing the link to in the lesson text but i did want to point out some important ones to remember so the first one i wanted to bring up is to break your configurations up into logical units so for example you should create separate configurations for networking services security services and compute services so this way each team will be able to easily take care of their own domain without having to sift through a massive template containing the code to the entire environment another best practice to follow is to use references and references should be used for values that are not defined until a resource is created such as resources selflink ip address or system generated id without references deployment manager creates all resources in parallel so there's no guarantee that dependent resources are created in the correct order using references would enforce the order in which resources are created the next one is to preview your deployments using the preview flag so you should always preview your deployments to assess how making an update will affect your deployment deployment manager does not actually deploy resources when you preview a configuration but runs a mock deployment of those resources instead this gives you the opportunity to see the changes to your deployment before committing to it you also want to consider automating the creation of projects as well as automating the creation of resources contained within the projects and this enables you to adopt an infrastructure as code approach for project provisioning this will allow you to provide a series of predefined project environments that can be quickly and easily provisioned it will also allow you to use version control to manage your base project configuration and it will also allow you to deploy reproducible and consistent project configurations and lastly using a version control system as part of the development process for your deployments is a great best practice to follow as it allows you to fall back to a previous known good configuration it provides an audit trail for changes as well it uses the configuration as part of a continuous deployment system now as you've seen here in this lesson deployment manager can be a powerful tool in your tool belt when it comes to implementing infrastructure as code and it has endless possibilities that you can explore on your own it can also provide a massive push towards devops practices and head down the path of continuous automation through continuous integration continuous delivery and continuous deployment and so that's pretty much all i wanted to cover when it comes to deployment manager and so whenever you're ready join me in the next one where we will go handson in a demonstration to deploy a configuration in deployment manager so you can now mark this lesson as complete and whenever you're ready join me in the console welcome back in this demonstration we're gonna go handson with deployment manager and deploy a small web server we're gonna first use the google cloud editor to copy in our code and we're gonna then do a dry run and then finally deploy our code we're gonna then do a walkthrough of deployment manager in the console and go through the manifest as well as some of the other features we're then going to verify all the deployed resources and we get to do an easy cleanup in the end by hitting the delete button and taking care of removing any resources that were created so there's quite a bit to go through here and so with that being said let's dive in and so as you can see here i am logged in as tonybowties gmail.com in the project of bowtie inc now since we're going to be doing most of our work in code the first thing that we want to do is go to the google cloud editor so i'm going to go up here to the top and open up cloud shell and i'm going to then click on the button open editor i'm going to make this full screen for better viewing and so in order to get the terminal in the same viewing pane as the editor i'm going to simply go up to the top menu and click on terminal and select new terminal now for better viewing and this is totally optional for you i'm going to change the color theme into a dark mode and so i'm going to go up to the menu click on file go down to settings and go over to color theme and i'm going to select dark visual studio and for those of you who are working in visual studio code this may look very familiar to you and i'm also going to increase the font size by again going back up to file over to settings and then over to open preferences here under workspace and then scroll down to terminal and if you scroll down to integrated font size i'm going to adjust the font size to 20 for better viewing and my cloud shell font size is a little bit easier to see and so once you've done that you can then close the preferences tab and we're now ready to create files in our editor okay so next up i want to create a folder for all my files to live in so i'm going to go up to the menu here i'm going to select on file and select new folder and i'm going to rename this folder as templates and hit ok and so now that we have the folder that all of our files are going to live in the next step is to open up the github repository in your text editor and have your files ready to copy over and so just as a note for those who are fluent in how to use git you can use this new feature in the cloud shell editor to clone the course repo without having to recreate the files so i'm going to go over my text editor and make sure that you've recently done a git pull we're going to open up the files under compute engine deployment manager and you'll see templates with a set of three files and i've already conveniently opened them up i'm going to go up to bow tie deploy.yaml and this is going to be the configuration file that i'm going to be copying over and once i finish copying all these files over i'll be going through this in a little bit of detail just so you can understand the format of this configuration and so i'm going to select all of this i'm going to copy this head back on over to the editor and here i'm going to select file new file so i'm going to rename this as bow tie dash deploy dot yaml hit okay and i'm going to paste in my code and so this configuration file is showing that i'm going to be importing two templates by the name of bowtie.webserver.jinja as well as bowtie.network.jinja so i'm going to have a template for my web server and a template for the network and under resources as you can see this code here will create my bow tie dash web server the type is going to be the template the properties will have the zone the machine type as well as a reference for the network as well underneath the bowtie web server is the bowtie network and again this is pulling from type bowtie.network.jinja so this is a another template file and under the properties we have the region of us east one and so we're going to copy over these two templates bowtie web server and bowtie network as we need both of these templates in order to complete this deployment and so i'm going to go ahead and do that now head back on over to my code editor i'm going to go to bowtie web server i'm going to copy everything here back to my editor and i'm going to create the new file called bowtie web server it's going to be dot jinja hit enter i'm going to paste the code in and just to do a quick run through of the template the instance name is going to be bow tie dash website the type is compute.v1.instance and as you can see here we are using a bunch of different properties here under zone we have property zone which is going to reference back to the yaml template here under zone you will see us east 1b and so this way if i have to create another web server i can enter whatever zone i like here in the configuration file and leave the bow tie dash web server template just the way it is under machine type i have variables set for both the zone and machine type under disks i'm going to have the device name as an environment variable and it's going to be a persistent disk and the source image is going to be debian9 i also put in some metadata here that will bring up the web server and lastly i have a network tag of http server as well as the configuration for the network interface the network referring to bowtie dash network and a sub network called public which i will be showing to you in just a moment and as well the access configs of the type one to one nat and this will give the instance a public ip address and so now that we've gone through that template we need to create one last template which is the bowtie dash network so i'm going to head back on over to my code editor and open up bowtie network select the code copy it back over to cloud editor and i'm going to create a new file call this bowtie network dot jinja hit enter paste in my code and to quickly walk you through this we're going to be creating a new custom network called bow tie dash network the type is going to be compute.v1.network as the vpc uses the compute engine api it's going to be a custom network so the value of the auto create sub networks is going to be false the name is going to be public here we have the custom ipcider range and you can also use this as a variable but for this demo i decided to just leave it under network i have a reference to the bowtie network the value for private google access is false and the region variable is fulfilled through the configuration file moving right along i have two firewall rules here one for ssh access and the other for web server access one opening up port 22 to the world as well as port 80. as well the web server access firewall rule has a target tag of http server referencing back to the network tag of the bowtie web server instance okay and so now we've finished creating the configuration file along with the templates so i'm going to head back on up to the menu click on file and select save all and since we've finished creating all of our files the next thing to do is to execute a mock deploy using the bowtie deploy configuration but first i know that we haven't used deployment manager before and so i need to go in and turn on the api and so i'm just going to go up here to the top to the search bar and i'm going to type in deployment and you should see deployment manager as the first result and bring this down a little bit and as expected the deployment manager api has not been enabled yet so i'm going to click on enable and after a few moments we should be good to go okay and as you can see here deployment manager is pretty empty as most of it is done through the command line but if you're looking to deploy a marketplace solution you can do that right here at the top and this will bring you right to the marketplace and will allow you to deploy from a large selection of preconfigured templates but i don't want to do that and so i'm just going to bring this up a little bit and i'm going to head on over to the terminal i'm going to run an ls i'm going to run the command ls and you should be able to see the templates folder i'm going to change my directory into the templates folder do another ls and here are all my files and so before we do a mock deploy of this configuration we want to make sure that we're deploying to the correct project i can see here that i am currently in bow tie inc but if you are ever unsure about the project that you're in you can always run the gcloud config list command in order to confirm so i'm going to quickly clear my screen and i'm going to run the command gcloud config list it's going to prompt me to authorize this api call and i'm going to authorize and as expected my project is set to deploy in project bowtie inc and so now that i've verified it i'm going to quickly clear my screen again and so i'm going to paste in my command gcloud deployment dash manager deployments create bowtie deploy which is the name of the deployment along with the configuration file flag dash dash config and then the name of the configuration file which is bowtie deploy.yaml and the preview flag as we're only doing a mock deploy and so if there are any errors i'll be able to see this before i actually deploy all the resources so i'm going to go ahead and hit enter and in just a minute we'll find out exactly what happens and as you can see here the mock deployment was a success and there are no errors and if i do a quick refresh up here in the console i'll be able to see my deployment which i can drill down into and here i will see my manifest file with my manifest name and i can view the config as well as my templates that it imported the layout as well as the expanded config so if i click on view of the config it'll show me here in the right hand panel exactly what this deployment has used for the config and i can do the same thing with my template files so i'm going to open up my network template and i can quickly go through that if i'd like as well i also have the option to download it and if i really want to get granular i can go over here to the left hand pane i can select on vm instance and it'll show me all the resource properties everything from the disks to the machine type to the metadata the network interfaces the zone that it's in and the network tag same thing if i go over here to the network and again because this is a custom network the value for the autocreate subnetworks is false i can check on the public sub network as well as the firewall rules and so because this is a preview it has not actually deployed anything now taking a look at compute engine instances in a new tab you can see here that i have no instances deployed and so the same goes for any of the other resources and so what we want to do now is we want to deploy this deployment and we can do that one of two ways we can simply click on the button here that says deploy or we can run the command in the command line and so i'm looking to show you how to do it in the command line so i'm going to move down to the command line i'm going to quickly clear my screen i'm going to paste in the code which is gcloud deployment dash manager deployments update bowtie deploy now you're probably wondering why update and this is because the configuration has been deployed even though it's a preview deployment manager still sees it as a deployment and has created what google cloud calls a shell and so by using update you can fully deploy the configuration using your last preview to perform that update and this will deploy your resources exactly how you see it in the manifest and so anytime i make an adjustment to either the configuration or the templates i can simply run the update command instead of doing the whole deployment again so i want to get this deployed now and so i'm going to hit enter and i'll be back in a minute once it's deployed all the resources and success my deployment is successful and as you can see here there are no errors and all the resources are in a completed state so i'm going to select my bow tie website in my manifest and i'll have access to the resource with a link up here at the top that will bring me to the instance as well i can ssh into the instance and i have all the same options that i have in the compute engine console and so in order to verify that all my resources have been deployed i'm going to go back over to the tab that i already have open and as you can see my instance has been deployed and i want to check to see if my network has been deployed so i'm going to go up to the navigation menu and i'm going to head on down to vpc network and as you can see here bowtie network has been deployed with its two corresponding firewall rules i'm going to drill down into bowtie network and check out the firewall rules and as you can see here ssh access and web server access have been created with its corresponding protocols and ports and so now that i know that all my resources have been deployed i want to head back on over to compute engine to see if my instance has been configured properly so i'm going to click on ssh to see if i can ssh into the instance and success with ssh so i know that this is working properly and so i'm going to close this tab down and i also want to see whether or not my web server has been configured properly with the metadata that i provided it and so i can directly open up the webpage by simply clicking on this link and success my you look dapper today why thank you tony bowtie and so as you can see the web server has been configured properly using the metadata that i provided so i wanted to congratulate you on making it to the end of this demo and hope it has been extremely useful and gave you an understanding of how infrastructure is code is used in google cloud using their native tools i hope this also triggered some possible use cases for you that will allow you to automate more resources and configurations in your environment and allow you to start innovating on fantastic new ways for cicd for those of you who are familiar with infrastructure as code this may have been a refresher but will give you some insight for questions on the exam that cover deployment manager and just as a quick note for those of you who are looking to learn more about infrastructure as code i have put a few links in the lesson text going into depth on deployment manager and another tool that google recommends called terraform and so now before you go we want to clean up all the resources that we've deployed to reduce any incurred costs and because deployment manager makes it easy we can do it in one simple step so i'm going to head back on over to my open tab where i have my console open to deployment manager and i'm going to head on over to the delete button and simply click on delete now deployment manager gives me the option of deleting all the resources it created or simply deleting the manifest but keeping the resources untouched and so you want to select delete bowtie deploy with all of its resources and simply click on delete all and this will initiate the teardown of all the resources that have been deployed from the bowtie deploy configuration and this will take a few minutes to tear down but if you ever have a larger configuration to deploy just as a note it may take a little bit longer to both deploy and to tear down and so just as a recap you've created a configuration file and two templates in the cloud shell editor you then deployed your configuration using deployment manager through the command line in cloud shell you then verified each individual resource that was deployed and verified the configuration of each resource congratulations again on a job well done and so that's pretty much all i wanted to cover in this demo when it comes to deploying resources using deployment manager so you can now mark this as complete and let's move on to the next one welcome back and in this lesson we're going to learn about google cloud load balancing and how it's used to distribute traffic within the google cloud platform google cloud load balancing is essential when using it with instance groups kubernetes clusters and is pretty much the defacto when it comes to balancing traffic coming in as well as within your gcp environment knowing the differences between the types of load balancers and which one to use for specific scenarios is crucial for the exam as you will be tested on it and so there's a lot to cover here so with that being said let's dive in now i wanted to start off with some basics with regards to what is low balancing and so when it comes to the low balancer itself a low balancer distributes user traffic across multiple instances of your application so by spreading the load you reduce the risk of your applications experiencing performance issues a load balancer is a single point of entry with either one or multiple back ends and within gcp these back ends could consist of either instance groups or negs and i'll be getting into any g's in just a little bit low balancers on gcp are fully distributed and software defined so there is no actual hardware load balancer involved in low balancing on gcp it is completely software defined and so there's no need to worry about any hardware any prewarming time as this is all done through software now depending on which low balancer you choose google cloud gives you the option of having either a global load balancer or a regional load balancer the load balancers are meant to serve content as close as possible to the users so that they don't experience increased latency and gives the users a better experience as well as reducing latency on your applications when dealing with low balancers in between services google cloud also offers auto scaling with health checks in their load balancers to make sure that your traffic is always routed to healthy instances and by using auto scaling able to scale up the amount of instances you need in order to handle the load automatically now as there are many different low balancers to choose from it helps to know what specific aspects you're looking for and how you want your traffic distributed and so google has broken them down for us into these three categories the first category is global versus regional global load balancing is great for when your back ends are distributed across multiple regions and your users need access to the same applications and content using a single anycast ip address as well when you're looking for ipv6 termination global load balancing will take care of that now when it comes to regional load balancing this is if you're looking at serving your back ends in a single region and handling only ipv4 traffic now once you've determined whether or not you need global versus regional low balancing the second category to dive into is external versus internal external load balancers are designed to distribute traffic coming into your network from the internet and internal load balancers are designed to distribute traffic within your network and finally the last category that will help you decide on what type of load balancer you need is the traffic type and shown here are all the traffic types that cover http https tcp and udp and so now that we've covered the different types of load balancing that's available on google cloud i wanted to dive into some more depth on the low balancers themselves here you can see that there are five load balancers available and i will be going through each one of these in detail now before diving into the low balancers themselves i wanted to introduce you to a concept using gcp for all load balancers called back end services how a low balancer knows exactly what to do is defined by a backend service and this is how cloud load balancing knows how to distribute the traffic the backend service configuration contains a set of values such as the protocol used to connect to back ends various distribution in session settings health checks and timeouts these settings provide fine grain control over how your load balancer behaves an external http or https load balancer must have at least one backend service and can have multiple backend services the back ends of a backend service can be either instance groups or network endpoint groups also known as negs but not a combination of both and so just as a note you'll hear me refer to negs over the course of this lesson and so a network endpoint group also known as neg is a configuration object that specifies a group of backend endpoints or services and a common use case for this configuration is deploying services into containers now moving on to the values themselves i wanted to first start with health checks and google cloud uses the overall health state of each back end to determine its eligibility for receiving new requests or connections back ends that respond successfully for the configured number of times are considered healthy backends that fail to respond successfully for a separate number of times are considered unhealthy and when a backend is considered unhealthy traffic will not be routed to it next up is session affinity and session affinity sends all requests from the same client to the same back end if the back end is healthy and it has capacity service timeout is the next value and this is the amount of time that the load balancer waits for a backend to return a full response to a request next up is traffic distribution and this comprises of three different values the first one is a balancing mode and this defines how the load balancer measures backend readiness for the new requests or connections the second one is target capacity and this defines a target maximum number of connections a target maximum rate or target maximum cpu utilization and the third value for traffic distribution is capacity scalar and this adjusts overall available capacity without modifying the target capacity and the last value for backend services are backends and a backend is a group of endpoints that receive traffic from a google cloud load balancer and there are several types of backends but the one that we are concentrating on for this section and for the exam is the instance group now backend services are not critical to know for the exam but i wanted to introduce you to this concept to add a bit more context for when you are creating low balancers in any environment and will help you understand other concepts in this lesson and so this is the end of part one of this lesson it was getting a bit long so i decided to break it up this would be a great opportunity for you to get up and have a stretch get yourself a coffee or tea and whenever you're ready join me in part two where we will be starting immediately from the end of part one so you can now complete this video and i will see you in part two this is part two of the cloud load balancers lesson and we'll be starting exactly where we left off in part one so with that being said let's dive in now before jumping right into the first load balancer that i wanted to introduce which is http and https low balancer there's a couple of different concepts that i wanted to introduce and these are the methods of how an http and https load balancer distributes traffic using forwarding rules and these are cross region low balancing and content based load balancing now touching on cross region load balancing when you configure an external http or https load balancer in premium tier it uses a global external ip address and can intelligently route requests from users to the closest backend instance group or neg based on proximity for example if you set up instance groups in north america and europe and attach them to a low balancers backend service user requests around the world are automatically sent to the vms closest to the users assuming that the vms pass health checks and have enough capacity if the closest vms are all unhealthy or if the closest instance group is at capacity and another instance group is not at capacity the load balancer automatically sends requests to the next closest region that has available capacity and so here in this diagram a user in switzerland hits the low balancer by going to bowtieinc.co and because there are vms that are able to serve that traffic in europe west 6 traffic is routed to that region and so now getting into content based load balancing http and https low balancing supports content based load balancing using url maps to select a backend service based on the requested host name request path or both for example you can use a set of instance groups or negs to handle your video content and another set to handle static as well as another set to handle any images you can also use http or https low balancing with cloud storage buckets and then after you have your load balancer set up you can add cloud storage buckets to it now moving right along when it comes to http and https load balancer this is a global proxy based layer 7 low balancer which is at the application layer and so just as a note here with all the other low balancers that are available in gcp the http and https low balancer is the only layer 7 load balancer all the other low balancers in gcp are layer 4 and will work at the network layer and so this low balancer enables you to serve your applications worldwide behind a single external unicast ip address external http and https load balancing distributes http and https traffic to back ends hosted on compute engine and gke external http and https load balancing is implemented on google front ends or gfes as shown here in the diagram gfes are distributed globally and operate together using google's global network and control plane in the premium tier gfes offer crossregional low balancing directing traffic to the closest healthy backend that has capacity and terminating http and https traffic as close as possible to your users with the standard tier the load balancing is handled regionally and this load balancer is available to be used both externally and internally that makes this load balancer global external and internal this load balancer also gives support for https and ssl which covers tls for encryption in transit as well this load balancer accepts all traffic whether it is ipv4 or ipv6 traffic and just know that ipv6 traffic will terminate at the low balancer and then it will forward traffic as ipv4 so it doesn't really matter which type of traffic you're sending the load balancer will still send the traffic to the back end using ipv4 this traffic is distributed by location or by content as shown in the previous diagram forwarding rules are in place to distribute defined targets to each target pool for the instance groups again defined targets could be content based and therefore as shown in the previous diagram video content could go to one target whereas static content could go to another target url maps direct your requests based on rules so you can create a bunch of rules depending on what type of traffic you want to direct and put them in maps for requests ssl certificates are needed for https and these can be either google managed or selfmanaged and so just as a quick note here the ports used for http are on 80 and 8080 as well on https the port that is used is port 443 now moving into the next low balancer is ssl proxy an ssl proxy low balancing is a reverse proxy load balancer that distributes ssl traffic coming from the internet to your vm instances when using ssl proxy load balancing for your ssl traffic user ssl connections are terminated at the low balancing layer and then proxied to the closest available backend instances by either using ssl or tcp with the premium tier ssl proxy low balancing can be configured as a global load balancing service with the standard tier the ssl proxy load balancer handles low balancing regionally this load balancer also distributes traffic by location only ssl proxy low balancing lets you use a single ip address for all users worldwide and is a layer 4 load balancer which works on the network layer this load balancer shows support for tcp with ssl offload and this is something specific to remember for the exam this is not like the http or https load balancer where we can use specific rules or specific configurations in order to direct traffic ssl proxy low balancer supports both ipv4 and ipv6 but again it does terminate at the load balancer and forwards the traffic to the back end as ipv4 traffic and forwarding rules are in place to distribute each defined target to its proper target pool and encryption is supported by configuring backend services to accept all the traffic over ssl now just as a note it can also be used for other protocols that use ssl such as web sockets and imap over ssl and carry a number of open ports to support them moving on to the next load balancer is tcp proxy now the tcp proxy load balancer is a reverse proxy load balancer that distributes tcp traffic coming from the internet to your vm instances when using tcp proxy load balancing traffic coming over a tcp connection is terminated at the load balancing layer and then forwarded to the closest available backend using tcp or ssl so this is where the low balancer will determine which instances are at capacity and send them to those instances that are not like ssl proxy load balancing tcp proxy load balancing lets you use a single ip address for all users worldwide the tcp proxy load balancer automatically routes traffic to the back ends that are closest to the user this is a layer 4 load balancer and again can serve traffic both globally and externally tcp proxy distributes traffic by location only and is intended for specifically nonhttp traffic although you can decide if you want to use ssl between the proxy and your back end and you can do this by selecting a certificate on the back end again this type of load balancer supports ipv4 and ipv6 traffic and ipv6 traffic will terminate at the low balancer and forwards that traffic to the back end as ipv4 traffic now tcp proxy low balancing is intended for tcp traffic and supports many wellknown ports such as port 25 for simple mail transfer protocol or smtp next up we have the network load balancer now the tcp udp network load balancer is a regional passthrough load balancer a network load balancer distributes tcp or udp traffic among instances in the same region network load balancers are not proxies and therefore responses from the back end vms go directly to the clients not back through the load balancer the term known for this is direct server return as shown here in the diagram this is a layer 4 regional load balancer and an external load balancer as well that can serve to regional locations it supports either tcp or udp but not both although it can low balance udp tcp and ssl traffic on the ports that are not supported by the tcp proxy and ssl proxy ssl traffic can still be decrypted by your back end instead of the load balancer itself traffic is also distributed by incoming protocol data this being protocols scheme and scope there is no tls offloading or proxying and forwarding rules are in place to distribute and define targets to their target pools and this is for tcp and udp only now with other protocols they use target instances as opposed to instance groups lastly a network load balancer can also only support selfmanaged ssl certificates as opposed to the google managed certificates as well and so the last low balancer to introduce is the internal load balancer now an internal tcp or udp load balancer is a layer 4 regional load balancer that enables you to distribute traffic behind an internal load balancing ip address that is accessible only to your internal vm instances internal tcp and udp load balancing distributes traffic among vm instances in the same region this load balancer supports tcp or udp traffic but not both and as i said before this type of load balancer is used to balance traffic within gcp across instances this low balancer cannot be used for balancing internet traffic as it is internal only traffic is automatically sent to the back end as it does not terminate client connections and for forwarding rules this load balancer follows specific specifications where you need to specify at least one and up to five ports by number as well you must specify all to forward traffic to all ports now again like the network load balancer you can use either tcp or udp and so that's pretty much all i had to cover with this lesson on low balancing please remember that for the exam you will need to know the differences between them all in my experience there are a few questions that come up on the exam where you will need to know what low balancer to use and so a good idea might be to dive into the console and have a look at the options as well as going back through this lesson as a refresher to understand each use case this is also a crucial component in any environment that is used especially when serving applications to the internet for any threetier web application or kubernetes cluster and so that pretty much sums up this lesson on low balancing so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i will be going into depth on instance groups along with instance templates instance groups are a great way to set up a group of identical servers used in conjunction with instance groups instance templates handles the instance properties to deploy the instance groups into your environment this lesson will dive into the details of the features use cases and how instance groups and instance templates work together to create a highly scalable and performing environment now there's a lot to cover here so with that being said let's dive in now an instance group is a collection of vm instances that you can manage as a single entity compute engine offers two kinds of vm instance groups managed and unmanaged manage instance groups or migs let you operate applications on multiple identical vms you can make your workload scalable and highly available by taking advantage of automated mig services like auto scaling auto healing regional and zonal deployments and automatic updating and i'll be getting into these services in just a sec now when it comes to unmanaged instance groups they also let you low balance across a fleet of vms but this is something that you need to manage and i'll be going deeper into unmanaged instance groups a bit later right now i wanted to take some time to go through the features and use cases of migs in a bit more detail for some more context starting off with its use cases now migs are great for stateless serving workloads such as website front ends web servers and website applications as the application does not preserve its state and saves no data to persistent storage all user and session data stays with the client and makes scaling up and down quick and easy migs are also great for stateless batch workloads and these are high performance or high throughput compute workloads such as image processing from a queue and lastly you can build highly available stateful workloads using stateful managed instance groups or stateful migs stateful workloads include applications with stateful data or configuration such as databases legacy monolith type applications and long running batch computations with checkpointing you can improve uptime and resiliency of these types of applications with auto healing controlled updates and multizone deployments while preserving each instance's unique state including instance names persistent disks and metadata now that i've covered the type of workloads that are used with migs i wanted to dive into the features starting with auto healing now when it comes to auto healing managed instance groups maintain high availability of your applications by proactively keeping your instances in a running state a mig automatically recreates an instance that is not running and managed instance groups also take care of applicationbased auto healing and this improves application availability by relying on a health check that detects things like freezing crashing or overloading if a health check determines that an application has failed on a vm the mig auto healer automatically recreates that vm instance the health check used to monitor the migs are similar to the health checks used for low balancing with a few little differences low balancing health checks help direct traffic away from unresponsive instances and towards healthy ones these health checks cannot recreate instances whereas mig health checks proactively signal to delete and recreate instances that become unhealthy moving on to managed instance groups regional or multizone feature now you have the option of creating regional migs or zonal migs regional migs provide higher availability compared to zonal migs because the instances in a regional mig are spread across multiple zones in a single region google recommends regional migs over zonal migs as you can manage twice as many migs as zonal migs so you can manage 2 000 migs instead of 1000 you can also spread your application load across multiple zones instead of a single zone or managing multiple zonal migs across different zones and this protects against zonal failures and unforeseen scenarios where an entire group of instances in a single zone malfunctions in the case of a zonal failure or if a group of instances in a zone stops responding a regional mig continues supporting your instances by continuing to serve traffic to the instances in the remaining zones now cloud low balancing can use instance groups to serve traffic so you can add instance groups to a target pool or to a back end an instance group is a type of back end and the instances in the instance group respond to traffic from the load balancer the back end service in turn knows which instances it can use and how much traffic they can handle and how much traffic they are currently handling in addition the backend service monitors health checking and does not send new connections to unhealthy instances now when your applications require additional compute resources migs support auto scaling that dynamically add or remove instances from the mig in response to an increase or decrease in load you can turn on auto scaling and configure an auto scaling policy to specify how you want the group to scale not only will auto scaling scale up to meet the load demands but will also shrink and remove instances as the load decreases to reduce your costs auto scaling policies include scaling based on cpu utilization load balancing capacity and cloud monitoring metrics and so when it comes to auto updating you can easily and safely deploy new versions of software to instances in a mig the rollout of an update happens automatically based on your specifications you can also control the speed and scope of the deployments in order to minimize disruptions to your application you can optionally perform rolling updates as well as partial rollouts for canary testing and for those who don't know rolling updates allow updates to take place with zero downtime by incrementally updating instances with new ones as well canary testing is a way to reduce risk and validate new software by releasing software to a small percentage of users with canary testing you can deliver to certain groups of users at a time and this is also referred to as stage rollouts and this is a best practice in devops and software development now there are a few more things that i wanted to point out that relate to migs you can reduce the cost of your workload by using preemptable vm instances in your instance group and when they are deleted auto healing will bring the instances back when preemptable capacity becomes available again you can also deploy containers to instances in managed instance groups when you specify a container image in an instance template and is used to create a mig each vm is created with the container optimized os that includes docker and your container starts automatically on each vm in the group and finally when creating migs you must define the vpc network that it will reside in although when you don't define the network google cloud will attempt to use the default network now moving on into unmanaged instance groups for just a minute unmanaged instance groups can contain heterogeneous instances and these are instances that are of mixed sizes of cpu ram as well as instance types and you can add and remove these instances from the group whenever you choose there's a major downside to this though unmanaged instance groups do not offer auto scaling auto healing rolling update support multizone support or the use of instance templates and are not a good fit for deploying highly available and scalable workloads you should only use unmanaged instance groups if you need to apply load balancing to groups of these mixed types of instances or if you need to manage the instances yourself so unmanaged instance groups are designed for very special use cases where you will need to mix instance types in almost all cases you will be using managed instance groups as they were intended to capture the benefits of all the features they have to offer now in order to launch an instance group into any environment you will need another resource to do this and this is where instance templates come into play an instance template is a resource that you can use to create vm instances and managed instance groups instance templates define the machine type boot disk image or container image as well as labels and other instance properties you can then use an instance template to create a mig or vm instance instance templates are an easy way to save a vm instances configuration so you can use it later to recreate vms or groups of vms an instance template is a global resource that is not bound to a zone or region although you can restrict a template to a zone by calling out specific zonal resources now there is something to note for when you are ever using migs if you want to create a group of identical instances you must use an instance template to create a mig and is something you should always keep in the front of mind when using migs these two resources both instance templates and managed instance groups go hand in hand now some other things to note is that instance templates are designed to create instances with identical configurations so you cannot update an existing instance template or change an instance template after you create it if you need to make changes to the configuration create a new instance template you can create a template based on an existing instance template or based on an existing instance to use an existing vm to make a template you can save the configuration using the gcloud command gcloud instance dash templates create or to use the console you can simply go to the instance templates page click on the template that you want to update and click on create similar the last thing that i wanted to point out is that you can use custom or public images in your instance templates and so that's pretty much all i had to cover when it comes to instance groups and instance templates managed instance groups are great for when you're looking at high availability as a priority and letting migs do all the work of keeping your environment up and running and so you can now mark this lesson as complete and whenever you're ready join me in the next one where we go handson with instance groups instance templates and load balancers in a demo welcome back in this demo we're going to put everything that we've learned together in a handson demo called managing bow ties we're going to create an instance template and next we're going to use it to create an instance group we're then going to create a low balancer with a new back end and create some health checks along the way we're then going to verify that all instances are working by browsing to the load balancer ip and verifying the website application we're then going to stress test one of the instances to simulate a scale out using auto scaling and then we're going to simulate scaling the instance group back in now there's quite a bit to do here so with that being said let's dive in so here i am logged in as tony bowties at gmail.com under project bowtie inc and so the first thing that you want to do is you want to make sure that you have a default vpc network already created and so just to double check i'm going to go over to the navigation menu i'm going to scroll down to vpc network and yes i do have a default vpc network so i'm going to go ahead and start creating my resources and so now what i want to do is i want to create my instance template and so in order to do that i'm going to go back up to the navigation menu i'm going to go down to compute engine and go up to instance templates as you can see i currently have no instance templates and yours should look the same and so you can go ahead and click on create instance template and so just as a note there are no monthly costs associated with instance templates but this estimate here on the right is to show you the cost of each instance you will be creating with this template okay so getting right into it i'm going to name this instance template bowtie template and since we're spinning up a lot of vms you want to be conscious on costs and so under series you're going to click on the drop down and you're going to select n1 and under machine type you're going to select f1 micro and this is the smallest instance type as well as the cheapest within google cloud you can go ahead and scroll down right to the bottom here under firewall you want to check off allow http traffic next you want to select management security disks networking and sold tenancy you scroll down a little bit and under startup script you're going to paste in the script that's available in the repo and you will find a link to this script and the repo in the lesson text and so you can leave all the other options as its default and simply click on create it's going to take a couple minutes here okay and the instance template is ready and so the next step that you want to do is create an instance group and as i said in a previous lesson in order to create an instance group you need an instance template hence why we made the instance template first okay and our instance template has been created and so now that you've created your instance template you can head on over to instance groups here in the left hand menu and as expected there are no instance groups and so you can go ahead and click on the big blue button and create an instance group you're going to make sure that new managed instance group stateless is selected and here you have the option of choosing a stateful instance group as well as an unmanaged instance group and so we're going to keep things stateless and so for the name of the instance group you can simply call this bowtie group i'm going to use the same name in the description and under location you want to check off multiple zones in under region you want to select us east one and if you click on configure zones you can see here that you can select all the different zones that's available in that region that you choose to have your instances in and so i'm going to keep it under all three zones i'm going to scroll down here a little bit and under instance template you should see bow tie template you can select that you can scroll down a little bit more and here under minimum number of instances you want to set the minimum number of instances to 3 and under maximum number of instances you want to set that to 6 and so this is going to be double the amount of the minimum number of instances so when you're scaled out you should have a maximum of 6 instances and when you're scaled in or you have very low traffic you should only have three instances so you can scroll down some more and under auto healing you want to select the health check and you're going to go ahead and create a new health check under name you can call this healthy bow ties i'm going to use the same for the description and i'm going to leave the rest as its default and go down and click on save and continue i'm going to scroll down some more and i'm going to leave the rest as is and simply click on create and it's going to take a couple minutes here and so i'm going to pause the video and i'll be back in a flash okay and my instance group has been created and so to get a better look at it i'm going to click on bow tie group and i can see here that three instances have been created if i go up to vm instances you can see here that i have three instances but under instance groups because i have health check enabled it shows that my instances are unhealthy and this is because i still need to create a firewall rule that will allow google's health check probes to reach my vm instances and so you're going to go ahead and create that firewall rule so you can bring the health check status up to healthy so i'm going to go over to the navigation menu and scroll down to vpc network and go over to firewall here under firewall as expected you have the default firewall rules from the default created vpc network and so i'm going to go up to create firewall and you can name this firewall rule allow health check i'm going to use the same for the description i'm going to scroll down here a little bit and under targets i'm going to select all instances in the network source filter i'm going to leave as i p ranges and so here under source i p ranges i want to enter in the ip addresses for the google cloud health check probes and you can find these in the documentation and i will also be supplying them in the instructions and there are two sets of ip addresses that need to be entered and just as a note you don't need to know this for the exam but it's always a good to know if you're ever adding health checks to any of your instances i'm going to scroll down a little bit to protocols and ports and under tcp i'm going to check it off and put in port 80. that's pretty much all you have to do here so whenever you entered all that information in you can simply click on create and so now i have a firewall rule that will allow health checks to be done and so it may take a minute or two but if i head back on over to my compute engine instances and go over to my instance groups i'll be able to see that all my instances are now healthy and so whenever you're creating instance groups and you're applying health checks this firewall rule is necessary so please be aware okay so now that we've created our instance templates we've created our instance groups and we created a firewall rule in order to satisfy health checks we can now move on to the next step which is creating the load balancer so i'm going to go back up to the navigation menu and i'm going to scroll down to network services and over to load balancing and as expected there are no load balancers created and so whenever you're ready you can click on the big blue button and create a new low balancer here you have the option of creating an http or https load balancer along with a tcp load balancer or a udp load balancer and because we're serving external traffic on port 80 we're going to use the http load balancer so you can click on start configuration and i'm being prompted to decide between internet facing or internal only and you're going to be accepting traffic from the internet to your load bouncer so make sure that from internet to my vms is checked off and simply click continue and so next you will be prompted with a page with a bunch of configurations that you can enter and so we'll get to that in just a second but first we need to name our load balancer and so i'm going to call this bowtie dash lb for low balancer and so next step for your load balancer is you need to configure a back end so you can click on back end configuration and here you have the option of selecting from backend services or backend buckets so you're going to go ahead and click on backend services and create a backend service and here you will be prompted with a bunch of fields to fill out in order to create your backend service and you can go ahead and name the backend service as bowtie backend service backend type is going to be instance group and you can leave the protocol named port and timeout as is as we're going to be using http under instance group in new backend if you select the dropdown you should see your available bow tie group instance group select that scroll down a little bit and under port numbers you can enter in port 80 and you can leave all the other options as default and simply click on done and so if you're ever interested you can always add a cache using cloud cdn now i know we haven't gone through cloud cdn in this course but just know that this is google's content delivery network and it uses google's global edge network to serve content closer to users and this accelerates your websites and your applications and delivers a better user experience for your user okay and moving on here under health check if i click on the drop down you should see healthy bow ties you can select that for your health check and so just as a note here under advanced configurations you can set your session affinity your connection draining timeout as well as request and response headers and so we don't need any of that for this demo and so i'm going to go ahead and collapse this and once you've finished filling in all the fields you can simply click on create okay and so you should now have your back end configuration and your host and path rules configured and so the only thing that's left to configure is the front end so you can go up and click on frontend configuration and you can name your frontend bowtie frontend service gonna keep the protocols http and here is where you would select the network service tier choosing either premium or standard and if you remember in the load balancing lesson in order to use this as a global load balancer i need to use a premium tier okay and we're going to keep this as ipv4 with an ephemeral ip address on port 80 so once you've finished configuring the front end you can simply click on done and you can go and click on review and finalize and this will give you a summary on your configuration and so i'm happy with the way everything's configured and if you are as well you can simply click on create and this may take a minute or two but it will create your low balancer along with your back end and your front end so again i'm going to pause the video here for just a minute and i'll be back before you can say cat in the hat okay and my load balancer has been created and to get a little bit more details i'm going to drill down into it and i can see here the details of my load balancer along with my monitoring and any caching but i don't have any caching enabled and therefore nothing is showing so going back to the details i can see here that i have a new ip address for my load balancer and i'll be getting into that in just a minute i'm going to go back here and i'm going to check out my back ends click on bow tie back end service and here i can see the requests per second as well as my configuration and if you do see this caution symbol here showing that some of your instances are unhealthy it's only because the low balancer needs time to do a full health check on all the instances in the instance group and so this will take some time okay and so i'm going to go back over and check out my front end and there's nothing to drill down into with the front end service but it does show me my scope the address the protocol network tier and the low balancer itself so this is the end of part one of this demo it was getting a bit long so i decided to break it up this would be a great opportunity for you to get up have a stretch get yourself a coffee or tea and whenever you're ready part two will be starting immediately from the end of part one so you can now mark this as complete and i'll see you in part two this is part two of the managing bow ties demo and we will be starting exactly where we left off in part one so with that being said let's dive in and so before you move forward you want to make sure that all your instances are considered healthy by your load balancer and as i can see here all my instances in my instance group are considered healthy by the load balancer and so just to verify this i'm going to go ahead and copy the i p address and you can open up a new tab in your browser and simply paste it in and success as you can see here managing the production of many bow ties can be automated but managing the wearer of them definitely cannot another fine message from the people at bow tie inc now although this is a simple web page i used a couple variables just to show you the low balancing that happens in the background and traffic will be load balanced in between all of the instances in the instance group so if you click on refresh then you should see the machine name and the data center change so every time i click refresh the traffic will be routed to a different instance in a different zone and so a simple simulation on how traffic is low balance between the different instances in their different zones okay so now that we've verified the website application i'm going to close down this tab and so now that we've created our instance template we've created our instance group and we've created our low balancer with the back end and front end service and it looks like everything seems to be working together nicely we're going to go ahead and simulate a scale out using auto scaling and so in order to simulate this we're going to do a stress test on one of the instances so i'm going to head back on over to the navigation menu scroll down to compute engine and here you can ssh into any one of these instances and run the stress test from there so i'm going to pick here the one at the top and so whenever you're logged in you can simply paste in the command that i've included in the instructions that will run the stress test and so this is a stress test application called stress that was included in the startup script and this again will put stress on the server itself and trigger a scale out to handle the load and it'll do this for 30 seconds so you can go ahead and hit enter and head back over to the console and in about a minute or two you should see some new instances that will be created by your instance group in order to handle the load okay and after about a couple minutes it's showing here that instances are being created and it will be scaling out to the maximum amount of instances that i've set it to which is six i'm going to drill down into this and yes a scale out is happening and some new instances are being created to handle the load so i'm going to give it just a minute here okay and as you can see here all the instances have been created they've been added to the instance group and all of them are marked as healthy and so just to verify that all the instances are working i'm going to go ahead and open up a new tab i'm going to plug in the ip address on my load balancer and i'm going to simply cycle through all these instances to make sure that all them are working and it looks like i have no issues and so now that you've simulated a scale out i wanted to go ahead and run a scale in and so i'm first going to close up these tabs now with regards to scaling there is a 10 minute stabilization period that cannot be adjusted for scaling and this is a builtin feature into google cloud now because i respect your time as a student i'm going to show you a work around to trigger a scale in sooner strictly for this demo and i also wanted to caution that this should never be done in a production or productionlike environment you should always wait for the scaling to happen on its own and never force it this method is being used strictly for learning purposes to save you some time and so i'm going to go ahead to the top menu and click on rolling restart and replace and this will bring up a new page where you will have the option to either restart or replace any instances in your instance group and so for your purposes under operation make sure that you have restart checked off and this will restart all of your instances and only bring up the ones that are needed so i'm going to go ahead and click on restart i'm going to go back to my instance group console and i'm just going to give this a few minutes to cook and i'll be right back in a flash okay so it looks like the instance group has scaled in and we are now down left to three instances the minimum that we configured for our instance group and so that pretty much covers the managing bow ties demo so i wanted to congratulate you on making it through this demo and i hope that this has been extremely useful in excelling your knowledge on managing instance templates managed instance groups and creating load balancers with backend and frontend services now this was a jampacked demo and there was a lot to pack in with everything you've learned from the last few lessons and so just as a recap you created an instance template with your startup script you then created a new instance group with a health check to go with it configuring auto scaling for a minimum of three instances you then created a firewall rule so that the health check probes were able to connect to the application and you then created a load balancer with its back end and frontend service and verified that the website application was indeed up and running you then ran a stress test to allow a simulation of a scale out of your instance group and then simulated a scale in of your instance group great job and so now that we've completed this demo you want to make sure that you're not accumulating any unnecessary costs and so i'm going to go ahead and walk you through the breakdown of deleting all these resources so first you're going to go ahead and delete the load balancer go back up to the navigation menu and scroll down to network services and go over to load balancing so i'm going to go ahead and check off bow tie lb and simply go up to the top and click on delete it's going to ask me if i'm sure i want to do this i'm also going to select bow tie back end service and i can delete my load balancer and my back end service all at once i'm going to go ahead and delete load balancer and the selected resources and this should clear up within a few seconds okay and our load balancer has been deleted i'm going to just go up here to the back end make sure everything's good yeah we're all clean same thing with front end and so now you can move on to instance groups so i'm going to head back up to the navigation menu go down a compute engine and go up to instance groups and here you can just simply check off bow tie group and simply click on delete you're going to be prompted with a notification to make sure you want to delete bow tie group yes i want to delete and again this should take about a minute okay it actually took a couple minutes but my instance group has been deleted and so now i'm going to go over to instance templates and i'm going to delete my template and check off bow tie template and simply click delete you're going to get a prompt to make sure you want to delete your instance template yes you want to delete and success you've now deleted all your resources although there is one more resource that you will not be billed for but since we're cleaning everything up we might as well clean that up as well and this is the firewall rule that we created and go over to the navigation menu and scroll down to vpc network i'm going to go to firewall here on the left hand menu and here i'm going to check off the allow health check firewall rule and simply click on delete i'm going to get a prompt to make sure that i want to delete it yes you want to delete i'm going to quickly hit refresh and yes we've deleted it and so this concludes the end of this demo so you can now mark this as complete and i'll see you in the next one welcome back in this next section we will be focusing on google cloud's premier container orchestration service called kubernetes but before we can dive right into kubernetes and the benefits that it gives to containers you'll need an understanding as to what containers are and what value containers provide in this lesson i will be covering the difference between virtual machines and containers what containers are how they work and the value proposition they bring so with that being said let's dive in now for those of you who didn't know container technology gets its name from the shipping industry products get placed into standardized shipping containers which are designed to fit into the ship that accommodates the container's standard size instead of having various sizes of packaging now by standardizing this process and keeping the items together the container can be moved as a unit and it costs less to do it this way as well the standardization allows for consistency when packing and moving the containers placing them on ships and docks as well as storage no matter where the container is it always stays the same size and the contents stay isolated from all the other containers that they are stacked with and so now before we get into the details of containers i wanted to cover how we got here and why so a great way to discuss containers is through their comparison to virtual machines now as we discussed in a previous lesson when it comes to vms the systems are virtualized through a hypervisor that sits on top of the underlying host infrastructure the underlying hardware is virtualized so that multiple operating system instances can run on the hardware each vm runs its own operating system and has access to virtualized resources representing the underlying hardware due to this process vms come with the cost of large overhead in cpu memory and disk as well can be very large due to the fact that each vm needs its own individual operating system there also lacks standardization between each vm making them unique due to the os configuration the software installed and the software libraries thus not making it very portable to be able to run in any environment now when dealing with containers things are run very differently the underlying host infrastructure is still there but instead of just using a hypervisor and abstracting the underlying hardware containerization takes it one step further and abstracts the operating system thus leaving the application with all of its dependencies in a neatly packaged standardized container this is done by installing the operating system on top of the host infrastructure and then a separate layer on top of the host operating system called the container engine now instead of having their own operating system the containers share the operating system kernel with other containers while operating independently running just the application code and the dependencies needed to run that application this allows each container to consume very little memory or disk making containers very lightweight efficient and portable containerized applications can start in seconds and many more instances of the application can fit onto the machine compared to a vm environment this container can now be brought over to other environments running docker and able to run without having the worries of running into issues of compatibility now although there are a few different container engines out there the one that has received the most popularity is docker and this is the engine that we will be referring to for the remainder of this course now a docker image is a collection or stack of layers that are created from sequential instructions on a docker file so each line in the dockerfile is run line by line and a unique readonly layer is written to the image what makes docker images unique is that each time you add another instruction in the docker file a new layer is created now going through a practical example here shown on the right is a docker file and we will be able to map each line of code to a layer shown on the docker image on the left the line marked from shows the base image that the image will be using the example shown here shows that the ubuntu image version 12.04 will be used next the run instruction is used which will perform a general update install apache 2 and output a message to be displayed that is written to the index.html file next up is the working directories and these are the environment variables set by using an env instruction and this will help run the apache runtime next layer is the expose instruction and this is used to expose the container's port on 8080 and lastly the command layer is an instruction that is executing the apache web server from its executable path and so this is a great example of how a docker file is broken down from each line to create the layers of this image and so just as a note here each docker image starts with a base image as well each line in a docker file creates a new layer that is added to the image and finally all the layers in a docker image are read only and cannot be changed unless the docker file is adjusted to reflect that change so now how do we get from a docker image to a container well a running docker container is actually an instantiation of an image so containers using the same image are identical to each other in terms of their application code and runtime dependencies so i could use the same image for multiple copies of the same container that have different tasks what makes each individual container different is that running containers include a writable layer on top of the readonly content runtime changes including any rights and updates to data and files are saved in this read write layer so in this example when using the command docker run fashionista a docker container will be instantiated from the docker image and a read write layer is always added on top of the readonly layers when a container is created writing any necessary files that's needed for the application and so just as a note here docker containers are always created from docker images and containers can use the same image yet will always have a different read write layer no matter the amount of containers running on a given host so now when your containers have been created you need a place to store them and so this is where a container registry comes into play now a container registry is a single place for you to store and manage docker images now when you create your docker file and then build your image you want to store that image in a central image repository whether it be a private one or a public one a popular public container registry is docker hub and this is a common registry where many open source images can be found including those used for the base layer images like the ubuntu example that i showed you earlier and so once you have your containers in a container registry you need to be able to run these containers so in order to run these containers you need docker hosts and these can consist of any machine running the docker engine and this could be your laptop server or you can run them in provided hosted cloud environments now this may have been a refresher for some but for those of you who are new to containers i hope this has given you a lot more clarity on what containers are what they do and the value that they bring to any environment and so that's pretty much all i wanted to cover on this short lesson of an introduction to containers so you can now mark this lesson as complete and let's move on to the next one welcome back so now that you've gotten familiar with what containers are and how they work i wanted to dive into google cloud's platform as a service offering for containers called google kubernetes engine also known as short as gke now although the exam goes into a more operational perspective with regards to gke knowing the foundation of kubernetes and the different topics of kubernetes is a must in order to understand the abstractions that take place with gke from regular kubernetes in this lesson i will be getting into key topics with regards to kubernetes and we'll be touching on the architecture components and how they all work together to achieve the desired state for your containerized workloads now there's a lot to get into so with that being said let's dive in now before i can get into gke i need to set the stage on explaining what kubernetes is put simply kubernetes is an orchestration platform for containers which was invented by google and eventually open source it is now maintained by the cncf short for the cloud native computing foundation and has achieved incredible widespread adoption kubernetes provides a platform to automate schedule and run containers on clusters of physical or virtual machines thus eliminating many of the manual processes involved in deploying and scaling containerized applications kubernetes manages the containers that run the applications and ensure that there is no downtime in a way that you the user can define for example if you define that when a container goes down and another container needs to start kubernetes would take care of that for you automatically and seamlessly kubernetes provides you with the framework to run distributed systems resiliently it takes care of scaling and failover for your application provides deployment patterns and allows you to manage your applications with tons of flexibility reliability and power it works with a range of container tools including docker now although this adoption was widespread it did come with its various challenges this included scaling at cd load balancing availability auto scaling networking rollback on faulty deployments and so much more so now google cloud has since developed a managed offering for kubernetes providing a managed environment for deploying managing and scaling your containerized applications using google infrastructure the gke environment consists of compute engine instances grouped together to form a cluster and it provides all the same benefits as onpremises kubernetes yet has abstracted the complexity of having to worry about the hardware and to top it off it has the benefits of advanced cluster management features that google cloud provides with things like cloud load balancing and being able to spread traffic amongst clusters and nodes node pools to designate subnets of nodes within a cluster for additional flexibility automatic scaling of your cluster's node instance count and automatic upgrades for your clusters node software it also allows you to maintain node health and availability with node auto repair and takes care of logging and monitoring with google cloud's operation suite for visibility into your cluster so as you can see here gke holds a lot of benefits when it comes to running kubernetes in google cloud so i wanted to take a moment now to dive into the cluster architecture and help familiarize you with all the components involved in a cluster so a cluster is the foundation of google kubernetes engine and kubernetes as a whole the kubernetes objects that represent your containerized applications all run on top of the cluster in gke a cluster consists of at least one control plane and multiple worker machines called nodes the control plane and node machines run the kubernetes cluster the control plane is responsible to coordinate the entire cluster and this can include scheduling workloads like containerized applications and managing the workload's life cycle scaling and upgrades the control plane also manages network and storage resources for those workloads and most importantly it manages the state of the cluster and make sure it is at the desired state now the nodes are the worker machines that run your containerized applications and other workloads the nodes are compute engine vm instances that gke creates on your behalf when you create a cluster each node is managed from the control plane which receives updates on each node's selfreported status a node also runs the services necessary to support the docker containers that make up your cluster's workloads these include the docker runtime and the kubernetes node agent known as the cubelet which communicates with the control plane and is responsible for starting and running docker containers scheduled on that node now diving deeper into the architecture there are components within the control plane and nodes that you should familiarize yourself with as these components are what ties the cluster together and helps manage the orchestration as well as the state now the control plane is the unified endpoint for your cluster the control plane's components make global decisions about the cluster for example scheduling as well as detecting and responding to cluster events all interactions with the cluster are done via kubernetes api calls and the control plane runs the kubernetes api server process to handle those requests you can make kubernetes api calls directly via http or grpc or can also be done indirectly by running commands from the kubernetes command line client called cubectl and of course you can interact with the ui in the cloud console the api server process is the hub for all communications for the cluster moving on to the next component is cube scheduler the cube scheduler is a component that discovers and assigns newly created pods to a node for them to run on so any new pods that are created will automatically be assigned to its appropriate node by the cube scheduler taking into consideration any constraints that are in place next up is the cube controller manager and this is the component that runs controller processes and is responsible for things like noticing and responding when nodes go down maintaining the correct number of pods populating the services and pods as well as creating default accounts and api access tokens for new namespaces it is these controllers that will basically look to make changes to the cluster when the current state does not meet the desired state now when it comes to the cloud controller manager this is what embeds cloudspecific control logic the cloud controller manager lets you link your cluster into any cloud providers api and separates out the components that interact with that cloud platform from components that just interact with your cluster the cloud controller manager only runs controllers that are specific to your cloud provider in this case google cloud and lastly we have fcd and this component is responsible to store the state of the cluster at cd is a consistent and highly available key value store that only interacts with the api server it saves all the configuration data along with what nodes are part of the cluster and what pods they are running so now the control plane needs a way to interact with the nodes of the cluster thus the nodes having components themselves for this communication to occur this component is called a cubelet and this is an agent that runs on each node in the cluster that communicates with the control plane it is responsible for starting and running docker containers scheduled on that node it takes a set of pod specs that are provided to it and ensures that the containers described in those pod specs are running and healthy and i will be diving into pod specs in a later lesson next up is cube proxy and this is the component that maintains network connectivity to the pods in a cluster and lastly the container runtime is the software that is responsible for running containers kubernetes supports container runtimes like docker and container d and so these are the main components in a cluster covering the control plane and nodes with regards to communication within the cluster now before i end this lesson there is one more topic i wanted to touch on with regards to the architecture of a gke cluster and that is the abstraction that happens and what exactly does gke manage with regards to kubernetes well gke manages all the control plane components the endpoint exposes the kubernetes api server that cubectl uses to communicate with your cluster control plane the endpoint exposes the kubernetes api server that cubectl uses to communicate with your cluster control plane the endpoint ip is displayed in cloud console and this ip will allow you to interact with the cluster when you run the command gcloud container clusters get dash credentials you see that the command gets the cluster endpoint as part of updating cubeconfig an ip address for the cluster is then exposed to interact with and is responsible for provisioning and managing all the infrastructure that is needed for the control plane gke also automates the kubernetes nodes by launching them as compute engine vms under the hood but still allows the user to change the machine type and access upgrade options by default google kubernetes engine clusters and node pools are upgraded automatically by google but you can also control when auto upgrades can and cannot occur by configuring maintenance windows and exclusions and just as a note a clusters control plane and nodes do not necessarily run the same version at all times and i will be digging more into that in a later lesson and so i know this is a lot of theory to take in but is as i said before a necessity to understanding kubernetes and gke and as we go further along into kubernetes and get into demos i promise that this will start to make a lot more sense and you will start becoming more comfortable with gke and the underlying components of kubernetes knowing kubernetes is a must when working in any cloud environment as it is a popular and growing technology that is not slowing down so knowing gke will put you in a really good position for your career as an engineer in google cloud as well will give you a leg up on diving into other cloud vendors implementation of kubernetes and so that's pretty much all i wanted to cover when it comes to google kubernetes engine and kubernetes so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i will be covering cluster and node management in gke as it refers to choosing different cluster types for your workloads cluster versions node pools as well as upgrades and the many different options to choose from it is good to familiarize yourself with these options as they may be the deciding factor of having to keep your workloads highly available and your tolerance to risk within your environment so with that being said let's dive in now in the last lesson we touched on nodes and how they are the workers for the kubernetes cluster so now that you are familiar with nodes i wanted to touch on a concept that builds on it called node pools now a node pool is a group of nodes within a cluster that all have the same configuration and using node config specification to achieve this a node pool can also contain one or multiple nodes when you first create a cluster the number and type of nodes that you specify becomes the default node pool as shown here in the diagram then you can add additional custom node pools of different sizes and types to your cluster all nodes in any given node pool are identical to one another now custom node pools are really useful when you need to schedule pods that require more resources than others such as more memory more disk space or even different machine types you can create upgrade and delete node pools individually without affecting the whole cluster and just as a note you cannot configure a single node in any node pool any configuration changes affect all nodes in the node pool and by default all new node pools run the latest stable version of kubernetes existing node pools can be manually upgraded or automatically upgraded you can also run multiple kubernetes node versions on each node pool in your cluster update each node pool independently and target different node pools for specific deployments in that node now with gke you can create a cluster tailored to your availability requirements and your budget the types of available clusters include zonal both single zone or multizonal and regional zonal clusters have a single control plane in a single zone depending on what kind of availability you want you can distribute your nodes for your zonal cluster in a single zone or in multiple zones now when you decide to deploy a single zone cluster it again has a single control plane running in one zone this control plane manages workloads on nodes running in the same zone a multizonal cluster on the other hand has a single replica of the control plane running in a single zone and has nodes running in multiple zones during an upgrade of the cluster or an outage of the zone where the control plane runs workloads still run however the cluster its nodes and its workloads cannot be configured until the control plane is available multizonal clusters are designed to balance availability and cost for consistent workloads and just as a note the same number of nodes will be deployed to each selected zone and may cost you more than budgeted so please be aware and of course when you're looking to achieve high availability for your cluster regional clusters are always the way to go a regional cluster has multiple replicas of the control plane running in multiple zones within a given region nodes also run in each zone where a replica of the control plane runs because a regional cluster replicates the control plane and nodes it consumes more compute engine resources than a similar single zone or multizonal cluster the same number of nodes will be deployed to each selected zone and the default when selecting regional clusters is three zones now if you're dealing with more sensitive workloads that require more strict guidelines private clusters give you the ability to isolate nodes from having inbound and outbound connectivity to the public internet this isolation is achieved as the nodes have internal ip addresses only if you want to provide outbound internet access for certain private nodes you can use cloudnat or manage your own nat gateway by default private google access is enabled in private clusters and their workloads with limited outbound access to google cloud apis and services over google's private network in private clusters the control plane's vpc network is connected to your clusters vpc network with vpc network peering your vpc network contains the cluster nodes and a separate google cloud vpc network contains your cluster's control plane the control plane's vpc network is located in a project controlled by google traffic between nodes and the control plane is routed entirely using internal ip addresses the control plane for a private cluster has a private endpoint in addition to a public endpoint the control plane for a nonprivate cluster only has a public endpoint the private endpoint is an internal ip address in the control plane's vpc network the public endpoint is the external ip address of the control plane and you can control access to this endpoint using authorized networks or you can disable access to the public endpoint as shown here in the diagram you can disable the public endpoint and connect to your network using an internal ip address using cloud interconnect or cloud vpn and you always have the option of enabling or disabling this public endpoint now when you create a cluster you can choose the cluster specific kubernetes version or you can mix the versions for flexibility on features either way it is always recommended that you enable auto upgrade for the cluster and its nodes now when you have auto upgrade enabled you are given the choice to choose from what are called release channels when you enroll a new cluster in a release channel google automatically manages the version and upgrade cadence for the cluster and its node pools all channels offer supported releases of gke and are considered in general availability you can choose from three different release channels for automatic management of your cluster's version and upgrade cadence as shown here the available release channels are rapid regular and stable release channels the rapid release channel gets the latest kubernetes release as early as possible and be able to use new gka features the moment that they go into general availability with the regular release channel you have access to gke and kubernetes features reasonably soon after they are released but on a version that has been qualified two to three months after releasing in the rapid release channel and finally we have the stable release channel where stability is prioritized over new functionality changes and new versions in this channel are rolled out last after being validated two to three months in the regular release channel and so if you're looking for more direct management of your cluster's version choose a static version when you enroll a cluster in a release channel that cluster is upgraded automatically when a new version is available in that channel now if you do not use a release channel or choose a cluster version the current default version is use the default version is selected based on usage and real world performance and is changed regularly while the default version is the most mature one other versions being made available are generally available versions that pass internal testing and qualification changes to the default version are announced in a release note now if you know that you need to use a specific supported version of kubernetes for a given workload you can specify it when creating the cluster if you do not need to control the specific patch version you use consider enrolling your cluster in a release channel instead of managing its version directly now when it comes to upgrading the cluster please be aware that control plane and nodes do not always run the same version at all times as well a control plane is always upgraded before its nodes when it comes to zonal clusters you cannot launch or edit workloads during that upgrade and with regional clusters each control plane is upgraded one by one as well with control planes auto upgrade is enabled by default and this is google cloud's best practice now again if you choose you can do a manual upgrade but you cannot upgrade the control plane more than one minor version at a time so please be aware as well with any cluster upgrades maintenance windows and exclusions are available and so this way you can choose the best times for your upgrades and so like cluster upgrades by default a clusters nodes have auto upgrade enabled and it is recommended that you do not disable it again this is best practice by google cloud and again like the cluster upgrades a manual upgrade is available and maintenance windows and exclusions are available for all of these upgrades now when a no pool is upgraded gke upgrades one node at a time while a node is being upgraded gke stops scheduling new pods onto it and attempts to schedule its running pods onto other nodes the node is then recreated at the new version but using the same name as before this is similar to other events that recreate the node such as enabling or disabling a feature on the node pool and the upgrade is only complete when all nodes have been recreated and the cluster is in the desired state when a newly upgraded node registers with the control plane gke marks the node as schedulable upgrading a no pool may disrupt workloads running in that pool and so in order to avoid this you can create a new node pool with the desired version and migrate the workload then after migration you can delete the old node pool now surge upgrades let you control the number of nodes gke can upgrade at a time and control how disruptive upgrades are to your workloads you can change how many nodes gke attempts to upgrade at once by changing the surge upgrade parameters on a no pool surge upgrades reduce disruption to your workloads during cluster maintenance and also allow you to control the number of nodes upgraded in parallel surge upgrades also work with the cluster auto scaler to prevent changes to nodes that are being upgraded now surge upgrade behavior is determined by two settings max surge upgrade and max unavailable upgrade now with max surge upgrade this is the number of additional nodes that can be added to the no pool during an upgrade increasing max surge upgrade raises the number of nodes that can be upgraded simultaneously and when it comes to the max unavailable upgrade this is the number of nodes that can be simultaneously unavailable during an upgrade increasing max unavailable upgrade raises the number of nodes that can be upgraded in parallel so with max surge upgrade the higher the number the more parallel upgrades which will end up costing you more money with max unavailable upgrade the higher the number the more disruptive it is and so the more risk you are taking and so during upgrades gke brings down at most the sum of the max surge upgrade added with the max unavailable upgrade so as you can see here there are a slew of options when it comes to deciding on the type of cluster you want as well as the type of upgrades that are available along with when you want them to occur and so your deciding factor in the end will be the workload that you are running and your risk tolerance and this will play a big factor in keeping up time for your cluster as well as saving money in any type of environment and so that's pretty much all i wanted to cover when it comes to gke cluster and node management so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i will be diving into some more theory within kubernetes and gke this time touching on objects and how objects are managed pods are only one type of object but there are many other parts that are involved in the management of these objects and this is what this lesson is set out to teach you now there's quite a bit to cover here so with that being said let's dive in now kubernetes objects are persistent entities in kubernetes kubernetes uses these entities to represent the state of your cluster for example it can describe things like what containerized applications are running and on which nodes and what resources are available to those applications a kubernetes object is a record of intent once you create the object kubernetes will constantly work to ensure that object exists by creating an object you're effectively telling kubernetes what you want your cluster's workload to look like and this is your cluster's desired state and you've heard me speak about this many times before and this is what i was referring to now almost every kubernetes object includes two nested object fields that govern the object's configuration the object spec and the object's status for objects that have a spec you have to set this when you create the object providing a description of the characteristics you want the resource to have its desired state the status describes the current state of the object supplied and updated by kubernetes and its components the kubernetes control plane continually and actively manages every object's actual state to match the desired state you supplied now each object in your cluster has a name that is unique for that type of resource every kubernetes object also has a uid that is unique across your whole cluster only one object of a given kind can have a given name at a time however if you delete the object you can make a new object with that same name every object created over the whole lifetime of a kubernetes cluster has a distinct uid these distinct uids are also known as uuids which we discussed earlier on in the course now when creating updating or deleting objects in kubernetes this is done through the use of a manifest file where you would specify the desired state of an object that kubernetes will maintain when you apply the manifest each configuration file can contain multiple manifests and is common practice to do so when possible a manifest file is defined in the form of a yaml file or a json file and it is recommended to use yaml now in each yaml file for the kubernetes object that you want to create there are some required values that need to be set the first one is the api version and this defines which version of the kubernetes api you're using to create this object the kind described in this example as a pod is the kind of object you want to create next up is the metadata and this is the data that helps uniquely identify the object including a string name a uid and an optional namespace and the last required value is the spec and this is what state you desire for the object and the spec in this example is a container by the name of bow tie dash web server and is to be built with the latest nginx web server image as well as having port 80 open on the container now when it comes to objects pods are the smallest most basic deployable objects in kubernetes a pod represents a single instance of a running process in your cluster pods contain one or more containers such as docker containers and when a pod runs multiple containers the containers are managed as a single entity and share the pods resources which also includes shared networking and shared storage for their containers generally one pod is meant to run a single instance of an application on your cluster which is selfcontained and isolated now although a pod is meant to run a single instance of your application on your cluster it is not recommended to create individual pods directly instead you generally create a set of identical pods called replicas to run your application a set of replicated pods are created and managed by a controller such as a deployment controllers manage the life cycle of their pods as well as performing horizontal scaling changing the number of pods is necessary now although you might occasionally interact with pods directly to debug troubleshoot or inspect them it's recommended that you use a controller to manage your pods and so once your pods are created they are then run on nodes in your cluster which we discussed earlier the pod will then remain on its node until its process is complete the pot is deleted the pod is evicted from the node due to lack of resources or the node fails if a node fails pods on the node are automatically scheduled for deletion now a single gke cluster should be able to satisfy the needs of multiple users or groups of users and kubernetes namespaces help different projects teams or customers to share a kubernetes cluster you can think of a namespace as a virtual cluster inside of your kubernetes cluster and you can have multiple namespaces logically isolated from each other they can help you and your teams with organization and security now you can name your namespaces whatever you'd like but kubernetes starts with four initial namespaces the first one is the default namespace and this is for objects with no other namespace so when creating new objects without a namespace your object will automatically be assigned to this namespace cube dash system is the next one and these are for objects created by kubernetes cubepublic is created automatically and is readable by all users but is mostly reserved for cluster usage in case that some resources should be visible and readable publicly throughout the whole cluster and finally cube node lease is the namespace for the lease objects associated with each node which improves the performance of the node heartbeats as the cluster scales and so like most resources in google cloud labels are key value pairs that help you organize your resources in this case kubernetes objects labels can be attached to objects at creation time and can be added or modified at any time each object can have a set of key value labels defined and each key must be unique for a given object and labels can be found under metadata in your manifest file and so the one thing to remember about pods is that they are ephemeral they are not designed to run forever and when a pod is terminated it cannot be brought back in general pods do not disappear until they are deleted by a user or by a controller pods do not heal or repair themselves for example if a pod is scheduled on a node which later fails the pod is deleted as well if a pod is evicted from a node for any reason the pod does not replace itself and so here is a diagram of a pod life cycle that shows the different phases of its running time to give you some better clarity of its ephemeral nature when first creating the pod the pod will start impending and this is the pod's initial phase and is waiting for one or more of the containers to be set up and made ready to run this includes the time a pod spends waiting to be scheduled as well as the time spent downloading container images over the network once the pod has completed the pending phase it is moved on to be scheduled and once it is scheduled it will move into the running phase and this is the phase where the pod has been bound to a node and all of the containers have been created the running phase has at least one container in the pod running or is in the process of starting or restarting and once the workload is complete the pod will move into the succeeded phase and this is where all the containers in the pod have terminated in success and will not be restarted now if all the containers in the pod have not terminated successfully the pod will move into a failed phase and this is where all the containers in the pod have terminated and at least one container has terminated in failure now there's one more phase in the pod life cycle that i wanted to bring up which is the unknown phase and this is the state of the pod that could not be obtained this phase typically occurs due to an error in communicating with the node where the pod should be running so now when you're creating pods using a deployment is a common way to do this a deployment runs multiple replicas of your application and automatically replaces any instances that fail or become unresponsive deployments help ensure that one or more instances of your application are available to serve user requests deployments use a pod template which contains a specification for its pods the pod specification determines how each pod should look like for instance what applications should run inside its containers which volumes the pods should mount its labels and more and so when a deployments pod template is changed new pods are automatically created one at a time now i wanted to quickly bring up replica sets for just a moment you'll hear about replica sets and i wanted to make sure that i covered it replica sets ensures that a specified number of pod replicas are running at any given time however a deployment is a higher level concept that manages replica sets and provides updates to pods along with other features and so using deployments is recommended over using replica sets unless your workload requires it and i will be including a link to replica sets in the lesson text so speaking of workloads in kubernetes workloads are objects that set deployment rules four pods based on these rules kubernetes performs the deployment and updates the workload with the current state of the application workloads let you define the rules for application scheduling scaling and upgrading now deployments which we just discussed is a type of workload and as we've seen a deployment runs multiple replicas of your application and automatically replaces any instances that fail or become unresponsive deployments are best used for stateless applications another type of workload is stateful sets and in contrast to deployments these are great for when your application needs to maintain its identity and store data so basically any application that requires some sort of persistent storage daemon sets is another common workload that ensures every node in the cluster runs a copy of that pod and this is for use cases where you're collecting logs or monitoring node performance now jobs is a workload that launches one or more pods and ensures that a specified number of them successfully terminate jobs are best used to run a finite task to completion as opposed to managing an ongoing desired application state and cron jobs are similar to jobs however cron jobs runs to completion on a cronbased schedule and so the last workload that i wanted to cover are config maps and these store general configuration information and so after you upload a config map any workload can reference it as either an environment variable or a volume mount and so just as a note config maps are not meant to store sensitive data if you're planning to do this please use secrets now i know this lesson has been extremely heavy in theory but these are fundamental concepts to know when dealing with kubernetes and gke as well as the objects that it supports so i recommend that if you need to go back and review this lesson if things aren't making sense so that you can better understand it as these concepts all tie in together and will come up in the exam and so that's pretty much all i wanted to cover in this lesson on pods and object management within gke so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i'm going to be diving into kubernetes services now services are a major networking component when it comes to working in kubernetes and can play a major factor when it comes to deciding on how you want to route your traffic within your kubernetes cluster as well in my experience services show up on the exam and so an understanding of how they work and the different types to use are essential to understanding the big picture of kubernetes this lesson will cover an overview on what services are what they do and the different types that are available along with their use cases now there's a lot to cover here so with that being said let's dive in now as i had discussed earlier kubernetes pods are ephemeral pods are created and destroyed to match the state of your cluster so these resources are never permanent a perfect example of this is by using a deployment object so you can create and destroy pods dynamically now when it comes to networking in kubernetes each pod gets its own ip address however in a deployment a pod that is running once destroyed will be recreated with a new ip address and there is no real way to keep track of these i p addresses for communication as they change very frequently and this is where services come into play now a service is an abstraction in the sense that it is not a process that listens on some network interface a service can be defined as a logical set of pods an abstraction on top of the pod which provides a single persistent ip address and dns name by which pods can be accessed it allows for routing external traffic into your kubernetes cluster and used inside your cluster for more intelligent routing with services it is also very easy to manage load balancing configuration for traffic between replicas it helps pods scale quickly and easily as the service will automatically handle the recreation of pods and their new ip addresses the main goal of services in kubernetes is to provide persistent access to its pods without the necessity to look for a pod's ip each time when the pod is recreated and again services also allow for external access from users to the applications inside the cluster without having to know the ip address of the individual pod in order to reach that application now in order for a service to route traffic to the correct pod in the cluster there are some fields in the manifest file that will help determine the end points on where traffic should be routed shown here on the right is the deployment manifest for reference and on the left is the services manifest now as you can see here in the service manifest on the left the kind is clearly defined as service under metadata is the name of the service and this will be the dns name of the service when it is created so when it comes to the spec there is a field here called a selector and this is what defines what pods should be included in the service and it is the labels under the selector that define which pods and labels are what we discussed in the last lesson as arbitrary key value pairs so any pod with these matching labels is what will be added to the service as shown here in the deployment file this workload will be a part of the service and its labels match that of the selector in the services file for type this is the type of service that you will want to use in this example type cluster ip is used but depending on the use case you have a few different ones to choose from now at the bottom here is a list of port configurations protocol being the network protocol to use with the port port being the port that incoming traffic goes to and finally the target port which is the port on the pod that traffic should be sent to and this will make more sense as we go through the upcoming diagrams so touching on selectors and labels for a moment kubernetes has a very unique way of routing traffic and when it comes to services it's not any different services select pods based on their labels now when a selector request is made to the service it selects all pods in the cluster matching the key value pair under the selector it chooses one of the pods if there are more than one with the same key value pair and forwards the network request to it and so here in this example you can see that the selector specified for the service has a key value pair of app inventory you can see the pod on node 1 on the left holds the label of app inventory as well which matches the key value pair of the selector and so traffic will get routed to that pod because of it if you look at the label for the pod in node 2 on the right the label does not match that of the selector and so it will not route traffic to that pod and so to sum it up the label on the pod matching the selector in the service determines where the network request will get routed to and so now i will be going through the many different service types that are available for routing network traffic within gke starting with cluster ip now a cluster ip service is the default kubernetes service it gives you a service inside your cluster that other apps inside your cluster can access the service is not exposed outside the cluster but can be addressed from within the cluster when you create a service of type cluster ip kubernetes creates a stable ip address that is accessible from nodes in the cluster clients in the cluster call the service by using the cluster ip address and the port value specified in the port field of the service manifest the request is forwarded to one of the member pods on the port specified in the target port field and just as a note this ip address is stable for the lifetime of the service so for this example a client calls the service at 10.176 on tcp port 80. the request is forwarded to one of the member pods on tcp port 80. note that the member pod must have a container that is listening on tcp port 80. if there is no container listening on port 80 clients will see a message like fail to connect or this site can't be reached think of the case when you have a dns record that you don't want to change and you want the name to resolve to the same ip address or you merely want a static ip address for your workload this would be a great use case for the use of the cluster ip service now although the service is not accessible by network requests outside of the cluster if you need to connect to the service you can still connect to it with the cloud sdk or cloud shell by using the exposed ip address of the cluster and so i wanted to take a moment to show you what a cluster ip manifest actually looks like and i will be going through the manifest for each service type for you to familiarize yourself with we first have the name of the service which is cluster ip dash service we then have the label used for the selector which is the key value pair of app inventory and then we have the service type which is cluster ip and we have the port number exposed internally in the cluster which is port 80 along with the target port that containers are listening on which again is port 80. and so the next service type we have is node port so when you create a service of type node port you specify a node port value the node port is a static port and is chosen from a preconfigured range between 30 000 and 32 760 you can specify your own value within this range but please note that any value outside of this range will not be accepted by kubernetes as well if you do not choose a value a random value within the range specified will be assigned once this port range has been assigned to the service then the service is accessible by using the ip address of any node along with the no port value the service is then exposed on a port on every node in the cluster the service can then be accessed externally at the node ip along with the node port when using node port services you must make sure that the selected port is not already open on your nodes and so just as a note the no port type is an extension of the cluster i p type so a service of type node port naturally has a cluster i p address and so this method isn't very secure as it opens up each node to external entry as well this method relies on knowing the ip addresses of the nodes which could change at any time and so going through the manifest of type node port service we start off with the name of the service which is node port dash service the label used for the selector which uses the key value pair of app inventory the type which is node port and notice the case sensitivity here which you will find in most service types along with the port number exposed internally in the cluster which is port 80 and again the port that the containers are listening on which is the target port which is port 80 as well and lastly and most importantly we have the no port value which is marked as you saw in the diagram earlier as port 32002 the next service type we have up is low balancer and this service is exposed as a load balancer in the cluster low balancer services will create an internal kubernetes service that is connected to a cloud provider's load balancer and in this case google cloud this will create a static publicly addressable ip address and a dns name that can be used to access your cluster from an external source the low balancer type is an extension of the no port type so a service of type load balancer naturally has a cluster ip address if you want to directly expose a service this is the default method all traffic on the port you specify will be forwarded to the service there is no filtering or routing and it means you can send many different types of traffic to it like http https tcp or udp and more the downside here is that for each service you expose with a low balancer you pay for that load balancer and so you can really rack up your bill if you're using multiple load balancers and shown here is the manifest for type load balancer it shows the name of the service load balancer dash service the label which is used for the selector which is the key value pair of app inventory the service type which is low balancer again notice the case sensitivity along with the port and the target port which are both port 80. and so this is the end of part one of this lesson it was getting a bit long so i decided to break it up this would be a great opportunity for you to get up and have a stretch get yourself a coffee or tea and whenever you're ready part two will be starting immediately from the end of part one so go ahead and mark this as complete and i'll see you in the next one welcome back this is part two of the kubernetes services lesson and we're going to continue immediately from the end of part one so whenever you're ready let's dive in and so the next service type we have is multiport services now for some services there is the need to expose more than one port kubernetes lets you configure multiple port definitions on a service object so when using multiple ports for a service you must give all your ports names and if you have multiple service ports these names must be unique in this example if a client calls the service at 10.176.1 on tcp port 80 the request is forwarded to a member pod on tcp port 80 on either node 1 or node 2. but if a client calls the service at 10.176.133.7 on tcp port 9752 the request is forwarded to the pod on tcp port 9752 that resides on node 1. each member pod must have a container listening on tcp port 80 and a container listening on tcp port 9752 this could be a single container with two threads or two containers running in the same pod and of course as shown here is a manifest showing the multiport services the name of the service the label used for the selector as well as the service type the port node exposed internally for each separate workload as well as the port that containers are listening on for each workload as well and as you saw before nginx was using target port 80 where appy was using port 9752 moving on to another service type is external name now a service of type external name provides an internal alias for an external dns name internal clients make requests using the internal dns name and the requests are redirected to the external name when you create a service kubernetes creates a dns name that internal clients can use to call the service in this example the internal dns name is bowtie.sql when an internal client makes a request to the internal dns name of bowtie.sql the request gets redirected to bowtie.sql2 dot bow tie inc dot private the external name service type is a bit different than other service types as it's not associated with a set of pods or an ip address it is a mapping from an internal dns name to an external dns name this service does a simple cname redirection and is a great use case for any external service that resides outside of your cluster and again here is a view of a manifest for type external name here showing the internal dns name along with the external dns name redirect and moving on to the last service type we have the headless service type now sometimes you don't need or want low balancing and a single service ip in this case you can create headless services by specifying none as the service type in the manifest file this option also allows you to choose other service discovery mechanisms without being tied to kubernetes implementation applications can still use a selfregistration pattern with this service and so a great use case for this is when you don't need any low balancing or routing you only need the service to patch the request to the back end pod no ips needed headless service is typically used with stateful sets where the name of the pods are fixed this is useful in situations like when you're setting up a mysql cluster where you need to know the name of the master and so here is a manifest for the headless service again the service type is marked as none and so to sum it up kubernetes services provides the interfaces through which pods can communicate with each other they also act as the main gateway for your application services use selectors to identify which pods they should control they expose an ip address and a port that is not necessarily the same port at which the pod is listening and services can expose more than one port and can also route traffic to other services external ip addresses or dns names services make it really easy to create network services in kubernetes each service can be backed with as many pods as needed without having to make your code aware of how each service is backed also please note that there are many other features and use cases within the services that have been mentioned that i've not brought up i will also include some links in the lesson text for those who are interested in diving deeper into services this lesson was to merely summarize the different service types and knowing these service types will put you in a great position on the exam for any questions that cover services within gke now i know this has been another lesson that's been extremely heavy in theory and has been a tremendous amount to take in but not to worry next up is a demo that will put all this theory into practice and we'll be going ahead and building a cluster along with touching on much of the components discussed within the past few lessons and so that's pretty much all i wanted to cover when it comes to kubernetes service types so you can now mark this lesson as complete and whenever you're ready join me in the console welcome back in this lesson i'll be going over ingress for gke an object within gke that defines rules for routing traffic to specific services ingress is a wellknown topic that comes up in the exam as well as being a common resource that is used in many gke clusters that you will see in most environments something that you will get very familiar with while diving deeper into more complex environments so whenever you're ready let's dive in now in gke an ingress object defines rules for routing http and https traffic to applications running in a cluster an ingress object is associated with one or more service objects each of which is associated with a set of pods when you create an ingress object the gke ingress controller creates a google cloud http or https load balancer and configures it according to the information in the ingress and its associated services gke ingress is a builtin and managed ingress controller this controller implements ingress resources as google cloud load balancers for http and https workloads in gke also the load balancer is given a stable ip address that you can associate with a domain name each external http and https load balancer or internal http or https load balancer uses a single url map which references one or more backend services one backend service corresponds to each service referenced by the ingress in this example assume that you have associated the load balancers ip address with the domain name bowtieinc.co when a client sends a request to bowtieinc.co the request is routed to a kubernetes service named products on port 80. and when a client sends a request to bowtieinc.co forward slash discontinued the request is routed to a kubernetes service named discontinued on port 21337 ingress is probably the most powerful way to expose your services but can also be very complex as there are also many types of ingress controllers to choose from along with plugins for ingress controllers ingress is the most useful and cost effective if you want to expose multiple services under the same ip address as you only pay for one load balancer if you are using the native gcp integration and comes with a slew of features and so shown here is the ingress manifest which is a bit different from the other manifest that you've seen as it holds rules for different paths explain in the previous diagram in the manifest shown here one path directs all traffic to the product's service name while the other path redirects traffic from discontinued to the back end service name of discontinued and note that each of these service names have their own independent manifest as it is needed to create the service and are referenced within the ingress manifest so the more rules you have for different paths or ports the more services you will need now i wanted to touch on network endpoint groups or any g's for short for just a second now this is a configuration object that specifies a group of backend endpoints or services negs are useful for container native load balancing where each container can be represented as an endpoint to the load balancer the negs are used to track pod endpoints dynamically so the google low balancer can route traffic to its appropriate back ends so traffic is low balanced from the load balancer directly to the pod ip as opposed to traversing the vm ip and coupe proxy networking in these conditions services will be annotated automatically indicating that a neg should be created to mirror the pod ips within the service the neg is what allows compute engine load balancers to communicate directly with pods the diagram shown here is the ingress to compute engine resource mappings of the manifest that you saw earlier where the gke ingress controller deploys and manages compute engine low balancer resources based on the ingressed resources that are deployed in the cluster now touching on health checks for just a minute if there are no specified health check parameters for a corresponding service using a backend custom resource definition a set of default and inferred parameters are used health check parameters for a backend service should be explicitly defined by creating a backend config custom resource definition for the service and this should be done if you're using anthos a backend config custom resource definition should also be used if you have more than one container in the serving pods as well if you need control over the port that's used for the low balancers health checks now you can specify the backend services health check parameters using the health check parameter of a backend config custom resource definition referenced by the corresponding service this gives you more flexibility and control over health checks for a google cloud external http or https load balancer or internal http or https load balancer created by an ingress and lastly i wanted to touch on ssl certificates and there are three ways to provide ssl certificates to an http or https load balancer the first way is google managed certificates and these are provisioned deployed renewed and managed for your domains and just as a note managed certificates do not support wildcard domains the second way to provide ssl certificates is through selfmanaged certificates that are shared with google cloud you can provision your own ssl certificate and create a certificate resource in your google cloud project you can then list the certificate resource in an annotation on an ingress to create an http or https load balancer that uses the certificate and the last way to provide ssl certificates is through selfmanaged certificates as secret resources so you can provision your own ssl certificate and create a secret to hold it you can then refer to the secret as an ingress specification to create an http or https load balancer that uses this certificate and just as a note you can specify multiple certificates in an ingress manifest the load balancer chooses a certificate if the common name in the certificate matches the host name used in the request and so that pretty much covers all the main topics in this short lesson on ingress for gke so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'll be going over gke storage options now kubernetes currently offers a slew of different storage options and is only enhanced by the added features available in google cloud for gke we'll also be getting into the different abstractions that kubernetes offers to manage storage and how they can be used for different types of workloads now there's quite a bit to go over here so with that being said let's dive in now as i stated before there are several storage options for applications running on gke the choices vary in terms of flexibility and ease of use google cloud offers several storage options that can be used for your specific workload kubernetes also provides storage abstractions which i will be getting into in just a bit the easiest storage options are google cloud's managed storage products if you need to connect a database to your cluster you can consider using cloud sql datastore or cloud spanner and when it comes to object storage cloud storage would be an excellent option to fill the gap file store is a great option for when your application requires managed network attached storage and if your application requires block storage the best option is to use persistent disks and can be provisioned manually or provisioned dynamically through kubernetes now i wanted to first start off with kubernetes storage abstractions but in order to understand kubernetes storage abstractions i wanted to take a moment to explain how storage is mounted in the concept of docker now docker has a concept of volumes though it is somewhat looser and less managed than kubernetes a docker volume is a directory on disk or in another container docker provides volume drivers but the functionality is somewhat limited a docker container has a writable layer and this is where the data is stored by default making the data ephemeral and so data is not persisted when the container is removed so storing data inside a container is not always recommended now there are three ways to mount data inside a docker container the first way is a docker volume and sits inside the docker area within the host's file system and can be shared amongst other containers this volume is a docker object and is decoupled from the container they can be attached and shared across multiple containers as well bind mounting is the second way to mount data and is coming directly from the host's file system bind mounts are great for local application development yet cannot be shared across containers and the last way to mount data is by using tempfs and is stored in the host's memory this way is great for ephemeral data and increases performance as it no longer lies in the container's writable layer now with kubernetes storage abstractions file system and block based storage are provided to your pods but are different than docker in nature volumes are the basic storage unit in kubernetes that decouples the storage from the container and tie it to the pod and not the container like in docker a regular volume simply called volume is basically a directory that the containers in a pod have access to the particular volume type used is what will determine its purpose some volume types are backed by ephemeral storage like empty dir config map and secrets and these volumes do not persist after the pod ceases to exist volumes are useful for caching temporary information sharing files between containers or to load data into a pod other volume types are backed by durable storage and persist beyond the lifetime of a pod like persistent volumes and persistent volume claims a persistent volume is a cluster resource that pods can use for durable storage a persistent volume claim can be used to dynamically provision a persistent volume backed by persistent disks persistent volume claims can also be used to provision other types of backing storage like nfs and i will be getting more into persistent volumes and persistent volume claims in just a bit now as you saw in docker on disk files in a container are the simplest place for an application to write data but files are lost when the container crashes or stops for any other reason as well as being unaccessible to other containers running in the same pod in kubernetes the volume source declared in the pod specification determines how the directory is created the storage medium used and the directory's initial contents a pod specifies what volumes it contains and the path where containers mount the volume ephemeral volume types live the same amount of time as the pods they are connected to these volumes are created when the pod is created and persist through container restarts only when the pod terminates or is deleted are the volumes terminated as well other volume types are interfaces to durable storage that exist independently of a pod like ephemeral volumes data in a volume backed by durable storage is preserved when the pod is removed the volume is merely unmounted and the data can be handed off to another pod now volumes differ in their storage implementation and their initial contents you can choose the volume source that best fits your use case and i will be going over some common volume sources that are used and you will see in many gke implementations the first volume that i want to bring up is empty dir now an empty dir volume provides an empty directory that containers in the pod can read and write from when the pod is removed from a node for any reason the data in the empty dir is deleted forever an empty dir volume is stored on whatever medium is backing the node which might be a disk ssd or network storage empty der volumes are useful for scratch space and sharing data between multiple containers in a pod the next type of volume that i wanted to go over is config map and config map is a resource that provides a way to inject configuration data into pods the data stored in a config map object can be referenced in a volume of type config map and then consumed through files running in a pod the next volume type is secret and a secret volume is used to make sensitive data such as passwords oauth tokens and ssh keys available to applications the data stored in a secret object can be referenced in a volume of type secret and then consumed through files running in a pod next volume type is downward api and this volume makes downward api data available to applications so this data includes information about the pod and container in which an application is running in an example of this would be to expose information about the pods namespace and ip address to applications and the last volume type that i wanted to touch on is persistent volume claim now a persistent volume claim volume can be used to provision durable storage so that they can be used by applications a pod uses a persistent volume claim to mount a volume that is backed by this durable storage and so now that i've covered volumes i wanted to go into a bit of detail about persistent volumes persistent volume resources are used to manage durable storage in a cluster in gke a persistent volume is typically backed by a persistent disk or file store can be used as an nfs solution unlike volumes the persistent volume life cycle is managed by kubernetes and can be dynamically provisioned without the need to manually create and delete the backing storage persistent volume resources are cluster resources that exist independently of pods and continue to persist as the cluster changes and as pods are deleted and recreated moving on to persistent volume claims this is a request for and claim to a persistent volume resource persistent volume claim objects request a specific size access mode and storage class for the persistent volume if an existing persistent volume can satisfy the request or can be provisioned the persistent volume claim is bound to that persistent volume and just as a note pods use claims as volumes the cluster inspects the claim to find the bound volume and mounts that volume for the pod now i wanted to take a moment to go over storage classes and how they apply to the overall storage in gke now these volume implementations such as gce persistent disk are configured through storage class resources gke creates a default storage class for you which uses the standard persistent disk type of ext4 as shown here the default storage class is used when a persistent volume claim doesn't specify a storage class name and can also be replaced with one of your choosing you can even create your own storage class resources to describe different classes of storage and is helpful when using windows node pools now as i stated before persistent volume claims can automatically provision persistent disks for you when you create this persistent volume claim object kubernetes dynamically creates a corresponding persistent volume object due to the gke default storage class this persistent volume is backed by a new empty compute engine persistent disk you use this disk in a pod by using the claim as a volume when you delete a claim the corresponding persistent volume object and the provision compute engine persistent disk are also deleted now to prevent deletion you can set the reclaim policy of the persistent disk resource or its storage class resource to retain now deployments as shown here in this diagram are designed for stateless applications so all replicas of a deployment share the same persistent volume claim which is why stateful sets are the recommended method of deploying stateful applications that require a unique volume per replica by using stateful sets with persistent volume claim templates you can have applications that can scale up automatically with unique persistent volume claims associated to each replica pod now lastly i wanted to touch on some topics that will determine the storage access that is available for any gke cluster in your environment now i first wanted to start off with access modes and there are three supported modes for your persistent disks that allow read write access and are listed here read write once is where the volume can be mounted as read write by a single node read only many is where the volume can be mounted as a read only by many nodes and lastly read write many is where the volume can be mounted as read write by many nodes and just as a note read write once is the most common use case for persistent disks and works as the default access mode for most applications next i wanted to touch on the type of persistent disks that are available and the benefits and caveats of access for each now going through the persistent disks lesson of this course you probably know by now about the available persistent disks when it comes to zonal versus regional availability and so this may be a refresher for some now going into regional persistent disks these are multizonal resources that replicate data between two zones in the same region and can be used similarly to zonal persistent disks in the event of a zonal outage kubernetes can fail over workloads using the volume to the other zone regional persistent disks are great for highly available solutions for stateful workloads on gke now zonal persistent disks are zonal resources and so unless a zone is specified gke assigns the disk to a single zone and chooses the zone at random once a persistent disk is provisioned any pods referencing the disk are scheduled to the same zone as the disk and just as a note using antiaffinity on zones allows stateful set pods to be spread across zones along with the corresponding disks and the last point that i wanted to cover when it comes to persistent volume access is the speed of access now as stated in an earlier lesson the size of persistent disks determine the iops and throughput of the disk gke typically uses persistent disks as boot disks and to back kubernetes persistent volumes so whenever possible use larger and fewer disks to achieve higher iops and throughput and so that pretty much covers everything that i wanted to go over in this lesson on gke storage options so you can now mark this lesson as complete and let's move on to the next one welcome back in these next few demos i'm going to be doing a complete walkthrough and putting all the theory we learned into practice through building and interacting with gke clusters and you'll be building and deploying your own containerized application on this cluster called box of bowties so in this demo we're going to be setting up our own gke cluster in the console along with going through all the options that are available when deploying it we're also going to use the command line to configure the cubectl command line tool so that we can interact with the cluster so with that being said let's dive in and so here in the console i am logged in as tonybowties gmail.com under the project of bow tie inc and so before launching the cluster i need to make sure that my default vpc has been created so i'm going to go over to the navigation menu and i'm going to scroll down to vpc network and as expected the default network is here so i can go ahead and create my cluster and so in order to get to my kubernetes engine console i'm going to go up to the navigation menu and i'm going to scroll down under compute and you will find here kubernetes engine and you'll see a few different options to choose from and over here on the left hand menu i will be going through these options in the upcoming demos but for now i want to concentrate on creating our cluster now gk makes things pretty easy as i have the option to create a cluster to deploy a container or even taking the quick start and so we're going to go ahead and click on create our cluster and so here we are prompted with our cluster basics now if i really wanted to i can simply fill out all the fields that you see here and click on create and it will use all the defaults to build my cluster but we're going to customize it a little bit so we're going to go ahead and go through all these options so first under name we're going to name this cluster bowtie dash cluster and so under location type we want to keep things as zonal and if i check off the specify default node locations i'll be able to make this a multizonal cluster as i have the option of selecting from multiple zones where i can situate my nodes and so i can select off a bunch of different zones if i choose but we want to keep it as a single zonal cluster and so i'm going to check these all off and under zone i'm going to click on the drop down menu and i'm going to select us east 1b and just as a note for each zone that you select this is where the control plane will live so if i was to create a multizonal cluster as you can see the master zone is the zone where the control plane will be created and is selected as us east 1b as that is the zone that i had selected and so if i change this to let's say us east 1d you can see that the control plane will change with it so i'm going to change it back to us east 1b and you also have the option of creating a regional cluster and the location selection will change from zone to region and here you will have to specify at least one zone to select but please also remember that the same number of nodes will be deployed to each selected zone so if i have three nodes in this cluster and i decide to select three zones then i will have nine nodes in this cluster and so doing something like this could get quite pricey when you're looking to be cost conscious okay so moving on i'm going to uncheck specify default node locations i'm going to change the location type back to zonal and make sure that my zone is at us east 1b moving down to the master version this is where we would select either a static version or optin to a release channel for the version of kubernetes that you want for your cluster and so with the static version i can choose from a bunch of different versions here all the way back from 1.14.10 all the way to the latest version and so with the release channel i have the release channel selection here and i can choose from the rapid channel the regular channel or the stable channel and so i'm going to keep things as the default with the regular channel as well i'm going to keep the default version as the version of my choice now i could go ahead and simply click on create here but as this demo is a walkthrough i'm going to go ahead and go through all the available options so i'm going to start by going over to the left hand menu and clicking on default pool under no pools now here i have one node pool already with three nodes and this is the default node pool that comes with any cluster but if i was doing something specific i could add another node pool and configure it from here but because i don't have a need for two node pools i'm gonna go ahead and remove nodepool1 so i'm going to go up here to remove nodepool and as you can see gke makes it really easy for me to add or remove node pools so i'm going to go back to the default pool and i'm going to keep the name as is i'm gonna keep my number of nodes as three and if i wanted to change the number of nodes i can simply select this i can choose six or however many nodes you need for your workload and so because we're not deploying a large workload i'm gonna keep this number at 3 and moving right along we do want to check off enable auto scaling and so this way we don't have to worry about scaling up or scaling down and here i'm going to put the minimum number of nodes as one and i'm going to keep my maximum number of nodes at 3. and so here i'm given the option to select the zone location for my nodes but again for each zone that i select it will run the same amount of nodes so basically i have another option in order to choose from having a zonal or multizonal cluster and because we're creating our cluster in a single zone i'm going to uncheck this and under automation as you can see enable auto upgrade and enable auto repair are both checked off and this is due to the fact that the auto upgrade feature is always enabled for the release channel that i selected but as i pointed out in a previous lesson that this is google's best practice to have auto upgrade and auto repair enabled and so moving down to the bottom are some fields to change the surge upgrade behavior and so just as a refresher surge upgrades allow you to control the number of nodes gke can upgrade at a time and control how disruptive those upgrades are to your workloads so max surge being the number of additional nodes that can be added to the node pool during an upgrade and max unavailable being the number of nodes that can be simultaneously unavailable during that upgrade and because we're not worried about disruptions we'll just leave it set as the default and so moving on we're going to move back over to the left hand menu and under no pools we're going to click on nodes and here is where i can choose the type of instance that i want to be using for my nodes and so i'm going to keep the image type as container optimize os and this is the default image type but i also have the option of choosing from others like ubuntu or windows and so i'm going to keep it as the default and under machine configuration i'm going to keep it under general purpose with series e2 but i do want to change the machine type to e2 micro just to be cost conscious and under boot disk size i want to keep it as 10 gigabytes as we don't really need 100 gigabytes for what we're doing here and you also have the option of choosing from a different boot disk type you can change it from standard persistent disk to ssd but i'm going to keep things as standard as well i also have the option here to use customer manage keys for encryption on my boot disk as well as selecting from preemptable nodes for some cost savings and so i'm going to now move down to networking and here if i wanted to get really granular i can add a maximum pods per node as well as some network tags but our demo doesn't require this so i'm going to leave it as is and i'm going to go back over to the left hand menu and click on security and under node security you have the option of changing your service account along with the access scopes and so for this demo we can keep things as the default service account and the access scopes can be left as is i'm going to go back over to the left hand menu and click on metadata and here i can add kubernetes labels as well as the instance metadata and so i know i didn't get into node taints but just to fill you in on no taints when you submit a workload to run in a cluster the scheduler determines where to place the pods associated with the workload and so the scheduler will place a pod on any node that satisfies the resource requirements for that workload so no taints will give you some more control over which workloads can run on a particular pool of nodes and so they let you mark a node so that the scheduler avoids or prevents using it for certain pods so for instance if you had a node pool that is dedicated to gpus you'd want to keep that node pool specifically for the workload that requires it and although it is in beta this is a great feature to have and so that pretty much covers no pools as we see it here and so this is the end of part one of this demo it was getting a bit long so i decided to break it up this would be a great opportunity for you to get up and have a stretch get yourself a coffee or a tea and whenever you're ready part two will be starting immediately from the end of part one so you can now mark this as complete and i'll see you in the next one this is part two of creating a gke cluster part 2 will be starting immediately from the end of part 1. so with that being said let's dive in and so i'm going to go back over to the left hand menu and under cluster i'm going to click on automation and here i have the option of enabling a maintenance window for aligning times when auto upgrades are allowed i have the option of adding the window here and i can do it at specified times during the week or i can create a custom maintenance window and so we don't need a maintenance window right now so i'm going to uncheck this and as well you have the option of doing maintenance exclusions for when you don't want maintenance to occur ngk gives you the option of doing multiple maintenance exclusions for whenever you need them and because we don't need any maintenance exclusions i'm going to delete these and here you have the option to enable vertical pod auto scaling and this is where gke will automatically schedule pods onto other nodes that satisfy the resources required for that workload as well here i can enable my node auto provisioning and enabling this option allows gke to automatically manage a set of node pools that can be created and deleted as needed and i have a bunch of fields that i can choose from the resource type the minimum and maximum for cpu and memory the service account as well as adding even more resources like gpus but our workload doesn't require anything this fancy so i'm going to delete this and i'm going to uncheck enable auto provisioning and lastly we have the auto scaling profile and i have the option from choosing the balance profile which is the default as well as the optimize utilization which is still in beta and so i'm going to keep things as the default and i'm going to move back on over to the left hand menu over to networking and so here i can get really granular with my cluster when it comes to networking i have the option of choosing from a public or a private cluster as well i can choose from a different network and since we only have the default that's what shows up but if you had different networks here you can choose from them as well as the subnets i can also choose from other networking options like pod address range maximum pods per node and there's a bunch of other options which i won't get into any detail with but i encourage you if you're very curious to go through the docs and to check out these different options now the one thing that i wanted to note here is the enable http low balancing and this is a addon that is required in order to use google cloud load balancer and so as we discussed previously in the services lesson when you enable service type load balancer a load balancer will be created for you by the cloud provider and so google requires you to check this off so that a controller can be installed in the cluster upon creation and will allow a load balancer to be created when the service is created and so i'm going to leave this checked as we will be deploying a load balancer a little bit later and so moving back over to the left hand menu i'm going to now click on security and there are many options here to choose from that will allow you to really lock down your cluster and again this would all depend on your specific type of workload now i'm not going to go through all these options here but i did want to highlight it for those who are looking to be more security focused with your cluster and so moving down the list in the menu i'm going to click on metadata and so here i can enter a description for my cluster as well as adding labels and so the last option on the cluster menu is features and here i have the option of running cloud run for anthos which will allow you to deploy serverless workloads to anthos clusters and runs on top of gke and here you can enable monitoring for gke and have it be natively monitored by google cloud monitoring and if i was running a thirdparty product to monitor my cluster i can simply uncheck this and use my thirdparty monitoring and there's a whole bunch of other features that i won't dive into right now but if you're curious you can always hover over the question mark and get some more information about what it does and so now i've pretty much covered all the configuration that's needed for this cluster and so now i'm going to finally head down to the bottom and click on create and so it may take a few minutes to create this cluster so i'm going to go ahead and pause this video here and i'll be back faster than you can say cat in the hat okay and the cluster has been created as you can see it's in the location of us east 1b with three nodes six vcpus and three gigabytes of memory and i can drill down and see exactly the details of the cluster as well if i wanted to edit any of these options i can simply go up to the top click on edit and make the necessary changes and so now you're probably wondering what will i need to do in order to create this cluster through the command line well it's a bit simpler than what you think and i'm going to show you right now i'm going to simply go over to the right hand menu and activate cloud shell and bring this up for better viewing and i'm going to paste in my command gcloud container clusters create bow tie dash cluster with the flag num nodes and the number of nodes that i choose which is three and so like i said before if i wanted to simply create a simple cluster i can do so like this but if i wanted to create the cluster exactly how i built my last cluster then i can use this command which has all the necessary flags that i need to make it customize to my liking a not so very exciting demonstration but at the same time shows you how easy yet powerful gke really is and so i'm not going to launch this cluster as i already have one and so now i wanted to show you how to interact with your new gke cluster so i'm going to simply clear my screen and so now in order for me to interact with my cluster i'm going to be using the cube ctl command line tool and this is the tool that is used to interact with any kubernetes cluster no matter the platform now i could use the gcloud container commands but they won't allow me to get very granular as the cubectl tool and so a caveat of creating your cluster through the console is that you need to run a command in order to retrieve the cluster's credentials and configure the cubectl command line tool and i'm going to go ahead and paste that in now and the command is gcloud container clusters get dash credentials and the name of my cluster which is bow tie dash cluster along with the zone flag dash dash zone followed by the zone itself which is us east 1b i'm going to go ahead and hit enter and as you can see cubectl has now been configured and so now i'm able to interact with my cluster so just to verify i'm going to run the command cubectl getpods and naturally as no workloads are currently deployed in the cluster there are no pods so i'm going to run the command cube ctl get nodes and as you can see the cubectl command line tool is configured correctly and so now this cluster is ready to have workloads deployed to it and is also configured with the cubectl command line tool so that you're able to manage the cluster and troubleshoot if necessary now i know that there has been a ton of features that i covered but i wanted to give you the full walkthrough so that you are able to tie in some of the theory from the last few lessons and get a feel for the gke cluster as we will be getting more involved with it over the next couple of demos and so that's pretty much all i wanted to cover when it comes to creating and setting up a gke cluster so you can now mark this as complete and whenever you're ready join me in the console in the next one where you will be building your box a bow ties container to deploy to your new cluster but if you are not planning to go straight into the next demo i do recommend that you delete your cluster to avoid any unnecessary costs and recreate it when you are ready to go into the next demo welcome back now in the last lesson you built a custom gke cluster and configured the cube ctl command line tool to interact with the cluster in this lesson you're going to be building a docker image for a box of bow ties using cloud build which will then be pushed over to google cloud container registry so that you can deploy it to your current gke cluster and so as you can see there's a lot to do here so with that being said let's dive in so now the first thing that you want to do is to clone your repo within cloud shell so you can run the necessary commands to build your image so i'm going to go up here to the top right and i'm going to open up cloud shell i'm going to make sure that i'm in my home directory so i'm going to run the command cd space tilde hit enter and i'm in my home directory if i run the command ls i can see that i only have cloud shell.txt and so now i'm going to clone my github repository and i'll have a link in the instructions in the github repo as well as having it in the lesson text below and so the command would be git clone along with the https address of the github repo and i'm going to hit enter and it's finished cloning my repo i'm going to quickly clear my screen and i'm going to run the command ls and i can see my repo here and now i'm going to drill down into the directory by running cd google cloud associate cloud engineer if i run an ls i can see all my clone files and folders and so now the files that we need are going to be found in the box of bowties folder under kubernetes engine and containers so i'm going to change directories to that location and run ls and under box of bow ties is a folder called container which will have all the necessary files that you need in order to build your image we have the jpeg for box of bow ties we have the docker file and we have our index.html and so these are the three files that we need in order to build the image and so as i said before we are going to be using a tool called cloud build which we have not discussed yet cloudbuild is a serverless ci cd platform that allows me to package source code into containers and you can get really fancy with cloud build but we're not going to be setting up any ci cd pipelines we're merely using cloud build to build our image and to push it out to container registry as well container registry is google cloud's private docker repository where you can manage your docker images and integrates with cloud build gke app engine cloud functions and other repos like github or bitbucket and it allows for an amazing build experience with absolutely no heavy lifting and because you're able to build images without having to leave google cloud i figured that this would be a great time to highlight these services so getting back to it we've cloned the repo and so we have our files here in cloud shell and so what you want to do now is you want to make sure the cloud build api has been enabled as this is a service that we haven't used before now we can go through the console and enable the api there but i'm going to run it here from cloud shell and i'm going to paste in the command gcloud services enable cloudbuild.googleapis.com i'm going to hit enter and you should get a prompt asking you to authorize the api call you definitely want to authorize should take a few seconds all right and the api has been enabled for cloud build so now i'm going to quickly clear my screen and so because i want to show you exactly what cloud build is doing i want to head on over there through the console and so i'm going to go over to the navigation menu and i'm going to scroll down to tools until you come to cloud build and as expected there is nothing here in the build history as well not a lot here to interact with and so now you're going to run the command that builds the image and so you're going to paste that command into the cloud shell which is gcloud builds submit dash dash tag gcr.io which is the google cloud container registry our variable for our google cloud project along with the image name of box bow ties version 1.0.0 and please don't forget the trailing dot at the end i'm going to go ahead and hit enter cloud build will now compress the files and move them to a cloud storage bucket and then cloud build takes those files from the bucket and uses the docker file to execute the docker build process and so i'm going to pause the video here till the build completes and i'll be back in a flash okay and the image is complete and is now showing up in the build history in the cloud build dashboard and so if i want to drill down into the actual build right beside the green check mark you will see the hot link so you can just simply click on that and here you will see a build summary with the build log the execution details along with the build artifacts and as well the compressed files are stored in cloud storage and it has a hot link right here if i wanted to download the build log i can do so here and i conveniently have a hot link to the image of box of bow ties and this will bring me to my container registry so you can go ahead and click on the link it should open up another tab and bring you right to the page of the image that covers a lot of its details now the great thing i love about container registry is again it's so tightly coupled with a lot of the other resources within google cloud that i am able to simply deploy right from here and i can deploy to cloud run to gke as well as compute engine now i could simply deploy this image right from here but i wanted to do it from gke so i'm going to go back over to gke in the other tab i'm going to go to the navigation menu go down to kubernetes engine and i'm going to go up to the top menu and click on deploy it's going to ask for the image you want to deploy and you want to click on select to select a new container image and you should have a menu pop up from the right hand side of your screen and under container registry you should see box of bow ties you can expand the node here and simply click on the image and then hit select and so now the container image has been populated into my image path and you want to scroll down and if i wanted to i could add another container and even add some environment variables and so we're not looking to do that right now so you can simply click on continue and you're going to be prompted with some fields to fill out for your configuration on your deployment and so the application name is going to be called box of bow ties i'm going to keep it in the default namespace as well i'm going to keep the key value pair as app box of bow ties for my labels and because this configuration will create a deployment file for me you can always have a look at the manifest by clicking on the view yaml button before it's deployed and this is always good practice before you deploy any workload so as you can see here at the top i have the kind as deployment the name as well as the namespace my labels replicas of three as well as my selector and my spec down here at the bottom as well this manifest also holds another kind of horizontal pod auto scaler and is coupled with the deployment in this manifest due to the reference of the deployment itself and so it's always common practice to try and group the manifest together whenever you can and so this is a really cool feature to take advantage of on gke so i'm going to close this now and i'm actually going to close cloud shell as i don't need it right now as well you can see here that it's going to deploy to my kubernetes cluster of bow tie cluster in us east 1b and if i wanted to i can deploy it to a new cluster and if i had any other clusters in my environment they would show up here and i'd be able to select from them as well but bow tie cluster is the only one that i have and so now that you've completed your configuration for your deployment you can simply click on deploy this is just going to take a couple minutes so i'm just going to pause the video here and i'll be back as soon as the deployment is done okay the workload has been deployed and i got some default messages that popped up i can set an automated pipeline for this workload but we're not going to do that for this demo but feel free to try it on your own later if you'd like and we will want to expose our service as we want to see if it's up and running and we're going to take care of that in just a bit and so if i scroll through some of the details here i can see that i have some metrics here for cpu memory and disk the cluster namespace labels and all the pods that it's running on basically a live visual representation of my deployment if i scroll back up to the top i can dive into some details events and even my manifest i can also copy my manifest and download it if i'd like so as you can see a lot of different options and so now i want to verify my deployment and so i'm going to use the cube ctl command line tool to run some commands to verify the information so i'm going to open back up my cloud shell and make this a little bit bigger for better viewing and i'm going to run the command cubectl get all and as you can see here i have a list of all the pods that are running the name of the service the deployment the replica set everything about my cluster and my deployment and you should be seeing the same when running this command and so next you want to pull up the details on your deployments in the cluster and so the command for that is cube ctl get deployments and it came out kind of crammed at the bottom so i'm going to simply clear my screen and run that command again and as you can see the box of bowties deployment is displayed how many replicas that are available how many of those replicas achieve their desired state and along with how long the application has been running and so now i want to dive into my pods and in order to do that i'm going to run the command cube ctl get pods and here i can see all my pods now if i wanted to look at a list of events for a specific pod the command for that would be cubectl describe pod and then the name of one of the pods so i'm going to pick this first one copy that i'm going to paste it and i'm going to hit enter and here i can see all the events that have occurred for this pod as well i also have access to some other information with regards to volumes conditions and even the container and image ids and this is a great command to use for when you're troubleshooting your pods and you're trying to get to the bottom of a problem and so now the final step that you want to do is you want to be able to expose your application so you can check to see if it's running properly and so we're going to go ahead and do that through the console so i'm going to close down cloud shell and i'm going to go to overview and scroll down to the bottom click on the button that says expose and if i wanted to i can do it from up here in the top right hand corner where it says expose deployment so i'm going to click on expose and this probably looks very familiar to you as this is a graphical representation of the services manifest and so the port mapping here will cover the ports configuration of the services manifest starting here with port target port as well as protocol for target port i'm going to open up port 80. here under service type you have the option of selecting cluster ip node port or load balancer and the service type you want to use is going to be low balancer and we can keep the service name as box of bowties service and again you can view the manifest file for this service and you can copy or download it if you need to but we don't need this right now so i'm going to close it in a pretty simple process so all i need to do is click on expose and within a minute or two you should have your service up and running with your shiny new low balancer okay and the service has been created and as you can see we're under the services and ingress from the left hand menu and if i go back to the main page of services in ingress you can see that box a bow tie service is the only one that's here i also have the option of creating a service type ingress but we don't want to do that right now so i'm going to go back to services and here you will see your endpoint and this is the hot link that should bring you to your application so you can click on it now you'll get a redirect notice as it is only http and not https so it's safe to click on it so i'm going to click on it now and success and here is your box of bow ties what were you expecting and so i wanted to congratulate you on deploying your first application box of bow ties on your gke cluster and so just as a recap you've cloned your repo into your cloud shell environment you then built a container image using cloud build and pushed the image to container registry you then created a deployment using this image and verified the deployment using the cube ctl command line tool you then launched a service of type low balancer to expose your application and verified that your application was working so fantastic job on your part and that's pretty much all i wanted to cover in this part of the demo so you can now mark this as complete and whenever you're ready join me in the console for the next part of the demo where you will manage your workload on the gke cluster so please be aware of the charges incurred on your currently deployed cluster if you plan to do the next demo at a later date again you can mark this as complete and i'll see you in the next welcome back in the last couple of demo lessons you built a custom gke cluster and deployed the box of bowties application in this lesson you will be interacting with this workload on gke by scaling the application editing your application and rebuilding your docker image so you can do a rolling update to the current workload in your cluster now there's a lot to do here so with that being said let's dive in so continuing where we left off you currently have your box of bow ties workload deployed on your gke cluster and so the first thing you want to do is scale your deployment and you are looking to scale down your cluster to one pod and then back up again to three and this is just to simulate scaling your workload so whether it be ten pods or one the action is still the same so now we can easily do it through the console by drilling down into the box of bowties workload going up to the top menu and clicking on actions and clicking on scale and here i can indicate how many replicas i'd like and scale it accordingly and so i wanted to do this using the command line so i'm going to cancel out of here and then i'm going to open up cloud shell instead okay and now that you have cloud shell open up you want to run the command cube ctl get pods to show the currently running available pods for the box of bowties workload and you may get a popup asking you to authorize the api call using your credentials and you definitely want to authorize and here you will get a list of all the pods that are running your box of bow ties workload and so now since you want to scale your replicas down to one you can run this command cube ctl scale deployment and your workload which is box of bowties dash dash replicas is equal to one you can hit enter and it is now scaled and in order to verify that i'm going to run cube ctl get pods and notice that there is only one pod running with my box of bow ties workload and in order for me to scale my deployment back up to three replicas i can simply run the same command but change the replicas from 1 to 3. hit enter it's been scaled i'm going to run cube ctl get pods and notice that i am now back up to 3 replicas and so as you can see increasing or decreasing the number of replicas in order to scale your application is pretty simple to do okay so now that you've learned how to scale your application you're gonna learn how to perform a rolling update but in order to do that you need to make changes to your application and so what you're going to do is edit your application then rebuild your docker image and apply a rolling update and in order to do that we can stay here in cloud shell as you're going to edit the file in cloud shell editor i'm going to first clear my screen i'm going to change directory into my home directory and now you want to change directories to your container folder where the files are that i need to edit i'm going to run ls and here's the files that i need and so what you're going to do now is edit the index.html file and the easiest way to do that is to simply type in edit index.html and hit enter and this will open up your editor so you can edit your index.html file and if you remember when we launched our application it looked exactly like this and so instead of what were you expecting we're going to actually change that text to something a little different and so i'm going to go back to the editor in my other tab and where it says what were you expecting i'm going to actually change this to well i could always use something to eat then i'm going to go back up to the menu click on file and click on save and so now in order for me to deploy this i need to rebuild my container and so i'm going to go back to my terminal i'm going to clear the screen and i'm going to run the same command that i did the last time which is gcloud build submit dash dash tag gcr dot io with the variable for your google cloud project followed by the image box of bowties colon 1.0.1 and so this will be a different version of the image also don't forget that trailing dot at the end and you can hit enter and again this is the process where cloud build compresses the files moves them to a cloud storage bucket and then takes the files from the bucket and uses the docker file to execute the docker build process and this will take a couple minutes so i'm going to pause the video here and i'll be back before you can say cat in the hat okay and my new image has been created and so i want to head over to cloud build just to make sure that there are no errors so i'm going to close down cloud shell because i don't need it right now i'm going to head back up to the navigation menu and scroll down to cloud build and under build history you should see your second build and if you drill down into it you will see that the build was successful and heading over to build artifacts you should now see your new image as version 1.0.1 and so now i'm going to head over to the registry and verify the image there and it seems like everything looks okay so now i'm gonna head back on over to my gke cluster i'm gonna go to the navigation menu down to kubernetes engine and here i'm gonna click on workloads i'm gonna select box of bowties and up at the top menu you can click on actions and select a rolling update and here you are prompted with a popup where you can enter in your minimum seconds ready your maximum search percentage as well as your maximum unavailable percentage and so here under container images i am prompted to enter in the sha256 hash of this docker image now a docker image's id is a digest which contains a sha256 hash of the image's configuration and if i go back over to the open tab for container registry you can see here the digest details to give you a little bit more context along with the sha 256 hash for the image that i need to deploy and so you can copy this digest by simply clicking on the copy button and then you can head back on over to the gke console head over to the container images highlight the hash and paste in the new hash and so when you copy it in make sure it's still in the same format of gcr dot io forward slash your project name forward slash box of bow ties the at symbol followed by the hash and so once you've done that you can click on the update button and this will schedule an update for your application and as you can see here at the top it says that pods are pending as well if i go down to active revisions you can see here that there is a summary and the status that pods are pending and so just as a note rolling updates allow the deployments update to take place with zero downtime by incrementally updating pods instances with new ones so the pods will be scheduled on nodes with available resources and if the nodes do not have enough resources the pods will stay in a pending state but i don't think we're going to have any problems with these nodes as this application is very light in resources and if i open up cloud shell and run a cube ctl get pods command you will see that new pods have started and you can tell this by the age of the pod as well if you ran the command keep ctl describe pod along with the pod name you could also see the event logs when the pod was created and if i close cloud shell i can see up here at the top of my deployment details it shows that my replicas have one updated four ready three available and one unavailable and if i click on refresh i can see now that my replicas are all updated and available and so now in order to check your new update you can simply go down to exposing services and click on the endpoints link you'll get that redirect notice you can simply click on the link and because the old site may be cached in your browser you may have to refresh your web page and success and you have now completed a rolling update in gke so i wanted to congratulate you on making it to the end of this multipart demo and hope that it's been extremely useful in excelling your knowledge in gke and so just as a recap you scaled your application to accommodate both less and more replicas you edited your application in the cloud shell editor and rebuilt your container image using cloud build you then applied the new digest to your rolling update and applied that rolling update to your deployment while verifying it all in the end fantastic job on your part as this was a pretty complex and long multipart demo and you can expect things like what you've experienced in this demo to pop up in your role of being a cloud engineer when dealing with gke and so that's pretty much all i wanted to cover with this multipart demo working with gke so before you go i wanted to take a few moments to delete all the resources you've created one by one so i'm going to go up to the top i'm going to close all my tabs i'm going to head on over to clusters and so i don't want to delete my cluster just yet but the first thing that i want to do is delete my container images so i'm going to head up to the top and open up cloud shell and i'm going to use the command gcloud container images delete gcr dot io forward slash your google cloud project variable forward slash along with your first image of box of bow ties colon 1.0.0 hit enter it's going to prompt you if you want to continue you want to hit y for yes and it has now deleted the image as well you want to delete your latest image which is 1.0.1 so i'm going to change the zero to one hit enter it's going to ask if you want to continue yes and so the container images have now been deleted and so now along with the images you want to delete the artifacts as well and those are stored in cloud storage so i'm going to close down cloud shell i'm going to head on up to the navigation menu and i'm going to head down to storage and you want to select your bucket that has your project name underscore cloud build select the source folder and click on delete and you're going to get a prompt asking you to delete the selected folder but in order to do this you need to type in the name of the folder so i'm going to type it in now you can click on confirm and so now the folder has been deleted along with the artifacts and so now that we've taken care of the images along with the artifacts we need to clean up our gke cluster so i'm going to head back on up to the navigation menu and i'm going to head on over to kubernetes engine and the first thing that i want to delete is the low balancer so i'm going to head on up to services and ingress and you can select box of bow tie service and go up to the top and click on delete you're going to get a confirmation and you want to click on delete and it's going to take a couple minutes you do quick refresh and the service has finally been deleted i now want to delete my workload so i'm going to go over to the left hand menu click on workloads select the workload box of bowties and go up to the top and click on delete and you want to delete all resources including the horizontal pod auto scaler so you can simply click on delete and it may take a few minutes to delete gonna go up to the top and hit refresh and my workload has been deleted and so now all that's left to delete is the gke cluster itself so i'm going to go back to clusters so you're going to select the cluster and go up to the top and click on delete and you're going to get a prompt asking you if you want to delete these storage pods and these are default storage pods that are installed with the cluster as well you can delete the cluster while the workload is still in play but i have this habit of being thorough so i wanted to delete the workload before deleting the cluster and so you want to go ahead and click on delete and so that's pretty much all i have for this demo and this section on google kubernetes engine and again congrats on the great job you can now mark this as complete and i'll see you in the next one welcome back and in this lesson i will be covering the features of cloud vpn an essential service for any engineer to know about when looking to connect another network to google cloud whether it be your onpremises network another cloud provider or even when connecting to vpcs this service is a must know for any engineer and for the exam so with that being said let's dive in now cloudvpn securely connects your peer network to your vpc network through an ipsec vpn connection when i talk about a peer network this is referring to an onpremises vpn device or vpn service a vpn gateway hosted by another cloud provider such as aws or azure or another google cloud vpn gateway and so this is an ipsec or encrypted tunnel from your peer network to your vpc network that traverses the public internet and so for those who don't know ipsec being short for internet security protocol and this is a set of protocols using algorithms allowing the transport of secure data over an ip network ipsec operates at the network layer so layer 3 of the osi model which allows it to be independent of any applications although it does come with some additional overhead so please be aware and so when creating your cloud vpn traffic traveling between the two networks is encrypted by one vpn gateway and then decrypted by the other vpn gateway now moving on to some details about cloud vpn this is a regional service and so please take that into consideration when connecting your onpremises location to google cloud for the least amount of latency it also means that if that region were to go down you would lose your connection until the region is back up and running now cloud vpn is also a sitetosite vpn only and therefore it does not support sitetoclient so this means that if you have a laptop or a computer at home you cannot use this option with a vpn client to connect to google cloud cloudvpn can also be used in conjunction with private google access for your onpremises hosts so if you're using private google access within gcp you can simply connect to your data center with vpn and have access as if you were already in gcp so if you're looking to extend private google access to your onpremises data center cloud vpn would be the perfect choice and so when it comes to speeds each cloud vpn tunnel can support up to three gigabits per second total for ingress and egress as well routing options that are available are both static and dynamic but are only available as dynamic for aha vpn and lastly cloudvpn supports ik version 1 and ike version 2 using shared secret and for those of you who are unaware ike stands for internet key exchange and this helps establish a secure authenticated communication channel by using a key exchange algorithm to generate a shared secret key to encrypt communications so know that when you choose cloudvpn that your connection is both private and secure so now there are two types of vpn options that are available in google cloud one being the classic vpn and the other being h a vpn and i'm going to take a moment to go through the differences now with classic vpn this provides a service level agreement of 99.9 percent also known as an sla of three nines while h a vpn provides a four nines sla when configured with two interfaces and two external ips now when it comes to routing classic vpn supports both static and dynamic routing whereas havpn supports dynamic routing only and this must be done through bgp using cloud router classic vpn gateways have a single interface and a single external ip address and support tunnels using static routing as well as dynamic routing and the static routing can be either route based or policy based whereas with havpn it can be configured for two interfaces and two external ips for true ha capabilities and as mentioned earlier when it comes to routing for havpn dynamic routing is the only available option now the one thing about classic vpn is that google cloud is deprecating certain functionality on october 31st of 2021 and is recommending all their customers to move to h a vpn and so know that this has not been reflected in the exam and not sure if and when it will be but know that when you are creating a cloud vpn connection in your current environment h a vpn is the recommended option and so now i wanted to dive into some architecture of how cloud vpn is set up for these two options starting with classic vpn now as i said before classic vpn is a cloud vpn solution that lets you connect your peer network to your vpc network through an ipsec vpn connection in a single region now unlike h a vpn classic vpn offers no redundancy out of the box you would have to create another vpn connection and if the connection were to go down you would have to manually switch over the connection from one to the other now as you can see here when you create a vpn gateway google cloud automatically chooses only one external ip address for its interface and the diagram shown here shows that of a classic vpn network connected from the bowtie dash network vpc in bowtie project to an onpremises network configured using a static route to connect now moving on to havpn again this is a highly available cloud vpn solution that lets you connect your peer network to your vpc network using an ipsec vpn connection in a single region exactly like classic vpn where havpn differs is that it provides four nines sla and as you can see here it supports double the connections so when you create an h a vpn gateway google cloud automatically chooses two external ip addresses one for each of its fixed number of two interfaces each ip address is automatically chosen from a unique address pool to support high availability each of these ha vpn gateway interfaces supports multiple tunnels which allows you to create multiple h a vpn gateways and you can configure an h a vpn gateway with only one active interface and one public ip address however this configuration does not provide a four nines sla now for h a vpn gateway you configure an external peer vpn gateway resource that represents your physical peer gateway in google cloud you can also create this resource as a standalone resource and use it later in this diagram the two interfaces of an h a vpn gateway in the bowtie network vpc living in bowtie project are connected to two peer vpn gateways in an onpremises network and this connection is using dynamic routing with bgp connecting to a cloud router in google cloud now when it comes to the times when using cloudvpn makes sense one of the first things you should think about is whether or not you need public internet access so when you're sharing files or your company needs a specific sas product that's only available on the internet vpn would be your only option as well when you're looking to use interconnect and your peering location is not available so you're not able to connect your data center to the colocation facility of your choice vpn would be the only other option that you have as well if budget constraints come into play when deciding on connecting to your peer network vpn would always be the way to go as cloud interconnect is going to be the more expensive option and lastly if you don't need a high speed network and low latency is not really a concern for you and you only have regular outgoing traffic coming from google cloud then vpn would suffice for your everyday needs and so the options shown here are also the deciding factors to look for when it comes to questions in the exam that refer to cloudvpn or connecting networks and so that's pretty much all i have for this short lesson on cloudvpn so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i'm going to go over another connection type that allows for onpremises connectivity to your google cloud vpcs which is cloud interconnect other than vpn this is the other connection type that allows connectivity from your onpremises environment to your google cloud vpc cloud interconnect is the most common connection for most larger organizations and are for those that demand fast low latency connections this lesson will cover the features of cloud interconnect and the different types that are available so with that being said let's dive in so getting right into it cloud interconnect is a low latency highly available connection between your onpremises data center and google cloud vpc networks also cloud interconnect connections provide internal ip address connection which means internal ip addresses are directly accessible from both networks and so on premises hosts can use internal ip addresses and take advantage of private google access rather than external ip addresses to reach google apis and services traffic between your onpremises network and your vpc network doesn't traverse the public internet traffic traverses a dedicated connection or through a service provider with a dedicated connection your vpc network's internal ip addresses are directly accessible from your onpremises network now unlike vpn this connection is not encrypted if you need to encrypt your traffic at the ip layer you can create one or more selfmanaged vpn gateways in your vpc network and assign a private ip address to each gateway now although this may be a very fast connection it also comes with a very high price tag now unlike vpn this connection type is not encrypted if you need to encrypt your traffic at the ip layer you can create one or more selfmanaged vpn gateways in your vpc network and assign a private ip address to each gateway now although this may be a very fast connection it also comes with a very high price tag and is the highest price connection type cloud interconnect offers two options for extending your onpremises network dedicated interconnect which provides a direct physical connection between your onpremises network and google's network as well as partner interconnect which provides connectivity between your onpremises and vpc networks through a supported service provider and so i wanted to take a moment to highlight the different options for cloud interconnect starting with dedicated interconnect now dedicated interconnect provides a direct physical connection between your onpremises network and google's network dedicated interconnect enables you to transfer large amounts of data between your network and google cloud which can be more cost effective than purchasing additional bandwidth over the public internet for dedicated interconnect you provision a dedicated interconnect connection between the google network and your own router in a common location the following example shown here shows a single dedicated interconnect connection between a vpc network and an onpremises network for this basic setup a dedicated interconnect connection is provisioned between the google network and the onpremises router in a common colocation facility when you create a vlan attachment you associate it with a cloud router this cloud router creates a bgp session for the vlan attachment and its corresponding onpremises peer router these routes are added as custom dynamic routes in your vpc network and so for dedicated interconnect connection capacity is delivered over one or more 10 gigabits per second or 100 gigabits per second ethernet connections with the followon maximum capacity supported per interconnect connection so with your 10 gigabit per second connections you can get up to eight connections totaling a speed of 80 gigabits per second with the 100 gigabit per second connection you can connect two of them together to have a total speed of 200 gigabits per second and so for dedicated interconnect your network must physically meet google's network in a supported colocation facility also known as an interconnect connection location this facility is where a vendor the colocation facility provider provisions a circuit between your network and a google edge point of presence also known as a pop the setup shown here is suitable for noncritical applications that can tolerate some downtime but for sensitive production applications at least two interconnect connections in two different edge availability domains are recommended now partner interconnect provides connectivity between your onpremises network and your vpc network through a supported service provider so this is not a direct connection from your onpremises network to google as the service provider provides a conduit between your onpremises network and google's pop now a partner interconnect connection is useful if a dedicated interconnect colocation facility is physically out of reach or your workloads don't warrant an entire 10 gigabit per second connection for partner interconnect 50 megabits per second to 50 gigabits per second vlan attachments are available with the maximum supported attachment size of 50 gigabits per second now service providers have existing physical connections to google's network that they make available for their customer to use so in this example shown here you would provision a partner interconnect connection with a service provider and connecting your onpremises network to that service provider after connectivity is established with the service provider a partner interconnect connection is requested from the service provider and the service provider configures your vln attachment for use once your connection is provisioned you can start passing traffic between your networks by using the service providers network now there are many more detailed steps involved to get a connection established along with traffic flowing but i just wanted to give you a high level summary of how a connection would be established with a service provider now as well to build a highly available topology you can use multiple service providers as well you must build redundant connections for each service provider in each metropolitan and so now there's a couple more connection types that run through service providers that are not on the exam but i wanted you to be aware of them if ever the situation arises in your role as a cloud engineer so the first one is direct peering and direct peering enables you to establish a direct peering connection between your business network and google's edge network and exchange high throughput cloud traffic this capability is available at any of more than 100 locations in 33 countries around the world when established direct peering provides a direct path from your onpremises network to google services including google cloud products that can be exposed through one or more public ip addresses traffic from google's network to your onpremises network also takes that direct path including traffic from vpc networks in your projects now you can also save money and receive direct egress pricing for your projects after they have established direct peering with google direct peering exists outside of google cloud unless you need to access google workspace applications the recommended methods of access to google cloud are dedicated interconnect or partner interconnect establishing a direct peering connection with google is free and there are no costs per port and no per hour charges you just have to meet google's technical peering requirements and can then be considered for the direct peering service and moving on to the last connection type is cdn interconnect now i know we haven't gotten into cdns in the course as the exam does not require you to know it but cdn standing for content delivery network is what caches content at the network edge to deliver files faster to those requesting it one of the main ways to improve website performance now moving on to cdn interconnect this connection type enables select thirdparty cdn providers like akamai and cloudflare along with others to establish and optimize your cdn population costs by using direct peering links with google's edge network and enables you to direct your traffic from your vpc networks to the provider's network and so your egress traffic from google cloud through one of these links benefits from the direct connectivity to the cdn provider and is billed automatically with reduced pricing typical use cases for cdn interconnect is if you're populating your cdn with large data files from google cloud or you have frequent content updates stored in different cdn locations and so getting into the use cases of when to use cloud interconnect a big purpose for it would be to prevent traffic from traversing the public internet it is a dedicated physical connection right to google's data centers so when you need an extension of your vpc network to your onpremises network interconnect is definitely the way to go now in speed and low latencies of extreme importance interconnect is always the best option and will support up to 200 gigabits per second as well when you have heavy outgoing traffic or egress traffic leaving google cloud cloud interconnect fits the bill perfectly and lastly when it comes to private google access this travels over the backbone of google's network and so when you are connected with interconnect this is an extension of that backbone and therefore your onpremises hosts will be able to take advantage of private google access and so i hope this has given you some clarity on the differences between the different connection types and how to extend your google cloud network to a peer or onpremises network so that's pretty much all i had to cover when it comes to cloud interconnect so you can now mark this lesson as complete and let's move on to the next one welcome back in this lesson i'm going to be covering an overview of app engine now this is not a deep dive lesson for app engine as there is so much to cover with this service but i will be listing a lot of the features of app engine to give you a good feel for what it can do and what you will need to know for the exam so with that being said let's dive in now app engine is a fully managed serverless platform for developing and hosting web applications at scale this is google's platform as a service offering that was designed for developers so that they can develop their application and let app engine do all the heavy lifting by taking care of provisioning the servers and scaling the instances needed based on demand app engine gives you the flexibility of launching your code as is or you can launch it as a container and uses runtime environments of a variety of different programming languages like python java node.js go ruby php or net applications deployed on app engine that experience regular traffic fluctuations or newly deployed applications where you're simply unsure about the load are auto scaled accordingly and automatically your apps scale up to the number of instances that are running to provide consistent performance or scale down to minimize idle instances and reduces costs app engine also has the capabilities of being able to deal with rapid scaling for sudden extreme spikes of traffic having multiple versions of your application within each service allows you to quickly switch between different versions of that application for rollbacks testing or other temporary events you can route traffic to one or more specific versions of your application by migrating or splitting traffic and you can use traffic splitting to specify a percentage distribution of traffic across two or more of the versions within a service and allows you to do a b testing or blue green deployment between your versions when rolling out new features app engine supports connecting to backend storage services such as cloud firestore cloud sql and cloud storage along with connecting to onpremises databases and even external databases that are hosted on other public clouds app engine is available in two separate flavors standard and flexible environments and each environment offers their own set of features that i will get into in just a sec now as i mentioned before app engine is available in standard and flexible environments and depending on your application needs either one will support what you need for your workload or you could even use both simultaneously the features shown here will give you a feel for both types of environments and i'm going to be doing a quick run through summarizing the features of each starting with the standard environment now with the standard environment applications run in a secure sandboxed environment allowing app engine standard to distribute requests across multiple servers and scaling servers to meet traffic demands your application runs with its own secure reliable environment that is independent of the hardware operating system or physical location of the server the source code is written in specific versions of the supported programming languages and with app engine standard it is intended to run for free or at a very low cost where you pay only for what you need and when you need it with app engine standard your application can scale to zero instances when there is no traffic app engine standard is designed for sudden and extreme spikes of traffic which require immediate scaling and pricing for standard app engine is based on instance hours and so when it comes to features for app engine flexible the application instances run within docker containers that includes a custom runtime or source code written in other programming languages these docker containers are then run on compute engine vms app engine flexible will run any source code that is written in a version of any of the supported programming languages for app engine flexible and unlike the standard environment unfortunately there is no free quota for app engine flexible as well app engine flexible is designed for consistent traffic or for applications that experience regular traffic fluctuations and pricing is based on the vm resources and not on instance hours like app engine standard and so where app engine flexible really shines over app engine standard are how the vms are managed so instances are health checked healed as necessary and colocated with other services within the project the vm's operating system is updated and applied automatically as well vms are restarted on a weekly basis to make sure any necessary operating system and security updates are applied ssh along with root access are available to the vm instances running your containers now deploying applications to app engine is as simple as using the gcloud app deploy command this command automatically builds a container image from your configuration file by using the cloud build service and then deploys that image to app engine now an app engine application is made up of a single application resource that consists of one or more services each service can be configured to use different runtimes and to operate with different performance settings services and app engine are used to factor your large applications into logical components that can securely share app engine features and communicate with one another these app engine services become loosely coupled behaving like microservices now within each service you deploy versions of that service and each version then runs within one or more instances depending on how much traffic you configured it to handle having multiple versions of your application within each service allows you to quickly switch between different versions of that application for rollbacks testing or other temporary events you can route traffic to one or more specific versions of your application by migrating traffic to one specific version or splitting your traffic between two separate versions and so the versions within your services run on one or more instances by default app engine scales your application to match the load your applications will scale up the number of instances that are running to provide consistent performance or scale down to minimize idle instances and reduce costs now when it comes to managing instances app engine can automatically create and shut down instances as traffic fluctuates or you can specify a number of instances to run regardless of the amount of traffic you can also configure how and when new instances are created by specifying a scaling type for your application and how you do this is you specify the scaling type in your application's app.yaml file now there are three different types of scaling choices to choose from and the first one being automatic scaling and this scaling type creates instances based on request rate response latencies and other application metrics you can specify thresholds for each of these metrics as well as a minimum number instances to keep running at all times if you use automatic scaling each instance in your application has its own queue for incoming requests before the queues become long enough to have a visible effect on your app's latency app engine automatically creates one or more new instances to handle the load the second type is basic scaling and this creates instances when your application receives requests each instance is shut down when the application becomes idle basic scaling is fantastic for intermittent workloads or if you're looking to drive your application by user activity app engine will try to keep your costs low even though it might result in higher latency as the volume of incoming requests increase and so the last scaling type is manual scaling and this is where you specify the number of instances that continuously run regardless of the load so these are instances that are constantly running and this allows complex startup tasks on the instances to have already been completed when receiving requests and applications that rely on the state of the memory over time so this is ideal for instances whose configuration scripts require some time to fully run their course so now that i've gone over managing the instances i wanted to take a few moments to go over how app engine manages traffic starting with traffic migration now traffic migration switches the request routing between the versions within a service of your application moving traffic from one or more versions to a single new version so when deploying a new version with the same name of an existing version it causes an immediate traffic migration all instances of the old version are immediately shut down in app engine standard you can choose to route requests to the target version either immediately or gradually you can also choose to enable warmup requests if you want the traffic gradually migrated to a version gradual traffic migration is not supported in app engine flexible and traffic is migrated immediately now one thing to note is that when you immediately migrate traffic to a new version without any running instances then your application will have a spike in latency for loading requests while instances are being created and so another way to manage traffic on app engine is through traffic splitting now you can use traffic splitting to specify a percentage distribution of traffic across two or more of the versions within a service so in this example if i'm deploying a new version of my service i can decide on how i want to distribute traffic to each version of my application and so i decide that i want to keep my current version in play but roll out the new version of my application to 10 of my users leaving the old version was still 90 of the traffic going to that version and so splitting traffic allows you to conduct a b testing between your versions and provides control over the pace when rolling out features and just as a note when you've specified two or more versions for splitting you must choose whether to split traffic by either by either ip address http cookie or do it randomly now again this has not been a deep dive lesson on app engine but i hope this has given you an overview of the features that are available as the exam touches on these features i also wanted to give you some familiarity with the service itself as coming up next i will be going into a demo where we will be launching an application using app engine and trying on some of these features for yourself and so that's pretty much all i wanted to cover when it comes to app engine so you can now mark this lesson as complete and whenever you're ready join me in the console where you will deploy an application on app engine and try out some of these features for yourself welcome back and in this demo you're going to build another application to deploy on app engine called serverless bowties this demo will run you through the ins and outs of deploying a website application on app engine along with managing it while experiencing no downtime so there's quite a bit of work to do here so with that being said let's dive in and so here in my console i am logged in as tonybowtieace gmail.com under project bowtie inc and so the first thing i want to do here is i want to head on over to app engine so in order to do that i'm going to go to the top lefthand navigation menu and i'm going to go down to app engine and because i haven't created any applications i'm going to be brought to this splash page now in order to deploy this application we're not going to be doing it through the console but we will be doing it through the command line and so to get started with that i'm going to go up to the top and open up cloud shell i'm going to make this bigger for better viewing and so in order for me to get the code to launch this application i'm going to be cloning my github repository into cloud shell and so for those of you who haven't deleted your repository from the last demo you can go ahead and skip the cloning step for those of you who need to clone your repository you will find a link to the instructions in the lesson text and there you'll be able to retrieve the command which will be git clone along with the address of the repo i'm going to hit enter and because i've already cloned this repo i'm receiving this error i'm going to do an ls and as you can see here the google cloud associate cloud engineer repo has already been cloned so i'm going to cd into that directory and in order to get the code i'm going to simply run the command git pull to get the latest and i'm going to simply clear my screen and so now that i've retrieved all the code that i need in order to deploy it i need to go to that directory and that directory is going to be 11 serverless services forward slash 0 1 serverless bowties and hit enter you're going to run ls and here you will find two versions of the website application site v1 and site v2 along with the instructions if you want to follow straight from here and so i want to go ahead and deploy my first website application so i'm going to cd into site v1 ls and here you will see the app.yaml which is the configuration file that you will need in order to run the application on app engine and so before i go ahead and deploy this i wanted to take a moment to show you the application configuration so i'm going to go ahead and open it up in cloud shell editor so i'm going to type in edit app.yaml enter and as you can see here my runtime is python 3.7 and as you can see i have a default expiration of two seconds along with an expiration underneath each handler and this is due to the caching issue that happens with app engine and so in order to simulate traffic splitting between the two website applications in order to make things easy i needed to expire the cash and this is an easy way to do it now there may be applications out there that do need that caching and so the expiration may be a lot higher but for the purposes of this demo two seconds expiration should suffice as well explain the two handlers here the first one showing the files that will be uploaded to the cloud storage bucket as well as the second stating what static files will be presented and so i'm going to go ahead back over to my terminal and i'm going to go ahead and clear my screen and i'm going to go ahead and run the command gcloud app deploy with the flag dash dash version and this is going to be version one so i'm going to go ahead and hit enter and you may get a popup asking you to authorize this api call using your credentials and you want to click on authorize and you're going to be prompted to enter in a region that you want to deploy your website application to we want to keep this in us east one so i'm going to type in 15 hit enter and you're going to be prompted to verify your configuration for your application before it's deployed you're also going to be prompted if you want to continue definitely yes so i'm going to hit y enter and so now as you've seen the files have been uploaded to cloud storage and app engine is going to take a few minutes to create the service along with the version so i'm going to let it do the needful and i'll be back before you know it okay and my application has been deployed now although you don't see it here in the console it has been deployed all i need to do is refresh my screen but i wanted to just point out a couple things that is shown here in the terminal the first one being the default service now the first time you deploy a version of your application it will always deploy to the default service initially and only then will you be able to deploy another named service to app engine now here where it says setting traffic split for service this is referring to the configuration for traffic splitting being applied in the background which i will be getting into a little bit later and lastly the url shown for the deployed service will always start with the name of your project followed by.ue.r.appspot.com which is why in production google recommends to run app engine in a completely separate project before this demo running it in the same project that we've been using will suffice okay so let's go ahead and take a look at the application so i'm going to go back up to the top here to the navigation menu and i'm gonna go down to app engine and go over to services and so here you will see the default service with version one and if i go over to versions i will see here my version the status the traffic allocation along with any instances that it needs the run time the specific environment and i'll have some diagnostic tools here that i could use and so because this is a static website application we won't be using any instances and so this will always show a zero so now i want to head back on over to services and i'm going to launch my application by simply clicking on this hot link and success serverless bow ties for all and so it looks like my application has been successfully deployed so i'm going to close down this tab now there's a couple of things that i wanted to run through here on the left hand menu just for your information so here i can click on instances and if i was running any instances i am able to see a summary of those instances and i can click on the drop down here and choose a different metric and find out any information that i need as well i can click on this drop down and select a version if i had multiple versions which i do not clicking on task queues here is where i can manage my task queues but this is a legacy service that will soon be deprecated clicking on cron jobs here i can schedule any tasks that i need to run at a specific time on a recurring basis i can edit or add any firewall rules if i need to and as you can see the default firewall rule is open to the world now you probably noticed memcache as being one of the options here in the menu but this is a legacy service that will soon be deprecated memcache is a distributed inmemory data store that is bundled into the python to runtime acting as a cache for specific tasks and google recommends moving to memory store for redis if you're planning on applying caching for your app engine application and so i'm not sure how much longer this will be here and lastly under settings here is where you can change your settings for your application i can add any custom domains any ssl certificates as well as setting up email for any applications that want to send email out to your users okay and now that we've done that walkthrough i want to go ahead and deploy my second version of the application and so i'm going to go ahead back down to cloud shell i'm going to quickly clear my screen and i want to move into the site v2 directory so i'm going to hit cd dot dot which will bring you back one directory you do an ls and i'm going to change directories into site v2 and do an ls just to verify and yes you will see serverless bow ties too i'm going to quickly clear my screen and i'm going to run the same command as before which is gcloud app deploy with the version flag dash dash version and instead of one i'm going to launch version 2. so i'm going to hit enter i'm going to be prompted if i want to continue yes i do and as you can see the files have been uploaded to cloud storage for version 2 of the website application and app engine is going to take a few minutes to create the service along with the version so i'm going to let it cook here for a couple minutes and i'll be back before you can say cat in the hat okay so version 2 has been deployed and so if i go up here to the console and i click on refresh you should see version 2 of your service and as you can see 100 of the traffic has been allocated to version 2 automatically and this is the default behavior for whenever you launch a new version of your service the only way to avoid this is to deploy your new version with the no promote flag and so if i go back to services here on the left and i click on the default service you should see success for version two and so i know that my website application for version 2 has been deployed successfully so i'm going to close down this tab again and i'm going to go back to versions and so what i want to do now is i want to simulate an a b test or blue green deployment by migrating my traffic back to the old version in this case being version one so in production let's say that you would release a new version and the version doesn't go according to plan you can always go back to the previous version and app engine allows you to do that very easily and so i'm going to click on version 1 and i'm going to go up to the top menu and click on migrate traffic you'll be prompted if you want to migrate traffic yes i do so i'm going to click on migrate and it should take a minute here and traffic should migrate over to version one and success traffic has been migrated and so we want to verify that this has happened i'm gonna go back to services i'm gonna click on the default service and yes the traffic has been allocated to version one okay so i'm going to shut down this tab i'm going to go back to versions and so now what i want to do is i want to simulate splitting the traffic between the two versions and so in order for you to do this you can go up to the top menu click on split traffic and you'll be prompted with a new menu here and here i can choose from different versions and because i only have two versions i'm going to add version 2 and in order to allocate the traffic between the two i can either use this slider and as you can see the allocation percentage will change or i can simply just type it in and so i'm going to leave this at 50 percent so fifty percent of version one fifty percent of version two i'm going to split traffic randomly i'm gonna move this down just a little bit and so that's exactly how you wanna allocate your traffic and so once you've completed that you can simply click on save it's going to take a moment to update the settings and it's been successful so if i head back on over to the previous page you can see here that traffic has been allocated to both versions and so now in order to verify this what you're going to do is go over to services and click on the default hot link and you'll see version one but if i continuously refresh my screen i can see that here i have version two so because it's random i have a 50 chance of getting version 1 and a 50 chance of getting version 2. and so this is a simulation of splitting traffic to different versions and usually with a b testing only a small percentage of the traffic is routed to the new version until verification can be made that the new version deployed has indeed been successful and this can be done by receiving feedback from the users and so now i wanted to take a quick moment to congratulate you on making it through this demo and hope that it has been extremely useful in excelling your knowledge in deploying and managing applications on app engine so just as a recap you've cloned the repo to cloud shell you then deployed version one of your application into app engine you verified its launch and then you deployed version two of the application and verified its launch as well you then migrated traffic from version two over to version one and then you went ahead and split traffic between both versions and allotted 50 of the traffic allocation to each version and so now before you go i want to make sure that we clean up any resources that we've deployed so that we don't incur any unnecessary costs and so the way to do this is very simple so first step you want to go over to the left hand menu and click on settings and simply click on disable application you're going to be prompted to type in the app's id for me it's bowtie inc so i'm going to type that in and i'm going to click on disable now unfortunately with app engine you can't actually delete the application it can only be disabled and so now here i'm going to hit the hot link to go over to the cloud storage bucket and as you can see here i have no files but i'm going to move back to my buckets and i'm going to move into the staging bucket which is appended with your project id.appspot.com and as you can see here there's a whole bunch of different files as well if i drill down into the directory marked as ae for app engine i can see here that i have some more directories along with the manifest and so now if you want to keep your application in order to run it later you don't need to delete this bucket but because i don't need it i'm going to go ahead and delete the bucket hit delete paste in my bucket name hit delete as well under us.artifacts you will find a directory called containers and as explained in the last lesson code build builds a container for your application before deploying it to app engine so i'm going to drill down into images so here's all the container digests and i don't need any of these so i'm gonna go ahead and delete this bucket as well and so this is the last step in order to delete all the directories and files that we use to deploy our application in an app engine okay and so i'm gonna head back on over to app engine and so now that cleanup has been taken care of that's pretty much all i wanted to cover in this demo for deploying and managing applications on app engine so you can now mark this as complete and i'll see you in the next one and again congrats on a job well done welcome back in this lesson i will be diving into another serverless product from google cloud by the name of cloud functions an extremely useful and advanced service that can be used with almost every service on the platform now there's quite a bit to cover here so with that being said let's dive in now cloud functions as i said before are a serverless execution environment and what i mean by this is like app engine there is no need to provision any servers or updating vms as the infrastructure is all handled by google but unlike app engine you will never see the servers so the provisioning of resources happens when the code is executed now cloud functions are a function as a service offering and this is where you upload code that is purposefully written in a supported programming language and when your code is triggered it is executed in a fully managed environment and your billed for when that code is executed cloud functions run in a runtime environment and support many different runtimes like python java node.js go and net core cloud functions are event driven so when something happens in your environment you can choose whether or not you'd like to respond to this event if you do then your code can be executed in response to the event these triggers can be one of a few different types such as http pub sub cloud storage and now firestore and firebase which are in beta and have yet to be seen in the exam cloud functions are priced according to how long your function runs and how many resources you provision for your function if your function makes an outbound network request there are also additional data transfer fees cloud functions also include a perpetual free tier which allows you 2 million invocations or executions of your function now cloud functions themselves are very simple but have a few steps to execute before actually running so i wanted to give you a walkthrough on exactly how cloud functions work now after selecting the name and region you want your function to live in you would then select the trigger you wish to use and you can choose from the many i listed earlier being http cloud storage pub sub cloud firestore and firebase a trigger is a declaration that you are interested in a certain event or set of events binding a function to a trigger allows you to capture and act on these events authentication configuration is the next step and can be selected with public access or configured through iam now there are some optional settings that can be configured where you would provide the amount of memory the function will need to run networking preferences and even selection for a service account now once all the settings have been solidified your written code can then be put into the function now the functions code supports a variety of languages as stated before like python java node.js or go now when writing your code there are two distinct types of cloud functions that you could use http functions and background functions with http functions you invoke them from standard http requests these http requests wait for the response and support handling of common http request methods like get put post delete and options when you use cloud functions a tls certificate is automatically provisioned for you so all http functions can be invoked via a secure connection now when it comes to background functions these are used to handle events from your gcp infrastructure such as messages on a pub sub topic or changes in a cloud storage bucket now once you have put all this together you are ready to deploy your code now there are two things that will happen when deploying your code the first one is the binding of your trigger to your function once you bind a trigger you cannot bind another one to the same function only one trigger can be bound to a function at a time now the second thing that will happen when you deploy your function's source code to cloud functions is that source code is stored in a cloud storage bucket as a zip file cloud build then automatically builds your code into a container image that pushes that image to container registry cloud functions accesses this image when it needs to run the container to execute your function the process of building the image is entirely automatic and requires no manual intervention and so at this point of the process the building of your function is now complete now that the function has been created we now wait for an event to happen and events are things that happen within your cloud environment that you might want to take action on these might be changes to data in cloud sql files added to cloud storage or a new vm being created currently cloud functions supports events from the same services used for triggers that i have just mentioned including other google services like bigquery cloud sql and cloud spanner now when an event triggers the execution of your cloud function data associated with the event is passed via the functions parameters the type of event determines the parameters that are passed to your function cloud functions handles incoming requests by assigning them to instances of your function now depending on the volume of requests as well as the number of existing function instances cloud functions may assign a request to an existing instance or create a new one so the cloud function will grab the image from cloud registry and hand off the image along with the event data to the instance for processing now each instance of a function handles only one concurrent request at a time this means that while your code is processing one request there is no possibility of a second request being routed to the same instance thus the original request can use the full amount of resources that you requested and this is the memory that you assign to your cloud function when deploying it now to allow google to automatically manage and scale the functions they must be stateless functions are not meant to be persistent nor is the data that is passed on to the function and so once the function has run and all data has been processed by the server it is then passed on to either a vpc or to the internet now by default functions have public internet access unless configured otherwise functions can also be private and used within your vpc but must be configured before deployment now there are so many use cases for cloud functions and there are many that have already been created by google for you to try out and can be located in the documentation that i've supplied in the lesson text below now the exam doesn't go into too much depth on cloud functions but i did want to give you some exposure to this fantastic serverless product from google as it is so commonly used in many production environments in a simple and easy way to take in data process it and return a result from any event you are given and i have no doubt that once you get the hang of deploying them that you will be a huge fan of them as well and so that's pretty much all i had to cover when it comes to cloud functions so you can now mark this lesson as complete and whenever you're ready join me in the next one where we go handson in the console creating and deploying your very first function welcome back and in this demo we will be diving into creating and deploying our very first cloud function we're going to take a tour of all the options in the console but we're going to do most of the work in cloud shell to get a good feel for doing it in the command line so with that being said let's dive in and so i'm logged in here as tony bowties gmail.com and i'm in the project of bowtie inc and so the first thing i want to do is head on over to cloud functions in the console so i'm going to go up to the top left to the navigation menu and i'm going to scroll down to cloud functions and as you can see here cloud functions is getting ready and this is because we've never used it before and the api is being enabled okay and the api has been enabled and we can go ahead and start creating our function so you can go ahead and click create function and you will be prompted with some fields to fill out for the configuration of your cloud function and so under basics for function name i'm going to name this hello underscore world for region i'm going to select us east one and under trigger for trigger type we're gonna keep this as http although if i click on the drop down menu you can see that i will have options for cloud pub sub cloud storage and the ones that i mentioned before that are in beta so we're going to keep things as http and here under url is the url for the actual cloud function under authentication i have the option of choosing require authentication or allow unauthenticated invocations and as you can see this is clearly marked saying that check this if you are creating a public api or website which we are and so this is the authentication method that you want to select and so now that we have all the fields filled out for the basic configuration i'm going to go ahead and click on save and just to give you a quick run through of what else is available i'm going to click on the drop down here and this will give me access to variables networking and advanced settings the first field here memory allocated i can actually add more memory depending what i am doing with my cloud function but i'm going to keep it as the default if you have a cloud function that runs a little bit longer and you need more time to run the cloud function you can add additional time for the timeout and as well i have the option of choosing a different service account for this cloud function and so moving on under environment variables you will see the options to add build environment variables along with runtime environment variables and the last option being connections here you can change the different networking settings for ingress and egress traffic under ingress settings i can allow all traffic which is the default i can allow internal traffic only as well i can allow internal traffic and traffic from cloud low balancing now as well when it comes to the egress settings as i said before by default your cloud function is able to send requests to the internet but not to resources in your vpc network and so this is where you would create a vpc connector to send requests from your cloud function to resources in your vpc so if i click on create a connector it'll open up a new tab and bring me to vpc network to add serverless vpc access and so i don't want to do that right now so i'm going to close down this tab and i'm going to go ahead and leave everything else as is and click on next and so now that the configuration is done i can dive right into the code and so google cloud gives you a inline editor right here along with the different runtime environments so if i click on the drop down menu you can see i have the options of net core go java node.js and python 3.7 and 3.8 and so for this demo i'm going to keep it as node.js 10. the entry point will be hello world and i'm going to keep the code exactly as is and this is a default cloud function that is packaged with any runtime whenever you create a function from the console and so if i had any different code i can change it here but i'm not going to do that i'm going to leave everything else as is and click on deploy and it'll take a couple minutes here to create my cloud function and so i'm going to pause the video here for just a quick sec and i'll be back in a flash okay and my cloud function has been deployed and i got a green check mark which means that i'm all good and so i want to dive right into it for just a second so i can get some more details here i have the metrics for my cloud function the invocations per second execution time memory utilization and active instances i have my versions up here at the top but since i only have one version only one version shows up if i click on details it'll show me the general information along with the networking settings the source will show me the code for this cloud function as well as the variables the trigger permissions logs and testing and here i can write in some code and test the function and so in order for me to invoke this function i can simply go to trigger and it'll show me the url but a quick way to do this through the command line is to simply open up cloud shell and make this a little bigger for better viewing and i'm going to paste in the command gcloud functions describe along with the function name which is hello underscore world along with the region flag dash dash region with the region that my cloud function has been deployed in which is us east one and i'm going to hit enter it's going to ask me to authorize my api call yes i want to authorize it and this command should output some information on your screen and so what we're looking for here is the http trigger which you will find here under https trigger and it is the same as what you see here in the console and so just know if you want to grab the http url trigger you can also do it from the command line and so i'm going to now trigger it by going to this url and you should see in the top left hand side of your screen hello world not as exciting as spinning bow ties but this example gives you an idea of what an http function can do and so i'm going to close down this tab and so now what i want to do is i want to deploy another function but i want to do it now through the command line and so i'm going to now quickly clear my screen and so since i've already uploaded the code to the repo i'm going to simply clone that repo and run it from here so i'm going to simply do a cd tilde to make sure i'm in my home directory for those of you who haven't deleted the directory you can simply cd into it so i'm going to run cd google cloud associate cloud engineer hit enter and i'm going to run a get pull command and it pull down all the files that i needed i'm going to quickly clear my screen and so i'm going to change directories into the directory that has my code and so you're going to find it under 11 serverless services under zero to you called hit enter and again i will have a link in the lesson text for the full instructions on this demo and it will list the directory where you can find this code okay so moving forward i'm going to run ls and you should see three files here main.py requirements.txt and the text file with the instructions and so now that i have everything in place in order to deploy my code i'm going to paste in the command to actually deploy my function which is gcloud functions deploy the name of the function which is you underscore called the flag for the runtime dash dash runtime and the runtime is going to be python 3.8 the flag for the trigger which is going to be http and because i'm a nice guy and i want everyone to have access to this i'm going to tag it with the flag dash dash allow unauthenticated so i'm going to hit enter okay and this function should take a couple minutes to deploy so i'm going to sit here and let it cook and i'll be back before you can say cat in the hat okay and our function has been deployed i'm going to do a quick refresh here in the console and it deployed successfully as you can see the green check mark is here okay and so now that it's been deployed we want to trigger our function and so because i just deployed this function the url trigger is conveniently located here in my screen so you can go ahead and click on it and hello lover of bow ties you called now although this may be similar to the hello world demo but i did add a small feature that might spice things up and so if you go up to the url and you type in question mark name equals and your name and since my name is anthony i'm going to type in anthony hit enter and hello anthony you called and so this is a perfect example of the many different ways you can use functions and although i've only highlighted some very simple demonstrations there are many different ways that you can use functions such as running pipelines running batch jobs and even event driven security now although the exam doesn't go into too much depth on cloud functions it's always good to know its use cases and where its strengths lie for when you do decide to use it in your role as a cloud engineer now before you go be sure to delete all the resources you've created by deleting the functions and the storage buckets that house the code for the cloud functions and i will walk you through the steps right now okay so first i'm going to close down this tab and next you're going to select all the functions and you're going to simply click on delete you're going to get a prompt to delete the functions you're going to click on delete and it's going to take a minute or two and the functions are deleted i'm going to close down my cloud shell and i'm going to head over to cloud storage and as you can see here both these buckets that start with gcf standing for google cloud functions can be safely deleted as inside them are the files that were used for the cloud function so i'm going to go back out i'm going to select both of these and i'm going to click on delete you get a prompt to delete two buckets you can simply type in delete and click on delete and the buckets have now been deleted and you've pretty much finished your cleanup and so just as a recap you created a default cloud function that was available from the console and then verified it by triggering the http url you then deployed another function from the command line by pulling the code from the repo and using it for deployment and then you verified that function by triggering it using the http url as well and then you modify the url for a different output great job on another successful demo so you can now mark this as complete and let's move on to the next one welcome back in this lesson we're going to dive into cloud storage the go to storage service from google cloud if you're an engineer working in google cloud you've probably used this many times as a storage solution and if you haven't this is definitely a service that you will need to know for both the exam and your daytoday role as a cloud engineer now there's quite a bit to cover here so with that being said let's dive in now cloud storage is a consistent scalable large capacity highly durable object storage and this is unlimited storage for objects with no minimum object size but please remember that this is object storage and is not designed to store an operating system on but to store whole objects like pictures or videos cloud storage has worldwide accessibility and worldwide storage locations so anywhere that there is a region or zone cloud storage is available from there and can be accessed at any time through an internet connection cloud storage is great for storing data from data analytics jobs text files with code pictures of the latest fashion from paris and videos of your favorite house dj at the shelter cloud storage excels for content delivery big data sets and backups and are all stored as objects in buckets and this is the heart of cloud storage that i will be diving into so starting with buckets these are the basic containers or construct that holds your data everything that you store in cloud storage must be contained in a bucket you can use buckets to organize your data and control access to your data but unlike directories and folders you cannot nest buckets and i'll get into that in just a minute now when you create a bucket you must specify a globally unique name as every bucket resides in a single cloud storage namespace as well as a name you must specify a geographic location where the bucket and its contents are stored and you have three available geography choices to choose from from region dual region and multiregion and so just as a note choosing dual region and multiregion is considered georedundant for dual region georedundancy is achieved using a specific pair of regions for multiregion georedundancy is achieved using a continent that contains two or more geographic places basically the more regions your data is available in the greater your availability for that data after you've chosen a geographic location a default storage class must be chosen and this applies to objects added to the bucket that don't have a storage class explicitly specified and i'll be diving into storage classes in just a bit and so after you create a bucket you can still change its default storage class to any class supported in the buckets location with some stipulations you can only change the bucket name and location by deleting and recreating the bucket as well once dual region is selected it cannot be changed to multiregion and when selecting multiregion you will not be able to change the bucket to be dual region and lastly you will need to choose what level of access you want others to have on your bucket whether you want to apply permissions using uniform or fine grained access uniform bucket level access allows you to use iam alone to manage permissions iam applies permissions to all the objects contained inside the bucket or groups of objects with common name prefixes the find green option enables you to use iam and access control lists or acls together to manage permissions acls are a legacy access control system for cloud storage designed for interoperability with amazon s3 for those of you who use aws you can specify access and apply permissions at both the bucket level and per individual object and i will also be diving more into depth with access control in just a bit and just as a note labels are an optional item for bucket creation like every other resource creation process in gcp now that we've covered buckets i wanted to cover what is stored in those buckets which is objects and objects are the individual pieces of data or data chunks that you store in a cloud storage bucket and there is no limit on the number of objects that you can create in a bucket so you can think of objects kind of like files objects have two components object data and object metadata object data is typically a file that you want to store in cloud storage and in this case it is the picture of the plaid bow tie and object metadata is a collection of name value pairs that describe the various properties of that object an object's name is treated as a piece of object metadata in cloud storage and must be unique within the bucket cloud storage uses a flat namespace to store objects which means that cloud storage isn't a file system hierarchy but sees all objects in a given bucket as independent with no relationship towards each other for convenience tools such as the console and gsutil work with objects that use the slash character as if they were stored in a virtual hierarchy for example you can name one object slash bow ties slash spring 2021 slash plaid bowtie.jpg when using the cloud console you can then navigate to these objects as if they were in a hierarchical directory structure under the folders bow ties and spring 2021 now i mentioned before that the part of the bucket creation is the selection of a storage class the storage class you set for an object affects the object's availability and pricing model so when you create a bucket you can specify a default storage class for the bucket when you add objects to the bucket they inherit this storage class unless explicitly set otherwise now i wanted to touch on these four storage classes now to give you a better understanding of the differences between them the first one is standard storage and is considered best for hot data or frequently accessed data and is best for shortterm use as it does not have any specified storage duration and this is excellent for use in analytical workloads and transcoding and the price for this storage class comes in at two cents per gigabyte per month next up is near line storage and this is considered hot data as well and is a lowcost storage class for storing in frequently accessed data nearline storage has a slightly lower availability a 30day minimum storage duration and comes with the cost for data access nearline storage is ideal if you're looking to continuously add files but only plan to access them once a month and is perfect for data backup and data archiving the price for this storage class comes in at a penny per gigabyte per month now cold line storage is considered cold data as it enters into more of the longer term storage classes and is a very low cost storage class for storing and frequently accessed data it comes with slightly lower availability than nearline storage a 90day minimum storage duration and comes with the cost for data access that is higher than the retrieval cost for nearline storage coldline storage is ideal for data you plan to read or modify at most once a quarter and is perfect for data backup and data archiving the price for this storage class comes in at less than half of a penny per gigabyte per month and finally archive storage is the lowest cost highly durable storage service for data archiving online backup and disaster recovery and even coming in at a lowest cost the data access is still available within milliseconds archive storage comes in at a higher cost for data retrieval as well as a day minimum storage duration and is the best choice for data that you plan to access less than once a year archive storage also comes with the highest price for data retrieval and it is ideal for archive data storage that's used for regulatory purposes or disaster recovery data in the event that there is an oopsies in your environment the price of the storage class comes in at a ridiculously low price per gigabyte per month at a fraction of a penny per gigabyte per month now when it comes to choosing your geographic location this will determine the availability of your data here as you can see the highest availability is the standard multiregion whereas archive has the lowest availability when stored in a regional setting now when it comes to the durability of your data meaning the measurement of how healthy and resilient your data is from data loss or data corruption google cloud boasts 11 9's durability annually on all data stored in any storage class on cloud storage so know that your data is stored safely and will be there holding the same integrity from the day you stored it now when it comes to granting permissions to your cloud storage buckets and the objects within them there are four different options to choose from the first is iam permissions and these are the standard permissions that control all your other resources in google cloud and follow the same topdown hierarchy that we discussed earlier the next available option are access control list or acls and these define who has access to your buckets and objects as well as what type of access they have and these can work in tandem with im permissions moving on to sign urls these are time limited reader write access urls that can be created by you to give access to the object in question for the duration that you specify and lastly is sign policy documents and these are documents to specify what can be uploaded to a bucket and i will be going into each one of these in a bit of detail now cloud storage offers two systems for granting users permission to access your buckets and objects iam and access control lists these systems act in parallel in order for a user to access a cloud storage resource only one of the systems needs to grant the user permission im is always the recommended method when it comes to giving access to buckets and the objects within those buckets granting roles at the bucket level does not affect any existing roles that you granted at the project level and vice versa giving you two levels of granularity to customize your permissions so for instance you can give a user permission to read objects in any bucket but permissions to create objects only in one specific bucket the roles that are available through iam are the primitive standard storage roles or the legacy roles which are equivalent to acls now acls are there if you need to customize access and really get granular with individual objects within a bucket and are used to define who has access to your buckets and objects as well as what level of access they have each acl consists of one or more entries and gives a specific user or group the ability to perform specific actions each entry consists of two pieces of information a permission which defines what actions can be performed and a scope which defines who can perform the specified actions now acls should be used with caution as iam roles and acls overlap cloud storage will grant a broader permission so if you allow specific users access to an object in a bucket and then an acl is applied to that object to make it public then it will be publicly accessible so please be aware now a signed url is a url that provides limited permission and time to make a request sign urls contain authentication information allowing users without credentials to perform specific actions on a resource when you generate a signed url you specify a user or service account which must have sufficient permission to make the request that the sign url will make after you generate a signed url anyone who possesses it can use the sign url to perform specified actions such as reading an object within a specified period of time now if you want to provide public access to a user who doesn't have an account you can provide a signed url to that user which gives the user read write or delete access to that resource for a limited time you specify an expiration date when you create the sign url so anyone who knows the url can access the resource until the expiration time for the url is reached or the key used to sign the url is rotated and the command to create the sign url is shown here and as you can see has been assigned for a limited time of 10 minutes so as you've seen when it comes to cloud storage there are so many configuration options to choose from and lots of different ways to store and give access and this makes this resource from google cloud such a flexible option and full of great potential for many different types of workloads this is also a service that comes up a lot in the exam as one of the many different storage options to choose from and so knowing the features storage classes pricing and access options will definitely give you a leg up when you are presented with questions regarding storage and so that's pretty much all i wanted to cover when it comes to this overview on cloud storage so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i will be covering object versioning and life cycle management a feature within cloud storage that is used to manage and sort through older files that need to be deleted along with files that are not in high need of regular access knowing the capabilities of these two features can really help organize accumulated objects in storage buckets and cut down on costs so without further ado let's dive in now to understand a bit more about objects i wanted to dive into immutability and versioning now objects are immutable which means that an uploaded object cannot change throughout its storage lifetime an object's storage lifetime is the time between a successful object creation or upload and successful object deletion this means that you cannot edit objects in place instead objects are always replaced with a new version so after the upload of the new object completes the new version of the object is served to readers this replacement marks the end of one object's life cycle and the beginning of a new one now to support the retrieval of objects that are deleted or replaced cloud storage offers the object versioning feature object versioning retains a noncurrent object version when the live object version gets replaced or deleted enabling object versioning increases storage costs which can be partially mitigated by configuring object lifecycle management to delete older object versions but more on that in just a bit cloud storage uses two properties that together identify the version of an object the generation which identifies the version of the object's data and the meta generation which identifies the version of the object's metadata these properties are always present with every version of the object even if object versioning is not enabled these properties can be used to enforce ordering of updates so in order to enable object versioning you would do that by enabling it on a bucket once enabled older versions remain in your bucket when a replacement or deletion occurs so by default when you replace an object cloud storage deletes the old version and adds a new version these older versions retain the name of the object but are uniquely identified by their generation number when object versioning has created an older version of an object you can use the generation number to refer to the older version this allows you to restore a replaced object in your bucket or permanently delete older object versions that you no longer need and so touching back on cost for just a minute these versions can really add up and start costing you some serious money if you have thousands of files with hundreds of versions and this is where life cycle management comes into play now cloud storage offers the object lifecycle management feature in order to support some common use cases like setting a time to live or ttl for objects retaining noncurrent versions of objects or downgrading storage classes of objects to help manage costs now in order to apply this feature to your objects you would assign a lifecycle management configuration to a bucket the configuration contains a set of rules which apply to current and feature objects in the bucket when an object meets the criteria of one of the rules cloud storage automatically performs the specified action on the object and so some example use cases are shown here so if you're looking to downgrade the storage class of objects older than 365 days to cold line storage for compliance purposes along with saving money life cycle management is perfect for this another use case is when you want to delete objects created before january 1st of 2020 and this is another great use case to save money as well with keeping only the three most recent versions of each object in a bucket with versioning enabled to keep from version objects building up object lifecycle management has so many other use cases across a myriad of industries and when used correctly is a great way to achieve object management along with saving money now i wanted to take a moment to dive into the lifecycle management configuration each lifecycle management configuration contains a set of components these are a set of rules conditions and the action when the conditions are met rules are any set of conditions for any action conditions is something an object must meet before the action defined in the rule occurs on the object and there are various conditions to choose from that allows you to get pretty granular and finally the action which is where you would have the option to delete or set storage class now when you delete current versions this will move the current version into a noncurrent state and when you delete a noncurrent version you will permanently delete the version and cannot get it back and so when you set the storage class it will transition the object to a different storage class so when defining a rule you can specify any set of conditions for any action if you specify multiple conditions in a rule an object has to match all of the conditions for the action to be taken so if you have three conditions and one of those conditions have not been met then the action will not take place if you specify multiple rules that contain the same action the action is taken when an object matches the conditions in any of these rules now if multiple rules have their conditions satisfied simultaneously for a single object cloud storage will either perform the delete action as it takes precedence over the set storage class action or the set storage class action that switches the object to the storage class with the lowest at rest storage pricing takes precedence so for example if you have one rule that deletes an object and another rule that changes the object storage class but both rules use the exact same condition the delete action always occurs when the condition is met or if you have one rule that changes the object storage class to near line storage and another rule that changes the object storage class to cold line storage but both rules use the exact same condition the object storage class always changes to cold line storage when the condition is met and so some considerations that i wanted to point out when it comes to cloud storage is that when it comes to object life cycle management changes are in accordance to object creation date as well once an object is deleted it cannot be undeleted so please be careful when permanently deleting a version as well life cycle rules can take up to 24 hours to take effect so be aware when setting them and always be sure to test these life cycle rules in development first before rolling them out into production and so that's pretty much all i had to cover when it comes to versioning and object life cycle management and so you can now mark this lesson as complete and whenever you're ready join me in the console where we go handson with versioning object life cycle management and cloud storage as a whole welcome back in this demo we're going to cement the knowledge that we learned from the past couple lessons on cloud storage and really dive into the nitty gritty when it comes to the features and configuration you're first going to create a cloud storage bucket and upload some files to it and then interact with the bucket and the files using the console as well you're going to get your hands dirty using the gsutil command line tool and this is the tool for managing cloud storage from the command line now there's quite a bit of work to do here so with that being said let's dive in and so i am logged in here as tony bowties at gmail.com along with being in project bowtie inc and so the first thing i want to do is i want to create a cloud storage bucket so in order for me to do that i'm going to head over to the navigation menu and i'm going to scroll down to storage and here i already have a couple of buckets that i created from earlier lessons and you may have a couple buckets as well but you're going to go ahead and create a new bucket by going up to the top here and click on create bucket now i know that we've gone through this before in previous lessons but this time i wanted to go through all the configuration options that are available and so the first thing that you're prompted to do here is to name your bucket as explained in an earlier lesson it needs to be a globally unique name and so you can pick any name you choose and so for me i'm going to call this bucket bowtie inc dash 2021 i'm going to hit continue and if it wasn't a globally unique name it would error out and you would have to enter in a new name but since this bucket name is globally unique i'm able to move forward for location type you can select from region dual region and multi region with multi region under location you can select from either the americas europe or asia pacific and under dual region you have the options of again choosing from america's europe and asia pacific and you will be given the regions for each and so for this demo we're going to go ahead and choose region and we're going to keep the location as u.s east one and once you've selected that you can go ahead and hit continue and you're going to be prompted to choose a default storage class and here you have the option of selecting from the four storage classes that we discussed in an earlier lesson and so for this demo you can keep it as standard and simply click on continue and so here you're prompted to choose access control and because we're going to be diving into acls you can keep this as the default fine grain access control you can go ahead and click continue and under encryption you can keep it as the default google manage key but know that you always have the option of choosing a customer manage key and once you've uploaded your customer manage key you can select it from here and because i have no customer managed keys no other keys show up so i'm going to click on google manage keys and here under retention policy i know i haven't touched into that but just to give you some context when placing a retention policy on a bucket it ensures that all current and future objects in the bucket can't be deleted or replaced until they reach the age that you define in the retention policy so if you try to delete or replace objects where the age is less than the retention period it will obviously fail and this is great for compliance purposes in areas where logs need to be audited by regulators every year or where government required retention periods apply as well with the retention policy you have the option of locking that retention policy and when you lock a retention policy on a bucket you prevent the policy from ever being removed or the retention period from ever being reduced and this feature is irreversible so please be aware if you're ever experimenting with lock retention policies so if i set a retention policy here i can retain objects for a certain amount of seconds days months and years and for this demo we're not going to set any retention policies so i'm going to check that off and i'm going to go ahead and add a label with the key being environment and the value being test and just as a note before you go ahead and click on create over on the right hand side you will see a monthly cost estimate and you will be given an estimate with storage and retrieval as well as how much it costs for operations your sla and your estimated monthly cost and so before creating any buckets you can always do a price check to see how much it'll cost for storage size retrieval to get a good idea of how much it'll cost you monthly okay so once you're all done here you can simply click on create and it'll go ahead and create your bucket and so now that your bucket is created we want to add some files and so we first want to go into copying files from an instance to your cloud storage bucket and so in order to do that we need to create an instance and so we're gonna go back over to the navigation menu we're gonna scroll down to compute engine and we're gonna create our instance and for those who do not have your default vpc set up please be sure to create one before going ahead and creating your instance i'm going to go ahead and click on create i'm going to name this instance bowtie instance going to give it a label of environment test click on save the region is going to be east one and you can keep the default zone as us east 1b the machine type we're going to change it to e2micro and you're going to scroll down to access scopes and here your instance is going to need access to your cloud storage bucket and so it's going to need cloud storage access so you're going to click on set access for each api scroll down to storage and for this demo we'll select full gonna leave everything else as the default and simply click on create and so we'll give it a couple minutes here for instance to create okay and my instance has been created and so now i want to create some files and copy them over to cloud storage so i'm going to first navigate over to cloud storage and into my bucket and this way you can see the files that you upload and so next you're going to open up cloud shell and make this a little bigger for better viewing and so now you're going to ssh into your instance by using the command gcloud compute ssh along with your instance name the zone flag dash dash zone with the zone of us east 1b i'm going to go ahead and hit enter and you may be prompted with a message asking to authorize this api call and you want to hit authorize and you're going to be prompted to enter a passphrase for your key pair enter it in again and one more time and success we're logged into the instance i'm going to quickly clear my screen and so i know i could have sshed into the instance from the compute engine console but i wanted to display both the console and the shell on the same screen to make viewing a bit easier as i add and remove files to and from the bucket okay and so now that you're logged in you want to create your first file that you can copy over to your bucket so you can enter in the command sudo nano file a bow ties dot text hit enter and this will allow you to open up the nano editor to edit the file of bowties.txt and here you can enter in any message that you'd like for me i'm going to enter in learning to tie a bow tie takes time okay and i'm going to hit ctrl o to save hit enter to verify the file name to right and ctrl x to exit and so now i want to copy this file up to my bucket and so here is where i'm going to use the gsutil command so i'm going to type in gsutil cp for copy the name of the file which is file of bowties text along with gs colon forward slash forward slash and the name of your bucket which in my case is bow tie ink dash 2021 and this should copy my file file a bowties.txt up to my bucket of bow tie inc 2021 i'm gonna hit enter okay and it's finished copying over and if i go up here to the top right and click on refresh i can see that my file successfully uploaded and this is a great and easy method to upload any files that you may have to cloud storage okay and so now that you've copied files from your instance to your bucket you're going to now copy some files from the repo to be uploaded to cloud storage for our next step so you're gonna go ahead and exit out of the instance by just simply typing in exit i'm gonna quickly clear the screen and so here i need to clone my repo if you already have clone the repo then you can skip this step i'm going to cd tilde to make sure i'm in my home directory i'm going to do an ls and so i can see here that i've already cloned my repo so i'm going to cd into that directory and i'm going to run the command git pull to get the latest files fantastic i'm going to now clear my screen and i'm going to cd back to my home directory and so now i want to copy up the files that i want to work with to my cloud storage bucket and they are two jpegs by the name of pink elephantbowtie as well as plaid bowtie and these files can be found in the repo marked 12 storage services under zero one cloud storage management and i will be providing this in the lesson text as well as can be found in the instructions and so i'm going to simply cd into that directory by typing in cd google cloud associate cloud engineer 12 storage services and 0 1 cloud storage management i'm going to list all the files in the directory and as you can see here pink elephant dash bow tie and plaid bow tie are both here and so i'm going to quickly clear my screen and so now for me to copy these files i'm going to use the command gsutil cp for copy star.jpg which is all the jpegs that are available along with gs colon forward slash forward slash and the bucket name which is bow tie inc dash 2021 i'm going to hit enter and it says that it's successfully copied the files i'm going to simply go up to the top right hand corner and do another refresh and success the files have been successfully uploaded another perfect example of copying files from another source to your bucket using the gsutil command line tool and so this is the end of part one of this demo it was getting a bit long so i decided to break it up and this would be a great opportunity for you to get up and have a stretch get yourself a coffee or tea and whenever you're ready part two will be starting immediately from the end of part one so you can complete this video and i will see you in part two this is part two of the managing cloud storage access demo and we'll be starting exactly where we left off in part 1. so with that being said let's dive in and so now that we've uploaded all these files we next want to make this bucket publicly available now please know that leaving a bucket public is not common practice and should only be used on the rare occasion that you are hosting a static website from your bucket and should always be kept private whenever possible especially in a production environment so please note that this is only for the purposes of this demo and so i'm going to quickly show this to you in the console so i'm going to shut down the cloud shell for just a minute and i'm going to go to the top menu and click on permissions and under permissions i'm going to click on add here you can add new members and because you want to make it publicly available you want to use the all users member so you type in all and you should get a popup bringing up all users and all authenticated users you want to click on all users and the role that you want to select for this demo is going to be storage object viewer so i'm going to type in storage object viewer and here it should pop up and select that and then you can click on save you're going to be prompted to make sure that this is what you want to do that you want to make this bucket public and so yes we do so you can simply click on allow public access and you will get a banner up here at the top saying that this bucket is public to internet and is a great fail safe to have in case you were to ever mistakenly make your bucket public and if i head back over to objects you can see that public access is available to all the files in the bucket and so just to verify this i'm going to copy the public url for pink elephant dash bowtie i'm going to open up a new tab paste in the url hit enter and as you can see i have public access to this picture and close this tab and so now that we've done our demo to make the bucket publicly accessible we should go ahead and remove public access so in order to remove public permissions i can simply go up to permissions and simply click on remove public permissions i'm going to get a prompt to make sure this is exactly what i want to do and yes it is so you can click on remove public permissions a very simple and elegant solution in order to remove public access from your bucket and if you go back to objects you'll see that all the public access has been removed from all the files and so now that you've experienced how to add public access to a bucket i wanted to get a little bit more granular and so we're going to go ahead and apply acl permissions for one specific object and because i like pink elephants let's go ahead and select pink elephant dash bow tie and so here i can go up to the top menu and click on edit permissions and i'll be prompted with a new window for permissions that are currently available for this object you can click on add entry click on the drop down and select public from the dropdown and it will automatically auto populate the name which is all users and the access which will be reader i'm going to go ahead and click on save and a public url will be generated and so just to verify this i'm going to click on the public url and success i now have public access to this picture yet once again i'm going to close down this tab and so now that you've configured this object for public access i want to show you how to remove public access using the command line this time so you're going to go up to the top right hand corner and open up cloud shell i'm going to quickly clear my screen and i'm going to paste in the command here which is gsutil acl ch for change minus d which is delete the name of the user which is all users and if this was a regular user you could enter in their email address along with gs colon forward slash forward slash the bucket name which in my case is bow tie ink dash 2021 and the name of the file which is pink elephant bow tie dot jpeg i'm going to hit enter and it says that it's been successfully updated and so if i go back up here to the console and i back out and go back into the file i can see here that the public url has been removed okay and now there's one last step that we need to do before ending this demo and this is to create a signed url for the file so in order to create a signed url we first need to create a private key and so we're gonna do this using a service account and so i'm gonna head on over to iam so i'm going to go up to the navigation menu i'm going to go to i am an admin and here with the menu on the left i'm going to click on service accounts here up at the top menu you're going to click on create service account and under service account name you can enter in any name but for me i'm going to enter in signed url i'm going to leave everything else as is i'm going to simply click on create i'm going to close down cloud shell because i don't really need it right now just select a role and i'm going to give it the role of storage object viewer i'm going to click on continue and i'm going to leave the rest blank and simply click on done and you should see a service account with the name of signed url and so in order to create a key i'm going to simply go over to actions and i'm going to click on the three dots and i'm going to select create key from the drop down menu and here i'm going to be prompted with what type of key that i want to create and you want to make sure that json is selected and simply click on create and this is where your key will be automatically downloaded to your downloads folder i'm going to click on close and so once you have your key downloaded you're able to start the process of generating a signed url and so i'm going to go ahead and use cloud shell in order to generate this signed url so i'm going to go ahead back up to the top and open up cloud shell again and then you can open up the cloud shell editor going to go up to the top menu in editor and click on file and you're going to select upload files and here's where you upload your key from your downloads folder and i can see my key has been uploaded right here and you can rename your key file to something a little bit more human readable so i'm going to right click i'm going to click on rename and you can rename this file as privatekey.json hit ok and so once you have your key uploaded and renamed you can now go back into the terminal to generate a signed url i'm going to quickly clear the screen i'm going to make sure that the private key is in my path by typing in ls and as you can see here privatekey.json is indeed in my path and so before i generate this key i'm going to head back on over to cloud storage i'm going to drill down into my bucket and as you can see here pink elephant dash bow tie does not have a public url and so when the sign url is generated you will get a public url that will not be shown here in the console and will be private to only the user that generated it and the users that the url has been distributed to okay and once you have everything in place you can then go ahead and paste in the command gsutil sign url minus d the allotted time which is 10 minutes the private key which is private key dot json along with gs colon forward slash forward slash your bucket name which in my case is bow tie ink dash 2021 along with the file name of pinkelephantbowtie.jpg i'm going to hit enter and so i purposely left this error here so you can see that when you generate a signed url you need pi open ssl in order to generate it and so the caveat here is that because python 2 is being deprecated the command pip install pi openssl will not work pi open ssl needs to be installed with python3 and so to install it you're going to run the command pip3 install pi open ssl and hit enter and so once it's finished installing you can now generate your signed url i'm going to quickly clear my screen paste in the command again hit enter and success you've now generated a sign url for the object pink elephant bowtie.jpg and because this is a signed url you will see under public url there is no url there available even though it is publicly accessible and so just to verify this i'm going to highlight the link here i'm going to copy it i'm going to open up a new tab i'm going to paste in this url hit enter and success this sign url is working and anyone who has access to it has viewing permissions of the file for 10 minutes and so again this is a great method for giving someone access to an object who doesn't have an account and will give them a limited time to view or edit this object and so i wanted to congratulate you on making it through this demo and hope that it has been extremely useful in excelling your knowledge on managing buckets files and access to the buckets and files in cloud storage and so just as a recap you created a cloud storage bucket you then created an instance and copied a file from that instance to the bucket you then clone your repo to cloud shell and copy two jpeg files to your cloud storage bucket you then assigned and then removed public access to your bucket and then applied an acl to a file in the bucket making it public as well as removing public access right after you then created a service account private key and generated a signed url to an object in that bucket congratulations again on a job well done and so that's pretty much all i wanted to cover in this demo on managing cloud storage access so you can now mark this as complete and let's move on to the next one welcome back in this demo we're going to be getting into the weeds with object versioning and life cycle management using both the console and the command line we're going to go through how versioning works and what happens when objects get promoted along with creation configuration and editing these life cycle policies and so with that being said let's dive in so we're going to be starting off from where we left off in the last demo with all the resources intact that we created before and we're going to go ahead and dive right into versioning and so the first thing that you want to do is turn on versioning for your current bucket so in my case for bow tie ink dash 2021 and we're going to do this through the command line so i'm going to first go up to the top right hand corner and open up cloud shell and so you first want to see if versioning is turned on for your bucket and you can do this by using the command gsutil versioning get along with gs colon forward slash forward slash with your bucket name and hit enter and you may be prompted with a message asking you to authorize this api call you definitely want to authorize and as expected versioning is not turned on on this bucket hence the return of suspended and so in order to turn versioning on we're going to use a similar command gsutil versioning and instead of get we're going to use set on gs colon forward slash forward slash and the bucket name and hit enter and versioning has been enabled and so if i run the command gsutil version in get again i'll get a response of enabled okay great now that we have versioning enabled we can go ahead with the next step which is to delete one of the files in the bucket and so you can go ahead and select plaid bowtie.jpg and simply click on delete you can confirm the deletion and the file has been deleted now technically the file has not been deleted it is merely been converted to a noncurrent version and so in order to check the current and noncurrent versions i'm going to use the command gsutil ls minus a along with the bucket name of g s colon forward slash forward slash bow tie inc dash 2021 i'm gonna hit enter and as you can see here plaid bow tie still shows up the ls minus a command is a linux command to show all files including the hidden files and so what's different about these files is right after the dot text or dot jpg you will see a hashtag number and this is the generation number and this determines the version of each object and so what i want to do now is bring back the noncurrent version and make it current so i'm going to promote the noncurrent version of plaid bowtie.jpg to the current version and so in order to do this i'm going to run the command gsutil and v for move along with the bucket of gs colon forward slash forward slash bowtie inc hyphen 2021 and the name of the file of plaid bow tie dot jpeg along with the generation number and i'm going to copy it from the currently listed i'm going to paste it in and so now we need to put in the target which is going to be the same without the generation number and paste that in then hit enter okay operation completed and so if i go up to the top right hand corner and click on refresh i can see that now there is a current version for plaid bow tie now just know that using the move command actually deletes the noncurrent version and gives the new current version a new generation number and so in order to verify this i'm going to quickly clear my screen and i'm going to run the command gsutil ls minus a along with the bucket name a bow tie inc dash 2021 and the generation number here is different than that of the last now if i use the cp or copy command it would leave the noncurrent version and create a new version on top of that leaving two objects with two different generation numbers okay so with that step being done you now want to log into your linux instance and we're going to be doing some versioning for file of bowties.text so i'm going to go ahead and clear my screen again and i'm going to run the command gcloud compute ssh bowtie instance which is the name of my instance along with the zone flag dash dash zone of the zone us east 1b i'm going to hit enter and you should be prompted for the passphrase of your key and i'm in and so here you want to edit file a bowties.txt to a different version so you can go ahead and run the command sudo nano file a bow ties dot text and hit enter and you should have learning to tie a bow tie takes time and what you want to do is append version 2 right at the end ctrl o to save enter to verify the file name to right and control x to exit and so now we want to copy file a bow ties dot text to your current bucket mine being bow tie ink dash 2021 so i'm going to go ahead and run the command gsutil cp the name of the file which is file of bowties dot text and the target which is going to be bowtie inc 2021 and hit enter and it's copied the file to the bucket and so if i hit refresh in the console you can see that there is only one version of file of bowties.text and so to check on all the versions that i have i'm going to go back to my cloud shell i'm going to quickly clear my screen and i'm going to run the command gsutil ls minus a along with the target bucket hit enter and as you can see here there are now two versions of file of bowties.text and if i quickly open this up i'm gonna click on the url you can see here that this is version two and so this should be the latest generation of file of bowties.txt that you edited over in your instance i'm going to close this tab now and so what i want to do now is i want to promote the noncurrent version to be the current version in essence making version 2 the noncurrent version and so i'm going to run the command gsutil cp and i'm going to take the older generation number and i'm going to copy it and paste it here and the target is going to be the same without the generation number and paste it and hit enter okay and the file has been copied over so i'm going to do a quick refresh in the console i'm going to drill down into file a bowties.txt and when i click on the url link it should come up as version 1. and so this is a way to promote noncurrent versions to current versions using the gsutil copy command or the gsutil move command i'm going to close on this tab now i'm going to quickly clear my screen and if i run the command gsutil ls minus a again you can see that i have even more files and so these files and versions of files will eventually accumulate and continuously take up space along with costing you money and so in order to mitigate this a good idea would be to put life cycle policies into place and so you're gonna go ahead now and add a life cycle policy to the bucket and this will help manage the evergrowing accumulation of files as more files are being added to the bucket and more versions are being produced something that is very common that is seen in many different environments and so we're going to go ahead and get this done in the console so i'm going to close down cloud shell and i'm going to go back to the main page of the bucket and under the menu you can click on lifecycle and here you'll be able to add the lifecycle rules and so here you're going to click on add a rule and the first thing that you're prompted to do is to select an action and so the first rule you're going to apply is to delete noncurrent objects after seven days so you're gonna click on delete object you're gonna be prompted with a warning gonna hit continue and you'll be prompted to select object conditions and as discussed in an earlier lesson there are many conditions to choose from and multiple conditions can be selected so here you're going to select days since becoming noncurrent and in the empty field you're going to type in 7. you can click on continue and before you click on create i wanted just to note that any life cycle rule can take up to 24 hours to take effect so i'm going to click on create and here you can see the rule has been applied to delete objects after seven days when object becomes noncurrent and so now that we added a delete rule we're going to go ahead and add another rule to move current files that are not being used to a storage class that can save the company money and so let's go ahead and create another lifecycle rule but this time to use this set storage class action and so the files that accumulate that have been there for over 90 days you want to set the storage class the cold line so this way it'll save you some money and so you're going to click on add a rule you're going to select set storage class to cold line and as a note here it says archive objects will not be changed to cold line so you can move forward with the storage class but you can't move backwards in other words i can't move from cold line to near line or archive the cold line i can only move from near line to cold line or cold line to archive so i'm going to go ahead and click continue for the object conditions you want to select age and in the field you want to enter 90 days and here you want to hit continue and finally click on create and so in order to actually see these rules take effect like i said before it'll take up to 24 hours and so before we end this demo i wanted to show you another way to edit a life cycle policy by editing the json file itself so you can head on up to the top right and open up cloud shell i'm going to bring this down a little bit and you're going to run the command gsutil lifecycle get along with the bucket name and output it to a file called lifecycle.json and hit enter and no errors so that's a good sign next i'm going to run the command ls and as you can see here the lifecycle.json file has been written and so i'd like to edit this file where it changes the set to cold line rule from 90 days to 120 days as tony bowtie's manager thinks that they should keep the files a little bit longer before sending it to coldline and so in order to edit this file you're going to run the command sudo nano along with the name of the file of lifecycle.js you hit enter and it's going to be a long string but if you use your arrow keys and move down and then back you'll see the set to cold line rule with the age of 90 days so i'm going to move over here and i'm going to edit this to 120 and i'm going to hit ctrl o to save enter to verify file name to write and ctrl x to exit and just know that you can also edit this file in cloud shell editor and so in order for me to put this lifecycle policy in place i need to set this as the new lifecycle policy and so in order for me to do that i'm going to run the command gsutil lifecycle set along with the name of the json file which is lifecycle.json along with the bucket name and hit enter and it looks like it said it and i'm going to do quick refresh in the console just to verify and success the rule has been changed from 90 days to 120 days congratulations on completing this demo now a lot of what you've experienced here is more of what you will see in the architect exam as the cloud engineer exam focuses on more of the high level theory of these cloud storage features but i wanted to show you some real life scenarios and how to apply the theory that was shown in previous lessons into practice and so just as a recap you set versioning on the current bucket that you are working in and you deleted a file and made it noncurrent you then brought it back to be current again you then edited a file on your instance and copied it over to replace the current version of that file in your bucket you then promoted the noncurrent version as the new one and moved into lifecycle rules where you created two separate rules you created a rule to delete files along with the rule to set storage class after a certain age of the file and the last step you took was to copy the lifecycle policy to your cloud shell and edited that policy and set it to a newer edited version and so that pretty much covers this demo on object versioning and lifecycle management congratulations again on a job well done and so before you go make sure you delete all the resources you've created for the past couple of demos as you want to make sure that you're not accumulating any unnecessary costs and so i'm going to do a quick run through on deleting these resources and so i'm going to quickly close down cloud shell and i'm going to head on over to the navigation menu go to compute engine i'm going to delete my instance and i'm going to head back on over to cloud storage and delete the bucket there i'm going to confirm the deletion i'm going to click on delete and so that covers the deletion of all the resources so you can now mark this as complete and i'll see you in the next one welcome back and in this lesson i'm going to be covering cloud sql one of google cloud's many database offerings that offers reliable secure and scalable sql databases without having to worry about the complexity to set it all up now there's quite a bit to cover here so with that being said let's dive in now cloud sql is a fully managed cloud native relational database service that offers mysql postgres and sql server engines with builtin support for replication cloud sql is a database as a service offering from google where google takes care of all the underlying infrastructure for the database along with the operating system and the database software now because there are a few different types of database offerings from google cloud sql was designed for low latency transactional and relational database workloads it's also available in three different flavors of databases mysql postgres and the newest edition is sql server and all of them support standard apis for connectivity cloud sql offers replication using different types of read replicas which i will get into a little bit later and offers capabilities for high availability for continuous access to your data cloud sql also offers backups in two different flavors and allows you to restore your database from these backups with the same amount of ease now along with your backups comes point in time recovery for when you want to restore a database from a specific point in time cloud sql storage relies on connected persistent disks in the same zone that are available in regular hard disk drives or ssds that currently give you up to 30 terabytes of storage capacity and because the same technologies lie in the background for persistent disks automatic storage increase is available to resize your disks for more storage cloud sql also offers encryption at rest and in transit for securing data entering and leaving your instance and when it comes to costs you are billed for cpu memory and storage of the instance along with egress traffic as well please be aware that there is a licensing cost when it comes to windows instances now cloud sql instances are not available in the same instance types as compute engine and are only available in the shared core standard and high memory cpu types and when you see them they will be clearly marked with a db on the beginning of the cpu type you cannot customize these instances like you can with compute engine and so memory will be predefined when choosing the instance type now storage types for cloud sql are only available in hard disk drives and ssds you are able to size them according to your needs and as stated earlier can be sized up to 30 terabytes in size and when entering the danger zone of having a full disk you do have the option of enabling automatic storage increase so you never have to worry about filling up your disk before that 30 terabyte limit now when it comes to connecting to your cloud sql instance you can configure it with a public or private ip but know that after configuring the instance with a private ip it cannot be changed although connecting with the private ip is preferred when connecting from a client on a resource with access to a vpc as well it is always best practice to use private i p addresses for any database in your environment whenever you can now moving on to authentication options the recommended method to connecting to your cloud sql instance is using cloud sql proxy the cloud sql proxy allows you to authorize and secure your connections using iam permissions unless using the cloud sql proxy connections to an instance's public ip address are only allowed if the connection comes from an authorized network authorized networks are ip addresses or ranges that the user has specified as having permission to connect once you are authorized you can connect to your instance through external clients or applications and even other google cloud services like compute engine gke app engine cloud functions and cloud run now i wanted to focus a moment here on the recommended method for connecting to your instance which is cloud sql proxy now as mentioned before the cloud sql proxy allows you to authorize and secure your connections using iam permissions the proxy validates connections using credentials for a user or service account and wrapping the connection in an ssl tls layer that is authorized for a cloud sql instance using the cloud sql proxy is the recommended method for authenticating connections to a cloud sql instance as it is the most secure the client proxy is an open source library distributed as an executable binary and is available for linux macos and windows the client proxy acts as an intermediary server that listens for incoming connections wraps them in ssl or tls and then passes them to a cloud sql instance the cloud sql proxy handles authentication with cloud sql providing secure access to cloud sql instances without the need to manage allowed ip addresses or configure ssl connections as well this is also the best solution for applications that hold ephemeral eyepiece and while the proxy can listen on any port it only creates outgoing connections to your cloud sql instance on port 3307 now when it comes to database replication it's more than just copying your data from one database to another the primary reason for using replication is to scale the use of data in a database without degrading performance other reasons include migrating data between regions and platforms and from an onpremises database to cloud sql you could also promote a replica if the original instance becomes corrupted and i'll be getting into promoting replicas a little bit later now when it comes to a cloud sql instance the instance that is replicated is called a primary instance and the copies are called read replicas the primary instance and read replicas all reside in cloud sql read replicas are readonly and you cannot write to them the read replica processes queries read requests and analytics traffics thus reducing the load on the primary instance read replicas can have more cpus in memory than the primary instance but they cannot have any less and you can have up to 10 read replicas per primary instance and you can connect to a replica directly using its connection name and ip address cloud sql supports the following types of replicas read replicas cross region read replicas external read replicas and cloud sql replicas when replicating from an external server now when it comes to read replicas you would use it to offload work from a cloud sql instance the read replica is an exact copy of the primary instance and data and other changes on the primary instance are updated in almost real time on the read replica a read replica is created in a different region from the primary instance and you can create a cross region read replica the same way as you would create an inregion replica this improves read performance by making replicas available closer to your application's region it also provides additional disaster recovery capability to guard you against a regional failure it also lets you migrate data from one region to another with minimum downtime and lastly when it comes to external read replicas these are external mysql instances that replicate from a cloud sql primary instance for example a mysql instance running on compute engine is considered an external instance and so just as a quick note here before you can create a read replica of a primary cloud sql instance the instance must meet the following requirements automated backups must be enabled binary logging must be enabled which requires pointintime recovery to be enabled and at least one backup must have been created after binary logging was enabled and so when you have read replicas in your environment it gives you the flexibility of promoting those replicas if needed now promoting replicas is a feature that can be used for when your primary database becomes corrupted or unreachable now you can promote an inregion read replica or crossregion rereplica depending on where you have your read replicas hosted so when you promote a read replica the instance stops replication and converts the instance to a standalone cloud sql primary instance with read and write capabilities please note that this cannot be undone and also note that when your new primary instance has started your other read replicas are not transferred over from the old primary instance you will need to reconnect your other read replicas to your new primary instance and as you can see here promoting a replica is done manually and intentionally whereas high availability has a standby instance that automatically becomes the primary in case of a failure horizontal outage now when it comes to promoting crossregion replicas there are two common scenarios for promotion regional migration which performs a planned migration of a database to a different region and disaster recovery and this is where you would fail over a database to another region in the event that the primary instances region becomes unavailable both use cases involve setting up crossregion replication and then promoting the replica the main difference between them is whether the promotion of the replica is planned or unplanned now if you're promoting your replicas for a regional migration you can use a cross region replica to migrate your database to another region with minimal downtime and this is so you can create a replica in another region wait until the replication catches up promote it and then direct your applications to the newly promoted instance the steps involved in promotion are the same as for promoting an inregion replica and so when you're promoting replicas for disaster recovery crossregion replicas can be used as part of this disaster recovery procedure you can promote a crossregion replica to fail over to another region should the primary instances region become unavailable for an extended period of time so in this example the entire u.s east 1 region has gone down yet the reed replica in the europe region is still up and running and although there may be a little bit more latency for your customers in north america i'm able to promote this read replica connect it to the needed resources and get back to business now moving along to high availability cloud sql offers aha capabilities out of the box the aha configuration sometimes called a cluster provides data redundancy so a cloud sql instance configured for ha is also called a regional instance and is located in a primary and secondary zone within the configured region within a regional instance the configuration is made up of a primary instance and a standby instance and through synchronous replication to each zone's persistent disk all rights made to the primary instance are also made to the standby instance each second the primary instance writes to a system database as a heartbeat signal if multiple heartbeats aren't detected failover is initiated and so if an haconfigured instance becomes unresponsive cloud sql automatically switches to serving data from the standby instance and this is called a failover in this example the primary instance or zone fails and failover is initiated so if the primary instance is unresponsive for approximately 60 seconds or the zone containing the primary instance experiences an outage failover will initiate the standby instance immediately starts serving data upon reconnection through a shared static ip address with the primary instance and the standby instance now serves data from the secondary zone and now when the primary instance is available again a fail back will happen and this is when traffic will be redirected back to the primary instance and the standby instance will go back into standby mode as well the regional persistent disk will pick up replication to the persistent disk in that same zone and with regards to billing an ha configured instance is charged at double the price of a standalone instance and this includes cpu ram and storage also note that the standby instance cannot be used for read queries and this is where it differs from read replicas as well a very important note here is that automatic backups and point in time recovery must be enabled for high availability and so the last topic that i wanted to touch on is backups and backups help you restore lost data to your cloud sql instance you can also restore an instance that is having problems from a backup you enable backups for any instance that contains necessary data backups protect your data from loss or damage enabling automated backups along with binary logging is also required for some operations such as clone and replica creation by default cloud sql stores backup data in two regions for redundancy one region can be the same region that the instance is in and the other is a different region if there are two regions in a continent the backup data remains on the same continent cloud sql also lets you select a custom location for your backup data and this is great if you need to comply with data residency regulations for your business now cloud sql performs two types of backups ondemand backups and automated backups now with ondemand backups you can create a backup at any time and this is useful for when you're making risky changes that may go sideways you can always create ondemand backups for any instance whether the instance has automatic backups enabled or not and these backups persist until you delete them or until their instance is deleted now when it comes to automated backups these use a four hour backup window these backups start during the backup window and just as a note when possible you should schedule your backups when your instance has the least activity automated backups occur every day when your instance is running at any time in the 36 hour window and by default up to seven most recent backups are retained you can also configure how many automated backups to retain from 1 to 365. now i've touched on this topic many times in this lesson and i wanted to highlight it for just a second and this is pointintime recovery so pointintime recovery helps you recover an instance to a specific point in time for example if an error causes a loss of data you can recover a database to its state before the error happened a point in time recovery always creates a new instance and you cannot perform a point in time recovery to an existing instance and point in time recovery is enabled by default when you create a new cloud sql instance and so when it comes to billing by default cloud sql retains seven days of automated backups plus all ondemand backups for an instance and so i know there is a lot to retain in this lesson on cloud sql but be sure that these concepts and knowing the difference between them as well as when to use each feature will be a sure help in the exam along with giving you the knowledge you need to use cloud sql in your role as a cloud engineer and so that's pretty much all i had to cover when it comes to cloud sql so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson i wanted to touch on google cloud's global relational database called cloud spanner now cloud spanner is the same in some ways as cloud sql when it comes to asset transactions sql querying and strong consistency but differs in the way that data is handled under the hood than cloud sql and so knowing this database only at a high level is needed for the exam but i'll be going into a bit more detail just to give you a better understanding on how it works so with that being said let's dive in now cloud spanner is a fully managed relational database service that is both strongly consistent and horizontally scalable cloud spanner is another database as a service offering from google and so it strips away all the headaches of setting up and maintaining the infrastructure and software needed to run your database in the cloud now being strongly consistent in this context is when data will get passed on to all the replicas as soon as a write request comes to one of the replicas of the database cloud spanner uses truetime a highly available distributed atomic clock system that is provided to applications on all google servers it applies a time stamp to every transaction on commit and so transactions in other regions are always executed sequentially cloud spanner can distribute and manage data at a global scale and support globally consistent reads along with strongly consistent distributed transactions now being fully managed cloud spanner handles any replicas that are needed for availability of your data and optimizes performance by automatically sharding the data based on request load and size of the data part of why cloud spanner's high availability is due to its automatic synchronous data replication between all replicas in independent zones cloud spanner scales horizontally automatically within regions but it can also scale across regions for workloads that have higher availability requirements making data available faster to users at a global scale along with node redundancy quietly added for every node deployed in the instance and when you quickly add up all these features of cloud spanner it's no wonder that it's available to achieve five nines availability on a multiregional instance and four nines availability on a regional instance cloud spanner is highly secure and offers data layer encryption audit logging and iam integration cloud spanner was designed to fit the needs of specific industries such as financial services ad tech retail and global supply chain along with gaming and pricing for cloud spanner comes in at 90 cents per node per hour with the cost of storage coming in at 30 cents per gigabyte per month definitely not cheap but the features are plentiful now this isn't in the exam but i did want to take a moment to dive into the architecture for a bit more context as to why this database is of a different breed than the typical sql database now to use cloud spanner you must first create a cloud spanner instance this instance is an allocation of resources that is used by cloud spanner databases created in that instance instance creation includes two important choices the instance configuration and the node count and these choices determine the location and the amount of the instances cpu and memory along with its storage resources your configuration choice is permanent for an instance and only the node count can be changed later if needed an instance configuration defines the geographic placement and replication of the database in that instance either regional or multiregion and please note that when you choose a multizone configuration it allows you to replicate the databases data not just in multiple zones but in multiple zones across multiple regions and when it comes to the node count this determines the number of nodes to allocate to that instance these nodes allocate the amount of cpu memory and storage needed for your instance to either increase throughput or storage capacity there is no instance types to choose from like cloud sql and so when you need more power you simply add another node now for any regional configuration cloud spanner maintains exactly three read write replicas each within a different zone in that region each read write replica contains a full copy of your operational database that is able to serve rewrite and read only requests cloud spanner uses replicas in different zones so that if a single zone failure occurs your database remains available in a multiregion instance configuration the instance is allotted a combination of four read write and read only replicas and just as a note a three node configuration minimum is what is recommended for production by google and as cloud spanner gets populated with data sharding happens which is also known as a split and cloud spanner creates replicas of each database split to improve performance and availability all of the data in a split is physically stored together in a replica and cloud spanner serves each replica out of an independent failure zone and within each replica set one replica is elected to act as the leader leader replicas are responsible for handling rights while any read write or read only replica can serve a read request without communicating with the leader and so this is the inner workings of cloud spanner at a high level and not meant to confuse you but to give you a better context of how cloud spanner although it is a relational sql database is so different than its cloud sql cousin now before ending this lesson i wanted to touch on node performance for a quick moment and so each cloud spanner node can provide up to 10 000 queries per second or qps of reads or 2000 qps of writes each node provides up to two terabytes of storage and so if you need to scale up the serving and storage resources in your instance you add more nodes to that instance and remember as noted earlier that adding a node does not increase the number of replicas but rather increases the resources each replica has in the instance adding nodes gives each replica more cpu and ram which increases the replicas throughput and so if you're looking to scale up automatically you can scale the numbers of nodes in your instance based on the cloud monitoring metrics on cpu or storage utilization in conjunction with using cloud functions to trigger and so when you are deciding on a relational database that provides global distribution and horizontally scalable that handles transactional workloads in google cloud cloud spanner will always be the obvious choice over cloud sql and so that's pretty much all i have to cover when it comes to this overview on cloud spanner so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson we will be going over the available nosql databases available in google cloud this lesson is meant to be another overview just to familiarize you with the nosql database options as they show up in the exam this lesson is not meant to go in depth on databases but an overview and will give you a good understanding on what features are available for each and their use cases so with that being said let's dive in now there are four managed nosql databases available in google cloud and i will be briefly going over them and i'll be starting this off by discussing bigtable now cloud bigtable is a fully managed wide column nosql database designed for terabyte and petabyte scale workloads that offers low latency and high throughput bigtable is built for realtime application serving workloads as well as largescale analytical workloads cloud bigtable is a regional service and if using replication a copy is stored in a different zone or region for durability cloud bigtable is designed for storing very large amounts of single keyed data while still being able to provide very low latency and because throughput scales linearly you can increase the queries per second by adding more bigtable nodes when you need them bigtable throughput can be dynamically adjusted by adding or removing cluster nodes without restarting meaning you can increase the size of a bigtable cluster for just a few hours to handle a large load and then reduce the cluster size again and do it all without any downtime bigtable is an ideal source for map reduce operations and integrates easily with all the existing big data tools such as hadoop dataproc and dataflow along with apache hbase and when it comes to price bigtable is definitely no joke pricing for bigtable starts at 65 cents per hour per node or over 450 dollars a month for a one node configuration with no data now you can use bigtable to store and query all of the following types of data such as cpu and memory usage over time for multiple servers marketing data such as purchase histories and customer preferences financial data such as transaction histories stock prices and currency exchange rates iot data or internet of things such as usage reports from energy meters and home appliances and lastly graph data such as information about how users are connected to one another cloud bigtable excels as a storage engine as it can batch mapreduce operations stream processing or analytics as well as being used for storage for machine learning applications now moving on to the next nosql database is cloud datastore and cloud datastore is a highly scalable nosql document database built for automatic scaling high performance and ease of application development datastore is redundant within your location to minimize impact from points of failures and therefore can offer high availability of reads and rights cloud datastore can execute atomic transactions where a set of operations either all succeed or none occur cloud datastore uses a distributed architecture to automatically manage scaling so you never have to worry about scaling manually as well what's very unique about cloud datastore is that it has a sqllike query language that's available called gql also known as gql gql maps roughly to sql however a sql role column lookup is limited to a single value whereas in gql a property can be a multiple value property this consistency model allows an application to handle large amounts of data and users while still being able to deliver a great user experience data is automatically encrypted before it is written to disk and automatically decrypted when read by an authorized user now this does not reflect in the exam as of yet and i will be updating this lesson if and when it happens but firestore is the newest version of datastore and introduces several improvements over datastore existing datastore users can access these improvements by creating a new firestore database instance in datastore mode and in the near future all existing datastore databases will be automatically upgraded to firestore in datastore mode now moving right along cloud datastore holds a really cool feature for developers that's called datastore emulator and this provides local emulation of the production datastore environment so that you can use to develop and test your application locally this is a component of the google cloud sdks gcloud tool and can be installed by using the gcloud components install command that we discussed earlier on in the course and so moving on to use cases for datastore it is ideal for applications that rely on highly available structured data at scale you can use datastore for things like product catalogs that provide realtime inventory and product details for a retailer user profiles that deliver a customized experience based on the user's past activities and preferences as well as transactions based on asset properties for example transferring funds from one bank account to another next up we have firestore for firebase and so this is a flexible scalable nosql cloud database to store and sync data for client and server side development and is available for native c plus unity node.js java go and python sdks in addition to rest and rpc apis pretty much covering the gamut of most major programming languages now with cloud firestore you store data in documents that contain fields mapping to values these documents are stored in collections which are containers for your documents that you can use to organize your data and build queries documents support many different data types as well you can also create sub collections within documents and build hierarchical data structures cloud firestore is serverless with absolutely no servers to manage update or maintain and with automatic multiregion replication and strong consistency google is able to hold a five nines availability guarantee and so when it comes to querying in cloud firestore it is expressive efficient and flexible you can create shallow queries to retrieve data at the document level without needing to retrieve the entire collection or any nested subcollections cloud firestore uses data synchronization to update data in real time for any connected device as well it also caches data that your application is actively using so that the application can write read listen to and query data even if the device is offline when the device comes back online cloud firestore synchronizes any local changes back to cloud firestore you can also secure your data in cloud firestore with firebase authentication and cloud firestore security rules for android ios and javascript or you can use iam for server side languages and when it comes to costs firestore falls into the always available free tier where you can use one database holding five gigabytes or if you need more you can move into their paid option now firebase also has another database sharing similar features like having no servers to deploy and maintain realtime updates along with the free tier in this database is called real time database and is used for more basic querying simple data structure and keeping things to one database it's something i like to call firestore lite real time database does not show up in the exam but i wanted to bring it to light as it is part of the firebase family just know that you can use both databases within the same firebase application or project as both can store the same types of data client libraries work in a similar manner and both hold realtime updates now although firebase is a development platform and not a database service i wanted to give it a quick mention for those of you who are unfamiliar with the tiein to firestore with firebase firebase is a mobile application development platform that provides tools and cloud services to help enable developers to develop applications faster and more easily and since it ties in nicely with firestore it becomes the perfect platform for mobile application development okay so moving on to our last nosql database is memorystore and memorystore is a fully managed service from google cloud for either redis or memcached in memory datastore to build application caches and this is a common service used in many production environments specifically when the need for caching arises memory store automates the administration tasks for redis and memcached like enabling high availability failover patching and monitoring so you don't have to and when it comes to memory store for redis instances in the standard tier these are replicated across zones monitored for health and have fast automatic failover standard tier instances also provide an sla of three nines availability memory store for redis also provides the ability to scale instant sizes seamlessly so that you can start small and increase the size of the instance as needed memory store is protected from the internet using vpc networks and private ip and also comes with iam integration systems are monitored around the clock ensuring that your data is protected at all times and know that the versions are always kept up to date with the latest critical patches ensuring your instances are secure now when it comes to use cases of course the first thing you will see is caching and this is the main reason to use memory store as it provides low latency access and high throughput for heavily accessed data compared to accessing the data from a disk common examples of caching is session management frequently accessed queries scripts or pages so when using memory store for leaderboards and gaming this is a common use case in the gaming industry as well as using it for player profiles memory store is also a perfect solution for stream processing combined with data flow memory store for redis provides a scalable fast in memory store for storing intermediate data that thousands of clients can access with very low latency and so when it comes to nosql databases these are all the available options on google cloud and as i said before it will only show up on the exam at merely a high level and so knowing what each of these databases are used for will be a huge benefit along with being an entry to diving deeper into possibly using these services within your daytoday job as a cloud engineer and so that's pretty much all i wanted to cover when it comes to nosql databases available in google cloud so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson we'll be going over the big data ecosystem in an overview just to familiarize you with the services that are available in google cloud and are the services that will show up in the exam this lesson is not meant to go in depth but is an overview and will give you a good understanding on what these services can do and how they all work together to make sense of big data as a whole so getting right into it i wanted to first ask the question what is big data i mean many people talk about it but what is it really well big data refers to massive amounts of data that would typically be too expensive to store manage and analyze using traditional database systems either relational or monolithic as the amount of data that we have been seeing over the past few years has started to increase these systems have become very inefficient because of their lack of flexibility for storing unstructured data such as images text or video as well as accommodating high velocity or realtime data or scaling to support very large petabyte scale data volumes for this reason the past few years has seen the mainstream adoption of new approaches to managing and processing big data including apache hadoop and nosql database systems however those options often prove to be complex to deploy manage and use in an onpremises situation now the ability to consistently get business value from data fast and efficiently is now becoming the de facto of successful organizations across every industry the more data a company has access to the more business insights and business value they're able to achieve like gain useful insights increase revenue get or retain customers and even improve operations and because machine learning models get more efficient as they are trained with more data machine learning and big data are highly complementary all in all big data brings some really great value to the table that is impossible for any organization to turn down and so now that we've gone through that overview of what big data is i wanted to dive into some shorter overviews of the services available for the big data ecosystem on google cloud and so the first service that i'd like to start with is bigquery now bigquery is a fully managed serverless data warehouse that enables scalable analysis over petabytes of data this service supports querying using sql and holds builtin machine learning capabilities you start by ingesting data into bigquery and then you are able to take advantage of all the power it provides so big data would ingest that data by doing a batch upload or by streaming it in real time and you can use any of the currently available google cloud services to load data into bigquery you can take a manual batch ingestion approach or stream using pub sub etl data and with bigquery data transfer service you can automatically transfer data from external google data sources and partner sas applications to bigquery on a scheduled and fully managed basis and the best part is batch and export is free bigquery's highspeed streaming api provides an incredible foundation for realtime analytics making business data immediately available for analysis and you can also leverage pub sub and data flow to stream data into bigquery bigquery transparently and automatically provides highly durable replicated storage in multiple locations for high availability as well as being able to achieve easy resource bigquery keeps a seven day history of changes in case something were to go wrong bigquery supports standard sql querying which reduces the need for code rewrites you can simply use it as you would for querying any other sql compliant database and with dataproc and dataflow bigquery provides integration with the apache big data ecosystem allowing existing hadoop spark and beam workloads to read or write data directly from bigquery using the storage api bigquery also makes it very easy to access this data by using the cloud console using the bq command line tool or making calls to the bigquery rest api using a variety of client libraries such as java.net or python there are also a variety of thirdparty tools that you can use to interact with bigquery when visualizing the data or loading the data bigquery provides strong security and governance controls with finegrained controls through integration with identity and access management bigquery gives you the option of geographic data control without the headaches of setting up and managing clusters and other computing resources in different zones and regions bigquery also provides fine grain identity and access management and rest assured that your data is always encrypted at rest and in transit now the way that bigquery calculates billing charges is by queries and by storage storing data in bigquery is comparable in price with storing data in cloud storage which makes it an easy decision for storing data in bigquery there is no upper limit to the amount of data that can be stored in bigquery so if tables are not edited for 90 days the price of storage for that table drops by 50 percent query costs are also available as ondemand and flat rate pricing and when it comes to ondemand pricing you are only charged for bytes read not bytes returned in the end bigquery scales seamlessly to store and analyze petabytes to exabytes of data with ease now there are so many more features to list but if you are interested feel free to dive into the other features with the supplied link in the lesson text now moving on to the next service is pub sub and pub sub is a fully managed realtime messaging service that allows you to send and receive messages between independent applications it acts as messaging oriented middleware or event ingestion and delivery for streaming analytics pipelines and so a publisher application creates and send messages to a topic subscriber applications create a subscription to a topic and receives messages from it and so i wanted to take a moment to show you exactly how it works so first the publisher creates messages and sends them to the messaging service on a specified topic a topic is a named entity that represents a feed of messages a publisher application creates a topic in the pub sub service and sends messages to that topic a message contains a payload and optional attributes that describe the content the service as a whole ensures that published messages are retained on behalf of subscriptions and so a published message is retained for a subscription in a message queue shown here as message storage until it is acknowledged by any subscriber consuming messages from that subscription pub sub then forwards messages from a topic to all of its subscriptions individually a subscriber then receives messages either by pub sub pushing them to the subscriber's chosen endpoint or by the subscriber pulling them from the service the subscriber then sends an acknowledgement to the pub sub service for each received message the service then removes acknowledged messages from the subscriptions message queue and some of the use cases for pub sub is balancing large task queues distributing event notifications and realtime data streaming from various sources and so the next service that i wanted to get into is composer now composer is a managed workflow orchestration service that is built on apache airflow this is a workflow automation tool for developers that's based on the open source apache airflow project similar to an onpremises deployment cloud composer deploys multiple components to run airflow in the cloud airflow is a platform created by the community to programmatically author schedule and monitor workflows the airflow scheduler as you see here executes the tasks on an array of workers while following the specified dependencies and storing the data in a database and having a ui component for easy management now breaking down these workflows for just a sec in data analytics a workflow represents a series of tasks for ingesting transforming analyzing or utilizing data in airflow workflows are created using dags which are a collection of tasks that you want to schedule and run and organizes these tasks to ensure that each task is executed at the right time in the right order or with the right issue handling now in order to run the specialized workflows provision environments are needed and so composer deploys these selfcontained environments on google kubernetes engine that work with other google cloud services using connectors built into airflow the beauty of composer is that you can create one or more of these environments in a single google cloud project using any supported region without having to do all the heavy lifting of creating a fullblown apache airflow environment now when it comes to data flow dataflow is a serverless fully managed processing service for executing apache beam pipelines for batch and realtime data streaming the apache beam sdk is an open source programming model that enables you to develop both batch and streaming pipelines using one of the apache beam sdks you build a program that defines the pipeline then one of apache beam's supported distributed processing backends such as data flow executes that pipeline the data flow service then takes care of all the lowlevel details like coordinating individual workers sharding data sets auto scaling and exactly once processing now in its simplest form google cloud data flow reads the data from a source transforms it and then writes the data back to a sink now getting a bit more granular with how this pipeline works data flow reads the data presented from a data source once the data has been read it is put together into a collection of data sets called a p collection and this allows the data to be read distributed and processed across multiple machines now at each step in which the data is transformed a new p collection is created and once the final collection has been created it is written to async and this is the full pipeline of how data goes from source to sync this pipeline within data flow is called a job and finally here is a highlevel overview of what a data flow job would look like when you involve other services within google cloud and put together in an endtoend solution from retrieving the data to visualizing it and finally when it comes to pricing data flow jobs are billed in per second increments so you're only charged for when you are processing your data now moving on to data proc this is a fast and easy way to run spark hadoop hive or pig on google cloud in an onpremises environment it takes 5 to 30 minutes to create spark and hadoop clusters data proc clusters take 90 seconds or less on average to be built in google cloud dataproc has builtin integration with other google cloud platform services and use spark and hadoop clusters without any admin assistance so when you're done with the cluster you can simply turn it off so you don't spend money on an idle cluster as well there's no need to worry about data loss because data proc is integrated with cloud storage bigquery and cloud bigtable the great thing about dataproc is you don't need to learn new tools or apis to use it spark hadoop pig and hive are all supported and frequently updated and when it comes to pricing you are billed at one cent per vcpu in your cluster per hour on top of the other resources you use you also have the flexibility of using preemptable instances for even lower compute cost now although cloud data proc and cloud data flow can both be used to implement etl data warehousing solutions they each have their strengths and weaknesses and so i wanted to take a quick moment to point them out now with dataproc you can easily spin up clusters through the console the sdk or the api and turn it off when you don't need it with dataflow it is serverless and fully managed so there are never any servers to worry about and when it comes to having any dependencies to tools in the hadoop or spark ecosystem data proc would be the way to go but if you're looking to make your jobs more portable across different execution engines apache beam allows you to do this and is only available on data flow moving on to the next service is cloud data lab now cloud data lab is an interactive developer tool created to explore analyze transform and visualize data and build machine learning models from your data data lab uses open sourced jupyter notebooks a wellknown format used in the world of data science it runs on compute engine and connects to multiple cloud services easily so you can focus on your data science tasks it also integrates with all of the google services that help you simplify data processing like bigquery and cloud storage cloud data lab is packaged as a container and run in a vm instance cloud data lab uses notebooks instead of text files containing code notebooks bring together code documentation written as markdown and the results of code execution whether it's text image or html or javascript like a code editor or ide notebooks help you write code and they allow you to execute code in an interactive and iterative manner rendering the results alongside the code cloud data lab notebooks can be stored in google cloud source repository this git repository is cloned onto persistent disk when attached to the vm now when it comes to prepping your data before consumption whether it be data cleansing cleaning prepping or alteration this is where data prep hits it out of the park dataprep is a serverless intelligent data service for visually exploring cleaning and preparing structured and unstructured data for analysis reporting and machine learning it automatically detects schemas data types possible joins and anomalies such as missing values outliers and duplicates so you don't have to the architecture that i'm about to show you is how data prep shines the raw data that's available from various different sources is ingested into cloud data prep to clean and prepare the data data prep then sends the data off to cloud data flow to refine that data and then sent off to cloud storage or bigquery for storage before being analyzed by one of the many available bi tools now these big data services are used by many data analysts in the field and it's great to know what services that can be used to help process the data needed for their specific job as well for the exam you only need to know these services at a high level and not to know them in depth but if you seem interested in diving into any of these services to know more about them i highly encourage you to dive in after the course and really take a look at them and that's pretty much all i have to cover in this lesson on the services that are available for the big data ecosystem in google cloud so you can now mark this lesson as complete and let's move on to the next one welcome back this lesson is going to be based on the foundation of machine learning i'm going to go over what machine learning is what it can do for us the machine learning ecosystem on google cloud and hopefully answer any questions along the way this lesson will be a high level overview of the services available on google cloud yet these services that are available are a need to know as they come up in the exam and hopefully will give you some really cool ideas on the possibilities of building something truly fantastic on google cloud so what is machine learning well machine learning is functionality that helps enable software to perform tasks without any explicit programming or rules traditionally considered a subcategory of artificial intelligence machine learning involves statistical techniques such as deep learning also known as neural networks that are inspired by theories about how the human brain processes information it is trained to recognize patterns in collected data using algorithmic models and this collected data includes video images speech or text and because machine learning is very expensive to run onpremises is an efficient place for machine learning due to the use of massive computation at scale and as explained before machine learning is always better with big data so now i wanted to touch on what can machine learning do for us well it can categorize images such as photos faces or satellite imagery it can look for keywords in text documents or emails it can flag potentially fraudulent transactions when it comes to credit cards or debit cards it can enable software to respond accurately to voice commands it can also translate languages in text or audio and these are just some of the common functions that machine learning can do for us so getting into google's machine learning platform itself machine learning has been a cornerstone of google's internal systems for years primarily because their need to automate datadriven systems on a massive scale and doing this has provided unique insight into the right techniques infrastructure and frameworks that help their customers get optimal value out of machine learning the originally developed open source framework for use inside of google called tensorflow is now the standard in the data science community in addition to heavily contributing to the academic and open source communities google's machine learning researchers helped bring that functionality into google products such as g suite search and photos in addition to google's internal operations when it comes to data center automation now here is an overview of all the machine learning services that we will be covering and that you will need to know only at a high level for the exam and we'll start off with the site api services starting with the vision api the vision api offers powerful pretrained machine learning models that allow you to assign labels to images and quickly classify them into millions of predefined categories vision api can read printed and handwritten text it can detect objects and faces and build metadata into an image catalog of your choice now when it comes to video intelligence it has pretrained machine learning models that automatically recognizes more than 20 000 objects places and actions in stored and streaming video you can gain insights from video in near real time using the video intelligence streaming video apis and trigger events based on objects detected you can easily search a video catalog the same way you search text documents and extract metadata that can be used to index organize and search video content now moving on to the language apis we start off with the natural language api and this uses machine learning to reveal the structure and meaning of text you can extract information about people places and events and better understand social media sentiment and customer conversations natural language enables you to analyze text and also integrate it with your document storage on cloud storage now with the translation api it enables you to dynamically translate between languages using google's pretrained or custom machine learning models translation api instantly translates text into more than 100 languages for your website and apps with optional customization features following another grouping of machine learning is the conversation apis first up we have dialog flow dialog flow is a natural language understanding platform that makes it easy to design and integrate a conversational user interface into your application or device it could be a mobile app a web application a bot or an interactive voice response system using dialogflow you can provide new and engaging ways for users to interact with your product dialogflow can analyze multiple types of input from your customers including text or audio inputs like from a phone or voice recording and it can also respond to your customers in a couple of ways either through text or with synthetic speech now with the speechtotext api this api accurately converts speech into text it can transcribe content with accurate captions and deliver better user experience in products through voice commands going the other way from text to speech this api enables developers to synthesize natural sounding speech with over a hundred different voices available in multiple languages and variants text to speech allows you to create lifelike interactions with their users across many applications and devices and to finish off our machine learning segment i wanted to touch on auto ml automl is a suite of machine learning products that enables developers with very limited machine learning expertise to train high quality models specific to their business needs in other words using automl allows making deep learning easier to use and relies on google's stateoftheart transfer learning and neural architecture search technology so you can now generate high quality training data and be able to deploy new models based on your data in minutes automl is available for vision video intelligence translation natural language tables inference and recommendation apis now i know this has been a lot to cover for this machine learning lesson and the ecosystem around it but is a necessity for the exam and will also help you build really cool products when it comes to your role as an engineer again all the services that i have discussed in this lesson should be known at a high level only although my recommendation would be to dive deeper into these services by checking out the links in the lesson text below and having some fun with these products getting to know these services will really help up your game when it comes to getting to know these services a little bit more in depth and will really help you gain more momentum when it comes to building any applications or applying them to any currently running applications i personally found it extremely valuable and really cemented my knowledge when it came to machine learning i also had a ton of fun doing it and so that's all i have for this lesson on machine learning so you can now mark this lesson as complete and let's move on to the next one welcome back and in this lesson we'll be diving into a suite of tools used on the google cloud platform that allow you to operate monitor and troubleshoot your environment known as operation suite and previously known as stackdriver this lesson will be mostly conceptual and gear more towards what the suite of tools do as it plays a big part not only in the exam but for the needs of gaining insight from all the resources that exist in your environment now there are a few tools to cover here so with that being said let's dive in now the operation suite is a suite of tools for logging monitoring and application diagnostics operation suite ingests this data and generates insights using dashboards charts and alerts this suite of tools are available for both gcp and aws you can connect to aws using an aws role and a gcp service account you can also monitor vms with specific agents that again both run on gcp for compute engine and aws ec2 operation suite also allows the added functionality of monitoring any applications that's running on those vms operation suite is also available for any onpremises infrastructure or hybrid cloud environments operation suite has a native integration within gcp out of the box so there's no real configurations that you need to do and integrates with almost all the resources on google cloud such as the previously mentioned compute engine gke app engine and bigquery and you can find and fix issues faster due to the many different tools an operation suite can reduce downtime with realtime alerting you can also find support from a growing partner ecosystem of technology integration tools to expand your operations security and compliance capabilities now the operation suite comprises of six available products that covers the gamut of all the available tools you will need that allows you to monitor troubleshoot and improve application performance on your google cloud environment and i will be going over these products in a bit of detail starting with monitoring now cloud monitoring collects measurements or metrics to help you understand how your applications and system services are performing giving you the information about the source of the measurements time stamped values and information of those values that can be broken down through time series data cloud monitoring can then take the data provided and use predefined dashboards that require no setup or configuration effort cloud monitoring also gives you the flexibility to create custom dashboards that display the content you select you can use the widgets available or you can install a dashboard configuration that is stored in github now in order for you to start using cloud monitoring you need to configure a workspace now workspaces organize monitoring information in cloud monitoring this is a single pane of glass where you can view everything that you're monitoring in your environment it is also best practice to use a multiproject workspace so you can monitor multiple projects from a single pane of glass now as i mentioned earlier cloud monitoring has an agent and this gathers system and application metrics from your vm and sends them to cloud monitoring you can monitor your vms without the agent but you will only get specific metrics such as cpu disk traffic network traffic and uptime using the agent is optional but is recommended by google and with the agent it allows you to monitor many thirdparty applications and just as a note cloud logging has an agent as well and works well together with cloud monitoring to create visualize and alert on metrics based on log data but more on that a little bit later cloud monitoring is also available for gke and this will allow you to monitor your clusters as it manages the monitoring and logging together and this will monitor clusters infrastructure its workloads and services as well as your nodes pods and containers so when it comes to alerting this is defined by policies and conditions so an a learning policy defines the conditions under which a service is considered unhealthy when these conditions are met the policy is triggered and it opens a new incident and sends off a notification a policy belongs to an individual workspace and each workspace can contain up to 500 policies now conditions determine when an alerting policy is triggered so all conditions watch for three separate things the first one is a metric the second one is a behavior in some way and the third one is for a period of time describing a condition includes a metric to be measured and a test for determining when the metric reaches a state that you want to know about so when an alert is triggered you could be notified using notification channels such as email sms as well as third party tools such as pagerduty and slack now moving on to cloud logging cloud logging is a central repository for log data from multiple sources and as described earlier logging can come not just from google but with aws as well as onpremises environments cloud logging handles realtime log management and analysis and has tight integration with cloud monitoring it collects platform system and application logs and you also have the option of exporting logs to other sources such as longterm storage like cloud storage or for analysis like bigquery you can also export to thirdparty tools as well now diving into the concepts of cloud logging these are associated primarily with gcp projects so logs viewer only shows logs from one specific project now when it comes to log entries log entry records a status or an event a project receives log entries when services being used produce log entries and to get down to the basics logs are a named collection of log entries within a google cloud resource and just as a note each log entry includes the name of its log logs only exist if they have log entries and the retention period is the length of time for which your logs are kept so digging into the types of logs that cloud logging handles there are three different types of logs there are audit logs transparency logs and agent logs now with audit logs these are logs that define who did what where and when they also show admin activity and data access as well as system events continuing on to access transparency logs these are logs for actions taken by google so when google staff is accessing your data due to a support ticket the actions that are taken by the google staff are logged within cloud logging now when it comes to agent logs these are the logs that come from agents that are installed on vms the logging agent sends system and thirdparty logs on the vm instance to cloud logging moving on to error reporting this looks at realtime error monitoring and alerting it counts analyzes and aggregates the errors that happen in your gcp environment and then alerts you when a new application error occurs details of the error can be sent through the api and notifications are still in beta error reporting is integrated into cloud functions and google app engine standard which is enabled automatically error reporting is in beta for compute engine kubernetes engine and app engine flexible as well as aws ec2 air reporting can be installed in a variety of languages such as go java.net node.js python php and ruby now moving into debugger this tool debugs a running application without slowing it down it captures and inspects the call stack and local variables in your application this tool debugs a running application without slowing it down it captures and inspects the call stack and local variables in your application this is also known as taking a snapshot once the snapshot has been taken a log point can be injected to allow you to start debugging debugger can be used with or without access to your application source code and if your repo is not local it can be hooked into a remote git repo such as github git lab or bitbucket debugger is integrated with google app engine automatically and can be installed on google compute engine gke and google app engine debugger is integrated with google app engine automatically and can be installed on gke debugger is integrated with google app engine automatically and can be installed on google compute engine google kubernetes engine google app engine and cloud run and just as a note installation on these products is all dependent on the library and again debugger can be installed like trace on nongcp environments and is available to be installed using a variety of different languages next up is trace and trace helps you understand how long it takes your application to handle incoming requests from users and applications trace collects latency data from app engine https load balancers and applications using the trace api this is also integrated with google app engine standard and is applied automatically so you would use trace for something like a website that is taking forever to load to troubleshoot that specific issue trace can be installed on google compute engine google kubernetes engine and google app engine as well it can also be installed on nongcp environments and it can be installed using a variety of different languages as shown here and coming up on the last tool of the bunch is profiler now profiler gathers cpu usage and memory allocation information from your applications continuously and this helps you discover patterns of resource consumption to help you better troubleshoot profiler is low profile and therefore won't take up a lot of memory or cpu on your system as well in order to use profiler an agent needs to be installed profiler can be installed on compute engine kubernetes engine and app engine as well and of course it can be installed on nongcp environments and profiler can be installed using the following languages just go java node.js and python and so just as a note for the exam only a high level overview of these tools are needed and so this concludes this lesson on a high level overview of operation suite so you can now mark this lesson as complete and let's move on to the next one
