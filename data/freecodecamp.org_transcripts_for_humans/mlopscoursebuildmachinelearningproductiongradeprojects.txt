With timestamps:

00:00 - mlops short for machine learning
00:02 - operations refers to the practice of
00:04 - applying devops principles to machine
00:06 - learning this mlops course will guide
00:08 - you through an endtoend mlops project
00:12 - covering everything from data ingestion
00:14 - to deployment using state-of-the-art
00:16 - tools like Zen ml ml flow and various ml
00:20 - libraries IU sing developed this course
00:23 - he has created many popular machine
00:24 - learning courses on our Channel this
00:26 - course will teach you fundamentals of
00:28 - emops as well as we be having one end to
00:31 - end project which will involve from data
00:32 - ingestion to deployment using several
00:35 - state-of-the-art tools like MLF flow
00:37 - zenel and Etc mlops is completely new
00:40 - field and there are very less resources
00:42 - around it and this is a gold mine and a
00:45 - game changer in the ml Community if you
00:48 - do this with the dedication and the
00:49 - patience you will be able to succeed in
00:52 - learning about mlops and then you will
00:54 - be also getting the international Pace
00:56 - or whatsoever job offer which you want
00:58 - in your hands but but wait who am I I'm
01:00 - lead data scientist atate I've L several
01:03 - products in crea's economy along with it
01:05 - I have worked as a emops engineer as one
01:07 - of the fastest growing emop streamwork
01:09 - which is zml and have experienced in
01:10 - working as a data scientist at artifact
01:12 - and building large scale NLP products
01:14 - before even GPT was launched and
01:17 - hopefully by all of this experience I'm
01:19 - the right guy to teach you about mlrs
01:21 - all the required links and everything
01:23 - resources is listed down in the
01:25 - description on boox below you can go
01:26 - ahead and check that
01:28 - out
01:33 - hey everyone welcome to the another
01:34 - lecture on mlops we'll be starting off
01:37 - with giving you a slight introduction to
01:38 - mlops also I'll make you aware with the
01:41 - terminologies of mlops which is very
01:43 - important for you to understand the
01:45 - later content of this course as well as
01:47 - we'll also make sure that you understand
01:49 - the basic things like pipeline steps
01:51 - which is a library which we'll be using
01:53 - out there but before that we'll make
01:55 - sure to that you understand why there's
01:57 - a need of mlops what like what are the
01:59 - stages and envelops and etc etc so let's
02:02 - get started with this uh lecture so the
02:04 - first thing first so the first thing
02:06 - first out is um about t the the growth
02:11 - of data is increasing there's
02:13 - exponential growth of data and the
02:16 - importance of artificial intelligence is
02:18 - also has also increased over time now
02:22 - the data has increased but now we need
02:25 - to make sure that we utilize that data
02:28 - in a right way and in a positive way
02:30 - right so that's where it come that that
02:33 - that's where we we have artificial
02:35 - intelligence right so now you might be
02:37 - thinking that okay fair enough we can
02:39 - just build a prediction model on top of
02:41 - it but you should understand that
02:43 - machine learning is not just about
02:45 - building model why I say this why I say
02:48 - this because your ml code or whatever
02:51 - Machinery model is just 20% of your
02:54 - whole machine learning project or a
02:57 - whole um business problem right there's
02:59 - a lot of things which comes into that
03:01 - place and your machine learning code is
03:03 - just 20% out of the whole set of things
03:06 - so I hope you will I'll prove you why
03:09 - why there's a 20% of ml code throughout
03:11 - this video and it's ml in the industry
03:15 - is more than the training models it is
03:17 - validated by chip Huen who is one of the
03:20 - ex experts in mlops and it is also
03:23 - validated by Elon Musk who just said
03:25 - yeah it's like machine learning
03:26 - engineering is just 10% machine learning
03:30 - and 90% engineering and that's something
03:33 - really interesting to worry about and
03:34 - you might be thinking that every other
03:35 - courses online teaches only about
03:37 - machine learning engineering which is a
03:39 - building machine learning models but
03:41 - nobody teaches about the engineering
03:43 - part of it right and you might be
03:45 - thinking it might be some data
03:46 - structures on algorithms or it might be
03:48 - some design patterns or Etc of course
03:51 - yes it has some factors but there are
03:53 - lot more than these DSA and stuff which
03:55 - will explore throughout the course
03:57 - through our
03:58 - project
04:00 - so we uh in a typical ml team uh in our
04:03 - corporate we have the following uh
04:05 - people who are actually responsible for
04:07 - doing x amount of tasks so your data
04:09 - scientists discover the raw data develop
04:12 - features and train models right and data
04:14 - engineer who productionize the data
04:16 - pipeline we'll talk about the term uh
04:19 - productionize in a Bild but data
04:21 - pipeline is like where the data is
04:23 - coming from and then making it on a
04:25 - large scale right and then we have a ml
04:28 - engineer who sits on front to deploy the
04:30 - model right so that it can be used by
04:32 - users by you we'll talk about what does
04:34 - deployment mean in just some seconds and
04:37 - then we integrate the service into into
04:39 - your website or application and then you
04:42 - have to monitor it we'll talk about each
04:44 - and every steps in grade detail and then
04:46 - you have a lawyer who who can just ask
04:49 - you we we should ask question to them
04:51 - can I use this data for my model yes or
04:53 - no and and I'm pretty much sure that you
04:56 - might not be aware with any of the any
04:58 - of the red line over here I'll make sure
05:01 - that you understand each of the things
05:02 - like training models productionizing
05:04 - deployment integrating monitoring and
05:06 - all the stuff throughout this int
05:08 - introductory lecture the reason why I'm
05:10 - doing this introductory lecture to make
05:12 - sure that you understand each and every
05:14 - bit in the project which will use which
05:16 - will make use of like terminologies
05:17 - which will make use over
05:19 - there so what data science actually sees
05:22 - you might be thinking about okay fair
05:24 - enough you have pd. read CSV you read
05:26 - the data you fit and then some some
05:28 - happens and then you simply uh and then
05:30 - you also have the classifier you also
05:32 - fit that and then you do predict and do
05:34 - score right that's what you see right
05:38 - but do you really think that uh by
05:41 - writing three these three lines of code
05:44 - people will get your job of course not
05:46 - right and the main focus 90% focus
05:50 - should be on engineering and what
05:51 - engineering sees is much more uh very
05:55 - scary than what data science
05:58 - is so ml in production you might if
06:01 - you're a bit aware even about how does
06:02 - it goes Etc the first step is you
06:05 - collect the data you train the model and
06:07 - then deploy the model introduction what
06:09 - does deployment mean so let's talk about
06:11 - a little bit about deployment so that
06:13 - you understand it a bit however I
06:15 - recommend if you want to understand
06:16 - deployment much more in great detail
06:18 - we'll have more sections on afterwards
06:21 - to actually understand what does
06:22 - deployment mean over here so deployment
06:25 - means that you once you once you have
06:27 - the trained model for example let's take
06:29 - an example you're you're working on a
06:31 - email spam detection project right and
06:33 - the model is currently is in a local
06:36 - server right is in a machine how can you
06:38 - use that and integrate into that Gmail
06:41 - right so that that we can use that model
06:44 - to make predictions for the users who
06:46 - who are who are whom for for whom we are
06:48 - making this model for right and that's
06:50 - where deployment comes in right
06:52 - deployment is about that you have to
06:55 - make your local model available to the
06:58 - lot of people to the users for which
07:01 - you're building the model
07:02 - for right that's what deployment means
07:05 - you to deploy the model online and and
07:07 - we'll see we'll talk about on deployment
07:10 - in very great detail in some
07:12 - time but you might be thinking that this
07:15 - is the process but what ex actually it
07:17 - looks like so basically what happens
07:20 - that first of all you collect the data
07:22 - you train the model and then you deploy
07:24 - the model now once your model is
07:27 - deployed you again go back and collect
07:29 - in the data and then training the model
07:31 - and this Loops goes in environment but
07:34 - how does you how how can we say how can
07:37 - we say that that okay the what's the
07:40 - loop is about you might be having
07:42 - several questions what is the loop what
07:44 - is the production environment and lot of
07:46 - things out there so let's talk about in
07:48 - great detail about what does the loop
07:49 - means and what is the production
07:51 - environment
07:52 - is so I'll take a very very simple
07:55 - example of uh this image right so for
07:59 - assume that you have you had the
08:00 - collected the data and then you had to
08:02 - trained the model and then you deployed
08:04 - the code right so assume that assume
08:07 - that it is deployed in production your
08:09 - spam production St spam production
08:11 - system and is being used right now what
08:14 - happens that you changed your model you
08:17 - changed your machine learning algorithm
08:19 - from logistic ration to n base right you
08:21 - changed your ml algorithm right so you
08:24 - have to you have to retrade go back to
08:26 - the model and then and then whatever is
08:29 - change you have to again deploy that
08:31 - changed model which is update the model
08:34 - right which is one case is different
08:36 - model needed or ml algorithm changes
08:39 - right another another point is for
08:41 - example you're building some span
08:42 - reduction project you might have trained
08:44 - it on a on a on a data set which which
08:47 - is which which might be unupdated right
08:50 - or a new data arrives right for example
08:53 - your hackers or spammers change their
08:56 - strategy of sending spam emails right so
08:59 - so the the the data changes and your
09:01 - model should be able to identify the new
09:03 - patterns which the spammers are
09:05 - following right so what happens that if
09:08 - any data changes happen it retrains the
09:10 - model so first of all data again new
09:12 - data comes in retrains the model and
09:13 - then deploy it that's why we call it in
09:15 - a loop in a production environment it is
09:17 - a NeverEnding process you know it is a
09:20 - never- ending process deployment goes in
09:23 - production right and it trains the model
09:25 - it it sorry uh it first of all collects
09:28 - the data trains the model and deployment
09:29 - goes in production what if if your model
09:32 - changes model changes if the model
09:33 - changes you again you have to go back
09:35 - and then push it again or if new data
09:37 - arrives you have to go back to data
09:39 - collection and then push it
09:41 - again I hope this really makes sense if
09:43 - it does not don't worry we'll have uh
09:46 - lot of examples to study more I'll take
09:48 - I'll give one possible scenario of this
09:50 - production when ml algorithm changes or
09:52 - of the about the loop in a production
09:55 - environment so one possible scenario of
09:58 - going back of of going back is about
10:02 - model performance starts to Decay right
10:05 - so once you train the model you deploy
10:06 - it after a certain period of a time your
10:08 - model starts to Decay so let's take an
10:11 - example of a fraud detection example so
10:13 - assume you have trained your model for
10:15 - fraud detection and let's say you have
10:17 - deployed it as well and you see your
10:20 - model is giving incorrect prediction
10:22 - right so it most probably happened that
10:24 - your frauders change the strategy or
10:26 - patterns to fraud right the the patterns
10:30 - which your machine learning algorithm
10:31 - has learned has changed so you need to
10:33 - recollect the data and retrain the model
10:36 - which means go back and then do it again
10:39 - and it may happen after some some time
10:40 - again hackers change the strategy right
10:43 - so this is what the model performance
10:44 - starts toate then you have to go back
10:46 - into the loop and then uh retrain and
10:49 - redeploy them more another scenario can
10:51 - be we might need to reformulate the
10:53 - problem as it difficult to get gathered
10:55 - data more data as we need so reformulate
10:58 - other problem s violation of assumption
11:01 - which we made during training so
11:02 - basically what happens basically what
11:04 - happens when you train the model we have
11:05 - certain assumption for our input data
11:07 - that the the input data will be in
11:09 - certain range or input data will will
11:11 - will will be there are a lot of
11:13 - assumptions which comes into this place
11:15 - so if the if the Assumption changes if
11:18 - the assumptions changes which we had in
11:21 - a training data we might need to uh
11:24 - reformulate the Assumption or maybe go
11:27 - go back to this and have those
11:29 - accommodate the Assumption which are
11:30 - being violated or simply the business
11:33 - objective changes and basically to
11:35 - restart again so a lot of things which
11:38 - comes into that place which can be of a
11:40 - loop and it is never ending process you
11:42 - have to have continuously um seeing your
11:45 - model monitoring your model and stuff so
11:48 - ml production which is data affects the
11:50 - output system and it's very hard to make
11:53 - it reliable when deploying model
11:55 - retraining and then collecting and then
11:57 - the loop is very very hard to make it
12:01 - reliable and that's where mlops comes in
12:04 - place mlops is a set of practices it is
12:07 - not some library or it is a tool it is a
12:09 - set of practices that aims to deploy and
12:13 - maintain machine learning models in
12:15 - production reliably and efficiently so
12:19 - to make sure that if there's anything
12:21 - changes in the data it retrains the
12:22 - model I'm just taking one one or two
12:24 - examples it rechange the model if the
12:26 - assumptions are violated it again goes
12:28 - back so we have to make a reliable
12:30 - production I which is which is happening
12:33 - at a large
12:34 - scale so uh the term mlops is like the
12:38 - extension of the devops methodology to
12:41 - include machine learning and data
12:43 - science assets at the first class
12:44 - citizens within the Devol ecology I'm
12:47 - pretty I'm pretty much sure that that
12:49 - you might be a bit uncomfortable with
12:52 - this so let's try to think about in
12:54 - another way okay I'll tell you a very
12:56 - simple example of mlops so assume that
12:58 - you you are you are given a a game to
13:01 - build a beautiful city right now you
13:04 - just build the be now if you build a
13:06 - beautiful building in that city is that
13:09 - helpful just write yes or no is that
13:12 - helpful yes or no building a beautiful
13:14 - city in that build uh in that city is
13:17 - not at all good thing because it needs
13:18 - the electrical connectivity it needs the
13:20 - maintenance it needs the security
13:23 - systems it needs the connection to the
13:24 - roads and the railways and lot of things
13:27 - which comes into the place right right
13:29 - so a single building is like a is like a
13:31 - model you have to connect it you have to
13:34 - securitize it or you have to monitor it
13:36 - there a lot of things which comes into
13:37 - that place for making the fully
13:39 - functional City and companies wants what
13:43 - companies wants the full Standalone
13:46 - cities not a full building right and
13:49 - that's where the people are not getting
13:51 - jobs it's only because they are only
13:53 - focusing on building that building not
13:55 - the whole city and mlops is a way to
13:58 - building that that full city which is
14:03 - required so we really talked about
14:05 - deployment but you might be thinking
14:07 - it's very very easy to deploy the model
14:09 - in production but let me tell you that
14:12 - the trouble begins after deployment so
14:15 - you might be worrying about why so I'll
14:17 - tell you what are the some of the things
14:19 - which needs to be taken care of the
14:21 - first one is accounting for latency so
14:24 - what is latency latency is about that
14:27 - that you might be shocked by the
14:28 - statistics that 53% of the visitors are
14:33 - abandoned if a mobile site takes more
14:35 - than 3 seconds to load so for example if
14:37 - your sites take more than 3 seconds to
14:39 - load 53% of the people will abandon the
14:43 - site and you know why and I'll tell you
14:46 - why is because for example you have you
14:49 - might have deployed a 120 billion
14:52 - parameters model or a very large model
14:54 - do you think that model will give
14:56 - prediction in less than 3 seconds that's
14:59 - that's that's really hard right and
15:01 - latency is one of the biggest problem
15:05 - right and if visitors are not VI your
15:07 - viewing a model or a website they most
15:09 - likely not is engaged with a brand and
15:12 - they most likely not buy your product or
15:14 - utilize that
15:15 - product right so this is one of the
15:18 - those another one is that fairness right
15:22 - so for example you deployed your model
15:24 - right so basically what happened that
15:26 - Microsoft created a Twitter bot to learn
15:28 - from users and you know it became the
15:31 - racist it started supporting the various
15:34 - bad ideologies after deployment they
15:36 - thought that this will be so good and it
15:38 - was taken down by Microsoft in just some
15:40 - hours it was against feminism it was
15:44 - sorry it was against um I I'll not take
15:47 - any names it was against x amount of
15:49 - thing right which was which was which
15:51 - was so racist out there that's why they
15:54 - had to take it down in matter of some
15:56 - some some some hours after deployment
15:58 - they thought that this will be good but
16:00 - but eventually it learned very bad
16:02 - things and then need to retrain it but
16:05 - however it never gone into production
16:06 - from from
16:08 - there another one is lack of
16:10 - explainability and audibility it's very
16:13 - hard to explain the prediction right um
16:16 - and and and and we also also have to
16:19 - make sure that it is authentic enough to
16:21 - trust this right and that's why there
16:23 - are several rules and guidelines which
16:25 - are coming by and by again to make sure
16:28 - from the U EU to make sure that we fit
16:31 - some of the principles of
16:34 - AI and it is painfully slow I'll tell
16:38 - you 36 the there was um basic you know
16:43 - um survey conducted from a set of data
16:46 - scientists about how much time they
16:48 - spend in deploying and machine learning
16:50 - models 36% of them said said that they
16:54 - spent a quarter of their time which is
16:57 - 36 % of their time deploying machine
17:00 - learning
17:01 - models right and and and this is so you
17:04 - know um um which is and and and they're
17:08 - like more 36% of S quarter to half of
17:12 - the time of deploying and 20% half to 3
17:14 - quars and 7% in more than three quarters
17:17 - it is very very slow and you might be
17:20 - noticing why it is slow and there's a
17:22 - lot of things which will will face when
17:24 - when when we are building project we
17:25 - might be very surprised so to see that
17:27 - I'll be I'll be so correct to you I'll
17:30 - be so truthfully to you that when I was
17:32 - building projects for this course I
17:35 - actually spent my whole week in
17:37 - deploying models because it's painfully
17:41 - slow and and you and you might be
17:44 - shocked that I built the whole ml model
17:45 - Tor procing in just two days that's it
17:48 - and spending four hours but a freaking
17:50 - whole week in deploying
17:53 - it so this is uh this is one of the so
17:56 - yeah so um so I'll talk about the model
17:59 - Centric and a data Centric in a b but
18:01 - what exactly model Centric and data
18:03 - Centric means so model Centric means
18:06 - that you are you want to improve the
18:08 - model while do not changing any data so
18:12 - you fixed the data so you have X amount
18:13 - of data you fix it and then you
18:15 - iteratively improve your code or model
18:18 - by tweening it some parameters and
18:19 - expect your model to perform well or in
18:21 - data Centric what you do you hold the
18:24 - model fixed and then you keep on
18:26 - iteratively improving the data
18:29 - and a lot of work is in this model
18:32 - Centric only few of the work is in data
18:35 - Centric so I suggest for you all to
18:37 - focus on data Centric more probably to
18:39 - focus on data have still the model but
18:42 - yeah it's to totally upon your choice
18:44 - this is also Sav by Andre d which is
18:45 - again the very U Pioneer which is one of
18:48 - one of one of my instructor too is's
18:50 - very nice in what he teaches and I think
18:52 - that his ideologies I was in one of the
18:54 - webinar of him and actually he told
18:56 - about this B Centric and data Centric
18:58 - and which which we really experience in
19:00 - our day-to-day Life as a data
19:04 - scientist so let's get started with
19:06 - talking about the whole process of mlops
19:09 - and what does it include so the first
19:12 - thing to worry about it what is the
19:14 - business problem we want to solve so
19:18 - what is the business problem we really
19:20 - want to solve that's the first question
19:22 - to start off so any melops project any
19:24 - machine learning product which we have
19:25 - to start first to worry about not about
19:28 - what Pro what what exactly ml thing
19:30 - would you have to solve what business
19:32 - problem which you want to solve so in
19:34 - that business problem you to solve you
19:36 - you have to take care of several things
19:38 - out there the first is the cost of wrong
19:42 - predictions so I'll tell you a very
19:44 - basic we we'll have a basic example so
19:46 - let's take an example what we want to do
19:48 - we want to predict we we want to Pro we
19:50 - want to forecast you know we want to
19:52 - forecast our retails for example what
19:54 - happens in a company that sometimes
19:56 - because of the wrong estim
19:58 - sometimes what happens there might be
20:00 - Overstock of a particular product which
20:02 - leads to wastage of resources and they
20:04 - underst stock which leads to again a
20:06 - revenue loss so in both cases underst
20:08 - stock or overstock of your um of your
20:10 - res of of your products is being the
20:13 - problem in a retail company so you not
20:15 - to you you want to really solve that so
20:18 - the first one is what is the cost of
20:20 - wrong predictions if we actually if if
20:23 - our model gives if we actually don't
20:25 - estimate the right thing the cost of
20:28 - wrong prediction is quite High Overstock
20:30 - means having more stock of your products
20:33 - leads to wasted resources and possible
20:35 - rofs for unsold products and understock
20:39 - which means which is Miss sales
20:40 - opportunity and unsatisfied customers
20:42 - because they're not able to get the
20:44 - things on time and both of them has the
20:47 - quite high pro quite High U costs
20:51 - because one at one point you if you
20:53 - Overstock wastage resources and one
20:55 - point it is like misses opportuni so we
20:58 - we have to worry about and if we solve
20:59 - this problem we'll fix Overstock and
21:01 - understock
21:03 - problems so let's break down the process
21:05 - of sale forecasting processes so
21:08 - basically in this sale for forecasting
21:10 - process you decompose the process of
21:13 - sale for forecasting into component task
21:16 - see see see just notice that we haven't
21:18 - reached to our ml thing right away we
21:21 - first of all talking about the problem
21:23 - which you want to solve right and then
21:25 - dividing the problem then right so now
21:27 - is a sales forecasting problem you're
21:30 - dividing the problem into several things
21:32 - first is data Gathering second
21:34 - historical sales analysis market rint
21:36 - analysis and actual forecasting so what
21:39 - does data Gathering mean getting the
21:40 - required amount of data which we need
21:42 - analysis of the past right what is the
21:44 - trend of the market and the last one is
21:47 - actual
21:48 - forecasting so what you do you you
21:51 - actually data Gathering is something
21:52 - which is pretty easy right not pretty
21:54 - easy I would say it is it is it is
21:56 - something which so let's worry about
21:58 - what things can be solved in ml in this
22:00 - case and also will will it return High
22:02 - Roi right High return of our time right
22:06 - which we devote in this so of course we
22:08 - can have data Gathering has all the
22:10 - equal importance but what eventually we
22:13 - could solve for using ml is this actual
22:15 - forecasting to actually estimate what
22:18 - will be the number of pro stocks which
22:21 - which we should have in a certain time
22:24 - period and we can actually use this IML
22:28 - in this actual forecasting task where it
22:31 - could analyze the past sales data and
22:33 - market trends right to predict future
22:35 - sales with higher accuracy than whatever
22:38 - traditional methods they're using so now
22:40 - you might have noticed that now after
22:42 - dividing it it into the components we
22:45 - understand actual forecasting is the one
22:47 - which we should solve for by by
22:50 - utilizing the past data and the market
22:55 - trends and the ROI could be estimated by
22:58 - potential increase and a decrease in
23:00 - sales wastage due to improve forecast
23:03 - for example if your for forecasting is
23:05 - good you will be actually noticing that
23:08 - you're that there's a decrease in a
23:09 - wasted resources which means eventually
23:11 - it's helping right and then what you do
23:14 - you actually worry about what what will
23:15 - the cost of developing and P maintain
23:18 - the solution so if your wasted resources
23:20 - are decreasing a lot so you might focus
23:23 - on building this AML solution of
23:26 - it and then you have to prioritize it in
23:29 - this case you or to prioritize the
23:31 - implementation which is the actual
23:33 - forecasting in this
23:35 - case Okay cool so we'll talk about
23:37 - structuring in our project as of now I'm
23:39 - just leaving the slide so there's a
23:41 - machine learning canvas which usually
23:43 - you have to really worry about while
23:45 - building a project or solving a business
23:48 - problem using machine
23:49 - learning the first one is value
23:53 - proposition right so first of all we
23:55 - have to define the value proposition
23:57 - position what is this importance Define
24:00 - the problem the importance of the
24:02 - problem and what who will be our end
24:04 - user right so basically you want to for
24:08 - you you need to understand for who we
24:11 - need a product service this product will
24:14 - be benefiting jofre M value proposition
24:17 - positioning statement which means for
24:19 - Target customer who need our product/
24:22 - service is a product category category
24:25 - that benefits basically we have to make
24:27 - sure we have to make sure the problem
24:30 - importance is so high right to proceed
24:34 - with the solving for the problem another
24:37 - one is we have data sources from where
24:40 - we should identify potential data
24:41 - sources and it can be including some
24:43 - internal databases or apis or open data
24:46 - sets and Etc which comes into that place
24:49 - we should also consider hidden costs
24:52 - such as data storage purchasing external
24:54 - data and Etc which comes into that place
24:57 - so basically this is the second second
24:59 - step third step is what will be the
25:02 - production task whether it is a
25:03 - supervised or unsupervised problem or
25:05 - aom detection classification regression
25:08 - or ranking problem you just worry about
25:10 - what will be my input what will be my
25:13 - output what will be the degree of model
25:15 - complexity right this will give you more
25:18 - clear clear Clarity before building the
25:21 - before going in actual cing part then
25:23 - next step is feature engineering so
25:25 - basically you have to interact with a do
25:28 - experts for example you might be working
25:30 - you you might building something a
25:31 - really good in in healthcare space but
25:33 - you're not a MBS doctor right you
25:35 - actually need to have the MB doctor to
25:38 - actually get more information to
25:40 - understand the terminologies and
25:41 - actually make more information extract
25:44 - more information from the available data
25:46 - sources which we get that's where
25:48 - feature engineering comes into comes
25:49 - into this place offline evaluation which
25:53 - means you set up some Matrix to evaluate
25:55 - your system before pushing it to to the
25:58 - deployment pre-employment means using
26:00 - the model by your own and understanding
26:02 - the prediction errors and what will the
26:04 - cost of wrong
26:07 - predictions and then using predictions
26:09 - to make decisions how will the end user
26:12 - interact with a interact with the
26:14 - predictions will it will it involve any
26:17 - hidden cost which can be in human
26:19 - intervention and lot of things which
26:21 - comes to that
26:24 - place and at last we collect the new
26:27 - data we keep on col collecting new data
26:29 - for model retraining and preventing
26:31 - model decaying performance we also
26:33 - consider cost for data collection and
26:36 - the role of human intervention in data
26:37 - labeling because it's very very
26:39 - important for having good labelers in
26:40 - the data to actually for for actually
26:43 - helping models to extract patterns from
26:46 - it and then you have deciding frequency
26:49 - of model retraining and Associated
26:51 - hidden cost for for how many time we'll
26:53 - retrain a model at what interval and as
26:55 - well as if there's any changes in a tech
26:57 - tag which you have to worry
27:00 - about and then what you do you set up
27:03 - Matrix to track system you to monitor
27:05 - your model once your model is deployed
27:07 - you know you have to deploy it so that
27:09 - so that for example in spam deduction
27:10 - you have to you have to keep on check
27:12 - you have to have some Matrix to keep on
27:14 - check whether your model is giving the
27:16 - wrong predictions or not that's the
27:18 - monitoring part and also identify
27:22 - situations where AIML may not be the
27:24 - best solution it can be some subtasks of
27:26 - it where you actually worry about
27:28 - something outside of IML right because
27:30 - it's very very important to understand
27:32 - if we can solve it without IML because
27:34 - it's very um hard as well as the the
27:38 - cost of implementing am solution is
27:39 - pretty much
27:42 - big and also so so that's that's pretty
27:46 - much about um what exactly we need to
27:48 - worry about uh in whole mlops procedure
27:51 - so there are three things which comes
27:53 - into into the workflow of building uh
27:56 - workflow building the machine learning
27:57 - based software development that there
27:59 - are three main artifacts in building
28:01 - ml-based software the first one is data
28:05 - second one is machine learning model and
28:07 - third one is code and three main phases
28:11 - which which is engineering data
28:12 - engineering ml model engineering and
28:15 - code engineering so let's talk about
28:17 - each step by step so data engineering is
28:19 - like you have to collect the data
28:21 - acquired the data and prepared the data
28:24 - accordingly right what is uh and then
28:27 - also make sure that there are certain
28:29 - things which is which we have to make
28:31 - sure in this so the pipeline of data
28:34 - engineering here how goes pipeline means
28:37 - stepbystep procedure to go ahead right
28:40 - first of all you ingest the data and the
28:42 - data which was ingested you explore and
28:44 - validate the data that is coming from a
28:46 - true space as well as explore it to
28:48 - understand the data you format and clean
28:51 - the data you label the data if it is a
28:53 - supervised learning problem and you
28:55 - divide the data into training validation
28:57 - test set so so that that can be used for
28:59 - training the models I'm assuming that
29:01 - you already know about training and
29:03 - validation and all those things which is
29:04 - already there I'm not over here to
29:07 - explain you ml things I'm over here to
29:08 - explain you things which really
29:12 - matters so the next step is model
29:14 - engineering so the core of ml workflow
29:17 - is writing and executing ml algorithms
29:20 - the pipeline here it like this you train
29:22 - the model you evaluate the model
29:24 - validate the model which predeployment
29:26 - so that makes that your model is working
29:28 - pretty well you test the model right
29:30 - using the unknown unseen data set which
29:33 - is the Unseen um samples which your
29:35 - model has never seen and then you
29:37 - package the model so that business can
29:39 - be used accordingly it can be pkl file
29:41 - or any such you know uh models and at
29:45 - last but not the least you have you
29:47 - deploy the model you serve the model in
29:49 - a production environment you monitor so
29:51 - that it's it's it's going well and then
29:53 - you record and then also log it so that
29:56 - for every INF for for every prediction
29:58 - it is making so that we can go back if
30:00 - there's anything goes
30:02 - wrong so I hope that you understand
30:04 - these three things pipelines we'll go
30:06 - into that greater detail when we
30:07 - actually do this project we'll implement
30:09 - it live over there so that you could see
30:11 - pretty much
30:13 - easily and we'll use zml to develop
30:16 - execute and manage our machine Learning
30:20 - Systems so I'll talk about uh pipelines
30:22 - and steps in pretty much small detail
30:24 - and we'll eventually go to projects
30:26 - because I think that is pretty long varo
30:28 - so we'll just try to talk in Greater
30:30 - detail later on as of now let's talk
30:32 - about what are
30:34 - pipelines so zml follows a pipeline
30:37 - based approach so don't worry about the
30:39 - zml thing as of now we'll come to that
30:40 - what exactly it is data on but currently
30:43 - let's talk about pipelines and steps so
30:44 - zml follows a pipeline based approach to
30:47 - organize machine learning workflows it
30:49 - can be methods to promote efficiency rep
30:52 - repetitively sorry repetitively and
30:54 - collaboration in your projects so SA for
30:57 - example uh so what is pipeline it's like
30:59 - a movie production process a pipeline is
31:02 - a high level workflow that organize a
31:05 - series of tasks to create a final
31:07 - product in the context of a movie
31:09 - production process it can be of script
31:11 - writing casting filming editing and
31:13 - distribution casting depends on stre
31:16 - script writing filming depends on
31:18 - casting editing depends on filming and
31:21 - distribution depends on editing
31:23 - everything is interrelated everything is
31:25 - step by step not you you you can't do
31:27 - scripting and then editing right so
31:29 - similarly in zml your pipeline
31:31 - represents a complete ml workflow and
31:34 - each step over there it can be involved
31:36 - a step can be dat a preparation feature
31:39 - another step feature enging so feature
31:40 - enging can only be done if the if the
31:42 - previous step is completed and then
31:44 - train the model to evaluate and deploy
31:48 - it so here's a very basic basic example
31:51 - over here so basically you you you
31:53 - actually first of all uh step one which
31:56 - is the prepare the you have some ml you
31:58 - load the data which is using the step
32:00 - decorator you have another function
32:02 - which using the step decorator you train
32:04 - the model you evaluate the model which
32:06 - is editing and then you deploy the model
32:08 - which is distribution now you combine
32:10 - all these steps into a pipeline right
32:12 - you have the data you give the input
32:13 - data which is using a pipeline decorator
32:15 - you give it to the feature engineering
32:16 - and then you give the features to the
32:18 - model and then you give the models to
32:20 - the for the evaluation to evaluate and
32:21 - then you deploy the model and then you
32:22 - run this whole pipeline so it's run step
32:25 - by step to reach and give you the
32:27 - trained model so there are a lot of
32:29 - things lot of benefits of it which we'll
32:31 - discuss throughout the course right so
32:34 - in the next set of lectures I'll make
32:35 - sure to introduce you to bit of bit of
32:38 - collab notebooks to make sure that
32:39 - you're aware about basic functionality
32:41 - of Zen so that we could actually use
32:43 - this in our project and then we'll be go
32:45 - in building our first project of mlops
32:48 - so let's get started uh with this uh our
32:50 - first mlops
32:54 - proy hey everyone welcome back to the
32:56 - new video on mlops course so basically
32:59 - today what I want to do is I want to
33:01 - make you familiar with core and the
33:02 - fundamentals of zenel because it's very
33:05 - very important to understand what are
33:06 - the co Core Concepts of ZL to actually
33:08 - start on building several projects using
33:10 - zml zml is an open source library for
33:13 - building uh full stack amops
33:15 - applications and the reason why I want
33:17 - to use this ziml because I personally
33:18 - worked over there for for about six to
33:20 - 7even months over there and I've worked
33:22 - there with their core team and actually
33:24 - this is super simple to use that's why I
33:26 - want to use ziml you can use several
33:27 - other orchestrators which are available
33:29 - in the market however the most easiest
33:31 - one with the best ones uh is ziml so
33:34 - that's why I want to use ziml however uh
33:36 - you might face sever other problems U
33:38 - but there's always a community which
33:40 - where you can interact and uh resolve
33:43 - your doubts so let's get started with ML
33:45 - pipelines with zml and today uh this is
33:47 - The cocol laab Notebook from Zen byes
33:50 - which zml Zen ml team has already built
33:52 - for us and over there this the the the
33:55 - Zen pites what they want to do from the
33:57 - these kind of collabs they want to teach
33:58 - you the Core Concepts so I want to
34:00 - utilize these and then uh record a
34:02 - videos on top of it to actually make you
34:04 - understand some of the Core Concepts of
34:05 - zml so let's get started um and also
34:08 - we'll be doing projects don't worry this
34:10 - is just for core understanding because
34:12 - as we say the core is the power so let's
34:15 - get started uh with this notebook so uh
34:18 - first of all what what we will do we
34:20 - will install the zml server which is the
34:23 - zml server which is very important for
34:24 - us to install U you can this is a
34:27 - command line uh command which you have
34:29 - to paste on a terminal if you're using
34:30 - vs code however you can just get started
34:33 - with collab will when when we'll go to
34:34 - projects you can actually see the way
34:36 - I'm doing over there we we'll we'll also
34:38 - make use of pyit learn because I want to
34:40 - show you the demo that's why I want to
34:42 - train a very simple model over here and
34:44 - pipe Haring which is important for
34:46 - collab and then we do it for simple
34:48 - things which needed however I have
34:50 - already did it so you don't need to uh
34:52 - you you only need to do I don't need to
34:53 - do because it takes bit of time to
34:55 - download so so uh and the next uh you
34:58 - need NG do account if you want to see
35:00 - the visualizations and all the stuff
35:01 - which is pretty easy you only need NG do
35:04 - account for colab you don't need NG do
35:06 - account for something if you're doing uh
35:08 - for on vs code or simple python code
35:11 - you'll be easily having access to that
35:13 - however for collab you need NG do you
35:16 - can actually have it I have a coupon
35:17 - code over here it will I you can
35:19 - actually I will hide it I'm so sorry for
35:22 - that I'll hide it or you can just have
35:24 - your own ngok token as well cool so uh
35:28 - and over here you this is just for
35:29 - collab setup this is not for uh any
35:32 - things you which you have to learn so uh
35:35 - what what we will do is uh will you you
35:38 - might be familiar as it says that you
35:39 - might be familiar with pyit learn
35:41 - pytorch or tens oflow so as I say the ml
35:44 - pipeline is simply an extension which
35:46 - includes the step by step as I as I told
35:49 - the example of a what I told the example
35:51 - of a movie production process so and and
35:55 - in that movie production you have
35:56 - scripting casting editing and all those
35:58 - things those are steps which are
36:00 - interconnected with each other right so
36:03 - the the the reason why we use pipelines
36:05 - is because of the following reasons the
36:07 - first one is we can easily rerun all our
36:10 - work not just the model right so
36:12 - basically you run each and everything
36:15 - from the starting so which which helps
36:17 - to uh eliminate any bucks also make our
36:20 - models easier to reproduce second one is
36:24 - that for every pipeline you run you have
36:26 - for for for every time you run you have
36:28 - the you have you can easily track the
36:30 - previous previous run in this in these
36:32 - kind of pipelines for example you run
36:34 - the code one time and then you run the
36:35 - second times and usually you don't have
36:37 - the access to the previous run so
36:39 - basically using pipelines you can have
36:40 - the access to every runs and it can be
36:42 - tracked as well and then you can comp
36:44 - then you can use for several purpose for
36:46 - for example comparing two different
36:48 - versions of the models right and also if
36:51 - the entire pipeline is coded up we can
36:52 - automate many operational tasks like
36:54 - retraining redeployment and all all
36:56 - those things which is needed via cicd
36:58 - workflows we uh don't worry about if you
37:01 - didn't understood this line when we'll
37:03 - actually do one simple uh project one
37:06 - simple project you will see if any
37:08 - things changes on the data how we how
37:11 - actually pipeline help us to um this
37:14 - redeploy or retrain our
37:16 - model Okay cool so let's get started
37:19 - with the zml so first of all you need to
37:21 - have the you need to have the Zen
37:22 - Library installed first of all what
37:24 - we'll do we'll remove any existing files
37:26 - of there and then initialize a ziml
37:28 - repository which is very important uh
37:30 - which is very important which is the
37:31 - first step whenever you use ziml Library
37:33 - so it initializes your zml in uh in your
37:36 - current directory So currently you can
37:38 - just use ZL in to initialize it and this
37:41 - happens this uh exclamation mark shows
37:43 - that we are dealing with terminal
37:45 - commands so now so now I'll show you
37:47 - what we exactly do for basically I'm
37:49 - going to train I I want to train uh
37:52 - class psychic learn SBC which is a
37:54 - support Vector machine support vector
37:56 - classifier on want a train a support
37:58 - Vector machine classifier to classify
38:00 - images of handwritten digits so
38:02 - basically we we will do the handwritten
38:04 - digit recognition using uh support
38:06 - Vector machine so over there we have
38:08 - there there is there are several images
38:10 - and each image is either 0 1 2 3 4 all
38:13 - the way around to the 10 so basically
38:14 - you want to classify the numbers the
38:16 - images handwritten the digits based on 0
38:20 - to 10 uh numbers so let's get started
38:23 - with it uh if you are not sure about
38:24 - what is handwritten digit recognition
38:26 - for should take a look at online what is
38:28 - the problem is about so basically what
38:30 - what we will do we we load the model we
38:32 - will train the model we'll trest the
38:33 - model on the and then we'll see the
38:35 - accuracy however this is not the right
38:38 - thing to do okay this is just for
38:40 - practicing this this can be thousand X
38:42 - more complex than this is very basic
38:44 - version of anything right this is just a
38:47 - dummy dummy thing just to Showcase you
38:49 - so basically you load the digits so load
38:51 - digits will load the data set from SK
38:53 - learn data set so basically we'll use
38:55 - the SK learn data sets to load the
38:56 - digits and then what we will do uh and
38:59 - then we'll reshape it so that we'll just
39:01 - do the little bit of processing and once
39:03 - the processing is done we'll divide our
39:05 - data sets into XT train X test y train
39:07 - and Y test the reason why we divide it
39:09 - because so that we can train our model
39:11 - on X train and Y train and then we can
39:13 - test our model on testing however again
39:15 - I'll say that machine learning algorithm
39:17 - coding is thousand sex much more better
39:19 - than this we teach in our code machine
39:21 - learning course uh which is if if you
39:24 - actually see the code machine Lear
39:25 - course you'll see what is this so yeah
39:27 - this is just for dumb example don't take
39:28 - inspiration from machine learning uh
39:30 - algorithm over here right and then you
39:33 - simply train toest split and then you
39:34 - have the support vector classifier and
39:36 - then you fit the model and then you
39:38 - evaluate the model okay pretty simple
39:41 - now what now this is I and then you run
39:43 - it you get the test accuracy now how can
39:46 - we run this into how can we divide this
39:48 - into experim into pipelines so what we
39:52 - will do as we have the Z ziml init which
39:54 - is a ziml repository we'll create our
39:57 - first pipeline the first pipeline will
39:59 - have the following components in that it
40:01 - will import it will train it will
40:04 - evaluate the model okay so are three
40:07 - distinct steps in this example loading
40:09 - of the data training of the model and
40:12 - evaluating the model right you can
40:15 - simply use we can will will we will
40:17 - simply make different different
40:18 - functions for different different
40:19 - components over here so first of all
40:21 - we'll make you'll make use of adate Step
40:23 - operator which you can simply import
40:25 - from zml and then what you'll have to do
40:27 - you the the Importer importer Does this
40:30 - does not takes anything this returns
40:32 - right this is called type set the the
40:35 - thing which we have to return over here
40:37 - so the so basically the utter will load
40:40 - the digit will reshape it will uh train
40:43 - to split and then return xtrain xest y
40:45 - TR by test the reason why we have to
40:47 - actually write over there what is
40:48 - actually returning is for several
40:51 - reasons which happens behind ziml behind
40:53 - the scenes because this ASV trainer
40:57 - should know what type of input it is
41:00 - coming to me because as we trainer will
41:02 - get X train and Y train right so they
41:04 - should know what type of soda to verify
41:06 - the type of the data types which
41:08 - importer is sending and SVC is getting
41:10 - we need to actually state that this is
41:13 - something which is it is going to return
41:16 - this is also helpful in readability also
41:18 - it happens it also helps in the back end
41:20 - of the
41:21 - system which is annotated and then np.
41:24 - and array this is the Xtreme will return
41:26 - the extem the type of that is nump array
41:29 - exess type of that is an so this is a
41:31 - formal annotated which will annotate our
41:33 - outputs right and then you have another
41:36 - step which is SBC trainer right we which
41:38 - we again decorated with at theate step
41:40 - and then it returns as we see over it
41:42 - takes X train which we say that it is a
41:44 - nump array y train it is also a numpy
41:46 - array and it Returns the classifier it
41:49 - Returns the classifier right so
41:51 - basically it Returns the classifier
41:52 - which means that we train the model and
41:54 - it Returns the classifier so basically
41:56 - you you can import classifier
41:59 - mixing from skarn base which says that
42:02 - this is the whatever it whatever it is
42:04 - the type it will just make make the type
42:06 - so it will be classifier mixing it will
42:08 - be mix of classifiers you can easily
42:10 - search online about classifier mixing if
42:11 - it un confused about what type of data
42:13 - type is this this is just the SVC
42:15 - classifier data type you can also write
42:17 - SVC over there by identifying the type
42:20 - of this model the next step is it will
42:22 - take the input as a X test and Y test
42:24 - and the model and the model type would
42:26 - be classified mix in and it will return
42:28 - a float and also it is decorated by step
42:30 - operator and then we just uh this test
42:34 - accurate score it and then however there
42:36 - there there can be several set of
42:38 - classification measures just as of now
42:40 - as we say that we are taking a baseline
42:42 - now once we have the steps now we to
42:44 - connect each and every steps so we'll
42:45 - use the zml pipeline and the pipeline
42:47 - will have following like this first of
42:49 - all you exrain xess and you import you
42:51 - use the Importer which you built over
42:53 - there and then you use the SVC trainer
42:55 - and then and then you use the SVC
42:57 - trainer where you give the the this step
43:00 - which X train and Y train you give it
43:01 - and then you evaluate up so once you run
43:04 - it once you run it you will be getting
43:06 - and then you simply use digits SVC and
43:08 - then digits pipeline when you run it it
43:10 - will initiate a new Run for the pipeline
43:13 - which is currently it I have I've
43:14 - already run so this is the version
43:16 - number two so you can you can in the
43:18 - dashboard you can visit the version
43:20 - number one okay and then revisit the
43:22 - accuracy over here and as well as
43:23 - previous
43:24 - one so um you can simply go this it says
43:28 - step importer has started step importer
43:30 - has finished in 2.73 to2 seconds SVC
43:33 - trainer has started as we see trainer
43:35 - has finished evaluator has started it is
43:38 - finished and then every run digits has
43:40 - finished you can visualize your pipeline
43:42 - runs in simply zml dashboard right and
43:46 - then you can run this code and then go
43:48 - to this U URL so basically when you run
43:50 - it you will be prompted to URL something
43:52 - like this and then you can easily go
43:54 - over there and then you will Simply
43:56 - Having your pipeline so basically your
43:58 - password should be default okay so
44:00 - basically whenever you um go over there
44:02 - so let me show it to you so basically um
44:05 - let me run it quickly if you want it
44:08 - uh I'll run
44:14 - it so you can see now it will train the
44:16 - third version okay so sorry it will
44:19 - train the second version because we have
44:20 - reinitiated our the stuffs right so you
44:23 - can simply go over there and then um it
44:25 - will will run
44:32 - it so now you see it's starting the zml
44:35 - server you can easily go over here and
44:37 - then it will it will open the zml
44:39 - dashboard now once it opens you can so
44:42 - basically you'll be prompted to
44:43 - something like this you have to write
44:44 - default over here yeah you you have to
44:47 - write default over here and then click
44:48 - on login so once you click on login
44:51 - you'll be automatically having you'll
44:53 - aut automatically go to pipelines and
44:55 - then visualize your pip pipelines over
44:56 - here however this is not the pipeline
44:58 - which is over here you you'll be viewing
45:00 - something like this so this helps you
45:01 - can easily go to the previous one see
45:03 - your model score come to this your model
45:05 - score and etc etc so I hope you really
45:08 - understood what exactly steps and
45:10 - pipelines means in ziml this is just the
45:11 - basic things in the next lecture in the
45:14 - lecture 1.2 what what we'll do we'll I
45:16 - I'll show you some of the Magics of what
45:18 - zml does and then you'll be surprised to
45:21 - know about that as well right so let's
45:23 - get started with a new lecture
45:32 - and that say pretty much simple that
45:34 - we'll first of all go ahead and read
45:36 - understand what our data looks like so
45:38 - that it gives you much more clarity
45:39 - about the problem statement which we
45:41 - really want to do currently I'm not
45:43 - setting this too much on business
45:45 - objectives and all most probably we
45:47 - focus on the technical aspects of
45:48 - building this envelops project so over
45:51 - here you have the data and and and the
45:53 - data says the oldest customers data set
45:56 - and that data set has the uh most
45:59 - probably over here if you see that
46:01 - customer ID customer unique customer
46:03 - City customer State and then we have
46:05 - geolocation data set and then we have
46:07 - items data set and lot of data sets out
46:10 - there so what we did we we made our
46:12 - custom data set over here so if you go
46:14 - and see our custom data set we have the
46:16 - lot of features where we combine
46:18 - everything to one and then we have the
46:20 - review score which is our uh most
46:22 - probably our uh review score which is
46:25 - the factions scode so I'll quickly show
46:27 - you um uh very basic how does it looks
46:30 - like in Excel sheet because it's more
46:33 - important to let you aware about the
46:35 - Excel sheet because it's much more
46:36 - common because currently it's bit
46:37 - complicated over here if you think uh in
46:39 - a basic um Visual Studio code so let's
46:42 - get started with actually show
46:43 - showcasing it actually takes a bit of
46:45 - time because the file is a little bit
46:47 - large but no worries we'll get started
46:49 - with it but as soon as it as it is
46:51 - opening so what I'll do what I what I'll
46:53 - do I'll create several fold folders
46:56 - which is very important for us which you
46:57 - can take as an template for for starting
47:00 - off right so let's get started actually
47:03 - showcasing but before that it it
47:04 - actually opened up so you see that order
47:06 - ID customer ID order status order
47:08 - purchase order approved that and lot of
47:10 - features comes in comes into this place
47:12 - and then finally you have review score
47:14 - which is from 1 to five which is from 1
47:17 - to five however currently we'll be not
47:19 - using this review comment and we'll
47:20 - delete lot of you know features though
47:23 - not not because of the feature not
47:24 - because of the it it does not holds
47:27 - importance but or because but because
47:30 - because we I don't want to make it
47:32 - complex project initially you can of
47:34 - course tweak it accordingly you make it
47:36 - on whole data you know do do in the
47:40 - setting of machine learning setting and
47:41 - lot of things which which you can do
47:42 - currently I want to make it pretty much
47:45 - simple that's very nice so let's get
47:48 - started this is our uh Target variable
47:50 - and all of this is our input features
47:53 - which we have to use to actually predict
47:56 - our uh customer satisfaction score so
47:59 - but before that what I'll do I'll
48:00 - quickly make several folders which is
48:02 - very important for us to get started so
48:05 - um and also but but before that let's
48:07 - install several libraries which are
48:09 - listed in readme.md so uh one thing
48:12 - which I just have to uh make a note that
48:14 - you have to actually perform all the
48:16 - actions all the installations every of
48:18 - your operations in a virtual environment
48:21 - currently I'm in customer satisfaction
48:23 - virtual environment I actually use
48:25 - something known as as you know what I
48:26 - use I I use spy EnV you can use cond or
48:29 - you can use um ven or literally any
48:32 - virtual environment which you're going
48:33 - to use for actually creating virtual
48:35 - environment if you're not aware of what
48:36 - is virtual environment it is actually
48:38 - containerize all your applications into
48:40 - one's environment so that your
48:42 - dependency conflict does not happen I
48:43 - know if you if you don't if you don't
48:45 - know you might be not able to understand
48:47 - this so we have linked a very nice
48:49 - resources in the GitHub repository just
48:51 - before this section uh which you'll see
48:52 - in the GitHub repository to actually
48:54 - understand what is it version
48:55 - environment means but it's very very
48:57 - important to in a virtual environment to
48:58 - actually have it everything on the good
49:00 - page cool so what I'll do I'll quickly
49:03 - install so basically uh I'll I'll first
49:06 - of all pip install zml server so that
49:09 - you can this is for the one who actually
49:11 - wants to run the whole project Let's
49:12 - ignore this and let's try to install
49:14 - this first of all zml so I'll I'll quick
49:17 - quickly go and install
49:19 - zml so you might be seeing that it is
49:21 - giving some errors so what what we have
49:23 - to do we have to actually add something
49:25 - like this
49:28 - I hope so it works if it does not we'll
49:30 - have to go to ZL ser and see it okay
49:32 - cool it's actually working so it will
49:34 - take some time to actually download a
49:36 - zml and and I'm personally downloading
49:38 - over here so that you can also see that
49:40 - how exact exactly these thing works and
49:42 - also what I'll do I'll quickly um import
49:44 - the requirements which I have it over
49:46 - here uh so that you understand it much
49:49 - more in a great detail so um what I'll
49:53 - do I will
49:58 - have it over here okay cool
50:04 - so this is the requirements.txt however
50:07 - you can uh this is just for show CAD
50:09 - boost SL jbm we'll not be learning about
50:11 - these algorithms if you want to learn
50:12 - these algorithms enroll in my core
50:14 - machine learning course but um but we'll
50:17 - be not learning this this is just for
50:18 - installation of the libraries which is
50:20 - very important for us to install it
50:21 - prior however you can totally choose to
50:23 - ignore this we just coding step by step
50:24 - so that you understand understand it
50:25 - much more in Greater detail so now it
50:27 - shows that it is actually installing and
50:29 - I pretty much think it is installed um
50:33 - yeah so it also say that that we have to
50:35 - upgrade so let's just copy from here to
50:37 - here and is because the reason why I
50:40 - like to upgrade it because it really
50:41 - shows pretty interesting and pretty
50:43 - beautiful the way it downloads not not
50:46 - is this white white one is actually very
50:48 - colorful that's why I like that I'll
50:49 - just clear it very quickly and then what
50:51 - I'll do I'll go ahead and zml up so what
50:54 - is zml up up does it UPS or the or
50:58 - awakes the zml server so that you can
51:00 - view your pipelines you can view a lot
51:02 - of things out there if you simply put
51:04 - zml up but before that what you see that
51:06 - it is not running the reason why it is
51:08 - not running that we forgot a very
51:10 - interesting thing out here before that
51:12 - we have to actually write ziml init
51:14 - which initiates the repository over here
51:18 - which means that it it initiates the
51:19 - ziml repository over here so right you
51:22 - could see that that a do Zen folder will
51:25 - be created over here as soon as possible
51:28 - as the runs is completed so let's just
51:29 - wait for a few seconds and let the let
51:32 - the Zen ml gets Zen ZL in it gets
51:34 - completed the reason why we want to
51:36 - create the repository because we want to
51:37 - containerize or have all our code inside
51:41 - that repository so that it can be used
51:43 - for several other purposes which you'll
51:45 - realize it a later on okay cool so uh
51:49 - but let let that running it's it is a
51:51 - first run that's why it is a bit taking
51:53 - time so let's just go ahead and create
51:55 - folder which is very important for us
51:56 - now you see that zml is z do dozen is
51:59 - created and it says that your zml fine
52:01 - version does not match the server
52:03 - version the version mismatch might lead
52:05 - to errors or unexpected Behavior kindly
52:07 - refer to blah blah blah so let's do one
52:09 - thing let's simply ziml down grade so
52:12 - that should definitely replicate all our
52:14 - errors out there which is this warning
52:17 - because it's it's you know it I'll tell
52:19 - you from my personal opinion that it's
52:22 - very very important I'll tell you from
52:24 - my person it's very important fix up the
52:25 - warnings the reason why uh I want you to
52:28 - fix up the warnings because it's because
52:31 - sometimes it might happen that you'll
52:32 - completely get unexpected error and
52:34 - you'll never realize that you were here
52:36 - right so that's why I really want you to
52:38 - first of all um uh make sure that you uh
52:42 - satisfy all the errors so basically it
52:43 - says that your Zen cine version doesn't
52:45 - as server version so you can I either
52:47 - downgrade or do a lot of things to
52:49 - actually get it done but uh living that
52:52 - it that it is let's just go ahead and
52:54 - create our folder
52:55 - so the folders which I'm going to create
52:57 - is first of all the Zen folders created
52:59 - the data folders created one thing which
53:01 - we will do in a future in another
53:03 - projects of this whole course is that we
53:06 - we we will not use CSV data set we will
53:09 - use the poster SQL and then retrieve the
53:11 - data set from SQL because in real world
53:13 - setting we not eventually use CSV we
53:16 - actually use uh SQL databases from cloud
53:19 - or somewhere like U fster SQL on local
53:22 - right and then we use from there and
53:23 - then we retrieve and play with that data
53:25 - right so that that we'll do it later on
53:27 - but most probably just let's just keep
53:29 - it very simple and let's just go ahead
53:31 - with data
53:32 - folder cool um another folder which I
53:35 - really want to have is something as
53:36 - model folder model will contain all my
53:40 - um for all my files which is of like
53:43 - models and stuff all the all the things
53:45 - which is required for training the model
53:47 - or you can also name it as a source so
53:49 - let's name it as a source because that's
53:50 - more important right so let's name it as
53:53 - a SRC and which which is which all which
53:56 - all contains your data sets now what
53:57 - I'll do I'll quickly create something
53:59 - known as
54:01 - pipelines so pipelines will contain all
54:03 - our pipelines which we have which will
54:05 - build saved model will contain if you if
54:07 - you want to save the model um eventually
54:09 - you don't need to but you know just just
54:11 - for reference we created steps so steps
54:14 - will contain all our components or the
54:15 - tasks which needs to be done over here
54:17 - and then at last you can actually have
54:19 - in. Pi so let's quickly create something
54:22 - know as
54:24 - inite pi and then after that we'll
54:27 - create there is always there's always a
54:30 - requirements txt what I'll do I'll
54:32 - create something known as run pipeline
54:35 - so we can run our pipeline over there
54:38 - run pipeline. P cool so what I'll
54:41 - quickly do I will first of all um code
54:44 - all the um the data things first of all
54:47 - what we'll do as as I said this is not a
54:49 - formal machine learning engineering
54:50 - course right is the mlops course so I'll
54:52 - make sure that to keep keep things very
54:53 - simple very knife right right but if you
54:55 - want to learn like more of the advanced
54:57 - things in machine learning there's
54:58 - always core machine learning course
54:59 - available out there to actually help you
55:00 - out the first thing which you want to do
55:03 - the first thing which we want to do is
55:04 - ingest the data so we'll start off with
55:07 - first of all uh steps so steps in that
55:09 - we'll create the file Lim ingest data.
55:12 - pi and ingest data. Pi will consist of
55:16 - the steps will will consist of the steps
55:18 - where we will inest the data in it so
55:21 - what I'll do I'll quickly import logging
55:23 - out here so that we could eventually log
55:25 - when when when things completed because
55:27 - it's very very important to log as well
55:29 - and then I'll import from import pandas
55:31 - as PD and then will from zml import step
55:36 - as I as as you have seen in basics of
55:38 - zml that we have to actually use this
55:40 - step over there then I'll create a class
55:42 - of ingest era I'll create the class of
55:45 - ingest era oh my God you know the the
55:48 - way my keyboard is not working pretty
55:51 - well and over there what I'll do I'm
55:53 - actually using copilot still but um but
55:57 - uh you have to also you know give the
55:58 - good documentation which will write
56:00 - pretty nicely so neat so let's just
56:03 - quickly do it so I'll just in it and you
56:08 - can actually do it like this um which is
56:10 - in it and then when you run it you can
56:12 - actually write the get data and then
56:14 - ingesting data from the data path pd.
56:17 - read CSP and then self. data path right
56:20 - or what you can do you can simply give
56:21 - the pcsv this direct file to this
56:24 - totally matters on what you want to
56:27 - give okay so then we'll create a step so
56:30 - where we can use that class we can use
56:32 - and then that step will consist of the
56:34 - data path which will take St Str as an
56:35 - input which will the string of course
56:37 - and it will return the data frame right
56:39 - it will return a very nice data frame
56:42 - and then what I'll do I'll first of all
56:43 - make a try statement I really don't
56:45 - don't want to take the help of um I
56:49 - really want to take take the help of
56:51 - this guy uh co-pilot but if he's helping
56:53 - me I can't do literally anything so what
56:55 - I'll do I'll show the way to write the
56:57 - documentation first of all you know what
56:59 - you write you write the description
57:00 - about that function so use
57:03 - ingesting the data from the data path
57:07 - then we'll write the arcs arcs means the
57:09 - argument it is going to take the data
57:11 - part which is the part to the data and
57:13 - then what it returns it Returns the
57:15 - Panda's data frame right this is how you
57:18 - actually do the inest data and this is
57:19 - what the very interesting workflow is
57:21 - then we'll write in a try try and accept
57:24 - a workflow which will have something
57:25 - like this where we first of all in in
57:28 - instantiate our class which is ingest
57:30 - data which is ingest data where will
57:33 - first of all ingest data which is the
57:34 - data path and then we will simply say DF
57:38 - ingest data. get data and then return DF
57:41 - this can be easily done in a three one
57:43 - line of code as well as shown by the
57:44 - co-pilot but I want to make it pretty
57:46 - simple as well for the beginners if
57:48 - you're watching it so for accept
57:49 - exception as e and then it says error
57:51 - while ingesting the data and this is
57:53 - what the error is so this this is this
57:55 - actually helps us to uh the best
57:57 - practices of coding and same goes to
57:59 - over here we have to actually maintain
58:01 - it nicely right um so let's just go
58:04 - ahead and quickly do it uh let me just
58:07 - remove it yeah so I'll just make use of
58:09 - interesting data from the T data path
58:11 - and then we actually instantiate the
58:13 - method so this is this is this is used
58:15 - that this is used as instantiating the
58:18 - method uh
58:21 - arcs that's it and
58:25 - you can also write instantiation but if
58:27 - not bits is not required eventually over
58:29 - here what what what we do inting from
58:32 - data path and then you simply write
58:33 - nothing and then you just that's it so
58:35 - this is a basic workflow which you have
58:37 - to go ahead and create this step the
58:39 - first thing which you have created is of
58:41 - course ingest data now we can actually
58:43 - use this ingest data which we will
58:44 - eventually use it later on as well so
58:47 - now the next step once we have ingested
58:49 - the data we need to we need the next one
58:52 - which is we need to clean the data so
58:54 - now now what no so now what what we will
58:56 - do we will we work on creating step
58:59 - which we will use for cleaning of the
59:02 - data okay so let's let's do quickly one
59:05 - thing we'll create first of all let's
59:06 - import loging which is pretty important
59:08 - to do
59:09 - so this is something which is of course
59:12 - and then from zml import step right and
59:15 - then I create a step that step will
59:17 - clean the data that step will clean the
59:19 - data right and it will take the data
59:21 - frame I don't know what it will return
59:23 - so let's skip it as this and then we'll
59:26 - pass it so we want to make this step we
59:28 - want to make this step right which
59:30 - cleans the data The Next Step which I
59:33 - want to make is the the is the one which
59:37 - trains our model okay which trains our
59:40 - model so first of all then I'll write
59:42 - model
59:43 - train. and in that what I'll do I'll
59:46 - create another step I'll create another
59:48 - step okay I'll create another step so
59:50 - again same thing so you have to just go
59:51 - ahead and import loging and uh
59:55 - import logging and then import pandas as
59:59 - PD and then from zml import step right
60:03 - and then you just write step and then
60:05 - you just go ahead and create the train
60:07 - model right train model and it takes
60:11 - blah blah blah and then it returns
60:13 - something and then chains the model past
60:15 - them okay so that's that's something
60:17 - which we have to go ahead and I'll just
60:19 - quickly make this like this cool my
60:21 - battery is low I'm so sorry for that but
60:23 - yeah first of all this is the model
60:25 - train which we have to have now once the
60:27 - model train is there we have the clean
60:29 - data we have inest data we have trained
60:30 - the model now the next step which should
60:32 - be it should be evaluation it should be
60:35 - evaluate the
60:38 - model uh so I'll just write evaluation
60:41 - do p and over there same thing which is
60:45 - something again so import logging uh
60:49 - from zml import step and then at
60:53 - step we just just have something Define
60:55 - evaluate model and then returns nothing
60:58 - okay so this is something which we have
61:00 - to actually have and then once the
61:02 - evaluation is done we'll have you know
61:04 - some know that's it so that's that's
61:06 - that's the four steps which we want to
61:08 - have now you might be saying that I'm
61:11 - not implemented I'll implement it the
61:12 - first step is always create a blueprint
61:14 - right so that it runs nicely okay that's
61:18 - the first step and whenever you go you
61:20 - have to actually understand the first
61:22 - step now what I'll do I'll create a
61:26 - pipeline okay I'll create a pipeline the
61:28 - pipeline would be first of all training
61:33 - pipeline pipeline dot
61:36 - pipe that pipeline first of all what
61:39 - what it will do it will from
61:42 - zml okay so we can just from
61:45 - zml zml import
61:49 - pipelines and then at theate pipeline so
61:52 - just just let's just write at theate pip
61:54 - P line and then what I'll do I will
61:57 - simply go ahead and create the training
61:59 - pipeline the training pipeline the
62:01 - training pipeline will consist of the
62:03 - following that will consist of the
62:04 - following that will ingest the data that
62:07 - will ingest the data cleans the model
62:09 - cleans the data trains the model and
62:11 - evaluates the model okay so the training
62:14 - pipeline will consist of the following
62:15 - so I think that something is wrong over
62:17 - there so let me just quickly do it uh
62:21 - just just give me a
62:23 - second
62:26 - okay so now what I'll do I'll create the
62:28 - uh training pipeline so let's just
62:30 - quickly create a simple training
62:38 - pipeline our training pipeline does not
62:40 - takes anything most probably it takes
62:42 - the data path right it takes the data
62:45 - path as an input and that's pretty much
62:47 - it I guess yeah so it takes the data
62:50 - part as a input it first of all we we'll
62:52 - import everything over here we'll import
62:53 - first of all from steps uh from steps.
62:57 - ingest data I'll import the ingest data
63:00 - I hope that is working so let's just
63:02 - make sure that it's not following any
63:05 - conversions so yeah inest
63:08 - DF cool and then what I'll do I after
63:11 - inje from steps
63:14 - do clean data yeah so we'll just go to
63:18 - clean data
63:19 - import clean maybe I'm not sure what it
63:22 - is so let let's let's just go okay clean
63:25 - data let's import clean data and most
63:27 - probably we'll have clean DF just for
63:30 - make sure that we have the good naming
63:32 - conventions and then after that what
63:34 - I'll do I'll import from Step strain
63:36 - model and then after that from evaluate
63:38 - model I just make sure that evaluate
63:40 - model is there cool so once we have all
63:42 - of these steps what I'll do I'll quickly
63:44 - do all of these things very nicely and
63:46 - show it to you the pipeline so we'll
63:48 - just go ahead DF is equals to ingest
63:50 - data it will ingest the path it will
63:53 - clean okay so most probably we'll just
63:55 - have something cleaning so clean data
63:58 - will take the dfz input fair enough it
64:00 - returns nothing so it returns nothing so
64:02 - we just have something very nicely over
64:04 - here after cleaning we'll have strain
64:06 - model and then after that we'll have
64:08 - evaluate model I just hope so that
64:10 - everything takes um DF as an input so
64:14 - that it makes sense Okay cool so now
64:16 - once we have this what I'll do I'll do
64:18 - nothing I'll just go ahead and then run
64:21 - the pipeline okay so how to run this
64:23 - pipeline so we can just create something
64:25 - known as run pipeline as we have created
64:28 - let's just go and create the Run
64:30 - pipeline as soon as possible so we'll
64:32 - just go from pipelines do training
64:35 - pipeline import training Pipeline and
64:37 - then you just go maybe just write train
64:40 - pipeline just just for make sure that we
64:41 - following the naming conventions so the
64:43 - train train Pipeline and then after that
64:45 - I'll just
64:46 - if name is equal equals to main I just
64:50 - hope that it does yeah we'll just run
64:52 - the pipeline so run pipeline will happen
64:54 - something like this and the data
64:55 - pipeline which I'll give is I'll just
64:57 - copy the whole path from here and send
64:59 - it out to here there are a lot of things
65:01 - which you can do you can actually upload
65:02 - in cloud and do stuff which we'll do it
65:04 - later on the of the
65:06 - course Okay cool so let's run it so are
65:09 - you ready if you are then give me a
65:10 - thumbs up I'll get started with it so
65:13 - I'll just go ahead and clear most
65:16 - probably and
65:18 - then let's run it Python and pipeline
65:23 - you know the when I code I actually
65:25 - listen you know lot of you know what do
65:28 - you say uh music but eventually I'm not
65:31 - as of now so sorry for that install
65:34 - pandas pip install pandas right so let's
65:37 - install quickly because that's more
65:43 - important okay something really happened
65:45 - over there uh evaluation right
65:48 - okay as I say uh
65:51 - import Hondas PD that's let's go
65:55 - ahead
66:03 - okay it says modu object is not
66:07 - callable I think most probably what's
66:11 - the error is about it's about uh let let
66:15 - let let me just quickly go to that error
66:18 - training pipeline
66:21 - pipelines okay it's actually pipelin bro
66:25 - right it's actually pipeline okay cool
66:27 - enough let's go
66:41 - ahead Okay cool so something is giving a
66:44 - really interesting error uh because of
66:46 - some of the things so let's just quickly
66:48 - fix it so it when so what is happening
66:50 - I'll tell you why the error is happening
66:52 - it says that wrong wrong type out wrong
66:55 - typee for output for step clean DF why
66:58 - it says that it is expecting Panda's
67:00 - data frame because you have told that it
67:02 - will give Panda's data frame but it is
67:04 - giving none so we have to actually write
67:07 - run over
67:08 - here same over here uh same over here so
67:12 - it is actually expecting that um you
67:14 - know uh it will return something that's
67:16 - why it was initiating that
67:20 - it it'll give warnings just go ahead
67:23 - with the
67:28 - warnings okay that's pretty nice thing
67:32 - which happened cool so I'll explain you
67:35 - what what what just happened you can
67:36 - choose to ignore um completely about all
67:40 - of these stuff like what is this user
67:43 - stack and orchestrator artifact store
67:45 - we'll explain later on so you'll see
67:47 - that the inest has started clean DF
67:50 - started right clean DF has finished
67:53 - evaluate model model started evaluate
67:54 - model finished trade model started trade
67:56 - models finished nothing goes over there
67:58 - that's it now what I'll do I I I'll
68:00 - showcase you this very simple dashboard
68:03 - which is out here so we'll just go ahead
68:05 - and go to the dashboard quickly so the
68:09 - username would be default and then login
68:12 - so when you log in simply go to the
68:14 - pipelines for you it might be super new
68:16 - so let's just go to pipelines this is
68:19 - the train pipeline so let's go to the
68:20 - first
68:21 - one okay fair enough so inest data gives
68:25 - the output which is the data frame so
68:28 - that is a data frame so if you go and
68:31 - and you see the this is the output and
68:33 - it also shows some of the visualizations
68:35 - or you know data type of it you see that
68:37 - data is imported this is called the
68:39 - artifacts okay the thing which is stored
68:42 - uh and over here if you see for after
68:44 - every step so this injest DF has
68:46 - something known as what is the name of
68:47 - this what is the doc string which is
68:49 - like the documentation right start time
68:52 - run time and all those things
68:54 - and then after that what what is the
68:56 - output and the artifact so artifact is
68:59 - something which is returned after every
69:01 - step which is stored in some local
69:03 - stores it is which is stored in some
69:06 - store which can be retrieved further so
69:08 - you see where where it is stored it is
69:10 - stored in this U uh URI so if you go
69:13 - over there and you will see a very nice
69:15 - output over there if you if you go to
69:18 - this particular
69:19 - location okay that is the and logs are
69:22 - simply nothing using cas version step to
69:24 - you you might be noticed what is using
69:26 - cast version of it this is pretty
69:28 - interesting to understand we we'll
69:29 - understand it way greater detail I'll
69:31 - show you a very nice example of it just
69:32 - wait uh and then you have clean DF which
69:35 - is again clean DF is finished evaluate
69:37 - model train model so you see the ingest
69:39 - data gives the and then it does not
69:41 - returns anything so this is a
69:42 - visualization which you can for sure see
69:43 - over
69:44 - here and that's pretty much it and now
69:48 - we pretty much think that our uh
69:50 - dashboard is working our pipeline is
69:52 - running up and we are good to go go with
69:54 - it right cool so one thing which I just
69:58 - want to make sure that you are aware
69:59 - about is something known as caching so
70:02 - uh so what if I do enable cache is
70:04 - equals to
70:06 - true false sorry let's run it
70:12 - okay um let it yeah
70:18 - cool run
70:22 - pipeline
70:31 - now let's see this on our dashboard
70:35 - pipelines the one the latest
70:43 - one so um so you see the same thing
70:47 - which happened over here so I'll just
70:49 - quickly you
70:50 - know do it and then show it to you
70:58 - okay so now this is the you you could
71:01 - see another version is over there which
71:02 - is version number four now ingest data
71:05 - started ingesting data from this ingest
71:08 - data has finished but over here do you
71:10 - think that something really happened
71:12 - interesting using cast version of inj DF
71:16 - so zenim has an amazing and super duper
71:18 - amazing feature what does it mean that
71:21 - it uses the cast version so if there's
71:23 - nothing nothing changes in the data if
71:25 - there's nothing changes in the code or
71:27 - if there's nothing changes in that step
71:29 - it will use the step from the previous
71:35 - run and you see that how interesting
71:38 - this is that see how's the level is
71:41 - going on that's nothing
71:43 - changes right and we eventually using
71:46 - those because the caching was enabled
71:50 - but but caching was disabled right so so
71:53 - catching was disabled but over here I
71:55 - told to enable to to actually uh
71:59 - actually I sa catching isal to for that
72:01 - don't do caching don't use run from the
72:04 - previous version it states step inest
72:06 - data has started uh ingesting data and
72:09 - then in just is finished so if you make
72:11 - it true if you make it true so let me
72:13 - let me just make it
72:16 - true let's run
72:22 - it
72:25 - so uh using cast version of ingest it
72:29 - uses from the clean DF also not change
72:31 - evaluate model it just trains the model
72:33 - in matter of seconds you see how good it
72:36 - is say you're training a large language
72:39 - model right and this is a feature over
72:41 - there you'll be super happy that your
72:42 - forast version has been used sometimes
72:44 - it causes error but most of the time it
72:46 - works like a
72:48 - charm okay that's pretty much cool uh I
72:51 - hope you understood most of most of the
72:52 - things from on the next session on the
72:55 - next video what I'll do I'll Implement
72:57 - all these steps and run it step by step
73:00 - uh and afterwards I'll deploy the model
73:03 - I'll deploy the model using mlflow I'll
73:05 - also track I'll show you how I how we
73:06 - can use ml FL experiment tracker to
73:09 - actually use this and then we'll make a
73:10 - very basic stream extrem lit application
73:13 - to actually use this uh to actually use
73:16 - a deployed model to actually make the
73:18 - Press inference right we'll use the ml4
73:21 - deployment and ml4 tracking a libraries
73:24 - to actually integrate into zml and then
73:25 - use it currently we have the blueprint
73:27 - ready so that what you have learned in
73:29 - this first of all you learned the way to
73:32 - about write code and to structure the
73:34 - code also you learned a very important
73:36 - thing that it's always good to start
73:38 - with preparing a blueprint and then
73:40 - starting coding it I hope it really made
73:42 - sense to you I'll be catching up in the
73:44 - next video
73:48 - bye-bye hey everyone uh welcome back to
73:50 - another video so what I'm going to
73:52 - achieve through the this video is we
73:54 - will Implement all the steps which is
73:56 - listed out here so that with the clean
73:58 - data ingest data evaluation stuff
74:00 - however uh we'll we'll do it in very uh
74:03 - nice way I'll show you the way to write
74:05 - code in a nice way by using design
74:08 - patterns I hope that you have already
74:10 - that you're already aware about the
74:12 - design patterns before you started this
74:14 - project if you are not then also we have
74:16 - a very nice resources uh which which
74:19 - will be linked of course before this
74:20 - lectures you'll be also you'll be taught
74:22 - the basics of patterns like strategy
74:25 - pattern Factory pattern singl ton
74:27 - pattern and all of this is already
74:29 - taught to you so let's get started with
74:33 - actual imp implementation of uh data
74:35 - cleaning and see if you're not aware
74:37 - about these patterns we actually teach
74:39 - in our course cor machine learning
74:41 - course you can actually consider
74:42 - enrolling over there or we'll add before
74:45 - this uh section as well cool so in
74:48 - Source what I'll do uh we have several
74:50 - other so first of all in s SRC will
74:53 - Implement all these classes the classes
74:55 - of these steps and then use the classes
74:57 - from this in these type in these steps
75:00 - first thing which which I really want to
75:01 - develop is data
75:03 - cleaning and data cleaning is something
75:06 - which is obvious which we have to work
75:08 - on so let's get started with actually uh
75:11 - creating uh data cleaning classes so I'm
75:13 - going to start off by importing logging
75:15 - if we really need to log literally
75:18 - anything and then I'll import from ABC
75:21 - import ABC and abstract
75:24 - method uh and then after that from
75:27 - typing import Union but I'll just make
75:30 - I'll just import some basic libraries
75:32 - and then as we need we can import more
75:35 - so I'll import pandas as well import
75:37 - pandas as PD and then from escale learn.
75:41 - model selection because we are going to
75:44 - split out data as well so what I'll do
75:46 - I'll create a abstract class abstract
75:50 - class for defining a strategy for
75:52 - handling data okay you might already be
75:54 - aware by several animals example of
75:56 - strategy pattern and all so first of all
75:58 - create abstract class for defining our
76:00 - strategy this is known as the data
76:02 - strategy this is known as the data
76:03 - strategy this is this will be an
76:06 - abstract class this abstract class would
76:09 - be abstract class
76:14 - defining strategy for
76:18 - handling uh data Okay cool so now this
76:22 - will uh with we'll create an abstract
76:24 - method in in it we'll create an abstract
76:26 - method this you know the reason why we
76:28 - do it is already known to you the reason
76:30 - why why we do it is to just make sure
76:32 - that we have a that we can just that
76:35 - that data clean class will show the same
76:37 - handle data right we have to make the
76:39 - same class so when when when we'll work
76:41 - on other the the the the strategies of
76:44 - these data cleaning so you you'll see
76:46 - how handy it is so handle data this will
76:49 - be the data frame because I expect the
76:51 - data frame uh
76:54 - DF pd. datf frame and that should return
77:00 - that should return um set of you know PD
77:03 - data frame or the series so basically we
77:06 - can we we we can just as I say
77:09 - from uh from typing import
77:13 - Union and then you simply add
77:16 - Union and then pd. data frame and it
77:20 - will return pd. cies to okay so this is
77:24 - what it is going to uh return however
77:27 - this is just an abstract class this is
77:29 - just a blueprint I would say blueprint
77:31 - mean this is what we have to implement
77:33 - in our strategies we can override this
77:35 - method to implement our own custom
77:37 - Solutions so let's first of all start
77:40 - building data pre-processing strategy so
77:42 - we'll we'll build a data pre
77:44 - pre-processing strategy data pre
77:48 - process okay let's make it little good
77:52 - strategy
77:53 - and that will have something know some
77:56 - something which is that will inherit
77:57 - data strategy which is an abstract class
77:59 - so that we can overwrite this handle
78:01 - data right override this handle data so
78:04 - when as soon as we have the handle data
78:06 - so it will it will take the data frame
78:08 - as an input and it Returns the data
78:09 - frame as an
78:11 - output okay so we don't need to logging
78:13 - as of now so it will try so we will have
78:16 - the try and exceptions try uh so
78:18 - basically so basically I'll I'll first
78:20 - of all drop certain you know uh um
78:23 - certain columns from the data because
78:26 - this is something which as as I already
78:28 - told to you that we want to make it
78:30 - super duper simple that's why I'll I
78:33 - I'll Dro several columns however these
78:35 - columns are not like they're not
78:37 - important they are actually very
78:39 - important just for Simplicity for this
78:41 - project I'm going to delete some of the
78:43 - uh some of the um uh columns from the
78:47 - data because that's more important right
78:49 - so what I'll do I'll just um I'll just I
78:52 - have already WR the names of the The
78:55 - Columns which I have to delete so I'll
78:57 - just copy them out from quickly from
78:59 - there
79:01 - uh yeah so you can also see that order
79:04 - approved that order you know received
79:06 - that and all those stuff which is
79:07 - required over here
79:10 - okay very cool so data. drop you do you
79:14 - drop certain uh columns you drop certain
79:16 - colums out there and then you simply go
79:18 - ahead with it okay cool so now what I'll
79:21 - do I'll go to go go to the next step I
79:23 - really hope that it works nicely yeah
79:25 - cool so now what I'll do um there are
79:27 - some certain columns there are some
79:29 - certain columns which
79:32 - has which has the um null values okay so
79:37 - what I'll do I I'll quickly fill up the
79:39 - null values you can do by two or three
79:42 - two or three things okay what what you
79:45 - can do you can actually uh do the when
79:47 - these things are analyzed when you do
79:48 - the Eda part however I have done already
79:51 - Eda on my part as as as I said you can
79:54 - actually make the Ed and then see which
79:56 - columns and all I've already did it just
79:58 - for Simplicity so that it actually makes
80:00 - sense to you to actually get started
80:01 - with directly with the project there are
80:03 - some certain columns um which is
80:06 - actually you know um there's some
80:08 - certain columns which which has the null
80:10 - values and we'll try to you know uh make
80:13 - it work so here we go so the data which
80:16 - we have they so basically these these
80:18 - columns fill now with a median of that
80:21 - column and in place equals to true with
80:23 - the median of that column in place equal
80:24 - two median means we have to take the
80:26 - median of this and then we have to
80:28 - permanently apply this on our data right
80:31 - and there's review common message which
80:33 - Al also fill the null values with no
80:35 - review because there are several n null
80:37 - values in the data so that we can just
80:39 - write no view over
80:43 - there okay that's very cool so now what
80:46 - I'll do I will just go ahead and we will
80:49 - we will drop the columns we'll drop the
80:51 - columns which are you know uh which are
80:54 - of non which are of non- number type or
80:58 - some some some columns which are
81:00 - actually you know um uh numbering times
81:04 - okay so basically we'll just take
81:05 - columns to train a model who are numbers
81:08 - however you can take like see the the
81:10 - reason why I'm doing this the selecting
81:12 - the number is not because of I want to
81:14 - you know I I'm doing it on purpose it is
81:17 - like the reason why is I just want to
81:19 - make this project simple so I'll select
81:21 - the AL select the columns which are of
81:24 - numeric so that I don't need to apply a
81:26 - lot of processing steps okay so what
81:28 - I'll do I will simply go ahead and data
81:32 - is equals to data do select data do
81:36 - select select data types I'll select the
81:38 - data types which are
81:40 - include uh numbers so NP do number okay
81:44 - so this data will have the
81:47 - columns we have the data which are of
81:50 - only numeric type so now we don't need
81:52 - to worry about categorical encoding
81:54 - ordinary encoding or whatsoever or even
81:56 - tokenizer of this this is also removed
81:58 - okay but you can do lot of things out
82:00 - here you don't need to remove this you
82:02 - you actually Implement another
82:03 - processing strategies where you encode
82:06 - the data where you tokenize this re
82:08 - review comment messages and a lot of
82:10 - things which you can do over here we'll
82:11 - also drop a couple of
82:14 - uh couple of you know uh columns and the
82:18 - columns which we have to drop is the
82:20 - following so the first one is is uh
82:23 - customer zip pre prefix and the order
82:26 - item so these are the columns which you
82:28 - have to drop the reason why we have to
82:30 - drop because this is not important at
82:32 - all okay so we can just write data is
82:34 - equal to data drop drop equals to true
82:36 - and then return data accept exception as
82:40 - e logging error and raas E cool so um
82:43 - you might be worried about like what did
82:45 - I did over here first I dropped certain
82:48 - columns which is not required for us as
82:50 - of now because of the because because of
82:52 - making the project simple second you
82:56 - drop the you actually fill up the null
82:57 - values which are available in the in
82:59 - these kind of columns and then you only
83:01 - select the data which is a numeric type
83:03 - we are not selecting the categorical
83:04 - encoding just categorical data that's
83:06 - just because of the Simplicity of the
83:08 - project and then you drop certain
83:10 - certain uh you know um uh columns and
83:14 - then you just return the data that's
83:16 - pretty much it that's pretty much it
83:18 - you're
83:20 - doing that's pretty nice so now what
83:22 - I'll do I'll create another strategy the
83:25 - another strategy is data split strategy
83:29 - so basically
83:30 - data
83:32 - divide
83:34 - strategy and then that will inherit the
83:36 - data strategy and in that we'll just
83:39 - quickly
83:42 - create strategy for dividing the data
83:44 - into train and testing set we'll again U
83:48 - make the handle
83:51 - data
83:53 - and then the data it will take p. data
83:56 - frame and it will return you know it
83:58 - will return it will return the union of
84:02 - pandas data frame and series you will
84:04 - notice why I'm saying Union Union means
84:07 - uh both of them so here we go so I
84:09 - quickly explain you what does it matter
84:11 - so X is equals this is all copilot
84:13 - that's why I love him so data. drop we
84:15 - are dropping the um the the the target
84:18 - variable and then we have Y which is the
84:20 - target variable and then xray in by xra
84:22 - y Train by test and give the test size
84:24 - to be 0.2 and the random state to be 42
84:27 - and the next train is the panas data
84:28 - frame X taste is the pan data frame y
84:31 - train is a series and Y test is a series
84:34 - that's why your output of the
84:36 - combination of both of
84:39 - them I hope it makes pretty much very
84:41 - sense so now once we have that now once
84:44 - we have that we will make we'll make a
84:47 - final class where we will utilize where
84:49 - we'll utilize both of these strategies
84:52 - into that class so we'll create another
84:54 - class which is data cleaning okay the
84:57 - data cleaning data cleaning class which
84:59 - will process the data and divide the
85:01 - data so I'll just create create data
85:05 - class for which will which
85:09 - preprocesses okay processes the data
85:13 - oops
85:15 - and divides it into training and testing
85:18 - set cool so what I'll do I'll quickly
85:20 - create Define need and then it will take
85:25 - self data frame and it will also take
85:27 - what strategy you want to implement that
85:30 - strategy would be the would be the data
85:33 - strategy it can be either you know this
85:36 - these are are the these are the types of
85:38 - data strategy right this abstract class
85:40 - so strategy will take either do you want
85:42 - data process strategy or divide
85:45 - strategy Okay cool so what I'll do I'll
85:48 - quickly self.
85:51 - strategy see equals to
85:54 - strategy okay so now we'll we'll have
85:57 - another class which is handle data sorry
85:59 - method and that will that will either
86:02 - return Union or you know a simple Panda
86:04 - State data frame so this will return
86:06 - self. strategy. handle data and then
86:09 - self data so basically the strategy will
86:13 - for for example if someone chooses this
86:14 - data divide strategy so so so data we
86:17 - can use the simple class so basically
86:19 - someone can someone will go and just run
86:20 - this class data cleaning
86:23 - so some something like this if name is
86:27 - equal equals
86:31 - to is equal equal to
86:33 - main
86:37 - oops I'm s for
86:47 - it okay um over here it will just say
86:50 - data cleaning
86:53 - okay so it will say something for for
86:55 - example assume that we reading this uh D
86:58 - CSV file however we are not going to do
87:00 - it right now and then data cleaning and
87:02 - then data cleaning is we'll instantiate
87:04 - this with data and then we want to use
87:06 - this data pre-process strategy so that
87:08 - data pre-process that data cleaning will
87:10 - use this data pre-process strategy in
87:12 - this case over here and then it it has a
87:14 - method called handle data it will run
87:17 - over
87:18 - there then you can same same way you can
87:20 - give it another strategy right which is
87:22 - data device strategy it will do nicely
87:25 - okay so I hope that you really
87:26 - understood the the wave that we do this
87:28 - is called a strategy pattern where you
87:30 - first of all create the abstract class
87:32 - and there are several strategies in it
87:33 - which is data pre-process and data
87:35 - divide and then create the final class
87:36 - which will make use of those strategies
87:39 - uh over
87:40 - here okay and this all this this is
87:42 - actually very helpful when actually be
87:45 - just for flexible code writing as well
87:47 - as readable as well as not writing so
87:49 - much of FNL statements
87:53 - cool so what I'll do I'll quickly
87:54 - implement this into um clean data so
87:57 - let's implement this into clean data
87:59 - that's more important for
88:01 - us okay so clean data we'll take the
88:03 - data frame as an input and then we'll
88:06 - just uh go ahead and
88:08 - try and then let's go so first of all
88:11 - we'll import we'll import what we we
88:13 - we'll have to import uh
88:18 - from source. data cleaning
88:23 - I'll import
88:25 - data
88:26 - cleaning data divide strategy and data
88:31 - pre-process
88:33 - strategy
88:35 - Okay cool so once once we import this
88:38 - now we'll just go ahead and use it try
88:41 - and then try and accept so basically
88:43 - first of all we'll we'll create a
88:44 - processes process strategy so over here
88:47 - we can just go ahead and create Pro
88:49 - pre-process process strategy process
88:53 - strategy and then we C the DAT data
88:56 - cleaning
88:58 - class data cleaning is equals to data in
89:02 - instantiate the data clean class by
89:04 - giving this process strategy and then we
89:07 - will uh have something which is
89:11 - processed data is equals to the the
89:15 - object which we have object which is
89:17 - data cleaning do process data okay
89:21 - process data sorry sorry sorry handle
89:25 - data so what is that eventually doing we
89:28 - have this class we are giving the
89:29 - strategy which we want to use we want we
89:31 - want to use the data preer strategy and
89:33 - then we are calling that strategies uh
89:35 - method which is handle data which will
89:38 - handle the data then we'll have the data
89:40 - divide
89:43 - strategy or maybe divide strategy right
89:47 - and then it will again data cleaning and
89:50 - then you have something known as process
89:51 - ESS data in this case not the data now
89:54 - you have the device strategy now we can
89:56 - simply make use of XT train YX test y
89:58 - train y test do handle data because it
90:00 - is returning Panda's data frame in
90:02 - series and then we have to actually
90:04 - loging data completed and then excepts
90:06 - except as e loging error raise e okay so
90:10 - now one thing which is missing is it
90:12 - returning none it's returning xtrain X
90:15 - test y train and Y test right so we'll
90:17 - use something known as annotated which
90:19 - is the python built-in type setting path
90:21 - parameters a type hend parameters so
90:24 - let's first of all quickly do this from
90:25 - typing extensions from
90:28 - typing extensions I'll import annotated
90:32 - which is a formal one so annotated what
90:34 - it will do and also we we'll have to
90:35 - import the tupo let's quickly import tle
90:38 - from
90:42 - typing import tle
90:46 - sorry uh over here we'll have two p and
90:50 - then
90:54 - okay so annotated the first output the
90:57 - first output is of course pd. data frame
91:01 - and then it will it it's actually xtrain
91:03 - right it's actually xra now we have
91:06 - another one which is annotated X test
91:08 - and then another one y train and Y test
91:12 - and mostly we are done so so basically
91:14 - this is what happens that we are done
91:16 - and now one can actually now it says
91:19 - that it will return the tle it will it
91:21 - it will return the following it will
91:23 - return the four four types which is data
91:25 - Frame data frame series and series which
91:26 - is an annotated using annotated uh from
91:29 - typing EXT extensions so I hope that it
91:32 - makes sense uh now I let me see what
91:34 - type of error it is giving it's mostly
91:36 - because of the this I hope this fixes it
91:40 - so now we are done with this step now we
91:43 - can just you know simple make it very
91:45 - basic doc string cleans the data and
91:47 - divides Orcs so let's just write
91:50 - Orcs
91:52 - raw data and then simply you just have
91:55 - this you can also write
92:01 - returns training data testing data
92:04 - training labels and testing labels okay
92:07 - so now we have this class ready for us
92:10 - and then we can actually use sorry step
92:12 - ready for us where we using several
92:13 - strategies and then we'll actually
92:15 - implement it okay so I hope this this
92:17 - actually makes sense to you all now the
92:19 - next thing which which we'll work on is
92:21 - something something known as um which is
92:25 - something known as model development so
92:28 - model development is something which is
92:29 - pretty much important we'll actually
92:31 - make use of uh linear regression which
92:34 - we will Implement right away from here
92:36 - Implement right away from here and you
92:38 - know so yeah um so we'll just Implement
92:41 - linear regression out here so that it
92:42 - makes sense for you to get started with
92:44 - it so we'll Implement a basic LR so that
92:47 - it is not however there's a lot of
92:49 - things which you can Implement I in the
92:51 - representative which you'll get you'll
92:52 - be having implemented these kind of like
92:54 - you know random forest or XG boost CAD
92:57 - boost and then after that we'll evaluate
92:59 - our model So currently we're not
93:01 - focusing on core machine learning kind
93:02 - of thing we're just focusing on building
93:04 - a full mlops project so I can build it
93:07 - in more complex situations another waye
93:10 - which we have is evaluation as well as
93:12 - where we'll we'll we'll make an EV
93:13 - evaluation measures and then after that
93:16 - we'll also make the steps for it and
93:18 - then we are mostly done however there's
93:20 - something which is left left which is
93:22 - something know as deployment pipeline
93:24 - we'll also deploy the pipeline right you
93:27 - will be amazed to see the the way we
93:29 - deploy the pipeline the way we run it
93:31 - right and the way we run it and also
93:33 - we'll just use a stream application to
93:35 - actually go ahead with this then
93:37 - deployment so I hope that really it
93:38 - makes sense so let's catch up in the
93:40 - next video so now everyone what I'll do
93:42 - I'll go to the next step which is model
93:44 - development which is pretty much
93:46 - important as well um so let's get
93:48 - started with model development quickly
93:50 - and then try try try to complete this
93:52 - project as soon as possible so modal
93:54 - depth. pi and in that what I'll do I'll
93:56 - create a I'll create again the you know
93:59 - uh abstract class and then we have to
94:02 - extend that abstract class from
94:05 - ABC from ABC import ABC and Abstract
94:09 - method so let let's just go and start
94:11 - off with it so we create a class model
94:14 - the class model will have ABC right this
94:17 - is the abstract class for all models
94:20 - this is abstract
94:22 - class for all
94:24 - models um and then after that we'll
94:27 - create an abstract method the abstract
94:30 - method and Abstract method will be
94:31 - called as a self train and that self
94:34 - train will have something as X train
94:36 - which is train training data by train
94:38 - which is testing sorry uh training
94:41 - labels we we can also create some method
94:43 - known as optimize uh but it's not
94:46 - required as of now so let's just leave
94:48 - it so uh I'll create a very simple class
94:51 - see my point again I'll say I'm
94:54 - emphasizing on it first of all focus on
94:56 - learning about amops and then
94:57 - implementing complex models and stuff so
95:00 - I'll just make a simple linear
95:01 - regression model on top of it so let's
95:03 - just make a simple linear regression
95:06 - model and then it will take xtrain and
95:09 - by train and
95:13 - quirks it will uh first of all for
95:15 - together and it would just Okay cool so
95:18 - we'll have some some something which is
95:20 - training and the training we'll first of
95:22 - all we'll just import from SK
95:28 - learn do linear model import linear
95:32 - regression and uh most probably let's
95:34 - name it as a model okay it makes much
95:37 - more sense okay so um I'll just make it
95:40 - over here quickly which is reg equals to
95:45 - linear
95:47 - regression
95:49 - qux and then reg. fit which is and then
95:53 - return the regression and then return
95:55 - the regression right if we can also put
95:58 - put this in a try and try an error so so
96:05 - try accept Okay
96:10 - cool okay that's very nice so now what
96:12 - I'll do uh so now what I'll do I'll
96:14 - simply go ahead and uh you know just we
96:17 - have the model training model ready
96:19 - however uh we'll see in the next Pro
96:21 - which we do this is model development is
96:23 - much more complex because we have to
96:25 - first of all train the model validate
96:26 - the Assumption test if things are
96:28 - working or not you know tweak the data
96:30 - fure engineering cleaning which we'll do
96:32 - in the next project don't need to worry
96:33 - about it okay so this is the model
96:35 - development which is linear regression
96:37 - model where we simply fit it and then we
96:39 - train the model we complete the training
96:40 - of the
96:41 - model so let's quickly go to model train
96:44 - and then we just Implement something
96:46 - over here so uh what I'll do I'll just
96:49 - go ahead and then import from model
96:51 - sorry from
96:54 - Source model def UT linear regression
96:58 - model right yeah L linear regression
97:01 - model now what I'll
97:03 - do I'll simply go ahead and
97:07 - uh I'll simply go ahead and then first
97:10 - of all we have to first of all get the
97:12 - get the data which we have to so it will
97:14 - take several input so it will take extr
97:17 - X test y train and Y test so let's just
97:19 - take cell extr
97:24 - and then Peter data frame X test y train
97:27 - okay X
97:30 - test white train and white test and it
97:33 - will uh yeah that's pretty much it it
97:35 - will return regression mixing so
97:38 - actually it will return the linear
97:39 - regression model right however there is
97:41 - something known as regress and mixing
97:43 - right so from SK learn from SK learn do
97:48 - base okay
97:52 - base import regression
97:54 - mixing regression mixing is a type of
97:57 - you know which is the type like of
97:58 - course we are going to um output the
98:01 - regression algorithm right trains the
98:04 - model and then simply you know
98:06 - appreciates the model my uh I mean just
98:08 - stain the model that's that's pretty
98:10 - much
98:10 - a Okay cool so let's just first of all
98:13 - do it and then let's go on the next PATH
98:15 - so the model which we have is equals to
98:17 - none and we'll also make a config do PI
98:20 - we make that
98:22 - config.py so make some something as
98:25 - config.py and that config.py will have
98:28 - from zml do steps
98:31 - import base
98:34 - parameter base parameters and then
98:37 - create something as model name config
98:41 - that will have base parameters out here
98:44 - and it will contain the model
98:46 - configurations which we which we want to
98:48 - add model configs which which can be
98:51 - model name first of all model what what
98:54 - model name which we want to use what
98:56 - model we want to use and then yeah so so
98:59 - that's it so that is the model name
99:00 - which we want to use so first of all
99:01 - we'll import some something which is
99:03 - over here we'll import a model train and
99:05 - then we'll import um
99:08 - from do config
99:11 - import model name config and it will
99:14 - also take config which will be the type
99:16 - of model name
99:19 - config okay Okay cool so now it will it
99:22 - will also take the config so config will
99:24 - contain the stuff so if so
99:28 - if if the config do model name is linear
99:31 - regression we will say um just just you
99:35 - know use that model which is linear
99:38 - regression model linear regression model
99:41 - and then just train the model on X train
99:44 - and Y train okay that that is something
99:46 - which you really want to do or what or
99:49 - what what we can do we can just have
99:51 - have something which is U
99:54 - model so it will of course it it should
99:56 - it should return something right so let
99:58 - me just go and quickly see yeah it is
100:00 - returning lineation model we just have
100:03 - train
100:05 - model is equals to model train and X
100:07 - train X test and it Returns the train
100:09 - model
100:10 - else uh we can just write you know
100:12 - something which is model name not listed
100:15 - or something some something like that
100:17 - you can raise a value
100:19 - error
100:24 - okay so um the reason the the reason why
100:27 - why I do this over here you might
100:29 - Implement other models as well you can
100:31 - just go ahead and Implement class random
100:34 - forest model right random forest model
100:37 - so you can just go ahead and don't don't
100:39 - don't need to change the name you just
100:41 - say if the config says if the config do
100:44 - model name says random for regressor you
100:46 - train another model so this is how it
100:48 - works okay you don't need to worry about
100:49 - like lot of things out out here it's
100:52 - very simple to understand so just have
100:55 - it as an
100:59 - try and accept exception as e logging
101:03 - error and then raise the E Okay cool so
101:06 - that's it about the training of the
101:07 - models we'll just go ahead and quick
101:08 - quickly create some something known as
101:10 - evaluation system part so let's just go
101:12 - and create the evaluation part as
101:16 - well
101:19 - evaluation
101:22 - Pi I'll just go ahead and create the
101:24 - evaluation. pi so again over here we'll
101:26 - create a very basic again abstract class
101:29 - and then it extend that abstract class
101:32 - to other um strategies which you're
101:34 - going to use over there from ABC from
101:36 - ABC import big ABC and Abstract
101:40 - method and then we'll just have class
101:44 - evaluation that that will take
101:46 - ABC and then it will have something it
101:49 - is an abstract class right it's an
101:51 - abstract class
101:53 - defining strategy defining strategy for
101:57 - evaluating our
102:00 - models right and then we'll have
102:02 - abstract method abstract method will
102:04 - have something calculate scores so it
102:07 - will calculate the scores out here which
102:11 - is y true and then it will it it it is a
102:14 - numai in the array so we just import
102:18 - numai as NP so nump and the
102:22 - array and by prediction which is also
102:25 - the numpy and the array so cool so over
102:27 - here you have something which is
102:29 - calculus scores which is abstract method
102:32 - and then abstract method will have
102:33 - something over here which is y to the
102:35 - the model prediction sorry ground thr
102:37 - and the model prediction now what I'll
102:39 - do I'll simply go ahead and uh create
102:41 - several strategies for it the first
102:43 - strategy which I which I'll create
102:45 - something as MSE so that that MSE will
102:47 - inherit the abstract class of evaluation
102:50 - and this is this is the evaluation
102:51 - strategy this is the
102:53 - evaluation this is the evaluation
102:56 - strategy that uses mean
103:01 - squ eror right mean squ eror and then
103:06 - we'll create the calculate score that
103:09 - calculate scores will take self again y
103:12 - y true and Y
103:14 - true and which is of of course num and
103:18 - I'll just copy it from
103:19 - here
103:22 - Okay cool so uh again so we'll just
103:25 - start
103:28 - with we'll just say we have
103:31 - entered calculating msse so it will
103:34 - start off with the calculating MSE so
103:35 - basically we can use simply something
103:37 - known as from Cy learn Matrix from SK
103:40 - learn. Matrix I'll import mean squ error
103:44 - and R2 score so we'll just go ahead and
103:46 - then just do it so let's let's do so
103:49 - we'll just have some something MS e
103:51 - we'll just give y true and Y PR we just
103:53 - say that it is done and then we return
103:55 - the MSE otherwise we see if there's
103:58 - anything wrong which is error in
103:59 - calculating scores and that's pretty
104:01 - much it so uh it Returns the MSE so now
104:04 - we have one strategy done we'll go and
104:06 - create another strategy we'll go and
104:07 - create another strategy another strategy
104:09 - would be R2 score so R2 score we'll have
104:12 - the evaluation strategy that you that
104:14 - uses so that that uses R2 score and then
104:17 - we'll just calculate the scores and then
104:19 - give everything out so it just
104:21 - implements automatically of course you
104:23 - can add your documentation on your own
104:24 - over here I'm I'm not adding it right
104:26 - now please add it by your own the way I
104:28 - have taught you to do so we'll have
104:31 - we'll have another um evaluation
104:33 - strategy which is evaluation rmsse and
104:36 - then over there we'll again that's
104:37 - evaluation strategy that uses the root
104:39 - mean squared error to calculate stuff so
104:43 - it just again just you know mean square
104:45 - error and then rmse and squared equals
104:48 - to false so basically over here your
104:50 - calculating the root mean square error
104:54 - right okay so now we have the rmsc also
104:56 - done so we have several evaluation
104:58 - strategies totally done now we'll just
105:01 - go ahead and then implement it in
105:03 - evaluation out here so now which is the
105:06 - last thing which you have to do is very
105:08 - very simple that you actually implement
105:11 - this so we have uh so first of all I'll
105:14 - import
105:15 - from SRC do model Dev sorry
105:21 - evaluation I'll import msse
105:24 - rmsc and
105:26 - R2
105:28 - R2 is it
105:30 - there evaluation R2 yeah R2 is there
105:33 - cool R2 is there um so it will just
105:36 - first of all we'll have the evaluate
105:38 - model this will take lot of things first
105:40 - of all it will take model okay that
105:43 - model would be a regressor mixing so
105:46 - we'll have to UT this is the type of the
105:48 - model would be the regression mixing
105:49 - because it is a regession regression
105:50 - model right so import regressor
105:54 - mixing then we'll uh then we'll uh get
105:57 - the X test then we'll get the X test
106:00 - that X test will be the Panda's data
106:03 - frame and then we'll get the Y test
106:05 - again for and for understanding now
106:08 - let's try try to implement the solution
106:10 - let's try to quickly implement the
106:11 - solution so first of all we'll get the
106:12 - prediction we'll get the prediction
106:14 - quickly the prediction which we'll get
106:15 - is model do
106:18 - predict and then on the X test so model
106:21 - predicts on X test we we create MSE
106:24 - which is is equals to MSE class so so
106:27 - sorry
106:29 - MSE class is equals to
106:33 - MSE and we use that MSE is equals to MSE
106:36 - class class. calculate scores it will
106:39 - simply just have to give y test and
106:41 - predictions and then we are done so now
106:43 - we have another ver is R2 class R2 class
106:46 - and then about and and then after that
106:49 - you have calculate the scores and that's
106:51 - pretty much it and then you have another
106:52 - rmsc class R calculator course and then
106:55 - that's pretty much it cool so we'll
106:58 - return at least let's return two things
107:02 - let's return um msse let's let's return
107:05 - R2
107:07 - score and rmsse right because that's
107:11 - more efficient to actually look at the
107:13 - Matrix so we are done and we can just
107:15 - put this into
107:19 - try and then this into accept eror
107:22 - evaluating the
107:25 - models Okay cool so now we are mostly
107:27 - done the one thing which is left out
107:29 - here so you might be thinking what is
107:31 - left guess what is left so over here we
107:33 - are returning to R2 score and RMS so we
107:36 - also have to indicate over here that
107:38 - what thing we are returning so I'll just
107:40 - import
107:41 - from typing import Tuple and from typing
107:46 - extensions I'll import
107:49 - annotated okay so this will have two and
107:52 - then it will return two things let's
107:55 - annotate the float R2 score as as the
107:59 - RMS okay so now I hope that it makes
108:02 - little bit more sense now uh I really
108:05 - hope so yeah cool so now we have the
108:07 - evaluate model also done which is which
108:09 - means that we are pretty much done with
108:12 - ingesting of the data cleaning the
108:15 - data training the model evaluating the
108:19 - model now you understand everything is
108:21 - completed now what is left let's worry
108:24 - about that so we have something known as
108:27 - run pipeline so let's try to go ahead
108:29 - into into that Pipeline and let's try to
108:32 - create the pipeline let's try to run
108:34 - that pipeline right away from here so
108:37 - let let me just quickly go and run the
108:39 - pipeline um so I'll just go
108:41 - [Music]
108:43 - to um
108:47 - yeah cool so I'll enable the cash as are
108:50 - true and this takes the data path that's
108:53 - that's good that's good takes the data
108:55 - path we have then clean what does clean
108:59 - DF takes clean DF takes something and
109:01 - what what is it returns this returns X
109:03 - TR XTR and by TR okay so let's just
109:04 - quickly write this xra X test and Y TR y
109:08 - test it it's a clean DF right this takes
109:11 - the data so now it is done we have train
109:14 - model so train model what does it does
109:16 - train model you know takes X train X
109:18 - test y train y test and the configs as
109:21 - well so what what we have to do over
109:23 - here we have to
109:25 - actually
109:27 - um
109:28 - okay so let's try to uh quick quickly do
109:31 - that as well um so I'll just go ahead
109:34 - and uh model train so this is train
109:40 - model okay so train
109:44 - model and then after that model is
109:47 - equals to train model
109:52 - train model X test y test all the the Y
109:55 - test and then we simply go ahead and
109:57 - msse which is
109:59 - msse sorry R2 score and rmse you
110:04 - evaluate the model by giving these
110:05 - things X test and Y test I hope that
110:07 - really is X test and Y test that's it
110:11 - yeah that's true and we are mostly done
110:14 - right so now we have the pipeline ready
110:16 - we have everything ready now let's go
110:19 - and run the whole pipeline to see the
110:22 - magic I'm pretty I'm pretty much sure
110:24 - that it it will give some sort of error
110:26 - but always be be on a positive side so
110:28 - let let's just quickly go and run the
110:31 - pipeline okay so no module named psyit
110:34 - lar so let let's just quickly go and so
110:36 - I'll just go escale learn pep
110:41 - install and then just go and uh
110:46 - okay I'll use this one because this is
110:49 - much more easy to install
110:51 - okay let's just wait for this to be
110:53 - installed because I'm I'm actually using
110:55 - the new environment that's why please
110:57 - activate your environment before working
110:59 - on the project please that's the request
111:02 - for you all of you out
111:14 - here let's wait and let's see the magic
111:16 - what
111:17 - happens it's running on so just wait for
111:20 - a few
111:21 - seconds um and after that we are mostly
111:23 - done done with the pipeline of melops
111:25 - you will see the dashboard the next
111:27 - thing which is left which is integration
111:29 - of tracking of our experiments which is
111:31 - using MLF flow and then deployment of
111:33 - our model using MLF flow deployment
111:35 - these two things are left and then we
111:36 - are mostly done with the project and
111:38 - you'll be seeing like now now I really
111:40 - hope that you are seeing the way we do
111:42 - the project the way you know we do the
111:43 - caching stuff the way we write the code
111:46 - that is much more visible in the next
111:47 - set of projects you'll see much more
111:49 - challenging code much more challenging
111:51 - topics which eventually you will learn
111:53 - by
111:55 - yourself so I don't know why it is not a
111:57 - running but okay so cannot unpack a non
112:01 - iterable step R effect object so I guess
112:05 - something really have an interesting out
112:06 - here so let let's just quickly go and
112:08 - see what the error it has given so this
112:11 - expects the data frame and this returns
112:14 - X test and Y train and Y test and then
112:17 - clean DF we have the
112:22 - following full name uh okay my
112:26 - test so let's see now if it is actually
112:29 - returning oh yeah so you see that that
112:32 - it is not returning anything we have to
112:34 - actually return it right that's why it
112:36 - is saying that that's it is returning
112:38 - none and when it is asking for the
112:40 - output so that's why it is not able to
112:42 - cool so inest it is started clean data
112:44 - completed and then we just go ahead and
112:46 - then it justes does something small
112:48 - training completed something failed in
112:50 - the pipeline and R2 score is not
112:53 - defined so let's go and quickly do it so
112:56 - uh let's go to evaluation and then this
112:59 - is R2 not R2 score so let's just go and
113:02 - do that as
113:04 - well so now you'll see how quickly it
113:07 - will be run first of all it will use the
113:09 - cast version you know it it it will use
113:11 - the cast version of it and it will just
113:13 - do the evaluate model and then you're
113:15 - done you see the magic you see you you
113:18 - just see the magic just simply install P
113:20 - install P Arrow to remove this error so
113:23 - let's just quickly go in zml up so let's
113:25 - just go in zml
113:27 - up please do
113:30 - it it will open
113:32 - it okay okay sir I'll give you the
113:37 - default let's go to pipelines let's go
113:39 - to train pipeline let's go to this
113:42 - pipeline you see it ingests the data it
113:45 - againsts the output clean the data at
113:48 - the clean DF it returns these these goes
113:50 - into training the model it returns these
113:52 - gexas goes in evaluation of the model it
113:55 - return R2 score and rmsc you see how
113:58 - magical this is right how magical and
114:00 - how really interesting these things has
114:02 - become right now I I I really think that
114:05 - this is the power this the future of ml
114:08 - right if you don't know about this you
114:09 - don't know anything right so I just go I
114:12 - just hope that you understand it much
114:13 - more greater detail we are most done
114:15 - with the project however there's two
114:17 - things which is left which is deployment
114:19 - as as well as we are also left with
114:21 - tracking of our experiments so I hope
114:25 - this makes sense I'll be catching up in
114:26 - the next lecture
114:29 - bye so hey everyone uh let's come back
114:32 - to our project so basically the project
114:34 - is left with two things the first thing
114:36 - which is left which is of course our
114:38 - most favorite experiment Draco and the
114:40 - second thing which is left was the
114:42 - deployment of our model so I'll talk
114:44 - about what this experiment tracker means
114:46 - along with I'll talk about the
114:47 - deployment pipeline so let's just first
114:49 - of all talk about about what thises
114:50 - experiment tracker means so when you do
114:52 - the data science engineering or a real
114:53 - world machine machine learning
114:55 - engineering job then most probably what
114:57 - you will see that whenever you have you
114:59 - actually want to track every run switch
115:02 - you do because you have to tweak the
115:03 - parameters and then rerun it and then
115:05 - check the scope from the previous one
115:07 - compare it with SE several Matrix and
115:09 - see how well well it was performing in
115:12 - the 30th run or even in the first run
115:15 - right so we need to track our every
115:17 - experiments which we are doing over here
115:20 - so so where should we Implement our
115:22 - experiment tracker the experiment
115:24 - tracker will will be implemented over
115:26 - the train model so what I'll do I'll
115:28 - quickly implement the experiment Tracker
115:30 - out there so when you go to the model
115:32 - train so model train will have something
115:34 - like this and uh I'm so sorry for the
115:36 - background noise I'm extremely sorry
115:38 - because this is India and you keep on
115:40 - hearing these voice so I'll simply
115:42 - import ml flow import ml flow so once we
115:46 - import the ml flow what I'll do I'll uh
115:49 - simp simply go ahead and then initiate
115:52 - an experiment tracker class sorry uh
115:55 - object but but before that we have to
115:57 - import something known as client from
115:58 - zml Cent import client and that seems
116:02 - like experiment
116:06 - tracker is equals to client get
116:09 - experiment uh get active stack so I'll
116:12 - just go ahead and then do
116:15 - active
116:17 - stat okay wait wait for a second
116:20 - yeah do active
116:24 - stack do
116:28 - experiment
116:30 - tracker so once we have this we can EAS
116:33 - easily use this so basically what what
116:35 - we have to do we have to use this ml4
116:36 - tracker right so we have to in in The
116:38 - Decorator we have to pass the experiment
116:39 - tracker and the experiment tracker which
116:41 - we'll use is the following experiment
116:43 - tracker do name then the name of that so
116:47 - that it should be notified that this
116:48 - step has the experiment track track
116:50 - right so now what we have to do in this
116:52 - case we have to actually you know um uh
116:56 - log our models okay log our models so in
116:59 - this case we have to actually use the
117:01 - pyit learn Auto so basically what I'll
117:03 - do I'll use mlflow dosk learn. autog
117:08 - this will automatically log your models
117:11 - scores and everything out there right in
117:14 - the same way you have for several other
117:15 - libraries so basically we'll do the m.
117:19 - log same what I'll do I'll do on
117:22 - something known as evaluation right so
117:24 - so what what what I can do I let me just
117:26 - go to evaluation part and in evaluation
117:29 - part I I I have to do the same thing I
117:31 - have to actually copy the two couple of
117:34 - things which I did and then what I'll do
117:36 - I'll simply copy the step as
117:39 - well step as well and then over here
117:42 - what I'll do I'll I'll simply go ahead
117:44 - and then select ml slow. log matric and
117:50 - then I'll log the msse right same goes
117:52 - with ML flow. log Matrix I log the R2
117:55 - and then I log rmsc right so I've
117:59 - already logged these three things now
118:00 - what I'll do it's mostly done so now we
118:02 - actually can use this particular um
118:06 - statement over here but but before that
118:08 - let's import sorry sorry sorry sorry
118:09 - let's import something known as import
118:17 - flow cool so now what we have to do
118:19 - guess what what we have to do we have
118:21 - simply go to non run Pipeline and then
118:23 - simply run the same pipeline so let's
118:24 - just quickly run I've already uh done
118:27 - this so let's just run the pipeline
118:28 - first so once we run the pipeline it
118:30 - will say that we are using the mlow
118:32 - tracker and once once it says that that
118:34 - we using the ml tracker it will say
118:36 - something like this just no module names
118:39 - ml flow so what you can do you can
118:41 - simply go ahead and just just just like
118:43 - zml integration install ml flow which is
118:46 - simply you going go over here and then
118:48 - search something thing which is like
118:51 - this right and then paste it sorry for
118:56 - that um simply past over here which is
118:59 - zml integration install ml flow it will
119:01 - take some time to install the ml flow
119:03 - but before going on to running that you
119:05 - have to make sure that first of I'll
119:07 - explain you what the stack means stack
119:09 - means that there's something the stack
119:12 - which is a cont containerized thing
119:14 - where your project is running and the
119:17 - stack I I'll show you what the stack
119:19 - contains stack contains very artifact
119:22 - stores which are default okay
119:23 - orchestrator which is default you don't
119:25 - need to worry about what these
119:26 - terminologies means so but basically the
119:29 - thing which is a default the the stack
119:31 - which you're working on the what do you
119:33 - say stack means I'll say in terms of
119:35 - environment which you're working on you
119:37 - also need to stay to the ziml that I'm
119:39 - going to use ml flow please register
119:42 - this experiment tracker okay and just
119:45 - like this as in stack you have
119:46 - orchestrator orchestrator means will
119:49 - will talk about it which eventually it's
119:50 - called as pipeline however you have
119:52 - artifacts right artifacts will talk
119:54 - about all of these terminologies in very
119:56 - great detail um however you don't need
119:58 - to know lot more you all have so much
120:00 - theor theoretical books but basically
120:03 - we'll first of all install our MSO
120:04 - integration once it is installed as one
120:06 - as you can see over here we can simply
120:08 - go and register our experiment tracker
120:11 - so once you go and register our
120:12 - experiment tracker you can just say
120:13 - zimal expand track register ml flow
120:15 - track but before that what I'll do I'll
120:17 - show you what the ziml stack
120:22 - list and it will show the set of stacks
120:25 - which we have over here right and uh
120:28 - it's taking too much time I guess
120:31 - yeah okay so basically this is a very
120:33 - common error which you might get if
120:35 - you're using Mac so you just have to do
120:37 - a couple of things the first thing we
120:38 - have to do is zenal disconnect and then
120:40 - what you have to do you have to run
120:42 - another command which is zml up so when
120:44 - you dis disconnect it so basically it is
120:46 - giving another error which is error
120:49 - initializing SQL store error
120:51 - initializing whatsoever this is
120:52 - something new error however you can
120:54 - totally choose to ignore this right just
120:57 - say ZL
121:02 - up ZL
121:07 - up okay cool something is really
121:10 - interesting over here so
121:13 - down so maybe down
121:15 - it or we might need okay fair enough so
121:19 - so then then we'll up it and if it gives
121:21 - the same error we have to r on the zml
121:23 - disconnect maybe and then let's see if
121:25 - it works
121:27 - zml
121:32 - disconnect
121:46 - okayl let's for wait for a few seconds
121:50 - and it should
121:52 - work okay cool it's working pretty fine
121:54 - that's very very nice so now we have we
121:56 - have fixed it so let's let's just quick
121:58 - quick quickly go and let let me show you
122:00 - what this set describe me so current
122:02 - stack which we have as of now the
122:04 - current stack which we have as of now it
122:05 - will say that everything is default
122:07 - right so your orchestrator is default
122:09 - and artifact store is default right
122:11 - orchestrator where you're running an
122:12 - artifact store where your variables you
122:15 - can assume the artifacts which are being
122:17 - stored over there so let let's quickly
122:19 - go and then also make the experiment
122:21 - tracker so you can just go with the read
122:22 - me and then copy this command and then
122:24 - paste it which is ziml experiment
122:26 - tracker register our ml flow tracker
122:28 - which will have the flavor of MLF flow
122:30 - so it says that unable to register
122:33 - mlflow tracker which is in the same
122:35 - Works Space so let's quick quickly go
122:37 - and change something customer okay so
122:40 - now it should work fine because I guess
122:43 - I've already used it somewhere cool then
122:45 - you have to just go and uh okay let's
122:48 - just quickly ignore model deployer as of
122:50 - now okay we'll come back to this or
122:52 - let's just do one thing let's just quick
122:54 - do do this as as a long because this is
122:56 - important to do
123:00 - so uh we will come back what does this
123:03 - model deployer means and then we'll
123:04 - register okay so I'll just customer and
123:08 - the over here as well as
123:10 - customer just copy it so that it makes
123:15 - sense and it will set the deployer so it
123:18 - will just set something this okay fair
123:20 - enough so it says that unable to
123:21 - register the stack name full stack so
123:24 - again it is saying the ml stack name is
123:25 - registered because I've already used it
123:27 - in the past so what I'll do I'll simply
123:30 - make it
123:31 - customer
123:33 - customer here let's just quickly do it
123:36 - and then just wait for a few seconds so
123:38 - most probably it's done so now you have
123:40 - the ml so when you do zml stack describe
123:43 - now so once you do it you most probably
123:46 - see your stack over there so which is
123:48 - different solve which is and now your
123:49 - model deployer is mlflow customer and XM
123:52 - tracker is ml tracker so we have done
123:54 - this so let's now run the pipeline and
123:57 - then let's see what is what what this
123:59 - leads to run pipeline. pi and let's just
124:02 - wait for a few seconds to to complete
124:04 - the Run of it so it is initiating a new
124:07 - run and it is saying something really
124:09 - happens you're using unsupported version
124:11 - if you cter errors blah blah blah you
124:13 - just have to downgrade or something
124:15 - upgrade ml flow or whatever it is giving
124:17 - you can totally choose to ignore this
124:20 - but there's something interesting comes
124:22 - in as we see it that there's something
124:24 - interesting comes in so me might need to
124:27 - um maybe most probably my case say that
124:32 - maybe just search it quickly because I'm
124:34 - not sure what this eror
124:37 - means I'm so sorry for
124:39 - it what do this given no
124:44 - okay
124:46 - uhuh Okay so so
124:54 - scorer maybe let's just go and upgrade
124:56 - it what do you think so it says the
124:59 - warning and let's the warning says
125:01 - that try upgrading and downgrading the
125:04 - scale learned version to a supported
125:06 - version or try upgrading your ml
125:10 - flow okay so pip install PP install
125:15 - psych okay
125:17 - upgrade
125:20 - py
125:24 - law okay it's
125:26 - done and then what I can do I can just
125:29 - quickly go and then serve it over
125:31 - here which is pick
125:36 - install
125:38 - upgrade
125:41 - flow okay so we might need to upgrade a
125:44 - little bit version of mlflow and let's
125:47 - see if this if the error is still until
125:48 - processes if it processed I'll just see
125:51 - the one another solution which I have in
125:52 - my mind so most probably your mosts is
125:55 - being fixed by you know just upgrading
125:58 - reinstalling you know disconnecting then
126:01 - connecting restarting your laptop fixes
126:03 - the erors because sometimes you you
126:05 - don't know what is happening behind the
126:07 - back so you actually have to be very
126:10 - careful while initiating
126:15 - stuffs
126:17 - please okay so that that was a very
126:20 - simple letter we might have to you know
126:23 - so we might have to you know upgrade
126:25 - then it run completely fine okay cool
126:27 - that's nice so let's let's just quick
126:29 - quickly go and search default
126:32 - login pipelines this
126:35 - one this
126:37 - one and here we go so if you go and see
126:41 - the configuration you have the
126:42 - experiment tracker as well you might
126:44 - think two things as of now the first
126:46 - thing which you which which you might be
126:48 - thinking that hey a how where can I find
126:52 - my um where can I find my you know uh
126:56 - most probably my ex tracking URI you
126:59 - know how can I view the MLF flow stuff
127:02 - and and my own stuffs right so let's
127:05 - I'll tell you two things okay the first
127:07 - thing which I'll tell you that how you
127:09 - can track the URI and stuff like how you
127:11 - can view the experiments so let let's
127:13 - Qui quickly go and search about uh how
127:17 - you can track the URI
127:25 - right so let's just quickly go
127:47 - cing
127:59 - so there was I'm just trying to search
128:01 - one thing which was they had given a
128:03 - very nice code actually you
128:07 - know okay we we got it we we got it okay
128:11 - so what you have to do you have to just
128:12 - go quickly over there and then just go
128:15 - quick quickly over there and in your run
128:17 - pipeline over there and just paste
128:25 - this okay just paste this because you
128:28 - will get your
128:33 - URI wait for a few seconds let it run we
128:36 - have initiated our cach so it'll just
128:39 - use the cach version of everything okay
128:41 - cool so it says that your file is
128:43 - available over here maybe yeah it makes
128:47 - sense okay cool so um so your file is
128:50 - available over there now what now what
128:53 - we will do we'll just run mlflow UI
128:56 - backend so something like this I'll show
128:58 - you which is which you can find on
129:00 - official you know ziml page as well M
129:03 - URI and then you paste the URI which you
129:06 - got by pasting that code which is ml
129:08 - runs right let's just do
129:17 - it
129:23 - okay there might be some error okay
129:25 - let's let's just quick quickly run it
129:26 - you know it's something very important
129:28 - to run I guess it will give some sort of
129:31 - er I'm not exactly sure okay got an
129:34 - expected argument which is maybe we have
129:39 - to make it in like
129:46 - this file and it last as well let let's
129:51 - just see if it works in if it works then
129:53 - it's
129:55 - fine all right it works so let's just go
130:00 - and paste it over there and expect it to
130:04 - run so this is 3 3 minutes ago you just
130:07 - go over there you see the Matrix which
130:09 - is listed out here right you see the
130:12 - parameters you see the model which is
130:14 - literally logged in ml model right you
130:17 - can use this model to make prediction
130:19 - make ml flow to make predictions or even
130:21 - pan to make predictions you see how
130:23 - interesting this is this is pretty
130:25 - pretty amazing this just loged each and
130:28 - everything so I that's what I wanted to
130:31 - Showcase to you uh I hope it makes sense
130:33 - to you not what I'll do I'll just cancel
130:35 - it up you can just all these commands
130:37 - are available you know
130:39 - get don't need to worry about
130:41 - it cool so now we done with experiment
130:44 - tracker in the next video we'll just go
130:46 - ahead and then worry about something
130:47 - known as
130:49 - um deployment of our model I hope that
130:51 - will also sound well to you K and catch
130:54 - you in the
131:01 - next hey everyone welcome back to this
131:03 - video in this video what exactly I'm
131:05 - going to do I'm going to actually cover
131:07 - the last of the last thing which is
131:09 - deployment pipeline so we'll use the ml
131:11 - deployer to actually deploy our model
131:13 - locally so that you can use it uh and
131:15 - and make predictions right and we'll see
131:17 - how to deploy a model you might have
131:19 - seen that you just save the model use
131:21 - some fast tape applications slow load
131:23 - with job lib and then you do it that's
131:25 - really not true which happens in the
131:26 - production use case you actually use
131:28 - something known as mlon deployment or
131:29 - Seldon deployment pipelines stuff to do
131:32 - it so what I'm going to do I'm going to
131:33 - actually use something known as uh some
131:36 - something really known as um ml4
131:37 - deployment which is entirely used for
131:41 - local deployment mostly use for local
131:43 - deployment for deployment on AWS or you
131:45 - know GC Cloud you might have to use S
131:48 - code because that's much more advanced
131:49 - deployment software but as of now let's
131:51 - just go with AML deployment software or
131:54 - sorry uh Tool uh to get started with
131:56 - that so the first thing which I'll do
131:58 - I'll go ahead and create something known
132:00 - as uh deployment pipeline okay a
132:04 - deployment pipe a deployment pipeline so
132:06 - let's let's just quick quickly go and uh
132:07 - make a deployment pipeline so what I'll
132:10 - do I'll just go ahead and then create uh
132:12 - deployment
132:16 - pipeline okay okay so over
132:21 - here I'll just scoll it down to looks
132:23 - good okay so run deployment run
132:27 - deployment dop right let let's just go
132:30 - there and then just first of all remove
132:32 - all of this because I you know the
132:33 - reason why I like to remove all of this
132:35 - because I think that gives me much more
132:37 - pleasure if I remove all of this because
132:40 - that seems like okay fair enough you
132:41 - have something uh off the load that's
132:44 - why I really really like this stuff okay
132:47 - cool so let's just go to run deployment
132:49 - and then in run deployment what I'll do
132:51 - so basically you might have already used
132:53 - uh click over there right so so
132:56 - basically we'll create two pipelines
132:57 - we'll create two pipelines the pipeline
132:59 - which we'll do so let's first of all
133:01 - create the pipeline as well which is
133:02 - deployment pipeline
133:04 - deployment uh
133:06 - pipeline Pi so in that deployment
133:09 - pipeline. Pi we'll make two pipelines
133:10 - which is continuous deployment pipeline
133:12 - I'll explain you what this continuous
133:13 - deployment pipeline means as well as
133:15 - inference pipeline later we'll explain
133:17 - what do it mean as of now let's just go
133:19 - in quickly as of now assume that
133:22 - continuous P pipeline it's like a
133:23 - traditional pipeline which we have built
133:25 - prior so let's just go from pipelines do
133:28 - deployment pipeline we we going to
133:30 - import some certain things so as of now
133:32 - let let's just go with this which
133:34 - is deployment Pipeline and inference
133:38 - pipeline okay so
133:41 - inference pipeline okay and then what
133:44 - I'll do I'll simply go ahead and create
133:46 - some do do one thing is I'm going to use
133:49 - click okay I'm I'm going to use click so
133:52 - that we can just state in a command that
133:53 - okay we want to deploy or we want to
133:55 - predict or whatsoever so I'll just go
133:58 - ahead and then create a click command
133:59 - which is click. command and click option
134:03 - click option would be click option would
134:06 - be uh so let me just quickly copy it
134:08 - because that's something is easy ra
134:11 - rather than I write whole set of thing
134:13 - so let me just copy it over here okay so
134:16 - click man is conf right config and then
134:19 - it will say okay in config what do you
134:21 - want to choose you want to choose deploy
134:23 - or you want to choose predict or you
134:25 - want to choose deploy
134:27 - predict so let me just quickly write
134:29 - over
134:30 - here deploy predict and deploy and
134:34 - predict okay so you can actually state
134:36 - in like this python run deployment pipe
134:39 - run deployment. py do SL sorry uh Das
134:43 - Dash config and then you want to deploy
134:45 - or predict you can simply write it over
134:47 - there and then you have minimum accuracy
134:49 - we'll come to come come to what this
134:51 - minimum accuracy means in some time so
134:54 - uh then what then what I'll do I'll
134:56 - create something known as this which is
134:58 - run deployment and that config is Str
135:01 - Str and minimum accuracy is of course
135:04 - number we we'll come to what this
135:05 - minimum accuracy means uh okay so we'll
135:08 - just go ahead and write float okay and
135:11 - over here what I'll do if it says deploy
135:14 - if it says deploy what I'll do I'll run
135:17 - the deploy Pipeline and if it says
135:19 - predict I'll run the inference pipeline
135:22 - I'll run the inference pipeline okay um
135:25 - so let's just quickly get started with
135:27 - it so now now you might be thinking hey
135:30 - is it done no of course not we have to
135:32 - actually build this deployment pipeline
135:34 - as well as the inference pipeline
135:35 - explain you what this minimum accuracy
135:37 - means so let's just go over there and
135:39 - quickly create our deployment pipeline
135:41 - right away so I'll just import import
135:45 - numai as NP import pandas as PD from zml
135:51 - import pipeline comma step and then from
135:56 - uh let's let's quickly import all of
135:58 - this from zl. config um you'll see where
136:01 - where we'll use this Docker settings
136:07 - Docker settings zl. config right that's
136:11 - correct okay so and then what I'll do I
136:13 - will just import some something which is
136:15 - mlflow deployer so I I'll just copy and
136:17 - paste all of this things so that it's
136:20 - much more easy as of now you can just
136:22 - forget it what does it mean and stuff we
136:24 - we'll come back to this later on so let
136:26 - me let me just quickly go and just copy
136:28 - and paste it over here so we have
136:30 - imported from zml con constants we'll
136:32 - actually make use of all of this please
136:34 - don't worry about it we we have also
136:36 - imported our um Steps From the steps
136:39 - which is clean data in that clean data
136:41 - we have imported clean DF so let's let
136:45 - let's just go and import clean DF then
136:47 - you have evaluation in evaluation you
136:50 - have evaluate model so let just go and
136:52 - write evaluate model inest data you have
136:55 - ingest DF so let's just go and import
136:59 - that as well and in model train you have
137:01 - train model which is already there okay
137:04 - so now what now what I do I'll just
137:06 - first of all so basically I want to
137:08 - train the model right sorry I want to
137:10 - deploy the model as well as if the model
137:13 - is good in accuracy we'll deploy it okay
137:16 - so let's and also we'll also create the
137:18 - docker setting so Docker setting is like
137:20 - what are the libraries or the tools
137:22 - which we need over here so in Docker
137:23 - setting the required Integrations which
137:25 - is the the the required Integrations
137:28 - which we
137:29 - have
137:31 - Integrations which is equals to uh only
137:34 - mlflow right so we we have we want to
137:36 - use only ml flow Library into this okay
137:40 - now what I'll do I'll create something
137:42 - known as I I want to actually use the
137:44 - model you know I want to use the model
137:46 - to
137:48 - um to actually deploy the to actually
137:51 - make the predictions but before that
137:53 - I'll I'll create the basic pipeline
137:54 - which is the continuous deployment
137:55 - pipeline so I'll explain you what this
137:57 - continuous deployment P pipeline means
137:58 - let's just goe first of
138:01 - all so
138:03 - pipeline comma we have to enable cache
138:06 - equals to true we want to enable the
138:09 - caching and then settings is equals to
138:13 - which is Docker first of all so we'll
138:15 - use Docker settings right stalker
138:17 - settings and then that's it that's
138:19 - that's pretty much it so we'll create a
138:21 - deployment pipeline so
138:23 - continuous I guess the spelling is
138:25 - correct
138:27 - continuous
138:29 - deployment Pipeline and then over there
138:33 - we'll have first of all minimum accuracy
138:35 - we'll come that what does minimum
138:36 - accuracy means uh then then after that
138:39 - um we'll have workers number of workers
138:42 - which we need and then we have timeout
138:45 - so timeout means how much like like what
138:48 - what will the required amount of if it
138:50 - is in Loop then at how at how much time
138:52 - we should stop the run right so when the
138:55 - time out should be there so so that's
138:57 - why we have imported this default
138:58 - service start which is from constant we
139:00 - have imported this defa default service
139:01 - start stop time time out so that we can
139:03 - actually stop the pipeline if it is
139:05 - taking too much okay so first of all
139:08 - let's just quick quickly go in and then
139:10 - just run the in justf we have actually
139:12 - imported over there and then we have
139:13 - xtrain so let's just quick quickly go
139:15 - over uh training pipeline and in trading
139:18 - pipeline let let's just import
139:19 - everything out Okay cool so let's I've
139:22 - imported everything which is we have the
139:23 - R2 score as well right so now um so you
139:27 - have the R2 score now what I'll do I'll
139:30 - create um what do you say the deployment
139:34 - uh the the deployment which is the
139:36 - deployment decision okay so now now we
139:39 - have once we have the rmsse now we have
139:40 - the trained model so now what we have to
139:42 - do we have to actually deploy the model
139:44 - so there should be some criteria for
139:46 - deploying our models what is that
139:48 - criteria criteria can be if your model
139:52 - is great if your model accuracy is
139:55 - greater than the minimum accuracy which
139:58 - which is required to deploy the model
140:00 - then only deploy the model that's where
140:02 - your minimum accuracy comes in place so
140:04 - let let's just quickly go and create
140:06 - something know as deployment trigger
140:08 - okay so the deployment decision will
140:09 - depend on this deployment trigger so
140:11 - first of all let's create a step called
140:13 - deployment decision okay so um so just
140:17 - quick quickly over there so I'll create
140:19 - a class and the class will have doc
140:22 - sorry
140:25 - deployment trigger config in that we
140:28 - have the base parameters right base
140:31 - parameters and then the minimum accuracy
140:34 - the the minimum accuracy of it we we'll
140:37 - change it don't worry we'll change it as
140:38 - if I'm just adding a random number we'll
140:40 - create a step and the step will say is
140:42 - the Define define
140:44 - deployment
140:46 - trigger
140:48 - trigger and that trigger first of all
140:51 - the
140:52 - accuracy uh which will be a float and
140:54 - config and the config from where we are
140:57 - using that the
140:59 - config so basically we need a config
141:02 - right so we need a
141:04 - config deployment
141:06 - trigger
141:08 - and so you have it's simp so basically
141:11 - what does it does let let me just write
141:13 - it implements a simple model deployment
141:17 - trigger that
141:19 - looks at
141:21 - the at the input model accuracy and
141:26 - decides if it is good enough to deploy
141:31 - or
141:33 - not okay so this is a very basic
141:36 - deployment trigger it says first of all
141:38 - it will return the accuracy greater than
141:41 - or equals to config do so basically so
141:44 - basically we we'll make use of the
141:45 - deployment out here so basically I'll
141:47 - I'll tell you what is so the we'll
141:49 - create a deployment decision deployment
141:51 - decision and deployment decision will
141:53 - contain the deployment trigger and let's
141:55 - use as of now rmse rather than R2 score
141:58 - so if if you don't know about R2 score
142:00 - you can just go online and search about
142:01 - it like space it's it's like it it it
142:04 - indicates like whe whether it's a
142:06 - goodness of it or not okay then then
142:09 - when you actually actually use it right
142:11 - so let's just go with something known as
142:14 - um MSE or rmse right or maybe let maybe
142:17 - let's let's go to R2 score because
142:19 - that's more good okay so now we have the
142:23 - deployment decision now so what it does
142:25 - it takes the R2 score and then it takes
142:28 - the which is the minimum accuracy which
142:29 - is required minimum R2 score which is
142:31 - 0.992 it only deploys this particular
142:34 - model if and only if if the deplo
142:37 - deployment decision true how does it
142:38 - evaluate first it checks that your R2
142:41 - score is greater than this or not if it
142:45 - is then when it deploys the model then
142:46 - it will go to the next step the next
142:48 - step is mlflow model deployer step so
142:52 - what is mlflow model deployer step so
142:54 - let's I'll I'll show you what does it
142:56 - mean mlow model deployer
142:59 - step which is over here so we actually
143:01 - import from Zen integration M steps we
143:03 - actually use this em model deployer
143:05 - steps which actually is that we is
143:07 - already pre-built step we can actually
143:09 - use that to deploy our model so that
143:11 - we'll have to give certain parameters so
143:13 - what is the model what is the deployment
143:15 - decision is the deployment decision
143:17 - workers which we need so workers is
143:19 - equals to workers timeout is equals to
143:23 - timeout Okay cool so now we have the
143:27 - deployment pipeline done now we can
143:29 - actually use this for inference so for
143:32 - for running our deployment pipeline okay
143:34 - now um so now I think we are mostly done
143:37 - with it right so let's just quickly go
143:40 - and run our deployment continuous
143:43 - deployment pipeline that's much more
143:45 - good to get started off with and then
143:47 - we'll come back to uh building up our
143:49 - inference pipeline so let's just quickly
143:51 - go to run deployment pipeline over there
143:54 - and um uh yeah so let's just quickly go
143:58 - over there and uh now what I'll do I
144:02 - will simply this is the config which we
144:05 - have right and this is the minimum
144:06 - accuracy which is required for us to
144:08 - deploy our model so we'll create
144:10 - something something known as
144:12 - mlflow model
144:15 - deployer component
144:17 - so this component will will be like em
144:19 - deployer so let me just quickly go ahe
144:21 - and then import with there and each and
144:23 - every libraries which we need
144:25 - technically yes so let's just import the
144:27 - libraries which is required I'll just
144:29 - paste it from my repository okay so
144:31 - basically we'll actually use the first
144:33 - one which
144:35 - is ml4 deployer which is ml4 deployer
144:38 - component and then this will ml4
144:39 - deployer get active model deployer this
144:42 - will take the active model deployer out
144:44 - there and then deploy is equals to
144:47 - config is equals to
144:49 - deploy uh deploy or conf config isal
144:55 - deploy this okay so if the deploy is
144:58 - this or this it will run the deploy if
145:01 - the predict is this or this it will run
145:02 - the
145:04 - protect cool so let me just quickly
145:06 - import
145:08 - my from pipelines import continuous
145:12 - continuous deployment pipeline that
145:15 - continuous deployment pipeline will be
145:17 - the following so let's just take that
145:19 - and then continuous deployment pipeline
145:21 - that deployment pipeline will contain
145:23 - the minimum accuracy which is required
145:26 - and then after that it will have
145:29 - following workers which let's may name
145:31 - it as a three and time out maybe 60
145:35 - seconds okay so this is our uh
145:39 - continuous deployment pip plane so now
145:41 - what what we can do we can actually you
145:43 - know um use it so let's let's quickly
145:47 - let's let me let me just copy it from
145:49 - the uh repository so now we we'll have
145:52 - the we'll make the predict one very soon
145:54 - but but let's just write it out so we
145:56 - can say that you can run your so this is
145:59 - the thing which which I've copied from
146:02 - um Zen M repository so it says you can
146:05 - run your ml for UI it Tak the so so you
146:08 - can see the visual representation of
146:09 - your models right and then what it then
146:12 - then what it does it Fetch and then we
146:14 - have to fetch the existing services with
146:16 - the same pipeline step name and model
146:18 - name right so that we can say that if
146:21 - there is any existing services are there
146:22 - so I've just copied it from their uh own
146:25 - mlflow examples repository because these
146:27 - are mostly same so basically we are
146:29 - fetching the existing Services if it is
146:31 - running if there's an existing Services
146:33 - running or not right and then we have
146:35 - employ deployer step and then model
146:37 - which is the model name if the existing
146:39 - services are there then then we say that
146:41 - the existing Services is running locally
146:43 - as a Derman to stop the service it will
146:45 - like this and if the service is failed
146:47 - then it says the ml for service is
146:49 - failed or you just say there's no ml for
146:52 - prodiction server is running right so
146:54 - there just use the deploy model to get
146:55 - started with so this is a very basic run
146:58 - deployment
147:00 - um as we don't have the inference
147:02 - pipeline so we don't need to worry too
147:04 - much about uh so let's just quickly go
147:06 - and run this pipeline up right and then
147:08 - we are and then we are mostly done so
147:10 - let's run this Pipeline and we have the
147:13 - minimum accuracy right so we don't need
147:15 - to really worry about stuff
147:19 - Okay cool so let's go and run python run
147:25 - deployment uh config and in that config
147:28 - going to deploy the
147:29 - model let's see if it gives any error if
147:32 - it gives then we'll solve it right away
147:36 - so it says that
147:37 - materializer is not found like material
147:40 - there's no module named must
147:42 - materializer where in in deployment
147:44 - pipeline so are we using the
147:45 - materializer
147:47 - I guess
147:48 - yeah okay let's remove this we not using
147:51 - it you don't need to worry about it just
147:53 - go
147:56 - ahead let's solve the error which it is
147:59 - going to
148:01 - give okay it says that invalid settings
148:04 - can either refer to this this this
148:06 - invalid setting doger settings settings
148:09 - can be refer to the general settings or
148:11 - stack component there there might be
148:13 - some error interesting error so let
148:15 - let's see first of all if where it is
148:17 - giving eror okay it is giving in
148:19 - deployment Pipeline and in deployment
148:20 - pipeline what is it giving it's that's
148:24 - that's why I say that most of our time
148:26 - will go into this only just by solving
148:28 - these pretty
148:32 - errors
148:33 - so where it is
148:38 - bro okay fair enough so we actually have
148:41 - to write settings to be Docker not
148:42 - Docker
148:44 - settings right and mostly we done okay
148:47 - fair
148:52 - enough okay cool let's run it
148:56 - now it can be available keys are either
148:59 - resources or Docker so we need to have
149:00 - the docker one over there rather than
149:02 - Docker settings okay fair enough so it
149:04 - gives the main why it gives the main so
149:06 - we have run deployment rather than main
149:09 - so let's run that sorry for
149:13 - that let's
149:15 - wait
149:17 - please
149:19 - run
149:20 - whoops so it says confix so basically I
149:24 - there is some naming error I'm so sorry
149:26 - for that again no problem you know these
149:29 - things you know very silly errors which
149:31 - I do you know this rectifies this I
149:34 - really want a tool that rectifies us all
149:36 - of this naming errors or you know import
149:38 - errors and all the stuff okay it says
149:41 - that wrong arguments emo deployer got an
149:44 - unexpected argument called deployment
149:47 - decision what is it so where does it
149:49 - gets an error it gets in done deployment
149:51 - and then it says okay fair enough so it
149:52 - says in continuous in that continuous
149:55 - you have the ml flow deployment decision
149:58 - and then it says that okay fair enough
149:59 - so let's let me just quick quickly go
150:01 - okay so basically it's actually not
150:03 - deployment
150:05 - decision it's deploy decision not
150:07 - deployment decision
150:10 - Okay Okay cool so let's just
150:15 - run
150:18 - wait I hope it works diplom decision is
150:22 - not defined so why did okay fair enough
150:24 - again I did the big mistake it should be
150:26 - over here not
150:29 - there I'm so sorry for it for these
150:31 - mistakes because these is this is
150:33 - something you know when when things are
150:34 - super occupied when you know things out
150:36 - there these mistakes happens so it
150:40 - initiates the new run it says missing
150:42 - entry point input data path so let's
150:45 - input our data path where to input the
150:48 - data path okay I'll I I I'll add it over
150:51 - here only because I guess that's more
150:56 - important right or let's do one thing
150:58 - let's write over here
151:02 - data
151:04 - path St Str and then let's just go over
151:07 - there and then also add the data
151:11 - path just copy it from directly over
151:15 - here
151:27 - I just quickly replace it okay I hope it
151:29 - works now if it does not then we again
151:32 - have to fix
151:34 - something inference is still left so
151:37 - stay tuned for inference and mostly
151:39 - we'll be done by
151:44 - then missing empty Point data path why
151:48 - where it is
151:51 - getting I guess okay okay okay again we
151:54 - made a good error so we have to actually
151:55 - put the data path to be data path sorry
151:59 - sorry for
152:04 - that this little little erors keeps
152:06 - keeps on happening so you have to
152:08 - actually debug it and see where the your
152:10 - VAR code is running and stuff it is
152:12 - using the cached version again I'm
152:15 - saying it's like ml
152:19 - deployer okay so it says that an mlflow
152:21 - model with the name was not logged in
152:23 - the current Pipeline and no running ml4
152:25 - server was found please ensure the
152:27 - pipeline includes a step with the ml4
152:29 - experiment to configure that trains a
152:31 - model and locks it to that so most
152:34 - probably what I feel that we have to
152:37 - make it false right and then let's run
152:40 - it now if it does not then you know then
152:43 - then we then then we'll get in a big
152:44 - trouble now if it does not deploys the
152:54 - model we are going to get in big trouble
152:57 - if it is
153:05 - not
153:15 - please
153:29 - let's
153:30 - wait know this is something where I just
153:33 - pray you know that it works out because
153:37 - this is a step where you get most of the
153:39 - errors and if some unknown error happens
153:42 - then just you have to actually spend
153:44 - your ton of time in it ton of people has
153:47 - to spend your time in it you know uh
153:49 - because it's not a very simple thing
153:51 - actually go inside your system and see
153:53 - if it works or not now see what is
153:56 - happening it says no materializer is
153:58 - registered for type linear regression so
154:00 - default pickle materializer was used
154:02 - it's not production easy so not we can
154:04 - blah blah blah so basically we have to
154:07 - actually make a materializer I'll show
154:09 - you how to make it later
154:11 - on I'm just waiting for it to deploy our
154:15 - model
154:16 - mlflow model deployer step and if it is
154:20 - and if it goes
154:21 - above service demo on is not
154:24 - running okay something really happened
154:26 - now so it says the fail to start the MSO
154:29 - deployment service model serving ml
154:34 - flow uhu okay for more information on
154:38 - the status please see the logging
154:41 - file
154:44 - okay something really interesting happen
154:46 - so basically timed out happened over
154:49 - there okay let's just go and see if what
154:52 - we can do in this
154:53 - case so what we can do
154:57 - is fail to start the
155:00 - service ml flow
155:03 - deployment service demon is not
155:08 - running okay so zml
155:15 - up
155:21 - NL stack
155:23 - describe is there anything which we did
155:42 - wrong in the deploy we have the the
155:46 - following why is giving the worst
155:51 - error okay fair enough so something is
155:53 - really interesting H happening over here
155:56 - we actually have to run
156:14 - deployment okay let's try to run it if
156:17 - you see it it like
156:44 - this
156:49 - this might be causing some problem this
156:52 - this
156:56 - warning okay okay okay skipping model
156:58 - deployment because the model quality
157:00 - does not match the criteria so again it
157:04 - will say the same thing so basically
157:05 - what's really happening that it is not
157:07 - matching the criteria so let's let's
157:09 - write
157:12 - 0.5 this is that skipping model because
157:15 - the model called does does not match the
157:16 - criteria using last s deployed by step
157:19 - and continuous for model so I guess
157:21 - that's that wasn't like it it was not
157:23 - meeting the criteria maybe that's why
157:26 - yeah so let's try to I have reduced my
157:28 - minimum accuracy or maybe I have not so
157:33 - minimum
157:34 - accuracy
157:43 - 0.5 let's wait now we just we can do
157:46 - just one thing just wait for it and see
157:48 - if it works so basically I'll tell you
157:49 - what happens is in these type of cases
157:52 - you have actually concentrate more you
157:54 - know you have to see actually what's
157:55 - going wrong what might go wrong even the
157:58 - smallest thing even restarting your
157:59 - laptop really works I literally seen a I
158:02 - was soling an error for two days and I
158:04 - saw okay fair enough like I restarted
158:07 - the laptop and it works like a charm so
158:10 - yeah so just just just wait for a few
158:12 - seconds and let's see if it works or not
158:14 - deployment trigger start ml deployment
158:16 - service skipping model because the model
158:18 - does not meet the criteria my goodness
158:21 - so it's really interesting things that
158:23 - happening out here right so uh what I'll
158:26 - do I'll just quickly go over through the
158:28 - code and let's see if it works and let's
158:29 - see if this let let it work right and
158:32 - then we'll just go and see what happens
158:34 - over there okay so we have the
158:38 - deployment existing
158:41 - services and uh okay so it will of
158:46 - course not work because this does not
158:47 - Tak too much of time right so I'll just
158:49 - you know go ahead and then see if it
158:51 - works on your
158:53 - side ml4 deployment
158:56 - service and okay and then you just go
159:01 - into deployment pipeline in that
159:05 - deployment pipeline you
159:08 - have
159:09 - [Music]
159:10 - the get data
159:14 - for ML slow deployment service parameter
159:19 - steps service demo is not working fair
159:22 - enough I I I get the words the error is
159:28 - about I get
159:30 - it what's the error is
159:33 - about step
159:41 - parameters if your accuracy is greater
159:45 - than then config do minimum
159:49 - accuracy and your minimum accuracy is
159:53 - this one
159:55 - 0.5 okay fair and then you have the MLF
160:00 - flow model load step parameters so
160:03 - basically we
160:05 - have pipeline step name
160:08 - running okay so let's just copy this m
160:10 - deployment loader steps as
160:14 - well
160:17 - so what it does it helps you to get the
160:19 - get all the stuff which is ml deployment
160:21 - and then we have the prediction service
160:23 - loader and predictor which will come to
160:25 - in some details you know I've already
160:27 - written this these code out actually
160:29 - right so let's just go and then just you
160:32 - know okay fair enough so I'll try I'll
160:35 - try to run one more time maybe right so
160:38 - I'll try to run one more time and then
160:40 - see if it works but before that what'll
160:42 - do I'll just check the everything is
160:44 - working fine on that site which is run
160:47 - deployment and in that run deployment
160:50 - you have the
160:51 - following ml flow deployment services
160:55 - and then you have the to get which is
160:58 - Zen model deployers MSO model deployer
161:01 - do we have the Emon model deployed okay
161:02 - we have the get tracking U and then okay
161:05 - fair enough so I'll just run it try it
161:06 - nicely again let's see if it works or
161:09 - not if it does not then we actually have
161:11 - to go inside and talk to zenel team and
161:13 - then see if it works because you know we
161:15 - have to actually have the continuous
161:17 - talks to the zml you know because
161:18 - because it's something you know some
161:20 - something which should be which maybe
161:22 - have which may be bit common in their
161:24 - side and which may have they may have
161:25 - solution to it or we may have to open
161:27 - the GitHub issues and then may most
161:29 - probably will have the error solved
161:32 - because this is how we solve it it's
161:34 - just we we are not expert in this we
161:35 - just go to some people and then talk to
161:37 - them about it right we'll try we'll try
161:39 - one one more solution which is this one
161:41 - we'll try do this solution which is the
161:43 - L linear regression model okay fair
161:46 - enough again it just just does not
161:48 - matches the deployment
161:51 - criteria so I want to see the R2 score
161:54 - R2 score is so bad bro okay fair that's
161:58 - why it was not given okay okay okay so I
162:02 - guess R2 score is very bad that's why it
162:05 - is not giving good errors so I'll just
162:07 - add
162:10 - zero maybe this this this might
162:13 - run zero means like we I I want to just
162:17 - showcase you that that it deploys the
162:20 - model right so now I'll go and run it R2
162:23 - score is zero right so of course the R2
162:26 - score is greater than that so
162:28 - yeah why it is zero bro something really
162:31 - interesting cases happening of with me
162:33 - really
162:34 - interesting Okay cool
162:37 - so it's weet and also we'll fix that um
162:41 - which is this one we'll fix that if
162:44 - these things does not Works
162:48 - bet please run bro
162:55 - run no materializer is registered that
162:58 - that that that that that is
163:01 - common deployer updating an existing
163:04 - mblo deployment
163:06 - service which is this
163:09 - one and let's see if it this works if it
163:11 - does not which is like it met the
163:14 - criteria now it met the
163:15 - area now let's see what it
163:19 - does it should work you
163:22 - know but if it does not we'll come back
163:25 - and then see what it works or not
163:28 - okay let's see if it works or
163:32 - not updating an existing ml flow
163:35 - deployment service at this this this
163:38 - this stage I think it will mostly not
163:41 - work because this does not takes this
163:44 - much time
163:47 - it will say that a demon is not working
163:50 - blah blah
163:55 - blah we might need to do
163:58 - something we'll try one one more
164:00 - solution which I have in my
164:04 - mind okay service dayon is not running
164:08 - uh for more information please see the
164:09 - following lock file so I'll just check
164:12 - it out and then come back very soon so
164:15 - everyone there was a very simple error
164:18 - so basically my I've already tried this
164:21 - mlro on couple of environments that's
164:24 - why it was like service that we have the
164:26 - current service running we cannot
164:28 - actually use that so what I done I
164:30 - actually deleted that uh then I you if
164:32 - you were working on a new the new uh you
164:35 - might be working on new stuff right you
164:37 - might working on new stack right so we
164:39 - have to actually use the new stack
164:41 - that's why it was giving me error now it
164:43 - is working to totally fine the only
164:44 - thing which is is not working fine is
164:46 - the following so let's just go and uh
164:48 - fix that thing so basically what what we
164:50 - need to do we need to import something
164:52 - as cast and let let me go and just
164:55 - import that from typing import
164:57 - cast
165:00 - from typing import cast okay fair enough
165:05 - so we'll just run it deploy and then
165:09 - let's just see if it
165:12 - works you can totally choose to ignore
165:14 - the mornings and
165:17 - stuff or you can just go and solve that
165:19 - thing if you want so it ingested data
165:23 - first after it is ingesting the data it
165:26 - cleans the data data cleaning is
165:28 - completed it goes to the next step which
165:30 - is trains the model trains the model
165:32 - then gives some sort of warnings you can
165:34 - totally choose to ignore this or maybe
165:36 - see if it works if model training is
165:38 - completed train model has finished and
165:40 - it gives some you know root squares and
165:41 - then deployment trigger has started
165:42 - deployment enow models step has started
165:45 - it updates an existing ml development
165:47 - services right it starts with the ml
165:50 - development services and most latest
165:51 - times right it starts the service so
165:54 - let's just go in then see if it
165:57 - works hopefully it should work if it
165:59 - does not then I'll just you know take
166:02 - his ass off okay I'm so sorry for that
166:05 - okay so now your model is available you
166:07 - can make the pred make your prediction
166:08 - over here because it is already running
166:10 - you can also delete the model if you
166:12 - want so now your model is successfully
166:15 - deploy it so now we need to do we need
166:17 - to actually make predictions from this
166:18 - model so I what I'll do I'll quickly go
166:20 - ahead and then create something known as
166:22 - um I'm so sorry for
166:26 - it go to deployment pipeline in that
166:30 - deployment pipeline first of all we have
166:32 - this MSO deployment loer step which will
166:34 - help us to load that model okay and then
166:37 - let's go and then start doing stuffs so
166:41 - uh we'll just go ahead and then create
166:43 - something define prediction service
166:45 - loader right so I'll just copy and paste
166:47 - the code if you want but yeah but okay
166:49 - it's like I already this is already
166:51 - pre-written Okay so let's just go and
166:53 - then write the prediction service load
166:56 - out so we'll create a step where we'll
166:57 - enable the caching equals to false
167:00 - because sometimes caching is also not
167:01 - very good then we'll create the
167:03 - prediction
167:05 - service service
167:09 - lower and in that we'll have the
167:11 - pipeline
167:13 - name pipeline name name will be St Str
167:16 - pipeline step name pipeline step name
167:19 - will be also also St Str is it
167:22 - running
167:24 - boom equals to true and then model name
167:27 - to be model okay and then it Returns
167:30 - what it returns it returns
167:33 - mlflow deployment service okay it
167:36 - Returns the ml flow deployment service
167:37 - so basically it gets the prediction
167:39 - service started by it it gets the
167:40 - prediction service over here just copy
167:42 - and paste it yeah so it gets the
167:44 - prediction service started by the
167:45 - deployment it takes all of these
167:46 - arguments in it so first of all get the
167:48 - ml flow deployer stack component so
167:50 - basically we'll get the mlflow
167:53 - deployer stack
167:55 - component so which is very simple get
167:58 - active model deployer over here and then
168:01 - what I'll do I'll existing fetch
168:03 - existing services with the same pipeline
168:05 - name and model name so what I'll do I'll
168:07 - go existing Services which is mlso MSO
168:10 - deployer which is MSO model deployer
168:11 - component. find model server and in that
168:14 - we
168:15 - pip pipeline name to pipeline name pip
168:18 - pipeline step name model name and
168:20 - running right so if there is
168:22 - running
168:25 - running running to be running that's it
168:29 - that's pretty much it so if not existing
168:33 - Services then we say we raas the runtime
168:36 - error and in that runtime error we say
168:39 - that no MSO Services is
168:42 - found which is like this
168:45 - no step in this Pipeline and something
168:49 - like this you know pipeline for the
168:51 - model name is not deployed so I just
168:53 - copy and paste the errors which is so
168:55 - traditional errors found from you can
168:57 - just just go at you know zenel examples
168:59 - and then just copy and paste there it's
169:00 - not a big thing then you print the
169:02 - existing services or maybe it's not then
169:05 - then then you return what you return you
169:08 - know you return existing services so you
169:12 - return the services by using so it is
169:14 - actually prediction service loader it
169:15 - loads the uh current prediction service
169:18 - so to actually use this for model
169:19 - predictions now what I'll do I'll create
169:21 - the predictor so I'll create the
169:23 - predictor the
169:25 - predictor will have the service that
169:28 - service would be the mlflow deployment
169:31 - service type of that and then it then
169:33 - then then it takes the NP Dot and the
169:36 - array right and it returns those array
169:39 - of predictions and b. ND array so what
169:41 - is Arrow of array of predictions so
169:44 - we'll first of will create a step over
169:45 - here as well that step will be of
169:47 - dynamic data importer so that will
169:49 - create the step of enable
169:53 - cache equals false then we'll create a
169:56 - dynamic
170:00 - importer that returns s Str that returns
170:04 - uh string right so it downloads the data
170:08 - from the first of all downloads the data
170:11 - from a mock API or maybe just just just
170:13 - go ahead and create data is equals to
170:16 - get data for
170:20 - test and it on data so we have to
170:23 - actually build this quickly so let's
170:24 - just go and quick quickly build this
170:26 - which is
170:29 - utils utils p let's go there and run it
170:35 - so we'll just have import
170:38 - login import andas SPD from source.
170:43 - model to sorry data cleaning input data
170:47 - cleaning and
170:50 - data process
170:54 - strategy pre-process strategy and then
170:57 - we'll just use this much get data for
171:01 - test where we first of all get the data
171:03 - for the test and then we want the 100
171:05 - one we actually clean the data we drop
171:07 - the review score we convert into date
171:09 - Json format that's why it is the it is
171:12 - returning St Str okay now what I'll do
171:15 - I'll simply go ahead and
171:19 - uh make this so let's just go quick
171:22 - quickly make this so I've already made
171:24 - this let me just copy and paste that
171:27 - because this is pretty simple to
171:30 - understand okay so first of all it
171:32 - starts the service um and then it loads
171:35 - the data it removes some of the column
171:37 - it for the columns which we want from
171:39 - the data we create we convert into P
171:41 - data frame we convert in list and then
171:42 - we finally convert that Json list to a
171:44 - nump array and then we make the
171:46 - prediction from that
171:50 - service
171:52 - okay I hope it makes sense
171:54 - now okay fair enough so let's just go
171:58 - and run it
171:59 - now so we mostly done now what now we
172:03 - have the prediction service loader now
172:04 - at last we'll create the inference
172:07 - pipeline so that inference pipeline will
172:10 - have okay sorry pipeline will enable the
172:14 - C setting the docker then then I'll
172:16 - create the inference pipeline that
172:17 - inference pipeline will contain the
172:18 - pipeline name which we want and the
172:21 - pipeline step
172:23 - name right which is Str Str so it first
172:26 - of all uses the dynamic importer Dynamic
172:29 - importer right and then it service which
172:31 - is the production service loader it
172:32 - gives the pipeline and running equals to
172:34 - false let's just write running equals to
172:36 - false as of now and the new prodution
172:37 - production should be predictor is this
172:40 - is service equals to service and then
172:42 - data like this and then we mostly down
172:44 - okay so we have the data over there and
172:46 - then we have the service over there now
172:48 - we give the service it uses that service
172:50 - from the from this and then make
172:51 - predictions on this data using predictor
172:53 - right which is like predictor you might
172:55 - have seen over here okay cool so now we
172:57 - we mostly done let's just go to run
172:59 - deployment and then run the pipelines so
173:01 - let's just go to run deployment and then
173:03 - we'll just import our inference pipeline
173:04 - out there so I just go and import
173:07 - inference Pipeline and once we import
173:09 - the inference pipeline we'll just go
173:11 - ahead and uh run our
173:15 - inference pipeline so
173:19 - yeah the
173:21 - pipeline the pipeline names should be
173:24 - continuous deployment pipeline right and
173:26 - then your pipeline step name which is
173:28 - mem deployer
173:29 - step I guess it should work now most
173:33 - probably okay so now we have done that
173:35 - so let's just run the
173:38 - predict it's so
173:43 - tiring
173:48 - please fix this error I want you all to
173:50 - fix this error this is very basic you
173:51 - just have to write ZL downgrade fair
173:53 - enough so we get the error nice nice
173:56 - nice that that is
173:58 - expected
174:00 - from uril import get data for
174:07 - test I'm very happy that uh that you so
174:11 - one thing which I'll tell you that it is
174:13 - very like that you sometimes don't
174:15 - understand it right because this is
174:17 - something very conceptual very technical
174:20 - things out there so I want you to be
174:22 - very strategic in understanding stuffs
174:25 - right so please be if if you're not able
174:27 - to understand it that's totally all
174:30 - right so it says that data process
174:33 - strategy is not
174:37 - there
174:40 - okay pre-process strategy is not defined
174:43 - what does it mean
174:48 - name the pr process
174:54 - strategy
175:06 - mhm if it is game gives th you have to
175:09 - worry about it this is something really
175:12 - interesting what happened
175:14 - I really hope that it does not give that
175:20 - now
175:22 - Yahoo okay so buil-in materializer can M
175:25 - write unable to handle class numpy and
175:28 - the array uh buil-in materializer can
175:30 - only handle the artifacts of the
175:33 - following so it gives some matter let
175:36 - let's go and fix this up this is so
175:41 - tiring um okay
175:44 - let's go to
175:47 - where built in in the predictor okay
175:51 - where is predictor in
175:54 - deployment it gives in
175:56 - predictor andd
176:06 - array
176:08 - array sorry this was St Str by num so
176:13 - basically the the data which you're
176:14 - getting is Str Str not an
176:20 - and see if it works if it is not then we
176:24 - have to worry about
176:29 - now uhuh
176:33 - okay Json is not
176:35 - defined import
176:42 - Json anything else
176:44 - please give me the error Fast Pro I'm
176:46 - I'm I'm so worried about
176:49 - errors fix this up so just say just say
176:51 - it's animal down downgrade it it should
176:58 - fix please yeah so we are done so now we
177:02 - actually completed our stuff right so
177:05 - let's just go over
177:08 - here enjoy so you have the dynamic
177:11 - importer prediction service load of
177:12 - dynamic import outputs and then service
177:14 - load output something which is a service
177:16 - and then this uses output and output
177:18 - output of the which is the data for test
177:20 - and this for service and then uses both
177:22 - of them to make the predictor and this
177:23 - actually the predictor outputs the
177:25 - following so if you go and see the
177:27 - visualization you see something really
177:28 - interesting that your predictions has
177:30 - been made over there so I guess this is
177:33 - not showing any V visualization because
177:35 - of some my error so you see that your
177:37 - mean is standard deviation of
177:39 - predictions is this right so actually it
177:41 - made the predictions out there okay I
177:45 - hope it makes sense now we have deployed
177:46 - the model made the predictions too now
177:49 - you might be thinking here how can I
177:50 - make the single handle had single handle
177:53 - predictions right so we are done with
177:55 - the deployment and inference as well now
177:57 - it is actually making good uh inference
178:00 - out there it is actually predicting but
178:02 - it might happen that you're still
178:03 - confused so let's just not have too much
178:05 - of confusion in your head and then fix
178:08 - that confusion too so I have actually
178:10 - made um simple streamlet
178:17 - dot send it app dop appy
178:22 - okay paste
178:24 - it right right I hope it works mostly so
178:28 - you from the deployment pipeline it
178:30 - Imports the prediction service loader
178:32 - and then run
178:33 - deployment from run deployment it
178:36 - Imports let's make it main rather than R
178:42 - right Main
178:44 - and then over here as
178:47 - main okay so everything is same the only
178:50 - thing which is if the sub person clicks
178:52 - the predict button it will go to the
178:54 - prediction service gives this and then
178:56 - it says that if the post service is
178:57 - there then creates the DAT data frame
178:59 - does the same step which which we have
179:00 - data and predictor and predict from the
179:02 - data right so let's run the
179:05 - streamlit Run streamlet
179:12 - app.py please
179:20 - run fair enough high level overview is
179:24 - not there so I'll just make sure that I
179:26 - remove all the
179:32 - images
179:35 - okay any
179:42 - images M
179:49 - okay let's run it
179:58 - [Music]
180:00 - now let's wait is it is
180:03 - running cool so I'll just make it z z
180:07 - everything so it's now it giv prediction
180:09 - so basically your detail is 4.22 so
180:11 - basically it is actually using the
180:13 - production from the model so you see
180:15 - that we haven't even saved the model
180:17 - save saved the model is not there it's
180:18 - actually using it it's actually the
180:20 - pipelines over there if you go and see
180:22 - the pipelines of our so let's go to
180:24 - pipelines and then let's go to
180:25 - continuous deployment pipeline The
180:27 - Continuous deployment pipeline you have
180:28 - the following inference pipeline which
180:30 - is done right and then let's go
180:33 - behind back okay let's just go
180:35 - continuous deployment continuous
180:37 - deployment continuous deployment and in
180:39 - that continuous deployment you see the
180:41 - continuous deployment is also there
180:43 - right so whole P pipeline is done so we
180:45 - are done with one project right I hope
180:48 - it was really good project for you uh I
180:51 - understand it cool so let's just go
180:53 - ahead and then that's it that's the app
180:55 - of the project in the next project we'll
180:57 - actually use something known as customer
180:58 - CH or maybe let's cover the next project
181:00 - up to catch you later bye-bye

Cleaned transcript:

mlops short for machine learning operations refers to the practice of applying devops principles to machine learning this mlops course will guide you through an endtoend mlops project covering everything from data ingestion to deployment using stateoftheart tools like Zen ml ml flow and various ml libraries IU sing developed this course he has created many popular machine learning courses on our Channel this course will teach you fundamentals of emops as well as we be having one end to end project which will involve from data ingestion to deployment using several stateoftheart tools like MLF flow zenel and Etc mlops is completely new field and there are very less resources around it and this is a gold mine and a game changer in the ml Community if you do this with the dedication and the patience you will be able to succeed in learning about mlops and then you will be also getting the international Pace or whatsoever job offer which you want in your hands but but wait who am I I'm lead data scientist atate I've L several products in crea's economy along with it I have worked as a emops engineer as one of the fastest growing emop streamwork which is zml and have experienced in working as a data scientist at artifact and building large scale NLP products before even GPT was launched and hopefully by all of this experience I'm the right guy to teach you about mlrs all the required links and everything resources is listed down in the description on boox below you can go ahead and check that out hey everyone welcome to the another lecture on mlops we'll be starting off with giving you a slight introduction to mlops also I'll make you aware with the terminologies of mlops which is very important for you to understand the later content of this course as well as we'll also make sure that you understand the basic things like pipeline steps which is a library which we'll be using out there but before that we'll make sure to that you understand why there's a need of mlops what like what are the stages and envelops and etc etc so let's get started with this uh lecture so the first thing first so the first thing first out is um about t the the growth of data is increasing there's exponential growth of data and the importance of artificial intelligence is also has also increased over time now the data has increased but now we need to make sure that we utilize that data in a right way and in a positive way right so that's where it come that that that's where we we have artificial intelligence right so now you might be thinking that okay fair enough we can just build a prediction model on top of it but you should understand that machine learning is not just about building model why I say this why I say this because your ml code or whatever Machinery model is just 20% of your whole machine learning project or a whole um business problem right there's a lot of things which comes into that place and your machine learning code is just 20% out of the whole set of things so I hope you will I'll prove you why why there's a 20% of ml code throughout this video and it's ml in the industry is more than the training models it is validated by chip Huen who is one of the ex experts in mlops and it is also validated by Elon Musk who just said yeah it's like machine learning engineering is just 10% machine learning and 90% engineering and that's something really interesting to worry about and you might be thinking that every other courses online teaches only about machine learning engineering which is a building machine learning models but nobody teaches about the engineering part of it right and you might be thinking it might be some data structures on algorithms or it might be some design patterns or Etc of course yes it has some factors but there are lot more than these DSA and stuff which will explore throughout the course through our project so we uh in a typical ml team uh in our corporate we have the following uh people who are actually responsible for doing x amount of tasks so your data scientists discover the raw data develop features and train models right and data engineer who productionize the data pipeline we'll talk about the term uh productionize in a Bild but data pipeline is like where the data is coming from and then making it on a large scale right and then we have a ml engineer who sits on front to deploy the model right so that it can be used by users by you we'll talk about what does deployment mean in just some seconds and then we integrate the service into into your website or application and then you have to monitor it we'll talk about each and every steps in grade detail and then you have a lawyer who who can just ask you we we should ask question to them can I use this data for my model yes or no and and I'm pretty much sure that you might not be aware with any of the any of the red line over here I'll make sure that you understand each of the things like training models productionizing deployment integrating monitoring and all the stuff throughout this int introductory lecture the reason why I'm doing this introductory lecture to make sure that you understand each and every bit in the project which will use which will make use of like terminologies which will make use over there so what data science actually sees you might be thinking about okay fair enough you have pd. read CSV you read the data you fit and then some some happens and then you simply uh and then you also have the classifier you also fit that and then you do predict and do score right that's what you see right but do you really think that uh by writing three these three lines of code people will get your job of course not right and the main focus 90% focus should be on engineering and what engineering sees is much more uh very scary than what data science is so ml in production you might if you're a bit aware even about how does it goes Etc the first step is you collect the data you train the model and then deploy the model introduction what does deployment mean so let's talk about a little bit about deployment so that you understand it a bit however I recommend if you want to understand deployment much more in great detail we'll have more sections on afterwards to actually understand what does deployment mean over here so deployment means that you once you once you have the trained model for example let's take an example you're you're working on a email spam detection project right and the model is currently is in a local server right is in a machine how can you use that and integrate into that Gmail right so that that we can use that model to make predictions for the users who who are who are whom for for whom we are making this model for right and that's where deployment comes in right deployment is about that you have to make your local model available to the lot of people to the users for which you're building the model for right that's what deployment means you to deploy the model online and and we'll see we'll talk about on deployment in very great detail in some time but you might be thinking that this is the process but what ex actually it looks like so basically what happens that first of all you collect the data you train the model and then you deploy the model now once your model is deployed you again go back and collect in the data and then training the model and this Loops goes in environment but how does you how how can we say how can we say that that okay the what's the loop is about you might be having several questions what is the loop what is the production environment and lot of things out there so let's talk about in great detail about what does the loop means and what is the production environment is so I'll take a very very simple example of uh this image right so for assume that you have you had the collected the data and then you had to trained the model and then you deployed the code right so assume that assume that it is deployed in production your spam production St spam production system and is being used right now what happens that you changed your model you changed your machine learning algorithm from logistic ration to n base right you changed your ml algorithm right so you have to you have to retrade go back to the model and then and then whatever is change you have to again deploy that changed model which is update the model right which is one case is different model needed or ml algorithm changes right another another point is for example you're building some span reduction project you might have trained it on a on a on a data set which which is which which might be unupdated right or a new data arrives right for example your hackers or spammers change their strategy of sending spam emails right so so the the the data changes and your model should be able to identify the new patterns which the spammers are following right so what happens that if any data changes happen it retrains the model so first of all data again new data comes in retrains the model and then deploy it that's why we call it in a loop in a production environment it is a NeverEnding process you know it is a never ending process deployment goes in production right and it trains the model it it sorry uh it first of all collects the data trains the model and deployment goes in production what if if your model changes model changes if the model changes you again you have to go back and then push it again or if new data arrives you have to go back to data collection and then push it again I hope this really makes sense if it does not don't worry we'll have uh lot of examples to study more I'll take I'll give one possible scenario of this production when ml algorithm changes or of the about the loop in a production environment so one possible scenario of going back of of going back is about model performance starts to Decay right so once you train the model you deploy it after a certain period of a time your model starts to Decay so let's take an example of a fraud detection example so assume you have trained your model for fraud detection and let's say you have deployed it as well and you see your model is giving incorrect prediction right so it most probably happened that your frauders change the strategy or patterns to fraud right the the patterns which your machine learning algorithm has learned has changed so you need to recollect the data and retrain the model which means go back and then do it again and it may happen after some some time again hackers change the strategy right so this is what the model performance starts toate then you have to go back into the loop and then uh retrain and redeploy them more another scenario can be we might need to reformulate the problem as it difficult to get gathered data more data as we need so reformulate other problem s violation of assumption which we made during training so basically what happens basically what happens when you train the model we have certain assumption for our input data that the the input data will be in certain range or input data will will will will be there are a lot of assumptions which comes into this place so if the if the Assumption changes if the assumptions changes which we had in a training data we might need to uh reformulate the Assumption or maybe go go back to this and have those accommodate the Assumption which are being violated or simply the business objective changes and basically to restart again so a lot of things which comes into that place which can be of a loop and it is never ending process you have to have continuously um seeing your model monitoring your model and stuff so ml production which is data affects the output system and it's very hard to make it reliable when deploying model retraining and then collecting and then the loop is very very hard to make it reliable and that's where mlops comes in place mlops is a set of practices it is not some library or it is a tool it is a set of practices that aims to deploy and maintain machine learning models in production reliably and efficiently so to make sure that if there's anything changes in the data it retrains the model I'm just taking one one or two examples it rechange the model if the assumptions are violated it again goes back so we have to make a reliable production I which is which is happening at a large scale so uh the term mlops is like the extension of the devops methodology to include machine learning and data science assets at the first class citizens within the Devol ecology I'm pretty I'm pretty much sure that that you might be a bit uncomfortable with this so let's try to think about in another way okay I'll tell you a very simple example of mlops so assume that you you are you are given a a game to build a beautiful city right now you just build the be now if you build a beautiful building in that city is that helpful just write yes or no is that helpful yes or no building a beautiful city in that build uh in that city is not at all good thing because it needs the electrical connectivity it needs the maintenance it needs the security systems it needs the connection to the roads and the railways and lot of things which comes into the place right right so a single building is like a is like a model you have to connect it you have to securitize it or you have to monitor it there a lot of things which comes into that place for making the fully functional City and companies wants what companies wants the full Standalone cities not a full building right and that's where the people are not getting jobs it's only because they are only focusing on building that building not the whole city and mlops is a way to building that that full city which is required so we really talked about deployment but you might be thinking it's very very easy to deploy the model in production but let me tell you that the trouble begins after deployment so you might be worrying about why so I'll tell you what are the some of the things which needs to be taken care of the first one is accounting for latency so what is latency latency is about that that you might be shocked by the statistics that 53% of the visitors are abandoned if a mobile site takes more than 3 seconds to load so for example if your sites take more than 3 seconds to load 53% of the people will abandon the site and you know why and I'll tell you why is because for example you have you might have deployed a 120 billion parameters model or a very large model do you think that model will give prediction in less than 3 seconds that's that's that's really hard right and latency is one of the biggest problem right and if visitors are not VI your viewing a model or a website they most likely not is engaged with a brand and they most likely not buy your product or utilize that product right so this is one of the those another one is that fairness right so for example you deployed your model right so basically what happened that Microsoft created a Twitter bot to learn from users and you know it became the racist it started supporting the various bad ideologies after deployment they thought that this will be so good and it was taken down by Microsoft in just some hours it was against feminism it was sorry it was against um I I'll not take any names it was against x amount of thing right which was which was which was so racist out there that's why they had to take it down in matter of some some some some hours after deployment they thought that this will be good but but eventually it learned very bad things and then need to retrain it but however it never gone into production from from there another one is lack of explainability and audibility it's very hard to explain the prediction right um and and and and we also also have to make sure that it is authentic enough to trust this right and that's why there are several rules and guidelines which are coming by and by again to make sure from the U EU to make sure that we fit some of the principles of AI and it is painfully slow I'll tell you 36 the there was um basic you know um survey conducted from a set of data scientists about how much time they spend in deploying and machine learning models 36% of them said said that they spent a quarter of their time which is 36 % of their time deploying machine learning models right and and and this is so you know um um which is and and and they're like more 36% of S quarter to half of the time of deploying and 20% half to 3 quars and 7% in more than three quarters it is very very slow and you might be noticing why it is slow and there's a lot of things which will will face when when when we are building project we might be very surprised so to see that I'll be I'll be so correct to you I'll be so truthfully to you that when I was building projects for this course I actually spent my whole week in deploying models because it's painfully slow and and you and you might be shocked that I built the whole ml model Tor procing in just two days that's it and spending four hours but a freaking whole week in deploying it so this is uh this is one of the so yeah so um so I'll talk about the model Centric and a data Centric in a b but what exactly model Centric and data Centric means so model Centric means that you are you want to improve the model while do not changing any data so you fixed the data so you have X amount of data you fix it and then you iteratively improve your code or model by tweening it some parameters and expect your model to perform well or in data Centric what you do you hold the model fixed and then you keep on iteratively improving the data and a lot of work is in this model Centric only few of the work is in data Centric so I suggest for you all to focus on data Centric more probably to focus on data have still the model but yeah it's to totally upon your choice this is also Sav by Andre d which is again the very U Pioneer which is one of one of one of my instructor too is's very nice in what he teaches and I think that his ideologies I was in one of the webinar of him and actually he told about this B Centric and data Centric and which which we really experience in our daytoday Life as a data scientist so let's get started with talking about the whole process of mlops and what does it include so the first thing to worry about it what is the business problem we want to solve so what is the business problem we really want to solve that's the first question to start off so any melops project any machine learning product which we have to start first to worry about not about what Pro what what exactly ml thing would you have to solve what business problem which you want to solve so in that business problem you to solve you you have to take care of several things out there the first is the cost of wrong predictions so I'll tell you a very basic we we'll have a basic example so let's take an example what we want to do we want to predict we we want to Pro we want to forecast you know we want to forecast our retails for example what happens in a company that sometimes because of the wrong estim sometimes what happens there might be Overstock of a particular product which leads to wastage of resources and they underst stock which leads to again a revenue loss so in both cases underst stock or overstock of your um of your res of of your products is being the problem in a retail company so you not to you you want to really solve that so the first one is what is the cost of wrong predictions if we actually if if our model gives if we actually don't estimate the right thing the cost of wrong prediction is quite High Overstock means having more stock of your products leads to wasted resources and possible rofs for unsold products and understock which means which is Miss sales opportunity and unsatisfied customers because they're not able to get the things on time and both of them has the quite high pro quite High U costs because one at one point you if you Overstock wastage resources and one point it is like misses opportuni so we we have to worry about and if we solve this problem we'll fix Overstock and understock problems so let's break down the process of sale forecasting processes so basically in this sale for forecasting process you decompose the process of sale for forecasting into component task see see see just notice that we haven't reached to our ml thing right away we first of all talking about the problem which you want to solve right and then dividing the problem then right so now is a sales forecasting problem you're dividing the problem into several things first is data Gathering second historical sales analysis market rint analysis and actual forecasting so what does data Gathering mean getting the required amount of data which we need analysis of the past right what is the trend of the market and the last one is actual forecasting so what you do you you actually data Gathering is something which is pretty easy right not pretty easy I would say it is it is it is something which so let's worry about what things can be solved in ml in this case and also will will it return High Roi right High return of our time right which we devote in this so of course we can have data Gathering has all the equal importance but what eventually we could solve for using ml is this actual forecasting to actually estimate what will be the number of pro stocks which which we should have in a certain time period and we can actually use this IML in this actual forecasting task where it could analyze the past sales data and market trends right to predict future sales with higher accuracy than whatever traditional methods they're using so now you might have noticed that now after dividing it it into the components we understand actual forecasting is the one which we should solve for by by utilizing the past data and the market trends and the ROI could be estimated by potential increase and a decrease in sales wastage due to improve forecast for example if your for forecasting is good you will be actually noticing that you're that there's a decrease in a wasted resources which means eventually it's helping right and then what you do you actually worry about what what will the cost of developing and P maintain the solution so if your wasted resources are decreasing a lot so you might focus on building this AML solution of it and then you have to prioritize it in this case you or to prioritize the implementation which is the actual forecasting in this case Okay cool so we'll talk about structuring in our project as of now I'm just leaving the slide so there's a machine learning canvas which usually you have to really worry about while building a project or solving a business problem using machine learning the first one is value proposition right so first of all we have to define the value proposition position what is this importance Define the problem the importance of the problem and what who will be our end user right so basically you want to for you you need to understand for who we need a product service this product will be benefiting jofre M value proposition positioning statement which means for Target customer who need our product/ service is a product category category that benefits basically we have to make sure we have to make sure the problem importance is so high right to proceed with the solving for the problem another one is we have data sources from where we should identify potential data sources and it can be including some internal databases or apis or open data sets and Etc which comes into that place we should also consider hidden costs such as data storage purchasing external data and Etc which comes into that place so basically this is the second second step third step is what will be the production task whether it is a supervised or unsupervised problem or aom detection classification regression or ranking problem you just worry about what will be my input what will be my output what will be the degree of model complexity right this will give you more clear clear Clarity before building the before going in actual cing part then next step is feature engineering so basically you have to interact with a do experts for example you might be working you you might building something a really good in in healthcare space but you're not a MBS doctor right you actually need to have the MB doctor to actually get more information to understand the terminologies and actually make more information extract more information from the available data sources which we get that's where feature engineering comes into comes into this place offline evaluation which means you set up some Matrix to evaluate your system before pushing it to to the deployment preemployment means using the model by your own and understanding the prediction errors and what will the cost of wrong predictions and then using predictions to make decisions how will the end user interact with a interact with the predictions will it will it involve any hidden cost which can be in human intervention and lot of things which comes to that place and at last we collect the new data we keep on col collecting new data for model retraining and preventing model decaying performance we also consider cost for data collection and the role of human intervention in data labeling because it's very very important for having good labelers in the data to actually for for actually helping models to extract patterns from it and then you have deciding frequency of model retraining and Associated hidden cost for for how many time we'll retrain a model at what interval and as well as if there's any changes in a tech tag which you have to worry about and then what you do you set up Matrix to track system you to monitor your model once your model is deployed you know you have to deploy it so that so that for example in spam deduction you have to you have to keep on check you have to have some Matrix to keep on check whether your model is giving the wrong predictions or not that's the monitoring part and also identify situations where AIML may not be the best solution it can be some subtasks of it where you actually worry about something outside of IML right because it's very very important to understand if we can solve it without IML because it's very um hard as well as the the cost of implementing am solution is pretty much big and also so so that's that's pretty much about um what exactly we need to worry about uh in whole mlops procedure so there are three things which comes into into the workflow of building uh workflow building the machine learning based software development that there are three main artifacts in building mlbased software the first one is data second one is machine learning model and third one is code and three main phases which which is engineering data engineering ml model engineering and code engineering so let's talk about each step by step so data engineering is like you have to collect the data acquired the data and prepared the data accordingly right what is uh and then also make sure that there are certain things which is which we have to make sure in this so the pipeline of data engineering here how goes pipeline means stepbystep procedure to go ahead right first of all you ingest the data and the data which was ingested you explore and validate the data that is coming from a true space as well as explore it to understand the data you format and clean the data you label the data if it is a supervised learning problem and you divide the data into training validation test set so so that that can be used for training the models I'm assuming that you already know about training and validation and all those things which is already there I'm not over here to explain you ml things I'm over here to explain you things which really matters so the next step is model engineering so the core of ml workflow is writing and executing ml algorithms the pipeline here it like this you train the model you evaluate the model validate the model which predeployment so that makes that your model is working pretty well you test the model right using the unknown unseen data set which is the Unseen um samples which your model has never seen and then you package the model so that business can be used accordingly it can be pkl file or any such you know uh models and at last but not the least you have you deploy the model you serve the model in a production environment you monitor so that it's it's it's going well and then you record and then also log it so that for every INF for for every prediction it is making so that we can go back if there's anything goes wrong so I hope that you understand these three things pipelines we'll go into that greater detail when we actually do this project we'll implement it live over there so that you could see pretty much easily and we'll use zml to develop execute and manage our machine Learning Systems so I'll talk about uh pipelines and steps in pretty much small detail and we'll eventually go to projects because I think that is pretty long varo so we'll just try to talk in Greater detail later on as of now let's talk about what are pipelines so zml follows a pipeline based approach so don't worry about the zml thing as of now we'll come to that what exactly it is data on but currently let's talk about pipelines and steps so zml follows a pipeline based approach to organize machine learning workflows it can be methods to promote efficiency rep repetitively sorry repetitively and collaboration in your projects so SA for example uh so what is pipeline it's like a movie production process a pipeline is a high level workflow that organize a series of tasks to create a final product in the context of a movie production process it can be of script writing casting filming editing and distribution casting depends on stre script writing filming depends on casting editing depends on filming and distribution depends on editing everything is interrelated everything is step by step not you you you can't do scripting and then editing right so similarly in zml your pipeline represents a complete ml workflow and each step over there it can be involved a step can be dat a preparation feature another step feature enging so feature enging can only be done if the if the previous step is completed and then train the model to evaluate and deploy it so here's a very basic basic example over here so basically you you you actually first of all uh step one which is the prepare the you have some ml you load the data which is using the step decorator you have another function which using the step decorator you train the model you evaluate the model which is editing and then you deploy the model which is distribution now you combine all these steps into a pipeline right you have the data you give the input data which is using a pipeline decorator you give it to the feature engineering and then you give the features to the model and then you give the models to the for the evaluation to evaluate and then you deploy the model and then you run this whole pipeline so it's run step by step to reach and give you the trained model so there are a lot of things lot of benefits of it which we'll discuss throughout the course right so in the next set of lectures I'll make sure to introduce you to bit of bit of collab notebooks to make sure that you're aware about basic functionality of Zen so that we could actually use this in our project and then we'll be go in building our first project of mlops so let's get started uh with this uh our first mlops proy hey everyone welcome back to the new video on mlops course so basically today what I want to do is I want to make you familiar with core and the fundamentals of zenel because it's very very important to understand what are the co Core Concepts of ZL to actually start on building several projects using zml zml is an open source library for building uh full stack amops applications and the reason why I want to use this ziml because I personally worked over there for for about six to 7even months over there and I've worked there with their core team and actually this is super simple to use that's why I want to use ziml you can use several other orchestrators which are available in the market however the most easiest one with the best ones uh is ziml so that's why I want to use ziml however uh you might face sever other problems U but there's always a community which where you can interact and uh resolve your doubts so let's get started with ML pipelines with zml and today uh this is The cocol laab Notebook from Zen byes which zml Zen ml team has already built for us and over there this the the the Zen pites what they want to do from the these kind of collabs they want to teach you the Core Concepts so I want to utilize these and then uh record a videos on top of it to actually make you understand some of the Core Concepts of zml so let's get started um and also we'll be doing projects don't worry this is just for core understanding because as we say the core is the power so let's get started uh with this notebook so uh first of all what what we will do we will install the zml server which is the zml server which is very important for us to install U you can this is a command line uh command which you have to paste on a terminal if you're using vs code however you can just get started with collab will when when we'll go to projects you can actually see the way I'm doing over there we we'll we'll also make use of pyit learn because I want to show you the demo that's why I want to train a very simple model over here and pipe Haring which is important for collab and then we do it for simple things which needed however I have already did it so you don't need to uh you you only need to do I don't need to do because it takes bit of time to download so so uh and the next uh you need NG do account if you want to see the visualizations and all the stuff which is pretty easy you only need NG do account for colab you don't need NG do account for something if you're doing uh for on vs code or simple python code you'll be easily having access to that however for collab you need NG do you can actually have it I have a coupon code over here it will I you can actually I will hide it I'm so sorry for that I'll hide it or you can just have your own ngok token as well cool so uh and over here you this is just for collab setup this is not for uh any things you which you have to learn so uh what what we will do is uh will you you might be familiar as it says that you might be familiar with pyit learn pytorch or tens oflow so as I say the ml pipeline is simply an extension which includes the step by step as I as I told the example of a what I told the example of a movie production process so and and in that movie production you have scripting casting editing and all those things those are steps which are interconnected with each other right so the the the reason why we use pipelines is because of the following reasons the first one is we can easily rerun all our work not just the model right so basically you run each and everything from the starting so which which helps to uh eliminate any bucks also make our models easier to reproduce second one is that for every pipeline you run you have for for for every time you run you have the you have you can easily track the previous previous run in this in these kind of pipelines for example you run the code one time and then you run the second times and usually you don't have the access to the previous run so basically using pipelines you can have the access to every runs and it can be tracked as well and then you can comp then you can use for several purpose for for example comparing two different versions of the models right and also if the entire pipeline is coded up we can automate many operational tasks like retraining redeployment and all all those things which is needed via cicd workflows we uh don't worry about if you didn't understood this line when we'll actually do one simple uh project one simple project you will see if any things changes on the data how we how actually pipeline help us to um this redeploy or retrain our model Okay cool so let's get started with the zml so first of all you need to have the you need to have the Zen Library installed first of all what we'll do we'll remove any existing files of there and then initialize a ziml repository which is very important uh which is very important which is the first step whenever you use ziml Library so it initializes your zml in uh in your current directory So currently you can just use ZL in to initialize it and this happens this uh exclamation mark shows that we are dealing with terminal commands so now so now I'll show you what we exactly do for basically I'm going to train I I want to train uh class psychic learn SBC which is a support Vector machine support vector classifier on want a train a support Vector machine classifier to classify images of handwritten digits so basically we we will do the handwritten digit recognition using uh support Vector machine so over there we have there there is there are several images and each image is either 0 1 2 3 4 all the way around to the 10 so basically you want to classify the numbers the images handwritten the digits based on 0 to 10 uh numbers so let's get started with it uh if you are not sure about what is handwritten digit recognition for should take a look at online what is the problem is about so basically what what we will do we we load the model we will train the model we'll trest the model on the and then we'll see the accuracy however this is not the right thing to do okay this is just for practicing this this can be thousand X more complex than this is very basic version of anything right this is just a dummy dummy thing just to Showcase you so basically you load the digits so load digits will load the data set from SK learn data set so basically we'll use the SK learn data sets to load the digits and then what we will do uh and then we'll reshape it so that we'll just do the little bit of processing and once the processing is done we'll divide our data sets into XT train X test y train and Y test the reason why we divide it because so that we can train our model on X train and Y train and then we can test our model on testing however again I'll say that machine learning algorithm coding is thousand sex much more better than this we teach in our code machine learning course uh which is if if you actually see the code machine Lear course you'll see what is this so yeah this is just for dumb example don't take inspiration from machine learning uh algorithm over here right and then you simply train toest split and then you have the support vector classifier and then you fit the model and then you evaluate the model okay pretty simple now what now this is I and then you run it you get the test accuracy now how can we run this into how can we divide this into experim into pipelines so what we will do as we have the Z ziml init which is a ziml repository we'll create our first pipeline the first pipeline will have the following components in that it will import it will train it will evaluate the model okay so are three distinct steps in this example loading of the data training of the model and evaluating the model right you can simply use we can will will we will simply make different different functions for different different components over here so first of all we'll make you'll make use of adate Step operator which you can simply import from zml and then what you'll have to do you the the Importer importer Does this does not takes anything this returns right this is called type set the the thing which we have to return over here so the so basically the utter will load the digit will reshape it will uh train to split and then return xtrain xest y TR by test the reason why we have to actually write over there what is actually returning is for several reasons which happens behind ziml behind the scenes because this ASV trainer should know what type of input it is coming to me because as we trainer will get X train and Y train right so they should know what type of soda to verify the type of the data types which importer is sending and SVC is getting we need to actually state that this is something which is it is going to return this is also helpful in readability also it happens it also helps in the back end of the system which is annotated and then np. and array this is the Xtreme will return the extem the type of that is nump array exess type of that is an so this is a formal annotated which will annotate our outputs right and then you have another step which is SBC trainer right we which we again decorated with at theate step and then it returns as we see over it takes X train which we say that it is a nump array y train it is also a numpy array and it Returns the classifier it Returns the classifier right so basically it Returns the classifier which means that we train the model and it Returns the classifier so basically you you can import classifier mixing from skarn base which says that this is the whatever it whatever it is the type it will just make make the type so it will be classifier mixing it will be mix of classifiers you can easily search online about classifier mixing if it un confused about what type of data type is this this is just the SVC classifier data type you can also write SVC over there by identifying the type of this model the next step is it will take the input as a X test and Y test and the model and the model type would be classified mix in and it will return a float and also it is decorated by step operator and then we just uh this test accurate score it and then however there there there can be several set of classification measures just as of now as we say that we are taking a baseline now once we have the steps now we to connect each and every steps so we'll use the zml pipeline and the pipeline will have following like this first of all you exrain xess and you import you use the Importer which you built over there and then you use the SVC trainer and then and then you use the SVC trainer where you give the the this step which X train and Y train you give it and then you evaluate up so once you run it once you run it you will be getting and then you simply use digits SVC and then digits pipeline when you run it it will initiate a new Run for the pipeline which is currently it I have I've already run so this is the version number two so you can you can in the dashboard you can visit the version number one okay and then revisit the accuracy over here and as well as previous one so um you can simply go this it says step importer has started step importer has finished in 2.73 to2 seconds SVC trainer has started as we see trainer has finished evaluator has started it is finished and then every run digits has finished you can visualize your pipeline runs in simply zml dashboard right and then you can run this code and then go to this U URL so basically when you run it you will be prompted to URL something like this and then you can easily go over there and then you will Simply Having your pipeline so basically your password should be default okay so basically whenever you um go over there so let me show it to you so basically um let me run it quickly if you want it uh I'll run it so you can see now it will train the third version okay so sorry it will train the second version because we have reinitiated our the stuffs right so you can simply go over there and then um it will will run it so now you see it's starting the zml server you can easily go over here and then it will it will open the zml dashboard now once it opens you can so basically you'll be prompted to something like this you have to write default over here yeah you you have to write default over here and then click on login so once you click on login you'll be automatically having you'll aut automatically go to pipelines and then visualize your pip pipelines over here however this is not the pipeline which is over here you you'll be viewing something like this so this helps you can easily go to the previous one see your model score come to this your model score and etc etc so I hope you really understood what exactly steps and pipelines means in ziml this is just the basic things in the next lecture in the lecture 1.2 what what we'll do we'll I I'll show you some of the Magics of what zml does and then you'll be surprised to know about that as well right so let's get started with a new lecture and that say pretty much simple that we'll first of all go ahead and read understand what our data looks like so that it gives you much more clarity about the problem statement which we really want to do currently I'm not setting this too much on business objectives and all most probably we focus on the technical aspects of building this envelops project so over here you have the data and and and the data says the oldest customers data set and that data set has the uh most probably over here if you see that customer ID customer unique customer City customer State and then we have geolocation data set and then we have items data set and lot of data sets out there so what we did we we made our custom data set over here so if you go and see our custom data set we have the lot of features where we combine everything to one and then we have the review score which is our uh most probably our uh review score which is the factions scode so I'll quickly show you um uh very basic how does it looks like in Excel sheet because it's more important to let you aware about the Excel sheet because it's much more common because currently it's bit complicated over here if you think uh in a basic um Visual Studio code so let's get started with actually show showcasing it actually takes a bit of time because the file is a little bit large but no worries we'll get started with it but as soon as it as it is opening so what I'll do what I what I'll do I'll create several fold folders which is very important for us which you can take as an template for for starting off right so let's get started actually showcasing but before that it it actually opened up so you see that order ID customer ID order status order purchase order approved that and lot of features comes in comes into this place and then finally you have review score which is from 1 to five which is from 1 to five however currently we'll be not using this review comment and we'll delete lot of you know features though not not because of the feature not because of the it it does not holds importance but or because but because because we I don't want to make it complex project initially you can of course tweak it accordingly you make it on whole data you know do do in the setting of machine learning setting and lot of things which which you can do currently I want to make it pretty much simple that's very nice so let's get started this is our uh Target variable and all of this is our input features which we have to use to actually predict our uh customer satisfaction score so but before that what I'll do I'll quickly make several folders which is very important for us to get started so um and also but but before that let's install several libraries which are listed in readme.md so uh one thing which I just have to uh make a note that you have to actually perform all the actions all the installations every of your operations in a virtual environment currently I'm in customer satisfaction virtual environment I actually use something known as as you know what I use I I use spy EnV you can use cond or you can use um ven or literally any virtual environment which you're going to use for actually creating virtual environment if you're not aware of what is virtual environment it is actually containerize all your applications into one's environment so that your dependency conflict does not happen I know if you if you don't if you don't know you might be not able to understand this so we have linked a very nice resources in the GitHub repository just before this section uh which you'll see in the GitHub repository to actually understand what is it version environment means but it's very very important to in a virtual environment to actually have it everything on the good page cool so what I'll do I'll quickly install so basically uh I'll I'll first of all pip install zml server so that you can this is for the one who actually wants to run the whole project Let's ignore this and let's try to install this first of all zml so I'll I'll quick quickly go and install zml so you might be seeing that it is giving some errors so what what we have to do we have to actually add something like this I hope so it works if it does not we'll have to go to ZL ser and see it okay cool it's actually working so it will take some time to actually download a zml and and I'm personally downloading over here so that you can also see that how exact exactly these thing works and also what I'll do I'll quickly um import the requirements which I have it over here uh so that you understand it much more in a great detail so um what I'll do I will have it over here okay cool so this is the requirements.txt however you can uh this is just for show CAD boost SL jbm we'll not be learning about these algorithms if you want to learn these algorithms enroll in my core machine learning course but um but we'll be not learning this this is just for installation of the libraries which is very important for us to install it prior however you can totally choose to ignore this we just coding step by step so that you understand understand it much more in Greater detail so now it shows that it is actually installing and I pretty much think it is installed um yeah so it also say that that we have to upgrade so let's just copy from here to here and is because the reason why I like to upgrade it because it really shows pretty interesting and pretty beautiful the way it downloads not not is this white white one is actually very colorful that's why I like that I'll just clear it very quickly and then what I'll do I'll go ahead and zml up so what is zml up up does it UPS or the or awakes the zml server so that you can view your pipelines you can view a lot of things out there if you simply put zml up but before that what you see that it is not running the reason why it is not running that we forgot a very interesting thing out here before that we have to actually write ziml init which initiates the repository over here which means that it it initiates the ziml repository over here so right you could see that that a do Zen folder will be created over here as soon as possible as the runs is completed so let's just wait for a few seconds and let the let the Zen ml gets Zen ZL in it gets completed the reason why we want to create the repository because we want to containerize or have all our code inside that repository so that it can be used for several other purposes which you'll realize it a later on okay cool so uh but let let that running it's it is a first run that's why it is a bit taking time so let's just go ahead and create folder which is very important for us now you see that zml is z do dozen is created and it says that your zml fine version does not match the server version the version mismatch might lead to errors or unexpected Behavior kindly refer to blah blah blah so let's do one thing let's simply ziml down grade so that should definitely replicate all our errors out there which is this warning because it's it's you know it I'll tell you from my personal opinion that it's very very important I'll tell you from my person it's very important fix up the warnings the reason why uh I want you to fix up the warnings because it's because sometimes it might happen that you'll completely get unexpected error and you'll never realize that you were here right so that's why I really want you to first of all um uh make sure that you uh satisfy all the errors so basically it says that your Zen cine version doesn't as server version so you can I either downgrade or do a lot of things to actually get it done but uh living that it that it is let's just go ahead and create our folder so the folders which I'm going to create is first of all the Zen folders created the data folders created one thing which we will do in a future in another projects of this whole course is that we we we will not use CSV data set we will use the poster SQL and then retrieve the data set from SQL because in real world setting we not eventually use CSV we actually use uh SQL databases from cloud or somewhere like U fster SQL on local right and then we use from there and then we retrieve and play with that data right so that that we'll do it later on but most probably just let's just keep it very simple and let's just go ahead with data folder cool um another folder which I really want to have is something as model folder model will contain all my um for all my files which is of like models and stuff all the all the things which is required for training the model or you can also name it as a source so let's name it as a source because that's more important right so let's name it as a SRC and which which is which all which all contains your data sets now what I'll do I'll quickly create something known as pipelines so pipelines will contain all our pipelines which we have which will build saved model will contain if you if you want to save the model um eventually you don't need to but you know just just for reference we created steps so steps will contain all our components or the tasks which needs to be done over here and then at last you can actually have in. Pi so let's quickly create something know as inite pi and then after that we'll create there is always there's always a requirements txt what I'll do I'll create something known as run pipeline so we can run our pipeline over there run pipeline. P cool so what I'll quickly do I will first of all um code all the um the data things first of all what we'll do as as I said this is not a formal machine learning engineering course right is the mlops course so I'll make sure that to keep keep things very simple very knife right right but if you want to learn like more of the advanced things in machine learning there's always core machine learning course available out there to actually help you out the first thing which you want to do the first thing which we want to do is ingest the data so we'll start off with first of all uh steps so steps in that we'll create the file Lim ingest data. pi and ingest data. Pi will consist of the steps will will consist of the steps where we will inest the data in it so what I'll do I'll quickly import logging out here so that we could eventually log when when when things completed because it's very very important to log as well and then I'll import from import pandas as PD and then will from zml import step as I as as you have seen in basics of zml that we have to actually use this step over there then I'll create a class of ingest era I'll create the class of ingest era oh my God you know the the way my keyboard is not working pretty well and over there what I'll do I'm actually using copilot still but um but uh you have to also you know give the good documentation which will write pretty nicely so neat so let's just quickly do it so I'll just in it and you can actually do it like this um which is in it and then when you run it you can actually write the get data and then ingesting data from the data path pd. read CSP and then self. data path right or what you can do you can simply give the pcsv this direct file to this totally matters on what you want to give okay so then we'll create a step so where we can use that class we can use and then that step will consist of the data path which will take St Str as an input which will the string of course and it will return the data frame right it will return a very nice data frame and then what I'll do I'll first of all make a try statement I really don't don't want to take the help of um I really want to take take the help of this guy uh copilot but if he's helping me I can't do literally anything so what I'll do I'll show the way to write the documentation first of all you know what you write you write the description about that function so use ingesting the data from the data path then we'll write the arcs arcs means the argument it is going to take the data part which is the part to the data and then what it returns it Returns the Panda's data frame right this is how you actually do the inest data and this is what the very interesting workflow is then we'll write in a try try and accept a workflow which will have something like this where we first of all in in instantiate our class which is ingest data which is ingest data where will first of all ingest data which is the data path and then we will simply say DF ingest data. get data and then return DF this can be easily done in a three one line of code as well as shown by the copilot but I want to make it pretty simple as well for the beginners if you're watching it so for accept exception as e and then it says error while ingesting the data and this is what the error is so this this is this actually helps us to uh the best practices of coding and same goes to over here we have to actually maintain it nicely right um so let's just go ahead and quickly do it uh let me just remove it yeah so I'll just make use of interesting data from the T data path and then we actually instantiate the method so this is this is this is used that this is used as instantiating the method uh arcs that's it and you can also write instantiation but if not bits is not required eventually over here what what what we do inting from data path and then you simply write nothing and then you just that's it so this is a basic workflow which you have to go ahead and create this step the first thing which you have created is of course ingest data now we can actually use this ingest data which we will eventually use it later on as well so now the next step once we have ingested the data we need to we need the next one which is we need to clean the data so now now what no so now what what we will do we will we work on creating step which we will use for cleaning of the data okay so let's let's do quickly one thing we'll create first of all let's import loging which is pretty important to do so this is something which is of course and then from zml import step right and then I create a step that step will clean the data that step will clean the data right and it will take the data frame I don't know what it will return so let's skip it as this and then we'll pass it so we want to make this step we want to make this step right which cleans the data The Next Step which I want to make is the the is the one which trains our model okay which trains our model so first of all then I'll write model train. and in that what I'll do I'll create another step I'll create another step okay I'll create another step so again same thing so you have to just go ahead and import loging and uh import logging and then import pandas as PD and then from zml import step right and then you just write step and then you just go ahead and create the train model right train model and it takes blah blah blah and then it returns something and then chains the model past them okay so that's that's something which we have to go ahead and I'll just quickly make this like this cool my battery is low I'm so sorry for that but yeah first of all this is the model train which we have to have now once the model train is there we have the clean data we have inest data we have trained the model now the next step which should be it should be evaluation it should be evaluate the model uh so I'll just write evaluation do p and over there same thing which is something again so import logging uh from zml import step and then at step we just just have something Define evaluate model and then returns nothing okay so this is something which we have to actually have and then once the evaluation is done we'll have you know some know that's it so that's that's that's the four steps which we want to have now you might be saying that I'm not implemented I'll implement it the first step is always create a blueprint right so that it runs nicely okay that's the first step and whenever you go you have to actually understand the first step now what I'll do I'll create a pipeline okay I'll create a pipeline the pipeline would be first of all training pipeline pipeline dot pipe that pipeline first of all what what it will do it will from zml okay so we can just from zml zml import pipelines and then at theate pipeline so just just let's just write at theate pip P line and then what I'll do I will simply go ahead and create the training pipeline the training pipeline the training pipeline will consist of the following that will consist of the following that will ingest the data that will ingest the data cleans the model cleans the data trains the model and evaluates the model okay so the training pipeline will consist of the following so I think that something is wrong over there so let me just quickly do it uh just just give me a second okay so now what I'll do I'll create the uh training pipeline so let's just quickly create a simple training pipeline our training pipeline does not takes anything most probably it takes the data path right it takes the data path as an input and that's pretty much it I guess yeah so it takes the data part as a input it first of all we we'll import everything over here we'll import first of all from steps uh from steps. ingest data I'll import the ingest data I hope that is working so let's just make sure that it's not following any conversions so yeah inest DF cool and then what I'll do I after inje from steps do clean data yeah so we'll just go to clean data import clean maybe I'm not sure what it is so let let's let's just go okay clean data let's import clean data and most probably we'll have clean DF just for make sure that we have the good naming conventions and then after that what I'll do I'll import from Step strain model and then after that from evaluate model I just make sure that evaluate model is there cool so once we have all of these steps what I'll do I'll quickly do all of these things very nicely and show it to you the pipeline so we'll just go ahead DF is equals to ingest data it will ingest the path it will clean okay so most probably we'll just have something cleaning so clean data will take the dfz input fair enough it returns nothing so it returns nothing so we just have something very nicely over here after cleaning we'll have strain model and then after that we'll have evaluate model I just hope so that everything takes um DF as an input so that it makes sense Okay cool so now once we have this what I'll do I'll do nothing I'll just go ahead and then run the pipeline okay so how to run this pipeline so we can just create something known as run pipeline as we have created let's just go and create the Run pipeline as soon as possible so we'll just go from pipelines do training pipeline import training Pipeline and then you just go maybe just write train pipeline just just for make sure that we following the naming conventions so the train train Pipeline and then after that I'll just if name is equal equals to main I just hope that it does yeah we'll just run the pipeline so run pipeline will happen something like this and the data pipeline which I'll give is I'll just copy the whole path from here and send it out to here there are a lot of things which you can do you can actually upload in cloud and do stuff which we'll do it later on the of the course Okay cool so let's run it so are you ready if you are then give me a thumbs up I'll get started with it so I'll just go ahead and clear most probably and then let's run it Python and pipeline you know the when I code I actually listen you know lot of you know what do you say uh music but eventually I'm not as of now so sorry for that install pandas pip install pandas right so let's install quickly because that's more important okay something really happened over there uh evaluation right okay as I say uh import Hondas PD that's let's go ahead okay it says modu object is not callable I think most probably what's the error is about it's about uh let let let let me just quickly go to that error training pipeline pipelines okay it's actually pipelin bro right it's actually pipeline okay cool enough let's go ahead Okay cool so something is giving a really interesting error uh because of some of the things so let's just quickly fix it so it when so what is happening I'll tell you why the error is happening it says that wrong wrong type out wrong typee for output for step clean DF why it says that it is expecting Panda's data frame because you have told that it will give Panda's data frame but it is giving none so we have to actually write run over here same over here uh same over here so it is actually expecting that um you know uh it will return something that's why it was initiating that it it'll give warnings just go ahead with the warnings okay that's pretty nice thing which happened cool so I'll explain you what what what just happened you can choose to ignore um completely about all of these stuff like what is this user stack and orchestrator artifact store we'll explain later on so you'll see that the inest has started clean DF started right clean DF has finished evaluate model model started evaluate model finished trade model started trade models finished nothing goes over there that's it now what I'll do I I I'll showcase you this very simple dashboard which is out here so we'll just go ahead and go to the dashboard quickly so the username would be default and then login so when you log in simply go to the pipelines for you it might be super new so let's just go to pipelines this is the train pipeline so let's go to the first one okay fair enough so inest data gives the output which is the data frame so that is a data frame so if you go and and you see the this is the output and it also shows some of the visualizations or you know data type of it you see that data is imported this is called the artifacts okay the thing which is stored uh and over here if you see for after every step so this injest DF has something known as what is the name of this what is the doc string which is like the documentation right start time run time and all those things and then after that what what is the output and the artifact so artifact is something which is returned after every step which is stored in some local stores it is which is stored in some store which can be retrieved further so you see where where it is stored it is stored in this U uh URI so if you go over there and you will see a very nice output over there if you if you go to this particular location okay that is the and logs are simply nothing using cas version step to you you might be noticed what is using cast version of it this is pretty interesting to understand we we'll understand it way greater detail I'll show you a very nice example of it just wait uh and then you have clean DF which is again clean DF is finished evaluate model train model so you see the ingest data gives the and then it does not returns anything so this is a visualization which you can for sure see over here and that's pretty much it and now we pretty much think that our uh dashboard is working our pipeline is running up and we are good to go go with it right cool so one thing which I just want to make sure that you are aware about is something known as caching so uh so what if I do enable cache is equals to true false sorry let's run it okay um let it yeah cool run pipeline now let's see this on our dashboard pipelines the one the latest one so um so you see the same thing which happened over here so I'll just quickly you know do it and then show it to you okay so now this is the you you could see another version is over there which is version number four now ingest data started ingesting data from this ingest data has finished but over here do you think that something really happened interesting using cast version of inj DF so zenim has an amazing and super duper amazing feature what does it mean that it uses the cast version so if there's nothing nothing changes in the data if there's nothing changes in the code or if there's nothing changes in that step it will use the step from the previous run and you see that how interesting this is that see how's the level is going on that's nothing changes right and we eventually using those because the caching was enabled but but caching was disabled right so so catching was disabled but over here I told to enable to to actually uh actually I sa catching isal to for that don't do caching don't use run from the previous version it states step inest data has started uh ingesting data and then in just is finished so if you make it true if you make it true so let me let me just make it true let's run it so uh using cast version of ingest it uses from the clean DF also not change evaluate model it just trains the model in matter of seconds you see how good it is say you're training a large language model right and this is a feature over there you'll be super happy that your forast version has been used sometimes it causes error but most of the time it works like a charm okay that's pretty much cool uh I hope you understood most of most of the things from on the next session on the next video what I'll do I'll Implement all these steps and run it step by step uh and afterwards I'll deploy the model I'll deploy the model using mlflow I'll also track I'll show you how I how we can use ml FL experiment tracker to actually use this and then we'll make a very basic stream extrem lit application to actually use this uh to actually use a deployed model to actually make the Press inference right we'll use the ml4 deployment and ml4 tracking a libraries to actually integrate into zml and then use it currently we have the blueprint ready so that what you have learned in this first of all you learned the way to about write code and to structure the code also you learned a very important thing that it's always good to start with preparing a blueprint and then starting coding it I hope it really made sense to you I'll be catching up in the next video byebye hey everyone uh welcome back to another video so what I'm going to achieve through the this video is we will Implement all the steps which is listed out here so that with the clean data ingest data evaluation stuff however uh we'll we'll do it in very uh nice way I'll show you the way to write code in a nice way by using design patterns I hope that you have already that you're already aware about the design patterns before you started this project if you are not then also we have a very nice resources uh which which will be linked of course before this lectures you'll be also you'll be taught the basics of patterns like strategy pattern Factory pattern singl ton pattern and all of this is already taught to you so let's get started with actual imp implementation of uh data cleaning and see if you're not aware about these patterns we actually teach in our course cor machine learning course you can actually consider enrolling over there or we'll add before this uh section as well cool so in Source what I'll do uh we have several other so first of all in s SRC will Implement all these classes the classes of these steps and then use the classes from this in these type in these steps first thing which which I really want to develop is data cleaning and data cleaning is something which is obvious which we have to work on so let's get started with actually uh creating uh data cleaning classes so I'm going to start off by importing logging if we really need to log literally anything and then I'll import from ABC import ABC and abstract method uh and then after that from typing import Union but I'll just make I'll just import some basic libraries and then as we need we can import more so I'll import pandas as well import pandas as PD and then from escale learn. model selection because we are going to split out data as well so what I'll do I'll create a abstract class abstract class for defining a strategy for handling data okay you might already be aware by several animals example of strategy pattern and all so first of all create abstract class for defining our strategy this is known as the data strategy this is known as the data strategy this is this will be an abstract class this abstract class would be abstract class defining strategy for handling uh data Okay cool so now this will uh with we'll create an abstract method in in it we'll create an abstract method this you know the reason why we do it is already known to you the reason why why we do it is to just make sure that we have a that we can just that that data clean class will show the same handle data right we have to make the same class so when when when we'll work on other the the the the strategies of these data cleaning so you you'll see how handy it is so handle data this will be the data frame because I expect the data frame uh DF pd. datf frame and that should return that should return um set of you know PD data frame or the series so basically we can we we we can just as I say from uh from typing import Union and then you simply add Union and then pd. data frame and it will return pd. cies to okay so this is what it is going to uh return however this is just an abstract class this is just a blueprint I would say blueprint mean this is what we have to implement in our strategies we can override this method to implement our own custom Solutions so let's first of all start building data preprocessing strategy so we'll we'll build a data pre preprocessing strategy data pre process okay let's make it little good strategy and that will have something know some something which is that will inherit data strategy which is an abstract class so that we can overwrite this handle data right override this handle data so when as soon as we have the handle data so it will it will take the data frame as an input and it Returns the data frame as an output okay so we don't need to logging as of now so it will try so we will have the try and exceptions try uh so basically so basically I'll I'll first of all drop certain you know uh um certain columns from the data because this is something which as as I already told to you that we want to make it super duper simple that's why I'll I I'll Dro several columns however these columns are not like they're not important they are actually very important just for Simplicity for this project I'm going to delete some of the uh some of the um uh columns from the data because that's more important right so what I'll do I'll just um I'll just I have already WR the names of the The Columns which I have to delete so I'll just copy them out from quickly from there uh yeah so you can also see that order approved that order you know received that and all those stuff which is required over here okay very cool so data. drop you do you drop certain uh columns you drop certain colums out there and then you simply go ahead with it okay cool so now what I'll do I'll go to go go to the next step I really hope that it works nicely yeah cool so now what I'll do um there are some certain columns there are some certain columns which has which has the um null values okay so what I'll do I I'll quickly fill up the null values you can do by two or three two or three things okay what what you can do you can actually uh do the when these things are analyzed when you do the Eda part however I have done already Eda on my part as as as I said you can actually make the Ed and then see which columns and all I've already did it just for Simplicity so that it actually makes sense to you to actually get started with directly with the project there are some certain columns um which is actually you know um there's some certain columns which which has the null values and we'll try to you know uh make it work so here we go so the data which we have they so basically these these columns fill now with a median of that column and in place equals to true with the median of that column in place equal two median means we have to take the median of this and then we have to permanently apply this on our data right and there's review common message which Al also fill the null values with no review because there are several n null values in the data so that we can just write no view over there okay that's very cool so now what I'll do I will just go ahead and we will we will drop the columns we'll drop the columns which are you know uh which are of non which are of non number type or some some some columns which are actually you know um uh numbering times okay so basically we'll just take columns to train a model who are numbers however you can take like see the the reason why I'm doing this the selecting the number is not because of I want to you know I I'm doing it on purpose it is like the reason why is I just want to make this project simple so I'll select the AL select the columns which are of numeric so that I don't need to apply a lot of processing steps okay so what I'll do I will simply go ahead and data is equals to data do select data do select select data types I'll select the data types which are include uh numbers so NP do number okay so this data will have the columns we have the data which are of only numeric type so now we don't need to worry about categorical encoding ordinary encoding or whatsoever or even tokenizer of this this is also removed okay but you can do lot of things out here you don't need to remove this you you actually Implement another processing strategies where you encode the data where you tokenize this re review comment messages and a lot of things which you can do over here we'll also drop a couple of uh couple of you know uh columns and the columns which we have to drop is the following so the first one is is uh customer zip pre prefix and the order item so these are the columns which you have to drop the reason why we have to drop because this is not important at all okay so we can just write data is equal to data drop drop equals to true and then return data accept exception as e logging error and raas E cool so um you might be worried about like what did I did over here first I dropped certain columns which is not required for us as of now because of the because because of making the project simple second you drop the you actually fill up the null values which are available in the in these kind of columns and then you only select the data which is a numeric type we are not selecting the categorical encoding just categorical data that's just because of the Simplicity of the project and then you drop certain certain uh you know um uh columns and then you just return the data that's pretty much it that's pretty much it you're doing that's pretty nice so now what I'll do I'll create another strategy the another strategy is data split strategy so basically data divide strategy and then that will inherit the data strategy and in that we'll just quickly create strategy for dividing the data into train and testing set we'll again U make the handle data and then the data it will take p. data frame and it will return you know it will return it will return the union of pandas data frame and series you will notice why I'm saying Union Union means uh both of them so here we go so I quickly explain you what does it matter so X is equals this is all copilot that's why I love him so data. drop we are dropping the um the the the target variable and then we have Y which is the target variable and then xray in by xra y Train by test and give the test size to be 0.2 and the random state to be 42 and the next train is the panas data frame X taste is the pan data frame y train is a series and Y test is a series that's why your output of the combination of both of them I hope it makes pretty much very sense so now once we have that now once we have that we will make we'll make a final class where we will utilize where we'll utilize both of these strategies into that class so we'll create another class which is data cleaning okay the data cleaning data cleaning class which will process the data and divide the data so I'll just create create data class for which will which preprocesses okay processes the data oops and divides it into training and testing set cool so what I'll do I'll quickly create Define need and then it will take self data frame and it will also take what strategy you want to implement that strategy would be the would be the data strategy it can be either you know this these are are the these are the types of data strategy right this abstract class so strategy will take either do you want data process strategy or divide strategy Okay cool so what I'll do I'll quickly self. strategy see equals to strategy okay so now we'll we'll have another class which is handle data sorry method and that will that will either return Union or you know a simple Panda State data frame so this will return self. strategy. handle data and then self data so basically the strategy will for for example if someone chooses this data divide strategy so so so data we can use the simple class so basically someone can someone will go and just run this class data cleaning so some something like this if name is equal equals to is equal equal to main oops I'm s for it okay um over here it will just say data cleaning okay so it will say something for for example assume that we reading this uh D CSV file however we are not going to do it right now and then data cleaning and then data cleaning is we'll instantiate this with data and then we want to use this data preprocess strategy so that data preprocess that data cleaning will use this data preprocess strategy in this case over here and then it it has a method called handle data it will run over there then you can same same way you can give it another strategy right which is data device strategy it will do nicely okay so I hope that you really understood the the wave that we do this is called a strategy pattern where you first of all create the abstract class and there are several strategies in it which is data preprocess and data divide and then create the final class which will make use of those strategies uh over here okay and this all this this is actually very helpful when actually be just for flexible code writing as well as readable as well as not writing so much of FNL statements cool so what I'll do I'll quickly implement this into um clean data so let's implement this into clean data that's more important for us okay so clean data we'll take the data frame as an input and then we'll just uh go ahead and try and then let's go so first of all we'll import we'll import what we we we'll have to import uh from source. data cleaning I'll import data cleaning data divide strategy and data preprocess strategy Okay cool so once once we import this now we'll just go ahead and use it try and then try and accept so basically first of all we'll we'll create a processes process strategy so over here we can just go ahead and create Pro preprocess process strategy process strategy and then we C the DAT data cleaning class data cleaning is equals to data in instantiate the data clean class by giving this process strategy and then we will uh have something which is processed data is equals to the the object which we have object which is data cleaning do process data okay process data sorry sorry sorry handle data so what is that eventually doing we have this class we are giving the strategy which we want to use we want we want to use the data preer strategy and then we are calling that strategies uh method which is handle data which will handle the data then we'll have the data divide strategy or maybe divide strategy right and then it will again data cleaning and then you have something known as process ESS data in this case not the data now you have the device strategy now we can simply make use of XT train YX test y train y test do handle data because it is returning Panda's data frame in series and then we have to actually loging data completed and then excepts except as e loging error raise e okay so now one thing which is missing is it returning none it's returning xtrain X test y train and Y test right so we'll use something known as annotated which is the python builtin type setting path parameters a type hend parameters so let's first of all quickly do this from typing extensions from typing extensions I'll import annotated which is a formal one so annotated what it will do and also we we'll have to import the tupo let's quickly import tle from typing import tle sorry uh over here we'll have two p and then okay so annotated the first output the first output is of course pd. data frame and then it will it it's actually xtrain right it's actually xra now we have another one which is annotated X test and then another one y train and Y test and mostly we are done so so basically this is what happens that we are done and now one can actually now it says that it will return the tle it will it it will return the following it will return the four four types which is data Frame data frame series and series which is an annotated using annotated uh from typing EXT extensions so I hope that it makes sense uh now I let me see what type of error it is giving it's mostly because of the this I hope this fixes it so now we are done with this step now we can just you know simple make it very basic doc string cleans the data and divides Orcs so let's just write Orcs raw data and then simply you just have this you can also write returns training data testing data training labels and testing labels okay so now we have this class ready for us and then we can actually use sorry step ready for us where we using several strategies and then we'll actually implement it okay so I hope this this actually makes sense to you all now the next thing which which we'll work on is something something known as um which is something known as model development so model development is something which is pretty much important we'll actually make use of uh linear regression which we will Implement right away from here Implement right away from here and you know so yeah um so we'll just Implement linear regression out here so that it makes sense for you to get started with it so we'll Implement a basic LR so that it is not however there's a lot of things which you can Implement I in the representative which you'll get you'll be having implemented these kind of like you know random forest or XG boost CAD boost and then after that we'll evaluate our model So currently we're not focusing on core machine learning kind of thing we're just focusing on building a full mlops project so I can build it in more complex situations another waye which we have is evaluation as well as where we'll we'll we'll make an EV evaluation measures and then after that we'll also make the steps for it and then we are mostly done however there's something which is left left which is something know as deployment pipeline we'll also deploy the pipeline right you will be amazed to see the the way we deploy the pipeline the way we run it right and the way we run it and also we'll just use a stream application to actually go ahead with this then deployment so I hope that really it makes sense so let's catch up in the next video so now everyone what I'll do I'll go to the next step which is model development which is pretty much important as well um so let's get started with model development quickly and then try try try to complete this project as soon as possible so modal depth. pi and in that what I'll do I'll create a I'll create again the you know uh abstract class and then we have to extend that abstract class from ABC from ABC import ABC and Abstract method so let let's just go and start off with it so we create a class model the class model will have ABC right this is the abstract class for all models this is abstract class for all models um and then after that we'll create an abstract method the abstract method and Abstract method will be called as a self train and that self train will have something as X train which is train training data by train which is testing sorry uh training labels we we can also create some method known as optimize uh but it's not required as of now so let's just leave it so uh I'll create a very simple class see my point again I'll say I'm emphasizing on it first of all focus on learning about amops and then implementing complex models and stuff so I'll just make a simple linear regression model on top of it so let's just make a simple linear regression model and then it will take xtrain and by train and quirks it will uh first of all for together and it would just Okay cool so we'll have some some something which is training and the training we'll first of all we'll just import from SK learn do linear model import linear regression and uh most probably let's name it as a model okay it makes much more sense okay so um I'll just make it over here quickly which is reg equals to linear regression qux and then reg. fit which is and then return the regression and then return the regression right if we can also put put this in a try and try an error so so try accept Okay cool okay that's very nice so now what I'll do uh so now what I'll do I'll simply go ahead and uh you know just we have the model training model ready however uh we'll see in the next Pro which we do this is model development is much more complex because we have to first of all train the model validate the Assumption test if things are working or not you know tweak the data fure engineering cleaning which we'll do in the next project don't need to worry about it okay so this is the model development which is linear regression model where we simply fit it and then we train the model we complete the training of the model so let's quickly go to model train and then we just Implement something over here so uh what I'll do I'll just go ahead and then import from model sorry from Source model def UT linear regression model right yeah L linear regression model now what I'll do I'll simply go ahead and uh I'll simply go ahead and then first of all we have to first of all get the get the data which we have to so it will take several input so it will take extr X test y train and Y test so let's just take cell extr and then Peter data frame X test y train okay X test white train and white test and it will uh yeah that's pretty much it it will return regression mixing so actually it will return the linear regression model right however there is something known as regress and mixing right so from SK learn from SK learn do base okay base import regression mixing regression mixing is a type of you know which is the type like of course we are going to um output the regression algorithm right trains the model and then simply you know appreciates the model my uh I mean just stain the model that's that's pretty much a Okay cool so let's just first of all do it and then let's go on the next PATH so the model which we have is equals to none and we'll also make a config do PI we make that config.py so make some something as config.py and that config.py will have from zml do steps import base parameter base parameters and then create something as model name config that will have base parameters out here and it will contain the model configurations which we which we want to add model configs which which can be model name first of all model what what model name which we want to use what model we want to use and then yeah so so that's it so that is the model name which we want to use so first of all we'll import some something which is over here we'll import a model train and then we'll import um from do config import model name config and it will also take config which will be the type of model name config okay Okay cool so now it will it will also take the config so config will contain the stuff so if so if if the config do model name is linear regression we will say um just just you know use that model which is linear regression model linear regression model and then just train the model on X train and Y train okay that that is something which you really want to do or what or what what we can do we can just have have something which is U model so it will of course it it should it should return something right so let me just go and quickly see yeah it is returning lineation model we just have train model is equals to model train and X train X test and it Returns the train model else uh we can just write you know something which is model name not listed or something some something like that you can raise a value error okay so um the reason the the reason why why I do this over here you might Implement other models as well you can just go ahead and Implement class random forest model right random forest model so you can just go ahead and don't don't don't need to change the name you just say if the config says if the config do model name says random for regressor you train another model so this is how it works okay you don't need to worry about like lot of things out out here it's very simple to understand so just have it as an try and accept exception as e logging error and then raise the E Okay cool so that's it about the training of the models we'll just go ahead and quick quickly create some something known as evaluation system part so let's just go and create the evaluation part as well evaluation Pi I'll just go ahead and create the evaluation. pi so again over here we'll create a very basic again abstract class and then it extend that abstract class to other um strategies which you're going to use over there from ABC from ABC import big ABC and Abstract method and then we'll just have class evaluation that that will take ABC and then it will have something it is an abstract class right it's an abstract class defining strategy defining strategy for evaluating our models right and then we'll have abstract method abstract method will have something calculate scores so it will calculate the scores out here which is y true and then it will it it it is a numai in the array so we just import numai as NP so nump and the array and by prediction which is also the numpy and the array so cool so over here you have something which is calculus scores which is abstract method and then abstract method will have something over here which is y to the the model prediction sorry ground thr and the model prediction now what I'll do I'll simply go ahead and uh create several strategies for it the first strategy which I which I'll create something as MSE so that that MSE will inherit the abstract class of evaluation and this is this is the evaluation strategy this is the evaluation this is the evaluation strategy that uses mean squ eror right mean squ eror and then we'll create the calculate score that calculate scores will take self again y y true and Y true and which is of of course num and I'll just copy it from here Okay cool so uh again so we'll just start with we'll just say we have entered calculating msse so it will start off with the calculating MSE so basically we can use simply something known as from Cy learn Matrix from SK learn. Matrix I'll import mean squ error and R2 score so we'll just go ahead and then just do it so let's let's do so we'll just have some something MS e we'll just give y true and Y PR we just say that it is done and then we return the MSE otherwise we see if there's anything wrong which is error in calculating scores and that's pretty much it so uh it Returns the MSE so now we have one strategy done we'll go and create another strategy we'll go and create another strategy another strategy would be R2 score so R2 score we'll have the evaluation strategy that you that uses so that that uses R2 score and then we'll just calculate the scores and then give everything out so it just implements automatically of course you can add your documentation on your own over here I'm I'm not adding it right now please add it by your own the way I have taught you to do so we'll have we'll have another um evaluation strategy which is evaluation rmsse and then over there we'll again that's evaluation strategy that uses the root mean squared error to calculate stuff so it just again just you know mean square error and then rmse and squared equals to false so basically over here your calculating the root mean square error right okay so now we have the rmsc also done so we have several evaluation strategies totally done now we'll just go ahead and then implement it in evaluation out here so now which is the last thing which you have to do is very very simple that you actually implement this so we have uh so first of all I'll import from SRC do model Dev sorry evaluation I'll import msse rmsc and R2 R2 is it there evaluation R2 yeah R2 is there cool R2 is there um so it will just first of all we'll have the evaluate model this will take lot of things first of all it will take model okay that model would be a regressor mixing so we'll have to UT this is the type of the model would be the regression mixing because it is a regession regression model right so import regressor mixing then we'll uh then we'll uh get the X test then we'll get the X test that X test will be the Panda's data frame and then we'll get the Y test again for and for understanding now let's try try to implement the solution let's try to quickly implement the solution so first of all we'll get the prediction we'll get the prediction quickly the prediction which we'll get is model do predict and then on the X test so model predicts on X test we we create MSE which is is equals to MSE class so so sorry MSE class is equals to MSE and we use that MSE is equals to MSE class class. calculate scores it will simply just have to give y test and predictions and then we are done so now we have another ver is R2 class R2 class and then about and and then after that you have calculate the scores and that's pretty much it and then you have another rmsc class R calculator course and then that's pretty much it cool so we'll return at least let's return two things let's return um msse let's let's return R2 score and rmsse right because that's more efficient to actually look at the Matrix so we are done and we can just put this into try and then this into accept eror evaluating the models Okay cool so now we are mostly done the one thing which is left out here so you might be thinking what is left guess what is left so over here we are returning to R2 score and RMS so we also have to indicate over here that what thing we are returning so I'll just import from typing import Tuple and from typing extensions I'll import annotated okay so this will have two and then it will return two things let's annotate the float R2 score as as the RMS okay so now I hope that it makes little bit more sense now uh I really hope so yeah cool so now we have the evaluate model also done which is which means that we are pretty much done with ingesting of the data cleaning the data training the model evaluating the model now you understand everything is completed now what is left let's worry about that so we have something known as run pipeline so let's try to go ahead into into that Pipeline and let's try to create the pipeline let's try to run that pipeline right away from here so let let me just quickly go and run the pipeline um so I'll just go to um yeah cool so I'll enable the cash as are true and this takes the data path that's that's good that's good takes the data path we have then clean what does clean DF takes clean DF takes something and what what is it returns this returns X TR XTR and by TR okay so let's just quickly write this xra X test and Y TR y test it it's a clean DF right this takes the data so now it is done we have train model so train model what does it does train model you know takes X train X test y train y test and the configs as well so what what we have to do over here we have to actually um okay so let's try to uh quick quickly do that as well um so I'll just go ahead and uh model train so this is train model okay so train model and then after that model is equals to train model train model X test y test all the the Y test and then we simply go ahead and msse which is msse sorry R2 score and rmse you evaluate the model by giving these things X test and Y test I hope that really is X test and Y test that's it yeah that's true and we are mostly done right so now we have the pipeline ready we have everything ready now let's go and run the whole pipeline to see the magic I'm pretty I'm pretty much sure that it it will give some sort of error but always be be on a positive side so let let's just quickly go and run the pipeline okay so no module named psyit lar so let let's just quickly go and so I'll just go escale learn pep install and then just go and uh okay I'll use this one because this is much more easy to install okay let's just wait for this to be installed because I'm I'm actually using the new environment that's why please activate your environment before working on the project please that's the request for you all of you out here let's wait and let's see the magic what happens it's running on so just wait for a few seconds um and after that we are mostly done done with the pipeline of melops you will see the dashboard the next thing which is left which is integration of tracking of our experiments which is using MLF flow and then deployment of our model using MLF flow deployment these two things are left and then we are mostly done with the project and you'll be seeing like now now I really hope that you are seeing the way we do the project the way you know we do the caching stuff the way we write the code that is much more visible in the next set of projects you'll see much more challenging code much more challenging topics which eventually you will learn by yourself so I don't know why it is not a running but okay so cannot unpack a non iterable step R effect object so I guess something really have an interesting out here so let let's just quickly go and see what the error it has given so this expects the data frame and this returns X test and Y train and Y test and then clean DF we have the following full name uh okay my test so let's see now if it is actually returning oh yeah so you see that that it is not returning anything we have to actually return it right that's why it is saying that that's it is returning none and when it is asking for the output so that's why it is not able to cool so inest it is started clean data completed and then we just go ahead and then it justes does something small training completed something failed in the pipeline and R2 score is not defined so let's go and quickly do it so uh let's go to evaluation and then this is R2 not R2 score so let's just go and do that as well so now you'll see how quickly it will be run first of all it will use the cast version you know it it it will use the cast version of it and it will just do the evaluate model and then you're done you see the magic you see you you just see the magic just simply install P install P Arrow to remove this error so let's just quickly go in zml up so let's just go in zml up please do it it will open it okay okay sir I'll give you the default let's go to pipelines let's go to train pipeline let's go to this pipeline you see it ingests the data it againsts the output clean the data at the clean DF it returns these these goes into training the model it returns these gexas goes in evaluation of the model it return R2 score and rmsc you see how magical this is right how magical and how really interesting these things has become right now I I I really think that this is the power this the future of ml right if you don't know about this you don't know anything right so I just go I just hope that you understand it much more greater detail we are most done with the project however there's two things which is left which is deployment as as well as we are also left with tracking of our experiments so I hope this makes sense I'll be catching up in the next lecture bye so hey everyone uh let's come back to our project so basically the project is left with two things the first thing which is left which is of course our most favorite experiment Draco and the second thing which is left was the deployment of our model so I'll talk about what this experiment tracker means along with I'll talk about the deployment pipeline so let's just first of all talk about about what thises experiment tracker means so when you do the data science engineering or a real world machine machine learning engineering job then most probably what you will see that whenever you have you actually want to track every run switch you do because you have to tweak the parameters and then rerun it and then check the scope from the previous one compare it with SE several Matrix and see how well well it was performing in the 30th run or even in the first run right so we need to track our every experiments which we are doing over here so so where should we Implement our experiment tracker the experiment tracker will will be implemented over the train model so what I'll do I'll quickly implement the experiment Tracker out there so when you go to the model train so model train will have something like this and uh I'm so sorry for the background noise I'm extremely sorry because this is India and you keep on hearing these voice so I'll simply import ml flow import ml flow so once we import the ml flow what I'll do I'll uh simp simply go ahead and then initiate an experiment tracker class sorry uh object but but before that we have to import something known as client from zml Cent import client and that seems like experiment tracker is equals to client get experiment uh get active stack so I'll just go ahead and then do active stat okay wait wait for a second yeah do active stack do experiment tracker so once we have this we can EAS easily use this so basically what what we have to do we have to use this ml4 tracker right so we have to in in The Decorator we have to pass the experiment tracker and the experiment tracker which we'll use is the following experiment tracker do name then the name of that so that it should be notified that this step has the experiment track track right so now what we have to do in this case we have to actually you know um uh log our models okay log our models so in this case we have to actually use the pyit learn Auto so basically what I'll do I'll use mlflow dosk learn. autog this will automatically log your models scores and everything out there right in the same way you have for several other libraries so basically we'll do the m. log same what I'll do I'll do on something known as evaluation right so so what what what I can do I let me just go to evaluation part and in evaluation part I I I have to do the same thing I have to actually copy the two couple of things which I did and then what I'll do I'll simply copy the step as well step as well and then over here what I'll do I'll I'll simply go ahead and then select ml slow. log matric and then I'll log the msse right same goes with ML flow. log Matrix I log the R2 and then I log rmsc right so I've already logged these three things now what I'll do it's mostly done so now we actually can use this particular um statement over here but but before that let's import sorry sorry sorry sorry let's import something known as import flow cool so now what we have to do guess what what we have to do we have simply go to non run Pipeline and then simply run the same pipeline so let's just quickly run I've already uh done this so let's just run the pipeline first so once we run the pipeline it will say that we are using the mlow tracker and once once it says that that we using the ml tracker it will say something like this just no module names ml flow so what you can do you can simply go ahead and just just just like zml integration install ml flow which is simply you going go over here and then search something thing which is like this right and then paste it sorry for that um simply past over here which is zml integration install ml flow it will take some time to install the ml flow but before going on to running that you have to make sure that first of I'll explain you what the stack means stack means that there's something the stack which is a cont containerized thing where your project is running and the stack I I'll show you what the stack contains stack contains very artifact stores which are default okay orchestrator which is default you don't need to worry about what these terminologies means so but basically the thing which is a default the the stack which you're working on the what do you say stack means I'll say in terms of environment which you're working on you also need to stay to the ziml that I'm going to use ml flow please register this experiment tracker okay and just like this as in stack you have orchestrator orchestrator means will will talk about it which eventually it's called as pipeline however you have artifacts right artifacts will talk about all of these terminologies in very great detail um however you don't need to know lot more you all have so much theor theoretical books but basically we'll first of all install our MSO integration once it is installed as one as you can see over here we can simply go and register our experiment tracker so once you go and register our experiment tracker you can just say zimal expand track register ml flow track but before that what I'll do I'll show you what the ziml stack list and it will show the set of stacks which we have over here right and uh it's taking too much time I guess yeah okay so basically this is a very common error which you might get if you're using Mac so you just have to do a couple of things the first thing we have to do is zenal disconnect and then what you have to do you have to run another command which is zml up so when you dis disconnect it so basically it is giving another error which is error initializing SQL store error initializing whatsoever this is something new error however you can totally choose to ignore this right just say ZL up ZL up okay cool something is really interesting over here so down so maybe down it or we might need okay fair enough so so then then we'll up it and if it gives the same error we have to r on the zml disconnect maybe and then let's see if it works zml disconnect okayl let's for wait for a few seconds and it should work okay cool it's working pretty fine that's very very nice so now we have we have fixed it so let's let's just quick quick quickly go and let let me show you what this set describe me so current stack which we have as of now the current stack which we have as of now it will say that everything is default right so your orchestrator is default and artifact store is default right orchestrator where you're running an artifact store where your variables you can assume the artifacts which are being stored over there so let let's quickly go and then also make the experiment tracker so you can just go with the read me and then copy this command and then paste it which is ziml experiment tracker register our ml flow tracker which will have the flavor of MLF flow so it says that unable to register mlflow tracker which is in the same Works Space so let's quick quickly go and change something customer okay so now it should work fine because I guess I've already used it somewhere cool then you have to just go and uh okay let's just quickly ignore model deployer as of now okay we'll come back to this or let's just do one thing let's just quick do do this as as a long because this is important to do so uh we will come back what does this model deployer means and then we'll register okay so I'll just customer and the over here as well as customer just copy it so that it makes sense and it will set the deployer so it will just set something this okay fair enough so it says that unable to register the stack name full stack so again it is saying the ml stack name is registered because I've already used it in the past so what I'll do I'll simply make it customer customer here let's just quickly do it and then just wait for a few seconds so most probably it's done so now you have the ml so when you do zml stack describe now so once you do it you most probably see your stack over there so which is different solve which is and now your model deployer is mlflow customer and XM tracker is ml tracker so we have done this so let's now run the pipeline and then let's see what is what what this leads to run pipeline. pi and let's just wait for a few seconds to to complete the Run of it so it is initiating a new run and it is saying something really happens you're using unsupported version if you cter errors blah blah blah you just have to downgrade or something upgrade ml flow or whatever it is giving you can totally choose to ignore this but there's something interesting comes in as we see it that there's something interesting comes in so me might need to um maybe most probably my case say that maybe just search it quickly because I'm not sure what this eror means I'm so sorry for it what do this given no okay uhuh Okay so so scorer maybe let's just go and upgrade it what do you think so it says the warning and let's the warning says that try upgrading and downgrading the scale learned version to a supported version or try upgrading your ml flow okay so pip install PP install psych okay upgrade py law okay it's done and then what I can do I can just quickly go and then serve it over here which is pick install upgrade flow okay so we might need to upgrade a little bit version of mlflow and let's see if this if the error is still until processes if it processed I'll just see the one another solution which I have in my mind so most probably your mosts is being fixed by you know just upgrading reinstalling you know disconnecting then connecting restarting your laptop fixes the erors because sometimes you you don't know what is happening behind the back so you actually have to be very careful while initiating stuffs please okay so that that was a very simple letter we might have to you know so we might have to you know upgrade then it run completely fine okay cool that's nice so let's let's just quick quickly go and search default login pipelines this one this one and here we go so if you go and see the configuration you have the experiment tracker as well you might think two things as of now the first thing which you which which you might be thinking that hey a how where can I find my um where can I find my you know uh most probably my ex tracking URI you know how can I view the MLF flow stuff and and my own stuffs right so let's I'll tell you two things okay the first thing which I'll tell you that how you can track the URI and stuff like how you can view the experiments so let let's Qui quickly go and search about uh how you can track the URI right so let's just quickly go cing so there was I'm just trying to search one thing which was they had given a very nice code actually you know okay we we got it we we got it okay so what you have to do you have to just go quickly over there and then just go quick quickly over there and in your run pipeline over there and just paste this okay just paste this because you will get your URI wait for a few seconds let it run we have initiated our cach so it'll just use the cach version of everything okay cool so it says that your file is available over here maybe yeah it makes sense okay cool so um so your file is available over there now what now what we will do we'll just run mlflow UI backend so something like this I'll show you which is which you can find on official you know ziml page as well M URI and then you paste the URI which you got by pasting that code which is ml runs right let's just do it okay there might be some error okay let's let's just quick quickly run it you know it's something very important to run I guess it will give some sort of er I'm not exactly sure okay got an expected argument which is maybe we have to make it in like this file and it last as well let let's just see if it works in if it works then it's fine all right it works so let's just go and paste it over there and expect it to run so this is 3 3 minutes ago you just go over there you see the Matrix which is listed out here right you see the parameters you see the model which is literally logged in ml model right you can use this model to make prediction make ml flow to make predictions or even pan to make predictions you see how interesting this is this is pretty pretty amazing this just loged each and everything so I that's what I wanted to Showcase to you uh I hope it makes sense to you not what I'll do I'll just cancel it up you can just all these commands are available you know get don't need to worry about it cool so now we done with experiment tracker in the next video we'll just go ahead and then worry about something known as um deployment of our model I hope that will also sound well to you K and catch you in the next hey everyone welcome back to this video in this video what exactly I'm going to do I'm going to actually cover the last of the last thing which is deployment pipeline so we'll use the ml deployer to actually deploy our model locally so that you can use it uh and and make predictions right and we'll see how to deploy a model you might have seen that you just save the model use some fast tape applications slow load with job lib and then you do it that's really not true which happens in the production use case you actually use something known as mlon deployment or Seldon deployment pipelines stuff to do it so what I'm going to do I'm going to actually use something known as uh some something really known as um ml4 deployment which is entirely used for local deployment mostly use for local deployment for deployment on AWS or you know GC Cloud you might have to use S code because that's much more advanced deployment software but as of now let's just go with AML deployment software or sorry uh Tool uh to get started with that so the first thing which I'll do I'll go ahead and create something known as uh deployment pipeline okay a deployment pipe a deployment pipeline so let's let's just quick quickly go and uh make a deployment pipeline so what I'll do I'll just go ahead and then create uh deployment pipeline okay okay so over here I'll just scoll it down to looks good okay so run deployment run deployment dop right let let's just go there and then just first of all remove all of this because I you know the reason why I like to remove all of this because I think that gives me much more pleasure if I remove all of this because that seems like okay fair enough you have something uh off the load that's why I really really like this stuff okay cool so let's just go to run deployment and then in run deployment what I'll do so basically you might have already used uh click over there right so so basically we'll create two pipelines we'll create two pipelines the pipeline which we'll do so let's first of all create the pipeline as well which is deployment pipeline deployment uh pipeline Pi so in that deployment pipeline. Pi we'll make two pipelines which is continuous deployment pipeline I'll explain you what this continuous deployment pipeline means as well as inference pipeline later we'll explain what do it mean as of now let's just go in quickly as of now assume that continuous P pipeline it's like a traditional pipeline which we have built prior so let's just go from pipelines do deployment pipeline we we going to import some certain things so as of now let let's just go with this which is deployment Pipeline and inference pipeline okay so inference pipeline okay and then what I'll do I'll simply go ahead and create some do do one thing is I'm going to use click okay I'm I'm going to use click so that we can just state in a command that okay we want to deploy or we want to predict or whatsoever so I'll just go ahead and then create a click command which is click. command and click option click option would be click option would be uh so let me just quickly copy it because that's something is easy ra rather than I write whole set of thing so let me just copy it over here okay so click man is conf right config and then it will say okay in config what do you want to choose you want to choose deploy or you want to choose predict or you want to choose deploy predict so let me just quickly write over here deploy predict and deploy and predict okay so you can actually state in like this python run deployment pipe run deployment. py do SL sorry uh Das Dash config and then you want to deploy or predict you can simply write it over there and then you have minimum accuracy we'll come to come come to what this minimum accuracy means in some time so uh then what then what I'll do I'll create something known as this which is run deployment and that config is Str Str and minimum accuracy is of course number we we'll come to what this minimum accuracy means uh okay so we'll just go ahead and write float okay and over here what I'll do if it says deploy if it says deploy what I'll do I'll run the deploy Pipeline and if it says predict I'll run the inference pipeline I'll run the inference pipeline okay um so let's just quickly get started with it so now now you might be thinking hey is it done no of course not we have to actually build this deployment pipeline as well as the inference pipeline explain you what this minimum accuracy means so let's just go over there and quickly create our deployment pipeline right away so I'll just import import numai as NP import pandas as PD from zml import pipeline comma step and then from uh let's let's quickly import all of this from zl. config um you'll see where where we'll use this Docker settings Docker settings zl. config right that's correct okay so and then what I'll do I will just import some something which is mlflow deployer so I I'll just copy and paste all of this things so that it's much more easy as of now you can just forget it what does it mean and stuff we we'll come back to this later on so let me let me just quickly go and just copy and paste it over here so we have imported from zml con constants we'll actually make use of all of this please don't worry about it we we have also imported our um Steps From the steps which is clean data in that clean data we have imported clean DF so let's let let's just go and import clean DF then you have evaluation in evaluation you have evaluate model so let just go and write evaluate model inest data you have ingest DF so let's just go and import that as well and in model train you have train model which is already there okay so now what now what I do I'll just first of all so basically I want to train the model right sorry I want to deploy the model as well as if the model is good in accuracy we'll deploy it okay so let's and also we'll also create the docker setting so Docker setting is like what are the libraries or the tools which we need over here so in Docker setting the required Integrations which is the the the required Integrations which we have Integrations which is equals to uh only mlflow right so we we have we want to use only ml flow Library into this okay now what I'll do I'll create something known as I I want to actually use the model you know I want to use the model to um to actually deploy the to actually make the predictions but before that I'll I'll create the basic pipeline which is the continuous deployment pipeline so I'll explain you what this continuous deployment P pipeline means let's just goe first of all so pipeline comma we have to enable cache equals to true we want to enable the caching and then settings is equals to which is Docker first of all so we'll use Docker settings right stalker settings and then that's it that's that's pretty much it so we'll create a deployment pipeline so continuous I guess the spelling is correct continuous deployment Pipeline and then over there we'll have first of all minimum accuracy we'll come that what does minimum accuracy means uh then then after that um we'll have workers number of workers which we need and then we have timeout so timeout means how much like like what what will the required amount of if it is in Loop then at how at how much time we should stop the run right so when the time out should be there so so that's why we have imported this default service start which is from constant we have imported this defa default service start stop time time out so that we can actually stop the pipeline if it is taking too much okay so first of all let's just quick quickly go in and then just run the in justf we have actually imported over there and then we have xtrain so let's just quick quickly go over uh training pipeline and in trading pipeline let let's just import everything out Okay cool so let's I've imported everything which is we have the R2 score as well right so now um so you have the R2 score now what I'll do I'll create um what do you say the deployment uh the the deployment which is the deployment decision okay so now now we have once we have the rmsse now we have the trained model so now what we have to do we have to actually deploy the model so there should be some criteria for deploying our models what is that criteria criteria can be if your model is great if your model accuracy is greater than the minimum accuracy which which is required to deploy the model then only deploy the model that's where your minimum accuracy comes in place so let let's just quickly go and create something know as deployment trigger okay so the deployment decision will depend on this deployment trigger so first of all let's create a step called deployment decision okay so um so just quick quickly over there so I'll create a class and the class will have doc sorry deployment trigger config in that we have the base parameters right base parameters and then the minimum accuracy the the minimum accuracy of it we we'll change it don't worry we'll change it as if I'm just adding a random number we'll create a step and the step will say is the Define define deployment trigger trigger and that trigger first of all the accuracy uh which will be a float and config and the config from where we are using that the config so basically we need a config right so we need a config deployment trigger and so you have it's simp so basically what does it does let let me just write it implements a simple model deployment trigger that looks at the at the input model accuracy and decides if it is good enough to deploy or not okay so this is a very basic deployment trigger it says first of all it will return the accuracy greater than or equals to config do so basically so basically we we'll make use of the deployment out here so basically I'll I'll tell you what is so the we'll create a deployment decision deployment decision and deployment decision will contain the deployment trigger and let's use as of now rmse rather than R2 score so if if you don't know about R2 score you can just go online and search about it like space it's it's like it it it indicates like whe whether it's a goodness of it or not okay then then when you actually actually use it right so let's just go with something known as um MSE or rmse right or maybe let maybe let's let's go to R2 score because that's more good okay so now we have the deployment decision now so what it does it takes the R2 score and then it takes the which is the minimum accuracy which is required minimum R2 score which is 0.992 it only deploys this particular model if and only if if the deplo deployment decision true how does it evaluate first it checks that your R2 score is greater than this or not if it is then when it deploys the model then it will go to the next step the next step is mlflow model deployer step so what is mlflow model deployer step so let's I'll I'll show you what does it mean mlow model deployer step which is over here so we actually import from Zen integration M steps we actually use this em model deployer steps which actually is that we is already prebuilt step we can actually use that to deploy our model so that we'll have to give certain parameters so what is the model what is the deployment decision is the deployment decision workers which we need so workers is equals to workers timeout is equals to timeout Okay cool so now we have the deployment pipeline done now we can actually use this for inference so for for running our deployment pipeline okay now um so now I think we are mostly done with it right so let's just quickly go and run our deployment continuous deployment pipeline that's much more good to get started off with and then we'll come back to uh building up our inference pipeline so let's just quickly go to run deployment pipeline over there and um uh yeah so let's just quickly go over there and uh now what I'll do I will simply this is the config which we have right and this is the minimum accuracy which is required for us to deploy our model so we'll create something something known as mlflow model deployer component so this component will will be like em deployer so let me just quickly go ahe and then import with there and each and every libraries which we need technically yes so let's just import the libraries which is required I'll just paste it from my repository okay so basically we'll actually use the first one which is ml4 deployer which is ml4 deployer component and then this will ml4 deployer get active model deployer this will take the active model deployer out there and then deploy is equals to config is equals to deploy uh deploy or conf config isal deploy this okay so if the deploy is this or this it will run the deploy if the predict is this or this it will run the protect cool so let me just quickly import my from pipelines import continuous continuous deployment pipeline that continuous deployment pipeline will be the following so let's just take that and then continuous deployment pipeline that deployment pipeline will contain the minimum accuracy which is required and then after that it will have following workers which let's may name it as a three and time out maybe 60 seconds okay so this is our uh continuous deployment pip plane so now what what we can do we can actually you know um use it so let's let's quickly let's let me let me just copy it from the uh repository so now we we'll have the we'll make the predict one very soon but but let's just write it out so we can say that you can run your so this is the thing which which I've copied from um Zen M repository so it says you can run your ml for UI it Tak the so so you can see the visual representation of your models right and then what it then then what it does it Fetch and then we have to fetch the existing services with the same pipeline step name and model name right so that we can say that if there is any existing services are there so I've just copied it from their uh own mlflow examples repository because these are mostly same so basically we are fetching the existing Services if it is running if there's an existing Services running or not right and then we have employ deployer step and then model which is the model name if the existing services are there then then we say that the existing Services is running locally as a Derman to stop the service it will like this and if the service is failed then it says the ml for service is failed or you just say there's no ml for prodiction server is running right so there just use the deploy model to get started with so this is a very basic run deployment um as we don't have the inference pipeline so we don't need to worry too much about uh so let's just quickly go and run this pipeline up right and then we are and then we are mostly done so let's run this Pipeline and we have the minimum accuracy right so we don't need to really worry about stuff Okay cool so let's go and run python run deployment uh config and in that config going to deploy the model let's see if it gives any error if it gives then we'll solve it right away so it says that materializer is not found like material there's no module named must materializer where in in deployment pipeline so are we using the materializer I guess yeah okay let's remove this we not using it you don't need to worry about it just go ahead let's solve the error which it is going to give okay it says that invalid settings can either refer to this this this invalid setting doger settings settings can be refer to the general settings or stack component there there might be some error interesting error so let let's see first of all if where it is giving eror okay it is giving in deployment Pipeline and in deployment pipeline what is it giving it's that's that's why I say that most of our time will go into this only just by solving these pretty errors so where it is bro okay fair enough so we actually have to write settings to be Docker not Docker settings right and mostly we done okay fair enough okay cool let's run it now it can be available keys are either resources or Docker so we need to have the docker one over there rather than Docker settings okay fair enough so it gives the main why it gives the main so we have run deployment rather than main so let's run that sorry for that let's wait please run whoops so it says confix so basically I there is some naming error I'm so sorry for that again no problem you know these things you know very silly errors which I do you know this rectifies this I really want a tool that rectifies us all of this naming errors or you know import errors and all the stuff okay it says that wrong arguments emo deployer got an unexpected argument called deployment decision what is it so where does it gets an error it gets in done deployment and then it says okay fair enough so it says in continuous in that continuous you have the ml flow deployment decision and then it says that okay fair enough so let's let me just quick quickly go okay so basically it's actually not deployment decision it's deploy decision not deployment decision Okay Okay cool so let's just run wait I hope it works diplom decision is not defined so why did okay fair enough again I did the big mistake it should be over here not there I'm so sorry for it for these mistakes because these is this is something you know when when things are super occupied when you know things out there these mistakes happens so it initiates the new run it says missing entry point input data path so let's input our data path where to input the data path okay I'll I I I'll add it over here only because I guess that's more important right or let's do one thing let's write over here data path St Str and then let's just go over there and then also add the data path just copy it from directly over here I just quickly replace it okay I hope it works now if it does not then we again have to fix something inference is still left so stay tuned for inference and mostly we'll be done by then missing empty Point data path why where it is getting I guess okay okay okay again we made a good error so we have to actually put the data path to be data path sorry sorry for that this little little erors keeps keeps on happening so you have to actually debug it and see where the your VAR code is running and stuff it is using the cached version again I'm saying it's like ml deployer okay so it says that an mlflow model with the name was not logged in the current Pipeline and no running ml4 server was found please ensure the pipeline includes a step with the ml4 experiment to configure that trains a model and locks it to that so most probably what I feel that we have to make it false right and then let's run it now if it does not then you know then then we then then we'll get in a big trouble now if it does not deploys the model we are going to get in big trouble if it is not please let's wait know this is something where I just pray you know that it works out because this is a step where you get most of the errors and if some unknown error happens then just you have to actually spend your ton of time in it ton of people has to spend your time in it you know uh because it's not a very simple thing actually go inside your system and see if it works or not now see what is happening it says no materializer is registered for type linear regression so default pickle materializer was used it's not production easy so not we can blah blah blah so basically we have to actually make a materializer I'll show you how to make it later on I'm just waiting for it to deploy our model mlflow model deployer step and if it is and if it goes above service demo on is not running okay something really happened now so it says the fail to start the MSO deployment service model serving ml flow uhu okay for more information on the status please see the logging file okay something really interesting happen so basically timed out happened over there okay let's just go and see if what we can do in this case so what we can do is fail to start the service ml flow deployment service demon is not running okay so zml up NL stack describe is there anything which we did wrong in the deploy we have the the following why is giving the worst error okay fair enough so something is really interesting H happening over here we actually have to run deployment okay let's try to run it if you see it it like this this might be causing some problem this this warning okay okay okay skipping model deployment because the model quality does not match the criteria so again it will say the same thing so basically what's really happening that it is not matching the criteria so let's let's write 0.5 this is that skipping model because the model called does does not match the criteria using last s deployed by step and continuous for model so I guess that's that wasn't like it it was not meeting the criteria maybe that's why yeah so let's try to I have reduced my minimum accuracy or maybe I have not so minimum accuracy 0.5 let's wait now we just we can do just one thing just wait for it and see if it works so basically I'll tell you what happens is in these type of cases you have actually concentrate more you know you have to see actually what's going wrong what might go wrong even the smallest thing even restarting your laptop really works I literally seen a I was soling an error for two days and I saw okay fair enough like I restarted the laptop and it works like a charm so yeah so just just just wait for a few seconds and let's see if it works or not deployment trigger start ml deployment service skipping model because the model does not meet the criteria my goodness so it's really interesting things that happening out here right so uh what I'll do I'll just quickly go over through the code and let's see if it works and let's see if this let let it work right and then we'll just go and see what happens over there okay so we have the deployment existing services and uh okay so it will of course not work because this does not Tak too much of time right so I'll just you know go ahead and then see if it works on your side ml4 deployment service and okay and then you just go into deployment pipeline in that deployment pipeline you have the get data for ML slow deployment service parameter steps service demo is not working fair enough I I I get the words the error is about I get it what's the error is about step parameters if your accuracy is greater than then config do minimum accuracy and your minimum accuracy is this one 0.5 okay fair and then you have the MLF flow model load step parameters so basically we have pipeline step name running okay so let's just copy this m deployment loader steps as well so what it does it helps you to get the get all the stuff which is ml deployment and then we have the prediction service loader and predictor which will come to in some details you know I've already written this these code out actually right so let's just go and then just you know okay fair enough so I'll try I'll try to run one more time maybe right so I'll try to run one more time and then see if it works but before that what'll do I'll just check the everything is working fine on that site which is run deployment and in that run deployment you have the following ml flow deployment services and then you have the to get which is Zen model deployers MSO model deployer do we have the Emon model deployed okay we have the get tracking U and then okay fair enough so I'll just run it try it nicely again let's see if it works or not if it does not then we actually have to go inside and talk to zenel team and then see if it works because you know we have to actually have the continuous talks to the zml you know because because it's something you know some something which should be which maybe have which may be bit common in their side and which may have they may have solution to it or we may have to open the GitHub issues and then may most probably will have the error solved because this is how we solve it it's just we we are not expert in this we just go to some people and then talk to them about it right we'll try we'll try one one more solution which is this one we'll try do this solution which is the L linear regression model okay fair enough again it just just does not matches the deployment criteria so I want to see the R2 score R2 score is so bad bro okay fair that's why it was not given okay okay okay so I guess R2 score is very bad that's why it is not giving good errors so I'll just add zero maybe this this this might run zero means like we I I want to just showcase you that that it deploys the model right so now I'll go and run it R2 score is zero right so of course the R2 score is greater than that so yeah why it is zero bro something really interesting cases happening of with me really interesting Okay cool so it's weet and also we'll fix that um which is this one we'll fix that if these things does not Works bet please run bro run no materializer is registered that that that that that that is common deployer updating an existing mblo deployment service which is this one and let's see if it this works if it does not which is like it met the criteria now it met the area now let's see what it does it should work you know but if it does not we'll come back and then see what it works or not okay let's see if it works or not updating an existing ml flow deployment service at this this this this stage I think it will mostly not work because this does not takes this much time it will say that a demon is not working blah blah blah we might need to do something we'll try one one more solution which I have in my mind okay service dayon is not running uh for more information please see the following lock file so I'll just check it out and then come back very soon so everyone there was a very simple error so basically my I've already tried this mlro on couple of environments that's why it was like service that we have the current service running we cannot actually use that so what I done I actually deleted that uh then I you if you were working on a new the new uh you might be working on new stuff right you might working on new stack right so we have to actually use the new stack that's why it was giving me error now it is working to totally fine the only thing which is is not working fine is the following so let's just go and uh fix that thing so basically what what we need to do we need to import something as cast and let let me go and just import that from typing import cast from typing import cast okay fair enough so we'll just run it deploy and then let's just see if it works you can totally choose to ignore the mornings and stuff or you can just go and solve that thing if you want so it ingested data first after it is ingesting the data it cleans the data data cleaning is completed it goes to the next step which is trains the model trains the model then gives some sort of warnings you can totally choose to ignore this or maybe see if it works if model training is completed train model has finished and it gives some you know root squares and then deployment trigger has started deployment enow models step has started it updates an existing ml development services right it starts with the ml development services and most latest times right it starts the service so let's just go in then see if it works hopefully it should work if it does not then I'll just you know take his ass off okay I'm so sorry for that okay so now your model is available you can make the pred make your prediction over here because it is already running you can also delete the model if you want so now your model is successfully deploy it so now we need to do we need to actually make predictions from this model so I what I'll do I'll quickly go ahead and then create something known as um I'm so sorry for it go to deployment pipeline in that deployment pipeline first of all we have this MSO deployment loer step which will help us to load that model okay and then let's go and then start doing stuffs so uh we'll just go ahead and then create something define prediction service loader right so I'll just copy and paste the code if you want but yeah but okay it's like I already this is already prewritten Okay so let's just go and then write the prediction service load out so we'll create a step where we'll enable the caching equals to false because sometimes caching is also not very good then we'll create the prediction service service lower and in that we'll have the pipeline name pipeline name name will be St Str pipeline step name pipeline step name will be also also St Str is it running boom equals to true and then model name to be model okay and then it Returns what it returns it returns mlflow deployment service okay it Returns the ml flow deployment service so basically it gets the prediction service started by it it gets the prediction service over here just copy and paste it yeah so it gets the prediction service started by the deployment it takes all of these arguments in it so first of all get the ml flow deployer stack component so basically we'll get the mlflow deployer stack component so which is very simple get active model deployer over here and then what I'll do I'll existing fetch existing services with the same pipeline name and model name so what I'll do I'll go existing Services which is mlso MSO deployer which is MSO model deployer component. find model server and in that we pip pipeline name to pipeline name pip pipeline step name model name and running right so if there is running running running to be running that's it that's pretty much it so if not existing Services then we say we raas the runtime error and in that runtime error we say that no MSO Services is found which is like this no step in this Pipeline and something like this you know pipeline for the model name is not deployed so I just copy and paste the errors which is so traditional errors found from you can just just go at you know zenel examples and then just copy and paste there it's not a big thing then you print the existing services or maybe it's not then then then you return what you return you know you return existing services so you return the services by using so it is actually prediction service loader it loads the uh current prediction service so to actually use this for model predictions now what I'll do I'll create the predictor so I'll create the predictor the predictor will have the service that service would be the mlflow deployment service type of that and then it then then then it takes the NP Dot and the array right and it returns those array of predictions and b. ND array so what is Arrow of array of predictions so we'll first of will create a step over here as well that step will be of dynamic data importer so that will create the step of enable cache equals false then we'll create a dynamic importer that returns s Str that returns uh string right so it downloads the data from the first of all downloads the data from a mock API or maybe just just just go ahead and create data is equals to get data for test and it on data so we have to actually build this quickly so let's just go and quick quickly build this which is utils utils p let's go there and run it so we'll just have import login import andas SPD from source. model to sorry data cleaning input data cleaning and data process strategy preprocess strategy and then we'll just use this much get data for test where we first of all get the data for the test and then we want the 100 one we actually clean the data we drop the review score we convert into date Json format that's why it is the it is returning St Str okay now what I'll do I'll simply go ahead and uh make this so let's just go quick quickly make this so I've already made this let me just copy and paste that because this is pretty simple to understand okay so first of all it starts the service um and then it loads the data it removes some of the column it for the columns which we want from the data we create we convert into P data frame we convert in list and then we finally convert that Json list to a nump array and then we make the prediction from that service okay I hope it makes sense now okay fair enough so let's just go and run it now so we mostly done now what now we have the prediction service loader now at last we'll create the inference pipeline so that inference pipeline will have okay sorry pipeline will enable the C setting the docker then then I'll create the inference pipeline that inference pipeline will contain the pipeline name which we want and the pipeline step name right which is Str Str so it first of all uses the dynamic importer Dynamic importer right and then it service which is the production service loader it gives the pipeline and running equals to false let's just write running equals to false as of now and the new prodution production should be predictor is this is service equals to service and then data like this and then we mostly down okay so we have the data over there and then we have the service over there now we give the service it uses that service from the from this and then make predictions on this data using predictor right which is like predictor you might have seen over here okay cool so now we we mostly done let's just go to run deployment and then run the pipelines so let's just go to run deployment and then we'll just import our inference pipeline out there so I just go and import inference Pipeline and once we import the inference pipeline we'll just go ahead and uh run our inference pipeline so yeah the pipeline the pipeline names should be continuous deployment pipeline right and then your pipeline step name which is mem deployer step I guess it should work now most probably okay so now we have done that so let's just run the predict it's so tiring please fix this error I want you all to fix this error this is very basic you just have to write ZL downgrade fair enough so we get the error nice nice nice that that is expected from uril import get data for test I'm very happy that uh that you so one thing which I'll tell you that it is very like that you sometimes don't understand it right because this is something very conceptual very technical things out there so I want you to be very strategic in understanding stuffs right so please be if if you're not able to understand it that's totally all right so it says that data process strategy is not there okay preprocess strategy is not defined what does it mean name the pr process strategy mhm if it is game gives th you have to worry about it this is something really interesting what happened I really hope that it does not give that now Yahoo okay so builin materializer can M write unable to handle class numpy and the array uh builin materializer can only handle the artifacts of the following so it gives some matter let let's go and fix this up this is so tiring um okay let's go to where built in in the predictor okay where is predictor in deployment it gives in predictor andd array array sorry this was St Str by num so basically the the data which you're getting is Str Str not an and see if it works if it is not then we have to worry about now uhuh okay Json is not defined import Json anything else please give me the error Fast Pro I'm I'm I'm so worried about errors fix this up so just say just say it's animal down downgrade it it should fix please yeah so we are done so now we actually completed our stuff right so let's just go over here enjoy so you have the dynamic importer prediction service load of dynamic import outputs and then service load output something which is a service and then this uses output and output output of the which is the data for test and this for service and then uses both of them to make the predictor and this actually the predictor outputs the following so if you go and see the visualization you see something really interesting that your predictions has been made over there so I guess this is not showing any V visualization because of some my error so you see that your mean is standard deviation of predictions is this right so actually it made the predictions out there okay I hope it makes sense now we have deployed the model made the predictions too now you might be thinking here how can I make the single handle had single handle predictions right so we are done with the deployment and inference as well now it is actually making good uh inference out there it is actually predicting but it might happen that you're still confused so let's just not have too much of confusion in your head and then fix that confusion too so I have actually made um simple streamlet dot send it app dop appy okay paste it right right I hope it works mostly so you from the deployment pipeline it Imports the prediction service loader and then run deployment from run deployment it Imports let's make it main rather than R right Main and then over here as main okay so everything is same the only thing which is if the sub person clicks the predict button it will go to the prediction service gives this and then it says that if the post service is there then creates the DAT data frame does the same step which which we have data and predictor and predict from the data right so let's run the streamlit Run streamlet app.py please run fair enough high level overview is not there so I'll just make sure that I remove all the images okay any images M okay let's run it now let's wait is it is running cool so I'll just make it z z everything so it's now it giv prediction so basically your detail is 4.22 so basically it is actually using the production from the model so you see that we haven't even saved the model save saved the model is not there it's actually using it it's actually the pipelines over there if you go and see the pipelines of our so let's go to pipelines and then let's go to continuous deployment pipeline The Continuous deployment pipeline you have the following inference pipeline which is done right and then let's go behind back okay let's just go continuous deployment continuous deployment continuous deployment and in that continuous deployment you see the continuous deployment is also there right so whole P pipeline is done so we are done with one project right I hope it was really good project for you uh I understand it cool so let's just go ahead and then that's it that's the app of the project in the next project we'll actually use something known as customer CH or maybe let's cover the next project up to catch you later byebye
