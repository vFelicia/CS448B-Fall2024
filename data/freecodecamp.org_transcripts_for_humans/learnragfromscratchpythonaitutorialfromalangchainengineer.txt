With timestamps:

00:00 - in this course Lance Martin will teach
00:02 - you how to implement rag from scratch
00:06 - Lance is a software engineer at Lang
00:08 - chain and Lang chain is one of the most
00:10 - common ways to implement rag Lance will
00:14 - help you understand how to use rag to
00:16 - combine custom data with llms hi this is
00:19 - Lance Martin I'm a software engineer at
00:21 - Lang chain I'm going to be giving a
00:23 - short course focused on rag or retrieval
00:26 - augmented generation which is one of the
00:28 - most popular kind of ideas and
00:30 - in llms
00:32 - today so really the motivation for this
00:34 - is that most of the world's data is
00:37 - private um whereas llms are trained on
00:41 - publicly available data so you can kind
00:43 - of see on the bottom on the x-axis the
00:45 - number of tokens using pre-training
00:47 - various llms so it kind of varies from
00:50 - say 1.5 trillion tokens in the case of
00:52 - smaller models like
00:54 - 52 out to some very large number that we
00:56 - actually don't know for proprietary
00:58 - models like GPT 4 CLA
01:00 - three but what's really interesting is
01:03 - that the context window or the ability
01:06 - to feed external information into these
01:08 - LMS is actually getting larger so about
01:11 - a year ago context windows were between
01:13 - 4 and 8,000 tokens you know that's like
01:16 - maybe a dozen pages of text we've
01:18 - recently seen models all the way out to
01:20 - a million tokens which is thousands of
01:22 - pages of text so while these llms are
01:24 - trained on large scale public data it's
01:28 - increasingly feasible to feed them
01:30 - this huge mass of private data that
01:33 - they've never seen that private data can
01:35 - be your kind of personal data it can be
01:37 - corporate data or you know other
01:40 - information that you want to pass to an
01:42 - LM that's not natively in his training
01:45 - set and so this is kind of the main
01:48 - motivation for rag it's really the idea
01:52 - that llms one are kind of the the center
01:54 - of a new kind of operating
01:56 - system and two it's increasingly
01:59 - critical to be able to feed information
02:01 - from external sources such as private
02:04 - data into llms for processing so that's
02:07 - kind of the overarching motivation for
02:10 - Rag and now rag refers to retrieval
02:13 - augmented generation and you can think
02:14 - of it in three very general steps
02:17 - there's a process of indexing of
02:19 - external data so you can think about
02:22 - this as you know building a database for
02:24 - example um many companies already have
02:27 - large scale databases in different forms
02:29 - they could be SQL DBS relational DBS um
02:32 - they could be Vector Stores um or
02:35 - otherwise but the point is that
02:38 - documents are indexed such that they can
02:40 - be retrieved based upon some heuristics
02:43 - relative to an input like a question and
02:45 - those relevant documents can be passed
02:47 - to an llm and the llm can produce
02:50 - answers that are grounded in that
02:52 - retrieved information so that's kind of
02:54 - the centerpiece or central idea behind
02:56 - Rag and why it's really powerful
02:58 - technology because it's really uniting
03:01 - the the knowledge and processing
03:03 - capacity of llms with large scale
03:06 - private external data source for which
03:09 - most of the important data in the world
03:11 - still
03:13 - lives and in the following short videos
03:16 - we're going to kind of build up a
03:17 - complete understanding of the rag
03:20 - landscape and we're going to be covering
03:22 - a bunch of interesting papers and
03:24 - techniques that explain kind of how to
03:25 - do rag and I've really broken it down
03:28 - into a few different sections
03:30 - so starting with a question on the left
03:32 - the first kind of section is what I call
03:35 - query trans translation so this captures
03:37 - a bunch of different methods to take a
03:39 - question from a user and modify it in
03:42 - some way to make it better suited for
03:44 - retrieval from you know one of these
03:45 - indexes we've talked
03:47 - about that can use methods like query
03:50 - writing it can be decomposing the query
03:52 - into you know constituent sub
03:55 - questions then there's a question of
03:57 - routing so taking that decomposed a
04:00 - Rewritten question and routing it to the
04:02 - right place you might have multiple
04:03 - Vector stores a relational DB graph DB
04:06 - and a vector store so it's the challenge
04:08 - of getting a question to the right
04:10 - Source then there's a there's kind of
04:12 - the challenge of query construction
04:15 - which is basically taking natural
04:16 - language and converting it into the DSL
04:19 - necessary for whatever data source you
04:21 - want to work with a classic example here
04:23 - is text a SQL which is kind of a very
04:25 - kind of well studied process but text a
04:28 - cipher for graph DV is very interesting
04:31 - text to metadata filters for Vector DBS
04:34 - is also a very big area of
04:36 - study um then there's indexing so that's
04:40 - the process of taking your documents and
04:42 - processing them in some way so they can
04:44 - be easily retrieved and there's a bunch
04:46 - of techniques for that we'll talk
04:47 - through we'll talk through different
04:48 - embedding methods we'll talk about
04:50 - different indexing
04:51 - strategies after
04:53 - retrieval there are different techniques
04:56 - to rerank or filter retrieve documents
04:59 - um and then finally we'll talk about
05:00 - generation and kind of an interesting
05:02 - new set of methods to do what we might
05:05 - call as active rag so in that retrieval
05:08 - or generation stage grade documents
05:12 - grade answers um grade for relevance to
05:15 - the question grade for faithfulness to
05:18 - the documents I.E check for
05:19 - hallucinations and if either fail
05:22 - feedback uh re- retrieve or rewrite the
05:25 - question uh regenerate the qu regenerate
05:28 - the answer and so forth so there's a
05:30 - really interesting set of methods we're
05:31 - going to talk through that cover that
05:33 - like retrieval and generation with
05:37 - feedback and you know in terms of
05:39 - General outline we'll cover the basics
05:41 - first it'll go through indexing
05:43 - retrieval and generation kind of in the
05:44 - Bare Bones and then we'll talk through
05:47 - more advanced techniques that we just
05:48 - saw on the prior slide career
05:50 - Transformations routing uh construction
05:52 - and so forth hi this is Lance from Lang
05:55 - chain this the second video in our
05:57 - series rack from scratch focused on
05:59 - indexing
06:01 - so in the past video you saw the main
06:04 - kind of overall components of rag
06:06 - pipelines indexing retrieval and
06:09 - generation and here we're going to kind
06:10 - of Deep dive on indexing and give like
06:13 - just a quick overview of it so the first
06:16 - aspect of indexing is we have some
06:18 - external documents that we actually want
06:20 - to load and put into what we're trying
06:22 - to call Retriever and the goal of this
06:25 - retriever is simply given an input
06:27 - question I want to fish out doents that
06:30 - are related to my question in some
06:32 - way now the way to establish that
06:35 - relationship or relevance or similarity
06:37 - is typically done using some kind of
06:39 - numerical representation of documents
06:42 - and the reason is that it's very easy to
06:44 - compare vectors for example of numbers
06:47 - uh relative to you know just free form
06:50 - text and so a lot of approaches have
06:53 - been a developed over the years to take
06:56 - text documents and compress them down
06:58 - into a numerical rep presentation that
07:01 - then can be very easily
07:03 - searched now there's a few ways to do
07:06 - that so Google and others came up with
07:08 - many interesting statistical methods
07:11 - where you take a document you look at
07:13 - the frequency of words and you build
07:15 - what they call sparse vectors such that
07:18 - the vector locations are you know a
07:20 - large vocabulary of possible words each
07:23 - value represents the number of
07:24 - occurrences of that particular word and
07:27 - it's sparse because there's of course
07:28 - many zeros it's a very large vocabulary
07:31 - relative to what's present in the
07:32 - document and there's very good search
07:34 - methods over this this type of numerical
07:37 - representation now a bit more recently
07:40 - uh embedding methods that are machine
07:41 - learned so you take a document and you
07:43 - build a compressed fixed length
07:45 - representation of that
07:47 - document um have been developed with
07:51 - correspondingly very strong search
07:53 - methods over
07:54 - embeddings um so the intuition here is
07:59 - that we take documents and we typically
08:01 - split them because embedding models
08:04 - actually have limited context windows so
08:07 - you know on the order of maybe 512
08:09 - tokens up to 8,000 tokens or Beyond but
08:12 - they're not infinitely large so
08:14 - documents are split and each document is
08:16 - compressed into a vector and that Vector
08:20 - captures a semantic meaning of the
08:22 - document itself the vectors are indexed
08:25 - questions can be embedded in the exactly
08:27 - same way and then numerical kind of
08:30 - comparison in some form you know using
08:33 - very different types of methods can be
08:34 - performed on these vectors to fish out
08:37 - relevant documents relative to my
08:41 - question um and let's just do a quick
08:43 - code walk through on some of these
08:46 - points so I have my notebook here I've
08:51 - installed here um now I've set a few API
08:55 - keys for lsmith which are very useful
08:58 - for tracing which we'll see
09:00 - shortly um previously I walked through
09:03 - this this kind of quick start that just
09:04 - showed overall how to lay out these rag
09:07 - pipelines and here what I'll do is I'll
09:09 - Deep dive a little bit more on indexing
09:11 - and I'm going to take a question and a
09:13 - document and first I'm just going to
09:16 - compute the number of tokens in for
09:18 - example the question and this is
09:19 - interesting because embedding models in
09:22 - llms more generally operate on tokens
09:24 - and so it's kind of nice to understand
09:27 - how large the documents are that I'm
09:28 - trying to feed in in this case it's
09:30 - obviously a very small in this case
09:33 - question now I'm going to specify open
09:35 - eye embeddings I specify an embedding
09:38 - model here and I just say embed embed
09:40 - query I can pass my question my document
09:43 - and what you can see here is that runs
09:47 - and this is mapped to now a vector of
09:49 - length 1536 and that fixed length Vector
09:53 - representation will be computed for both
09:56 - documents and really for any document so
09:58 - you're always is kind of computing this
10:00 - fix length Vector that encodes the
10:02 - semantics of the text that you've passed
10:05 - now I can do things like cosine
10:06 - similarity to compare
10:08 - them and as we'll see here I can load
10:13 - some documents this is just like we saw
10:16 - previously I can split
10:18 - them and I can index them here just like
10:22 - we did before but we can see under the
10:23 - hood really what we're doing is we're
10:25 - taking each split we're embedding it
10:27 - using open eye embeddings into this this
10:29 - kind of this Vector representation and
10:31 - that's stored with a link to the rod
10:33 - document itself in our Vector store and
10:36 - next we'll see how to actually do
10:38 - retrieval using this Vector store hi
10:41 - this is Lance from Lang chain and this
10:43 - is the third video in our series rag
10:45 - from scratch building up a lot of the
10:47 - motivations for rag uh from the very
10:50 - basic
10:51 - components um so we're going to be
10:54 - talking about retrieval today in the
10:56 - last two uh short videos I outlined
10:59 - indexing and gave kind of an overview of
11:01 - this flow which starts with indexing of
11:04 - our documents retrieval of documents
11:06 - relevant to our question and then
11:07 - generation of answers based on the
11:09 - retriev
11:11 - documents and so we saw that the
11:13 - indexing process basically makes
11:15 - documents easy to retrieve and it goes
11:18 - through a flow that basically looks like
11:20 - you take our documents you split them in
11:22 - some way into these smaller chunks that
11:25 - can be easily embedded um those
11:28 - embeddings are then numerical
11:30 - representations of those documents that
11:31 - are easily
11:32 - searchable and they're stored in an
11:35 - index when given a question that's also
11:38 - embedded the index performs a similarity
11:41 - search and returns splits that are
11:43 - relevant to the
11:45 - question now if we dig a little bit more
11:47 - under the hood we can think about it
11:49 - like this if we take a document and
11:52 - embed it let's imagine that embedding
11:54 - just had three dimensions so you know
11:56 - each document is projected into some
11:58 - point in this 3D
12:00 - space now the point is that the location
12:03 - in space is determined by the semantic
12:06 - meaning or content in that document so
12:09 - to follow that then documents in similar
12:12 - locations in space contain similar
12:15 - semantic information and this very
12:17 - simple idea is really the Cornerstone
12:19 - for a lot of search and retrieval
12:21 - methods that you'll see with modern
12:22 - Vector stores so in particular we take
12:25 - our documents we embed them into this in
12:27 - this case a toy 3D space
12:30 - we take our question do the
12:32 - same we can then do a search like a
12:35 - local neighborhood search you can think
12:37 - about in this 3D space around our
12:39 - question to say hey what documents are
12:42 - nearby and these nearby neighbors are
12:45 - then retrieved because they can they
12:47 - have similar semantics relative to our
12:50 - question and that's really what's going
12:53 - on here so again we took our documents
12:56 - we split them we embed them and now they
12:58 - exist in this high dimensional space
13:00 - we've taken our question embedded it
13:02 - projected in that same space and we just
13:04 - do a search around the question from
13:07 - nearby documents and grab ones that are
13:09 - close and we can pick some number we can
13:12 - say we want one or two or three or n
13:14 - documents close to my question in this
13:17 - embedding space and there's a lot of
13:19 - really interesting methods that
13:20 - implement this very effectively I I link
13:22 - one
13:24 - here um and we have a lot of really nice
13:28 - uh Integrations to play with this
13:30 - general idea so many different embedding
13:32 - models many different indexes lots of
13:35 - document loaders um and lots of
13:37 - Splitters that can be kind of recombined
13:39 - to test different ways of doing this
13:41 - kind of indexing or
13:43 - retrieval um so now I'll show a bit of a
13:45 - code walkth through so here we defined
13:50 - um we kind of had walked through this
13:52 - previously this is our notebook we've
13:54 - installed a few packages we've set a few
13:57 - environment variables using lsmith
14:00 - and we showed this previously this is
14:02 - just an overview showing how to run rag
14:04 - like kind of end to end in the last uh
14:07 - short talk we went through
14:09 - indexing um and what I'm going to do
14:11 - very simply is I'm just going to reload
14:14 - our
14:15 - documents so now I have our documents
14:19 - I'm going to resplit
14:21 - them and we saw before how we can build
14:23 - our
14:24 - index now here let's actually do the
14:27 - same thing but in the slide we actually
14:29 - showed kind of that notion of search in
14:31 - that 3D
14:33 - space and a nice parameter to think
14:35 - about in building your your retriever is
14:38 - K so K tells you the number of nearby
14:41 - neighbors to fetch when you do that
14:42 - retrieval process and we talked about
14:44 - you know in that 3D space do I want one
14:47 - nearby neighbor or two or three so here
14:49 - we can specify k equals 1 for example
14:53 - now we're building our index so we're
14:54 - taking every split embedding it storing
14:57 - it now what's nice is I asked a a
14:59 - question what is Task decomposition this
15:01 - is related to the blog post and I'm
15:03 - going to run get relevant documents so I
15:05 - run that and now how many documents do I
15:08 - get back I get one as expected based
15:10 - upon k equals 1 so this retrieve
15:12 - document should be related to my
15:14 - question now I can go to lsmith and we
15:17 - can open it up and we can look at our
15:19 - Retriever and we can see here was our
15:21 - question here's the one document we got
15:23 - back and okay so that makes sense this
15:27 - document pertains to task ke
15:29 - decomposition in particular and it kind
15:31 - of lays out a number of different
15:33 - approaches that can be used to do that
15:35 - this all kind of makes sense and this
15:37 - shows kind of in practice how you can
15:38 - implement this this NE this kind of KNN
15:42 - or k nearest neighbor search uh really
15:45 - easily uh just using a few lines of code
15:48 - and next we're going to talk about
15:50 - generation
15:52 - thanks hey this is Lance from Lang chain
15:55 - this is the fourth uh short video in our
15:57 - rack from scratch series
15:59 - that's going to be focused on
16:01 - generation now in the past few videos we
16:04 - walked through the general flow uh for
16:07 - kind of basic rag starting with indexing
16:10 - Fall by
16:11 - retrieval then
16:13 - generation of an answer based upon the
16:15 - documents that we retrieved that are
16:17 - relevant to our question this is kind of
16:19 - the the very basic
16:21 - flow now an important consideration in
16:25 - generation is really what's happening is
16:28 - we're taking the documents you retrieve
16:30 - and we're stuffing them into the llm
16:32 - context window so if we kind of walk
16:34 - back through the process we take
16:37 - documents we split them for convenience
16:39 - or embedding we then embed each split
16:43 - and we store that in a vector store as
16:45 - this kind of easily searchable numerical
16:47 - representation or vector and we take a
16:50 - question embed it to produce a similar
16:52 - kind of numerical representation we can
16:55 - then search for example using something
16:57 - like KN andn in this kind of dimensional
17:00 - space for documents that are similar to
17:02 - our question based on their proximity or
17:05 - location in this space in this case you
17:07 - can see 3D is a toy kind of toy
17:10 - example now we've recovered relevant
17:12 - splits to our question we pack those
17:15 - into the context window and we produce
17:17 - our
17:19 - answer now this introduces the notion of
17:22 - a prompt so the prompt is kind of a you
17:25 - can think have a placeholder that has
17:27 - for example you know in our case B keys
17:30 - so those keys can be like context and
17:33 - question so they basically are like
17:35 - buckets that we're going to take those
17:37 - retrieve documents and Slot them in
17:40 - we're going to take our question and
17:41 - also slot it in and if you kind of walk
17:44 - through this flow you can kind of see
17:45 - that we can build like a dictionary from
17:48 - our retrieve documents and from our
17:50 - question and then we can basically
17:52 - populate our prompt template with the
17:54 - values from the dict and then becomes a
17:57 - prompt value which can be passed to llm
17:59 - like a chat model resulting in chat
18:01 - messages which we then parse into a
18:03 - string and get our answer so that's like
18:06 - the basic workflow that we're going to
18:07 - see and let's just walk through that in
18:09 - code very quickly to kind of give you
18:11 - like a Hands-On intuition so we had our
18:14 - notebook we walk through previously
18:16 - install a few packages I'm setting a few
18:19 - lsmith environment variables we'll see
18:21 - it's it's nice for uh kind of observing
18:23 - and debugging our
18:25 - traces um previously we did this quick
18:27 - start we're going to skip that over
18:30 - um and what I will do is I'm going to
18:33 - build our retriever so again I'm going
18:36 - to take documents and load them uh and
18:38 - then I'm going to split them here we've
18:40 - kind of done this previously so I'll go
18:42 - through this kind of quickly and then
18:43 - we're going to embed them and store them
18:45 - in our index so now we have this
18:47 - retriever object here now I'm going to
18:50 - jump down here now here's where it's
18:51 - kind of fun this is the generation bit
18:54 - and you can see here I'm defining
18:55 - something new this is a prompt template
18:58 - and what my prompt template is something
18:59 - really simple it's just going to say
19:00 - answer the following question based on
19:02 - this context it's going to have this
19:04 - context variable and a question so now
19:06 - I'm building my prompt so great now I
19:08 - have this prompt let's define an llm
19:11 - I'll choose
19:12 - 35 now this introdu the notion of a
19:15 - chain so in Lang chain we have an
19:17 - expression language called L Cel Lang
19:20 - chain expression language which lets you
19:22 - really easily compose things like
19:24 - prompts LMS parsers retrievers and other
19:27 - things but the very simple kind of you
19:30 - know example here is just let's just
19:32 - take our prompt which you defined right
19:33 - here and connect it to an LM which you
19:35 - defined right here into this chain so
19:37 - there's our chain now all we're doing is
19:39 - we're invoking that chain so every L
19:42 - expression language chain has a few
19:44 - common methods like invoke bat stream in
19:47 - this case we just invoke it with a dict
19:50 - so context and question that maps to the
19:54 - expected Keys here in our template
19:58 - and so if we run invoke what we see is
20:01 - it's just going to execute that chain
20:02 - and we get our answer now if we zoom
20:05 - over to Langs Smith we should see that
20:07 - it's been populated so yeah we see a
20:09 - very simple runable
20:11 - sequence here was our
20:14 - document um and here's our output and
20:18 - here is our prompt answer the following
20:21 - question based on the context here's the
20:24 - document we passed in here is the
20:26 - question and then we get our answer so
20:28 - that's pretty nice um now there's a lot
20:32 - of other options for rag prompts I'll
20:34 - pull one in from our prompt tub this
20:36 - one's like kind of a popular prompt so
20:39 - it just like has a little bit more
20:41 - detail but you know it's the main the
20:43 - main intuition is the same um you're
20:47 - passing in documents you're asking them
20:48 - to reason about the documents given a
20:50 - question produce an answer and now here
20:53 - I'm going to find a rag chain which will
20:55 - automatically do the retrieval for us
20:57 - and all I have to do is specify here's
20:59 - my retriever which we defined
21:01 - before here's our question we which we
21:03 - invoke with the question gets passed
21:06 - through to the key question in our dict
21:10 - and it automatically will trigger the
21:12 - retriever which will return documents
21:14 - which get passed into our context so
21:16 - it's exactly what we did up here except
21:18 - before we did this
21:20 - manually and
21:22 - now um this is all kind of automated for
21:25 - us we pass that dick which is autop
21:28 - populated
21:29 - into our prompt llm out to parser now
21:31 - let invoke it and that should all just
21:34 - run and great we get an answer and we
21:37 - can look at the
21:39 - trace and we can see everything that
21:41 - happened so we can see our retriever was
21:44 - run these documents were
21:46 - retrieved they get passed into our
21:49 - LM and we get our final answer so this
21:53 - kind of the end of our overview um where
21:56 - we talked about I'll go back to the
21:58 - slide here quickly we talked about
22:00 - indexing retrieval and now
22:02 - generation and follow-up short videos
22:04 - we'll kind of dig into some of the more
22:06 - com complex or detailed themes that
22:09 - address some limitations that can arise
22:11 - in this very simple pipeline
22:14 - thanks hi my from Lang chain over the
22:17 - next few videos we're going to be
22:18 - talking about career
22:20 - translation um and in this first video
22:23 - we're going to cover the topic of
22:24 - multi-query
22:26 - so query translation sits kind of at the
22:29 - first stage of an advanced rag Pipeline
22:34 - and the goal of career translation is
22:36 - really to take an input user question
22:39 - and to translate in some way in order to
22:42 - improve
22:44 - retrieval so the problem statement is
22:47 - pretty intuitive user queries um can be
22:51 - ambiguous and if the query is poorly
22:53 - written because we're typically doing
22:56 - some kind of semantic similarity search
22:57 - between the query and our documents if
23:01 - the query is poorly written or ill
23:02 - opposed we won't retrieve the proper
23:05 - documents from our
23:07 - index so there's a few approaches to
23:10 - attack this problem and you can kind of
23:13 - group them in a few different ways so
23:15 - here's one way I like to think about it
23:17 - a few approaches has involveed query
23:19 - rewriting so taking a query and
23:22 - reframing it like writing from a
23:23 - different perspective um and that's what
23:26 - we're going to talk about a little bit
23:27 - here in depth using approaches like
23:29 - multi-query or rag Fusion which we'll
23:32 - talk about in the next video you can
23:34 - also do things like take a question and
23:36 - break it down to make it less abstract
23:38 - like into sub questions and there's a
23:40 - bunch of interesting papers focused on
23:42 - that like least to most from
23:44 - Google you can also take the opposite
23:46 - approach of take a question to make it
23:48 - more abstract uh and there's actually
23:50 - approach we're going to talk about later
23:51 - in a future video called stepback
23:53 - prompting that focuses on like kind of
23:56 - higher a higher level question from the
23:59 - input so the intuition though for this
24:03 - multier approach is we're taking a
24:05 - question and we're going to break it
24:06 - down into a few differently worded
24:08 - questions uh from different
24:10 - perspectives and the intuition here is
24:13 - simply that um it is possible that the
24:17 - way a question is initially worded once
24:21 - embedded it is not well aligned or in
24:24 - close proximity in this High dimensional
24:26 - embedding space to a document that we
24:27 - want to R that's actually related so the
24:30 - thinking is that by kind of rewriting it
24:32 - in a few different ways you actually
24:34 - increase the likel of actually
24:36 - retrieving the document that you really
24:37 - want to um because of nuances in the way
24:41 - that documents and questions are
24:43 - embedded this kind of more shotgun
24:46 - approach of taking a question Fanning it
24:47 - out into a few different perspectives
24:50 - May improve and increase the reliability
24:51 - of retrieval that's like the intuition
24:54 - really um and of course we can com
24:56 - combine this with retrieval so we can
24:59 - take our our kind of fan out questions
25:01 - do retrieval on each one and combine
25:03 - them in some way and perform rag so
25:06 - that's kind of the overview and now
25:07 - let's what let's go over to um our code
25:11 - so this is a notebook and we're going to
25:13 - share all
25:14 - this um we're just installing a few
25:18 - packages we're setting a lsmith API Keys
25:21 - which we'll see why that's quite useful
25:22 - here shortly there's our diagram now
25:25 - first I'm going to Index this blog post
25:27 - on agents I'm going to split it um well
25:31 - I'm going to load it I'm going to split
25:32 - it and then I'm going to index it in
25:34 - chroma locally so this is a vector store
25:37 - we've done this previously so now I have
25:38 - my index defined so here is where I'm
25:41 - defining my prompt for multiquery which
25:44 - is your your assistant your task is to
25:47 - basically reframe this question into a
25:48 - few different sub
25:50 - questions um so there's our
25:53 - prompt um right here we'll pass that to
25:56 - an llm part it um into a string and then
26:01 - split the string by new lines and so
26:02 - we'll get a list of questions out of
26:04 - this chain that's really all we're doing
26:06 - here now all we're doing is here's a
26:09 - sample input question there's our
26:11 - generate queries chain which we defined
26:13 - we're going to take that list and then
26:15 - simply apply each question to retriever
26:19 - so we'll do retrieval per question and
26:21 - this little function here is just going
26:22 - to take the unique Union of documents uh
26:25 - across all those retrievals so let's run
26:27 - this and see what happens so we're going
26:29 - to run this and we're going to get some
26:32 - set of questions uh or documents back so
26:36 - let's go to Langs Smith now we can
26:37 - actually see what happened under the
26:39 - hood so here's the key
26:42 - point we ran our initial chain to
26:44 - generate a set of of reframed questions
26:48 - from our input and here was that prompt
26:51 - and here is that set of questions that
26:52 - we generated now what happened is for
26:55 - every one of those questions we did an
26:57 - independent retrieval that's what we're
26:58 - showing here so that's kind of the first
27:00 - step which is great now I can go back to
27:03 - the notebook and we can show this
27:05 - working end to end so now we're going to
27:07 - take that retrieval chain we'll pass it
27:09 - into context of our final rag prompt
27:12 - we'll also pass through the question
27:14 - we'll pass that to our rag prompt here
27:16 - pass it to an LM and then Pary output
27:19 - now let's let's kind of see how that
27:21 - works so again that's okay there it is
27:24 - so let's actually go into langth and see
27:26 - what happened under the hood so this was
27:28 - our final chain so this is great we took
27:31 - our input question we broke it out to
27:33 - these like five rephrase questions for
27:36 - every one of those we did a retrieval
27:38 - that's all great we then took the unique
27:41 - Union of documents and you can see in
27:42 - our final llm prompt answer the
27:45 - following cont following question based
27:46 - on the context this is the final set of
27:50 - unique documents that we retrieved from
27:52 - all of our sub
27:54 - questions um here's our initial question
27:57 - there's our answer so that kind of shows
27:59 - you how you can set this up really
28:00 - easily how you can use l Smith to kind
28:02 - of investigate what's going on and in
28:04 - particular use l Smith to investigate
28:06 - those intermediate questions that you
28:08 - generate in that like kind of question
28:11 - generation phase and in a future talks
28:13 - we're going to go through um some of
28:15 - these other methods that we kind of
28:17 - introduced at the start of this one
28:18 - thank
28:20 - you last L chain this is the second
28:23 - video of our Deep dive on query
28:25 - translation in our rag from scratch
28:27 - series focused on a method called rag
28:30 - Fusion so as we kind of showed before
28:33 - career translation you can think of as
28:35 - the first stage in an advanced rag
28:38 - pipeline we're taking an input user
28:40 - question and We're translating it some
28:42 - way in order to improve
28:44 - retrievable now we showed this General
28:48 - mapping of approaches previously so
28:50 - again you have kind of like rewriting so
28:52 - you can take a question and like kind of
28:54 - break it down into uh differently worded
28:58 - are different different perspectives of
29:00 - the same question so that's kind of
29:02 - rewriting there's sub questions where
29:04 - you take a question break it down into
29:06 - smaller problems solve each one
29:08 - independently and then there step back
29:10 - where you take a question and kind of go
29:11 - more abstract where you kind of ask a
29:14 - higher level question as a precondition
29:16 - to answer the user question so those are
29:18 - the approaches and we're going to dig
29:20 - into one of the particular approaches
29:22 - for rewriting called rat Fusion now this
29:25 - is really similar to what we just saw
29:27 - with multiquery
29:28 - the difference being we actually apply a
29:31 - a kind of a clever rank ranking step of
29:33 - our retriev documents um which you call
29:36 - reciprocal rank Fusion that's really the
29:38 - only difference the the input stage of
29:41 - taking a question breaking it out into a
29:44 - few kind of differently worded questions
29:47 - retrieval on each one is all the same
29:50 - and we're going to see that in the code
29:51 - here shortly so let's just hop over
29:54 - there and then look at this so again
29:56 - here is a notebook that we introduced
29:59 - previously here's the packages we've
30:01 - installed we've set a few API keys for
30:04 - lsmith which we see why is quite
30:07 - useful um and you can kind of go down
30:11 - here to a rag Fusion
30:13 - section and the first thing you'll note
30:16 - is what our prompt is so it looks really
30:18 - similar to The Prompt we just saw with
30:19 - multiquery and simply your helpful
30:22 - assistant that generates multiple search
30:23 - queries based upon user input and here's
30:26 - the question output for queries so let's
30:30 - define our prompt and here was our query
30:33 - Generation chain again this looks a lot
30:34 - like we just saw we take our prompt Plum
30:37 - that into an llm and then basically
30:39 - parse by new lines and that'll basically
30:42 - split out these questions into a list
30:46 - that's all it's going to happen here so
30:47 - that's cool now here's where the novelty
30:51 - comes
30:52 - in each time we do retrieval from one of
30:56 - those questions we're going to get back
30:58 - a list of documents from our Retriever
31:01 - and so we do it over that we generate
31:03 - four questions here based on our prompt
31:06 - we do the over four questions well like
31:07 - a list of lists
31:09 - basically now reciprocal rank Fusion is
31:12 - really well suited for this exact
31:13 - problem we want to take this list to
31:15 - list and build a single Consolidated
31:17 - list and really all that's going on is
31:20 - it's looking at the documents in each
31:22 - list and kind of aggregating them into a
31:24 - final output ranking um and that's
31:28 - really the intuition around what's
31:29 - happening
31:33 - here um so let's go ahead
31:39 - and so
31:46 - let's so let's go ahead and look at that
31:49 - in some detail so we can see we
31:53 - run
31:55 - retrieval that's great now let's go over
31:57 - to Lang Smith and have a look at what's
32:00 - going on here so we can see that here
32:05 - was our prompt to your helpful assistant
32:06 - that generates multiple search queries
32:08 - based on a single input and here is our
32:10 - search queries and then here are our
32:14 - four retrievals so that's that's really
32:16 - good so we know that all is
32:19 - working um and then those retrievals
32:23 - simply went into this rank
32:25 - function and our correspondingly ranked
32:29 - to a final list of six unique rank
32:31 - documents that's really all we
32:34 - did so let's actually put that all
32:37 - together into an a full rag chain that's
32:41 - going to run
32:43 - retrieval return that final list of rank
32:47 - documents and pass it to our context
32:50 - pass through our question send that to a
32:53 - rag prompt pass it to an LM parse it to
32:56 - an output and let's run all that
32:58 - together and see that
33:01 - working cool so there's our final
33:07 - answer now let's have a look in lsmith
33:10 - we can see here was our four questions
33:13 - here's our retrievals and then our final
33:15 - rag prompt plumed through the final list
33:18 - of ranked six questions which we can see
33:22 - laid out here and our final answer so
33:25 - this can be really convenient
33:26 - particularly if we're operating across
33:29 - like maybe different Vector stores uh or
33:32 - we want to do like retrieval across a
33:34 - large number of of kind of differently
33:36 - worded questions this reciprocal rank
33:38 - Fusion step is really nice um for
33:41 - example if we wanted to only take the
33:43 - top three documents or something um it
33:46 - can be really nice to build that
33:47 - Consolidated ranking across all these
33:49 - independent retrievals then pass that to
33:52 - for the final generation so that's
33:54 - really the intuition about what's
33:55 - happening here thanks
33:58 - hi this is Lance from Lang chain this is
34:00 - our third video focused on query
34:02 - translation in the rag from scratch
34:04 - series and we're going to be talking
34:06 - about
34:07 - decomposition so query translation in
34:09 - general is a set of approaches that sits
34:11 - kind of towards the front of this
34:13 - overall rag Pipeline and the objective
34:16 - is to modify or rewrite or otherwise
34:18 - decompose an input question from a user
34:21 - in order improve
34:24 - retrieval so we can talk through some of
34:26 - these approaches previously in
34:28 - particular various ways to do query
34:30 - writing like rag fusion and multiquery
34:32 - there's a separate set of techniques
34:34 - that become pretty popular and are
34:35 - really interesting for certain problems
34:37 - which we might call like kind of
34:39 - breaking down or decomposing an input
34:41 - question into a set of sub
34:43 - questions um so some of the papers here
34:46 - that are are pretty cool are for example
34:49 - this work from
34:50 - Google um and the objective really is
34:54 - first to take an input question and
34:57 - decompose it into a set of sub problems
35:00 - so this particular example from the
35:02 - paper was the problem of um last letter
35:07 - concatenation and so it took the inut
35:10 - question of three words think machine
35:13 - learning and broke it down into three
35:15 - sub problems think think machine think
35:17 - machine learning as the third sub
35:19 - problem and then you can see in this
35:20 - bottom panel it solves each one
35:23 - individually so it shows for example in
35:25 - green solving the problem think machine
35:28 - where you can catenate the last letter
35:30 - of k with the last letter of machine or
35:32 - last letter think K less machine e can
35:36 - concatenate those to K and then for the
35:39 - overall problem taking that solution and
35:43 - then and basically building on it to get
35:45 - the overall solution of keg so that's
35:47 - kind of one concept of decomposing into
35:50 - sub problems solving them
35:52 - sequentially now a related work called
35:55 - IRC or in leap retrieval combines
35:58 - retrieval with Chain of Thought
36:01 - reasoning and so you can kind of put
36:03 - these together into one approach which
36:05 - you can think of as kind of dynamically
36:08 - retrieval um to solve a set of sub
36:11 - problems kind of that retrieval kind of
36:13 - interleaving with Chain of Thought as
36:15 - noted in the second paper and a set of
36:19 - decomposed questions based on your
36:21 - initial question from the first work
36:23 - from Google so really the idea here is
36:25 - we're taking one sub question we're
36:28 - answering it we're taking that answer
36:29 - and using it to help answer the second
36:31 - sub question and so forth so let's
36:34 - actually just walk through this in code
36:35 - to show how this might
36:37 - work so this is The Notebook we've been
36:40 - working with from some of the other uh
36:42 - videos you can see we already have a
36:43 - retriever to find uh up here at the top
36:47 - and what we're going to do
36:49 - is we're first going to find a prompt
36:52 - that's basically going to say given an
36:54 - input question let's break it down to
36:57 - set of sub problems or sub question
36:59 - which can be solved individually so we
37:01 - can do that and this blog post is
37:03 - focused on agents so let's ask a
37:05 - question about what are the main
37:06 - components of an LM powerered autonomous
37:08 - agent system so let's run this and
37:13 - see what the decomposed questions are so
37:16 - you can see the decomposed questions are
37:18 - what is LM technology how does it work
37:21 - um what are components and then how the
37:23 - components interact so it's kind of a
37:25 - sane way to kind of break down this
37:27 - problem into a few sub problems which
37:28 - you might attack individually now here's
37:32 - where um we Define a prompt that very
37:35 - simply is going to take our question
37:38 - we'll take any prior questions we've
37:40 - answered and we'll take our retrieval
37:42 - and basically just combine them and we
37:45 - can Define this very simple
37:47 - chain um actually let's go back and make
37:49 - sure retriever is defined up at the
37:52 - top so now we are building our
37:56 - retriever good we have that now so we
37:59 - can go back down here and let's run this
38:02 - so
38:03 - now we are
38:06 - running and what's happening is we're
38:10 - trying to solve each of these questions
38:12 - individually using retrieval and using
38:15 - any prior question answers so okay very
38:18 - good looks like that's been done and we
38:20 - can see here's our answer now let's go
38:23 - over to langth and actually see what
38:25 - happened under the hood so here's what's
38:27 - kind of of interesting and helpful to
38:28 - see for the first question so here's our
38:31 - first one it looks like it just does
38:33 - retrieval which is we expect and then it
38:36 - uses that to answer this initial
38:37 - question now for the second question
38:40 - should be a little bit more interesting
38:42 - because if you look at our prompt here's
38:44 - our question now here is our background
38:47 - available question answer pair so this
38:49 - was the answer question answer pair from
38:51 - the first question which we add to our
38:52 - prompt and then here's the retrieval for
38:55 - this particular question so we're kind
38:56 - of building up up the solution because
38:58 - we're pending the question answer pair
39:00 - from question one and then likewise with
39:03 - question three it should combine all of
39:05 - that so we can look at here here's our
39:08 - question here's question one here's
39:10 - question two great now here's additional
39:14 - retrieval related to this particular
39:16 - question and we get our final answer so
39:18 - that's like a really nice way you can
39:20 - kind of build up Solutions um using this
39:23 - kind of
39:24 - interleaved uh retrieval and
39:26 - concatenating question answer pairs I do
39:29 - want to mention very briefly that we can
39:31 - also take a different approach where we
39:33 - can just answer these all individually
39:36 - and then just concatenate all those
39:37 - answers to produce a final answer and
39:39 - I'll show that really quickly here um
39:43 - it's like a little bit less interesting
39:44 - maybe because you're not using answers
39:46 - from each uh question to inform the next
39:50 - one you're just answering them all in
39:51 - parallel this might be better for cases
39:53 - where it's not really like a sub
39:55 - question decomposition but maybe it's
39:57 - like like a set of set of several in
40:00 - independent questions whose answers
40:02 - don't depend on each other that might be
40:04 - relevant for some
40:05 - problems um and we can go ahead and run
40:08 - okay so this ran as well we can look at
40:11 - our
40:12 - trace and in this
40:14 - case um yeah we can see that this
40:16 - actually just kind of concatenates all
40:19 - of our QA pairs to produce the final
40:22 - answer so this gives you a sense for how
40:24 - you can use quer decomposition employ
40:27 - IDE IDE from uh from two different
40:28 - papers that are pretty cool thanks hi
40:32 - this is Lance from Lang chain this is
40:34 - the fourth video uh in our Deep dive on
40:37 - queer translation in the rag from
40:39 - scratch series and we're going to be
40:40 - focused on step back
40:42 - prompting so queer translation as we
40:45 - said in some of the prior videos kind of
40:48 - sits at the the kind of first stage of
40:51 - kind of a a a rag pipeline or flow and
40:56 - the main aim is to take an question and
40:58 - to translate it or modify in such a way
41:00 - that it improves
41:02 - retrieval now we talked through a few
41:04 - different ways to approach this problem
41:07 - so one General approach involves
41:08 - rewriting a question and we talk about
41:10 - two ways to do that rag fusion
41:12 - multiquery and again this is this is
41:14 - really about taking a question and
41:16 - modifying it to capture a few different
41:19 - perspectives um which may improve the
41:21 - retrieval
41:22 - process now another approach is to take
41:25 - a question and kind of make it less
41:26 - abstract like break it down into sub
41:29 - questions um and then solve each of
41:31 - those independently so that's what we
41:32 - saw with like least to most prompting um
41:35 - and a bunch of other variants kind of in
41:37 - that in that vein of sub problem solving
41:42 - and then consolidating those Solutions
41:44 - into a final
41:45 - answer now a different approach
41:48 - presented um by again Google as well is
41:52 - stepback prompting so stepback prompting
41:55 - kind of takes the the the opposite
41:57 - approach where it tries to ask a more
42:00 - abstract question so the paper talks a
42:04 - lot
42:05 - about um using F shot
42:09 - prompting to produce what they call the
42:12 - stepback or more abstract questions and
42:15 - the way it does it is it provides a
42:17 - number of
42:19 - examples of stepb back questions given
42:22 - your original question so like this is
42:24 - like this is for example they like for
42:26 - prompt temp
42:27 - you're an expert World Knowledge I asked
42:29 - you a question your response should be
42:30 - comprehensive not contradict with the
42:32 - following um and this is kind of where
42:35 - you provide your like original and then
42:38 - step back so here's like some example um
42:43 - questions so like um
42:46 - like uh at year saw the creation of the
42:50 - region where the country is located
42:53 - which region of the
42:54 - country um is the county of of herir
42:58 - related um Janell was born in what
43:01 - country what is janell's personal
43:04 - history so that that's maybe a more
43:05 - intuitive example so it's like you ask a
43:07 - very specific question about like the
43:09 - country someone's born the more abstract
43:11 - question is like just give me the
43:12 - general history of this individual
43:14 - without worrying about that particular
43:16 - um more specific question um so let's
43:21 - actually just walk through how this can
43:22 - be done in practice um so again here's
43:25 - kind of like a a diagram of uh the
43:28 - various approaches um from less
43:31 - abstraction to more
43:32 - abstraction now here is where we're
43:35 - formulating our prompt using a few of
43:38 - the few shot examples from the
43:40 - paper um so again like input um yeah
43:45 - something about like the police perform
43:46 - wful arrests and what what camp members
43:48 - of the police do so like it it basically
43:51 - gives the model a few examples um we
43:54 - basically formulate this into a prompt
43:56 - that's really all going on here again we
43:58 - we repeat um this overall prompt which
44:02 - we saw from the paper your expert World
44:04 - Knowledge your test is to step back and
44:06 - paraphrase a question generate more a
44:08 - generic step back question which is
44:10 - easier to answer here are some examples
44:12 - so it's like a very intuitive prompt so
44:16 - okay let's start with the question what
44:18 - is Task composition for llm agents and
44:21 - we're going to say generate stack
44:23 - question okay so this is pretty
44:24 - intuitive right what is a process of
44:26 - task compos I so like not worrying as
44:28 - much about agents but what is that
44:30 - process of task composition in general
44:33 - and then hopefully that can be
44:36 - independently um retrieved we we can
44:39 - independently retrieve documents related
44:41 - to the stepb back question and in
44:43 - addition retrieve documents related to
44:45 - the the actual question and combine
44:47 - those to produce kind of final answer so
44:49 - that's really all that's going on um and
44:52 - here's the response template where we're
44:53 - Plumbing in the stepback context and our
44:57 - question context and so what we're going
45:00 - to do here is we're going to take our
45:01 - input question and perform retrieval on
45:04 - that we're also going to generate our
45:06 - stepb back question and perform
45:07 - retrieval on that we're going to plumb
45:10 - those into the prompt as here's our very
45:13 - here's our basically uh our prompt Keys
45:15 - normal question step back question um
45:19 - and our overall question again we
45:21 - formulate those as a dict we Plum those
45:23 - into our response prompt um and then we
45:29 - go ahead and attempt to answer our
45:31 - overall question so we're going to run
45:33 - that that's
45:35 - running and okay we have our answer now
45:38 - I want to hop over to Langs Smith and
45:41 - attempt to show you um kind of what that
45:45 - looked like under the hood so let's see
45:48 - let's like go into each of these steps
45:51 - so here was our prompt right you're an
45:53 - expert World Knowledge your test to to
45:55 - step back and paraph as a question
45:58 - um so um here were our few shot prompts
46:03 - and this was our this was our uh stepb
46:06 - question so what is the process of task
46:08 - composition um good from the input what
46:12 - is Tas composition for LM agents we
46:14 - perform retrieval on both what is
46:16 - process
46:17 - composition uh and what is for LM agents
46:20 - we perform both retrievals we then
46:23 - populate our prompt with both
46:27 - uh original question answer and then
46:29 - here's the context retrieve from both
46:31 - the question and the stepb back question
46:33 - here was our final answer so again this
46:35 - is kind of a nice technique um probably
46:38 - depends on a lot of the types of like
46:41 - the type of domain you want to perform
46:43 - retrieval on um but in some domains
46:47 - where for example there's a lot of kind
46:48 - of conceptual knowledge that underpins
46:52 - questions you expect users to ask this
46:54 - stepback approach could be really
46:55 - convenient to automatically formulate a
46:59 - higher level question um to for example
47:02 - try to improve retrieval I can imagine
47:04 - if you're working with like kind of
47:05 - textbooks or like technical
47:07 - documentation where you make independent
47:10 - chapters focused on more highlevel kind
47:12 - of like Concepts and then other chapters
47:15 - on like more detailed uh like
47:18 - implementations this kind of like stepb
47:19 - back approach and independent retrieval
47:22 - could be really helpful thanks hi this
47:25 - is Lance from Lang chain
47:27 - this is the fifth video focused on queer
47:29 - translation in our rack from scratch
47:31 - series we're going to be talking about a
47:33 - technique called
47:34 - hide so again queer translation sits
47:37 - kind of at the front of the overall rag
47:39 - flow um and the objective is to take an
47:41 - input question and translate it in some
47:43 - way that improves
47:46 - retrieval now hide is an interesting
47:49 - approach that takes advantage of a very
47:51 - simple idea the basic rag flow takes a
47:55 - question and embeds it takes a document
47:57 - and embeds it and looks for similarity
48:00 - between an embedded document and
48:02 - embedded question but questions and
48:04 - documents are very different text
48:06 - objects so documents can be like very
48:08 - large chunks taken from dense um
48:11 - Publications or other sources whereas
48:14 - questions are short kind of tur
48:16 - potentially ill worded from users and
48:19 - the intuition behind hide is take
48:22 - questions and map them into document
48:25 - space using a hypothetical document or
48:28 - by generating a hypothetical document um
48:31 - that's the basic intuition and the idea
48:34 - kind of shown here visually is that in
48:36 - principle for certain cases a
48:38 - hypothetical document is closer to a
48:41 - desired document you actually want to
48:42 - retrieve in this you know High
48:45 - dimensional embedding space than the
48:47 - sparse raw input question itself so
48:50 - again it's just kind of means of trans
48:52 - translating raw questions into these
48:54 - hypothetical documents that are better
48:57 - suited for
48:58 - retrieval so let's actually do a Code
49:00 - walkthrough to see how this works and
49:02 - it's actually pretty easy to implement
49:04 - which is really nice so first we're just
49:06 - starting with a prompt and we're using
49:08 - the same notebook that we've used for
49:09 - prior videos we have a blog post on
49:12 - agents r index um so what we're going to
49:15 - do is Define a prompt to generate a
49:17 - hypothetical documents in this case
49:19 - we'll say write a write a paper passage
49:22 - uh to answer a given question so let's
49:25 - just run this and see what happens again
49:26 - we're taking our prompt piping it to to
49:29 - open Ai chck gpte and then using string
49:32 - Opa parer and so here's a hypothetical
49:34 - document section related to our question
49:38 - okay and this is derived of course lm's
49:40 - kind of embedded uh kind of World
49:43 - Knowledge which is you know a sane place
49:45 - to generate hypothetical documents now
49:48 - let's now take that hypothetical
49:50 - document and basically we're going to
49:52 - pipe that into a retriever so this means
49:54 - we're going to fetch documents from our
49:56 - index related to this hypothetical
49:59 - document that's been embedded and you
50:01 - can see we get a few qu a few retrieved
50:04 - uh chunks that are related to uh this
50:08 - hypothetical document that's all we've
50:11 - done um and then let's take the final
50:15 - step where we take those retrieve
50:17 - documents here which we
50:19 - defined and our question we're going to
50:22 - pipe that into this rag prompt and then
50:25 - we're going to run our kind of rag chain
50:27 - right here which you've seen before and
50:29 - we get our answer so that's really it we
50:32 - can go to lsmith and we can actually
50:33 - look at what happened um so here for
50:37 - example this was our final um rag prompt
50:43 - answer the following question based on
50:44 - this context and here is the retrieve
50:47 - documents that we passed in so that
50:48 - part's kind of straightforward we can
50:50 - also look at um okay this is our
50:55 - retrieval okay now this is this is
50:57 - actually what we we generated a
51:01 - hypothetical document here um okay so
51:05 - this is our hypothetical document so
51:08 - we've run chat open AI we generated this
51:10 - passage with our hypothetical document
51:12 - and then we've run
51:13 - retrieval here so this is basically
51:15 - showing hypothetical document generation
51:17 - followed by retrieval um so again here
51:21 - was our passage which we passed in and
51:24 - then here's our retrieve documents from
51:25 - the retriever which are related to the
51:27 - passage content so again in this
51:30 - particular index case it's possible that
51:32 - the input question was sufficient to
51:34 - retrieve these documents in fact given
51:36 - prior examples uh I know that some of
51:38 - these same documents are indeed
51:39 - retrieved just from the raw question but
51:42 - in other context it may not be the case
51:44 - so folks have reported nice performance
51:46 - using Hyde uh for certain domains and
51:49 - the Really convenient thing is that you
51:52 - can take this this document generation
51:55 - prompt you can tune this arbitrarily for
51:57 - your domain of Interest so it's
51:59 - absolutely worth experimenting with it's
52:01 - a it's a need approach uh that can
52:03 - overcome some of the challenges with
52:04 - retrieval uh thanks very much hi this is
52:08 - Lance from Lang chain this is the 10th
52:10 - video in our rack from scratch series
52:12 - focused on
52:13 - routing so we talk through query
52:16 - translation which is the process of
52:17 - taking a question and translating in
52:19 - some way it could be decomposing it
52:21 - using stepback prompting or otherwise
52:24 - but the idea here was take our question
52:26 - change it into a form that's better
52:27 - suited for retrieval now routing is the
52:30 - next step which is basically routing
52:32 - that potentially decomposed question to
52:35 - the right source and in many cases that
52:37 - could be a different database so let's
52:38 - say in this toy example we have a vector
52:40 - store a relational DB and a graph DB the
52:43 - what we redo with routing is we simply
52:45 - route the question based upon the cont
52:47 - of the question to the relevant data
52:50 - source so there's a few different ways
52:52 - to do that one is what we call logical
52:54 - routing in this case we basically give
52:56 - an llm knowledge of the various data
52:59 - sources that we have at our disposal and
53:02 - we let the llm kind of Reason about
53:04 - which one to apply the question to so
53:07 - it's kind of like the the LM is applying
53:09 - some logic to determine you which which
53:11 - data sour for example to to use
53:13 - alternatively you can use semantic
53:15 - routing which is where we take a
53:17 - question we embed it and for example we
53:19 - embed prompts we then compute the
53:22 - similarity between our question and
53:24 - those prompts and then we choose a
53:27 - prompt based upon the similarity so the
53:29 - general idea is in our diagram we talk
53:31 - about routing to for example a different
53:33 - database but it can be very general can
53:35 - be routing to different prompt it can be
53:37 - you know really arbitrarily taking this
53:40 - question and sending it at different
53:41 - places be at different prompts be at
53:43 - different Vector
53:44 - stores so let's walk through the code a
53:46 - little bit so you can see just like
53:48 - before we've done a few pip installs we
53:51 - set up lsmith and let's talk through uh
53:54 - logical routing first so so in this toy
53:57 - example let's say we had for example uh
54:00 - three different docs like we had python
54:02 - docs we had JS docs we had goang docs
54:05 - what we want to do is take a question
54:07 - route it to one of those three so what
54:10 - we're actually doing is we're setting up
54:11 - a data model which is basically going to
54:15 - U be bound to our llm and allow the llm
54:19 - to Output one of these three options as
54:23 - a structured object so you really think
54:26 - about this as like
54:27 - classification classification plus
54:29 - function calling to produce a structured
54:31 - output which is constrained to these
54:33 - three
54:34 - possibilities so the way we do that is
54:37 - let's just zoom in here a little bit we
54:39 - can Define like a structured object that
54:41 - we want to get out from our llm like in
54:44 - this case we want for example you know
54:46 - one of these three data sources to be
54:49 - output we can take this and we can
54:52 - actually convert it into open like open
54:54 - for example function schema
54:57 - and then we actually pass that in and
54:58 - bind it to our llm so what happens is we
55:01 - ask a question our llm invokes this
55:04 - function on the output to produce an
55:07 - output that adheres to the schema that
55:09 - we specify so in this case for example
55:12 - um we output like you know in this toy
55:15 - example let's say we wanted like you
55:17 - know an output to be data source Vector
55:19 - store or SQL database the output will
55:22 - contain a data source object and it'll
55:23 - be you know one of the options we
55:25 - specify as a Json string we also
55:28 - instantiate a parser from this object to
55:32 - parse that Json string to an output like
55:35 - a pantic object for example so that's
55:37 - just one toy example and let's show one
55:39 - up here so in this case again we had our
55:41 - three doc sources um we bind that to our
55:46 - llm so you can see we do with structured
55:49 - output basically under the hood that's
55:51 - taking that object definition turning
55:54 - into function schema and binding that
55:55 - function schema to our llm and we call
55:58 - our prompt you're an expert at routing a
56:00 - user question based on you know
56:03 - programming language um that user
56:05 - referring to so let's define our router
56:08 - here now what we're going to do is we'll
56:10 - ask a question that is python code so
56:14 - we'll call that and now it's done and
56:16 - you see the object we get out is indeed
56:18 - it's a route query object so it's
56:20 - exactly it aderes to this data model
56:22 - we've set up and in this case it's it's
56:26 - it's correct so it's calling this python
56:27 - doc so you can we can extract that right
56:29 - here as a string now once we have this
56:33 - you can really easily set up like a
56:35 - route so this could be like our full
56:37 - chain where we take this router we
56:39 - should defined here and then this choose
56:41 - route function can basically take that
56:44 - output and do something with it so for
56:46 - example if python docs this could then
56:49 - apply the question to like a retriever
56:51 - full of python information uh or JS same
56:55 - thing so this is where you would hook
56:57 - basically that question up to different
56:59 - chains that are like you know retriever
57:02 - chain one for python retriever chain two
57:04 - for JS and so forth so this is kind of
57:06 - like the routing mechanism but this is
57:08 - really doing the heavy lifting of taking
57:10 - an input question and turning into a
57:12 - structured object that restricts the
57:14 - output to one of a few output types that
57:18 - we care about in our like routing
57:20 - problem so that's really kind of the way
57:22 - this all hooks
57:23 - together now semantic outing is actually
57:26 - maybe even a little bit more
57:27 - straightforward based on what we've seen
57:29 - previously so in that case let's say we
57:32 - have two prompts we have a physics
57:33 - prompt we have a math
57:35 - prompt we can embed those prompts no
57:38 - problem we do that here now let's say we
57:41 - have an input question from a user like
57:43 - in this case what is a black hole we
57:45 - pass that through we then apply this
57:47 - runnable Lambda function which is
57:49 - defined right here what we're doing here
57:51 - is we're embedding the question we're
57:53 - Computing similarity between the
57:54 - question and the prompts uh we're taking
57:57 - the most similar and then we're
58:00 - basically choosing the prompt based on
58:02 - that similarity and you can see let's
58:03 - run that and try it
58:05 - out and we're using the physics prompt
58:07 - and there we go black holes region and
58:09 - space so that just shows you kind of how
58:11 - you can use semantic routing uh to
58:15 - basically embed a question embed for
58:17 - example various prompts pick the prompt
58:19 - based on sematic similarity so that
58:22 - really gives you just two ways to do
58:23 - routing one is logical routing with
58:25 - function in uh can be used very
58:27 - generally in this case we applied it to
58:29 - like different coding languages but
58:31 - imagine these could be swapped out for
58:33 - like you know my python uh my like
58:36 - vector store versus My Graph DB versus
58:38 - my relational DB and you could just very
58:42 - simply have some description of what
58:43 - each is and you know then not only will
58:46 - the llm do reasoning but it'll also
58:49 - return an object uh that can be parsed
58:52 - very cleanly to produce like one of a
58:54 - few very specific types which then you
58:56 - can reason over like we did here in your
58:59 - routing function so that kind of gives
59:01 - you the general idea and these are
59:02 - really very useful tools and I encourage
59:05 - you to experiment with them
59:07 - thanks hi this is Lance from Lang chain
59:10 - this is the 11th part of our rag from
59:12 - scratch video series focused on query
59:15 - construction so we previously talked
59:17 - through uh query translation which is
59:19 - the process of taking a question and
59:21 - converting it or translating it into a
59:24 - question that's better optimized for
59:25 - retrieval then we talked about routing
59:27 - which is the process of going taking
59:29 - that question routing it to the right
59:30 - Source be it a given Vector store graph
59:33 - DB um or SQL DB for example now we're
59:37 - going to talk about the process of query
59:38 - construction which is basically taking
59:40 - natural language and converting it into
59:42 - particular domain specific language uh
59:45 - for one of these sources now we're going
59:47 - to talk specifically about the process
59:49 - of going from natural language to uh
59:52 - meditated filters for Vector
59:54 - Stores um the problem statement is
59:56 - basically this let's imagine we had an
59:58 - index of Lang Chain video transcripts um
60:01 - you might want to ask a question give me
60:04 - you know or find find me videos on chat
60:06 - Lang chain published after 2024 for
60:09 - example um the the process of query
60:13 - structuring basically converts this
60:15 - natural language question into a
60:17 - structured query that can be applied to
60:19 - the metadata uh filters on your vector
60:22 - store so most Vector stores will have
60:24 - some kind of meditative filters that can
60:26 - do kind of structur querying on top of
60:29 - uh the chunks that are indexed um so for
60:32 - example this type of query will retrieve
60:34 - all chunks uh that talk about the topic
60:36 - of chat Lang chain uh published after
60:39 - the date 2024 that's kind of the problem
60:41 - statement and to do this we're going to
60:43 - use function calling um in this case you
60:46 - can use for example open AI or other
60:48 - providers to do that and we're going to
60:50 - do is at a high level take the metadata
60:53 - fields that are present in our Vector
60:55 - store and divide them to the model as
60:57 - kind of information and the model then
61:00 - can take those and produce queries that
61:03 - adhere to the schema provided um and
61:06 - then we can parse those out to a
61:07 - structured object like a identic object
61:10 - which again which can then be used in
61:12 - search so that's kind of the problem
61:14 - statement and let's actually walk
61:15 - through
61:17 - code um so here's our notebook which
61:20 - we've kind of gone through previously
61:22 - and I'll just show you as an example
61:24 - let's take a example YouTube video and
61:26 - let's look at the metadata that you get
61:28 - with the transcript so you can see you
61:30 - get stuff like description uh URL um
61:34 - yeah publish date length things like
61:36 - that now let's say we had an index that
61:39 - had um basically a that had a number of
61:42 - different metadata fields and filters uh
61:46 - that allowed us to do range filtering on
61:47 - like view count publication date the
61:49 - video length um or unstructured search
61:52 - on contents and title so those are kind
61:54 - of like the imagine we had an index that
61:56 - had uh those kind of filters available
62:00 - to us what we can do is capture that
62:03 - information about the available filters
62:05 - in an object so we're calling that this
62:06 - tutorial search object kind of
62:08 - encapsulates that information about the
62:10 - available searches that we can do and so
62:12 - we basically enumerate it here content
62:14 - search and title search or semantic
62:15 - searches that can be done over those
62:19 - fields um and then these filters then
62:22 - are various types of structure searches
62:24 - we can do on like the length um The View
62:28 - count and so forth and so we can just
62:30 - kind of build that object now we can set
62:33 - this up really easily with a basic
62:35 - simple prompt that says you know you're
62:36 - an expert can bring natural language
62:38 - into database queries you have access to
62:40 - the database tutorial videos um given a
62:43 - question return a database query
62:44 - optimize retrieval so that's kind of it
62:47 - now here's the key point though when you
62:49 - call this LM with structured output
62:51 - you're binding this pantic object which
62:53 - contains all the information about our
62:55 - index to the llm which is exactly what
62:58 - we talked about previously it's really
63:00 - this process right here you're taking
63:02 - this object you're converting it to a
63:04 - function schema for example open AI
63:05 - you're binding that to your model and
63:07 - then you're going to be able to get um
63:10 - structured object out versus a Json
63:13 - string from a natural language question
63:15 - which can then be parsed into a pantic
63:18 - object which you get out so that's
63:19 - really the flow and it's taking
63:21 - advantage of function calling as we said
63:23 - so if we go back down we set up our
63:26 - query analyzer chain right here now
63:28 - let's try to run that just on a on a
63:30 - purely semantic input so rag from
63:32 - scratch let's run that and you can see
63:35 - this just does like a Content search and
63:36 - a title search that's exactly what you
63:38 - would expect now if we pass a question
63:41 - that includes like a date filter let's
63:43 - just see if that would work
63:45 - and there we go so you kind of still get
63:48 - that semantic search um but you also get
63:52 - um search over for example publish date
63:54 - earliest and latest publish date kind of
63:56 - as as you would expect let's try another
63:58 - one here so videos focus on the topic of
64:01 - chat Lang chain they're published before
64:02 - 2024 this is just kind of a rewrite of
64:04 - this question in slightly different way
64:06 - using a different date filter and then
64:08 - you can see we can get we get content
64:10 - search title search and then we can get
64:12 - kind of a date search so this is a very
64:14 - general strategy that can be applied
64:16 - kind of broadly to um different kinds of
64:19 - querying you want to do it's really the
64:21 - process of going from an unstructured
64:23 - input to a structured query object out
64:26 - following an arbitrary schema that you
64:29 - provide and so as noted really this
64:32 - whole thing we created here this
64:33 - tutorial search is based upon the
64:35 - specifics of our Vector store of
64:37 - interest and if you want to learn more
64:39 - about this I link to some documentation
64:41 - here that talks a lot about different uh
64:44 - types of of Integrations we have with
64:46 - different Vector store providers to do
64:47 - exactly this so it's a very useful trick
64:50 - um it allows you to do kind of query uh
64:54 - uh say metadata filter filtering on the
64:56 - fly from a natural language question
64:58 - it's a very convenient trick uh that
65:01 - works with many different Vector DBS so
65:03 - encourage you to play with it
65:06 - thanks this is Lance from Lang chain I'm
65:09 - going to talk about indexing uh and
65:11 - mulation indexing in particular for the
65:14 - 12th part of our rag from scratch series
65:17 - here so we previously talked about a few
65:20 - different major areas we talk about
65:22 - query translation which takes a question
65:24 - and translates it in some way to
65:26 - optimize for retrieval we talk about
65:28 - routing which is the process of taking a
65:30 - question routing it to the right data
65:32 - source be it a vector store graph DB uh
65:35 - SQL DB we talked about queer
65:37 - construction we dug into uh basically
65:39 - queer construction for Vector stores but
65:41 - of course there's also text SQL text to
65:44 - Cipher um so now we're going to talk
65:46 - about indexing a bit in particular we're
65:48 - going to talk about indexing indexing
65:50 - techniques for Vector Stores um and I
65:53 - want to highlight one particular method
65:54 - today called multi-representation
65:57 - indexing so the high LEL idea here is
66:01 - derived a bit from a paper called
66:03 - proposition indexing which kind of makes
66:05 - a simple
66:06 - observation you can think about
66:10 - decoupling raw documents and the unit
66:13 - you use for
66:15 - retrieval so in the typical case you
66:18 - take a document you split it up in some
66:21 - way to index it and then you embed the
66:24 - split directly
66:26 - um this paper talks about actually
66:29 - taking a document splitting it in some
66:32 - way but then using an llm to produce
66:35 - what they call a proposition which you
66:37 - can think of as like kind of a
66:38 - distillation of that split so it's kind
66:41 - of like using an llm to modify that
66:43 - split in some way to distill it or make
66:45 - it like a crisper uh like summary so to
66:49 - speak that's better optimized for
66:51 - retrieval so that's kind of one
66:52 - highlight one piece of intuition so we
66:55 - actually taken that idea and we've kind
66:57 - of built on it a bit in kind of a really
66:59 - nice way that I think is very well
67:01 - suited actually for long context llms so
67:04 - the idea is pretty simple you take a
67:07 - document and you you actually distill it
67:10 - or create a proposition like they show
67:11 - in the prior paper I kind of typically
67:14 - think of this as just produce a summary
67:16 - of the document and you embed that
67:18 - summary so that summary is meant to be
67:21 - optimized for retrieval so might contain
67:23 - a bunch of keywords from the document or
67:25 - like the big
67:26 - ideas such that when you embed the
67:29 - summary you embed a question you do
67:31 - search you basically can find that
67:34 - document based upon this highly
67:35 - optimized summary for retrieval so
67:38 - that's kind of represented here in your
67:39 - vector store but here's the catch you
67:42 - independently store the raw document in
67:44 - a dock store and when you when you
67:47 - basically retrieve the summary in the
67:50 - vector store you return the full
67:52 - document for the llm to perform
67:54 - generation and this is a nice trick
67:56 - because at generation time now with long
67:59 - condex LMS for example the LM can handle
68:02 - that entire document you don't need to
68:03 - worry about splitting it or anything you
68:05 - just simply use the summary to prod like
68:08 - to create a really nice representation
68:10 - for fishing out that full dock use that
68:13 - full dock in generation there might be a
68:14 - lot of reasons you want to do that you
68:16 - want to make sure the LM has the full
68:17 - context to actually answer the question
68:19 - so that's the big idea it's a nice trick
68:22 - and let's walk through some code
68:23 - here we have a notebook all set up uh
68:26 - just like before we done some pip
68:28 - installs um set to maybe I Keys here for
68:32 - lsmith um kind of here's a diagram now
68:35 - let me show an example let's just load
68:37 - two different uh blog posts uh one is
68:39 - about agents one is about uh you know
68:41 - human data quality um and what we're
68:45 - going to do is let's create a summary of
68:46 - each of those so this is kind of the
68:48 - first step of that process where we're
68:50 - going from like the raw documents to
68:51 - summaries let's just have a look and
68:53 - make sure those ran So Okay cool so the
68:57 - first DOC discusses you know building
68:58 - autonomous agents the second doc
69:00 - contains the importance of high quality
69:01 - human data and training okay so that's
69:03 - pretty nice we have our summaries now
69:06 - we're going to go through a process
69:07 - that's pretty
69:08 - simple first we Define a vector store
69:10 - that's going to index those
69:12 - summaries now we're going to Define what
69:14 - we call like our our document storage is
69:16 - going to store the full documents okay
69:19 - so this multiv Vector retriever kind of
69:21 - just pulls those two things together we
69:23 - basically add our Dock Store we had this
69:25 - bite store is basically the the the full
69:28 - document store uh the vector store is
69:30 - our Vector store um and now this ID is
69:32 - what we're going to use to reference
69:34 - between the chunks or the summaries and
69:37 - the full documents that's really it so
69:40 - now for every document we'll Define a
69:42 - new Doc ID um and then we're basically
69:44 - going to like take our summary documents
69:47 - um and we're going to extract um for
69:51 - each of our summaries we're going to get
69:53 - the associated doc ID so we go um so
69:58 - let's go ahead and do that so we have
70:01 - our summary docs which we add to the
70:03 - vector store we have our full documents
70:06 - uh our doc IDs and the full raw
70:08 - documents which are added to our doc
70:10 - store and then let's just do a query
70:12 - Vector store like a similarity search on
70:14 - our Vector store so memory and agents
70:16 - and we can see okay so we can extract
70:19 - you know from the summaries we can get
70:22 - for example the summary that pertains to
70:24 - um a agents so that's a good thing now
70:27 - let's go ahead and run a query get
70:30 - relevant documents on our retriever
70:32 - which basically combines the summaries
70:35 - uh which we use for retrieval then the
70:38 - doc store which we use to get the full
70:40 - doc back so we're going to apply our
70:42 - query we're going to basically run this
70:45 - and here's the key Point we've gotten
70:48 - back the entire
70:49 - article um and we can actually if you
70:53 - want to look at the whole thing we we
70:55 - can just go ahead and do this here we go
70:58 - so this is the entire article that we
70:59 - get back from that search so it's a
71:02 - pretty nice trick again we query with
71:04 - just memory and agents um and we can
71:07 - kind of go back to our diagram here we
71:09 - quered for memory and agents it started
71:11 - our summaries it found the summary
71:13 - related to memory and agents it uses
71:15 - that doc ID to reference between the
71:17 - vector store and the doc store it fishes
71:19 - out the right full doc returns us the
71:21 - full document in this case the full web
71:23 - page that's really it simple idea nice
71:27 - way to go from basically like nice
71:30 - simple proposition style or summary
71:32 - style indexing to full document
71:34 - retrieval which is very useful
71:36 - especially with long contact LMS thank
71:40 - you hi this is Lance from Lang chain
71:43 - this is the 13th part of our rag from
71:45 - scratch series focused on a technique
71:47 - called
71:48 - Raptor so Raptor sits within kind of an
71:51 - array of different indexing techniques
71:54 - that can be applied on Vector Stores um
71:57 - we just talked about
71:58 - multi-representation indexing um we I
72:01 - priv a link to a video that's very good
72:04 - talking about the different means of
72:05 - chunking so I encourage you to look at
72:06 - that and we're going to talk today about
72:09 - a technique called Raptor which you can
72:10 - kind of think of it as a technique for
72:12 - hierarchical
72:14 - indexing so the highle intuition is
72:17 - this some questions require very
72:20 - detailed information from a corpus to
72:23 - answer like pertain to a single document
72:25 - or single chunk so like we can call
72:28 - those low-level
72:29 - questions some questions require
72:31 - consolidation across kind broad swast of
72:34 - a document so across like many documents
72:37 - or many chunks within a document and you
72:39 - can call those like higher level
72:41 - questions and so there's kind of this
72:44 - challenge in retrieval and that
72:46 - typically we do like K nearest neighbors
72:48 - retrieval like we've been talking about
72:50 - you're fishing out some number of chunks
72:53 - but what if you have a question that
72:55 - requires information across like five
72:57 - six you know or a number of different
72:59 - chunks which may exceed you know the K
73:02 - parameter in your retrieval so again
73:04 - when you typically do retrieval you
73:06 - might set a k parameter of three which
73:08 - means you're retrieving three chunks
73:10 - from your vector store um and maybe you
73:12 - have a high very high level question
73:14 - that could benefit from infation across
73:16 - more than three so this technique called
73:18 - raptor is basically a way to build a
73:21 - hierarchical index of document summaries
73:25 - and the intuition is this you start with
73:27 - a set of documents as your Leafs here on
73:30 - the left you cluster them and then you
73:33 - Summarize each cluster so each cluster
73:36 - of similar documents um will consult
73:39 - information from across your context
73:42 - which is you know your context could be
73:44 - a bunch of different splits or could
73:45 - even be across a bunch of different
73:47 - documents you're basically capturing
73:49 - similar ones and you're consolidating
73:51 - the information across them in a summary
73:53 - and here's the interesting thing you do
73:54 - that
73:55 - recursively until either you hit like a
73:57 - limit or you end up with one single
73:59 - cluster that's a kind of very high level
74:01 - summary of all of your
74:03 - documents and what the paper shows is
74:05 - that if you basically just collapse all
74:08 - these and index them together as a big
74:10 - pool you end up with a really nice array
74:12 - of chunks that span the abstraction
74:15 - hierarchy like you have a bunch of
74:17 - chunks from Individual documents that
74:19 - are just like more detailed chunks
74:21 - pertaining to that you know single
74:23 - document but you also have chunks from
74:25 - these summaries or I would say like you
74:27 - know maybe not chunks but in this case
74:29 - the summary is like a distillation so
74:31 - you know raw chunks on the left that
74:33 - represent your leavs are kind of like
74:35 - the rawest form of information either
74:37 - raw chunks or raw documents and then you
74:40 - have these higher level summaries which
74:42 - are all indexed together so if you have
74:44 - higher level questions they should
74:46 - basically be more similar uh in sematic
74:49 - search for example to these higher level
74:50 - summary chunks if you have lower level
74:53 - questions then they'll retrieve these
74:55 - more lower level chunks and so you have
74:56 - better semantic coverage across like the
74:59 - abstraction hierarchy of question types
75:01 - that's the intuition they do a bunch of
75:02 - nice studies to show that this works
75:04 - pretty well um I actually did a deep
75:07 - dive video just on this which I link
75:09 - below um I did want to cover it briefly
75:12 - just at a very high level um so let's
75:15 - actually just do kind of a code walkr
75:17 - and I've added it to this rack from
75:19 - scratch course notebook but I link over
75:21 - to my deep dive video as well as the
75:23 - paper and the the full code notebook
75:27 - which is already checked in is discussed
75:28 - at more length in the Deep dive the
75:31 - technique is a little bit detailed so I
75:33 - only want to give you very high levels
75:35 - kind of overview here and you can look
75:37 - at the Deep dive video if you want to go
75:39 - in more depth again we talked through
75:41 - this abstraction
75:43 - hierarchy um I applied this to a large
75:46 - set of Lang chain documents um so this
75:49 - is me loading basically all of our Lang
75:51 - chain expression language docs so this
75:53 - is on the order of 30 documents you can
75:55 - see I do a histogram here of the token
75:57 - counts per document some are pretty big
76:00 - most are fairly small less than you know
76:02 - 4,000 tokens um and what I did is I
76:05 - indexed all of them um individually so
76:09 - the all those raw documents you can kind
76:11 - of Imagine are here on the left and then
76:14 - I do um I do embedding I do clustering
76:18 - summarization and I do that recursively
76:21 - um until I end up with in this case I
76:24 - believe I only set like three levels of
76:27 - recursion and then I save them all my
76:29 - Vector store so that's like the highle
76:31 - idea I'm applying this Raptor technique
76:33 - to a whole bunch of Lang chain documents
76:36 - um that have fairly large number of
76:38 - tokens um so I do that um and yeah I use
76:45 - actually use both CLA as well as open AI
76:48 - here um this talks through the
76:50 - clustering method which they that they
76:52 - use which is pretty interesting you can
76:54 - kind of dig into that on your own if if
76:55 - you're really um interested this is a
76:58 - lot of their code um which I cite
77:00 - accordingly um this is basically
77:02 - implementing the clustering method that
77:03 - they use um and this is just simply the
77:08 - document embedding stage um this is like
77:12 - basically embedding uh and clustering
77:15 - that's really it uh some text formatting
77:19 - um summarizing of the clusters right
77:22 - here um and then this is just running
77:24 - that whole process recursively that's
77:26 - really it um this is tree building so
77:30 - basically I have the RO the rod docs
77:33 - let's just go back and look at Doc texts
77:35 - so this should be all my raw documents
77:37 - uh so that's right you can see it here
77:39 - doc text is basically just the text in
77:41 - all those Lang chain documents that I
77:43 - pulled
77:45 - um and so I run this process on them
77:49 - right
77:50 - here uh so this is that recursive
77:52 - embedding cluster basically runs and
77:54 - produces is that tree here's the results
77:57 - um this is me just going through the
77:59 - results and basically adding the result
78:02 - text to this list of uh texts um oh okay
78:07 - so here's what I do this Leaf text is
78:09 - all the raw documents and I'm appending
78:12 - to that all the summaries that's all
78:14 - it's going on and then I'm indexing them
78:16 - all together that's the key Point rag
78:19 - chain and there you have it that's
78:21 - really all you do um so anyway I
78:23 - encourage you to look at this in depth
78:24 - it's a pretty interesting technique it
78:26 - works well long with long contexts so
78:29 - for example one of the arguments I made
78:30 - is that it's kind of a nice approach to
78:32 - consult information across like a span
78:35 - of large
78:36 - documents like in this particular case
78:38 - my individual documents were lch
78:40 - expression language docs uh each each
78:42 - being somewhere in the order of you know
78:44 - in this case like you know most of them
78:46 - are less than 4,000 tokens some pretty
78:48 - big but I index them all I cluster them
78:51 - without any splits uh embed them cluster
78:54 - them build this tree um and go from
78:56 - there and it all works because we now
78:58 - have llms that can go out to you know
79:01 - 100 or 200,000 up to million tokens and
79:03 - Contex so you can actually just do this
79:05 - process for big swats of documents in
79:08 - place without any without any splitting
79:10 - uh it's a pretty nice approach so I
79:12 - encourage you to think about it look at
79:13 - it watch the deep that video If you
79:14 - really want to go deeper on this um
79:20 - thanks hi this is Lance from Lang chain
79:22 - this is the 14th part of our rag from
79:24 - scratch series we're going to I'm going
79:26 - to be talking about an approach called
79:28 - cold
79:29 - bear um so we've talked about a few
79:34 - different approaches for indexing and
79:37 - just as kind of a refresher indexing
79:38 - Falls uh kind of right down here in our
79:41 - flow we started initially with career
79:43 - translation taking a question
79:45 - translating it in some way to optimize
79:47 - retrieval we talked about routing it to
79:49 - a particular database we then talked
79:51 - about query construction so going from
79:53 - natural language to the DSL or domain
79:56 - specific language for E any of the
79:59 - databases that you want to work with
80:01 - those are you know metadata filters for
80:02 - Vector stores or Cipher
80:05 - for graph DB or SQL for relational DB so
80:09 - that's kind of the flow we talked about
80:10 - today we talked about some indexing
80:12 - approaches like multi-representation
80:14 - indexing we gave a small shout out to
80:16 - greet camer in the series on chunking uh
80:19 - we talked about hierarchical indexing
80:22 - and I want to include one Advanced kind
80:24 - embedding approach so we talked a lot
80:26 - about embeddings are obviously very
80:28 - Central to semantic similarity search um
80:31 - and
80:32 - retrieval so one of the interesting
80:35 - points that's been brought up is that
80:38 - embedding models of course take a
80:39 - document you can see here on the top and
80:42 - embed it basically compress it to a
80:45 - vector so it's kind of a compression
80:48 - process you representing all the
80:49 - semantics of that document in a single
80:51 - Vector you're doing the same to your
80:53 - question you're doing similarity search
80:55 - between the question embedding and the
80:57 - document embedding um in order to
80:59 - perform retrieval you're typically
81:01 - taking the you know K most similar um
81:05 - document abetting is given a question
81:07 - and that's really how you're doing it
81:10 - now a lot of people said well hey the
81:12 - compressing a full document with all
81:13 - this Nuance to single Vector seems a
81:15 - little bit um overly restrictive right
81:18 - and this is a fair question to ask um
81:21 - there's been some interesting approaches
81:22 - to try to address that and one is this
81:24 - this this approach method called Co bear
81:28 - so the intuition is actually pretty
81:30 - straightforward there's a bunch of good
81:31 - articles I link down here this is my
81:33 - little cartoon to explain it which I
81:35 - think is hopefully kind of helpful but
81:37 - here's the main idea instead of just
81:39 - taking a document and compressing it
81:41 - down to a single Vector basically single
81:44 - uh what we might call embedding Vector
81:46 - we take the document we break it up into
81:49 - tokens so tokens are just like you know
81:51 - units of of content it depends on the
81:54 - token areas you use we talked about this
81:56 - earlier so you basically tokenize it and
81:59 - you produce basically an embedding or
82:01 - vector for every token and there's some
82:03 - kind of positional uh waiting that
82:05 - occurs when you do this process so you
82:08 - obviously you look to look at the
82:09 - implementation understand the details
82:10 - but the intuition is that you're
82:12 - producing some kind of representation
82:14 - for every token okay and you're doing
82:17 - the same thing for your question so
82:19 - you're taking your question you're
82:20 - breaking into a tokens and you have some
82:22 - representation or vector per token
82:25 - and then what you're doing is for every
82:27 - token in the question you're Computing
82:29 - the similarity across all the tokens in
82:33 - the document and you're finding the max
82:36 - you're taking the max you're storing
82:38 - that and you're doing that process for
82:42 - all the tokens in the question so again
82:45 - token two you compare it to every token
82:48 - in the in the document compute the
82:51 - Max and then the final score is in this
82:54 - case the sum of the max similarities uh
82:57 - between every question token and any
83:00 - document token so it's an interesting
83:03 - approach uh it reports very strong
83:06 - performance latency is definitely a
83:08 - question um so kind of production
83:11 - Readiness is something you should look
83:12 - into but it's a it's an approach that's
83:14 - worth mentioning here uh because it's
83:16 - pretty
83:18 - interesting um and let's walk through
83:20 - the
83:21 - code
83:22 - so there's actually nice Library called
83:25 - rouille which makes it very easy to play
83:27 - with Co bear um she's pip install it
83:30 - here I've already done that and we can
83:33 - use one of their pre-train models to
83:35 - mediate this process so I'm basically
83:36 - following their documentation this is
83:38 - kind of what they recommended um so I'm
83:40 - running this
83:41 - now hopefully this runs somewhat quickly
83:43 - I'm not sure I I previously have loaded
83:45 - this model so hopefully it won't take
83:47 - too long and yeah you can see it's
83:48 - pretty quick uh I'm on a Mac M2 with 32
83:51 - gigs um so just as like a context in
83:54 - terms of my my system um this is from
83:56 - their documentation we're just grabbing
83:57 - a Wikipedia page this is getting a full
83:59 - document on Miyazaki so that's cool
84:03 - we're going to grab that now this is
84:05 - just from their docs this is basically
84:06 - how we create an index so we provide the
84:09 - you know some index name the collection
84:11 - um the max document length and yeah you
84:13 - should look at their documentation for
84:15 - these flags these are just the defaults
84:16 - so I'm going to create my index um so I
84:19 - get some logging here so it it's working
84:21 - under the hood um and by the way I
84:24 - actually have their documentation open
84:26 - so you can kind of follow along um
84:30 - so
84:31 - um let's see yeah right about here so
84:35 - you can kind of follow this indexing
84:36 - process to create an index you need to
84:37 - load a train uh a trained model this can
84:41 - be either your own pre-train model or
84:42 - one of ours from The Hub um and this is
84:44 - kind of the process we're doing right
84:45 - now create index is just a few lines of
84:48 - code and this is exactly what we're
84:49 - doing um so this is the you know my
84:52 - documents and this is the indexing step
84:54 - that we just we just kind of walk
84:56 - through and it looks like it's done um
84:58 - so you get a bunch of logging here
85:00 - that's fine um now let's actually see if
85:02 - this works so we're going to run drag
85:04 - search what an emotion Studio did Miaki
85:06 - found set our K parameter and we get
85:09 - some results okay so it's running and
85:13 - cool we get some documents out so you
85:15 - know it seems to work now what's nice is
85:17 - you can run this within lighting chain
85:18 - as a liting chain retriever so that
85:20 - basically wraps this as a lighting chain
85:22 - Retriever and then you can use it freely
85:24 - as a retriever within Lang chain it
85:25 - works with all the other different LMS
85:27 - and all the other components like
85:28 - rankers and so forth that we talk
85:30 - through so you can use this directly as
85:32 - a retriever let's try this out and boom
85:35 - nice and fast um and we get our
85:38 - documents again this is a super simple
85:40 - test example you should run this maybe
85:42 - on more complex cases but it's pretty
85:44 - pretty easy spin up it's a really
85:45 - interesting alternative indexing
85:47 - approach um using again like we talked
85:49 - through um a very different algorithm
85:53 - for computing do similarity that may
85:55 - work better I think an interesting
85:57 - regime to consider this would be longer
85:59 - documents so if you want like longer um
86:02 - yeah if if you basically want kind of
86:04 - long context embedding I think you
86:06 - should look into for example the uh Max
86:09 - token limits for this approach because
86:11 - it partitions the document into into
86:13 - each token um I would be curious to dig
86:16 - into kind of what the overall context
86:18 - limits are for this approach of coar but
86:21 - it's really interesting to consider and
86:22 - it reports very strong performance so
86:24 - again I encourage you to play with it
86:26 - and this is just kind of an intro to how
86:27 - to get set up and to start experimenting
86:29 - with it really quickly
86:32 - thanks hi this is Lance from Lang chain
86:35 - I'm going to be talking about using
86:36 - langra to build a diverse and
86:39 - sophisticated rag
86:40 - flows so just to set the stage the basic
86:44 - rag flow you can see here starts with a
86:47 - question retrieval of relevant documents
86:49 - from an index which are passed into the
86:52 - context window of an llm for generation
86:54 - of an answer ground in your documents
86:57 - that's kind of the basic
86:59 - outline and we can see it's like a very
87:01 - linear
87:02 - path um in practice though you often
87:06 - encounter a few different types of
87:08 - questions like when do we actually want
87:10 - to
87:11 - retrieve based upon the context of the
87:13 - question um are the retrieve documents
87:16 - actually good or not and if they're not
87:18 - good should we discard them and then how
87:21 - do we loot back and retry retrieval with
87:23 - for example and improved
87:25 - question so these types of questions
87:29 - motivate an idea of active rag which is
87:32 - a process where an llm actually decides
87:34 - when and where to retrieve based upon
87:36 - like existing
87:37 - retrievals or existing
87:40 - Generations now when you think about
87:43 - this there's a few different levels of
87:45 - control that you have over an llm in a
87:48 - rag
87:49 - application the base case like we saw
87:52 - with our chain is just use an llm to
87:55 - choose a single steps output so for
87:58 - example in traditional rag you feed it
88:00 - documents and it decides to generation
88:04 - so it's just kind of one step now a lot
88:07 - of rag workflows will use the idea of
88:09 - routing so like given a question should
88:12 - I route it to a vector store or a graph
88:16 - DB um and we have seen this quite a
88:20 - bit now this newer idea that I want to
88:23 - introduce
88:24 - is how do we build more sophisticated
88:28 - logical
88:29 - flows um in a rag
88:32 - pipeline um that you let the llm choose
88:36 - between different steps but specify all
88:41 - the transitions that are
88:43 - available and this is known as we call a
88:45 - state
88:46 - machine now there's a few different
88:50 - architectures that have emerged uh to
88:53 - build different types of rag
88:54 - chains and of course chains are
88:58 - traditionally used just for like very
88:59 - basic rag but this notion of State
89:02 - machine is a bit newer and Lang graph
89:06 - which we recently released provides a
89:07 - really nice way to build State machines
89:10 - for Rag and for other
89:13 - things and the general idea here is that
89:16 - you can lay out more diverse and
89:18 - complicated rag
89:20 - flows and then Implement them as graphs
89:24 - and it kind of motivates this more broad
89:26 - idea of of like flow engineering and
89:28 - thinking through the actual like
89:30 - workflow that you want and then
89:32 - implementing it um and we're gonna
89:35 - actually do that right now so I'm GNA Pi
89:39 - a recent paper called CAG corrective rag
89:42 - which is really a nice method um for
89:46 - active rag that incorporates a few
89:48 - different
89:50 - ideas um so first you retrieve documents
89:54 - and then you grade
89:55 - them now if at least one document
89:59 - exceeds the threshold for
90:01 - relevance you go to generation you
90:04 - generate your
90:06 - answer um and it does this knowledge
90:10 - refinement stage after that but let's
90:13 - not worry about that for right now it's
90:15 - kind of not essential for understanding
90:16 - the basic flow here so again you do a
90:20 - grade for relevance for every document
90:23 - if any is relevant you
90:25 - generate now if they're all ambiguous or
90:29 - incorrect based upon your
90:31 - grader you retrieve from an external
90:34 - Source they use web
90:36 - search and then they pass that as their
90:39 - context for answer
90:42 - generation so it's a really neat
90:44 - workflow where you're doing retrieval
90:46 - just like with basic rag but then you're
90:48 - reasoning about the documents if they're
90:50 - relevant go ahead and at least one is
90:53 - relevant go ahead and generate if
90:54 - they're not retrieve from alternative
90:57 - source and then pack that into the
90:59 - context and generate your
91:02 - answer so let's see how we would
91:04 - implement this as a estate machine using
91:09 - Lang
91:10 - graph um we'll make a few
91:13 - simplifications
91:15 - um we're going to first decide if any
91:19 - documents are relevant we'll go ahead
91:22 - and do the the web search
91:25 - um to supplement the output so that's
91:28 - just like kind of one minor
91:29 - modification um we'll use tab search for
91:32 - web search um we use Query writing to
91:35 - optimize the search for uh to optimize
91:38 - the web search but it follows a lot of
91:40 - the the intuitions of the main paper uh
91:44 - small note here we set the Tav API key
91:48 - and another small mode I've already set
91:50 - my lsmith API key um with which we'll
91:54 - see is useful a bit later for observing
91:57 - the resulting
91:59 - traces now I'm going to index three blog
92:02 - posts that I
92:03 - like um I'm going to use chroma DB I'm G
92:06 - use open ey embeddings I'm going to run
92:09 - this right now this will create a vector
92:11 - store for me from these three blog
92:15 - posts and then what I'm going to do is
92:19 - Define
92:20 - State now this is kind of the core
92:23 - object that going to be passed around my
92:25 - graph that I'm going to
92:27 - modify and right here is where I Define
92:29 - it and the key point to note right now
92:32 - is it's just a dictionary and it can
92:35 - contain things that are relevant for rag
92:37 - like question documents generation and
92:40 - we'll see how we update that in in in a
92:42 - little bit but the first thing to note
92:44 - is we Define our state and this is
92:47 - what's going to be modified in every Noe
92:49 - of our
92:50 - graph now here's really the Crux of it
92:53 - and this is the thing I want to zoom in
92:54 - on a little bit um
92:58 - so when you kind of move from just
93:00 - thinking about promps to thinking about
93:02 - overall flows it it's like kind of a fun
93:05 - and interesting exercise I kind of think
93:07 - about this as it's been mentioned on
93:09 - Twitter a little bit more like flow
93:12 - engineering so let's think through what
93:15 - was actually done in the paper and what
93:19 - modifications to our state are going to
93:21 - happen in each stage so we start with a
93:24 - question you can see that on the far
93:25 - left and this kind of state is represent
93:28 - as a dictionary like we have we start
93:29 - with a question we perform retrieval
93:31 - from our Vector store which we just
93:33 - created that's going to give us
93:35 - documents so that's one node we made an
93:38 - an adjustment to our state by adding
93:41 - documents that's step
93:43 - one now we have a second node where
93:45 - we're going to grade the documents and
93:48 - in this node we might filter some out so
93:50 - we are making a modification to state
93:52 - which is why it's a node so we're going
93:54 - to have a
93:55 - greater then we're going to have what
93:58 - we're going to call a conditional Edge
94:00 - so we saw we went from question to
94:02 - retrieval retrieval always goes to
94:05 - grading and now we have a
94:07 - decision if any document is
94:10 - irrelevant we're going to go ahead and
94:14 - do web search to
94:15 - supplement and if they're all relevant
94:17 - will go to generation it's a minor kind
94:20 - of a minor kind of logical uh decision
94:23 - ision that we're going to
94:25 - make um if any are not relevant we'll
94:28 - transform the query and we'll do web
94:31 - search and we'll use that for Generation
94:33 - so that's really it and that's how we
94:35 - can kind of think about our flow and how
94:37 - our States can be modified throughout
94:39 - this
94:40 - flow now all we then need to do and I I
94:44 - kind of found
94:46 - spending 10 minutes thinking carefully
94:48 - through your flow
94:50 - engineering is really valuable because
94:52 - from here it's really implementation
94:55 - details um and it's pretty easy as
94:58 - you'll see so basically I'm going to run
95:02 - this code block but then we can like
95:03 - walk through some of it I won't show you
95:04 - everything so it'll get a little bit
95:06 - boring but really all we're doing is
95:09 - we're finding functions for every node
95:12 - that take in the state and modify in
95:15 - some way that's all it's going on so
95:17 - think about retrieval we run retrieval
95:19 - we take in state remember it's a dict we
95:22 - get our state dick like this
95:24 - we extract one keyy question from our
95:26 - dick we pass that to a retriever we get
95:29 - documents and we write back out State
95:32 - now with documents key added that's
95:36 - all generate going to be similar we take
95:39 - in state now we have our question and
95:41 - documents we pull in a prompt we Define
95:44 - an llm we do minor post processing on
95:47 - documents we set up a chain for
95:49 - retrieval uh or sorry for Generation
95:51 - which is just going to be take our
95:52 - prompt pump Plum that to an llm
95:55 - partially output a string and we run it
95:57 - right here invoking our documents in our
96:01 - question to get our answer we write that
96:04 - back to State that's
96:06 - it and you can kind of follow here for
96:09 - every node we just Define a function
96:12 - that performs the state modification
96:13 - that we want to do on that
96:15 - node grading documents is going to be
96:17 - the same um in this case I do a little
96:21 - thing extra here because I actually
96:23 - Define a identic data model for my
96:25 - grader so that the output of that
96:28 - particular grading chain is a binary yes
96:31 - or no you can look at the code make sure
96:33 - it's all shared um and that just makes
96:36 - sure that our output is is very
96:38 - deterministic so that we then can down
96:41 - here perform logical filtering so what
96:45 - you can see here is um we Define this
96:49 - search value no and we iterate through
96:53 - our documents we grade them if any
96:56 - document uh is graded as not relevant we
97:00 - flag this search thing to yes that means
97:04 - we're going to perform web search we
97:06 - then add that to our state dict at the
97:08 - end so run web search now that value is
97:10 - true that's
97:12 - it and you can kind of see we go through
97:14 - some other nodes here there's web search
97:16 - node um now here is where our one
97:20 - conditional Edge we Define right here
97:23 - this is where where we decide to
97:24 - generate or not based on that search key
97:27 - so we again get our state let's extract
97:30 - the various values so we have this
97:32 - search value now if search is yes we
97:37 - return the next no that we want to go to
97:40 - so in this case it'll be transform query
97:42 - which will then go to web search else we
97:46 - go to
97:47 - generate so what we can see is we laid
97:51 - out our graph which you can kind of see
97:53 - up
97:55 - here and now we Define functions for all
97:57 - those nodes as well as the conditional
98:01 - Edge and now we scroll down all we have
98:05 - to do is just lay that out here again as
98:08 - our flow and this is kind of what you
98:10 - might think of as like kind of flow
98:11 - engineering where you're just laying out
98:13 - the graph as you drew it where we have
98:17 - set our entry point as retrieve we're
98:19 - adding an edge between retrieve and
98:21 - grade documents so we went retrieval
98:23 - grade documents we add our conditional
98:25 - Edge depending on the grade either
98:28 - transform the query go to web search or
98:31 - just go to generate we create an edge
98:34 - between transform the query and web
98:35 - search then web search to generate and
98:38 - then we also have an edge generate to
98:40 - end and that's our whole graph that's it
98:42 - so we can just run
98:44 - this and now I'm going to ask a question
98:47 - so let's just say um how does agent
98:51 - memory work for example let's just try
98:53 - that and what this is going to do is
98:55 - going to print out what's going on as we
98:58 - run through this graph so um first we
99:01 - going to see output from
99:03 - retrieve this is going to be all of our
99:05 - documents that we retrieved so that's
99:07 - that's fine this just from our our
99:08 - retriever then you can see that we're
99:11 - doing a relevance check across our
99:13 - documents and this is kind of
99:15 - interesting right you can see we grading
99:17 - them here one is grade as not
99:20 - relevant um and okay you can see the
99:23 - documents are now filtered because we
99:24 - removed the one that's not relevant and
99:26 - because one is not relevant we decide
99:29 - okay we're going to just transform the
99:31 - query and run web
99:33 - search and um you can see after query
99:37 - transformation we rewrite the question
99:39 - slightly we then run web
99:42 - search um and you can see from web
99:44 - search it searched from some additional
99:47 - sources um which you can actually see
99:50 - here it's
99:51 - appended as a so here it is so here it's
99:55 - a new document appended from web search
99:57 - which is from memory knowledge
99:59 - requirements so it it basically looked
100:01 - up some AI architecture related to
100:03 - memory uh web results so that's fine
100:06 - that's exactly what we want to
100:08 - do and then um we generate a
100:11 - response so that's great and this is
100:14 - just showing you everything in kind of
100:15 - gory detail but I'm going to show you
100:18 - one other thing that's that's really
100:19 - nice about this if I go to lsmith
100:24 - I have my AP I ke set so all my
100:26 - Generations are just logged to to lsmith
100:29 - and I can see my Lang graph run here now
100:33 - what's really cool is this shows me all
100:36 - of my nodes so remember we had retrieve
100:42 - grade we evaluated the grade because one
100:45 - was irrelevant we then went ahead and
100:47 - transformed the query we did a web
100:50 - search we pended that to our context you
100:52 - can see all those steps are laid out
100:54 - here in fact you can even look at every
100:56 - single uh grader and its output I will
100:59 - move this up
101:01 - slightly um so you can see the the
101:04 - different scores for grades okay so this
101:06 - particular retrieval was graded as as
101:09 - not relevant so that's fine that that
101:11 - can happen in some cases and because of
101:15 - that um we did a query transformation so
101:18 - we modified the question slightly how
101:21 - does memory how does the memory system
101:23 - an artificial agents function so it's
101:25 - just a minor rephrasing of the question
101:27 - we did this Tav web search this is where
101:30 - it queried from this particular blog
101:33 - post from medium so it's like a sing web
101:35 - query we can like sanity check it and
101:37 - then what's need is we can go to our
101:39 - generate step look at open Ai and here's
101:41 - our full prompt how does the memory
101:43 - system in our official agents function
101:46 - and then here's all of our documents so
101:49 - this is the this is the web search as
101:51 - well as we still have the Rel chunks
101:54 - that were retrieved from our blog posts
101:57 - um and then here's our answer so that's
102:01 - really it you can see how um really
102:05 - moving from the notion of just like I'll
102:09 - actually go back to the original um
102:12 - moving
102:14 - from uh I will try to open this up a
102:17 - little
102:18 - bit um
102:24 - yeah I can see my face
102:26 - still um the transition from laying out
102:33 - simple
102:35 - chains to
102:38 - flows is a really interesting and
102:40 - helpful way of thinking about why graphs
102:42 - are really interesting because you can
102:45 - encode more sophisticated logical
102:48 - reasoning
102:49 - workflows but in a
102:51 - very like clean and well-engineered way
102:56 - where you can specify all the
102:58 - transitions that you actually want to
103:00 - have
103:01 - executed um and I actually find this way
103:05 - of thinking and building kind of logical
103:08 - uh like workflows really
103:10 - intuitive um we have a blog post coming
103:13 - out uh tomorrow that discusses both
103:17 - implementing self rag as well as C rag
103:20 - for two different active rag approaches
103:22 - using using uh this idea of of State
103:25 - machines and Lang graph um so I
103:28 - encourage you to play with it uh I found
103:31 - it really uh intuitive to work with um I
103:35 - also found uh inspection of traces to be
103:38 - quite intuitive using Lang graph because
103:43 - every node is enumerated pretty clearly
103:46 - for you which is not always the case
103:48 - when you're using other types of of more
103:50 - complex reasoning approaches for example
103:52 - like agents so in any case um I hope
103:56 - this was helpful and I definitely
103:58 - encourage you to check out um kind of
104:00 - this notion of like flow engineering
104:02 - using Lang graph and in the context of
104:04 - rag it can be really powerful hopefully
104:06 - as you've seen here thank
104:09 - you hey this is Lance from Lang chain I
104:12 - want to talk to a recent paper that I
104:14 - saw called adaptive rag which brings
104:16 - together some interesting ideas that
104:17 - have kind of been covered in other
104:18 - videos but this actually ties them all
104:21 - together in kind of a fun way so the the
104:23 - two big ideas to talk about here are one
104:27 - of query analysis so we've actually done
104:30 - kind of a whole rag from scratch series
104:31 - that walks through each of these things
104:32 - in detail but this is a very nice
104:35 - example of how this comes together um
104:37 - with some other ideas we've been talking
104:39 - about so query analysis is typically the
104:41 - process of taking an input question and
104:44 - modifying in some way uh to better
104:46 - optimize retrieval there's a bunch of
104:48 - different methods for this it could be
104:50 - decomposing it into sub questions it
104:52 - could be using some clever techniques
104:54 - like stepb back prompting um but that's
104:57 - kind of like the first stage of query
104:59 - analysis then typically you can do
105:01 - routing so you route a question to one
105:03 - of multiple potential sources it could
105:05 - be one or two different Vector stores it
105:08 - could be relational DB versus Vector
105:10 - store it could be web search it could
105:12 - just be like an llm fallback right so
105:15 - this is like one kind of big idea query
105:17 - analysis right it's kind of like the
105:18 - front end of your rag pipeline it's
105:20 - taking your question it's modifying it
105:22 - in some way it's sending it to the right
105:24 - place be it a web search be it a vector
105:26 - store be it a relational DB so that's
105:29 - kind of topic one now topic two is
105:32 - something that's been brought up in a
105:34 - few other videos um of what I kind of
105:36 - call Flow engineering or adaptive rag
105:40 - which is the idea of doing tests in your
105:43 - rag pipeline or in your rag inference
105:45 - flow uh to do things like check
105:47 - relevance documents um check whether or
105:51 - not the answer contains hallucinations
105:53 - so this recent blog post from Hamil
105:55 - Hussein actually covers evaluation in in
105:58 - some really nice detail and one of the
106:00 - things he highlighted
106:01 - explicitly is actually this topic so he
106:04 - talks about unit tests and in particular
106:06 - he says something really interesting
106:08 - here he says you know unlike typical
106:09 - unit tests you want to organize these
106:11 - assertions in places Beyond typical unit
106:14 - testing such as data cleaning and here's
106:17 - the key Point automatic retries during
106:19 - model inference that's the key thing I
106:21 - want to like draw your attention to to
106:23 - it's a really nice approach we've talked
106:25 - about some other papers that do that
106:26 - like corrective rag self rag but it's
106:28 - also cool to see it here and kind of
106:30 - encapsulated in this way the main idea
106:33 - is that you're using kind of unit tests
106:34 - in your flow to make Corrections like if
106:38 - your retrieval is bad you can correct
106:39 - from that if your generation has
106:41 - hallucinations you can correct from that
106:43 - so I'm going to kind of draw out like a
106:45 - cartoon diagram of what we're going to
106:47 - do here and you can kind of see it here
106:50 - we're starting with a question we talked
106:52 - about query analysis we're going to take
106:53 - our question and we're going to decide
106:55 - where it needs to go and for this
106:57 - particular toy example I'm going to say
106:59 - either send it to a vector store send it
107:01 - to web search or just have the llm
107:03 - answer it right so that's like kind of
107:05 - my fallback
107:06 - Behavior then we're going to bring in
107:08 - that idea of kind of online flow
107:10 - engineering or unit testing where I'm
107:13 - going to have my retrieval either from
107:15 - the VOR store or web search I'm then
107:17 - going to ask is this actually relevant
107:19 - to the question if it isn't I'm actually
107:21 - going to kick back to web sech so this
107:23 - is a little bit more relevant in the
107:24 - case if I've routed to to the vector
107:27 - store done retrieval documents aren't
107:29 - relevant I'll have a fallback
107:32 - mechanism um then I'm going to generate
107:34 - I check for hallucinations in my
107:36 - generation and then I check for um for
107:39 - whether or not the the generation
107:41 - actually answers the question then I
107:42 - return my answer so again we're tying
107:44 - together two ideas one is query analysis
107:47 - like basically taking a question routing
107:49 - it to the right place modifying it as
107:51 - needed and then kind of online unit
107:54 - testing and iterative flow
107:56 - feedback so to do this I've actually
108:00 - heard a lot of people talk online about
108:02 - command r a new model release from gooh
108:04 - here it has some pretty nice properties
108:07 - that I was kind of reading about
108:08 - recently so one it has nice support for
108:11 - Tool use and it does support query
108:14 - writing in the context of tool use uh so
108:18 - this all rolls up in really nice
108:20 - capabilities for routing it's kind of
108:22 - one now two it's small it's 35 billion
108:26 - parameter uh it's actually open weight
108:28 - so you can actually run this locally and
108:30 - I've tried that we can we can talk about
108:31 - that later uh so and it's also fast
108:34 - served via the API so it's kind of a
108:36 - small model and it's well tuned for rag
108:39 - so I heard a lot of people talking about
108:40 - using coher for Rag and it has a large
108:43 - context 120,000 tokens so this like a
108:45 - nice combination of properties it
108:47 - supports to and routing it's small and
108:49 - fast so it's like quick for grading and
108:51 - it's well tuned for rag so it's actually
108:54 - a really nice fit for this particular
108:55 - workflow where I want to do query
108:57 - analysis and routing and I want to do
108:59 - kind of online checking uh and rag so
109:02 - kind of there you go now let's just get
109:05 - to the coding bit so I have a notebook
109:07 - kind of like usual I've done a few pip
109:09 - installs you can see it's nothing exotic
109:11 - I'm bringing Lang chain coh here I set
109:13 - my coher API key now I'm just going to
109:16 - set this Lang chain project within
109:18 - lsmith so all my traces for this go to
109:20 - that project and I have enabled tracing
109:23 - so I'm using Langs Smith here so we're
109:25 - going to walk through this flow and
109:27 - let's do the first thing let's just
109:28 - build a vector store so I'm going to
109:30 - build a vector store using coherent
109:32 - beddings with chroma open source Vector
109:34 - DB runs locally from three different web
109:37 - pages on blog post that I like so it
109:39 - pertains to agents prompt engineering
109:41 - and adversarial attacks so now I have a
109:43 - retriever I can run retriever invoke and
109:46 - I can ask a question about you know
109:48 - agent
109:50 - memory agent
109:53 - memory and there we go so we get
109:56 - documents back so there we go we have a
109:58 - retriever now now here's where I'm going
110:01 - to bring in coh here I also want a
110:03 - router so you look at our flow the first
110:06 - step is this routing stage right so what
110:08 - I'm going to do is I'm guess we going to
110:10 - find two tools a web search tool and a
110:14 - vector store tool okay in my Preamble is
110:17 - just going to say you're an expert
110:18 - routing user questions to Vector store
110:20 - web search now here's the key I tell it
110:23 - what the vector store has so again my
110:25 - index my Vector store has agents prompt
110:28 - engineering adial tax I just repeat that
110:30 - here agents prompt adversarial tax so it
110:32 - knows what's in the vector store um so
110:35 - use it for questions on these topics
110:36 - otherwise use web search so that's it I
110:40 - use command R here now I'm going to bind
110:42 - these tools to the model and attach the
110:44 - Preamble and I have a structured LM
110:47 - router so let's give it a let's give
110:49 - this a few tests just to like kind of
110:51 - sandbox this a little bit
110:53 - so I can inval here's my chain I have a
110:55 - router prompt I pass that to the
110:56 - structured LM router which I defined
110:58 - right here and um let's ask a few
111:01 - different questions like who will the
111:02 - Bears draft in the NFL draft with types
111:05 - of agent memory and Hi how are you so
111:08 - I'm going to kick that off and you can
111:10 - see you know it does web search it does
111:13 - it goes to Vector store and then
111:14 - actually returns this false so that's
111:16 - kind of interesting um this is actually
111:19 - just
111:20 - saying if it does not use either tool so
111:23 - for that particular query web search or
111:26 - the vector store was inappropriate it'll
111:28 - just say hey I didn't call one of those
111:30 - tools so that's interesting we'll use
111:31 - that later so that's my router tool now
111:35 - the second thing is my grader and here's
111:38 - where I want to show you something
111:40 - really nice that is generally useful uh
111:43 - for many different problems you might
111:45 - encounter so here's what I'm doing I'm
111:48 - defining a data model uh for My Grade so
111:51 - basically grade documents it's going to
111:54 - have this is a pantic object it is just
111:57 - basically a binary score here um field
112:00 - specified here uh documents are relevant
112:03 - to the question yes no I have a preamble
112:05 - your grer assessing relevance of
112:06 - retrieve documents to a user question um
112:09 - blah blah blah so you know and then
112:11 - basically give it a b score yes no I'm
112:13 - using command R but here's the catch I'm
112:16 - using this wi structured outputs thing
112:18 - and I'm passing my grade documents uh
112:21 - data model to that that so this is the
112:23 - key thing we can test this right now as
112:26 - well it's going to return an object
112:29 - based on the schema I give it which is
112:31 - extremely useful for all sorts of use
112:33 - cases and let's actually Zoom back up so
112:36 - we're actually right here so this
112:39 - greater stage right I want to constrain
112:41 - the output to yes no I don't want any
112:43 - preambles I want anything because the
112:45 - logic I'm going to build in this graph
112:48 - is going to require a yes no binary
112:50 - response from this particular Edge in
112:52 - our graph
112:53 - so that's why this greater tool is
112:55 - really
112:56 - useful and I'm asking like a mock
112:59 - question types of agent memory I do a
113:01 - retriever I do a retrieval from our
113:03 - Vector store I get the tuck and I test
113:05 - it um I invoke our greater retrieval
113:08 - grater chain with the question the doc
113:11 - text and it's relevant as we would
113:13 - expect so that's good but again let's
113:16 - just kind of look at that a little bit
113:17 - more closely what's actually happening
113:19 - under the hood here here's the pantic
113:21 - object we passed
113:23 - here's the document in question I'm
113:24 - providing basically it's converting this
113:26 - object into coher function schema it's
113:29 - binding that to the
113:31 - llm we pass in the document question it
113:34 - returns an object basic a Json string
113:37 - per our pantic schema that's it and then
113:41 - it's just going to like parse that into
113:42 - a pantic object which we get at the end
113:44 - of the day so that's what's happening
113:45 - under the hood with this with structured
113:47 - output thing but it's extremely useful
113:49 - and you'll see we're going to use that a
113:50 - few different places um um because we
113:53 - want to ensure that in our in our flow
113:57 - here we have three different grading
113:58 - steps and each time we want to constrain
114:00 - the output to yes no we're going to use
114:02 - that structured output more than
114:04 - once um this is just my generation so
114:07 - this is good Old Rag let's just make
114:09 - sure that works um I'm using rag chain
114:12 - typical rag prompt again I'm using
114:15 - cohere for rag pretty easy and yeah so
114:19 - the rag piece works that's totally fine
114:21 - nothing to it crazy there um I'm going
114:24 - to find this llm fallback so this is
114:27 - basically if you saw a router chain if
114:31 - it doesn't use a tool I want to fall
114:33 - back and just fall back to the llm so
114:35 - I'm going to kind of build that as a
114:37 - little chain here so okay this is just a
114:39 - fallback I have my Preamble just you're
114:42 - you're an assistant answer the question
114:43 - based upon your internal knowledge so
114:46 - again that fallback behavior is what we
114:48 - have here so what we've done already is
114:51 - we defined our router piece we've
114:53 - defined our our basic retrieval our
114:56 - Vector store we already have here um
114:58 - we've defined our first logic or like
115:01 - grade check and we defined our fallback
115:04 - and we're just kind of roll through the
115:05 - parts of our graph and Define each piece
115:08 - um so I'm going to have two other
115:10 - graders and they're going to use the
115:11 - same thing we just talked about slightly
115:14 - different data model I mean same output
115:16 - but actually just slightly different uh
115:19 - prompt um and you know descript destion
115:22 - this in this case is the aners grounded
115:24 - the facts yes no this is my
115:25 - hallucination
115:26 - grater uh and then I have an answer
115:28 - grader as well and I've also run a test
115:31 - on each one and you can see I'm getting
115:32 - binary this this these objects out have
115:35 - a binary score so this a pantic object
115:38 - with a binary score uh and that's
115:39 - exactly what we want cool
115:43 - and I have a Search tool so that's
115:46 - really nice we've actually gone through
115:48 - and we've kind of laid out I have like a
115:50 - router I've tested it we have a vector
115:51 - story tested we've tested each of our
115:54 - graders here we've also tested
115:55 - generation of just doing rag so we have
115:58 - a bunch of pieces built here we have a
116:00 - fallback piece we have web search now
116:02 - the question is how do I Stitch these
116:04 - together into this kind of flow and for
116:06 - that I I like to use Lang graph we'll
116:09 - talk a little about Lang graph versus
116:10 - agents a bit later but I want to show
116:12 - you why this is really easy to do using
116:14 - Lang graph so what's kind of nice is
116:17 - I've kind of laid out all my logic here
116:19 - we've tested individually and now all
116:21 - I'm going to do
116:22 - is I'm going to first lay out uh the
116:25 - parts of my graph so what you're going
116:27 - to notice here is first there's a graph
116:30 - state so this state represents kind of
116:32 - the key parts of the graph or the key
116:35 - kind of objects within the graph that
116:36 - we're going to be modifying so this is
116:38 - basically a graph centered around rag
116:40 - we're going to have question generation
116:42 - and documents that's really kind of the
116:44 - main things we're going to be working
116:45 - with in our
116:46 - graph so then you're going to see
116:49 - something that's pretty intuitive I
116:51 - think what you're going to see is we're
116:53 - going to basically walk through this
116:55 - flow and for each of these little
116:57 - circles we're just going to find a
116:58 - function and these uh little squares or
117:02 - these these you can think about every
117:04 - Circle as a node and every kind of
117:06 - diamond here as as an edge or
117:08 - conditional Edge so that's actually what
117:10 - we're going to do right now we're going
117:11 - to lay out all of our nodes and edges
117:14 - and each one of them are just going to
117:15 - be a function and you're going to see
117:16 - how we do that right now so I'm going to
117:20 - go down here I def find my graph state
117:22 - so this is what's going to be kind of
117:23 - modified and propagated throughout my
117:24 - graph now all I'm going to do is I'm
117:27 - just going to find a function uh for
117:29 - each of those nodes so let me kind of go
117:32 - side by side and show you the diagram
117:33 - and then like kind of show the nodes
117:35 - next to it so here's the
117:38 - diagram so we have uh a retrieve node so
117:42 - that kind of represents our Vector store
117:43 - we have a fallback node that's this
117:46 - piece we have a generate node so that's
117:49 - basically going to do our rag you can
117:51 - see there we have a grade documents node
117:54 - kind of right
117:55 - here um and we have a web search node so
117:58 - that's right here cool now here's where
118:02 - we're actually to find the edges so you
118:04 - can see our edges are the pieces of the
118:05 - graph that are kind of making different
118:06 - decisions so this route question Edge
118:09 - basic conditional Edge is basically
118:12 - going to take an input question and
118:14 - decide where it needs to go and that's
118:15 - all we're doing down here it kind of
118:18 - follows what we did up at the top where
118:20 - we tested this individually so recall we
118:23 - basically just invoke that question
118:24 - router returns our source now remember
118:27 - if tool calls were not in the source we
118:30 - do our fall back so we show actually
118:32 - showed that all the way up here remember
118:34 - this if tool calls is not in the
118:36 - response this thing will just be false
118:38 - so that means we didn't either we didn't
118:39 - call web search and we didn't call uh
118:43 - our retriever tool so then we're just
118:45 - going to fall
118:46 - back
118:48 - um yep right here and this is just like
118:53 - uh you know a catch just in case a tool
118:54 - could make a decision but most
118:57 - interestingly here's where we choose a
118:59 - data source basically so um this is the
119:02 - output of our tool call we're just going
119:05 - to fish out the name of the tool so
119:08 - that's data source and then here we go
119:10 - if the data source is web search I'm
119:12 - returning web search as basically the
119:14 - next node to go to um otherwise if it's
119:18 - Vector store we return Vector store as
119:19 - the next node to go to so what's this
119:22 - search thing well remember we right up
119:24 - here Define this node web search that's
119:27 - it we're just going to go to that node
119:30 - um what's this Vector store um you'll
119:32 - see below how we can kind of tie these
119:35 - strings that we returned from the
119:36 - conditional Edge to the node we want to
119:38 - go to that's really it um same kind of
119:42 - thing here decide to generate that's
119:44 - going to roll in these two conditional
119:46 - edges into one um and basically it's
119:49 - going to do if there's no documents so
119:51 - basic basically if we filtered out all
119:54 - of our documents from this first test
119:56 - here um then what we're going to do is
120:01 - we've decided all documents are not
120:02 - relevant to the question and we're going
120:04 - to kick back to web search exactly as we
120:06 - show here so that's this piece um
120:09 - otherwise we're going to go to generate
120:11 - so that's this piece so again in these
120:14 - conditional edges you're basically
120:15 - implementing the logic that you see in
120:17 - our diagram right here that's all that's
120:19 - going on um and again this is just
120:23 - implementing the final two checks uh for
120:25 - hallucinations and and answer
120:27 - relevance um
120:30 - and um yep so here's our hallucination
120:34 - grader we then extract the grade if the
120:38 - if basically there are
120:39 - hallucinations um oh sorry in this case
120:42 - the grade actually yes means that the
120:45 - answer is grounded so we say answer is
120:48 - actually grounded and then we go to the
120:50 - next step we go to the next test that's
120:52 - all this is doing it's just basically
120:53 - wrapping this logic that we're
120:55 - implementing here in our graph so that's
120:57 - all that's going on and let's go ahead
120:59 - and Define all those things so nice we
121:02 - have all that um now we can actually go
121:05 - down a little bit and we can pull
121:09 - um this is actually where we stitch
121:12 - together everything so all it's
121:13 - happening here is you see we defined all
121:16 - these functions up here we just add them
121:18 - as nodes in our graph here and then we
121:21 - build our graph here basically by by
121:24 - basically laying out the flow or the
121:26 - connectivity between our nodes and edges
121:29 - so you know you can look at this
121:30 - notebook to kind of study in a bit of
121:31 - detail what's going on but frankly what
121:33 - I like to do here typically just draw
121:36 - out a graph kind of like we did up
121:38 - here and then Implement uh the Lo
121:42 - logical flow here in your graph as nodes
121:45 - and edges just like we're doing here
121:47 - that's all that's happening uh so again
121:49 - we have like our entry point is the
121:51 - router
121:52 - um this is like the output is this is
121:55 - basically directing like here's what the
121:57 - router is outputting and here's the next
122:00 - node to go to so that's it um and then
122:03 - for each node we're kind of applying
122:05 - like we're saying like what's what's the
122:06 - flow so web search goes to generate
122:09 - after um and retrieve goes to grade
122:12 - documents grade documents um kind of is
122:16 - is like is a conditional Edge um
122:18 - depending on the results we either do
122:19 - web search or generate and then our
122:21 - second one we go from generate to uh
122:25 - basically this grade uh generation
122:27 - versus documents in question based on
122:29 - the output of that we either have
122:31 - hallucinations we regenerate uh we found
122:34 - that the answer is not useful we kick
122:36 - back to web search or we end um finally
122:39 - we have that llm fallback and that's
122:41 - also if we go to the fallback we end so
122:44 - what you're seeing here is actually the
122:46 - the logic flow we're laying out in this
122:49 - graph matches the diagram
122:52 - that we laid out up top I'm just going
122:54 - to copy these over and I'll actually go
122:56 - then back to the diagram and and kind of
122:57 - underscore that a little bit more so
123:01 - here is the flow we've laid out again
123:03 - here is our diagram and you can kind of
123:05 - look at them side by side and see how
123:07 - they basically match up so here's kind
123:10 - of our flow diagram going from basically
123:12 - query analysis that's this thing this
123:15 - route question and you can see web
123:18 - search Vector store LM fallback LM
123:20 - fallback web search vector store so
123:22 - those are like the three options that
123:23 - can come out of this conditional Edge
123:26 - and then here's where we connect so if
123:27 - we go to web search then basically we
123:30 - next go to
123:31 - generate so that's kind of this whole
123:34 - flow um now if we go to
123:38 - retrieve um then we're going to grade so
123:41 - that's
123:42 - it um and you know it follows kind of as
123:46 - you can see here that's really it uh so
123:49 - it's just nice to draw the these
123:51 - diagrams out first and then it's pretty
123:54 - quick to implement each node and each
123:56 - Edge just as a function and then stitch
123:59 - them together in a graph just like I
124:00 - show here and of course we'll make sure
124:01 - this code's publ so you can use it as a
124:03 - reference um so there we go now let's
124:07 - try a few a few different test questions
124:10 - so like what player the Bears to draft
124:12 - and NFL draft right let's have a look at
124:14 - that and they should print everything
124:16 - it's doing as we go so okay this is
124:19 - important route question it just decides
124:21 - to route to web search that's good it
124:23 - doesn't go to our Vector store this is a
124:24 - current event not related to our Vector
124:26 - store at all it goes to web search um
124:29 - and then it goes to generate so that's
124:30 - what we'd expect so basically web search
124:32 - goes through to generate
124:35 - um and we check hallucinations
124:38 - Generations ground the documents we
124:40 - check generation versus question the
124:42 - generation addresses the question the
124:43 - Chicago Bears expected to draft Caleb
124:45 - Williams that's right that's that's the
124:48 - consensus so cool that works now let's
124:50 - ask a question related to our Vector
124:52 - store what are the types of agent memory
124:54 - we'll kick this off so we're routing
124:56 - okay we're routing to rag now look how
124:58 - fast this is that's really fast so we
125:01 - basically whip through that document
125:03 - grading determine they're all relevant
125:06 - uh we go to decision to
125:07 - generate um we check hallucinations we
125:10 - check answer versus question and there
125:13 - are several types of memory stored in
125:15 - the human brain memory can also be
125:16 - stored in G of Agents you have LM agents
125:19 - memory stream retrieval model and and
125:20 - reflection mechanism so it's
125:22 - representing what's captured on the blog
125:23 - post pretty reasonably now let me show
125:25 - you something else is kind of nice I can
125:27 - go to Langs Smith and I can go to my
125:29 - projects we create this new project
125:31 - coher adaptive rag at the start and
125:33 - everything is actually logged there
125:35 - everything we just did so I can open
125:37 - this up and I can actually just kind of
125:40 - look through all the stages of my Lang
125:42 - graph to here's my retrieval stage um
125:45 - here's my grade document stage and we
125:47 - can kind of audit the grading itself we
125:49 - kind of looked at this one by one
125:51 - previously but it's actually pretty nice
125:53 - we can actually audit every single
125:54 - individual document grade to see what's
125:56 - happening um we can basically go through
126:00 - um to this is going to be one of the
126:02 - other graders here
126:05 - um yep so this is actually going to be
126:09 - the hallucination grading right here uh
126:12 - and then this is going to be the answer
126:13 - grading right here so that's really it
126:15 - you can kind of walk through the entire
126:16 - graph you can you can kind of study
126:18 - what's going on um which is actually
126:20 - very useful so it looks like this worked
126:23 - pretty well um and finally let's just
126:25 - ask a question that should go to that
126:27 - fallback uh path down at the bottom like
126:30 - not related at all to our Vector store
126:32 - current events and yeah hello I'm doing
126:34 - well so it's pretty neat we've seen in
126:36 - maybe 15 minutes we've from scratch
126:38 - built basically a semi- sophisticated
126:40 - rag system that has agentic properties
126:42 - we've done in Lang graph we've done with
126:44 - coher uh command R you can see it's
126:46 - pretty darn fast in fact we can go to
126:48 - Langs Smith and look at so this whole
126:50 - thing took 7 seconds uh that is not bad
126:54 - let's look at the most recent one so
126:56 - this takes one second so the fallback
126:58 - mechanism to the LM is like 1 second um
127:01 - the let's just look here so 6 seconds
127:04 - for the initial uh land graph so this is
127:08 - not bad at all it's quite fast it done
127:10 - it does quite a few different checks we
127:12 - do routing uh and then we have kind of a
127:15 - bunch of nice fallback behavior and
127:17 - inline checking uh for both relevance
127:20 - hallucinations and and answer uh kind of
127:23 - groundedness or answer usefulness so you
127:27 - know this is pretty nice I definitely
127:28 - encourage you to play with a notebook
127:30 - command R is a really nice option for
127:31 - this due to the fact that is tool use
127:33 - routing uh small and quite fast and it's
127:36 - really good for Rags it's a very nice
127:38 - kind of uh a very nice option for
127:41 - workflows like this and I think you're
127:43 - going to see more and more of this kind
127:44 - of like uh adaptive or self-reflective
127:47 - rag um just because this is something
127:50 - that a lot systems can benefit from like
127:53 - a a lot of production rack systems kind
127:55 - of don't necessarily have
127:57 - fallbacks uh depending on for example
128:00 - like um you know if the documents
128:03 - retrieved are not relevant uh if the
128:05 - answer contains hallucinations and so
128:07 - forth so this opportunity to apply
128:09 - inline checking along with rag is like a
128:11 - really nice theme I think we're going to
128:13 - see more and more of especially as model
128:15 - inference gets faster and faster and
128:17 - these checks get cheaper and cheaper to
128:18 - do kind of in the inference Loop
128:22 - now as a final thing I do want to bring
128:23 - up the a point about you know we've
128:25 - shown this Lang graph stuff what about
128:27 - agents you know how do you think about
128:28 - agents versus Lang graph right and and I
128:30 - think the way I like to frame this is
128:33 - that um Lang graph is really good
128:37 - for um flows that you have kind of very
128:41 - clearly defined that don't have like
128:43 - kind of open-endedness but like in this
128:45 - case we know the steps we want to take
128:47 - every time we want to do um basically
128:49 - query analysis routing and then we want
128:51 - to do a three grading steps and that's
128:53 - it um Lang graph is really good for
128:56 - building very reliable flows uh it's
128:58 - kind of like putting an agent on guard
129:00 - rails and it's really nice uh it's less
129:04 - flexible but highly reliable and so you
129:07 - can actually use smaller faster models
129:08 - with langra so that's the thing I like
129:10 - about we saw here command R 35 billion
129:12 - parameter model works really well with
129:14 - langra quite quick we' were able to
129:16 - implement a pretty sophisticated rag
129:18 - flow really quickly 15 minutes um in
129:21 - time is on the order of like less than
129:23 - you know around 5 to 6 seconds so so
129:25 - pretty good right now what about agents
129:28 - right so I think Agents come into play
129:30 - when you want more flexible workflows
129:32 - you don't want to necessarily follow a
129:34 - defined pattern a priori you want an
129:36 - agent to be able to kind of reason and
129:38 - make of open-end decisions which is
129:40 - interesting for certain like long
129:41 - Horizon planning problems you know
129:43 - agents are really
129:44 - interesting the catch is that
129:46 - reliability is a bit worse with agents
129:48 - and so you know that's a big question a
129:50 - lot of people bring up and that's kind
129:52 - of where larger LMS kind of come into
129:53 - play with a you know there's been a lot
129:55 - of questions about using small LMS even
129:57 - open source models with agents and
129:59 - reliabilities kind of continuously being
130:01 - an issue whereas I've been able to run
130:03 - these types of land graphs with um with
130:06 - uh like mraw or you know command R
130:09 - actually is open weights you can run it
130:10 - locally um I've been able to run them
130:12 - very reproducibly with open source
130:14 - models on my laptop um so you know I
130:17 - think there's a tradeoff and Comm
130:20 - actually there's a new coher model
130:21 - coming out uh believe command R plus
130:25 - which uh is a larger model so it's
130:27 - probably more suitable for kind of more
130:29 - open-ended agentic use cases and there's
130:32 - actually a new integration with Lang
130:33 - chain that support uh coher agents um
130:36 - which is quite nice so I think it's it's
130:38 - worth experimenting for certain problems
130:40 - in workflows you may need more
130:42 - open-ended reasoning in which case use
130:43 - an agent with a larger model otherwise
130:46 - you can use like Lang graph for more uh
130:49 - a more reliable potential
130:51 - but con strain flow and it can also use
130:53 - smaller models faster LMS so those are
130:56 - some of the trade-offs to keep in mind
130:57 - but anyway encourage you play with a
130:59 - notebook explore for yourself I think
131:01 - command R is a really nice model um I've
131:03 - also been experimenting with running it
131:04 - locally with AMA uh currently the
131:07 - quantise model is like uh two bit
131:09 - quantise is like 13 billion uh or so uh
131:14 - yeah 13 gigs it's it's a little bit too
131:17 - large to run quickly locally for me
131:22 - um inference for things like rag we're
131:25 - on the order of 30 seconds so again it's
131:27 - not great for a live demo but it does
131:29 - work it is available on a llama so I
131:30 - encourage you to play with that I have a
131:32 - Mac M2 32 gig um so you know if I if
131:35 - you're a larger machine then it
131:36 - absolutely could be worth working with
131:38 - locally so encourage you to play with
131:40 - that anyway hopefully this was useful
131:41 - and interesting I think this is a cool
131:43 - paper cool flow um coher command R is a
131:46 - nice option for these types of like
131:48 - routing uh it's quick good with Lang
131:52 - graph good for rag good for Tool use so
131:55 - you know have a have a look and uh you
131:57 - know reply anything uh any feedback in
132:00 - the comments
132:06 - thanks hi this is Lance from Lang chain
132:09 - this is a talk I gave at two recent
132:10 - meetups in San Francisco called is rag
132:13 - really dead um and I figured since you
132:16 - know a lot of people actually weren't
132:17 - able to make those meetups uh I just
132:19 - record this and put this on YouTube and
132:21 - see if this is of interest to folks um
132:24 - so we all kind of recognize that Contex
132:26 - windows are getting larger for llms so
132:29 - on the x-axis you can see the tokens
132:31 - used in pre-training that's of course
132:33 - you know getting larger as well um
132:35 - proprietary models are somewhere over
132:37 - the two trillion token regime we don't
132:39 - quite know where they sit uh and we've
132:41 - all the way down to smaller models like
132:43 - 52 trained on far fewer tokens um but
132:46 - what's really notable is on the y axis
132:49 - you can see about a year ago da the art
132:52 - models were on the order of 4,000 to
132:54 - 8,000 tokens and that's you know dozens
132:56 - of pages um we saw Claude 2 come out
132:59 - with the 200,000 token model earlier I
133:02 - think it was last year um gbd4 128,000
133:06 - tokens now that's hundreds of pages and
133:09 - now we're seeing Claud 3 and Gemini come
133:11 - out with million token models so this is
133:13 - hundreds to thousands of pages so
133:16 - because of this phenomenon people have
133:17 - been kind of wondering is rag dead if
133:19 - you can stuff you know many thousands of
133:22 - pages into the context window llm why do
133:24 - you need a reteval system um it's a good
133:27 - question spoke sparked a lot of
133:29 - interesting debate on Twitter um and
133:32 - it's maybe first just kind of grounding
133:33 - on what is rag so rag is really the
133:35 - process of reasoning and retrieval over
133:37 - chunks of of information that have been
133:40 - retrieved um it's starting with you know
133:42 - documents that are indexed um they're
133:45 - retrievable through some mechanism
133:47 - typically some kind of semantic
133:48 - similarity search or keyword search
133:50 - other mechanisms
133:51 - retriev docs should then pass to an llm
133:54 - and the llm reasons about them to ground
133:56 - response to the question in the retrieve
133:58 - document so that's kind of the overall
134:00 - flow but the important point to make is
134:02 - that typically it's multiple documents
134:04 - and involve some form of
134:06 - reasoning so one of the questions I
134:08 - asked recently is you know if long
134:10 - condex llms can replace rag it should be
134:12 - able to perform you know multia
134:15 - retrieval and reasoning from its own
134:16 - context really effectively so I teamed
134:19 - up with Greg Cameron uh to kind of
134:21 - pressure test this and he had done some
134:23 - really nice needle the Haack analyses
134:25 - already focused on kind of single facts
134:28 - called needles placed in a Hy stack of
134:31 - Paul Graham essays um so I kind of
134:33 - extended that to kind of mirror the rag
134:36 - use case or kind of the rag context uh
134:39 - where I took multiple facts so I call it
134:41 - multi needle um I buil on a funny needle
134:45 - in the HTO challenge published by
134:46 - anthropic where they add they basically
134:48 - placed Pizza ingredients in the context
134:51 - uh and asked the LM to retrieve this
134:53 - combination of pizza ingredients I did I
134:56 - kind of Rift on that and I basically
134:58 - split the pizza ingredients up into
134:59 - three different needles and place those
135:01 - three ingredients in different places in
135:03 - the context and then ask the um to
135:06 - recover those three ingredients um from
135:09 - the context so again the setup is the
135:12 - question is what the secret ingredients
135:13 - need to build a perfect Pizza the
135:15 - needles are the ingredients figs Pudo
135:17 - goat cheese um I place them in the
135:21 - context at some specified intervals the
135:23 - way this test works is you can basically
135:25 - set the percent of context you want to
135:28 - place the first needle and the remaining
135:30 - two are placed at roughly equal
135:31 - intervals in the remaining context after
135:33 - the first so that's kind of the way the
135:35 - test is set up now it's all open source
135:36 - by the way the link is below so needs
135:39 - are placed um you ask a question you
135:42 - promp L them with with kind of um with
135:45 - this context and the question and then
135:47 - produces the answer and now the the
135:48 - framework will grade the response
135:51 - both one are you know all are all the
135:55 - the specified ingredients present in the
135:56 - answer and two if not which ones are
136:00 - missing so I ran a bunch of analysis on
136:03 - this with GPD 4 and came kind of came up
136:05 - with some with some fun results um so
136:07 - you can see on the left here what this
136:09 - is looking at is different numbers of
136:11 - needles placed in 120,000 token context
136:14 - window for
136:15 - gbd4 and I'm asking um gbd4 to retrieve
136:20 - either one three or 10 needles now I'm
136:24 - also asking it to do reasoning on those
136:26 - needles that's what you can see in those
136:28 - red bars so green is just retrieve the
136:30 - ingredients red is reasoning and the
136:32 - reasoning challenge here is just return
136:34 - the first letter of each ingredient so
136:37 - we find is basically two things the
136:39 - performance or the percentage of needles
136:42 - retrieved drops with respect to the
136:44 - number of needles that's kind of
136:45 - intuitive you place more facts
136:48 - performance gets worse but also it gets
136:50 - worse if you ask it to reason so if you
136:53 - say um just return the needles it does a
136:56 - little bit better than if you say return
136:58 - the needles and tell me the first letter
137:00 - so you overlay reasoning so this is the
137:02 - first observation more facts is harder
137:06 - uh and reasoning is harder uh than just
137:09 - retrieval now the second question we ask
137:11 - is where are these needles actually
137:12 - present in the context that we're
137:14 - missing right so we know for example um
137:18 - retrieval of um 10 needles is around 60%
137:23 - so where are the missing needles in the
137:26 - context so on the right you can see
137:28 - results telling us actually which
137:30 - specific needles uh are are the model
137:33 - fails to retrieve so what we can see is
137:36 - as you go from a th000 tokens up to
137:39 - 120,000 tokens on the X here and you
137:42 - look at needle one place at the start of
137:44 - the document to needle 10 placed at the
137:46 - end at a th000 token context link you
137:49 - can retrieve them all so again kind of
137:52 - match what we see over here small well
137:54 - actually sorry over here everything I'm
137:56 - looking at is 120,000 tokens so that's
137:59 - really not the point uh the point is
138:01 - actually smaller context uh better
138:04 - retrieval so that's kind of point one um
138:08 - as I increase the context window I
138:10 - actually see that uh there is increased
138:14 - failure to retrieve needles which you
138:15 - see can see in red here towards the
138:18 - start of the
138:19 - document um and so this is an
138:21 - interesting result um and it actually
138:23 - matches what Greg saw with single needle
138:25 - case as well so the way to think about
138:27 - it is it appears that um you know if you
138:31 - for example read a book and I asked you
138:33 - a question about the first chapter you
138:34 - might have forgotten it same kind of
138:36 - phenomenon appears to happen here with
138:38 - retrieval where needles towards the
138:40 - start of the context are are kind of
138:42 - Forgotten or are not well retrieved
138:45 - relative to those of the end so this is
138:47 - an effect we see with gbd4 it's been
138:49 - reproduced quite a bit so ran nine
138:51 - different trials here Greg's also seen
138:53 - this repeatedly with single needle so it
138:55 - seems like a pretty consistent
138:57 - result and there's an interesting point
138:59 - I put this on Twitter and a number of
139:00 - folks um you know replied and someone
139:03 - sent me this paper which is pretty
139:04 - interesting and it mentions recency bias
139:07 - is one possible reason so the most
139:09 - informative tokens for predicting the
139:10 - next token uh you know are are are
139:14 - present close to or recent to kind of
139:17 - where you're doing your generation and
139:18 - so there's a bias to attend to recent
139:20 - tokens which is obviously not great for
139:23 - the retrieval problem as we saw here so
139:26 - again the results show us that um
139:30 - reasoning is a bit harder than retrieval
139:32 - more needles is more difficult and
139:34 - needles towards the start of your
139:36 - context are harder to retrieve than
139:38 - towards the end those are three main
139:40 - observations from this and it maybe
139:42 - indeed due to this recency bias so
139:45 - overall what this kind of tells you is
139:47 - be wary of just context stuffing in
139:49 - large long context
139:51 - there are no retrieval
139:52 - guarantees and also there's some recent
139:55 - results that came out actually just
139:56 - today suggesting that single needle may
139:58 - be misleadingly easy um you know there's
140:02 - no reasoning it's retrieving a single
140:04 - needle um and also these guys I'm I
140:08 - showed this tweet here show that um the
140:12 - in a lot of these needle and Haack
140:13 - challenges including mine the facts that
140:16 - we look for are very different than um
140:20 - the background kind of Hy stack of Paul
140:21 - Graham essays and so that may be kind of
140:23 - an interesting artifact they note that
140:25 - indeed if the needle is more subtle
140:28 - retrievals is worse so I think basically
140:31 - when you see these really strong
140:33 - performing needle and hyack analyses put
140:35 - up by model providers you should be
140:37 - skeptical um you shouldn't necessarily
140:39 - assume that you're going to get high
140:40 - quality retrieval from these long
140:41 - contact LMS uh for numerous reasons you
140:45 - need to think about retrieval of
140:46 - multiple facts um you need to think
140:48 - about reasoning on top of retrieval you
140:50 - need need to think about the subtlety of
140:52 - the retrieval relative to the background
140:54 - context because for many of these needle
140:56 - and the Haack challenges it's a single
140:57 - needle no reasoning and the needle
141:00 - itself is very different from the
141:01 - background so anyway those may all make
141:03 - the challenge a bit easier than a real
141:05 - world scenario of fact retrieval so I
141:07 - just want to like kind of lay out that
141:09 - those cautionary notes but you know I
141:13 - think it is fair to say this will
141:14 - certainly get better and I think it's
141:17 - also fair to say that rag will change
141:19 - and this is just like a nearly not a
141:21 - great joke but Frank zap a musician made
141:24 - the point Jazz isn't dead it just smells
141:26 - funny you know I think same for rag rag
141:28 - is not dead but it will change I think
141:30 - that's like kind of the key Point here
141:32 - um so just as a followup on that rag
141:35 - today's focus on precise retrieval of
141:37 - relevant doc chunks so it's very focused
141:39 - on typically taking documents chunking
141:42 - them in some particular way often using
141:44 - very OS syncratic chunking methods
141:46 - things like chunk size are kind of
141:48 - picked almost arbitrarily embeding them
141:50 - storing them in an index taking a
141:52 - question embedding it doing K&N uh
141:55 - similarity search to retrieve relevant
141:57 - chunks you're often setting a k
141:59 - parameter which is the number of chunks
142:00 - you retrieve you often will do some kind
142:02 - of filtering or Pro processing on the
142:04 - retrieve chunks and then ground your
142:06 - answer in those retrieved chunks so it's
142:08 - very focused on precise retrieval of
142:10 - just the right chunks now in a world
142:14 - where you have very long context models
142:16 - I think there's a fair question to ask
142:18 - is is this really kind of the most most
142:20 - reasonable approach so kind of on the
142:22 - left here you can kind of see this
142:25 - notion closer to today of I need the
142:27 - exact relevant chunk you can risk over
142:29 - engineering you can have you know higher
142:31 - complexity sensitivity to these odd
142:33 - parameters like chunk size k um and you
142:36 - can indeed suffer lower recall because
142:38 - you're really only picking very precise
142:40 - chunks you're beholden to very
142:41 - particular embedding models so you know
142:44 - I think going forward as long context
142:46 - models get better and better there are
142:48 - definitely question you should certainly
142:50 - question the current kind of very
142:52 - precise chunking rag Paradigm but on the
142:54 - flip side I think just throwing all your
142:56 - docs into context probably will also not
142:59 - be the preferred approach you'll suffer
143:01 - higher latency higher token usage I
143:03 - should note that today 100,000 token GPD
143:06 - 4 is like $1 per generation I spent a
143:09 - lot of money on Lang Chain's account uh
143:11 - on that multile analysis I don't want to
143:13 - tell Harrison how much I spent uh so
143:16 - it's it's you know it's not good right
143:18 - um You Can't audit retrieve
143:21 - um and security and and authentication
143:23 - are issues if for example you need
143:25 - different users different different
143:26 - access to certain kind of retriev
143:28 - documents or chunks in the Contex
143:30 - stuffing case you you kind of can't
143:32 - manage security as easily so there's
143:33 - probably some predo optimal regime kind
143:35 - of here in the middle and um you know I
143:39 - I put this out on Twitter I think
143:40 - there's some reasonable points raised I
143:42 - think you know this inclusion at the
143:44 - document level is probably pretty sane
143:46 - documents are self-contained chunks of
143:48 - context um so you know what about
143:51 - document Centric rag so no chunking uh
143:54 - but just like operate on the context of
143:56 - full documents so you know if you think
143:59 - forward to the rag Paradigm that's
144:01 - document Centric you still have the
144:03 - problem of taking an input question
144:05 - routing it to the right document um this
144:07 - doesn't change so I think a lot of
144:09 - methods that we think about for kind of
144:11 - query analysis um taking an input
144:14 - question rewriting it in a certain way
144:16 - to optimize retrieval things like
144:18 - routing taking a question routing to the
144:20 - right database be it a relational
144:22 - database graph database Vector store um
144:25 - and quer construction methods so for
144:27 - example text to SQL text to Cipher for
144:30 - graphs um or text to even like metadata
144:32 - filters for for Vector stores those are
144:35 - all still relevant in the world that you
144:37 - have long Contex llms um you're probably
144:40 - not going to dump your entire SQL DB and
144:42 - feed that to the llm you're still going
144:43 - to have SQL queries you're still going
144:45 - to have graph queries um you may be more
144:48 - permissive with what you extract but it
144:50 - still is very reasonable to store the
144:52 - majority of your structured data in
144:53 - these in these forms likewise with
144:55 - unstructured data like documents like we
144:58 - said before it still probably makes
145:00 - sense to ENC to you know store documents
145:02 - independently but just simply aim to
145:04 - retrieve full documents rather than
145:05 - worrying about these idiosyncratic
145:07 - parameters like like chunk size um and
145:10 - along those lines there's a lot of
145:12 - methods out there we've we've done a few
145:14 - of these that are kind of well optimized
145:16 - for document retrieval so one I want a
145:18 - flag is what we call multi repesent
145:20 - presentation indexing and there's
145:21 - actually a really nice paper on this
145:23 - called dense X retriever or proposition
145:25 - indexing but the main point is simply
145:27 - this would you do is you take your OD
145:29 - document you produce a representation
145:31 - like a summary of that document you
145:33 - index that summary right and then um at
145:37 - retrieval time you ask your question you
145:39 - embed your question and you simply use a
145:41 - highle summary to just retrieve the
145:43 - right document you pass the full
145:45 - document to the LM for a kind of final
145:48 - generation so it's kind of a trick where
145:51 - you don't have to worry about embedding
145:52 - full documents in this particular case
145:54 - you can use kind of very nice
145:56 - descriptive summarization prompts to
145:58 - build descriptive summaries and the
146:00 - problem you're solving here is just get
146:02 - me the right document it's an easier
146:04 - problem than get me the right chunk so
146:06 - this is kind of a nice approach it
146:08 - there's also different variants of it
146:10 - which I share below one is called parent
146:11 - document retriever where you could use
146:13 - in principle if you wanted smaller
146:15 - chunks but then just return full
146:17 - documents but anyway the point is
146:19 - preserving full documents for Generation
146:21 - but using representations like summaries
146:23 - or chunks for retrieval so that's kind
146:25 - of like approach one that I think is
146:27 - really interesting approach two is this
146:30 - idea of raptor is a cool paper came out
146:32 - of Stamper somewhat recently and this
146:34 - solves the problem of what if for
146:36 - certain questions I need to integrate
146:38 - information across many documents so
146:41 - what this approach does is it takes
146:42 - documents and it it embeds them and
146:45 - clusters them and then it summarizes
146:47 - each cluster um and it does this
146:49 - recursively in up with only one very
146:51 - high level summary for the entire Corpus
146:53 - of documents and what they do is they
146:55 - take this kind of this abstraction
146:57 - hierarchy so to speak of different
146:59 - document summarizations and they just
147:01 - index all of it and they use this in
147:03 - retrieval and so basically if you have a
147:05 - question that draws an information
147:07 - across numerous documents you probably
147:09 - have a summary present and and indexed
147:12 - that kind of has that answer captured so
147:15 - it's a nice trick to consolidate
147:16 - information across documents um they
147:19 - they paper actually reports you know
147:22 - these documents in their case or the
147:23 - leavs are actually document chunks or
147:25 - slices but I actually showed I have a
147:28 - video on it and a notebook that this
147:29 - works across full documents as well um
147:33 - and this and I segue into to do this you
147:36 - do need to think about long context
147:37 - embedding models because you're
147:38 - embedding full documents and that's a
147:40 - really interesting thing to track um the
147:43 - you know hazy research uh put out a
147:45 - really nice um uh blog post on this
147:48 - using uh what the Monch mixer so it's
147:50 - kind of a new architecture that tends to
147:53 - longer context they have a 32,000 token
147:56 - embedding model that's pres that's
147:58 - available on together AI absolutely
147:59 - worth experimenting with I think this is
148:01 - really interesting Trend so long long
148:03 - Contex and beddings kind of play really
148:05 - well with this kind of idea you take
148:07 - full documents embed them using for
148:09 - example long Contex embedding models and
148:11 - you can kind of build these document
148:12 - summarization trees um really
148:14 - effectively so I think this another nice
148:16 - trick for working with full documents in
148:19 - the long context kind of llm regime um
148:23 - one other thing I'll note I think
148:25 - there's also going to Mo be move away
148:27 - from kind of single shot rag well
148:29 - today's rag we typically you know we
148:31 - chunk documents uh uh embed them store
148:34 - them in an index you know do retrieval
148:36 - and then do generation but there's no
148:38 - reason why you shouldn't kind of do
148:40 - reasoning on top of the generation or
148:42 - reasoning on top of the retrieval and
148:44 - feedback if there are errors so there's
148:46 - a really nice paper called selfrag um
148:49 - that kind of reports this we implemented
148:50 - this using Lang graph works really well
148:53 - and the simp the idea is simply to you
148:55 - know grade the relevance of your
148:57 - documents relative to your question
148:59 - first if they're not relevant you
149:01 - rewrite the question you can do you can
149:02 - do many things in this case we do
149:04 - question rewriting and try again um we
149:06 - also grade for hallucinations we grade
149:09 - for answer relevance but anyway it kind
149:11 - of moves rag from like a single shot
149:12 - Paradigm to a kind of a cyclic flow uh
149:16 - in which you actually do various
149:17 - gradings Downstream and this is all
149:19 - relev in the long context llm regime as
149:22 - well in fact you know it you you
149:24 - absolutely should take advantage of of
149:27 - for example increasingly fast and
149:30 - Performing LMS to do this grading um
149:33 - Frameworks like langra allow you to
149:35 - build these kind of these flows which
149:37 - build which allows you to kind of have a
149:38 - more performant uh kind of kind of
149:42 - self-reflective rag pipeline now I did
149:44 - get a lot of questions about latency
149:46 - here and I completely agree there's a
149:47 - trade-off between kind of performance
149:49 - accuracy and latency that's present here
149:51 - I think the real answer is you can opt
149:54 - to use very fast uh for example models
149:57 - like grock where seeing um you know gp35
150:00 - turbos very fast these are fairly easy
150:03 - grading challenges so you can use very
150:05 - very fast LMS to do the grading and for
150:07 - example um you you can also restrict
150:11 - this to only do one turn of of kind of
150:13 - cyclic iteration so you can kind of
150:14 - restrict the latency in that way as well
150:16 - so anyway I think it's a really cool
150:18 - approach still relevant in the world as
150:20 - we move towards longer context so it's
150:22 - kind of like building reasoning on top
150:23 - of rag um in the uh generation and
150:28 - retrieval stages and a related point one
150:30 - of the challenges with rag is that your
150:33 - index for example you you may have a
150:36 - question that is that asks something
150:38 - that's outside the scope of your index
150:39 - and this is kind of always a problem so
150:41 - a really cool paper called c c rag or
150:44 - corrective rag came out you know a
150:45 - couple months ago that basically does
150:47 - grading just like we talked about before
150:50 - and then if the documents are not
150:51 - relevant you kick off and do a web
150:53 - search and basically return the search
150:55 - results to the LM for final generation
150:57 - so it's a nice fallback in cases where
151:00 - um your you the questions out of the
151:02 - domain of your retriever so you know
151:04 - again nice trick overlaying reasoning on
151:07 - top of rag I think this trend you know
151:09 - continues um because you know it it just
151:12 - it makes rag systems you know more
151:14 - performant uh and less brittle to
151:18 - questions that are out of domain so you
151:19 - know you know that's another kind of
151:21 - nice idea this particular approach also
151:23 - we showed works really well with with uh
151:25 - with open source models so I ran this
151:27 - with mraw 7B it can run locally on my
151:29 - laptop using a llama so again really
151:32 - nice approach I encourage you to look
151:33 - into this um and this is all kind of
151:35 - independent of the llm kind of context
151:37 - length these are reasoning you can add
151:40 - on top of the retrieval stage that that
151:42 - can kind of improve overall performance
151:45 - and so the overall picture kind of looks
151:47 - like this where you know I think that
151:49 - the the the the problem of routing your
151:52 - question to the right database Andor to
151:54 - the right document kind of remains in
151:56 - place query analysis is still quite
151:58 - relevant routing is still relevant query
152:00 - construction is still relevant um in the
152:02 - long Contex regime I think there is less
152:04 - of an emphasis on document chunking
152:07 - working with full documents is probably
152:08 - kind of more parto optimal so to speak
152:11 - um there's some some clever tricks for
152:13 - IND indexing of documents like the
152:15 - multi-representation indexing we talked
152:16 - about the hierarchical indexing using
152:19 - Raptor that we talked about as well are
152:20 - two interesting ideas for document
152:22 - Centric indexing um and then kind of
152:25 - reasoning in generation post retrieval
152:28 - on retrieval itself tog grade on the
152:30 - generations themselves checking for
152:32 - hallucinations those are all kind of
152:34 - interesting and relevant parts of a rag
152:36 - system that I think we'll probably will
152:38 - see more and more of as we move more
152:40 - away from like a more naive prompt
152:42 - response Paradigm more to like a flow
152:44 - Paradigm we're seeing that actually
152:45 - already in codenation it's probably
152:47 - going to carry over to rag as well where
152:49 - we kind of build rag systems that have
152:51 - kind of a cyclic flow to them operate on
152:53 - documents use longc Comics llms um and
152:55 - still use kind of routing and query
152:57 - analysis so reasoning pre- retrieval
152:59 - reasoning post- retrieval so anyway that
153:01 - was kind of my talk um and yeah feel
153:04 - free to leave any comments on the video
153:05 - and I'll try to answer any questions but
153:07 - um yeah that's that's probably about it
153:09 - thank you

Cleaned transcript:

in this course Lance Martin will teach you how to implement rag from scratch Lance is a software engineer at Lang chain and Lang chain is one of the most common ways to implement rag Lance will help you understand how to use rag to combine custom data with llms hi this is Lance Martin I'm a software engineer at Lang chain I'm going to be giving a short course focused on rag or retrieval augmented generation which is one of the most popular kind of ideas and in llms today so really the motivation for this is that most of the world's data is private um whereas llms are trained on publicly available data so you can kind of see on the bottom on the xaxis the number of tokens using pretraining various llms so it kind of varies from say 1.5 trillion tokens in the case of smaller models like 52 out to some very large number that we actually don't know for proprietary models like GPT 4 CLA three but what's really interesting is that the context window or the ability to feed external information into these LMS is actually getting larger so about a year ago context windows were between 4 and 8,000 tokens you know that's like maybe a dozen pages of text we've recently seen models all the way out to a million tokens which is thousands of pages of text so while these llms are trained on large scale public data it's increasingly feasible to feed them this huge mass of private data that they've never seen that private data can be your kind of personal data it can be corporate data or you know other information that you want to pass to an LM that's not natively in his training set and so this is kind of the main motivation for rag it's really the idea that llms one are kind of the the center of a new kind of operating system and two it's increasingly critical to be able to feed information from external sources such as private data into llms for processing so that's kind of the overarching motivation for Rag and now rag refers to retrieval augmented generation and you can think of it in three very general steps there's a process of indexing of external data so you can think about this as you know building a database for example um many companies already have large scale databases in different forms they could be SQL DBS relational DBS um they could be Vector Stores um or otherwise but the point is that documents are indexed such that they can be retrieved based upon some heuristics relative to an input like a question and those relevant documents can be passed to an llm and the llm can produce answers that are grounded in that retrieved information so that's kind of the centerpiece or central idea behind Rag and why it's really powerful technology because it's really uniting the the knowledge and processing capacity of llms with large scale private external data source for which most of the important data in the world still lives and in the following short videos we're going to kind of build up a complete understanding of the rag landscape and we're going to be covering a bunch of interesting papers and techniques that explain kind of how to do rag and I've really broken it down into a few different sections so starting with a question on the left the first kind of section is what I call query trans translation so this captures a bunch of different methods to take a question from a user and modify it in some way to make it better suited for retrieval from you know one of these indexes we've talked about that can use methods like query writing it can be decomposing the query into you know constituent sub questions then there's a question of routing so taking that decomposed a Rewritten question and routing it to the right place you might have multiple Vector stores a relational DB graph DB and a vector store so it's the challenge of getting a question to the right Source then there's a there's kind of the challenge of query construction which is basically taking natural language and converting it into the DSL necessary for whatever data source you want to work with a classic example here is text a SQL which is kind of a very kind of well studied process but text a cipher for graph DV is very interesting text to metadata filters for Vector DBS is also a very big area of study um then there's indexing so that's the process of taking your documents and processing them in some way so they can be easily retrieved and there's a bunch of techniques for that we'll talk through we'll talk through different embedding methods we'll talk about different indexing strategies after retrieval there are different techniques to rerank or filter retrieve documents um and then finally we'll talk about generation and kind of an interesting new set of methods to do what we might call as active rag so in that retrieval or generation stage grade documents grade answers um grade for relevance to the question grade for faithfulness to the documents I.E check for hallucinations and if either fail feedback uh re retrieve or rewrite the question uh regenerate the qu regenerate the answer and so forth so there's a really interesting set of methods we're going to talk through that cover that like retrieval and generation with feedback and you know in terms of General outline we'll cover the basics first it'll go through indexing retrieval and generation kind of in the Bare Bones and then we'll talk through more advanced techniques that we just saw on the prior slide career Transformations routing uh construction and so forth hi this is Lance from Lang chain this the second video in our series rack from scratch focused on indexing so in the past video you saw the main kind of overall components of rag pipelines indexing retrieval and generation and here we're going to kind of Deep dive on indexing and give like just a quick overview of it so the first aspect of indexing is we have some external documents that we actually want to load and put into what we're trying to call Retriever and the goal of this retriever is simply given an input question I want to fish out doents that are related to my question in some way now the way to establish that relationship or relevance or similarity is typically done using some kind of numerical representation of documents and the reason is that it's very easy to compare vectors for example of numbers uh relative to you know just free form text and so a lot of approaches have been a developed over the years to take text documents and compress them down into a numerical rep presentation that then can be very easily searched now there's a few ways to do that so Google and others came up with many interesting statistical methods where you take a document you look at the frequency of words and you build what they call sparse vectors such that the vector locations are you know a large vocabulary of possible words each value represents the number of occurrences of that particular word and it's sparse because there's of course many zeros it's a very large vocabulary relative to what's present in the document and there's very good search methods over this this type of numerical representation now a bit more recently uh embedding methods that are machine learned so you take a document and you build a compressed fixed length representation of that document um have been developed with correspondingly very strong search methods over embeddings um so the intuition here is that we take documents and we typically split them because embedding models actually have limited context windows so you know on the order of maybe 512 tokens up to 8,000 tokens or Beyond but they're not infinitely large so documents are split and each document is compressed into a vector and that Vector captures a semantic meaning of the document itself the vectors are indexed questions can be embedded in the exactly same way and then numerical kind of comparison in some form you know using very different types of methods can be performed on these vectors to fish out relevant documents relative to my question um and let's just do a quick code walk through on some of these points so I have my notebook here I've installed here um now I've set a few API keys for lsmith which are very useful for tracing which we'll see shortly um previously I walked through this this kind of quick start that just showed overall how to lay out these rag pipelines and here what I'll do is I'll Deep dive a little bit more on indexing and I'm going to take a question and a document and first I'm just going to compute the number of tokens in for example the question and this is interesting because embedding models in llms more generally operate on tokens and so it's kind of nice to understand how large the documents are that I'm trying to feed in in this case it's obviously a very small in this case question now I'm going to specify open eye embeddings I specify an embedding model here and I just say embed embed query I can pass my question my document and what you can see here is that runs and this is mapped to now a vector of length 1536 and that fixed length Vector representation will be computed for both documents and really for any document so you're always is kind of computing this fix length Vector that encodes the semantics of the text that you've passed now I can do things like cosine similarity to compare them and as we'll see here I can load some documents this is just like we saw previously I can split them and I can index them here just like we did before but we can see under the hood really what we're doing is we're taking each split we're embedding it using open eye embeddings into this this kind of this Vector representation and that's stored with a link to the rod document itself in our Vector store and next we'll see how to actually do retrieval using this Vector store hi this is Lance from Lang chain and this is the third video in our series rag from scratch building up a lot of the motivations for rag uh from the very basic components um so we're going to be talking about retrieval today in the last two uh short videos I outlined indexing and gave kind of an overview of this flow which starts with indexing of our documents retrieval of documents relevant to our question and then generation of answers based on the retriev documents and so we saw that the indexing process basically makes documents easy to retrieve and it goes through a flow that basically looks like you take our documents you split them in some way into these smaller chunks that can be easily embedded um those embeddings are then numerical representations of those documents that are easily searchable and they're stored in an index when given a question that's also embedded the index performs a similarity search and returns splits that are relevant to the question now if we dig a little bit more under the hood we can think about it like this if we take a document and embed it let's imagine that embedding just had three dimensions so you know each document is projected into some point in this 3D space now the point is that the location in space is determined by the semantic meaning or content in that document so to follow that then documents in similar locations in space contain similar semantic information and this very simple idea is really the Cornerstone for a lot of search and retrieval methods that you'll see with modern Vector stores so in particular we take our documents we embed them into this in this case a toy 3D space we take our question do the same we can then do a search like a local neighborhood search you can think about in this 3D space around our question to say hey what documents are nearby and these nearby neighbors are then retrieved because they can they have similar semantics relative to our question and that's really what's going on here so again we took our documents we split them we embed them and now they exist in this high dimensional space we've taken our question embedded it projected in that same space and we just do a search around the question from nearby documents and grab ones that are close and we can pick some number we can say we want one or two or three or n documents close to my question in this embedding space and there's a lot of really interesting methods that implement this very effectively I I link one here um and we have a lot of really nice uh Integrations to play with this general idea so many different embedding models many different indexes lots of document loaders um and lots of Splitters that can be kind of recombined to test different ways of doing this kind of indexing or retrieval um so now I'll show a bit of a code walkth through so here we defined um we kind of had walked through this previously this is our notebook we've installed a few packages we've set a few environment variables using lsmith and we showed this previously this is just an overview showing how to run rag like kind of end to end in the last uh short talk we went through indexing um and what I'm going to do very simply is I'm just going to reload our documents so now I have our documents I'm going to resplit them and we saw before how we can build our index now here let's actually do the same thing but in the slide we actually showed kind of that notion of search in that 3D space and a nice parameter to think about in building your your retriever is K so K tells you the number of nearby neighbors to fetch when you do that retrieval process and we talked about you know in that 3D space do I want one nearby neighbor or two or three so here we can specify k equals 1 for example now we're building our index so we're taking every split embedding it storing it now what's nice is I asked a a question what is Task decomposition this is related to the blog post and I'm going to run get relevant documents so I run that and now how many documents do I get back I get one as expected based upon k equals 1 so this retrieve document should be related to my question now I can go to lsmith and we can open it up and we can look at our Retriever and we can see here was our question here's the one document we got back and okay so that makes sense this document pertains to task ke decomposition in particular and it kind of lays out a number of different approaches that can be used to do that this all kind of makes sense and this shows kind of in practice how you can implement this this NE this kind of KNN or k nearest neighbor search uh really easily uh just using a few lines of code and next we're going to talk about generation thanks hey this is Lance from Lang chain this is the fourth uh short video in our rack from scratch series that's going to be focused on generation now in the past few videos we walked through the general flow uh for kind of basic rag starting with indexing Fall by retrieval then generation of an answer based upon the documents that we retrieved that are relevant to our question this is kind of the the very basic flow now an important consideration in generation is really what's happening is we're taking the documents you retrieve and we're stuffing them into the llm context window so if we kind of walk back through the process we take documents we split them for convenience or embedding we then embed each split and we store that in a vector store as this kind of easily searchable numerical representation or vector and we take a question embed it to produce a similar kind of numerical representation we can then search for example using something like KN andn in this kind of dimensional space for documents that are similar to our question based on their proximity or location in this space in this case you can see 3D is a toy kind of toy example now we've recovered relevant splits to our question we pack those into the context window and we produce our answer now this introduces the notion of a prompt so the prompt is kind of a you can think have a placeholder that has for example you know in our case B keys so those keys can be like context and question so they basically are like buckets that we're going to take those retrieve documents and Slot them in we're going to take our question and also slot it in and if you kind of walk through this flow you can kind of see that we can build like a dictionary from our retrieve documents and from our question and then we can basically populate our prompt template with the values from the dict and then becomes a prompt value which can be passed to llm like a chat model resulting in chat messages which we then parse into a string and get our answer so that's like the basic workflow that we're going to see and let's just walk through that in code very quickly to kind of give you like a HandsOn intuition so we had our notebook we walk through previously install a few packages I'm setting a few lsmith environment variables we'll see it's it's nice for uh kind of observing and debugging our traces um previously we did this quick start we're going to skip that over um and what I will do is I'm going to build our retriever so again I'm going to take documents and load them uh and then I'm going to split them here we've kind of done this previously so I'll go through this kind of quickly and then we're going to embed them and store them in our index so now we have this retriever object here now I'm going to jump down here now here's where it's kind of fun this is the generation bit and you can see here I'm defining something new this is a prompt template and what my prompt template is something really simple it's just going to say answer the following question based on this context it's going to have this context variable and a question so now I'm building my prompt so great now I have this prompt let's define an llm I'll choose 35 now this introdu the notion of a chain so in Lang chain we have an expression language called L Cel Lang chain expression language which lets you really easily compose things like prompts LMS parsers retrievers and other things but the very simple kind of you know example here is just let's just take our prompt which you defined right here and connect it to an LM which you defined right here into this chain so there's our chain now all we're doing is we're invoking that chain so every L expression language chain has a few common methods like invoke bat stream in this case we just invoke it with a dict so context and question that maps to the expected Keys here in our template and so if we run invoke what we see is it's just going to execute that chain and we get our answer now if we zoom over to Langs Smith we should see that it's been populated so yeah we see a very simple runable sequence here was our document um and here's our output and here is our prompt answer the following question based on the context here's the document we passed in here is the question and then we get our answer so that's pretty nice um now there's a lot of other options for rag prompts I'll pull one in from our prompt tub this one's like kind of a popular prompt so it just like has a little bit more detail but you know it's the main the main intuition is the same um you're passing in documents you're asking them to reason about the documents given a question produce an answer and now here I'm going to find a rag chain which will automatically do the retrieval for us and all I have to do is specify here's my retriever which we defined before here's our question we which we invoke with the question gets passed through to the key question in our dict and it automatically will trigger the retriever which will return documents which get passed into our context so it's exactly what we did up here except before we did this manually and now um this is all kind of automated for us we pass that dick which is autop populated into our prompt llm out to parser now let invoke it and that should all just run and great we get an answer and we can look at the trace and we can see everything that happened so we can see our retriever was run these documents were retrieved they get passed into our LM and we get our final answer so this kind of the end of our overview um where we talked about I'll go back to the slide here quickly we talked about indexing retrieval and now generation and followup short videos we'll kind of dig into some of the more com complex or detailed themes that address some limitations that can arise in this very simple pipeline thanks hi my from Lang chain over the next few videos we're going to be talking about career translation um and in this first video we're going to cover the topic of multiquery so query translation sits kind of at the first stage of an advanced rag Pipeline and the goal of career translation is really to take an input user question and to translate in some way in order to improve retrieval so the problem statement is pretty intuitive user queries um can be ambiguous and if the query is poorly written because we're typically doing some kind of semantic similarity search between the query and our documents if the query is poorly written or ill opposed we won't retrieve the proper documents from our index so there's a few approaches to attack this problem and you can kind of group them in a few different ways so here's one way I like to think about it a few approaches has involveed query rewriting so taking a query and reframing it like writing from a different perspective um and that's what we're going to talk about a little bit here in depth using approaches like multiquery or rag Fusion which we'll talk about in the next video you can also do things like take a question and break it down to make it less abstract like into sub questions and there's a bunch of interesting papers focused on that like least to most from Google you can also take the opposite approach of take a question to make it more abstract uh and there's actually approach we're going to talk about later in a future video called stepback prompting that focuses on like kind of higher a higher level question from the input so the intuition though for this multier approach is we're taking a question and we're going to break it down into a few differently worded questions uh from different perspectives and the intuition here is simply that um it is possible that the way a question is initially worded once embedded it is not well aligned or in close proximity in this High dimensional embedding space to a document that we want to R that's actually related so the thinking is that by kind of rewriting it in a few different ways you actually increase the likel of actually retrieving the document that you really want to um because of nuances in the way that documents and questions are embedded this kind of more shotgun approach of taking a question Fanning it out into a few different perspectives May improve and increase the reliability of retrieval that's like the intuition really um and of course we can com combine this with retrieval so we can take our our kind of fan out questions do retrieval on each one and combine them in some way and perform rag so that's kind of the overview and now let's what let's go over to um our code so this is a notebook and we're going to share all this um we're just installing a few packages we're setting a lsmith API Keys which we'll see why that's quite useful here shortly there's our diagram now first I'm going to Index this blog post on agents I'm going to split it um well I'm going to load it I'm going to split it and then I'm going to index it in chroma locally so this is a vector store we've done this previously so now I have my index defined so here is where I'm defining my prompt for multiquery which is your your assistant your task is to basically reframe this question into a few different sub questions um so there's our prompt um right here we'll pass that to an llm part it um into a string and then split the string by new lines and so we'll get a list of questions out of this chain that's really all we're doing here now all we're doing is here's a sample input question there's our generate queries chain which we defined we're going to take that list and then simply apply each question to retriever so we'll do retrieval per question and this little function here is just going to take the unique Union of documents uh across all those retrievals so let's run this and see what happens so we're going to run this and we're going to get some set of questions uh or documents back so let's go to Langs Smith now we can actually see what happened under the hood so here's the key point we ran our initial chain to generate a set of of reframed questions from our input and here was that prompt and here is that set of questions that we generated now what happened is for every one of those questions we did an independent retrieval that's what we're showing here so that's kind of the first step which is great now I can go back to the notebook and we can show this working end to end so now we're going to take that retrieval chain we'll pass it into context of our final rag prompt we'll also pass through the question we'll pass that to our rag prompt here pass it to an LM and then Pary output now let's let's kind of see how that works so again that's okay there it is so let's actually go into langth and see what happened under the hood so this was our final chain so this is great we took our input question we broke it out to these like five rephrase questions for every one of those we did a retrieval that's all great we then took the unique Union of documents and you can see in our final llm prompt answer the following cont following question based on the context this is the final set of unique documents that we retrieved from all of our sub questions um here's our initial question there's our answer so that kind of shows you how you can set this up really easily how you can use l Smith to kind of investigate what's going on and in particular use l Smith to investigate those intermediate questions that you generate in that like kind of question generation phase and in a future talks we're going to go through um some of these other methods that we kind of introduced at the start of this one thank you last L chain this is the second video of our Deep dive on query translation in our rag from scratch series focused on a method called rag Fusion so as we kind of showed before career translation you can think of as the first stage in an advanced rag pipeline we're taking an input user question and We're translating it some way in order to improve retrievable now we showed this General mapping of approaches previously so again you have kind of like rewriting so you can take a question and like kind of break it down into uh differently worded are different different perspectives of the same question so that's kind of rewriting there's sub questions where you take a question break it down into smaller problems solve each one independently and then there step back where you take a question and kind of go more abstract where you kind of ask a higher level question as a precondition to answer the user question so those are the approaches and we're going to dig into one of the particular approaches for rewriting called rat Fusion now this is really similar to what we just saw with multiquery the difference being we actually apply a a kind of a clever rank ranking step of our retriev documents um which you call reciprocal rank Fusion that's really the only difference the the input stage of taking a question breaking it out into a few kind of differently worded questions retrieval on each one is all the same and we're going to see that in the code here shortly so let's just hop over there and then look at this so again here is a notebook that we introduced previously here's the packages we've installed we've set a few API keys for lsmith which we see why is quite useful um and you can kind of go down here to a rag Fusion section and the first thing you'll note is what our prompt is so it looks really similar to The Prompt we just saw with multiquery and simply your helpful assistant that generates multiple search queries based upon user input and here's the question output for queries so let's define our prompt and here was our query Generation chain again this looks a lot like we just saw we take our prompt Plum that into an llm and then basically parse by new lines and that'll basically split out these questions into a list that's all it's going to happen here so that's cool now here's where the novelty comes in each time we do retrieval from one of those questions we're going to get back a list of documents from our Retriever and so we do it over that we generate four questions here based on our prompt we do the over four questions well like a list of lists basically now reciprocal rank Fusion is really well suited for this exact problem we want to take this list to list and build a single Consolidated list and really all that's going on is it's looking at the documents in each list and kind of aggregating them into a final output ranking um and that's really the intuition around what's happening here um so let's go ahead and so let's so let's go ahead and look at that in some detail so we can see we run retrieval that's great now let's go over to Lang Smith and have a look at what's going on here so we can see that here was our prompt to your helpful assistant that generates multiple search queries based on a single input and here is our search queries and then here are our four retrievals so that's that's really good so we know that all is working um and then those retrievals simply went into this rank function and our correspondingly ranked to a final list of six unique rank documents that's really all we did so let's actually put that all together into an a full rag chain that's going to run retrieval return that final list of rank documents and pass it to our context pass through our question send that to a rag prompt pass it to an LM parse it to an output and let's run all that together and see that working cool so there's our final answer now let's have a look in lsmith we can see here was our four questions here's our retrievals and then our final rag prompt plumed through the final list of ranked six questions which we can see laid out here and our final answer so this can be really convenient particularly if we're operating across like maybe different Vector stores uh or we want to do like retrieval across a large number of of kind of differently worded questions this reciprocal rank Fusion step is really nice um for example if we wanted to only take the top three documents or something um it can be really nice to build that Consolidated ranking across all these independent retrievals then pass that to for the final generation so that's really the intuition about what's happening here thanks hi this is Lance from Lang chain this is our third video focused on query translation in the rag from scratch series and we're going to be talking about decomposition so query translation in general is a set of approaches that sits kind of towards the front of this overall rag Pipeline and the objective is to modify or rewrite or otherwise decompose an input question from a user in order improve retrieval so we can talk through some of these approaches previously in particular various ways to do query writing like rag fusion and multiquery there's a separate set of techniques that become pretty popular and are really interesting for certain problems which we might call like kind of breaking down or decomposing an input question into a set of sub questions um so some of the papers here that are are pretty cool are for example this work from Google um and the objective really is first to take an input question and decompose it into a set of sub problems so this particular example from the paper was the problem of um last letter concatenation and so it took the inut question of three words think machine learning and broke it down into three sub problems think think machine think machine learning as the third sub problem and then you can see in this bottom panel it solves each one individually so it shows for example in green solving the problem think machine where you can catenate the last letter of k with the last letter of machine or last letter think K less machine e can concatenate those to K and then for the overall problem taking that solution and then and basically building on it to get the overall solution of keg so that's kind of one concept of decomposing into sub problems solving them sequentially now a related work called IRC or in leap retrieval combines retrieval with Chain of Thought reasoning and so you can kind of put these together into one approach which you can think of as kind of dynamically retrieval um to solve a set of sub problems kind of that retrieval kind of interleaving with Chain of Thought as noted in the second paper and a set of decomposed questions based on your initial question from the first work from Google so really the idea here is we're taking one sub question we're answering it we're taking that answer and using it to help answer the second sub question and so forth so let's actually just walk through this in code to show how this might work so this is The Notebook we've been working with from some of the other uh videos you can see we already have a retriever to find uh up here at the top and what we're going to do is we're first going to find a prompt that's basically going to say given an input question let's break it down to set of sub problems or sub question which can be solved individually so we can do that and this blog post is focused on agents so let's ask a question about what are the main components of an LM powerered autonomous agent system so let's run this and see what the decomposed questions are so you can see the decomposed questions are what is LM technology how does it work um what are components and then how the components interact so it's kind of a sane way to kind of break down this problem into a few sub problems which you might attack individually now here's where um we Define a prompt that very simply is going to take our question we'll take any prior questions we've answered and we'll take our retrieval and basically just combine them and we can Define this very simple chain um actually let's go back and make sure retriever is defined up at the top so now we are building our retriever good we have that now so we can go back down here and let's run this so now we are running and what's happening is we're trying to solve each of these questions individually using retrieval and using any prior question answers so okay very good looks like that's been done and we can see here's our answer now let's go over to langth and actually see what happened under the hood so here's what's kind of of interesting and helpful to see for the first question so here's our first one it looks like it just does retrieval which is we expect and then it uses that to answer this initial question now for the second question should be a little bit more interesting because if you look at our prompt here's our question now here is our background available question answer pair so this was the answer question answer pair from the first question which we add to our prompt and then here's the retrieval for this particular question so we're kind of building up up the solution because we're pending the question answer pair from question one and then likewise with question three it should combine all of that so we can look at here here's our question here's question one here's question two great now here's additional retrieval related to this particular question and we get our final answer so that's like a really nice way you can kind of build up Solutions um using this kind of interleaved uh retrieval and concatenating question answer pairs I do want to mention very briefly that we can also take a different approach where we can just answer these all individually and then just concatenate all those answers to produce a final answer and I'll show that really quickly here um it's like a little bit less interesting maybe because you're not using answers from each uh question to inform the next one you're just answering them all in parallel this might be better for cases where it's not really like a sub question decomposition but maybe it's like like a set of set of several in independent questions whose answers don't depend on each other that might be relevant for some problems um and we can go ahead and run okay so this ran as well we can look at our trace and in this case um yeah we can see that this actually just kind of concatenates all of our QA pairs to produce the final answer so this gives you a sense for how you can use quer decomposition employ IDE IDE from uh from two different papers that are pretty cool thanks hi this is Lance from Lang chain this is the fourth video uh in our Deep dive on queer translation in the rag from scratch series and we're going to be focused on step back prompting so queer translation as we said in some of the prior videos kind of sits at the the kind of first stage of kind of a a a rag pipeline or flow and the main aim is to take an question and to translate it or modify in such a way that it improves retrieval now we talked through a few different ways to approach this problem so one General approach involves rewriting a question and we talk about two ways to do that rag fusion multiquery and again this is this is really about taking a question and modifying it to capture a few different perspectives um which may improve the retrieval process now another approach is to take a question and kind of make it less abstract like break it down into sub questions um and then solve each of those independently so that's what we saw with like least to most prompting um and a bunch of other variants kind of in that in that vein of sub problem solving and then consolidating those Solutions into a final answer now a different approach presented um by again Google as well is stepback prompting so stepback prompting kind of takes the the the opposite approach where it tries to ask a more abstract question so the paper talks a lot about um using F shot prompting to produce what they call the stepback or more abstract questions and the way it does it is it provides a number of examples of stepb back questions given your original question so like this is like this is for example they like for prompt temp you're an expert World Knowledge I asked you a question your response should be comprehensive not contradict with the following um and this is kind of where you provide your like original and then step back so here's like some example um questions so like um like uh at year saw the creation of the region where the country is located which region of the country um is the county of of herir related um Janell was born in what country what is janell's personal history so that that's maybe a more intuitive example so it's like you ask a very specific question about like the country someone's born the more abstract question is like just give me the general history of this individual without worrying about that particular um more specific question um so let's actually just walk through how this can be done in practice um so again here's kind of like a a diagram of uh the various approaches um from less abstraction to more abstraction now here is where we're formulating our prompt using a few of the few shot examples from the paper um so again like input um yeah something about like the police perform wful arrests and what what camp members of the police do so like it it basically gives the model a few examples um we basically formulate this into a prompt that's really all going on here again we we repeat um this overall prompt which we saw from the paper your expert World Knowledge your test is to step back and paraphrase a question generate more a generic step back question which is easier to answer here are some examples so it's like a very intuitive prompt so okay let's start with the question what is Task composition for llm agents and we're going to say generate stack question okay so this is pretty intuitive right what is a process of task compos I so like not worrying as much about agents but what is that process of task composition in general and then hopefully that can be independently um retrieved we we can independently retrieve documents related to the stepb back question and in addition retrieve documents related to the the actual question and combine those to produce kind of final answer so that's really all that's going on um and here's the response template where we're Plumbing in the stepback context and our question context and so what we're going to do here is we're going to take our input question and perform retrieval on that we're also going to generate our stepb back question and perform retrieval on that we're going to plumb those into the prompt as here's our very here's our basically uh our prompt Keys normal question step back question um and our overall question again we formulate those as a dict we Plum those into our response prompt um and then we go ahead and attempt to answer our overall question so we're going to run that that's running and okay we have our answer now I want to hop over to Langs Smith and attempt to show you um kind of what that looked like under the hood so let's see let's like go into each of these steps so here was our prompt right you're an expert World Knowledge your test to to step back and paraph as a question um so um here were our few shot prompts and this was our this was our uh stepb question so what is the process of task composition um good from the input what is Tas composition for LM agents we perform retrieval on both what is process composition uh and what is for LM agents we perform both retrievals we then populate our prompt with both uh original question answer and then here's the context retrieve from both the question and the stepb back question here was our final answer so again this is kind of a nice technique um probably depends on a lot of the types of like the type of domain you want to perform retrieval on um but in some domains where for example there's a lot of kind of conceptual knowledge that underpins questions you expect users to ask this stepback approach could be really convenient to automatically formulate a higher level question um to for example try to improve retrieval I can imagine if you're working with like kind of textbooks or like technical documentation where you make independent chapters focused on more highlevel kind of like Concepts and then other chapters on like more detailed uh like implementations this kind of like stepb back approach and independent retrieval could be really helpful thanks hi this is Lance from Lang chain this is the fifth video focused on queer translation in our rack from scratch series we're going to be talking about a technique called hide so again queer translation sits kind of at the front of the overall rag flow um and the objective is to take an input question and translate it in some way that improves retrieval now hide is an interesting approach that takes advantage of a very simple idea the basic rag flow takes a question and embeds it takes a document and embeds it and looks for similarity between an embedded document and embedded question but questions and documents are very different text objects so documents can be like very large chunks taken from dense um Publications or other sources whereas questions are short kind of tur potentially ill worded from users and the intuition behind hide is take questions and map them into document space using a hypothetical document or by generating a hypothetical document um that's the basic intuition and the idea kind of shown here visually is that in principle for certain cases a hypothetical document is closer to a desired document you actually want to retrieve in this you know High dimensional embedding space than the sparse raw input question itself so again it's just kind of means of trans translating raw questions into these hypothetical documents that are better suited for retrieval so let's actually do a Code walkthrough to see how this works and it's actually pretty easy to implement which is really nice so first we're just starting with a prompt and we're using the same notebook that we've used for prior videos we have a blog post on agents r index um so what we're going to do is Define a prompt to generate a hypothetical documents in this case we'll say write a write a paper passage uh to answer a given question so let's just run this and see what happens again we're taking our prompt piping it to to open Ai chck gpte and then using string Opa parer and so here's a hypothetical document section related to our question okay and this is derived of course lm's kind of embedded uh kind of World Knowledge which is you know a sane place to generate hypothetical documents now let's now take that hypothetical document and basically we're going to pipe that into a retriever so this means we're going to fetch documents from our index related to this hypothetical document that's been embedded and you can see we get a few qu a few retrieved uh chunks that are related to uh this hypothetical document that's all we've done um and then let's take the final step where we take those retrieve documents here which we defined and our question we're going to pipe that into this rag prompt and then we're going to run our kind of rag chain right here which you've seen before and we get our answer so that's really it we can go to lsmith and we can actually look at what happened um so here for example this was our final um rag prompt answer the following question based on this context and here is the retrieve documents that we passed in so that part's kind of straightforward we can also look at um okay this is our retrieval okay now this is this is actually what we we generated a hypothetical document here um okay so this is our hypothetical document so we've run chat open AI we generated this passage with our hypothetical document and then we've run retrieval here so this is basically showing hypothetical document generation followed by retrieval um so again here was our passage which we passed in and then here's our retrieve documents from the retriever which are related to the passage content so again in this particular index case it's possible that the input question was sufficient to retrieve these documents in fact given prior examples uh I know that some of these same documents are indeed retrieved just from the raw question but in other context it may not be the case so folks have reported nice performance using Hyde uh for certain domains and the Really convenient thing is that you can take this this document generation prompt you can tune this arbitrarily for your domain of Interest so it's absolutely worth experimenting with it's a it's a need approach uh that can overcome some of the challenges with retrieval uh thanks very much hi this is Lance from Lang chain this is the 10th video in our rack from scratch series focused on routing so we talk through query translation which is the process of taking a question and translating in some way it could be decomposing it using stepback prompting or otherwise but the idea here was take our question change it into a form that's better suited for retrieval now routing is the next step which is basically routing that potentially decomposed question to the right source and in many cases that could be a different database so let's say in this toy example we have a vector store a relational DB and a graph DB the what we redo with routing is we simply route the question based upon the cont of the question to the relevant data source so there's a few different ways to do that one is what we call logical routing in this case we basically give an llm knowledge of the various data sources that we have at our disposal and we let the llm kind of Reason about which one to apply the question to so it's kind of like the the LM is applying some logic to determine you which which data sour for example to to use alternatively you can use semantic routing which is where we take a question we embed it and for example we embed prompts we then compute the similarity between our question and those prompts and then we choose a prompt based upon the similarity so the general idea is in our diagram we talk about routing to for example a different database but it can be very general can be routing to different prompt it can be you know really arbitrarily taking this question and sending it at different places be at different prompts be at different Vector stores so let's walk through the code a little bit so you can see just like before we've done a few pip installs we set up lsmith and let's talk through uh logical routing first so so in this toy example let's say we had for example uh three different docs like we had python docs we had JS docs we had goang docs what we want to do is take a question route it to one of those three so what we're actually doing is we're setting up a data model which is basically going to U be bound to our llm and allow the llm to Output one of these three options as a structured object so you really think about this as like classification classification plus function calling to produce a structured output which is constrained to these three possibilities so the way we do that is let's just zoom in here a little bit we can Define like a structured object that we want to get out from our llm like in this case we want for example you know one of these three data sources to be output we can take this and we can actually convert it into open like open for example function schema and then we actually pass that in and bind it to our llm so what happens is we ask a question our llm invokes this function on the output to produce an output that adheres to the schema that we specify so in this case for example um we output like you know in this toy example let's say we wanted like you know an output to be data source Vector store or SQL database the output will contain a data source object and it'll be you know one of the options we specify as a Json string we also instantiate a parser from this object to parse that Json string to an output like a pantic object for example so that's just one toy example and let's show one up here so in this case again we had our three doc sources um we bind that to our llm so you can see we do with structured output basically under the hood that's taking that object definition turning into function schema and binding that function schema to our llm and we call our prompt you're an expert at routing a user question based on you know programming language um that user referring to so let's define our router here now what we're going to do is we'll ask a question that is python code so we'll call that and now it's done and you see the object we get out is indeed it's a route query object so it's exactly it aderes to this data model we've set up and in this case it's it's it's correct so it's calling this python doc so you can we can extract that right here as a string now once we have this you can really easily set up like a route so this could be like our full chain where we take this router we should defined here and then this choose route function can basically take that output and do something with it so for example if python docs this could then apply the question to like a retriever full of python information uh or JS same thing so this is where you would hook basically that question up to different chains that are like you know retriever chain one for python retriever chain two for JS and so forth so this is kind of like the routing mechanism but this is really doing the heavy lifting of taking an input question and turning into a structured object that restricts the output to one of a few output types that we care about in our like routing problem so that's really kind of the way this all hooks together now semantic outing is actually maybe even a little bit more straightforward based on what we've seen previously so in that case let's say we have two prompts we have a physics prompt we have a math prompt we can embed those prompts no problem we do that here now let's say we have an input question from a user like in this case what is a black hole we pass that through we then apply this runnable Lambda function which is defined right here what we're doing here is we're embedding the question we're Computing similarity between the question and the prompts uh we're taking the most similar and then we're basically choosing the prompt based on that similarity and you can see let's run that and try it out and we're using the physics prompt and there we go black holes region and space so that just shows you kind of how you can use semantic routing uh to basically embed a question embed for example various prompts pick the prompt based on sematic similarity so that really gives you just two ways to do routing one is logical routing with function in uh can be used very generally in this case we applied it to like different coding languages but imagine these could be swapped out for like you know my python uh my like vector store versus My Graph DB versus my relational DB and you could just very simply have some description of what each is and you know then not only will the llm do reasoning but it'll also return an object uh that can be parsed very cleanly to produce like one of a few very specific types which then you can reason over like we did here in your routing function so that kind of gives you the general idea and these are really very useful tools and I encourage you to experiment with them thanks hi this is Lance from Lang chain this is the 11th part of our rag from scratch video series focused on query construction so we previously talked through uh query translation which is the process of taking a question and converting it or translating it into a question that's better optimized for retrieval then we talked about routing which is the process of going taking that question routing it to the right Source be it a given Vector store graph DB um or SQL DB for example now we're going to talk about the process of query construction which is basically taking natural language and converting it into particular domain specific language uh for one of these sources now we're going to talk specifically about the process of going from natural language to uh meditated filters for Vector Stores um the problem statement is basically this let's imagine we had an index of Lang Chain video transcripts um you might want to ask a question give me you know or find find me videos on chat Lang chain published after 2024 for example um the the process of query structuring basically converts this natural language question into a structured query that can be applied to the metadata uh filters on your vector store so most Vector stores will have some kind of meditative filters that can do kind of structur querying on top of uh the chunks that are indexed um so for example this type of query will retrieve all chunks uh that talk about the topic of chat Lang chain uh published after the date 2024 that's kind of the problem statement and to do this we're going to use function calling um in this case you can use for example open AI or other providers to do that and we're going to do is at a high level take the metadata fields that are present in our Vector store and divide them to the model as kind of information and the model then can take those and produce queries that adhere to the schema provided um and then we can parse those out to a structured object like a identic object which again which can then be used in search so that's kind of the problem statement and let's actually walk through code um so here's our notebook which we've kind of gone through previously and I'll just show you as an example let's take a example YouTube video and let's look at the metadata that you get with the transcript so you can see you get stuff like description uh URL um yeah publish date length things like that now let's say we had an index that had um basically a that had a number of different metadata fields and filters uh that allowed us to do range filtering on like view count publication date the video length um or unstructured search on contents and title so those are kind of like the imagine we had an index that had uh those kind of filters available to us what we can do is capture that information about the available filters in an object so we're calling that this tutorial search object kind of encapsulates that information about the available searches that we can do and so we basically enumerate it here content search and title search or semantic searches that can be done over those fields um and then these filters then are various types of structure searches we can do on like the length um The View count and so forth and so we can just kind of build that object now we can set this up really easily with a basic simple prompt that says you know you're an expert can bring natural language into database queries you have access to the database tutorial videos um given a question return a database query optimize retrieval so that's kind of it now here's the key point though when you call this LM with structured output you're binding this pantic object which contains all the information about our index to the llm which is exactly what we talked about previously it's really this process right here you're taking this object you're converting it to a function schema for example open AI you're binding that to your model and then you're going to be able to get um structured object out versus a Json string from a natural language question which can then be parsed into a pantic object which you get out so that's really the flow and it's taking advantage of function calling as we said so if we go back down we set up our query analyzer chain right here now let's try to run that just on a on a purely semantic input so rag from scratch let's run that and you can see this just does like a Content search and a title search that's exactly what you would expect now if we pass a question that includes like a date filter let's just see if that would work and there we go so you kind of still get that semantic search um but you also get um search over for example publish date earliest and latest publish date kind of as as you would expect let's try another one here so videos focus on the topic of chat Lang chain they're published before 2024 this is just kind of a rewrite of this question in slightly different way using a different date filter and then you can see we can get we get content search title search and then we can get kind of a date search so this is a very general strategy that can be applied kind of broadly to um different kinds of querying you want to do it's really the process of going from an unstructured input to a structured query object out following an arbitrary schema that you provide and so as noted really this whole thing we created here this tutorial search is based upon the specifics of our Vector store of interest and if you want to learn more about this I link to some documentation here that talks a lot about different uh types of of Integrations we have with different Vector store providers to do exactly this so it's a very useful trick um it allows you to do kind of query uh uh say metadata filter filtering on the fly from a natural language question it's a very convenient trick uh that works with many different Vector DBS so encourage you to play with it thanks this is Lance from Lang chain I'm going to talk about indexing uh and mulation indexing in particular for the 12th part of our rag from scratch series here so we previously talked about a few different major areas we talk about query translation which takes a question and translates it in some way to optimize for retrieval we talk about routing which is the process of taking a question routing it to the right data source be it a vector store graph DB uh SQL DB we talked about queer construction we dug into uh basically queer construction for Vector stores but of course there's also text SQL text to Cipher um so now we're going to talk about indexing a bit in particular we're going to talk about indexing indexing techniques for Vector Stores um and I want to highlight one particular method today called multirepresentation indexing so the high LEL idea here is derived a bit from a paper called proposition indexing which kind of makes a simple observation you can think about decoupling raw documents and the unit you use for retrieval so in the typical case you take a document you split it up in some way to index it and then you embed the split directly um this paper talks about actually taking a document splitting it in some way but then using an llm to produce what they call a proposition which you can think of as like kind of a distillation of that split so it's kind of like using an llm to modify that split in some way to distill it or make it like a crisper uh like summary so to speak that's better optimized for retrieval so that's kind of one highlight one piece of intuition so we actually taken that idea and we've kind of built on it a bit in kind of a really nice way that I think is very well suited actually for long context llms so the idea is pretty simple you take a document and you you actually distill it or create a proposition like they show in the prior paper I kind of typically think of this as just produce a summary of the document and you embed that summary so that summary is meant to be optimized for retrieval so might contain a bunch of keywords from the document or like the big ideas such that when you embed the summary you embed a question you do search you basically can find that document based upon this highly optimized summary for retrieval so that's kind of represented here in your vector store but here's the catch you independently store the raw document in a dock store and when you when you basically retrieve the summary in the vector store you return the full document for the llm to perform generation and this is a nice trick because at generation time now with long condex LMS for example the LM can handle that entire document you don't need to worry about splitting it or anything you just simply use the summary to prod like to create a really nice representation for fishing out that full dock use that full dock in generation there might be a lot of reasons you want to do that you want to make sure the LM has the full context to actually answer the question so that's the big idea it's a nice trick and let's walk through some code here we have a notebook all set up uh just like before we done some pip installs um set to maybe I Keys here for lsmith um kind of here's a diagram now let me show an example let's just load two different uh blog posts uh one is about agents one is about uh you know human data quality um and what we're going to do is let's create a summary of each of those so this is kind of the first step of that process where we're going from like the raw documents to summaries let's just have a look and make sure those ran So Okay cool so the first DOC discusses you know building autonomous agents the second doc contains the importance of high quality human data and training okay so that's pretty nice we have our summaries now we're going to go through a process that's pretty simple first we Define a vector store that's going to index those summaries now we're going to Define what we call like our our document storage is going to store the full documents okay so this multiv Vector retriever kind of just pulls those two things together we basically add our Dock Store we had this bite store is basically the the the full document store uh the vector store is our Vector store um and now this ID is what we're going to use to reference between the chunks or the summaries and the full documents that's really it so now for every document we'll Define a new Doc ID um and then we're basically going to like take our summary documents um and we're going to extract um for each of our summaries we're going to get the associated doc ID so we go um so let's go ahead and do that so we have our summary docs which we add to the vector store we have our full documents uh our doc IDs and the full raw documents which are added to our doc store and then let's just do a query Vector store like a similarity search on our Vector store so memory and agents and we can see okay so we can extract you know from the summaries we can get for example the summary that pertains to um a agents so that's a good thing now let's go ahead and run a query get relevant documents on our retriever which basically combines the summaries uh which we use for retrieval then the doc store which we use to get the full doc back so we're going to apply our query we're going to basically run this and here's the key Point we've gotten back the entire article um and we can actually if you want to look at the whole thing we we can just go ahead and do this here we go so this is the entire article that we get back from that search so it's a pretty nice trick again we query with just memory and agents um and we can kind of go back to our diagram here we quered for memory and agents it started our summaries it found the summary related to memory and agents it uses that doc ID to reference between the vector store and the doc store it fishes out the right full doc returns us the full document in this case the full web page that's really it simple idea nice way to go from basically like nice simple proposition style or summary style indexing to full document retrieval which is very useful especially with long contact LMS thank you hi this is Lance from Lang chain this is the 13th part of our rag from scratch series focused on a technique called Raptor so Raptor sits within kind of an array of different indexing techniques that can be applied on Vector Stores um we just talked about multirepresentation indexing um we I priv a link to a video that's very good talking about the different means of chunking so I encourage you to look at that and we're going to talk today about a technique called Raptor which you can kind of think of it as a technique for hierarchical indexing so the highle intuition is this some questions require very detailed information from a corpus to answer like pertain to a single document or single chunk so like we can call those lowlevel questions some questions require consolidation across kind broad swast of a document so across like many documents or many chunks within a document and you can call those like higher level questions and so there's kind of this challenge in retrieval and that typically we do like K nearest neighbors retrieval like we've been talking about you're fishing out some number of chunks but what if you have a question that requires information across like five six you know or a number of different chunks which may exceed you know the K parameter in your retrieval so again when you typically do retrieval you might set a k parameter of three which means you're retrieving three chunks from your vector store um and maybe you have a high very high level question that could benefit from infation across more than three so this technique called raptor is basically a way to build a hierarchical index of document summaries and the intuition is this you start with a set of documents as your Leafs here on the left you cluster them and then you Summarize each cluster so each cluster of similar documents um will consult information from across your context which is you know your context could be a bunch of different splits or could even be across a bunch of different documents you're basically capturing similar ones and you're consolidating the information across them in a summary and here's the interesting thing you do that recursively until either you hit like a limit or you end up with one single cluster that's a kind of very high level summary of all of your documents and what the paper shows is that if you basically just collapse all these and index them together as a big pool you end up with a really nice array of chunks that span the abstraction hierarchy like you have a bunch of chunks from Individual documents that are just like more detailed chunks pertaining to that you know single document but you also have chunks from these summaries or I would say like you know maybe not chunks but in this case the summary is like a distillation so you know raw chunks on the left that represent your leavs are kind of like the rawest form of information either raw chunks or raw documents and then you have these higher level summaries which are all indexed together so if you have higher level questions they should basically be more similar uh in sematic search for example to these higher level summary chunks if you have lower level questions then they'll retrieve these more lower level chunks and so you have better semantic coverage across like the abstraction hierarchy of question types that's the intuition they do a bunch of nice studies to show that this works pretty well um I actually did a deep dive video just on this which I link below um I did want to cover it briefly just at a very high level um so let's actually just do kind of a code walkr and I've added it to this rack from scratch course notebook but I link over to my deep dive video as well as the paper and the the full code notebook which is already checked in is discussed at more length in the Deep dive the technique is a little bit detailed so I only want to give you very high levels kind of overview here and you can look at the Deep dive video if you want to go in more depth again we talked through this abstraction hierarchy um I applied this to a large set of Lang chain documents um so this is me loading basically all of our Lang chain expression language docs so this is on the order of 30 documents you can see I do a histogram here of the token counts per document some are pretty big most are fairly small less than you know 4,000 tokens um and what I did is I indexed all of them um individually so the all those raw documents you can kind of Imagine are here on the left and then I do um I do embedding I do clustering summarization and I do that recursively um until I end up with in this case I believe I only set like three levels of recursion and then I save them all my Vector store so that's like the highle idea I'm applying this Raptor technique to a whole bunch of Lang chain documents um that have fairly large number of tokens um so I do that um and yeah I use actually use both CLA as well as open AI here um this talks through the clustering method which they that they use which is pretty interesting you can kind of dig into that on your own if if you're really um interested this is a lot of their code um which I cite accordingly um this is basically implementing the clustering method that they use um and this is just simply the document embedding stage um this is like basically embedding uh and clustering that's really it uh some text formatting um summarizing of the clusters right here um and then this is just running that whole process recursively that's really it um this is tree building so basically I have the RO the rod docs let's just go back and look at Doc texts so this should be all my raw documents uh so that's right you can see it here doc text is basically just the text in all those Lang chain documents that I pulled um and so I run this process on them right here uh so this is that recursive embedding cluster basically runs and produces is that tree here's the results um this is me just going through the results and basically adding the result text to this list of uh texts um oh okay so here's what I do this Leaf text is all the raw documents and I'm appending to that all the summaries that's all it's going on and then I'm indexing them all together that's the key Point rag chain and there you have it that's really all you do um so anyway I encourage you to look at this in depth it's a pretty interesting technique it works well long with long contexts so for example one of the arguments I made is that it's kind of a nice approach to consult information across like a span of large documents like in this particular case my individual documents were lch expression language docs uh each each being somewhere in the order of you know in this case like you know most of them are less than 4,000 tokens some pretty big but I index them all I cluster them without any splits uh embed them cluster them build this tree um and go from there and it all works because we now have llms that can go out to you know 100 or 200,000 up to million tokens and Contex so you can actually just do this process for big swats of documents in place without any without any splitting uh it's a pretty nice approach so I encourage you to think about it look at it watch the deep that video If you really want to go deeper on this um thanks hi this is Lance from Lang chain this is the 14th part of our rag from scratch series we're going to I'm going to be talking about an approach called cold bear um so we've talked about a few different approaches for indexing and just as kind of a refresher indexing Falls uh kind of right down here in our flow we started initially with career translation taking a question translating it in some way to optimize retrieval we talked about routing it to a particular database we then talked about query construction so going from natural language to the DSL or domain specific language for E any of the databases that you want to work with those are you know metadata filters for Vector stores or Cipher for graph DB or SQL for relational DB so that's kind of the flow we talked about today we talked about some indexing approaches like multirepresentation indexing we gave a small shout out to greet camer in the series on chunking uh we talked about hierarchical indexing and I want to include one Advanced kind embedding approach so we talked a lot about embeddings are obviously very Central to semantic similarity search um and retrieval so one of the interesting points that's been brought up is that embedding models of course take a document you can see here on the top and embed it basically compress it to a vector so it's kind of a compression process you representing all the semantics of that document in a single Vector you're doing the same to your question you're doing similarity search between the question embedding and the document embedding um in order to perform retrieval you're typically taking the you know K most similar um document abetting is given a question and that's really how you're doing it now a lot of people said well hey the compressing a full document with all this Nuance to single Vector seems a little bit um overly restrictive right and this is a fair question to ask um there's been some interesting approaches to try to address that and one is this this this approach method called Co bear so the intuition is actually pretty straightforward there's a bunch of good articles I link down here this is my little cartoon to explain it which I think is hopefully kind of helpful but here's the main idea instead of just taking a document and compressing it down to a single Vector basically single uh what we might call embedding Vector we take the document we break it up into tokens so tokens are just like you know units of of content it depends on the token areas you use we talked about this earlier so you basically tokenize it and you produce basically an embedding or vector for every token and there's some kind of positional uh waiting that occurs when you do this process so you obviously you look to look at the implementation understand the details but the intuition is that you're producing some kind of representation for every token okay and you're doing the same thing for your question so you're taking your question you're breaking into a tokens and you have some representation or vector per token and then what you're doing is for every token in the question you're Computing the similarity across all the tokens in the document and you're finding the max you're taking the max you're storing that and you're doing that process for all the tokens in the question so again token two you compare it to every token in the in the document compute the Max and then the final score is in this case the sum of the max similarities uh between every question token and any document token so it's an interesting approach uh it reports very strong performance latency is definitely a question um so kind of production Readiness is something you should look into but it's a it's an approach that's worth mentioning here uh because it's pretty interesting um and let's walk through the code so there's actually nice Library called rouille which makes it very easy to play with Co bear um she's pip install it here I've already done that and we can use one of their pretrain models to mediate this process so I'm basically following their documentation this is kind of what they recommended um so I'm running this now hopefully this runs somewhat quickly I'm not sure I I previously have loaded this model so hopefully it won't take too long and yeah you can see it's pretty quick uh I'm on a Mac M2 with 32 gigs um so just as like a context in terms of my my system um this is from their documentation we're just grabbing a Wikipedia page this is getting a full document on Miyazaki so that's cool we're going to grab that now this is just from their docs this is basically how we create an index so we provide the you know some index name the collection um the max document length and yeah you should look at their documentation for these flags these are just the defaults so I'm going to create my index um so I get some logging here so it it's working under the hood um and by the way I actually have their documentation open so you can kind of follow along um so um let's see yeah right about here so you can kind of follow this indexing process to create an index you need to load a train uh a trained model this can be either your own pretrain model or one of ours from The Hub um and this is kind of the process we're doing right now create index is just a few lines of code and this is exactly what we're doing um so this is the you know my documents and this is the indexing step that we just we just kind of walk through and it looks like it's done um so you get a bunch of logging here that's fine um now let's actually see if this works so we're going to run drag search what an emotion Studio did Miaki found set our K parameter and we get some results okay so it's running and cool we get some documents out so you know it seems to work now what's nice is you can run this within lighting chain as a liting chain retriever so that basically wraps this as a lighting chain Retriever and then you can use it freely as a retriever within Lang chain it works with all the other different LMS and all the other components like rankers and so forth that we talk through so you can use this directly as a retriever let's try this out and boom nice and fast um and we get our documents again this is a super simple test example you should run this maybe on more complex cases but it's pretty pretty easy spin up it's a really interesting alternative indexing approach um using again like we talked through um a very different algorithm for computing do similarity that may work better I think an interesting regime to consider this would be longer documents so if you want like longer um yeah if if you basically want kind of long context embedding I think you should look into for example the uh Max token limits for this approach because it partitions the document into into each token um I would be curious to dig into kind of what the overall context limits are for this approach of coar but it's really interesting to consider and it reports very strong performance so again I encourage you to play with it and this is just kind of an intro to how to get set up and to start experimenting with it really quickly thanks hi this is Lance from Lang chain I'm going to be talking about using langra to build a diverse and sophisticated rag flows so just to set the stage the basic rag flow you can see here starts with a question retrieval of relevant documents from an index which are passed into the context window of an llm for generation of an answer ground in your documents that's kind of the basic outline and we can see it's like a very linear path um in practice though you often encounter a few different types of questions like when do we actually want to retrieve based upon the context of the question um are the retrieve documents actually good or not and if they're not good should we discard them and then how do we loot back and retry retrieval with for example and improved question so these types of questions motivate an idea of active rag which is a process where an llm actually decides when and where to retrieve based upon like existing retrievals or existing Generations now when you think about this there's a few different levels of control that you have over an llm in a rag application the base case like we saw with our chain is just use an llm to choose a single steps output so for example in traditional rag you feed it documents and it decides to generation so it's just kind of one step now a lot of rag workflows will use the idea of routing so like given a question should I route it to a vector store or a graph DB um and we have seen this quite a bit now this newer idea that I want to introduce is how do we build more sophisticated logical flows um in a rag pipeline um that you let the llm choose between different steps but specify all the transitions that are available and this is known as we call a state machine now there's a few different architectures that have emerged uh to build different types of rag chains and of course chains are traditionally used just for like very basic rag but this notion of State machine is a bit newer and Lang graph which we recently released provides a really nice way to build State machines for Rag and for other things and the general idea here is that you can lay out more diverse and complicated rag flows and then Implement them as graphs and it kind of motivates this more broad idea of of like flow engineering and thinking through the actual like workflow that you want and then implementing it um and we're gonna actually do that right now so I'm GNA Pi a recent paper called CAG corrective rag which is really a nice method um for active rag that incorporates a few different ideas um so first you retrieve documents and then you grade them now if at least one document exceeds the threshold for relevance you go to generation you generate your answer um and it does this knowledge refinement stage after that but let's not worry about that for right now it's kind of not essential for understanding the basic flow here so again you do a grade for relevance for every document if any is relevant you generate now if they're all ambiguous or incorrect based upon your grader you retrieve from an external Source they use web search and then they pass that as their context for answer generation so it's a really neat workflow where you're doing retrieval just like with basic rag but then you're reasoning about the documents if they're relevant go ahead and at least one is relevant go ahead and generate if they're not retrieve from alternative source and then pack that into the context and generate your answer so let's see how we would implement this as a estate machine using Lang graph um we'll make a few simplifications um we're going to first decide if any documents are relevant we'll go ahead and do the the web search um to supplement the output so that's just like kind of one minor modification um we'll use tab search for web search um we use Query writing to optimize the search for uh to optimize the web search but it follows a lot of the the intuitions of the main paper uh small note here we set the Tav API key and another small mode I've already set my lsmith API key um with which we'll see is useful a bit later for observing the resulting traces now I'm going to index three blog posts that I like um I'm going to use chroma DB I'm G use open ey embeddings I'm going to run this right now this will create a vector store for me from these three blog posts and then what I'm going to do is Define State now this is kind of the core object that going to be passed around my graph that I'm going to modify and right here is where I Define it and the key point to note right now is it's just a dictionary and it can contain things that are relevant for rag like question documents generation and we'll see how we update that in in in a little bit but the first thing to note is we Define our state and this is what's going to be modified in every Noe of our graph now here's really the Crux of it and this is the thing I want to zoom in on a little bit um so when you kind of move from just thinking about promps to thinking about overall flows it it's like kind of a fun and interesting exercise I kind of think about this as it's been mentioned on Twitter a little bit more like flow engineering so let's think through what was actually done in the paper and what modifications to our state are going to happen in each stage so we start with a question you can see that on the far left and this kind of state is represent as a dictionary like we have we start with a question we perform retrieval from our Vector store which we just created that's going to give us documents so that's one node we made an an adjustment to our state by adding documents that's step one now we have a second node where we're going to grade the documents and in this node we might filter some out so we are making a modification to state which is why it's a node so we're going to have a greater then we're going to have what we're going to call a conditional Edge so we saw we went from question to retrieval retrieval always goes to grading and now we have a decision if any document is irrelevant we're going to go ahead and do web search to supplement and if they're all relevant will go to generation it's a minor kind of a minor kind of logical uh decision ision that we're going to make um if any are not relevant we'll transform the query and we'll do web search and we'll use that for Generation so that's really it and that's how we can kind of think about our flow and how our States can be modified throughout this flow now all we then need to do and I I kind of found spending 10 minutes thinking carefully through your flow engineering is really valuable because from here it's really implementation details um and it's pretty easy as you'll see so basically I'm going to run this code block but then we can like walk through some of it I won't show you everything so it'll get a little bit boring but really all we're doing is we're finding functions for every node that take in the state and modify in some way that's all it's going on so think about retrieval we run retrieval we take in state remember it's a dict we get our state dick like this we extract one keyy question from our dick we pass that to a retriever we get documents and we write back out State now with documents key added that's all generate going to be similar we take in state now we have our question and documents we pull in a prompt we Define an llm we do minor post processing on documents we set up a chain for retrieval uh or sorry for Generation which is just going to be take our prompt pump Plum that to an llm partially output a string and we run it right here invoking our documents in our question to get our answer we write that back to State that's it and you can kind of follow here for every node we just Define a function that performs the state modification that we want to do on that node grading documents is going to be the same um in this case I do a little thing extra here because I actually Define a identic data model for my grader so that the output of that particular grading chain is a binary yes or no you can look at the code make sure it's all shared um and that just makes sure that our output is is very deterministic so that we then can down here perform logical filtering so what you can see here is um we Define this search value no and we iterate through our documents we grade them if any document uh is graded as not relevant we flag this search thing to yes that means we're going to perform web search we then add that to our state dict at the end so run web search now that value is true that's it and you can kind of see we go through some other nodes here there's web search node um now here is where our one conditional Edge we Define right here this is where where we decide to generate or not based on that search key so we again get our state let's extract the various values so we have this search value now if search is yes we return the next no that we want to go to so in this case it'll be transform query which will then go to web search else we go to generate so what we can see is we laid out our graph which you can kind of see up here and now we Define functions for all those nodes as well as the conditional Edge and now we scroll down all we have to do is just lay that out here again as our flow and this is kind of what you might think of as like kind of flow engineering where you're just laying out the graph as you drew it where we have set our entry point as retrieve we're adding an edge between retrieve and grade documents so we went retrieval grade documents we add our conditional Edge depending on the grade either transform the query go to web search or just go to generate we create an edge between transform the query and web search then web search to generate and then we also have an edge generate to end and that's our whole graph that's it so we can just run this and now I'm going to ask a question so let's just say um how does agent memory work for example let's just try that and what this is going to do is going to print out what's going on as we run through this graph so um first we going to see output from retrieve this is going to be all of our documents that we retrieved so that's that's fine this just from our our retriever then you can see that we're doing a relevance check across our documents and this is kind of interesting right you can see we grading them here one is grade as not relevant um and okay you can see the documents are now filtered because we removed the one that's not relevant and because one is not relevant we decide okay we're going to just transform the query and run web search and um you can see after query transformation we rewrite the question slightly we then run web search um and you can see from web search it searched from some additional sources um which you can actually see here it's appended as a so here it is so here it's a new document appended from web search which is from memory knowledge requirements so it it basically looked up some AI architecture related to memory uh web results so that's fine that's exactly what we want to do and then um we generate a response so that's great and this is just showing you everything in kind of gory detail but I'm going to show you one other thing that's that's really nice about this if I go to lsmith I have my AP I ke set so all my Generations are just logged to to lsmith and I can see my Lang graph run here now what's really cool is this shows me all of my nodes so remember we had retrieve grade we evaluated the grade because one was irrelevant we then went ahead and transformed the query we did a web search we pended that to our context you can see all those steps are laid out here in fact you can even look at every single uh grader and its output I will move this up slightly um so you can see the the different scores for grades okay so this particular retrieval was graded as as not relevant so that's fine that that can happen in some cases and because of that um we did a query transformation so we modified the question slightly how does memory how does the memory system an artificial agents function so it's just a minor rephrasing of the question we did this Tav web search this is where it queried from this particular blog post from medium so it's like a sing web query we can like sanity check it and then what's need is we can go to our generate step look at open Ai and here's our full prompt how does the memory system in our official agents function and then here's all of our documents so this is the this is the web search as well as we still have the Rel chunks that were retrieved from our blog posts um and then here's our answer so that's really it you can see how um really moving from the notion of just like I'll actually go back to the original um moving from uh I will try to open this up a little bit um yeah I can see my face still um the transition from laying out simple chains to flows is a really interesting and helpful way of thinking about why graphs are really interesting because you can encode more sophisticated logical reasoning workflows but in a very like clean and wellengineered way where you can specify all the transitions that you actually want to have executed um and I actually find this way of thinking and building kind of logical uh like workflows really intuitive um we have a blog post coming out uh tomorrow that discusses both implementing self rag as well as C rag for two different active rag approaches using using uh this idea of of State machines and Lang graph um so I encourage you to play with it uh I found it really uh intuitive to work with um I also found uh inspection of traces to be quite intuitive using Lang graph because every node is enumerated pretty clearly for you which is not always the case when you're using other types of of more complex reasoning approaches for example like agents so in any case um I hope this was helpful and I definitely encourage you to check out um kind of this notion of like flow engineering using Lang graph and in the context of rag it can be really powerful hopefully as you've seen here thank you hey this is Lance from Lang chain I want to talk to a recent paper that I saw called adaptive rag which brings together some interesting ideas that have kind of been covered in other videos but this actually ties them all together in kind of a fun way so the the two big ideas to talk about here are one of query analysis so we've actually done kind of a whole rag from scratch series that walks through each of these things in detail but this is a very nice example of how this comes together um with some other ideas we've been talking about so query analysis is typically the process of taking an input question and modifying in some way uh to better optimize retrieval there's a bunch of different methods for this it could be decomposing it into sub questions it could be using some clever techniques like stepb back prompting um but that's kind of like the first stage of query analysis then typically you can do routing so you route a question to one of multiple potential sources it could be one or two different Vector stores it could be relational DB versus Vector store it could be web search it could just be like an llm fallback right so this is like one kind of big idea query analysis right it's kind of like the front end of your rag pipeline it's taking your question it's modifying it in some way it's sending it to the right place be it a web search be it a vector store be it a relational DB so that's kind of topic one now topic two is something that's been brought up in a few other videos um of what I kind of call Flow engineering or adaptive rag which is the idea of doing tests in your rag pipeline or in your rag inference flow uh to do things like check relevance documents um check whether or not the answer contains hallucinations so this recent blog post from Hamil Hussein actually covers evaluation in in some really nice detail and one of the things he highlighted explicitly is actually this topic so he talks about unit tests and in particular he says something really interesting here he says you know unlike typical unit tests you want to organize these assertions in places Beyond typical unit testing such as data cleaning and here's the key Point automatic retries during model inference that's the key thing I want to like draw your attention to to it's a really nice approach we've talked about some other papers that do that like corrective rag self rag but it's also cool to see it here and kind of encapsulated in this way the main idea is that you're using kind of unit tests in your flow to make Corrections like if your retrieval is bad you can correct from that if your generation has hallucinations you can correct from that so I'm going to kind of draw out like a cartoon diagram of what we're going to do here and you can kind of see it here we're starting with a question we talked about query analysis we're going to take our question and we're going to decide where it needs to go and for this particular toy example I'm going to say either send it to a vector store send it to web search or just have the llm answer it right so that's like kind of my fallback Behavior then we're going to bring in that idea of kind of online flow engineering or unit testing where I'm going to have my retrieval either from the VOR store or web search I'm then going to ask is this actually relevant to the question if it isn't I'm actually going to kick back to web sech so this is a little bit more relevant in the case if I've routed to to the vector store done retrieval documents aren't relevant I'll have a fallback mechanism um then I'm going to generate I check for hallucinations in my generation and then I check for um for whether or not the the generation actually answers the question then I return my answer so again we're tying together two ideas one is query analysis like basically taking a question routing it to the right place modifying it as needed and then kind of online unit testing and iterative flow feedback so to do this I've actually heard a lot of people talk online about command r a new model release from gooh here it has some pretty nice properties that I was kind of reading about recently so one it has nice support for Tool use and it does support query writing in the context of tool use uh so this all rolls up in really nice capabilities for routing it's kind of one now two it's small it's 35 billion parameter uh it's actually open weight so you can actually run this locally and I've tried that we can we can talk about that later uh so and it's also fast served via the API so it's kind of a small model and it's well tuned for rag so I heard a lot of people talking about using coher for Rag and it has a large context 120,000 tokens so this like a nice combination of properties it supports to and routing it's small and fast so it's like quick for grading and it's well tuned for rag so it's actually a really nice fit for this particular workflow where I want to do query analysis and routing and I want to do kind of online checking uh and rag so kind of there you go now let's just get to the coding bit so I have a notebook kind of like usual I've done a few pip installs you can see it's nothing exotic I'm bringing Lang chain coh here I set my coher API key now I'm just going to set this Lang chain project within lsmith so all my traces for this go to that project and I have enabled tracing so I'm using Langs Smith here so we're going to walk through this flow and let's do the first thing let's just build a vector store so I'm going to build a vector store using coherent beddings with chroma open source Vector DB runs locally from three different web pages on blog post that I like so it pertains to agents prompt engineering and adversarial attacks so now I have a retriever I can run retriever invoke and I can ask a question about you know agent memory agent memory and there we go so we get documents back so there we go we have a retriever now now here's where I'm going to bring in coh here I also want a router so you look at our flow the first step is this routing stage right so what I'm going to do is I'm guess we going to find two tools a web search tool and a vector store tool okay in my Preamble is just going to say you're an expert routing user questions to Vector store web search now here's the key I tell it what the vector store has so again my index my Vector store has agents prompt engineering adial tax I just repeat that here agents prompt adversarial tax so it knows what's in the vector store um so use it for questions on these topics otherwise use web search so that's it I use command R here now I'm going to bind these tools to the model and attach the Preamble and I have a structured LM router so let's give it a let's give this a few tests just to like kind of sandbox this a little bit so I can inval here's my chain I have a router prompt I pass that to the structured LM router which I defined right here and um let's ask a few different questions like who will the Bears draft in the NFL draft with types of agent memory and Hi how are you so I'm going to kick that off and you can see you know it does web search it does it goes to Vector store and then actually returns this false so that's kind of interesting um this is actually just saying if it does not use either tool so for that particular query web search or the vector store was inappropriate it'll just say hey I didn't call one of those tools so that's interesting we'll use that later so that's my router tool now the second thing is my grader and here's where I want to show you something really nice that is generally useful uh for many different problems you might encounter so here's what I'm doing I'm defining a data model uh for My Grade so basically grade documents it's going to have this is a pantic object it is just basically a binary score here um field specified here uh documents are relevant to the question yes no I have a preamble your grer assessing relevance of retrieve documents to a user question um blah blah blah so you know and then basically give it a b score yes no I'm using command R but here's the catch I'm using this wi structured outputs thing and I'm passing my grade documents uh data model to that that so this is the key thing we can test this right now as well it's going to return an object based on the schema I give it which is extremely useful for all sorts of use cases and let's actually Zoom back up so we're actually right here so this greater stage right I want to constrain the output to yes no I don't want any preambles I want anything because the logic I'm going to build in this graph is going to require a yes no binary response from this particular Edge in our graph so that's why this greater tool is really useful and I'm asking like a mock question types of agent memory I do a retriever I do a retrieval from our Vector store I get the tuck and I test it um I invoke our greater retrieval grater chain with the question the doc text and it's relevant as we would expect so that's good but again let's just kind of look at that a little bit more closely what's actually happening under the hood here here's the pantic object we passed here's the document in question I'm providing basically it's converting this object into coher function schema it's binding that to the llm we pass in the document question it returns an object basic a Json string per our pantic schema that's it and then it's just going to like parse that into a pantic object which we get at the end of the day so that's what's happening under the hood with this with structured output thing but it's extremely useful and you'll see we're going to use that a few different places um um because we want to ensure that in our in our flow here we have three different grading steps and each time we want to constrain the output to yes no we're going to use that structured output more than once um this is just my generation so this is good Old Rag let's just make sure that works um I'm using rag chain typical rag prompt again I'm using cohere for rag pretty easy and yeah so the rag piece works that's totally fine nothing to it crazy there um I'm going to find this llm fallback so this is basically if you saw a router chain if it doesn't use a tool I want to fall back and just fall back to the llm so I'm going to kind of build that as a little chain here so okay this is just a fallback I have my Preamble just you're you're an assistant answer the question based upon your internal knowledge so again that fallback behavior is what we have here so what we've done already is we defined our router piece we've defined our our basic retrieval our Vector store we already have here um we've defined our first logic or like grade check and we defined our fallback and we're just kind of roll through the parts of our graph and Define each piece um so I'm going to have two other graders and they're going to use the same thing we just talked about slightly different data model I mean same output but actually just slightly different uh prompt um and you know descript destion this in this case is the aners grounded the facts yes no this is my hallucination grater uh and then I have an answer grader as well and I've also run a test on each one and you can see I'm getting binary this this these objects out have a binary score so this a pantic object with a binary score uh and that's exactly what we want cool and I have a Search tool so that's really nice we've actually gone through and we've kind of laid out I have like a router I've tested it we have a vector story tested we've tested each of our graders here we've also tested generation of just doing rag so we have a bunch of pieces built here we have a fallback piece we have web search now the question is how do I Stitch these together into this kind of flow and for that I I like to use Lang graph we'll talk a little about Lang graph versus agents a bit later but I want to show you why this is really easy to do using Lang graph so what's kind of nice is I've kind of laid out all my logic here we've tested individually and now all I'm going to do is I'm going to first lay out uh the parts of my graph so what you're going to notice here is first there's a graph state so this state represents kind of the key parts of the graph or the key kind of objects within the graph that we're going to be modifying so this is basically a graph centered around rag we're going to have question generation and documents that's really kind of the main things we're going to be working with in our graph so then you're going to see something that's pretty intuitive I think what you're going to see is we're going to basically walk through this flow and for each of these little circles we're just going to find a function and these uh little squares or these these you can think about every Circle as a node and every kind of diamond here as as an edge or conditional Edge so that's actually what we're going to do right now we're going to lay out all of our nodes and edges and each one of them are just going to be a function and you're going to see how we do that right now so I'm going to go down here I def find my graph state so this is what's going to be kind of modified and propagated throughout my graph now all I'm going to do is I'm just going to find a function uh for each of those nodes so let me kind of go side by side and show you the diagram and then like kind of show the nodes next to it so here's the diagram so we have uh a retrieve node so that kind of represents our Vector store we have a fallback node that's this piece we have a generate node so that's basically going to do our rag you can see there we have a grade documents node kind of right here um and we have a web search node so that's right here cool now here's where we're actually to find the edges so you can see our edges are the pieces of the graph that are kind of making different decisions so this route question Edge basic conditional Edge is basically going to take an input question and decide where it needs to go and that's all we're doing down here it kind of follows what we did up at the top where we tested this individually so recall we basically just invoke that question router returns our source now remember if tool calls were not in the source we do our fall back so we show actually showed that all the way up here remember this if tool calls is not in the response this thing will just be false so that means we didn't either we didn't call web search and we didn't call uh our retriever tool so then we're just going to fall back um yep right here and this is just like uh you know a catch just in case a tool could make a decision but most interestingly here's where we choose a data source basically so um this is the output of our tool call we're just going to fish out the name of the tool so that's data source and then here we go if the data source is web search I'm returning web search as basically the next node to go to um otherwise if it's Vector store we return Vector store as the next node to go to so what's this search thing well remember we right up here Define this node web search that's it we're just going to go to that node um what's this Vector store um you'll see below how we can kind of tie these strings that we returned from the conditional Edge to the node we want to go to that's really it um same kind of thing here decide to generate that's going to roll in these two conditional edges into one um and basically it's going to do if there's no documents so basic basically if we filtered out all of our documents from this first test here um then what we're going to do is we've decided all documents are not relevant to the question and we're going to kick back to web search exactly as we show here so that's this piece um otherwise we're going to go to generate so that's this piece so again in these conditional edges you're basically implementing the logic that you see in our diagram right here that's all that's going on um and again this is just implementing the final two checks uh for hallucinations and and answer relevance um and um yep so here's our hallucination grader we then extract the grade if the if basically there are hallucinations um oh sorry in this case the grade actually yes means that the answer is grounded so we say answer is actually grounded and then we go to the next step we go to the next test that's all this is doing it's just basically wrapping this logic that we're implementing here in our graph so that's all that's going on and let's go ahead and Define all those things so nice we have all that um now we can actually go down a little bit and we can pull um this is actually where we stitch together everything so all it's happening here is you see we defined all these functions up here we just add them as nodes in our graph here and then we build our graph here basically by by basically laying out the flow or the connectivity between our nodes and edges so you know you can look at this notebook to kind of study in a bit of detail what's going on but frankly what I like to do here typically just draw out a graph kind of like we did up here and then Implement uh the Lo logical flow here in your graph as nodes and edges just like we're doing here that's all that's happening uh so again we have like our entry point is the router um this is like the output is this is basically directing like here's what the router is outputting and here's the next node to go to so that's it um and then for each node we're kind of applying like we're saying like what's what's the flow so web search goes to generate after um and retrieve goes to grade documents grade documents um kind of is is like is a conditional Edge um depending on the results we either do web search or generate and then our second one we go from generate to uh basically this grade uh generation versus documents in question based on the output of that we either have hallucinations we regenerate uh we found that the answer is not useful we kick back to web search or we end um finally we have that llm fallback and that's also if we go to the fallback we end so what you're seeing here is actually the the logic flow we're laying out in this graph matches the diagram that we laid out up top I'm just going to copy these over and I'll actually go then back to the diagram and and kind of underscore that a little bit more so here is the flow we've laid out again here is our diagram and you can kind of look at them side by side and see how they basically match up so here's kind of our flow diagram going from basically query analysis that's this thing this route question and you can see web search Vector store LM fallback LM fallback web search vector store so those are like the three options that can come out of this conditional Edge and then here's where we connect so if we go to web search then basically we next go to generate so that's kind of this whole flow um now if we go to retrieve um then we're going to grade so that's it um and you know it follows kind of as you can see here that's really it uh so it's just nice to draw the these diagrams out first and then it's pretty quick to implement each node and each Edge just as a function and then stitch them together in a graph just like I show here and of course we'll make sure this code's publ so you can use it as a reference um so there we go now let's try a few a few different test questions so like what player the Bears to draft and NFL draft right let's have a look at that and they should print everything it's doing as we go so okay this is important route question it just decides to route to web search that's good it doesn't go to our Vector store this is a current event not related to our Vector store at all it goes to web search um and then it goes to generate so that's what we'd expect so basically web search goes through to generate um and we check hallucinations Generations ground the documents we check generation versus question the generation addresses the question the Chicago Bears expected to draft Caleb Williams that's right that's that's the consensus so cool that works now let's ask a question related to our Vector store what are the types of agent memory we'll kick this off so we're routing okay we're routing to rag now look how fast this is that's really fast so we basically whip through that document grading determine they're all relevant uh we go to decision to generate um we check hallucinations we check answer versus question and there are several types of memory stored in the human brain memory can also be stored in G of Agents you have LM agents memory stream retrieval model and and reflection mechanism so it's representing what's captured on the blog post pretty reasonably now let me show you something else is kind of nice I can go to Langs Smith and I can go to my projects we create this new project coher adaptive rag at the start and everything is actually logged there everything we just did so I can open this up and I can actually just kind of look through all the stages of my Lang graph to here's my retrieval stage um here's my grade document stage and we can kind of audit the grading itself we kind of looked at this one by one previously but it's actually pretty nice we can actually audit every single individual document grade to see what's happening um we can basically go through um to this is going to be one of the other graders here um yep so this is actually going to be the hallucination grading right here uh and then this is going to be the answer grading right here so that's really it you can kind of walk through the entire graph you can you can kind of study what's going on um which is actually very useful so it looks like this worked pretty well um and finally let's just ask a question that should go to that fallback uh path down at the bottom like not related at all to our Vector store current events and yeah hello I'm doing well so it's pretty neat we've seen in maybe 15 minutes we've from scratch built basically a semi sophisticated rag system that has agentic properties we've done in Lang graph we've done with coher uh command R you can see it's pretty darn fast in fact we can go to Langs Smith and look at so this whole thing took 7 seconds uh that is not bad let's look at the most recent one so this takes one second so the fallback mechanism to the LM is like 1 second um the let's just look here so 6 seconds for the initial uh land graph so this is not bad at all it's quite fast it done it does quite a few different checks we do routing uh and then we have kind of a bunch of nice fallback behavior and inline checking uh for both relevance hallucinations and and answer uh kind of groundedness or answer usefulness so you know this is pretty nice I definitely encourage you to play with a notebook command R is a really nice option for this due to the fact that is tool use routing uh small and quite fast and it's really good for Rags it's a very nice kind of uh a very nice option for workflows like this and I think you're going to see more and more of this kind of like uh adaptive or selfreflective rag um just because this is something that a lot systems can benefit from like a a lot of production rack systems kind of don't necessarily have fallbacks uh depending on for example like um you know if the documents retrieved are not relevant uh if the answer contains hallucinations and so forth so this opportunity to apply inline checking along with rag is like a really nice theme I think we're going to see more and more of especially as model inference gets faster and faster and these checks get cheaper and cheaper to do kind of in the inference Loop now as a final thing I do want to bring up the a point about you know we've shown this Lang graph stuff what about agents you know how do you think about agents versus Lang graph right and and I think the way I like to frame this is that um Lang graph is really good for um flows that you have kind of very clearly defined that don't have like kind of openendedness but like in this case we know the steps we want to take every time we want to do um basically query analysis routing and then we want to do a three grading steps and that's it um Lang graph is really good for building very reliable flows uh it's kind of like putting an agent on guard rails and it's really nice uh it's less flexible but highly reliable and so you can actually use smaller faster models with langra so that's the thing I like about we saw here command R 35 billion parameter model works really well with langra quite quick we' were able to implement a pretty sophisticated rag flow really quickly 15 minutes um in time is on the order of like less than you know around 5 to 6 seconds so so pretty good right now what about agents right so I think Agents come into play when you want more flexible workflows you don't want to necessarily follow a defined pattern a priori you want an agent to be able to kind of reason and make of openend decisions which is interesting for certain like long Horizon planning problems you know agents are really interesting the catch is that reliability is a bit worse with agents and so you know that's a big question a lot of people bring up and that's kind of where larger LMS kind of come into play with a you know there's been a lot of questions about using small LMS even open source models with agents and reliabilities kind of continuously being an issue whereas I've been able to run these types of land graphs with um with uh like mraw or you know command R actually is open weights you can run it locally um I've been able to run them very reproducibly with open source models on my laptop um so you know I think there's a tradeoff and Comm actually there's a new coher model coming out uh believe command R plus which uh is a larger model so it's probably more suitable for kind of more openended agentic use cases and there's actually a new integration with Lang chain that support uh coher agents um which is quite nice so I think it's it's worth experimenting for certain problems in workflows you may need more openended reasoning in which case use an agent with a larger model otherwise you can use like Lang graph for more uh a more reliable potential but con strain flow and it can also use smaller models faster LMS so those are some of the tradeoffs to keep in mind but anyway encourage you play with a notebook explore for yourself I think command R is a really nice model um I've also been experimenting with running it locally with AMA uh currently the quantise model is like uh two bit quantise is like 13 billion uh or so uh yeah 13 gigs it's it's a little bit too large to run quickly locally for me um inference for things like rag we're on the order of 30 seconds so again it's not great for a live demo but it does work it is available on a llama so I encourage you to play with that I have a Mac M2 32 gig um so you know if I if you're a larger machine then it absolutely could be worth working with locally so encourage you to play with that anyway hopefully this was useful and interesting I think this is a cool paper cool flow um coher command R is a nice option for these types of like routing uh it's quick good with Lang graph good for rag good for Tool use so you know have a have a look and uh you know reply anything uh any feedback in the comments thanks hi this is Lance from Lang chain this is a talk I gave at two recent meetups in San Francisco called is rag really dead um and I figured since you know a lot of people actually weren't able to make those meetups uh I just record this and put this on YouTube and see if this is of interest to folks um so we all kind of recognize that Contex windows are getting larger for llms so on the xaxis you can see the tokens used in pretraining that's of course you know getting larger as well um proprietary models are somewhere over the two trillion token regime we don't quite know where they sit uh and we've all the way down to smaller models like 52 trained on far fewer tokens um but what's really notable is on the y axis you can see about a year ago da the art models were on the order of 4,000 to 8,000 tokens and that's you know dozens of pages um we saw Claude 2 come out with the 200,000 token model earlier I think it was last year um gbd4 128,000 tokens now that's hundreds of pages and now we're seeing Claud 3 and Gemini come out with million token models so this is hundreds to thousands of pages so because of this phenomenon people have been kind of wondering is rag dead if you can stuff you know many thousands of pages into the context window llm why do you need a reteval system um it's a good question spoke sparked a lot of interesting debate on Twitter um and it's maybe first just kind of grounding on what is rag so rag is really the process of reasoning and retrieval over chunks of of information that have been retrieved um it's starting with you know documents that are indexed um they're retrievable through some mechanism typically some kind of semantic similarity search or keyword search other mechanisms retriev docs should then pass to an llm and the llm reasons about them to ground response to the question in the retrieve document so that's kind of the overall flow but the important point to make is that typically it's multiple documents and involve some form of reasoning so one of the questions I asked recently is you know if long condex llms can replace rag it should be able to perform you know multia retrieval and reasoning from its own context really effectively so I teamed up with Greg Cameron uh to kind of pressure test this and he had done some really nice needle the Haack analyses already focused on kind of single facts called needles placed in a Hy stack of Paul Graham essays um so I kind of extended that to kind of mirror the rag use case or kind of the rag context uh where I took multiple facts so I call it multi needle um I buil on a funny needle in the HTO challenge published by anthropic where they add they basically placed Pizza ingredients in the context uh and asked the LM to retrieve this combination of pizza ingredients I did I kind of Rift on that and I basically split the pizza ingredients up into three different needles and place those three ingredients in different places in the context and then ask the um to recover those three ingredients um from the context so again the setup is the question is what the secret ingredients need to build a perfect Pizza the needles are the ingredients figs Pudo goat cheese um I place them in the context at some specified intervals the way this test works is you can basically set the percent of context you want to place the first needle and the remaining two are placed at roughly equal intervals in the remaining context after the first so that's kind of the way the test is set up now it's all open source by the way the link is below so needs are placed um you ask a question you promp L them with with kind of um with this context and the question and then produces the answer and now the the framework will grade the response both one are you know all are all the the specified ingredients present in the answer and two if not which ones are missing so I ran a bunch of analysis on this with GPD 4 and came kind of came up with some with some fun results um so you can see on the left here what this is looking at is different numbers of needles placed in 120,000 token context window for gbd4 and I'm asking um gbd4 to retrieve either one three or 10 needles now I'm also asking it to do reasoning on those needles that's what you can see in those red bars so green is just retrieve the ingredients red is reasoning and the reasoning challenge here is just return the first letter of each ingredient so we find is basically two things the performance or the percentage of needles retrieved drops with respect to the number of needles that's kind of intuitive you place more facts performance gets worse but also it gets worse if you ask it to reason so if you say um just return the needles it does a little bit better than if you say return the needles and tell me the first letter so you overlay reasoning so this is the first observation more facts is harder uh and reasoning is harder uh than just retrieval now the second question we ask is where are these needles actually present in the context that we're missing right so we know for example um retrieval of um 10 needles is around 60% so where are the missing needles in the context so on the right you can see results telling us actually which specific needles uh are are the model fails to retrieve so what we can see is as you go from a th000 tokens up to 120,000 tokens on the X here and you look at needle one place at the start of the document to needle 10 placed at the end at a th000 token context link you can retrieve them all so again kind of match what we see over here small well actually sorry over here everything I'm looking at is 120,000 tokens so that's really not the point uh the point is actually smaller context uh better retrieval so that's kind of point one um as I increase the context window I actually see that uh there is increased failure to retrieve needles which you see can see in red here towards the start of the document um and so this is an interesting result um and it actually matches what Greg saw with single needle case as well so the way to think about it is it appears that um you know if you for example read a book and I asked you a question about the first chapter you might have forgotten it same kind of phenomenon appears to happen here with retrieval where needles towards the start of the context are are kind of Forgotten or are not well retrieved relative to those of the end so this is an effect we see with gbd4 it's been reproduced quite a bit so ran nine different trials here Greg's also seen this repeatedly with single needle so it seems like a pretty consistent result and there's an interesting point I put this on Twitter and a number of folks um you know replied and someone sent me this paper which is pretty interesting and it mentions recency bias is one possible reason so the most informative tokens for predicting the next token uh you know are are are present close to or recent to kind of where you're doing your generation and so there's a bias to attend to recent tokens which is obviously not great for the retrieval problem as we saw here so again the results show us that um reasoning is a bit harder than retrieval more needles is more difficult and needles towards the start of your context are harder to retrieve than towards the end those are three main observations from this and it maybe indeed due to this recency bias so overall what this kind of tells you is be wary of just context stuffing in large long context there are no retrieval guarantees and also there's some recent results that came out actually just today suggesting that single needle may be misleadingly easy um you know there's no reasoning it's retrieving a single needle um and also these guys I'm I showed this tweet here show that um the in a lot of these needle and Haack challenges including mine the facts that we look for are very different than um the background kind of Hy stack of Paul Graham essays and so that may be kind of an interesting artifact they note that indeed if the needle is more subtle retrievals is worse so I think basically when you see these really strong performing needle and hyack analyses put up by model providers you should be skeptical um you shouldn't necessarily assume that you're going to get high quality retrieval from these long contact LMS uh for numerous reasons you need to think about retrieval of multiple facts um you need to think about reasoning on top of retrieval you need need to think about the subtlety of the retrieval relative to the background context because for many of these needle and the Haack challenges it's a single needle no reasoning and the needle itself is very different from the background so anyway those may all make the challenge a bit easier than a real world scenario of fact retrieval so I just want to like kind of lay out that those cautionary notes but you know I think it is fair to say this will certainly get better and I think it's also fair to say that rag will change and this is just like a nearly not a great joke but Frank zap a musician made the point Jazz isn't dead it just smells funny you know I think same for rag rag is not dead but it will change I think that's like kind of the key Point here um so just as a followup on that rag today's focus on precise retrieval of relevant doc chunks so it's very focused on typically taking documents chunking them in some particular way often using very OS syncratic chunking methods things like chunk size are kind of picked almost arbitrarily embeding them storing them in an index taking a question embedding it doing K&N uh similarity search to retrieve relevant chunks you're often setting a k parameter which is the number of chunks you retrieve you often will do some kind of filtering or Pro processing on the retrieve chunks and then ground your answer in those retrieved chunks so it's very focused on precise retrieval of just the right chunks now in a world where you have very long context models I think there's a fair question to ask is is this really kind of the most most reasonable approach so kind of on the left here you can kind of see this notion closer to today of I need the exact relevant chunk you can risk over engineering you can have you know higher complexity sensitivity to these odd parameters like chunk size k um and you can indeed suffer lower recall because you're really only picking very precise chunks you're beholden to very particular embedding models so you know I think going forward as long context models get better and better there are definitely question you should certainly question the current kind of very precise chunking rag Paradigm but on the flip side I think just throwing all your docs into context probably will also not be the preferred approach you'll suffer higher latency higher token usage I should note that today 100,000 token GPD 4 is like $1 per generation I spent a lot of money on Lang Chain's account uh on that multile analysis I don't want to tell Harrison how much I spent uh so it's it's you know it's not good right um You Can't audit retrieve um and security and and authentication are issues if for example you need different users different different access to certain kind of retriev documents or chunks in the Contex stuffing case you you kind of can't manage security as easily so there's probably some predo optimal regime kind of here in the middle and um you know I I put this out on Twitter I think there's some reasonable points raised I think you know this inclusion at the document level is probably pretty sane documents are selfcontained chunks of context um so you know what about document Centric rag so no chunking uh but just like operate on the context of full documents so you know if you think forward to the rag Paradigm that's document Centric you still have the problem of taking an input question routing it to the right document um this doesn't change so I think a lot of methods that we think about for kind of query analysis um taking an input question rewriting it in a certain way to optimize retrieval things like routing taking a question routing to the right database be it a relational database graph database Vector store um and quer construction methods so for example text to SQL text to Cipher for graphs um or text to even like metadata filters for for Vector stores those are all still relevant in the world that you have long Contex llms um you're probably not going to dump your entire SQL DB and feed that to the llm you're still going to have SQL queries you're still going to have graph queries um you may be more permissive with what you extract but it still is very reasonable to store the majority of your structured data in these in these forms likewise with unstructured data like documents like we said before it still probably makes sense to ENC to you know store documents independently but just simply aim to retrieve full documents rather than worrying about these idiosyncratic parameters like like chunk size um and along those lines there's a lot of methods out there we've we've done a few of these that are kind of well optimized for document retrieval so one I want a flag is what we call multi repesent presentation indexing and there's actually a really nice paper on this called dense X retriever or proposition indexing but the main point is simply this would you do is you take your OD document you produce a representation like a summary of that document you index that summary right and then um at retrieval time you ask your question you embed your question and you simply use a highle summary to just retrieve the right document you pass the full document to the LM for a kind of final generation so it's kind of a trick where you don't have to worry about embedding full documents in this particular case you can use kind of very nice descriptive summarization prompts to build descriptive summaries and the problem you're solving here is just get me the right document it's an easier problem than get me the right chunk so this is kind of a nice approach it there's also different variants of it which I share below one is called parent document retriever where you could use in principle if you wanted smaller chunks but then just return full documents but anyway the point is preserving full documents for Generation but using representations like summaries or chunks for retrieval so that's kind of like approach one that I think is really interesting approach two is this idea of raptor is a cool paper came out of Stamper somewhat recently and this solves the problem of what if for certain questions I need to integrate information across many documents so what this approach does is it takes documents and it it embeds them and clusters them and then it summarizes each cluster um and it does this recursively in up with only one very high level summary for the entire Corpus of documents and what they do is they take this kind of this abstraction hierarchy so to speak of different document summarizations and they just index all of it and they use this in retrieval and so basically if you have a question that draws an information across numerous documents you probably have a summary present and and indexed that kind of has that answer captured so it's a nice trick to consolidate information across documents um they they paper actually reports you know these documents in their case or the leavs are actually document chunks or slices but I actually showed I have a video on it and a notebook that this works across full documents as well um and this and I segue into to do this you do need to think about long context embedding models because you're embedding full documents and that's a really interesting thing to track um the you know hazy research uh put out a really nice um uh blog post on this using uh what the Monch mixer so it's kind of a new architecture that tends to longer context they have a 32,000 token embedding model that's pres that's available on together AI absolutely worth experimenting with I think this is really interesting Trend so long long Contex and beddings kind of play really well with this kind of idea you take full documents embed them using for example long Contex embedding models and you can kind of build these document summarization trees um really effectively so I think this another nice trick for working with full documents in the long context kind of llm regime um one other thing I'll note I think there's also going to Mo be move away from kind of single shot rag well today's rag we typically you know we chunk documents uh uh embed them store them in an index you know do retrieval and then do generation but there's no reason why you shouldn't kind of do reasoning on top of the generation or reasoning on top of the retrieval and feedback if there are errors so there's a really nice paper called selfrag um that kind of reports this we implemented this using Lang graph works really well and the simp the idea is simply to you know grade the relevance of your documents relative to your question first if they're not relevant you rewrite the question you can do you can do many things in this case we do question rewriting and try again um we also grade for hallucinations we grade for answer relevance but anyway it kind of moves rag from like a single shot Paradigm to a kind of a cyclic flow uh in which you actually do various gradings Downstream and this is all relev in the long context llm regime as well in fact you know it you you absolutely should take advantage of of for example increasingly fast and Performing LMS to do this grading um Frameworks like langra allow you to build these kind of these flows which build which allows you to kind of have a more performant uh kind of kind of selfreflective rag pipeline now I did get a lot of questions about latency here and I completely agree there's a tradeoff between kind of performance accuracy and latency that's present here I think the real answer is you can opt to use very fast uh for example models like grock where seeing um you know gp35 turbos very fast these are fairly easy grading challenges so you can use very very fast LMS to do the grading and for example um you you can also restrict this to only do one turn of of kind of cyclic iteration so you can kind of restrict the latency in that way as well so anyway I think it's a really cool approach still relevant in the world as we move towards longer context so it's kind of like building reasoning on top of rag um in the uh generation and retrieval stages and a related point one of the challenges with rag is that your index for example you you may have a question that is that asks something that's outside the scope of your index and this is kind of always a problem so a really cool paper called c c rag or corrective rag came out you know a couple months ago that basically does grading just like we talked about before and then if the documents are not relevant you kick off and do a web search and basically return the search results to the LM for final generation so it's a nice fallback in cases where um your you the questions out of the domain of your retriever so you know again nice trick overlaying reasoning on top of rag I think this trend you know continues um because you know it it just it makes rag systems you know more performant uh and less brittle to questions that are out of domain so you know you know that's another kind of nice idea this particular approach also we showed works really well with with uh with open source models so I ran this with mraw 7B it can run locally on my laptop using a llama so again really nice approach I encourage you to look into this um and this is all kind of independent of the llm kind of context length these are reasoning you can add on top of the retrieval stage that that can kind of improve overall performance and so the overall picture kind of looks like this where you know I think that the the the the problem of routing your question to the right database Andor to the right document kind of remains in place query analysis is still quite relevant routing is still relevant query construction is still relevant um in the long Contex regime I think there is less of an emphasis on document chunking working with full documents is probably kind of more parto optimal so to speak um there's some some clever tricks for IND indexing of documents like the multirepresentation indexing we talked about the hierarchical indexing using Raptor that we talked about as well are two interesting ideas for document Centric indexing um and then kind of reasoning in generation post retrieval on retrieval itself tog grade on the generations themselves checking for hallucinations those are all kind of interesting and relevant parts of a rag system that I think we'll probably will see more and more of as we move more away from like a more naive prompt response Paradigm more to like a flow Paradigm we're seeing that actually already in codenation it's probably going to carry over to rag as well where we kind of build rag systems that have kind of a cyclic flow to them operate on documents use longc Comics llms um and still use kind of routing and query analysis so reasoning pre retrieval reasoning post retrieval so anyway that was kind of my talk um and yeah feel free to leave any comments on the video and I'll try to answer any questions but um yeah that's that's probably about it thank you
