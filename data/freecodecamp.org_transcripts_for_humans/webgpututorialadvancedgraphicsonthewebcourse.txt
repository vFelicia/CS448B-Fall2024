With timestamps:

00:00 - Dr. Jack Xu is the author of many WebGPU books.
In this course, he will teach you the basics of
00:06 - WebGPU. Hi, I'm Jack. From this video, you will
learn the basics of WebGPU Graphics Programming.
00:14 - By building ten separate WebGPU
projects. Each project will be created on top
00:22 - of the previous one from simple primitives to
complicated 3d graphics. Our end product will be a
00:31 - beautiful 3D Sinc surface, as shown here. We will
create these WebGPU projects from scratch and
00:40 - show you how to add 3D graphics with GPU acceleration
to your web applications. Here, I want to thank
00:48 - Beau and FreeCodeCamp for publishing this
video. FreeCodeCamp is a great resource
00:54 - for the programming community. And thank you for
all that you do. Before getting into this video,
01:02 - I'd like to quickly tell you a little bit about
my background. I got my PhD in theoretical physics
01:10 - and have over 25 years of programming experience
in C, C++, C# .NET and web development. I
01:20 - have published over 20 books about practical
programming on a variety of topics,
01:28 - including graphics programming, machine learning,
quantitative finance, numerical
01:34 - computation methods, and web applications.
Recently, I created a YouTube channel: Practical
01:41 - programming based on my books. In this channel, I
will present several step by step video series,
01:50 - in which I will emphasize the usefulness of
example code to real world applications. The first
01:58 - video series is about WebGPU Graphics Programming
that is based on my recently published book,
02:07 - "Practical WebGPU Graphics". The projects
presented here are also selected from this book.
02:15 - My channel: Practical Programming with Dr.
Xu is new and I plan to update it every week.
02:22 - I would greatly appreciate if you can check
out my channel and subscribe it. Now let's go to
02:31 - WebGPU Graphics Programming. Now let's start
project one that will set up a development
02:38 - environment. The source code used in this project
can be downloaded from GitHub link here,
02:45 - This project uses this specific version. So
now what is WebGPU?  WebGPU is the next generation
02:53 - graphics API. It is future web standard for
graphics and compute WebGPU will provide
03:02 - modern 3d graphics and computation
capability with GPU acceleration. In this video,
03:11 - we will use the following development tools to
build our WebGPU applications. First is the
03:19 - Visual Studio code, node.js. We use a TypeScript
as our programming language, and we'll use webpack
03:28 - as our modular bundler. In order to run WebGPU
applications, currently, we have three options.
03:37 - The first one is to use a Chrome Canary to test
our WebGPU applications. The other option
03:45 - is to run WebGPU applications on a regular Crome
through origin trial. Starting from Chrome 94, Web
03:53 - GPU is available as the origin trial in the regular
Chrome. You'll need to register to origin trials
04:02 - and request the token for your origin. The third
option is that Chome will officially support
04:10 - WebGPU in May of this year. So after this May,
you will be able to run your WebGPU applications
04:19 - on regular Chrome. In this video series, I will use
Chrome Canary to test our WebGPU projects. Here, I
04:28 - suppose you already installed Visual Studio code
node.js, and also installed TypeScript globally
04:37 - on your machine. Let's start programming. Now
let's open a command prompt window and let's
04:45 - make a folder make directory gpuapp
and cd into it. And then, we are going to run this command:  npm
04:57 - init -y to create a package.json file.
Okay, this file will store our dependencies
05:06 - used in this project. Now, we are going to
install webpack and its command-line interface
05:14 - using this command: npm install cli. WebPack is
modular bundler that Bundles relevant files
05:24 - together to generate static assets that can be
used by the web applications. Here we use webpack,
05:32 - mainly for transpiling TypeScript files, but here
we don't use it to bundle the other types of files,
05:40 - such as HTML and image files. Now instalation
is complicated. At this point, we are going
05:48 - to start Visual Studio Code with a command: code
period. Here is Visual Studio Code interface,
05:58 - you can see we only have a package.json
file, but not much happening here. Now
06:05 - open a terminal window by pressing Ctrl+J key.
Now, let's install some dependency packages. First,
06:13 - we will install jQuery and corresponding type
package. npm install jQuery. We will use jQuery
06:22 - to manipulate the DOM elements in our applications.
Next, we want to install styling CSS and ts
06:32 - loaders. In case if our project contains
styling sheet file, we need to use style
06:41 - loader and CSS loader to bundle it. In order to
create TypeScript modules in our project,
06:49 - We also need to install TypeScript locally. npm
install TypeScript locally. We already installed it
06:58 - globally before. Now, opening the package.json file. You
can see the script section here only contains the
07:07 - test attribute, that is not very useful. Here we
are going to replace this section with the code.
07:16 - Here the code allows you to run WebPack
in different modes like development,
07:23 - production and watch mode. Now we can see with this
file and close it. Now all the installed packages
07:34 - are going to be stored in the node_modules
folder. Here the package_lock.json file is
07:41 - automatically generated for any npm operations.
So we don't need to do anything with it.
07:48 - Next step is to initialize TypeScript file
with the command tsc --init.
07:59 - This operation created a configuration file
for TypeScript called the ts_config here.
08:06 - Now, we open this configuration file
and we need to replace its content
08:16 - You can see here we set the target to the es6
also note we define the root directory as src
08:26 - and output the directory as dist, distribution
folder. Now save this file and close it.
08:35 - We then create some folder structures for
our project. First, let's create the src folder.
08:42 - So here we create a folder called src. We will
store all the source files in this folder. Next,
08:51 - we create a dist folder, which will contain all
files that will be uploaded to the web server for
09:03 - running our applications. In this folder, there
is at least one HTML file. let's add an index
09:12 - html file to this folder. Add another file index.html,
and add some code to it. You can see here we add
09:24 - the title WebGPU project. Here h1 title check
whether your browser supports WebGPU
09:33 - or not. We also have h2 DOM elements here
and set its id equal to id_gpu_check. This
09:41 - h2 element will be used to display text string
that will be returned by a typescript function
09:50 - which will be created later. Now we are about
to finish the development environment
09:56 - setup, but we have missed a key step, that is,  we
need to install the WebGPU package. WebGPU
10:04 - working group has created a type definition
for WebGPU standard. This package matches the
10:12 - work-in-progress WebGPU API, which is currently
not very stable. So it can change
10:21 - almost weekly. So be careful to use it. Anyway,
let's go ahead to install it with this command.
10:31 - So here we specify the version 0.1.12,
this is the latest version, we install it
10:41 - called  webgpu types. Because the WebGPU is not
backward compatible, so you better use the same
10:49 - version to run our project. Otherwise, you
need to do some modification to your code
10:55 - to run application. So I suggest you use the
same version here 0.1.12. After
11:02 - installation we need to configure it.
Open TypeScript configuration file.
11:09 - You can see here we already configured it. We add
WebGPU types and node in the types section here
11:18 - you can see here we have node type. So we
need to install it use this command to install
11:26 - this node type. Okay, the WebGPU is now available
for our project. Now let's add some TypeScript
11:37 - file to our project. First, added a new file
called helper.ts file to the src folder. So
11:45 - we added a new file called helper.ts file and
add some code to it. You can see here we use
11:57 - this navigator.gpu variable to check the GPU
availability. This variable is defined in the web
12:06 - GPU API. So you can see here we use the fat arrow
to define the typescript function. Next, we
12:16 - need to create an entry point file called main.ts
file. Okay, save this file first. And close it.
12:24 - Let's add another new file to the src folder
called the main.ts file. Now add some code
12:33 - to this file, we just import the jQuery and
also import the checkWebGPU function from helper.ts
12:43 - file, just implemented. So here we
define id_gpu_check to display the string returned
12:52 - by the checkWebGPU function. Here the id represents
h2 DOM element defined in the index.html file,
13:03 - make sure this ID is the same as ID defined
in that file. Next, we also need to
13:10 - create a webpack configuration file. Now add
a new file to the root directory webpack_config.js file.
13:23 - You can see here we define a bundle output
directory as dist and an entry point is named
13:31 - main here. The main, src main.ts File. Note that
the output file has always with bundle.js
13:40 - extension. Now we can save this file and
close it. Now we can run the following command
13:49 - from terminal window to bundle our TypeScript
file: npm run prod in the production mode
14:00 - Okay finished. You can check the bundle file in the
dist folder. You can see here we have several
14:07 - main bundle.js files. This is our bundle file,
this is license file, this is source map file. This
14:14 - main.bundle.js file has been
referred in the index.html file. You can
14:22 - see here, this is the main.bundle.js This bundle
file is generated by webpack. In order to test
14:30 - our application we also need a server. Here we will
use live server extension. Click on the extension link
14:41 - search for live server. This 
first one is live server. Here I already
14:48 - installed it. Otherwise we have install
link here. If you haven't installed it, just click
14:55 - install button to install the live server. So
I suppose you already installed it. Now we can open
15:04 - index.html file, right click anywhere open
with live server, click this link. This will
15:13 - open our default browser to display the web page.
Unfortunately, if your default browser is regular
15:22 - Chrome, it will displays the message: your current
browser does not support WebGPU. So in order
15:31 - to run WebGPU applications here we will install
Chrome Canary. So here search for Chrome Canary.
15:40 - Click it. So this nightly build for developer. Just
download Chrome Canary and install it.
15:47 - I already installed this chrome Canary on my
machine. Please remember the location where
15:54 - your Chrome Canary is installed. Okay, I suppose
you already installed it. Let's go back to Visual
16:02 - Studio Code. We want to change the default
browser to a Chrome Canary, which will be easy to test
16:10 - our WebGPU applications. So press Ctrl+Shift+P and
type the preference opening setting here. So
16:20 - open this file. Here, you need to add these
three lines here. live server setting don't so
16:29 - info message. I set it to true, this doesn't
matter. I also set the root directory as dist
16:36 - because our index.html file is in this directory.
So this the ones custom browser command
16:45 - line here defines the default browser. So you
need to replace this code. So you need to copy
16:53 - your Chrome Canary exe file location and paste it
here, you must change it to match your situation.
17:01 - Now we can save this file and close it. Now
start the Chrome Canary and type Chrome
17:12 - flags and search for WebGPU. Here,
enable it here I already enabled it.
17:20 - If you're not, you just need to enable it to make
WebGPU available for our Chrome Canary. Okay,
17:28 - we can close it. Okay finally we can test our
development environment for WebGPU applications.
17:35 - Here we have a link on the status bar area you can
see go live link. This time it will start Chrome
17:43 - Canary browser. Click this link. You can see
Greate your current browser supports WebGPU,
17:52 - Congratulations, we have successfully set up the
development environment for WebGPU applications.
18:00 - Now we completed our first WebGPU project that
shows how to set up development environment.
18:09 - This is project 2 of this video series.
Here I will explain how to create a colorful
18:17 - triangle in WebGPU as shown here. This
is a colorful triangle. This project is based
18:24 - on our first project. You can open project one
directly from Visual Studio Code or clone the
18:32 - source code from the GitHub repository. git clone.
This is a GitHub link for the project one
18:41 - source code. I also put this project 2 source
code into the GitHub repository. Here is the link.
18:48 - So you can download the source code used in
this project from this link. Now we will open the
18:54 - project one directly from Visual Studio Code. Here
we first add a CSS file called the site.css to the
19:06 - src folder, src folder, add a new file called
site.css. Type some code into this file
19:16 - then save and close it. Just like WebGL,
The first thing we need to do to use WebGPU
19:26 - for graphics rendering is to create a canvas element.
From the dist folder, open
19:35 - index.html file. Here we need
to change h2 elements from here.
19:43 - Here we define canvas element
with ID canvas-webgpu. Later we will
19:50 - create WebGPU Graphics on this canvas.
The WebGPU programming consists of two parts
19:58 - one is the control code written in typescript
or javascript, and the other part is shader code
20:06 - run on the GPU. Here we will use the official shader
language for WebGPU called WGSL, which is short
20:16 - for WebGPU Shading Language. To take advantage
of the code highlight in VS Code, we will
20:25 - convert the shader code with wgsl
extension into typescript-like module.
20:32 - To do this, we need to install a shader loader
for webpack. Now open a new terminal window.
20:40 - Run the following command to install the shader
loader called npm install ts-shader-loader
20:52 - This loader was originally created for loading
GLSL shader code. But we will make it works
21:00 - for WGSL here. Next, open WebPack
configuration.js file and add the following
21:09 - code to it. Here the file with wgsl, glsl, vs,
vs means vertex shader, fs, fs means
21:22 - fragment shader. So the file with these extensions
will be treated as a shader code by this
21:29 - shader loader. We then need to install
WGSL extension from VS Code. Click this
21:37 - extension link, search for WGSL. Here
is first one: wgsL provides syntax highlight
21:47 - I already installed this module. So you can see
here the code highlight. In this case, it is easy
21:54 - to write shader code using this extension. So if you
haven't installed it, so click this install button
22:02 - to install it. Next we need to add some type
declarations for our shader. Add a new sub folder
22:11 - to the src folder. A new folder called types. Then
add a new file called shader.d.ts file. Inside this
22:24 - file we add some contents to this file. Here we
declare module. The file with wgsl, glsl, vs or fs
22:37 - we declare this file as module. So right now we
can save this file and close it. Now we can
22:44 - write our shader code. In the src folder add a new
file called a shader.wgsl file. Then add some code
22:57 - From this code, you can see that there are two
shaders here. One is vertext shader and another
23:05 - is fragment shader. We have shader function for
vertex called vs_main function. vs means
23:13 - vertex shader function. Here we have fs
main: fragment shader main function.
23:19 - The vertex shader takes vertex data including
word position, color, and texture as the input the
23:28 - output is the position in the clip coordinates. While
the other outputs such as color and texture
23:37 - will be passed to the fragment shader. These
values will then be interpolated over the
23:44 - fragments to produce a smooth color gradient. Now
back to our example. In the vertex shader, we first
23:52 - define the output structure here, that contains
built-in position and the output color
24:00 - called  vColor. Inside the vs_main function, we first
define three vertices of our triangle using the
24:10 - floating point vec2 vector. So we have
three vertices. We also specify a colors for each
24:19 - vertex using a floating point vec3 vector.
You can see the first one is 1 0 0. This means
24:27 - red. The second color is 0, 1, 0 the
green, the final one is 0, 0, 1, which is a blue. So we define
24:36 - different color for each vortex. Next, we
define output using the output structure here
24:43 - we thenconvert 2D vector of the position
into a vec4, the four dimensional vector
24:50 - we put a z component at zero and a w
component as one. Similarly we convert
24:56 - 3d vector for the color into vec4 here. The
one represents transparency of the color
25:05 - In the fragment shader, we take the output
color from the vertex shader as an input and
25:12 - return it as the fragment color. Okay,
this is the shader we will use when
25:18 - creating our colorful triangle in WebGPU.
Now we can save this file and close it.
25:26 - Next, we'll use TypeScript to write WebGPU
control code. Open the main.ts file.
25:34 - And replace is content with the following new
code. Here we first introduce the checkWebGPU
25:45 - function from helper.js File. Then, from shader.wgsl
file, we introduce our shader from this file,
25:55 - we also import the site.css file.
Here we will use it to configure our web
26:02 - page layout. Next, we create a new function called
Create triangle. This function must be async, you can
26:12 - see async function because WebGPU API itself is
async. Inside this function we first check
26:20 - whether your browser supports WebGPU or not.
In WebGPU, we can access the GPU by calling the
26:29 - request adapter here. Once we have this adapter
we can call request device method to get
26:38 - GPU device. This device provides a context to work
with the hardware. And an interface to create
26:46 - GPU objects, such as buffer and texture.
We can also run commands on this GPU device
26:55 - As with WebGL, we need a context for our canvas
element that will be used to display graphics.
27:04 - Here we use the canvas element to request web
GPU context. You can see here get content by
27:12 - the name webgpu. This is a keyword used by WebGPU
to create context. We then configure this context
27:21 - with the device and the format. Next, we define
the render pipeline by calling the Create render
27:28 - pipeline method. Inside this pipeline we
have several attributes. The first one is a vertex,
27:37 - where we'll assign the shader to the module. Here
with an entry point as the vs_main function
27:44 - this function is created in our shader file.
Similarly for the fragment attribute we
27:51 - assign the shader to the module an entry point as
fs_main function. The another required attribute
28:00 - is a primitive here, this attribute
contains a field called the topology. We set it to
28:08 - triangle-list because we want to create a triangle
in this project. Next we create the command encoder
28:17 - and render pass by calling the beginning
render pass. This render pass accepts a
28:24 - parameter of the type GPU render pass descriptor
as the render pass option. Here we only use color
28:33 - attachments that is used to store image information.
In our example, it stores the background color
28:41 - in the clearValue area we have another field called
the loadValue. This field will be removed from
28:50 - WebGPU in next version, so we don't use it here.
Next, we'll assign the pipeline to our render
28:58 - pass and draw our triangle by calling the
draw method here. we call the end function
29:07 - to finish current render pass, meaning
that no more instructions are sent to the GPU.
29:15 - Finally, we submit all instructions to the queue
of the GPU device for execution. After running the
29:24 - command, our triangle will be written to the GPU
context and displayed on our canvas element. Now we
29:35 - finished the implementation of our create triangle
method. This method provides basic procedure
29:43 - to create WebGPU applications, that is, first we
initialize WebGPU API, create shader program,
29:53 - set up render pipeline and build render pass,
call draw function, submit instructions to the GPU
30:02 - for execution. Finally, we call the Create
triangle function to generate our triangle.
30:10 - Up to now we have finished our programming for
this project. We can now run the following command
30:18 - on the terminal window: npm run prod to
bundle our typescript code in production model.
30:30 - Okay finished. Our Bundle file is created
successfully. Now we can click the go-live link
30:38 - from here to open Chrome Canary to
view our triangle. Click this link.
30:45 - Here is our colorful triangle. You can see here is red,
green and blue. From one vertex to another you
30:54 - can see the color changes smoothly because
WebGPU interpolates the color internally
31:01 - which gives a smooth color gradient.
Now we have completed project 2.
31:09 - This is a project three: we will create a colorful
square using GPU buffer. In the last project,
31:18 - we create a colorful triangle in WebGPU by writing
the vertex and color data directly in the shader
31:27 - code. This approach is only possible for creating
very simple shapes. It is almost impossible to use
31:36 - this direct approach for creating complex 3d
graphics with different colors and textures.
31:45 - In this project, I will introduce GPU
buffer and show you how to use it to store vertex
31:53 - and color information. In this example, we
will create a colorful square to explain
31:59 - the concept of the GPU buffer. Here we will
introduce several new concepts that
32:06 - will be the foundation to create complex
3d graphics objects. GPU buffer in Web
32:14 - GPU represents a block of memory that can be used
to store data for GPU operations. In this project,
32:24 - I will show you how to use the GPU buffer to
store vertex position and color data. You can
32:31 - download the source code used in this project
from this GitHub repository link. You
32:37 - can see we have specific commitment
version, this version is specific for our current
32:45 - project. Now let's start Visual Studio Code and
open project 2 that we built in the last video.
32:54 - Okay, here's the project 2 we used in the last
video. Now let's first make some changes to
33:02 - the index,html file. From the dist folder,
open index.html file. Now we need to change
33:13 - here the h1 title to Create Square Using GPU
Buffer. So we don't need to change the other part
33:25 - of the code. So we can save this file. Next from the
src folder, open the helper.ts file. Now we need to
33:36 - add a new function called the InitGPU to this
helper file. So we need to add some code here.
33:46 - You can see we place all WebGPU initialization
code to this function, initGPU. So this can
33:57 - avoid code duplication because initialization
will be same for all WebGPU projects. This
34:07 - function is an async function, and it returns
the device, canvas, format, and context,
34:16 - which will be used when we create a render pipeline
and render pass. We also want to add another
34:23 - function called Create GPU buffer to this helper
file. So we need to add another function here
34:33 - called Create GPU buffer. You can see this
function takes three inputs: the device, the data,
34:42 - and the usage flag. Here we set the default
usage flag to vertex or copy destination.
34:51 - Here you can see we call the device create
buffer method to create a GPU buffer
34:58 - and a set the size as the data the byte length to
allocate the room for the buffer data. The other
35:08 - attribute here mapped at creation, we set this
attribute to true. This means that the GPU buffer
35:16 - is owned by CPU, which is accessible in Read
Write from typescript code. Once the GPU buffer
35:26 - is mapped, the application can ask for access to
range of content with get, for example, here
35:35 - get mapped range method and set it to
the data to be stored in the buffer. Pleace note
35:43 - the mapped GPU buffer cannot be directly used by the
GPU and it must be unmaped by calling the upmap
35:53 - method. Here, our create GPU buffer method
returns unmapped buffer, which can be used
36:03 - by the GPU. So be careful here about the GPU
buffer mapped and unmapped states. Now we
36:11 - can save this file and close it. Next
we need to make changes to the main.ts file
36:20 - from src folder open the main.ts file. We
need to replace is content with the new code.
36:30 - You can see here we first introduce
initGPU and create GPU buffer
36:37 - from helper.ts file and then we introduce
the shader. The shader code will be discussed
36:44 - in a moment. Inside the Create square method, we first
call the initGPU method to create a GPU
36:54 - object and then get the device from it. Next, we
define the vertex data and the color data here.
37:04 - You can see this is our vertex data, color data.
Here is vertex coordinates of our square a, b, c, d.
37:16 - so, the coordinates like here a minus point
five minus one and this is a, b, d, we have
37:24 - divide this square into two triangles a, b, d, d, b, c,
the vertices of each triangle must be arranged
37:35 - in the counter-clockwise order. You can see
here a-b-d, d-b-c. We also assign a color
37:44 - to each vertex. You can see a red b green c blue
and d yellow. You can see here it for a is red for
37:56 - B is green, and so so. These data are consistent
with the coordinates, shown in this figure. Of course,
38:05 - you can also divide the two triangle in other way
around. For example, a-b-c, c-d-a. So you draw
38:16 - the diagonal this way. So get another two triangles.
This doesn't metter. Next, we create a two buffers
38:24 - by calling the Creator GPU buffer: one
is the vertex buffer and another is color buffer. Next,
38:33 - we create a pipeline by calling the Create render
pipeline method. The new code we add here is for
38:41 - the buffer attribute. Before we don't have this
buffer attribute. Now, it is a two element array,
38:50 - the first element is for the vertex position.
The second is for the color. You can see for
38:57 - the vertex position we have arrayStride
we set it to eight since each vertex
39:04 - is represented using two floating 32 elements.
Each floating 32 number requires four bytes while
39:13 - for the color it is represented for each vertex
using three floating 32 elements. So, its array
39:22 - stride is set to 12. The another important
parameter here is shader location. For the vertex
39:30 - position we set the shader location to zero, and
for the color we set the shader location to one
39:36 - The other part of the code is very similar to that
used in the previous project. Here we set the
39:45 - primitive topology to the triangle list. One
new thing here is that after setting the pipeline
39:53 - to the render pass here, we also need to set
the vertex buffer to the render pass using vertex
40:01 - buffer and color buffer. You can see here we set vertex
buffer as zero slop is vertex buffer. And
40:09 - at slot one, we set the color buffer. In the draw
function here, we set the number of vertices
40:17 - to six because we have two triangles now.
Now we finished the modification to this file.
40:25 - Now we can save this file. Next we need
to make modifications to our shader code.
40:32 - from the src folder, open shader.wgsl file. Here
we need to replace his context with new code.
40:45 - Here you can see we didn't define any vertices
and colors in this shader code because we store
40:53 - these data in GPU buffer. Instead we introduce
two inputs one is at location zero pos, and
41:02 - other is the color at location one, which must be
consistent with the shader location definition in the
41:11 - pipeline in our main.ts file, because
we define the shader location as zero you can
41:17 - remember for the position and the shader location
is one for the color. So here position is zero.
41:25 - And here color location is one. This is how we set up
the relationship between the vertex shader and the
41:34 - GPU buffers. Note we define them here as a vec4
f32.  For the color same thing vec4 f32.
41:43 - They look like they are not very consistent
with the data stored in the vertex and color
41:50 - buffers where the for the position, it is a 2d array
and for the color, it is 3d array. This is because
41:57 - WGSL shader is smart enough to automatically
convert them into  vec4 types by adding
42:06 - zero to the z component and one for the w
component. You can see here we also define
42:14 - two outputs one is position, one is vColor using a
structure. Inside this vs_main function, we
42:24 - process the position and color data by assigning
the pos to the position and a color to the V
42:32 - color. Inside the fragment shader we introduce
the vColor from the vertex shader and return
42:41 - as fragment color. Okay, this is the shader we
will use when creating our square. We can now save
42:50 - this file and close it. Now we can run the
following command in the terminal window to bundle
42:57 - our typescript code: npm run prod 
to bundle our code in production mode. Okay,
43:07 - the bundle file is created successfully.
Now we can click the Go Live link
43:15 - to open Chrome Canary to view our square. So click
this go live link. Here is our colorful square
43:25 - with different vertex color. Here is red,
green, blue and yellow. From one vertex to another
43:34 - you can see the color changes smoothly because
WebGPU interpolates color internally
43:42 - which gives a smooth color gradient. Okay,
now we finished the project three.
43:51 - In projects two and three, I demonstrated how
to create a triangle and a square graphics in
43:58 - WebGPU applications. In fact, this two objects
are flat and two dimensional. We have not created
44:08 - any real 3d graphics yet. In this project 4, I will explain
how to create a 3d cube with distinct
44:18 - face colors. This is the first real 3d object we
are going to create in a WebGPU application. You
44:26 - can download the source code used in this project
from this GitHub repository link. Here we have
44:33 - specific version for this project.
This example involves a lot of code and
44:40 - mathematics. So please get ready to digest it. In
order to create a real 3d objects in WebGPU,
44:49 - you need to have the math background of 3d
matrices and transformations. Since our computer
44:56 - screen is a two dimensional, it cannot directly
display 3d Objects. To view 3d objects on a 2d
45:05 - screen, you have to project your object from
2d to 3d, that will involve a series of coordinate
45:15 - transformations. From here you can see that we
define 3d object in the object coordinate system
45:25 - here. We then perform various transformations on
the object, including scaling, translation and
45:33 - rotation. We call this transformation as a model
transform. After this transformation, we convert
45:41 - an object in the object coordinates into
the object in the world space. Next, the view
45:49 - transformation locates the viewer in World space
and transform our 3d object into camera space
45:59 - that also called the eye coordinates. The purpose
of the projection transform here is to define a view
46:08 - volume called  view frustum, right here.
This is a view frustum. This is used in two
46:16 - ways: it determines how an object is a project
on the screen. It also defines which portions
46:24 - of the object are clipped out of the final image.
That is only the portion inside this frustum
46:32 - will be kept and anything outside of this frustum
will be clipped out. Finally, we use the view
46:41 - port transform to convert the clip coordinates
into the normalized device coordinates. In WebGPU.
46:51 - The viewport transform here is automatically
performed in the shader code. Please note
46:58 - that like a WebGL, the WebGPU API
does not provide any functions for
47:06 - working with transformation. In this project, we
will use a JavaScript package called GL matrix to
47:14 - create 3d matrix operations and transformations.
Since the 3d matrix and transformations are common
47:23 - for any computer graphics programming, here, I
suppose you already have this math background,
47:30 - I will not spend any more time on this. Instead,
I will concentrate on how to create 3d objecta
47:38 - in WebGPU applications. From this project, you
will learn somehow important concepts in web
47:45 - GPU that is uniform buffer and binding group.
We will use a uniform buffer to represent
47:53 - the transformation and projection matrices,
and then use the binding group to pass
47:59 - the uniform buffers to the vertex shader. Now
we start with our Visual Studio Code
48:07 - and open our project three that we built
in the last section. Here is code we used in
48:16 - project three. First we need to install a new
NPM package called GL matrix: npm install gl
48:27 - matrix. We will use this package to perform
matrix operations and 3d transformations
48:36 - Now let's make some changes to the index
dot HTML file. From the dist folder, open
48:44 - index dot HTML file. Here we need to
change the h1 title to Cube with Distinct
48:55 - face Color. So we don't need to change other
parts of the code. Save this file.
49:03 - Here we are going to create our 3d cube. 
these are the coordinates our cube. From this
49:11 - diagram you can see there are eight vertices and
six faces. if we each face has a different color
49:20 - each vertex will then have three different
colors. For these vertices, For example, c
49:27 - we have the top face, front face, and the right
face. Because different face a different
49:32 - color. So the vertex C can have three different
colors depending on which face we talk about
49:40 - Here is the front face A, B C D. You can see A B C
D here. As we did before, we can now
49:50 - divide this front face into two triangles. One
is  A-B-C, C-D-A, two triangles you can see here
50:00 - ABC and CBA. In this way, you can perform triangulization
for the other faces. Next add a new TypeScript
50:10 - file to the src folder and add a new file vertex
_data.ts file. Add the new code to this file
50:21 - Here is the positionn data for our cube:
this is front face, right face, and six faces
50:29 - This is the color data for the different faces. You
can see front face we use blue color, right face
50:35 - is red, and back face is yellow and so on.
So different face has a different color. All the
50:41 - position coordinates uand the color are
consistent with this diagram shown here. Now we can
50:50 - seave this file and close it. Next we will make
some changes to the shaderr code. From src folder
50:59 - open shader.wgsl file. Now we
need to replace this content with the new code
51:09 - You can see this shader is different from that
we used in project three because we need to
51:16 - incorporate the uniform buffers that store
transformation and projection matrices
51:22 - Here we define a uniform structure and
then use the binding group to pass the
51:29 - model-view-projection matrix to the shader. Here
the mvp matrix means model view projection matrix
51:36 - You can see here the variable type is uniform.
Next we define the output structure
51:44 - as we did in the last project.  We define position
and vColor. This is the same as before. For
51:51 - the vs_main function we also define the two inputs:
one is the position one is the color, the same as
51:59 - the last project. And inside this function only
difference is the position not only has the pos
52:07 - here but we also multiply by this model view
projection matrix. So perform the transformation
52:17 - on this position. For the fragment shader, we still
use vColor as input and return it as
52:25 - fragment color. This is the same as in the project
three. Now we can save this file and close it
52:34 - Now we open the helper.ts file from
src folder. Here we first need to import from
52:44 - gl-matrix. We need to introduce the vector
three, mat4, and then add a new method
52:53 - called Create View projection to this
file. This method create view projection takes
53:04 - four input arguments: first is the aspect
ratio, the camera position, camera look at
53:11 - direction, and the camera up direction. Here
we first create a view projection and
53:18 - view projection matrices. We then use
a mat4.perspective and mat4.lookAt
53:26 - to create projection and view matrices. Next
we use mat4.multiply method to combining
53:34 - projection matrix with view matrix to form
our view projection matrix. This function
53:42 - returns the view matrix, projection matrix, and view
projection, and the camera option. The camera option
53:49 - will be used in the next project when we discuss
the camera control. Now, we need to add another
53:57 - function called the create transformation, which
is used to construct the model matrix
54:05 - Here we first create three rotation
matrices for rotation about X Y and Z axes
54:15 - We then create a translation and scaling matrices.
Next we perform individual transformation
54:23 - for the input arguments here. Finally
we combine all transformations together to
54:30 - form our final transform matrices which is our
model matrices. Now save this file and close it
54:41 - Next we need to make some changes to the
main.ts file. From the src folder open main.ts file.
54:49 - Here we need to replace is content with the
new code. Here we first introduce some methods
54:57 - from a helper.ts file, including the
newly created createTransforms, create view
55:05 - projection, we alsointroduce the cube data
from the vertex_data.js file, as well as mat4
55:16 - from the gl-matrix library. Inside this create3d
object method, the code is very similar to that
55:25 - we used in the project three. Here, we create the
vertex buffer, you can see and the color buffer
55:33 - using a cube data positions and cube data colors.
Here is pipeline code. Also similar to the
55:43 - project three. The difference here is array
stride for the position is 12 instead of
55:50 - eight. Right now for our cube, we have x,
y, z coordinates. So we have 12 here
55:58 - For our color, still 12 Because we have RGB
elements. Here, we also set the shader location to zero
56:06 - for the position and the shader location is one for
the color. The only difference in the pipeline
56:14 - here is we add this depth_stencil attribute.
Here we set depthWriteEnabled to true to enable
56:23 - the deoth stencil testing. The depth-stencil
testing determines whether or not a giving pixel
56:31 - should be drawn. For 3d graphics. enabling
depth stencil testing is very important. Otherwise,
56:39 - you may get unexpected results. The
following code is new and specific to our 3d cube.
56:48 - Here we create a model view and a model view
projection matrices by calling the CREATE
56:57 - VIEW projection method. We then create a uniform
buffer for our mvp matrices. You can see here
57:06 - the usage, we set it as a uniform for our
transformation. The size is 64 since
57:13 - our matrix is four by four, so we have 16
elements. Each element is a floating 32 number,
57:21 - so its size is 64. We then define
the binding group by calling Create Binding group
57:29 - for this uniform buffer. Here we set the layout
as a default pipeline get binding group layout
57:37 - zero. This means we use the binding group
zero layout. In the entries attribute, we set
57:44 - the bindings to zero and set buffer here
to the uniform buffer we just defined here.
57:50 - If we have a more uniform buffers, we can add the
elements here. Put them into this entries array. In
58:00 - addition to the texture view from a context we
also introduce the depth texture. Here we set
58:08 - the size as our canvas width and height. And
the format that we use here is depth 24 Plus, you
58:16 - can also use other format. The usage here we set it to
render_attachment. Now our render pass description,
58:25 - includes two parts: one is  the color attachments
and other part is teh depth stencil attachment.
58:32 - The color attachments is the same as we use in
the project three. The depth stencil attachment is
58:40 - specific for our 3d Cube. Next, we construct the
model matrix by calling create transforms method
58:51 - defined in the helper.ts file. We then obtain
Model View MVP matrix by multiplying the
59:00 - model matrix with the view projection matrix.
So we got a final model view projection matrix
59:08 - and then we add this model view projection matrix
to our uniform buffer by calling the write buffer
59:16 - method. You should already be familiar
with the following code here. We just
59:24 - define render pass and a set vertex color buffer.
But we also need to set the binding group here using
59:32 - uniform binding group which is needed to
pass the uniform buffer to the vertex shader. Now,
59:40 - we finished the modification to the
main.ts file. We can then save this file.
59:47 - Up to now we have finished all programming for
this project. Then we can run the following
59:55 - command in the terminal window to bundle
our code in production mode: npm run prod.
60:06 - Okay, the bundle file is created successfully.
Now we can click the Go Live link to open
60:14 - Chrome Canary to view our 3d Cube. Click
this link. Here is our cube with distinct
60:24 - Face Color displayed on this page. Okay
we have finished this project.
60:33 - This is project 5, in which we will discuss
the animation and camera control. In the last
60:41 - project, I explained how to use the uniform
buffer to create a 3d cube. In this project,
60:48 - I already illustrate how to animate that cube
and how to use mouse to interact with the cube,
60:56 - you can download the source code used in this
project from this GitHub repository link
61:03 - Here the specific commit version is used
for this project. For animation we will use
61:10 - the JavaScript requestAnimationFrame function.
This function can make changes to your screen
61:18 - in an efficient and optimized manner. Here
is an example of using request animation frame.
61:26 - You can see here we loop over and over again using
a recursion. Here we first define the DOM element
61:35 - in the UI and define a start number of zero
here. Next, we create a count function that
61:42 - increases number by one. And then set it as
a text content for the counter element. Inside
61:51 - this counter function we call request animation
frame and pass the counter function itself as
61:59 - a callback function. This causes it to run
again just before the next frame. Finally,
62:05 - we use the request animation frame function
to start the animation. This is a basic usage
62:14 - of the request animation frame. For using the mouse
to interact with cube, we will use the npm package
62:23 - called the 3d view controls. This package can be
used to control the camera with your mouse you can
62:30 - then interact with 3d object by controlling the
camera. Now let's start Visual Studio Code and open
62:40 - project four that we built in the last section.
Here is the code used in project four.
62:47 - First we need to install JavaScript
package called 3d view controls. Open
62:54 - terminal window and run
npm install 3d view controls
63:04 - We will use this package for camera
control. The package is already installed. Now
63:12 - let's make some changes to index dot HTML file.
From the dist folder, open this file. First
63:20 - we need to change h1 title to Animation and Camera
control. We also need to add two
63:28 - radio buttons that let you select whether
you want animation or camera control here
63:37 - You can see the radio button. Okay now we can save this
file. The shader code in this example is the
63:46 - same as that used in the project 4. So
we don't need to make any change to the shader
63:52 - code. Now from the src folder, open helper.ts
file. Here we need to add a new method called
64:02 - Create Animation. We need to add create_animation
here. This is code for create animation.
64:13 - method. Here the draw augments is a callback
function. Here we want to animate the rotation
64:20 - around X, Y and Z axes using the step function
here, the step function. The Create Animation has a
64:29 - boolean argument here is_animation that
controls whether you want to animate the object
64:37 - or not. Now we can save this file and close it.
Next we need to make some changes to the main.ts
64:47 - file. Open main.ts file. We need to
replace the content with the your code.
64:57 - This code actually is very similar
to the code used in the last project. Here,
65:04 - in addition to the other function introduced from
helper.ts file, we also introduce Create Animation
65:12 - functionwe just created. Note that we use here
require to introduce 3d view controls
65:21 - module, instead of the import. This is because
the 3d view controls package was created using
65:30 - old modules approach. So we have to use require
instead of import here. Here we define
65:39 - create camera from this package. In the create 3d
object method, we introduce an input argument
65:49 - is_animation, and we set its default value to
true here. Here is the initialization code and
65:57 - pipeline code are the same as those used in the
last project, we don't need to make any change to
66:05 - this code. In the creating a uniform data section
we define v-matrix, this  is view matrix
66:13 - is a new matrix. Next, we need to add rotation and
camera. Rotation we define using vec3 and
66:21 - camera we define create-camera here.
we have arguments canvas and camera option,
66:29 - we don't need to make any changes to the code
for creating a uniform buffer here and a uniform
66:36 - binding group. This is render pass description is
also the same as that used in the last project,
66:43 - here, we put this render pass related code inside
the new function called draw function. This
66:52 - draw function is defined as a callback
function for our animation. You can see here
66:59 - if is_animation is not true we use
the camera to generate our view matrix.
67:07 - And then multiply projection matrix with this view
matrix to form our view projection matrix. The
67:16 - rest code of the draw method is the same as that
we used in the last of the project. Here we start
67:25 - animation by calling the Create Animation
function. You can see we have callback function,
67:32 - using the draw function defined here. Finally,
we need to add a radio button selection here,
67:38 - you can see that if the radio checked value is
equal to animation, we run animation. Otherwise,
67:46 - we use the camera control. Now we finished
the modification to the main.ts file. So we can
67:53 - save this file now. Up to now we have finished
the all programming. Now we can run the following
68:00 - command in the terminal window: npm run prod
to bundle our TypeScript code.
68:10 - Okay, the bundle file is created successfully.
Now we can click the Go Live link to open Chrome
68:19 - Canary to view our 3d Cube. Click this
go live link. You can see the animated
68:28 - cube on this page. It rotates continuously because
we call the animation mode. Now you can click
68:36 - on this camera control radio button here,
then you can use the left mouse button
68:43 - to rotate the cube and the use right button
to move it around. And you can also use mouse
68:54 - wheel to zoom in, zoom out the cube. So
you can use the mouse to interact with the cube.
69:08 - So now with how completed this
project. In the previous several projects,
69:16 - I explained how to create some 2d and 3d objects.
However, when rendering these graphics objects with
69:25 - solid colors, you may find that the image
looks flat and it is failed to illustrate
69:33 - the 3d nature of the objects. This is because we
neglect interaction between light and surface in
69:41 - our objects. Lighting helps provide visual effect to
a scene so that it looks more realistic. Lighting
69:50 - is one of the most important factors for creating
real world shaded 3d graphics objects. However,
69:59 - WebGPU does not provide much built-in
features on lighting. It just runs two functions:
70:07 - vertex and fragment shaders. If you want the
lighting effect in your 3d scene, you have
70:14 - to create the lighting model yourself. In this
project, I will build a simple lighting model in
70:22 - WebGPU and use it to simulate the light source.
Here you can download the source code used in this
70:30 - project from this GitHub repository link. Here
The specific commit version is used
70:38 - for this project. Here I will discuss three
types of light sources, ambient light,
70:45 - diffuse light, and specular light. The
ambient light is the light that illuminates
70:53 - all objects uniformly regardless of their location
or orientation. It is a global illumination
71:01 - in an environment. The diffuse light and
specular light reflection depends on the angle
71:08 - of the light to a surface. When the light hits
the surface., the diffuse reflection occurs in
71:15 - all directions, as shown here, while the specular
light reflection occurs in a single direction.
71:24 - as shown here. Here shows the light reflection on a
torus surface. On the left here, light is only from
71:35 - ambient light, you can see this torus looks very
flat. Here in the center shows light from both
71:44 - ambient plus the diffuse light. You can see
the 3d feature already shows here. On the right here,
71:54 - we have three light sources: ambient plus diffuse
and plus specular lights. You can see this
72:02 - torus looks more realistic. So, this is the
light model we want to build in this project.
72:10 - The light reflection depends on the angle at which
the light hits the surface. The angle is essential
72:17 - to the diffuse light and the specular light.
This angle is always associated with surface
72:24 - normal that is a perpendicular to the surface.
To build the light model, we need to calculate the
72:31 - surface normal based on the direction in which
the surface is facing. Since the surface of 3d
72:39 - object can be curved, it can face different
directions at different vertex. So,
72:46 - a normal vector is usually different for different
vertex that is normal vector is always associated
72:54 - with a particular point on the surface. Here shows
normal vector for 3d cube. Here, the front face is
73:03 - pointing towards the screen. So its normal
vector is 001. The right face is facing
73:12 - direction to the right. So its normal is 1
00. And the top face is facing towards the top. So,
73:21 - its normal vector is 010. So, similarly, you
can specify the normal vectors for the back
73:32 - left and the bottom faces. Note that the
length of the normal vectors should be always
73:40 - one, that is, we need to normalize the
normal vectors. For a general quadrilateral A B
73:47 - C, D, its 4 vertices may not in the same plane,
we can divide it into two triangles, A, B,
73:57 - C and C D A. We can calculate its surface
normal in two steps. First, we calculate the
74:06 - weighted normal for each triangle, which is a
weighted by the area of that triangle.
74:13 - The triangle with larger area will get
more weight. As we know the area of a triangle
74:20 - is proportional to its cross product of two
sides. For example, here, for triangle A, B,
74:29 - C, we have a cross product B, C, and C-A,
b c and a c. Similarly for the triangle C
74:38 - D A, we have a cross product D A and A C. The next
step is to calculate the surface normal from the
74:47 - weighted triangle normals, that is, the surface
normal equals the sum of two weighted normals,
74:54 - and we can then normalize it. So we get this
quadrilateral normal equals normalize this
75:02 - two cross products. The sum of this one triangle
plus another one. So, we can combine these two
75:10 - terms to form one term equals cross product of
C A and D B. So, this means the surface normal
75:18 - for this general quadrilateral just equals
to the cross product of C, A, and d b. Just
75:28 - equal to the cross product of these two diagonal
vectors. We will use this formula to calculate
75:37 - the light model for our 3d surfaces. Once we
have calculated the normal vector, we can use it
75:45 - to compute the light intensity. For diffuse light,
its intensity is proportional to the cosine
75:54 - between the light vector here and a normal
vector N. You can see here the cosine alpha equals
76:02 - normalized L dot N here. If we define the Id, the
diffuse intensity, Kd the diffuse material
76:11 - property, so we can have this formula: the diffuse
intensity equals Kd times maximum cosine alpha and 0. Here
76:20 - the maximum is used to avoid the negative value of
the cosine alpha here, we only take the positive
76:29 - value. We can also add the ambient light to
the diffuse light intensity like this. Here Ia
76:39 - is ambient light intensity. So the total light intensity
is maxima Kd and here is the Ia. So we add
76:48 - the ambient intensity to this diffuse light like
this. For specular light, we have two models
76:56 - one is Phong model, this model is very simple. So
the Is, the intensity of the specular light, equals
77:04 - to Ks, Ks is speclar material property, and s
is the shininess of the surface or roughness,
77:11 - the V is the view direction, R is reflection
direction. One issue with Phong model is that
77:18 - the angle between the view direction and reflection
direction has to be less than 90 degrees in order
77:26 - for the specular Phong term to contribute. We can
use the Blinn-Phong model to address this issue.
77:35 - Blinn-Phong model uses a different set of
vectors for its computation, based on the half
77:42 - angle vector. Here's a half angle vector, you can see L 
is the light direction, N is normal vector, V is
77:50 - view direction, H is the half angle vector defined
L is the light direction plus the view direction. The
77:58 - view direction plus the light direction gives H
here. The formula in Blinn-Phong model becomes
78:06 - N dot H instead of V dot R here. So the
angle between this N and H is always less
78:16 - than 90 degrees. So this N dot H
always gives positive value. So we will use this
78:25 - Blinn-Phong mode in our calculation for the specular
light. Now let's go to the programming part.
78:32 - We started with Visual Studio Code and open
project five that we built in the last section.
78:41 - Here is the code for our project file. Now let's
implement the light model in the shader code. From
78:50 - src folder open the shader.wgsl. We need 
to replace its content with the new code.
79:00 - Here we first define the uniform structure
here that contains view-projection matrix,
79:08 - the model matrix and normal matrix. Here
the normal matrix is transformation matrix
79:16 - for the normal vector data. When applying a transform
to a surface, we need to derive the normal vector
79:23 - for the resulting surface from original normal
vectors. In order to transform the normal vector
79:31 - correctly, we don't simply multiply them by the
same matrix used to transform our object, but we
79:41 - need to multiply them by the transpose of the
inverse of that matrix. Since the current WG
79:50 - SL version does not implement the matrix inverse
function yet, so we have to make this transpose
79:59 - of the inverse of the transform matrix in
TypeScript code, and then pass it as a uniform
80:07 - matrix as shown here, normal matrix. We then
define output structure that contains three
80:15 - variables: built-in position, the v-position
the position after some transformation, we call it the
80:23 - v-position and the v_normal, which is a normal
vector after transformation. In our vs
80:29 - main function, it contains two input arguments
one is vertex position called the pos and
80:37 - one is normal vecctor data. Inside this function, we only
perform the model transform on the position. This gives us
80:46 - v-position here because it is used in the light
calculation. The  v-normal is a result of normal
80:54 - vector data multiplied by the normal transform
matrix here. And the default position here
81:02 - is obtained by performing model-view and projection
transform on the vertex data. For the fragment shader
81:12 - we first introduce the uniform
structure called FragUniform that contains two
81:20 - variables: one is light position of light
vector, and the other one is eye position. Or
81:28 - view vector. It also has another structure called
LightUniform. Here contains parameters for our
81:37 - light model. Here's a light color and a specular
color. The specular light can have its own
81:44 - color, and the other parameters contain ambient
intensity, diffuse intensity, specular intensity
81:51 - and the specular shininess. Here the fs_main
function takes v_position and v_normal
81:59 - as inputs. Inside this function, we calculate
the light model. Here is the diffuse. Just use N dot
82:07 - L, the cosine alpha. For the specular light, we use Blinn
Phong model. This  is N dot H. The output color
82:15 - from fragment shader will be weighted, you can
see, by ambient, diffuse, and specular light.
82:24 - Its combination gives us the final color. So, this
is the shader we are going to use to calculate the
82:32 - light model. Now we can save this file and close
it. Next in the src folder, we will add a new type
82:41 - script file called light.ts. Add a new file
light.ts file. Here is the code for this file.
82:52 - Here we first create an interface named light
inputs that contains parameters used for
83:00 - calculating light model. This code also contains
a new function create-shape-with-light
83:07 - which has four input arguments: one is the vertex
data, normal data, and light input, and the is
83:17 - animation parameters. Here we set the default
light parameters if no parameters were passed,
83:26 - so we have some default values for
the light parameters. Next we create
83:31 - a vertex buffer and a normal buffer here.
You can see that the pipeline contains
83:37 - a buffers attribute here. It is an array that contains
two elements. The first element is the vertex
83:45 - data with the shader location being zero. The
second element is for the normal vector data
83:52 - with the shader location being one here. The
other part of the pipeline is the same to that
84:00 - we used in Project five. Next we create a uniform
data. Here we define normal matrix and a model
84:09 - matrix, and view, and view-projection matrices here. For
simplicity, here we set the eye position we get
84:18 - from the camera equals eye position. This means
the light vector is equal to the view vector. Here
84:26 - we then create three uniform buffers: one is vertex
uniform buffer, another one is a fragment uniform
84:35 - buffer, and finally it is light uniform buffer.
The vertex uniform buffer is used to store
84:42 - the model matrix, vew-projection matrix, and
a normal matrix. The fragment uniform buffer is used
84:50 - to store the eye position and light
position. And the light uniform buffer is used to
84:56 - store the light parameters which will be used in 
calculating the light model in the fragment shader
85:04 - please note here, inside it is a callback
draw function, we write a model matrix
85:11 - and a normal matrix inside this draw function,
because we want to use the real time model matrix
85:18 - to perform various transformations. Note we
construct the normal matrix here by first inverting
85:26 - this model matrix and then transposing this normal
matrix. And get the final normal matrix. This means
85:34 - we do the inverse and the transpose on the original
model matrix. And finally, get normal matrix.
85:42 - when we set the data to the buffer, please
note here is 64 and 128 is offset so,
85:51 - we also need to use the correct offset to
write this data to the uniform buffer.
85:57 - Finally, we set the vertex buffer, normal
buffer, and a uniform binding group to the
86:04 - render pass. Now, we finished the coding for
the file. We can save this file and close it
86:11 - Okay we completed the programming for
calculating the light model. In next project, we
86:17 - will use the light model implemented here
to create a 3d cube with lighting effect.
86:27 - In Project 6, I explain how to build a
simple light model in WebGPU. In this project,
86:35 - I will show you how to use this light model to
add lighting effect to a 3d cube, as shown here,
86:44 - You can download the source code used in this
project from this GitHub repository link.
86:51 - This specific commitment
version is used for this project.
86:57 - Now start Visual Studio Code and open our project
6 that we built in the last section. Here is
87:05 - the code used in the project 6 as we discussed in
the last section. In order to calculate the light
87:14 - model, we have to know the normal vector at each
vertex on the surface of the 3d object. Now
87:22 - we need to add normal vector data to the vertex_data
of our cube. From the src folder open vertex_data.ts
87:33 - file. Here for the CubeData method, we
need to add the normal vector data here
87:41 - You can see that vertices on the same face
have the same normal vector. For example,
87:47 - the front face all have 001.
Similarly for the right is the 100, all
87:54 - the 100 and so on. Once we add the normal vector
data to this CubeData function, we need to return
88:02 - the normal data here normals. We also need to
return normals. Okay now we can save this file and
88:12 - close it. Next we need to make some changes to the
index.html file. From the dist folder, open
88:22 - the index.html file. Now we need to replace
its content by the new code. Here we add some
88:32 - input parameters here that let you test our light
model. This parameters also allow you to specify
88:42 - object color, ambent, diffuse, specular
light coefficients. You can also specify
88:51 - surface shininess and specular color for the
specular light computation. Now we can save this
89:00 - file. Next we need to make some modifications to
the main.js file. From the src folder, open the main.js
89:10 - file. Now replace its content with new code.
Since most of the code for render pipeline and
89:20 - render pass hve been already included in the light
dot ts file, here the main.ts file becomes very
89:29 - simple. First, we introduce the create 
shape with light and the light inputs interface from
89:36 - the light.ts file, and then introduce the cube data from vertex
data.ts file. Next we call the cube data method
89:48 - to get the vertex data and define the default
parameter here, the light inputs, we just use
89:56 - the default parameters. And also is_animation, we set
it to true. And then we call the create shape
90:04 - with light function to create a 3d cube with
lighting effect. This part of the code allows the
90:12 - user to recreate the cube with different lighting
effect by changing the input parameters. Now we
90:21 - finished the modifications to the main.js file.
We can now save this file. Now we can run the
90:29 - following command in the terminal window to bundle our
TypeScript code in production mode: npm run prod
90:41 - Okay, the bundle file is created successfully.
Now we can click the Go Live link
90:48 - to open Chrome Canary to view our 3d cube
with light effect. Click this link.
90:58 - Here is a red cube with the lighting effect
displayed on this page. We can make some changes
91:05 - to the input parameters and click the redraw button
to recreate the cube. Now, for example, we can set
91:13 - the diffuse and specular coefficients to zero
here, to zero, and ambient light to one. Now click
91:26 - redraw, we get a cube with ambient light only. You
can see the cube looks flat and no 3d feature.
91:37 - Now let's add the diffuse light
and specular light back.
91:46 - point 2. We can then change the color of the
object. For example, this red we change it to
91:53 - green, 010. And also change the specular color
from white to yellow, and then click the redraw
92:04 - button. You can see here the yellow you can
increase the shininess, for example, 100. You can see
92:13 - the spot a little bit yellow. This is specular light.
Now, go back to animation. So by changing this parameter
92:22 - you can get new lighting effect on this cube.
You can use this light model to easily create
92:31 - your own 3d object by following the procedure
present here. Okay, now we have completed
92:40 - project 7. The projects we discussed
so far were appled a fixed color by assigning a desired
92:49 - color value to the fragment color variable
in the fragment shader. In this project,
92:56 - I will explain how to use the colormap to render
3d surfaces. Surfaces play an important role in
93:06 - various applications, including computer graphics
games, and 3d data visualization. In some graphics
93:16 - and chart applications, we need a custom
colormap to achieve special visual effects.
93:24 - In fact, the colormap is just a table or
list of colors that are organized
93:31 - in some desired fashion. We can create a
custom colormap with an mx3 color matrix
93:40 - with each row representing RGB values. The row index
can represent the y data of a 2d chart
93:51 - or height of a 3d surface plot. For a given color
map matrix, the color data values can be linearly
94:01 - scaled to the colormap. Here shows the color
strips for the colormaps that we will create
94:10 - in this project. You can see that each color
map has a familiar colormap name, such as
94:19 - hsv, hot, cool, and jet and so on. You can
download the source code used in this project
94:28 - from this GitHub repository link. This specific
commit version is used for this project. Now
94:37 - start with Visual Studio Code and open our project
7 that we just built in the last section
94:47 - Here contains the source code used in the
last project. We can easily create a custom
94:55 - colormap using simple mathematical formula.
Here, I will provide a several commonly
95:02 - used colormaps using mx3 colormap array.
Now add a new typescript file
95:10 - called colormap_data.ts file to the src
folder, s src folder, add a new file
95:18 - called thecolormap_data.ts file.
And then add a new method to this file.
95:30 - Here the colormap data method contains
11 different colormaps data. You can
95:37 - see hsv, hot, and jet as default.
Inside this function, you can see each colormap
95:47 - contains 11 RGB color arrays, each, I mean, array 
here is RGB values. Here shaws the color strips generated
95:58 - using this colormap dat. This is hsv, corresponding to
this color strip. So here is the jet
96:05 - corresponding to this color strip. So, these color
strips are generated using colormap data. In
96:13 - the colormap_data function, we assume this 11
color arrays are uniformly distributed in the range
96:22 - of zero and one. For example, the color zero, the
first element represents a color at zero
96:31 - location, while color 5 represents a color at
0.5, etc. However, we have to use interpolation
96:43 - method to get a color at, for example, 0.55
or any other arbitrary locations. So
96:51 - here, we will use an npm package called interpolate
arrays to do the color interpolation. Now in the
97:01 - terminal window, we run the command: npm install,
interpolate arrays to install this package.
97:15 - Now at the top of this colormap
file, we need to introduce this package
97:22 - here. Here you can see again, we use a
require to introduce this package.
97:30 - because it was created originally using the
old modules method. So we have to use the require
97:38 - instead of import to get this package. Next,
we add a new function here called addColors
97:49 - Here is addColors method to this file. This
function accepts the x argument whose value
97:57 - value is in the range of minimum and maximum. This
function allows you to interpolate color for any
98:07 - arbitrary x value. Inside this function, we first
get the color array by calling the colormap
98:15 - data method with a colormap name as
the input. And then make sure the input
98:22 - x parameter is in the range of minimum and maximum
range. Next, we normalize x to the range of
98:32 - zero and one. Finally, we interpolate the color
for the X variable by calling the interp here
98:40 - the color and normalize x here. This interp 
comes from the interpolate_arrays package.
98:49 - Now this function returns RGB array with
each component being in the range of zero and
98:57 - one. So now we can save this file and
close it. Next in the src folder, rename
99:05 - the light.ts file to the surface.ts
file. So we rename this file to surface.
99:14 - This surface.ts file will be
used to create different surface plots.
99:21 - First, we need to make small changes to the light
inputs interface. Here we can remove the color field
99:30 - here, because we don't need to
specify the solid color for our object,
99:37 - but we will use the colormap for the
object color. Then we add another new
99:45 - field called is_two_side_lighting. Since the 3d
surfaces are usually open surfaces we need to
99:54 - implement the light model for both the front and
back sides, in order to see the surface clearly,
100:03 - but this parameter controls whether you
want the one-side or two-side lighting.
100:08 - Next we change the function name here create
surface with the colormap. So we change
100:15 - create colormap,  and then add a color data as its
input argument. Here we have normal data,
100:23 - we need to add the color data. This color data means
the colormap data. Here we need to remove this color,
100:33 - and add a new before the value for the is_two_side_
lighting here. Here we set the default value to one
100:43 - This means the two side lighting. If zero, it is one side.
Here we create a vertex buffer, normal buffer,
100:51 - we also need to create the color buffer.
Color and use the color data.
101:01 - Next, here is a buffer array: the first is vertex buffer,
this is normal buffer. We also need to add
101:10 - another buffer for the colormap data. So,
here we need to set the shader location to 2.
101:20 - we don't need to change the other part of the code
for the pipeline. So, other part is the same as
101:28 - that used in the last project. The code
for the uniform data, camera, and uniform
101:35 - buffer layout is the same as that used in the
last project. So we don't need to change anything
101:43 - here. But here the light parameters we need
to make some changes because the light color
101:50 - we already removed. So we remove this light color,
but we need to add another parameter called is
101:58 - two side lighting parameter. So we need to add
a parameter here, push the is_two_side_lighting
102:08 - parameter by 0 0, 0. But we will make the parameter
consistent with the vec4 array. The other thing
102:17 - we need to make a change is that we should add
the color buffer to the render pass. We have
102:24 - vertex buffer and normal buffer. We also
need to add color buffer here, called color buffer here
102:33 - color buffer at slot 2. Okay we finished
the modification to this file. Right now,
102:39 - So we can save this file. Next we need to make
some modification to the shader code
102:46 - because we want to incorporate lighting and the
colormap into the shader. So now from src folder
102:56 - open the shader.wgsl file from this, open this file and
we needed to replace its content with new code.
103:07 - This shader is very similar to that
used in the last project. Inside the vertex
103:14 - shader, in addition to the vertex position, you
can see and normal vector we also add the color
103:21 - at the location 2 to this input. Within the
vs_main function we process the colormap
103:31 - and assign this colormap to the v_color.
In the fragment shader here we use the processed
103:38 - colormap data as our object primary color here.
You can see v_color, this processed from
103:46 - vertex shader. We use it as a primary color to
get the final color. Inside the fragment shader
103:53 - the main function you can see here
the is_two_side_lighting parameter
104:00 - here called the prameter zero, controls whether
the lighting is applied to one side or two sides
104:08 - of our surface. We set it to one for two side
lighting, which is also default setting. For two
104:16 - side lighting, you can see this for one side for
front side the diffuse we use the N dot L, specular
104:25 - light we use N dot H. If two side lighting
we add another term here for the diffuse we use
104:34 - minus N dot L. For the specular we use minus N dot
H. This means for the backside light, we just
104:43 - reverse the normal vector to get the light for
the backside. Now we can save this file. Okay
104:50 - now we've finished the programming for colormap
in this project. In the last project we discussed
105:00 - the colormap model. In this project I will
explain how to use this model to build simple 3d
105:07 - surfaces. Here is an example of a 3d Simple
surface created using a peaks function.
105:16 - Mathematically, a surface draws a Y function on the
surface for each x and z coordinates in a region
105:26 - of interest. For each x and z value pair the
simple 3d surface can have at the most one Y
105:35 - value. We can define a simple surface by the y
coordinates of point above a rectangular grid.
105:43 - In the x-z plane, the surface is formed by 
jointing adjacent points using straight lines.
105:51 - Typically, the surface is formed
using rectangular mesh grdids.
105:56 - However, WebGPU only provides
triangles as the basic units to represent
106:03 - any surface in 3d. In order to represent a surface
using traditional quadrilaterals, we need to write
106:12 - our own functions. You can download the source
code used in this project from this GitHub
106:19 - repository link. This specific commit
version is used for this project.
106:25 - Now start Visual Studio Code and open project
8 that we built in the last section.
106:34 - Here shows the code used in the project 8.
First we need to add a new typescript file
106:43 - called surface-data.ts to the src folder, SRC
folder. Add a new file called surface-data.ts
106:55 - We need to introduce vec3,  import vec3,
from gl-matrix library, and also need to import from
107:09 - colormap-data.ts file, introduce
addColors function. First, let
107:16 - add a utility function called normalize-point that
will be used to normalize the point to the range
107:24 - of minus one and a one. So, we need to add a
function normalize function. You can see here
107:31 - this function takes a 3d vec3 here
the points. x data range: X minimum, X
107:39 - maximum, y minimum, y maximum, z minimum,
z maximum, and also introduce a global scale,
107:46 - parameter that allows you to change the value
range for the x, y, and z coordinates. This will
107:53 - be convenient for setting the default size of our
surface. This function returns the normalized 3d
108:01 - point. For our simple 3d surface as shown here, this
surface can be formed by quadrilateral mesh grid
108:10 - Here is one of the quadrilateral. This, as
shown here, is a unit grid cell. This
108:17 - unit grid is a quadrilateral with 4 vertices
P0, p1, p2 and p3. We can divide this
108:25 - quadrilateral into two triangles P0-p1-
p2, this one triangle, another triangle is
108:33 - p2-p3 and P0 as shown here. We will
create a vertex data, normal vector data and
108:40 - colormap data for this quadrilateral. Now,
we can add a new function called Create_Quad.
108:49 - You can see here that this function takes
4 points just as shown here, point 012
108:56 - and 3. It takes these 4 points as its input
arguments. It also takes vertex data range y
109:04 - minimum and y maximum, as well as the colormap
name as its input parameters, because we
109:11 - want to add the colormap to the y values
for our surface. Of course you can colormap
109:18 - the data value in the other direction such as
x and z direction. But we usually add colormap to the y
109:26 - values. here we first create
the vertex position data for the six vertices
109:33 - This is point zero, point one,  point two.
This is the first triangle, this is second
109:39 - triangle: point 2, point 3, and point zero. So this
is vertices for this unit cell for these two
109:47 - triangles. Next, we define the normal vector for
this quadrilateral. We already discussed how to
109:55 - obtain the normal vector for this quadrilateral
in the previous project. It is simply equal to
110:02 - the cross product of these two diagonal lines of
this quadrilateral. Here we introduce these two
110:10 - lines: first is p2-p0, p two p zero,
we call the CA, another line is  p3 and
110:18 - p1, p three P one, we call it DB. Then, we get
the cross product, we call CP, these two lines,
110:25 - then we normalize this CP cross product, we get
the normal vector for this quadrilateral. So, all
110:33 - the vertices on this quadrilateral have the same
normal vector. Next, we add the colormap data
110:41 - to vertices for this quadrilateral. You can
see the P zero p one, P two and p three, this is
110:49 - the colormap data for these 4 vertices.
For P zero, you can see we add the colormap
110:57 - data to it by calling the addColors function
with its y component p0[1], and using the y data
111:06 - range: y minimum and Y maximum. The AddColors
function was implemented in the last project.
111:13 - If you want to add colormap data to the other
direction, for example, the x direction you
111:19 - should use x data range and also P zero
here inside should be zero, this is the x component.
111:27 - Similarly, we add the colormap data to the
other three vertices. And finally, we add this
111:35 - colormap data to these two triangles. These have
six vertices, they have a different colormap on
111:42 - different vertices. Finally, create-quad function
returns vertex, normal, and color for this unit grid.
111:51 - This create-quad function here only creates
the data for single unit grid. Now, we need
111:59 - to create a data for the entire surface. We need
to add a new function called simple surface data.
112:09 - This is a function that creates the data for entire
surface. This function takes f here this input
112:18 - argument. F is a math function that describes the
3d Simple surface. We then define the data range
112:26 - in the x z plane use a minimum and a maximum value
for the x and the Z. Next two input parameters
112:36 - nx and nz represent the grid divisions along
the x and z directions. Here the scale is a global
112:46 - scaling parameter used in the normalize
point function. The scale-y parameter is
112:53 - used to control the y value height relative
to the X and Z value, that is, the scale-y parameter
113:02 - here controls aspect ratio of our surface plot.
Inside this method, we first define the size
113:11 - of a unit grid, you can see  dx and dz. We
then calculate the vertex position on our surface by
113:20 - calling the function f. You can see we use this for
loop inside that we call the F(X,Z) function. This
113:29 - describes our surface. Here we also calculate y value
range: y minimum 1, y maximum
113:37 - one. Next, we reset the y value range
usng the scale-y parameter here
113:44 - next, we normalize vertex position by calling
the normalized point using the value range for X
113:54 - y, and Z, and also the scale. Inside this
double for loop, we first define the unit cell
114:03 - P zero p one p two and p three. This is a unit
cell. Then call the Create-Quad function
114:10 - to get thevertex position, normal vector, and
colormap data for this unit grid. So this
114:18 - vertex normal and color for our surface.
Finally, this simple surface data function
114:26 - returns the vertex data, normal data, and
colormap data. Now we finished programming
114:34 - for 3d Simple surfaces in this project. In
next project, we will use this framework
114:42 - to create a 3d Sinc surface with both the lighting
effect and the colormap. In the last two projects,
114:52 - we discussed the colormap model and simple
3d surface construction. In this project, I will
115:01 - explain how to use the colormap model
and the simple surface data function to create a
115:07 - 3d sinc surface as shown here. This is a pretty
sinc surface we want to create in this project.
115:16 - You can download the source code used in this
project from this GitHub repository link.
115:23 - This specific commit version is used for
this project. Now start with Visual Studio
115:30 - Code and open our project 9 that we built
in the last section. Here is the code
115:40 - used in the last project. First, we need to
make some changes to the index.html file
115:49 - From the dist folder, open index dot
HTML file. Here we need to change
115:58 - this h1 title to Sinc surface and then
we need to change the parameter here.
116:08 - You can see here we have two_ sided_ light
parameter that controls whether we want
116:15 - to apply the lighting effect to one side or two
sides of the surface. Here is a dropdown menu
116:24 - for the colormap that contains 11 colormap
names we defined in the colormap-data.ts file,
116:33 - so you can select different colormap for our
surface from this dropdown. Here is the scale parameter
116:41 - that lets you set the default size and aspect ratio
for the sinc surface. Now we can save this file
116:52 - Now let's define our sinc function. Add a new
TypeScript file called the math-func.ts file
117:00 - to the src folder. Src folder, add
a new file called the math-func.ts file.
117:09 - Here is a definition of the sinc
function. Here r is defined as the square root
117:16 - x square plus z square. This function equals
to sign R or R if r not equal to zero. Otherwise
117:24 - if it equal to zero this function is equal to one.
In fact, this function is the Fourier transform
117:31 - of a rectangular function. Now we add
some code here to define this function.
117:39 - You can see this function is very simple:
it takes x and z and center parameters here as
117:47 - its input arguments. The center parameter here lets
you set the location of our sinc surface. Inside
117:57 - this function we define r using this formula
sas shown here. So here let y, r
118:03 - equals to zero, this function equal to one, otherwise
it equals to sin r  over r. This function returns a
118:12 - vector three point on the sinc surface
here. Now we can save this file and close it. Next
118:20 - we need to make some changes to the main.ts
file. From src folder open main.ts file and
118:28 - now we need to replace the code with the new code.
Since most code for render pipeline and
118:39 - render pass have been already included in the surface.ts
file. So the main.ts file here becomes very simple
118:47 - Here we first introduce the simple
surface data from surface-data.ts file,
118:54 - and then we introduce the sinc function from
math-func.ts file. Next from surface.ts file
119:03 - we introduce create surface with Colormap
and also introduce the light inputs interface
119:11 - from the surface.ts file. Next, we create a new
function called Create surface. This function takes
119:21 - light inputs, is_animation, and the colormap
name, and scale, and scale-y as its input
119:28 - arguments. Inside this function, we call the simple
surface data with, you see here, the sinc function
119:37 - as the input argument. From this simple surface
data method, we can get the vertex position,
119:46 - normal vector, and colormap data,
we then call create surface with
119:52 - colormap function to create our sinc surface with
lighting and colormap effects. Here we define
120:01 - the default input parameters and then we
call create surface function to create a 3d
120:09 - sinc surface with default lighting
and colormap effects. This part
120:16 - of the code allows the user to recreate the
sinc surface with different input parameters.
120:23 - Here, this code allows the user to select the
different colormap from the dropdown menu.
120:30 - Now we've finished the modification to the main.ts
file. Okay, save this file. Now we can run the
120:38 - following command in the terminal window to bundle
our typescript code. Open a terminal window
120:46 - and run the command: npm run prod to
bundle our TypeScript code in production mode,
120:56 - Okay, the bundle file is created successfully. Now
we can click this go live link to open Chrome
121:05 - Canary to view our sinc surface. Click
this link. Okay, here is our sinc surface
121:14 - with default two_side_lighting and jet color
map displayed on this page. Now let's check what
121:22 - happens if we use one side lighting. So we use
camera control. This is one side and we set it to zero.
121:33 - You see, you see the back there is no diffusion
and specular light in the back. But the only
121:39 - have very weak ambient light here,
If we set it to one you can see the back
121:49 - the difference the one side and two
side. You have light on the back. Here the scale
121:55 - sets the default size of surface. For example,
we change it to one, you get a smaller surface here
122:03 - change it to 3, you will get a bigger surface.
So they can the default size of the
122:09 - surface. We go back to 2. And here the scale y
lets you control the aspect ratio. For example,
122:17 - we set it to zero, you get a taller surface.
And change it to the 0.5 you get
122:26 - a shorter and fatter surface. We change it
to 0.3, so you can use this parameter
122:32 - to control the aspect ratio. Next we can
change the colormap from this dropdown
122:39 - menu. For example, antum, bone, cool, Cooper
gray, hot, hsv, spring, summer
122:59 - and winter. So you can get different
colormap for our surface. You can see
123:07 - that in WebGPU, we can easily create a
beautiful 3d surface with lighting and color
123:15 - map effect. You can create your own surfaces
by simply providing your own math functions. So now
123:23 - we've completed our project 10. You can use WebGPU to
create advanced graphics in your web applications.
123:32 - Here are some more examples from my recently
published book "Practical WebGPU Graphics"
123:40 - Here are some parametric 3d surface examples.
They all look beautiful because they have both
123:47 - colormap and lighting effects as we did for the
simple 3d sinc surface in our projects. Here are
123:56 - texture map examples on 3d objects. Texture plays
an important role in 3d graphics. The modern
124:05 - GPUs have support for image texture built-in on
the hardware level. Texture mapping on 3d object
124:14 - provides more interesting and realistic look.
These examples demonstrate that you can do texture
124:22 - mapping on sphere, cylinder, and 3d surface.
You can also use multiple textures on 3d object.
124:30 - You can see here: each face of our 3d Cube has a
different image texture. Here are examples of
124:39 - domain coloring for functions with complex
variables. These beautiful pictures are
124:46 - drawing pixel by pixel based on bitmap
render, which is a computation intensive process.
124:54 - Here we perform all calculations and rendering
directly in GPU, which makes it possible for
125:02 - our real time animation here. This is impossible
in CPU. These are 3d fratal examples. They provide
125:12 - more structures than 2d fractals. Again, this
nice pictures can be only create in GPU. In CPU,
125:23 - it is too slow and impossible to have this
real time animation. Here are some example for
125:36 - large particle systems. This is a compute
boids and we try to simulate the flocking
125:43 - behavior of birds. This picture shows
particle kinematics with collision to the wall.
125:50 - This shows three mass centers that
attract particles. Here we use Compute shader
125:56 - to perform physics related computations and update
the positions and velocities on each frame. This
126:06 - is only possible in GPU for simulating this large
particle system in real time. If you want to learn
126:13 - how to create these pictures and particle
systems, please visit my website at Dr. Xu
126:21 - dotnet.com and my YouTube channel at Practical
Programming with Dr. Xu. Thank you for watching

Cleaned transcript:

Dr. Jack Xu is the author of many WebGPU books. In this course, he will teach you the basics of WebGPU. Hi, I'm Jack. From this video, you will learn the basics of WebGPU Graphics Programming. By building ten separate WebGPU projects. Each project will be created on top of the previous one from simple primitives to complicated 3d graphics. Our end product will be a beautiful 3D Sinc surface, as shown here. We will create these WebGPU projects from scratch and show you how to add 3D graphics with GPU acceleration to your web applications. Here, I want to thank Beau and FreeCodeCamp for publishing this video. FreeCodeCamp is a great resource for the programming community. And thank you for all that you do. Before getting into this video, I'd like to quickly tell you a little bit about my background. I got my PhD in theoretical physics and have over 25 years of programming experience in C, C++, C# .NET and web development. I have published over 20 books about practical programming on a variety of topics, including graphics programming, machine learning, quantitative finance, numerical computation methods, and web applications. Recently, I created a YouTube channel Practical programming based on my books. In this channel, I will present several step by step video series, in which I will emphasize the usefulness of example code to real world applications. The first video series is about WebGPU Graphics Programming that is based on my recently published book, "Practical WebGPU Graphics". The projects presented here are also selected from this book. My channel Practical Programming with Dr. Xu is new and I plan to update it every week. I would greatly appreciate if you can check out my channel and subscribe it. Now let's go to WebGPU Graphics Programming. Now let's start project one that will set up a development environment. The source code used in this project can be downloaded from GitHub link here, This project uses this specific version. So now what is WebGPU? WebGPU is the next generation graphics API. It is future web standard for graphics and compute WebGPU will provide modern 3d graphics and computation capability with GPU acceleration. In this video, we will use the following development tools to build our WebGPU applications. First is the Visual Studio code, node.js. We use a TypeScript as our programming language, and we'll use webpack as our modular bundler. In order to run WebGPU applications, currently, we have three options. The first one is to use a Chrome Canary to test our WebGPU applications. The other option is to run WebGPU applications on a regular Crome through origin trial. Starting from Chrome 94, Web GPU is available as the origin trial in the regular Chrome. You'll need to register to origin trials and request the token for your origin. The third option is that Chome will officially support WebGPU in May of this year. So after this May, you will be able to run your WebGPU applications on regular Chrome. In this video series, I will use Chrome Canary to test our WebGPU projects. Here, I suppose you already installed Visual Studio code node.js, and also installed TypeScript globally on your machine. Let's start programming. Now let's open a command prompt window and let's make a folder make directory gpuapp and cd into it. And then, we are going to run this command npm init y to create a package.json file. Okay, this file will store our dependencies used in this project. Now, we are going to install webpack and its commandline interface using this command npm install cli. WebPack is modular bundler that Bundles relevant files together to generate static assets that can be used by the web applications. Here we use webpack, mainly for transpiling TypeScript files, but here we don't use it to bundle the other types of files, such as HTML and image files. Now instalation is complicated. At this point, we are going to start Visual Studio Code with a command code period. Here is Visual Studio Code interface, you can see we only have a package.json file, but not much happening here. Now open a terminal window by pressing Ctrl+J key. Now, let's install some dependency packages. First, we will install jQuery and corresponding type package. npm install jQuery. We will use jQuery to manipulate the DOM elements in our applications. Next, we want to install styling CSS and ts loaders. In case if our project contains styling sheet file, we need to use style loader and CSS loader to bundle it. In order to create TypeScript modules in our project, We also need to install TypeScript locally. npm install TypeScript locally. We already installed it globally before. Now, opening the package.json file. You can see the script section here only contains the test attribute, that is not very useful. Here we are going to replace this section with the code. Here the code allows you to run WebPack in different modes like development, production and watch mode. Now we can see with this file and close it. Now all the installed packages are going to be stored in the node_modules folder. Here the package_lock.json file is automatically generated for any npm operations. So we don't need to do anything with it. Next step is to initialize TypeScript file with the command tsc init. This operation created a configuration file for TypeScript called the ts_config here. Now, we open this configuration file and we need to replace its content You can see here we set the target to the es6 also note we define the root directory as src and output the directory as dist, distribution folder. Now save this file and close it. We then create some folder structures for our project. First, let's create the src folder. So here we create a folder called src. We will store all the source files in this folder. Next, we create a dist folder, which will contain all files that will be uploaded to the web server for running our applications. In this folder, there is at least one HTML file. let's add an index html file to this folder. Add another file index.html, and add some code to it. You can see here we add the title WebGPU project. Here h1 title check whether your browser supports WebGPU or not. We also have h2 DOM elements here and set its id equal to id_gpu_check. This h2 element will be used to display text string that will be returned by a typescript function which will be created later. Now we are about to finish the development environment setup, but we have missed a key step, that is, we need to install the WebGPU package. WebGPU working group has created a type definition for WebGPU standard. This package matches the workinprogress WebGPU API, which is currently not very stable. So it can change almost weekly. So be careful to use it. Anyway, let's go ahead to install it with this command. So here we specify the version 0.1.12, this is the latest version, we install it called webgpu types. Because the WebGPU is not backward compatible, so you better use the same version to run our project. Otherwise, you need to do some modification to your code to run application. So I suggest you use the same version here 0.1.12. After installation we need to configure it. Open TypeScript configuration file. You can see here we already configured it. We add WebGPU types and node in the types section here you can see here we have node type. So we need to install it use this command to install this node type. Okay, the WebGPU is now available for our project. Now let's add some TypeScript file to our project. First, added a new file called helper.ts file to the src folder. So we added a new file called helper.ts file and add some code to it. You can see here we use this navigator.gpu variable to check the GPU availability. This variable is defined in the web GPU API. So you can see here we use the fat arrow to define the typescript function. Next, we need to create an entry point file called main.ts file. Okay, save this file first. And close it. Let's add another new file to the src folder called the main.ts file. Now add some code to this file, we just import the jQuery and also import the checkWebGPU function from helper.ts file, just implemented. So here we define id_gpu_check to display the string returned by the checkWebGPU function. Here the id represents h2 DOM element defined in the index.html file, make sure this ID is the same as ID defined in that file. Next, we also need to create a webpack configuration file. Now add a new file to the root directory webpack_config.js file. You can see here we define a bundle output directory as dist and an entry point is named main here. The main, src main.ts File. Note that the output file has always with bundle.js extension. Now we can save this file and close it. Now we can run the following command from terminal window to bundle our TypeScript file npm run prod in the production mode Okay finished. You can check the bundle file in the dist folder. You can see here we have several main bundle.js files. This is our bundle file, this is license file, this is source map file. This main.bundle.js file has been referred in the index.html file. You can see here, this is the main.bundle.js This bundle file is generated by webpack. In order to test our application we also need a server. Here we will use live server extension. Click on the extension link search for live server. This first one is live server. Here I already installed it. Otherwise we have install link here. If you haven't installed it, just click install button to install the live server. So I suppose you already installed it. Now we can open index.html file, right click anywhere open with live server, click this link. This will open our default browser to display the web page. Unfortunately, if your default browser is regular Chrome, it will displays the message your current browser does not support WebGPU. So in order to run WebGPU applications here we will install Chrome Canary. So here search for Chrome Canary. Click it. So this nightly build for developer. Just download Chrome Canary and install it. I already installed this chrome Canary on my machine. Please remember the location where your Chrome Canary is installed. Okay, I suppose you already installed it. Let's go back to Visual Studio Code. We want to change the default browser to a Chrome Canary, which will be easy to test our WebGPU applications. So press Ctrl+Shift+P and type the preference opening setting here. So open this file. Here, you need to add these three lines here. live server setting don't so info message. I set it to true, this doesn't matter. I also set the root directory as dist because our index.html file is in this directory. So this the ones custom browser command line here defines the default browser. So you need to replace this code. So you need to copy your Chrome Canary exe file location and paste it here, you must change it to match your situation. Now we can save this file and close it. Now start the Chrome Canary and type Chrome flags and search for WebGPU. Here, enable it here I already enabled it. If you're not, you just need to enable it to make WebGPU available for our Chrome Canary. Okay, we can close it. Okay finally we can test our development environment for WebGPU applications. Here we have a link on the status bar area you can see go live link. This time it will start Chrome Canary browser. Click this link. You can see Greate your current browser supports WebGPU, Congratulations, we have successfully set up the development environment for WebGPU applications. Now we completed our first WebGPU project that shows how to set up development environment. This is project 2 of this video series. Here I will explain how to create a colorful triangle in WebGPU as shown here. This is a colorful triangle. This project is based on our first project. You can open project one directly from Visual Studio Code or clone the source code from the GitHub repository. git clone. This is a GitHub link for the project one source code. I also put this project 2 source code into the GitHub repository. Here is the link. So you can download the source code used in this project from this link. Now we will open the project one directly from Visual Studio Code. Here we first add a CSS file called the site.css to the src folder, src folder, add a new file called site.css. Type some code into this file then save and close it. Just like WebGL, The first thing we need to do to use WebGPU for graphics rendering is to create a canvas element. From the dist folder, open index.html file. Here we need to change h2 elements from here. Here we define canvas element with ID canvaswebgpu. Later we will create WebGPU Graphics on this canvas. The WebGPU programming consists of two parts one is the control code written in typescript or javascript, and the other part is shader code run on the GPU. Here we will use the official shader language for WebGPU called WGSL, which is short for WebGPU Shading Language. To take advantage of the code highlight in VS Code, we will convert the shader code with wgsl extension into typescriptlike module. To do this, we need to install a shader loader for webpack. Now open a new terminal window. Run the following command to install the shader loader called npm install tsshaderloader This loader was originally created for loading GLSL shader code. But we will make it works for WGSL here. Next, open WebPack configuration.js file and add the following code to it. Here the file with wgsl, glsl, vs, vs means vertex shader, fs, fs means fragment shader. So the file with these extensions will be treated as a shader code by this shader loader. We then need to install WGSL extension from VS Code. Click this extension link, search for WGSL. Here is first one wgsL provides syntax highlight I already installed this module. So you can see here the code highlight. In this case, it is easy to write shader code using this extension. So if you haven't installed it, so click this install button to install it. Next we need to add some type declarations for our shader. Add a new sub folder to the src folder. A new folder called types. Then add a new file called shader.d.ts file. Inside this file we add some contents to this file. Here we declare module. The file with wgsl, glsl, vs or fs we declare this file as module. So right now we can save this file and close it. Now we can write our shader code. In the src folder add a new file called a shader.wgsl file. Then add some code From this code, you can see that there are two shaders here. One is vertext shader and another is fragment shader. We have shader function for vertex called vs_main function. vs means vertex shader function. Here we have fs main fragment shader main function. The vertex shader takes vertex data including word position, color, and texture as the input the output is the position in the clip coordinates. While the other outputs such as color and texture will be passed to the fragment shader. These values will then be interpolated over the fragments to produce a smooth color gradient. Now back to our example. In the vertex shader, we first define the output structure here, that contains builtin position and the output color called vColor. Inside the vs_main function, we first define three vertices of our triangle using the floating point vec2 vector. So we have three vertices. We also specify a colors for each vertex using a floating point vec3 vector. You can see the first one is 1 0 0. This means red. The second color is 0, 1, 0 the green, the final one is 0, 0, 1, which is a blue. So we define different color for each vortex. Next, we define output using the output structure here we thenconvert 2D vector of the position into a vec4, the four dimensional vector we put a z component at zero and a w component as one. Similarly we convert 3d vector for the color into vec4 here. The one represents transparency of the color In the fragment shader, we take the output color from the vertex shader as an input and return it as the fragment color. Okay, this is the shader we will use when creating our colorful triangle in WebGPU. Now we can save this file and close it. Next, we'll use TypeScript to write WebGPU control code. Open the main.ts file. And replace is content with the following new code. Here we first introduce the checkWebGPU function from helper.js File. Then, from shader.wgsl file, we introduce our shader from this file, we also import the site.css file. Here we will use it to configure our web page layout. Next, we create a new function called Create triangle. This function must be async, you can see async function because WebGPU API itself is async. Inside this function we first check whether your browser supports WebGPU or not. In WebGPU, we can access the GPU by calling the request adapter here. Once we have this adapter we can call request device method to get GPU device. This device provides a context to work with the hardware. And an interface to create GPU objects, such as buffer and texture. We can also run commands on this GPU device As with WebGL, we need a context for our canvas element that will be used to display graphics. Here we use the canvas element to request web GPU context. You can see here get content by the name webgpu. This is a keyword used by WebGPU to create context. We then configure this context with the device and the format. Next, we define the render pipeline by calling the Create render pipeline method. Inside this pipeline we have several attributes. The first one is a vertex, where we'll assign the shader to the module. Here with an entry point as the vs_main function this function is created in our shader file. Similarly for the fragment attribute we assign the shader to the module an entry point as fs_main function. The another required attribute is a primitive here, this attribute contains a field called the topology. We set it to trianglelist because we want to create a triangle in this project. Next we create the command encoder and render pass by calling the beginning render pass. This render pass accepts a parameter of the type GPU render pass descriptor as the render pass option. Here we only use color attachments that is used to store image information. In our example, it stores the background color in the clearValue area we have another field called the loadValue. This field will be removed from WebGPU in next version, so we don't use it here. Next, we'll assign the pipeline to our render pass and draw our triangle by calling the draw method here. we call the end function to finish current render pass, meaning that no more instructions are sent to the GPU. Finally, we submit all instructions to the queue of the GPU device for execution. After running the command, our triangle will be written to the GPU context and displayed on our canvas element. Now we finished the implementation of our create triangle method. This method provides basic procedure to create WebGPU applications, that is, first we initialize WebGPU API, create shader program, set up render pipeline and build render pass, call draw function, submit instructions to the GPU for execution. Finally, we call the Create triangle function to generate our triangle. Up to now we have finished our programming for this project. We can now run the following command on the terminal window npm run prod to bundle our typescript code in production model. Okay finished. Our Bundle file is created successfully. Now we can click the golive link from here to open Chrome Canary to view our triangle. Click this link. Here is our colorful triangle. You can see here is red, green and blue. From one vertex to another you can see the color changes smoothly because WebGPU interpolates the color internally which gives a smooth color gradient. Now we have completed project 2. This is a project three we will create a colorful square using GPU buffer. In the last project, we create a colorful triangle in WebGPU by writing the vertex and color data directly in the shader code. This approach is only possible for creating very simple shapes. It is almost impossible to use this direct approach for creating complex 3d graphics with different colors and textures. In this project, I will introduce GPU buffer and show you how to use it to store vertex and color information. In this example, we will create a colorful square to explain the concept of the GPU buffer. Here we will introduce several new concepts that will be the foundation to create complex 3d graphics objects. GPU buffer in Web GPU represents a block of memory that can be used to store data for GPU operations. In this project, I will show you how to use the GPU buffer to store vertex position and color data. You can download the source code used in this project from this GitHub repository link. You can see we have specific commitment version, this version is specific for our current project. Now let's start Visual Studio Code and open project 2 that we built in the last video. Okay, here's the project 2 we used in the last video. Now let's first make some changes to the index,html file. From the dist folder, open index.html file. Now we need to change here the h1 title to Create Square Using GPU Buffer. So we don't need to change the other part of the code. So we can save this file. Next from the src folder, open the helper.ts file. Now we need to add a new function called the InitGPU to this helper file. So we need to add some code here. You can see we place all WebGPU initialization code to this function, initGPU. So this can avoid code duplication because initialization will be same for all WebGPU projects. This function is an async function, and it returns the device, canvas, format, and context, which will be used when we create a render pipeline and render pass. We also want to add another function called Create GPU buffer to this helper file. So we need to add another function here called Create GPU buffer. You can see this function takes three inputs the device, the data, and the usage flag. Here we set the default usage flag to vertex or copy destination. Here you can see we call the device create buffer method to create a GPU buffer and a set the size as the data the byte length to allocate the room for the buffer data. The other attribute here mapped at creation, we set this attribute to true. This means that the GPU buffer is owned by CPU, which is accessible in Read Write from typescript code. Once the GPU buffer is mapped, the application can ask for access to range of content with get, for example, here get mapped range method and set it to the data to be stored in the buffer. Pleace note the mapped GPU buffer cannot be directly used by the GPU and it must be unmaped by calling the upmap method. Here, our create GPU buffer method returns unmapped buffer, which can be used by the GPU. So be careful here about the GPU buffer mapped and unmapped states. Now we can save this file and close it. Next we need to make changes to the main.ts file from src folder open the main.ts file. We need to replace is content with the new code. You can see here we first introduce initGPU and create GPU buffer from helper.ts file and then we introduce the shader. The shader code will be discussed in a moment. Inside the Create square method, we first call the initGPU method to create a GPU object and then get the device from it. Next, we define the vertex data and the color data here. You can see this is our vertex data, color data. Here is vertex coordinates of our square a, b, c, d. so, the coordinates like here a minus point five minus one and this is a, b, d, we have divide this square into two triangles a, b, d, d, b, c, the vertices of each triangle must be arranged in the counterclockwise order. You can see here abd, dbc. We also assign a color to each vertex. You can see a red b green c blue and d yellow. You can see here it for a is red for B is green, and so so. These data are consistent with the coordinates, shown in this figure. Of course, you can also divide the two triangle in other way around. For example, abc, cda. So you draw the diagonal this way. So get another two triangles. This doesn't metter. Next, we create a two buffers by calling the Creator GPU buffer one is the vertex buffer and another is color buffer. Next, we create a pipeline by calling the Create render pipeline method. The new code we add here is for the buffer attribute. Before we don't have this buffer attribute. Now, it is a two element array, the first element is for the vertex position. The second is for the color. You can see for the vertex position we have arrayStride we set it to eight since each vertex is represented using two floating 32 elements. Each floating 32 number requires four bytes while for the color it is represented for each vertex using three floating 32 elements. So, its array stride is set to 12. The another important parameter here is shader location. For the vertex position we set the shader location to zero, and for the color we set the shader location to one The other part of the code is very similar to that used in the previous project. Here we set the primitive topology to the triangle list. One new thing here is that after setting the pipeline to the render pass here, we also need to set the vertex buffer to the render pass using vertex buffer and color buffer. You can see here we set vertex buffer as zero slop is vertex buffer. And at slot one, we set the color buffer. In the draw function here, we set the number of vertices to six because we have two triangles now. Now we finished the modification to this file. Now we can save this file. Next we need to make modifications to our shader code. from the src folder, open shader.wgsl file. Here we need to replace his context with new code. Here you can see we didn't define any vertices and colors in this shader code because we store these data in GPU buffer. Instead we introduce two inputs one is at location zero pos, and other is the color at location one, which must be consistent with the shader location definition in the pipeline in our main.ts file, because we define the shader location as zero you can remember for the position and the shader location is one for the color. So here position is zero. And here color location is one. This is how we set up the relationship between the vertex shader and the GPU buffers. Note we define them here as a vec4 f32. For the color same thing vec4 f32. They look like they are not very consistent with the data stored in the vertex and color buffers where the for the position, it is a 2d array and for the color, it is 3d array. This is because WGSL shader is smart enough to automatically convert them into vec4 types by adding zero to the z component and one for the w component. You can see here we also define two outputs one is position, one is vColor using a structure. Inside this vs_main function, we process the position and color data by assigning the pos to the position and a color to the V color. Inside the fragment shader we introduce the vColor from the vertex shader and return as fragment color. Okay, this is the shader we will use when creating our square. We can now save this file and close it. Now we can run the following command in the terminal window to bundle our typescript code npm run prod to bundle our code in production mode. Okay, the bundle file is created successfully. Now we can click the Go Live link to open Chrome Canary to view our square. So click this go live link. Here is our colorful square with different vertex color. Here is red, green, blue and yellow. From one vertex to another you can see the color changes smoothly because WebGPU interpolates color internally which gives a smooth color gradient. Okay, now we finished the project three. In projects two and three, I demonstrated how to create a triangle and a square graphics in WebGPU applications. In fact, this two objects are flat and two dimensional. We have not created any real 3d graphics yet. In this project 4, I will explain how to create a 3d cube with distinct face colors. This is the first real 3d object we are going to create in a WebGPU application. You can download the source code used in this project from this GitHub repository link. Here we have specific version for this project. This example involves a lot of code and mathematics. So please get ready to digest it. In order to create a real 3d objects in WebGPU, you need to have the math background of 3d matrices and transformations. Since our computer screen is a two dimensional, it cannot directly display 3d Objects. To view 3d objects on a 2d screen, you have to project your object from 2d to 3d, that will involve a series of coordinate transformations. From here you can see that we define 3d object in the object coordinate system here. We then perform various transformations on the object, including scaling, translation and rotation. We call this transformation as a model transform. After this transformation, we convert an object in the object coordinates into the object in the world space. Next, the view transformation locates the viewer in World space and transform our 3d object into camera space that also called the eye coordinates. The purpose of the projection transform here is to define a view volume called view frustum, right here. This is a view frustum. This is used in two ways it determines how an object is a project on the screen. It also defines which portions of the object are clipped out of the final image. That is only the portion inside this frustum will be kept and anything outside of this frustum will be clipped out. Finally, we use the view port transform to convert the clip coordinates into the normalized device coordinates. In WebGPU. The viewport transform here is automatically performed in the shader code. Please note that like a WebGL, the WebGPU API does not provide any functions for working with transformation. In this project, we will use a JavaScript package called GL matrix to create 3d matrix operations and transformations. Since the 3d matrix and transformations are common for any computer graphics programming, here, I suppose you already have this math background, I will not spend any more time on this. Instead, I will concentrate on how to create 3d objecta in WebGPU applications. From this project, you will learn somehow important concepts in web GPU that is uniform buffer and binding group. We will use a uniform buffer to represent the transformation and projection matrices, and then use the binding group to pass the uniform buffers to the vertex shader. Now we start with our Visual Studio Code and open our project three that we built in the last section. Here is code we used in project three. First we need to install a new NPM package called GL matrix npm install gl matrix. We will use this package to perform matrix operations and 3d transformations Now let's make some changes to the index dot HTML file. From the dist folder, open index dot HTML file. Here we need to change the h1 title to Cube with Distinct face Color. So we don't need to change other parts of the code. Save this file. Here we are going to create our 3d cube. these are the coordinates our cube. From this diagram you can see there are eight vertices and six faces. if we each face has a different color each vertex will then have three different colors. For these vertices, For example, c we have the top face, front face, and the right face. Because different face a different color. So the vertex C can have three different colors depending on which face we talk about Here is the front face A, B C D. You can see A B C D here. As we did before, we can now divide this front face into two triangles. One is ABC, CDA, two triangles you can see here ABC and CBA. In this way, you can perform triangulization for the other faces. Next add a new TypeScript file to the src folder and add a new file vertex _data.ts file. Add the new code to this file Here is the positionn data for our cube this is front face, right face, and six faces This is the color data for the different faces. You can see front face we use blue color, right face is red, and back face is yellow and so on. So different face has a different color. All the position coordinates uand the color are consistent with this diagram shown here. Now we can seave this file and close it. Next we will make some changes to the shaderr code. From src folder open shader.wgsl file. Now we need to replace this content with the new code You can see this shader is different from that we used in project three because we need to incorporate the uniform buffers that store transformation and projection matrices Here we define a uniform structure and then use the binding group to pass the modelviewprojection matrix to the shader. Here the mvp matrix means model view projection matrix You can see here the variable type is uniform. Next we define the output structure as we did in the last project. We define position and vColor. This is the same as before. For the vs_main function we also define the two inputs one is the position one is the color, the same as the last project. And inside this function only difference is the position not only has the pos here but we also multiply by this model view projection matrix. So perform the transformation on this position. For the fragment shader, we still use vColor as input and return it as fragment color. This is the same as in the project three. Now we can save this file and close it Now we open the helper.ts file from src folder. Here we first need to import from glmatrix. We need to introduce the vector three, mat4, and then add a new method called Create View projection to this file. This method create view projection takes four input arguments first is the aspect ratio, the camera position, camera look at direction, and the camera up direction. Here we first create a view projection and view projection matrices. We then use a mat4.perspective and mat4.lookAt to create projection and view matrices. Next we use mat4.multiply method to combining projection matrix with view matrix to form our view projection matrix. This function returns the view matrix, projection matrix, and view projection, and the camera option. The camera option will be used in the next project when we discuss the camera control. Now, we need to add another function called the create transformation, which is used to construct the model matrix Here we first create three rotation matrices for rotation about X Y and Z axes We then create a translation and scaling matrices. Next we perform individual transformation for the input arguments here. Finally we combine all transformations together to form our final transform matrices which is our model matrices. Now save this file and close it Next we need to make some changes to the main.ts file. From the src folder open main.ts file. Here we need to replace is content with the new code. Here we first introduce some methods from a helper.ts file, including the newly created createTransforms, create view projection, we alsointroduce the cube data from the vertex_data.js file, as well as mat4 from the glmatrix library. Inside this create3d object method, the code is very similar to that we used in the project three. Here, we create the vertex buffer, you can see and the color buffer using a cube data positions and cube data colors. Here is pipeline code. Also similar to the project three. The difference here is array stride for the position is 12 instead of eight. Right now for our cube, we have x, y, z coordinates. So we have 12 here For our color, still 12 Because we have RGB elements. Here, we also set the shader location to zero for the position and the shader location is one for the color. The only difference in the pipeline here is we add this depth_stencil attribute. Here we set depthWriteEnabled to true to enable the deoth stencil testing. The depthstencil testing determines whether or not a giving pixel should be drawn. For 3d graphics. enabling depth stencil testing is very important. Otherwise, you may get unexpected results. The following code is new and specific to our 3d cube. Here we create a model view and a model view projection matrices by calling the CREATE VIEW projection method. We then create a uniform buffer for our mvp matrices. You can see here the usage, we set it as a uniform for our transformation. The size is 64 since our matrix is four by four, so we have 16 elements. Each element is a floating 32 number, so its size is 64. We then define the binding group by calling Create Binding group for this uniform buffer. Here we set the layout as a default pipeline get binding group layout zero. This means we use the binding group zero layout. In the entries attribute, we set the bindings to zero and set buffer here to the uniform buffer we just defined here. If we have a more uniform buffers, we can add the elements here. Put them into this entries array. In addition to the texture view from a context we also introduce the depth texture. Here we set the size as our canvas width and height. And the format that we use here is depth 24 Plus, you can also use other format. The usage here we set it to render_attachment. Now our render pass description, includes two parts one is the color attachments and other part is teh depth stencil attachment. The color attachments is the same as we use in the project three. The depth stencil attachment is specific for our 3d Cube. Next, we construct the model matrix by calling create transforms method defined in the helper.ts file. We then obtain Model View MVP matrix by multiplying the model matrix with the view projection matrix. So we got a final model view projection matrix and then we add this model view projection matrix to our uniform buffer by calling the write buffer method. You should already be familiar with the following code here. We just define render pass and a set vertex color buffer. But we also need to set the binding group here using uniform binding group which is needed to pass the uniform buffer to the vertex shader. Now, we finished the modification to the main.ts file. We can then save this file. Up to now we have finished all programming for this project. Then we can run the following command in the terminal window to bundle our code in production mode npm run prod. Okay, the bundle file is created successfully. Now we can click the Go Live link to open Chrome Canary to view our 3d Cube. Click this link. Here is our cube with distinct Face Color displayed on this page. Okay we have finished this project. This is project 5, in which we will discuss the animation and camera control. In the last project, I explained how to use the uniform buffer to create a 3d cube. In this project, I already illustrate how to animate that cube and how to use mouse to interact with the cube, you can download the source code used in this project from this GitHub repository link Here the specific commit version is used for this project. For animation we will use the JavaScript requestAnimationFrame function. This function can make changes to your screen in an efficient and optimized manner. Here is an example of using request animation frame. You can see here we loop over and over again using a recursion. Here we first define the DOM element in the UI and define a start number of zero here. Next, we create a count function that increases number by one. And then set it as a text content for the counter element. Inside this counter function we call request animation frame and pass the counter function itself as a callback function. This causes it to run again just before the next frame. Finally, we use the request animation frame function to start the animation. This is a basic usage of the request animation frame. For using the mouse to interact with cube, we will use the npm package called the 3d view controls. This package can be used to control the camera with your mouse you can then interact with 3d object by controlling the camera. Now let's start Visual Studio Code and open project four that we built in the last section. Here is the code used in project four. First we need to install JavaScript package called 3d view controls. Open terminal window and run npm install 3d view controls We will use this package for camera control. The package is already installed. Now let's make some changes to index dot HTML file. From the dist folder, open this file. First we need to change h1 title to Animation and Camera control. We also need to add two radio buttons that let you select whether you want animation or camera control here You can see the radio button. Okay now we can save this file. The shader code in this example is the same as that used in the project 4. So we don't need to make any change to the shader code. Now from the src folder, open helper.ts file. Here we need to add a new method called Create Animation. We need to add create_animation here. This is code for create animation. method. Here the draw augments is a callback function. Here we want to animate the rotation around X, Y and Z axes using the step function here, the step function. The Create Animation has a boolean argument here is_animation that controls whether you want to animate the object or not. Now we can save this file and close it. Next we need to make some changes to the main.ts file. Open main.ts file. We need to replace the content with the your code. This code actually is very similar to the code used in the last project. Here, in addition to the other function introduced from helper.ts file, we also introduce Create Animation functionwe just created. Note that we use here require to introduce 3d view controls module, instead of the import. This is because the 3d view controls package was created using old modules approach. So we have to use require instead of import here. Here we define create camera from this package. In the create 3d object method, we introduce an input argument is_animation, and we set its default value to true here. Here is the initialization code and pipeline code are the same as those used in the last project, we don't need to make any change to this code. In the creating a uniform data section we define vmatrix, this is view matrix is a new matrix. Next, we need to add rotation and camera. Rotation we define using vec3 and camera we define createcamera here. we have arguments canvas and camera option, we don't need to make any changes to the code for creating a uniform buffer here and a uniform binding group. This is render pass description is also the same as that used in the last project, here, we put this render pass related code inside the new function called draw function. This draw function is defined as a callback function for our animation. You can see here if is_animation is not true we use the camera to generate our view matrix. And then multiply projection matrix with this view matrix to form our view projection matrix. The rest code of the draw method is the same as that we used in the last of the project. Here we start animation by calling the Create Animation function. You can see we have callback function, using the draw function defined here. Finally, we need to add a radio button selection here, you can see that if the radio checked value is equal to animation, we run animation. Otherwise, we use the camera control. Now we finished the modification to the main.ts file. So we can save this file now. Up to now we have finished the all programming. Now we can run the following command in the terminal window npm run prod to bundle our TypeScript code. Okay, the bundle file is created successfully. Now we can click the Go Live link to open Chrome Canary to view our 3d Cube. Click this go live link. You can see the animated cube on this page. It rotates continuously because we call the animation mode. Now you can click on this camera control radio button here, then you can use the left mouse button to rotate the cube and the use right button to move it around. And you can also use mouse wheel to zoom in, zoom out the cube. So you can use the mouse to interact with the cube. So now with how completed this project. In the previous several projects, I explained how to create some 2d and 3d objects. However, when rendering these graphics objects with solid colors, you may find that the image looks flat and it is failed to illustrate the 3d nature of the objects. This is because we neglect interaction between light and surface in our objects. Lighting helps provide visual effect to a scene so that it looks more realistic. Lighting is one of the most important factors for creating real world shaded 3d graphics objects. However, WebGPU does not provide much builtin features on lighting. It just runs two functions vertex and fragment shaders. If you want the lighting effect in your 3d scene, you have to create the lighting model yourself. In this project, I will build a simple lighting model in WebGPU and use it to simulate the light source. Here you can download the source code used in this project from this GitHub repository link. Here The specific commit version is used for this project. Here I will discuss three types of light sources, ambient light, diffuse light, and specular light. The ambient light is the light that illuminates all objects uniformly regardless of their location or orientation. It is a global illumination in an environment. The diffuse light and specular light reflection depends on the angle of the light to a surface. When the light hits the surface., the diffuse reflection occurs in all directions, as shown here, while the specular light reflection occurs in a single direction. as shown here. Here shows the light reflection on a torus surface. On the left here, light is only from ambient light, you can see this torus looks very flat. Here in the center shows light from both ambient plus the diffuse light. You can see the 3d feature already shows here. On the right here, we have three light sources ambient plus diffuse and plus specular lights. You can see this torus looks more realistic. So, this is the light model we want to build in this project. The light reflection depends on the angle at which the light hits the surface. The angle is essential to the diffuse light and the specular light. This angle is always associated with surface normal that is a perpendicular to the surface. To build the light model, we need to calculate the surface normal based on the direction in which the surface is facing. Since the surface of 3d object can be curved, it can face different directions at different vertex. So, a normal vector is usually different for different vertex that is normal vector is always associated with a particular point on the surface. Here shows normal vector for 3d cube. Here, the front face is pointing towards the screen. So its normal vector is 001. The right face is facing direction to the right. So its normal is 1 00. And the top face is facing towards the top. So, its normal vector is 010. So, similarly, you can specify the normal vectors for the back left and the bottom faces. Note that the length of the normal vectors should be always one, that is, we need to normalize the normal vectors. For a general quadrilateral A B C, D, its 4 vertices may not in the same plane, we can divide it into two triangles, A, B, C and C D A. We can calculate its surface normal in two steps. First, we calculate the weighted normal for each triangle, which is a weighted by the area of that triangle. The triangle with larger area will get more weight. As we know the area of a triangle is proportional to its cross product of two sides. For example, here, for triangle A, B, C, we have a cross product B, C, and CA, b c and a c. Similarly for the triangle C D A, we have a cross product D A and A C. The next step is to calculate the surface normal from the weighted triangle normals, that is, the surface normal equals the sum of two weighted normals, and we can then normalize it. So we get this quadrilateral normal equals normalize this two cross products. The sum of this one triangle plus another one. So, we can combine these two terms to form one term equals cross product of C A and D B. So, this means the surface normal for this general quadrilateral just equals to the cross product of C, A, and d b. Just equal to the cross product of these two diagonal vectors. We will use this formula to calculate the light model for our 3d surfaces. Once we have calculated the normal vector, we can use it to compute the light intensity. For diffuse light, its intensity is proportional to the cosine between the light vector here and a normal vector N. You can see here the cosine alpha equals normalized L dot N here. If we define the Id, the diffuse intensity, Kd the diffuse material property, so we can have this formula the diffuse intensity equals Kd times maximum cosine alpha and 0. Here the maximum is used to avoid the negative value of the cosine alpha here, we only take the positive value. We can also add the ambient light to the diffuse light intensity like this. Here Ia is ambient light intensity. So the total light intensity is maxima Kd and here is the Ia. So we add the ambient intensity to this diffuse light like this. For specular light, we have two models one is Phong model, this model is very simple. So the Is, the intensity of the specular light, equals to Ks, Ks is speclar material property, and s is the shininess of the surface or roughness, the V is the view direction, R is reflection direction. One issue with Phong model is that the angle between the view direction and reflection direction has to be less than 90 degrees in order for the specular Phong term to contribute. We can use the BlinnPhong model to address this issue. BlinnPhong model uses a different set of vectors for its computation, based on the half angle vector. Here's a half angle vector, you can see L is the light direction, N is normal vector, V is view direction, H is the half angle vector defined L is the light direction plus the view direction. The view direction plus the light direction gives H here. The formula in BlinnPhong model becomes N dot H instead of V dot R here. So the angle between this N and H is always less than 90 degrees. So this N dot H always gives positive value. So we will use this BlinnPhong mode in our calculation for the specular light. Now let's go to the programming part. We started with Visual Studio Code and open project five that we built in the last section. Here is the code for our project file. Now let's implement the light model in the shader code. From src folder open the shader.wgsl. We need to replace its content with the new code. Here we first define the uniform structure here that contains viewprojection matrix, the model matrix and normal matrix. Here the normal matrix is transformation matrix for the normal vector data. When applying a transform to a surface, we need to derive the normal vector for the resulting surface from original normal vectors. In order to transform the normal vector correctly, we don't simply multiply them by the same matrix used to transform our object, but we need to multiply them by the transpose of the inverse of that matrix. Since the current WG SL version does not implement the matrix inverse function yet, so we have to make this transpose of the inverse of the transform matrix in TypeScript code, and then pass it as a uniform matrix as shown here, normal matrix. We then define output structure that contains three variables builtin position, the vposition the position after some transformation, we call it the vposition and the v_normal, which is a normal vector after transformation. In our vs main function, it contains two input arguments one is vertex position called the pos and one is normal vecctor data. Inside this function, we only perform the model transform on the position. This gives us vposition here because it is used in the light calculation. The vnormal is a result of normal vector data multiplied by the normal transform matrix here. And the default position here is obtained by performing modelview and projection transform on the vertex data. For the fragment shader we first introduce the uniform structure called FragUniform that contains two variables one is light position of light vector, and the other one is eye position. Or view vector. It also has another structure called LightUniform. Here contains parameters for our light model. Here's a light color and a specular color. The specular light can have its own color, and the other parameters contain ambient intensity, diffuse intensity, specular intensity and the specular shininess. Here the fs_main function takes v_position and v_normal as inputs. Inside this function, we calculate the light model. Here is the diffuse. Just use N dot L, the cosine alpha. For the specular light, we use Blinn Phong model. This is N dot H. The output color from fragment shader will be weighted, you can see, by ambient, diffuse, and specular light. Its combination gives us the final color. So, this is the shader we are going to use to calculate the light model. Now we can save this file and close it. Next in the src folder, we will add a new type script file called light.ts. Add a new file light.ts file. Here is the code for this file. Here we first create an interface named light inputs that contains parameters used for calculating light model. This code also contains a new function createshapewithlight which has four input arguments one is the vertex data, normal data, and light input, and the is animation parameters. Here we set the default light parameters if no parameters were passed, so we have some default values for the light parameters. Next we create a vertex buffer and a normal buffer here. You can see that the pipeline contains a buffers attribute here. It is an array that contains two elements. The first element is the vertex data with the shader location being zero. The second element is for the normal vector data with the shader location being one here. The other part of the pipeline is the same to that we used in Project five. Next we create a uniform data. Here we define normal matrix and a model matrix, and view, and viewprojection matrices here. For simplicity, here we set the eye position we get from the camera equals eye position. This means the light vector is equal to the view vector. Here we then create three uniform buffers one is vertex uniform buffer, another one is a fragment uniform buffer, and finally it is light uniform buffer. The vertex uniform buffer is used to store the model matrix, vewprojection matrix, and a normal matrix. The fragment uniform buffer is used to store the eye position and light position. And the light uniform buffer is used to store the light parameters which will be used in calculating the light model in the fragment shader please note here, inside it is a callback draw function, we write a model matrix and a normal matrix inside this draw function, because we want to use the real time model matrix to perform various transformations. Note we construct the normal matrix here by first inverting this model matrix and then transposing this normal matrix. And get the final normal matrix. This means we do the inverse and the transpose on the original model matrix. And finally, get normal matrix. when we set the data to the buffer, please note here is 64 and 128 is offset so, we also need to use the correct offset to write this data to the uniform buffer. Finally, we set the vertex buffer, normal buffer, and a uniform binding group to the render pass. Now, we finished the coding for the file. We can save this file and close it Okay we completed the programming for calculating the light model. In next project, we will use the light model implemented here to create a 3d cube with lighting effect. In Project 6, I explain how to build a simple light model in WebGPU. In this project, I will show you how to use this light model to add lighting effect to a 3d cube, as shown here, You can download the source code used in this project from this GitHub repository link. This specific commitment version is used for this project. Now start Visual Studio Code and open our project 6 that we built in the last section. Here is the code used in the project 6 as we discussed in the last section. In order to calculate the light model, we have to know the normal vector at each vertex on the surface of the 3d object. Now we need to add normal vector data to the vertex_data of our cube. From the src folder open vertex_data.ts file. Here for the CubeData method, we need to add the normal vector data here You can see that vertices on the same face have the same normal vector. For example, the front face all have 001. Similarly for the right is the 100, all the 100 and so on. Once we add the normal vector data to this CubeData function, we need to return the normal data here normals. We also need to return normals. Okay now we can save this file and close it. Next we need to make some changes to the index.html file. From the dist folder, open the index.html file. Now we need to replace its content by the new code. Here we add some input parameters here that let you test our light model. This parameters also allow you to specify object color, ambent, diffuse, specular light coefficients. You can also specify surface shininess and specular color for the specular light computation. Now we can save this file. Next we need to make some modifications to the main.js file. From the src folder, open the main.js file. Now replace its content with new code. Since most of the code for render pipeline and render pass hve been already included in the light dot ts file, here the main.ts file becomes very simple. First, we introduce the create shape with light and the light inputs interface from the light.ts file, and then introduce the cube data from vertex data.ts file. Next we call the cube data method to get the vertex data and define the default parameter here, the light inputs, we just use the default parameters. And also is_animation, we set it to true. And then we call the create shape with light function to create a 3d cube with lighting effect. This part of the code allows the user to recreate the cube with different lighting effect by changing the input parameters. Now we finished the modifications to the main.js file. We can now save this file. Now we can run the following command in the terminal window to bundle our TypeScript code in production mode npm run prod Okay, the bundle file is created successfully. Now we can click the Go Live link to open Chrome Canary to view our 3d cube with light effect. Click this link. Here is a red cube with the lighting effect displayed on this page. We can make some changes to the input parameters and click the redraw button to recreate the cube. Now, for example, we can set the diffuse and specular coefficients to zero here, to zero, and ambient light to one. Now click redraw, we get a cube with ambient light only. You can see the cube looks flat and no 3d feature. Now let's add the diffuse light and specular light back. point 2. We can then change the color of the object. For example, this red we change it to green, 010. And also change the specular color from white to yellow, and then click the redraw button. You can see here the yellow you can increase the shininess, for example, 100. You can see the spot a little bit yellow. This is specular light. Now, go back to animation. So by changing this parameter you can get new lighting effect on this cube. You can use this light model to easily create your own 3d object by following the procedure present here. Okay, now we have completed project 7. The projects we discussed so far were appled a fixed color by assigning a desired color value to the fragment color variable in the fragment shader. In this project, I will explain how to use the colormap to render 3d surfaces. Surfaces play an important role in various applications, including computer graphics games, and 3d data visualization. In some graphics and chart applications, we need a custom colormap to achieve special visual effects. In fact, the colormap is just a table or list of colors that are organized in some desired fashion. We can create a custom colormap with an mx3 color matrix with each row representing RGB values. The row index can represent the y data of a 2d chart or height of a 3d surface plot. For a given color map matrix, the color data values can be linearly scaled to the colormap. Here shows the color strips for the colormaps that we will create in this project. You can see that each color map has a familiar colormap name, such as hsv, hot, cool, and jet and so on. You can download the source code used in this project from this GitHub repository link. This specific commit version is used for this project. Now start with Visual Studio Code and open our project 7 that we just built in the last section Here contains the source code used in the last project. We can easily create a custom colormap using simple mathematical formula. Here, I will provide a several commonly used colormaps using mx3 colormap array. Now add a new typescript file called colormap_data.ts file to the src folder, s src folder, add a new file called thecolormap_data.ts file. And then add a new method to this file. Here the colormap data method contains 11 different colormaps data. You can see hsv, hot, and jet as default. Inside this function, you can see each colormap contains 11 RGB color arrays, each, I mean, array here is RGB values. Here shaws the color strips generated using this colormap dat. This is hsv, corresponding to this color strip. So here is the jet corresponding to this color strip. So, these color strips are generated using colormap data. In the colormap_data function, we assume this 11 color arrays are uniformly distributed in the range of zero and one. For example, the color zero, the first element represents a color at zero location, while color 5 represents a color at 0.5, etc. However, we have to use interpolation method to get a color at, for example, 0.55 or any other arbitrary locations. So here, we will use an npm package called interpolate arrays to do the color interpolation. Now in the terminal window, we run the command npm install, interpolate arrays to install this package. Now at the top of this colormap file, we need to introduce this package here. Here you can see again, we use a require to introduce this package. because it was created originally using the old modules method. So we have to use the require instead of import to get this package. Next, we add a new function here called addColors Here is addColors method to this file. This function accepts the x argument whose value value is in the range of minimum and maximum. This function allows you to interpolate color for any arbitrary x value. Inside this function, we first get the color array by calling the colormap data method with a colormap name as the input. And then make sure the input x parameter is in the range of minimum and maximum range. Next, we normalize x to the range of zero and one. Finally, we interpolate the color for the X variable by calling the interp here the color and normalize x here. This interp comes from the interpolate_arrays package. Now this function returns RGB array with each component being in the range of zero and one. So now we can save this file and close it. Next in the src folder, rename the light.ts file to the surface.ts file. So we rename this file to surface. This surface.ts file will be used to create different surface plots. First, we need to make small changes to the light inputs interface. Here we can remove the color field here, because we don't need to specify the solid color for our object, but we will use the colormap for the object color. Then we add another new field called is_two_side_lighting. Since the 3d surfaces are usually open surfaces we need to implement the light model for both the front and back sides, in order to see the surface clearly, but this parameter controls whether you want the oneside or twoside lighting. Next we change the function name here create surface with the colormap. So we change create colormap, and then add a color data as its input argument. Here we have normal data, we need to add the color data. This color data means the colormap data. Here we need to remove this color, and add a new before the value for the is_two_side_ lighting here. Here we set the default value to one This means the two side lighting. If zero, it is one side. Here we create a vertex buffer, normal buffer, we also need to create the color buffer. Color and use the color data. Next, here is a buffer array the first is vertex buffer, this is normal buffer. We also need to add another buffer for the colormap data. So, here we need to set the shader location to 2. we don't need to change the other part of the code for the pipeline. So, other part is the same as that used in the last project. The code for the uniform data, camera, and uniform buffer layout is the same as that used in the last project. So we don't need to change anything here. But here the light parameters we need to make some changes because the light color we already removed. So we remove this light color, but we need to add another parameter called is two side lighting parameter. So we need to add a parameter here, push the is_two_side_lighting parameter by 0 0, 0. But we will make the parameter consistent with the vec4 array. The other thing we need to make a change is that we should add the color buffer to the render pass. We have vertex buffer and normal buffer. We also need to add color buffer here, called color buffer here color buffer at slot 2. Okay we finished the modification to this file. Right now, So we can save this file. Next we need to make some modification to the shader code because we want to incorporate lighting and the colormap into the shader. So now from src folder open the shader.wgsl file from this, open this file and we needed to replace its content with new code. This shader is very similar to that used in the last project. Inside the vertex shader, in addition to the vertex position, you can see and normal vector we also add the color at the location 2 to this input. Within the vs_main function we process the colormap and assign this colormap to the v_color. In the fragment shader here we use the processed colormap data as our object primary color here. You can see v_color, this processed from vertex shader. We use it as a primary color to get the final color. Inside the fragment shader the main function you can see here the is_two_side_lighting parameter here called the prameter zero, controls whether the lighting is applied to one side or two sides of our surface. We set it to one for two side lighting, which is also default setting. For two side lighting, you can see this for one side for front side the diffuse we use the N dot L, specular light we use N dot H. If two side lighting we add another term here for the diffuse we use minus N dot L. For the specular we use minus N dot H. This means for the backside light, we just reverse the normal vector to get the light for the backside. Now we can save this file. Okay now we've finished the programming for colormap in this project. In the last project we discussed the colormap model. In this project I will explain how to use this model to build simple 3d surfaces. Here is an example of a 3d Simple surface created using a peaks function. Mathematically, a surface draws a Y function on the surface for each x and z coordinates in a region of interest. For each x and z value pair the simple 3d surface can have at the most one Y value. We can define a simple surface by the y coordinates of point above a rectangular grid. In the xz plane, the surface is formed by jointing adjacent points using straight lines. Typically, the surface is formed using rectangular mesh grdids. However, WebGPU only provides triangles as the basic units to represent any surface in 3d. In order to represent a surface using traditional quadrilaterals, we need to write our own functions. You can download the source code used in this project from this GitHub repository link. This specific commit version is used for this project. Now start Visual Studio Code and open project 8 that we built in the last section. Here shows the code used in the project 8. First we need to add a new typescript file called surfacedata.ts to the src folder, SRC folder. Add a new file called surfacedata.ts We need to introduce vec3, import vec3, from glmatrix library, and also need to import from colormapdata.ts file, introduce addColors function. First, let add a utility function called normalizepoint that will be used to normalize the point to the range of minus one and a one. So, we need to add a function normalize function. You can see here this function takes a 3d vec3 here the points. x data range X minimum, X maximum, y minimum, y maximum, z minimum, z maximum, and also introduce a global scale, parameter that allows you to change the value range for the x, y, and z coordinates. This will be convenient for setting the default size of our surface. This function returns the normalized 3d point. For our simple 3d surface as shown here, this surface can be formed by quadrilateral mesh grid Here is one of the quadrilateral. This, as shown here, is a unit grid cell. This unit grid is a quadrilateral with 4 vertices P0, p1, p2 and p3. We can divide this quadrilateral into two triangles P0p1 p2, this one triangle, another triangle is p2p3 and P0 as shown here. We will create a vertex data, normal vector data and colormap data for this quadrilateral. Now, we can add a new function called Create_Quad. You can see here that this function takes 4 points just as shown here, point 012 and 3. It takes these 4 points as its input arguments. It also takes vertex data range y minimum and y maximum, as well as the colormap name as its input parameters, because we want to add the colormap to the y values for our surface. Of course you can colormap the data value in the other direction such as x and z direction. But we usually add colormap to the y values. here we first create the vertex position data for the six vertices This is point zero, point one, point two. This is the first triangle, this is second triangle point 2, point 3, and point zero. So this is vertices for this unit cell for these two triangles. Next, we define the normal vector for this quadrilateral. We already discussed how to obtain the normal vector for this quadrilateral in the previous project. It is simply equal to the cross product of these two diagonal lines of this quadrilateral. Here we introduce these two lines first is p2p0, p two p zero, we call the CA, another line is p3 and p1, p three P one, we call it DB. Then, we get the cross product, we call CP, these two lines, then we normalize this CP cross product, we get the normal vector for this quadrilateral. So, all the vertices on this quadrilateral have the same normal vector. Next, we add the colormap data to vertices for this quadrilateral. You can see the P zero p one, P two and p three, this is the colormap data for these 4 vertices. For P zero, you can see we add the colormap data to it by calling the addColors function with its y component p0[1], and using the y data range y minimum and Y maximum. The AddColors function was implemented in the last project. If you want to add colormap data to the other direction, for example, the x direction you should use x data range and also P zero here inside should be zero, this is the x component. Similarly, we add the colormap data to the other three vertices. And finally, we add this colormap data to these two triangles. These have six vertices, they have a different colormap on different vertices. Finally, createquad function returns vertex, normal, and color for this unit grid. This createquad function here only creates the data for single unit grid. Now, we need to create a data for the entire surface. We need to add a new function called simple surface data. This is a function that creates the data for entire surface. This function takes f here this input argument. F is a math function that describes the 3d Simple surface. We then define the data range in the x z plane use a minimum and a maximum value for the x and the Z. Next two input parameters nx and nz represent the grid divisions along the x and z directions. Here the scale is a global scaling parameter used in the normalize point function. The scaley parameter is used to control the y value height relative to the X and Z value, that is, the scaley parameter here controls aspect ratio of our surface plot. Inside this method, we first define the size of a unit grid, you can see dx and dz. We then calculate the vertex position on our surface by calling the function f. You can see we use this for loop inside that we call the F(X,Z) function. This describes our surface. Here we also calculate y value range y minimum 1, y maximum one. Next, we reset the y value range usng the scaley parameter here next, we normalize vertex position by calling the normalized point using the value range for X y, and Z, and also the scale. Inside this double for loop, we first define the unit cell P zero p one p two and p three. This is a unit cell. Then call the CreateQuad function to get thevertex position, normal vector, and colormap data for this unit grid. So this vertex normal and color for our surface. Finally, this simple surface data function returns the vertex data, normal data, and colormap data. Now we finished programming for 3d Simple surfaces in this project. In next project, we will use this framework to create a 3d Sinc surface with both the lighting effect and the colormap. In the last two projects, we discussed the colormap model and simple 3d surface construction. In this project, I will explain how to use the colormap model and the simple surface data function to create a 3d sinc surface as shown here. This is a pretty sinc surface we want to create in this project. You can download the source code used in this project from this GitHub repository link. This specific commit version is used for this project. Now start with Visual Studio Code and open our project 9 that we built in the last section. Here is the code used in the last project. First, we need to make some changes to the index.html file From the dist folder, open index dot HTML file. Here we need to change this h1 title to Sinc surface and then we need to change the parameter here. You can see here we have two_ sided_ light parameter that controls whether we want to apply the lighting effect to one side or two sides of the surface. Here is a dropdown menu for the colormap that contains 11 colormap names we defined in the colormapdata.ts file, so you can select different colormap for our surface from this dropdown. Here is the scale parameter that lets you set the default size and aspect ratio for the sinc surface. Now we can save this file Now let's define our sinc function. Add a new TypeScript file called the mathfunc.ts file to the src folder. Src folder, add a new file called the mathfunc.ts file. Here is a definition of the sinc function. Here r is defined as the square root x square plus z square. This function equals to sign R or R if r not equal to zero. Otherwise if it equal to zero this function is equal to one. In fact, this function is the Fourier transform of a rectangular function. Now we add some code here to define this function. You can see this function is very simple it takes x and z and center parameters here as its input arguments. The center parameter here lets you set the location of our sinc surface. Inside this function we define r using this formula sas shown here. So here let y, r equals to zero, this function equal to one, otherwise it equals to sin r over r. This function returns a vector three point on the sinc surface here. Now we can save this file and close it. Next we need to make some changes to the main.ts file. From src folder open main.ts file and now we need to replace the code with the new code. Since most code for render pipeline and render pass have been already included in the surface.ts file. So the main.ts file here becomes very simple Here we first introduce the simple surface data from surfacedata.ts file, and then we introduce the sinc function from mathfunc.ts file. Next from surface.ts file we introduce create surface with Colormap and also introduce the light inputs interface from the surface.ts file. Next, we create a new function called Create surface. This function takes light inputs, is_animation, and the colormap name, and scale, and scaley as its input arguments. Inside this function, we call the simple surface data with, you see here, the sinc function as the input argument. From this simple surface data method, we can get the vertex position, normal vector, and colormap data, we then call create surface with colormap function to create our sinc surface with lighting and colormap effects. Here we define the default input parameters and then we call create surface function to create a 3d sinc surface with default lighting and colormap effects. This part of the code allows the user to recreate the sinc surface with different input parameters. Here, this code allows the user to select the different colormap from the dropdown menu. Now we've finished the modification to the main.ts file. Okay, save this file. Now we can run the following command in the terminal window to bundle our typescript code. Open a terminal window and run the command npm run prod to bundle our TypeScript code in production mode, Okay, the bundle file is created successfully. Now we can click this go live link to open Chrome Canary to view our sinc surface. Click this link. Okay, here is our sinc surface with default two_side_lighting and jet color map displayed on this page. Now let's check what happens if we use one side lighting. So we use camera control. This is one side and we set it to zero. You see, you see the back there is no diffusion and specular light in the back. But the only have very weak ambient light here, If we set it to one you can see the back the difference the one side and two side. You have light on the back. Here the scale sets the default size of surface. For example, we change it to one, you get a smaller surface here change it to 3, you will get a bigger surface. So they can the default size of the surface. We go back to 2. And here the scale y lets you control the aspect ratio. For example, we set it to zero, you get a taller surface. And change it to the 0.5 you get a shorter and fatter surface. We change it to 0.3, so you can use this parameter to control the aspect ratio. Next we can change the colormap from this dropdown menu. For example, antum, bone, cool, Cooper gray, hot, hsv, spring, summer and winter. So you can get different colormap for our surface. You can see that in WebGPU, we can easily create a beautiful 3d surface with lighting and color map effect. You can create your own surfaces by simply providing your own math functions. So now we've completed our project 10. You can use WebGPU to create advanced graphics in your web applications. Here are some more examples from my recently published book "Practical WebGPU Graphics" Here are some parametric 3d surface examples. They all look beautiful because they have both colormap and lighting effects as we did for the simple 3d sinc surface in our projects. Here are texture map examples on 3d objects. Texture plays an important role in 3d graphics. The modern GPUs have support for image texture builtin on the hardware level. Texture mapping on 3d object provides more interesting and realistic look. These examples demonstrate that you can do texture mapping on sphere, cylinder, and 3d surface. You can also use multiple textures on 3d object. You can see here each face of our 3d Cube has a different image texture. Here are examples of domain coloring for functions with complex variables. These beautiful pictures are drawing pixel by pixel based on bitmap render, which is a computation intensive process. Here we perform all calculations and rendering directly in GPU, which makes it possible for our real time animation here. This is impossible in CPU. These are 3d fratal examples. They provide more structures than 2d fractals. Again, this nice pictures can be only create in GPU. In CPU, it is too slow and impossible to have this real time animation. Here are some example for large particle systems. This is a compute boids and we try to simulate the flocking behavior of birds. This picture shows particle kinematics with collision to the wall. This shows three mass centers that attract particles. Here we use Compute shader to perform physics related computations and update the positions and velocities on each frame. This is only possible in GPU for simulating this large particle system in real time. If you want to learn how to create these pictures and particle systems, please visit my website at Dr. Xu dotnet.com and my YouTube channel at Practical Programming with Dr. Xu. Thank you for watching
