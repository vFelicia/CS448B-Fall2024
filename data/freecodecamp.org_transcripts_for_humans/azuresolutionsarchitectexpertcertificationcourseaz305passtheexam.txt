With timestamps:

00:00 - hey this is Andrew Brown over here at
00:01 - free Camp bringing you another free
00:03 - Cloud certification study course and
00:04 - this time it's the Azure Solutions
00:06 - architect expert also known as the a305
00:10 - and the way we're going to achieve uh
00:12 - Azure certification is through lectur
00:13 - content Hands-On labs and as always I
00:15 - provide you a free practice exam that
00:18 - way you can go get that certification to
00:20 - put it on your resume or LinkedIn to go
00:22 - get that Solutions architecture role if
00:25 - you like these kind of free Cloud study
00:28 - courses the best way to support more of
00:30 - these materials is by purchasing the
00:32 - optional paid materials over on the
00:35 - exampro platform exampro doco uh if you
00:38 - don't know me I'm Andrew Brown and I've
00:40 - taught a lot of different Cloud
00:41 - certification study courses here Azure
00:44 - adabs gcp kubernetes terraform uh you
00:47 - name it I've taught it so you're in
00:48 - great hands and I will see you in class
00:51 - uh in a moment
00:54 - [Music]
00:56 - ciaoo hey everyone it's Andrew Brown and
00:58 - we're at the start of our journey asking
01:00 - the most important question first which
01:02 - is what is the a305 so the a305 um is a
01:06 - certification or an exam uh to get the
01:09 - certification Microsoft Azure solution
01:11 - architect expert I use a funny wording
01:14 - there because um previously you had to
01:17 - take two separate exams the 303 and the
01:19 - 304 and then that would get you the
01:21 - certification but uh Microsoft has seen
01:24 - the light and now we just have one exam
01:27 - for a certification it looks like it's
01:29 - mostly like that now but just understand
01:31 - the future they might change that but
01:33 - yeah the a305 more or less means
01:35 - Microsoft Azure Solutions architect
01:38 - expert going to get off screen here now
01:40 - but uh yeah it is an expert level
01:42 - certification and it's focusing on
01:45 - comprehensive strategies for Designing
01:46 - Solutions on Azure it has a deeper dive
01:49 - into advanced networking configurations
01:52 - you don't see me doing this but I'm
01:53 - doing quotations air quotations here
01:56 - sorry air
01:57 - quotations for the word advance because
01:59 - it's not as in-depth as something like
02:01 - the um uh the A700 Advanced networking
02:05 - but yes you do need to know your
02:06 - networking your Cloud net working very
02:08 - well for the certification it has a
02:10 - broad examination of data and Security
02:13 - Solutions in Azure uh the sc900 is a
02:16 - great recommendation for that but we'll
02:17 - talk about that when we look at the road
02:20 - map here and this has extensive Hands-On
02:23 - knowledge you should know how to do
02:25 - things in the Azure portal arm templates
02:27 - and other uh uh resilient architectural
02:30 - tools all Azure exams are very code and
02:34 - script driven so having great developer
02:37 - experience is a must make sure you do
02:40 - things in the um the portal um now this
02:43 - one the 305 is not as um technically
02:47 - heavy like the 104 in terms of Hands-On
02:49 - but it still is a great challenge
02:51 - because of how broad uh the information
02:54 - is being covered in the 305 who's the
02:57 - certification for well consider the
02:58 - certification if you're uh looking to
03:01 - get an Azure Solutions architect expert
03:04 - role uh you're designing Solutions using
03:06 - Microsoft Azure you're implementing and
03:08 - monitoring Azure infrastructure you're
03:10 - creating configuring resources for Azure
03:12 - applications uh enjoy crafting resilient
03:15 - and scalable Cloud architectures I want
03:18 - to warn you this is a hard exam um it is
03:22 - very common to fail this exam even if
03:25 - you practice a lot and you do your best
03:27 - job uh it may just come down to the type
03:29 - of questions you get so you know we do
03:32 - the best to give you practice exams and
03:34 - all the information here just understand
03:36 - when you get to expert level or
03:37 - professional level certifications it's
03:40 - not you it's just the questions and you
03:42 - might have to try multiple times so
03:44 - don't get discouraged if you uh if you
03:47 - take an exam and you're just like one
03:49 - point shy away that is what it's like
03:51 - doing these professional or expert
03:53 - certifications let's take a look at our
03:55 - Azure road map um and I'm right away I'm
03:58 - already noticing that this is a bit
03:59 - buggy this is supposed to just say 305
04:01 - but just you know ignore that one one
04:02 - little bug there that says 305 but
04:06 - anyway uh we have a bunch of uh Azure
04:09 - certifications Microsoft certifications
04:12 - this isn't the full list um in fact
04:14 - there are some that I probably would
04:15 - even recommend on Route there but
04:18 - definitely before you go for expert you
04:20 - should have your A9 a900 gives you a
04:23 - good broad view of all services uh the a
04:26 - 104 is generally what is uh or
04:29 - absolutely recommended uh on path to the
04:31 - 305 I would also pick up the 204 on the
04:34 - way um in fact in this course a lot of
04:37 - the content not a lot of the content but
04:39 - a good chunk of the content came from
04:41 - our 104 and our 204 and then we had to
04:43 - round it out with other things so
04:46 - understand that these two are are part
04:47 - of the course if you've done the 104 204
04:49 - you're going to be in really good shape
04:51 - or well aligned for the 305 and then you
04:54 - can spend time trying to figure out case
04:55 - studies and things like that I would
04:57 - probably also add the sc9 00 on here uh
05:01 - that is a fundamental certification the
05:04 - dp900 would be also a good one um I'm
05:07 - just kind of running out of space for
05:08 - all these things that's why I don't have
05:10 - them all listed like here you can see I
05:11 - have the networking one shoved down in
05:13 - here um but Azure just has a lot a lot
05:16 - of certifications okay um but anyway how
05:20 - long would it take to uh pass uh for
05:22 - studying well if you're beginner don't
05:24 - do this exam it is not a beginner
05:26 - certification it would take you uh well
05:28 - over a month if you're a beginner start
05:31 - in the fundamentals do the associates um
05:34 - and work your way up to it if you're
05:35 - experienced you already have that 204
05:37 - that that or and the 104 and you already
05:40 - have multiple uh years working
05:42 - experience it might not take you that
05:44 - long to study but you know on average I
05:46 - think 50 hours is for somebody in the
05:49 - mid-tier area we're looking at 50%
05:52 - lecture 50% uh practice or sorry 50%
05:55 - lecture in Labs 50% practice exams I
05:57 - actually probably would even bump this
05:58 - up a bit more more try to get your hands
06:01 - on as many practice exams as you can the
06:03 - huge challenge with um uh Microsoft
06:06 - certifications is they have a rich uh
06:09 - type of exam questions and not everyone
06:12 - can emulate those questions so a lot of
06:14 - times you are uh needing to overstudy uh
06:18 - with simpler exam type questions in
06:20 - order to try to be able to tackle the
06:23 - harder exam so just understand that that
06:25 - is something you have to consider um 30
06:28 - days absolutely you need a whole month
06:30 - for this exam uh for for uh for studying
06:33 - a recommended study one to two hours
06:35 - really Max your time out and so that you
06:38 - are in the best shape possible um what
06:41 - other things should we consider well
06:42 - make sure you watch the video lecture
06:44 - content do the Hands-On Labs absolutely
06:47 - do all of them and do the best that you
06:49 - can in fact there are Labs that we
06:52 - wanted to record but they were so hard
06:53 - to do and this course was already so
06:55 - long that we didn't even record them and
06:57 - put them in here but we actually do have
06:59 - um I believe additional Labs with the
07:02 - instructions on our platform so you know
07:04 - max out as many Labs as you can to make
07:06 - sure or or Hands-On work make sure
07:09 - you're in good shape absolutely do paid
07:12 - online practice exams we have our own
07:14 - sets here um just ignore these numbers
07:16 - here I just did not feel like taking a
07:18 - new screenshot um but uh yeah just the
07:21 - the times are a little bit different
07:22 - here because you actually get 120
07:24 - minutes and it's more around 60
07:27 - Questions so 57 makes sense um but
07:30 - anyway yeah you definitely want to look
07:32 - into that in terms of the actual exam
07:36 - guide outline there are four domains and
07:38 - each domain has its own waiting this
07:40 - determines how many questions in a
07:41 - domain that will show up uh Microsoft
07:43 - exams they like to do arrange of
07:45 - questions which it's not great for the
07:47 - test taker but I guess it makes the exam
07:50 - harder I you know I don't personally
07:53 - like this I don't like that they do
07:54 - ranges like this but uh you know I guess
07:56 - they think that makes it if it's more
07:58 - more confusing then therefore the exam
08:00 - must be more worth it for the end result
08:03 - I don't know so the first domain um or
08:06 - section whatever you want to call it
08:08 - it's between 25 and 30% of the exams so
08:10 - this design design identity governance
08:13 - Monitoring Solutions the next one is 20
08:15 - to 25% for design data storage solutions
08:18 - the next one is 12 to 20% so design
08:21 - business continuity Solutions and then
08:23 - 30 to 35% to design infrastructure
08:26 - Solutions where are you going to take
08:28 - this exam it's going to be with Pearson
08:30 - view um previously Microsoft I think
08:33 - they offered it I think it had PSI
08:36 - online before um even adab us is doing
08:39 - the same thing they're just doing
08:39 - Pearson so Pearson and Pearson view um
08:43 - so you can do it at inperson test center
08:45 - or online for the convenience your home
08:46 - uh from your own home so what we're
08:48 - talking about is Pearson view online
08:50 - this is the online product exam system
08:52 - and then you have the Pearson view uh
08:54 - network of test centers this is where
08:55 - you do it in person the word Proctor
08:58 - means a supervisor visor so someone is
09:00 - going to monitor you during this exam um
09:04 - sometimes when you uh go for your exam
09:07 - they're uh uh like online they will go
09:11 - and actually call and talk to you and
09:13 - ask you to show the room and it can be
09:15 - very involved so just understand that uh
09:18 - there is somebody watching while you
09:19 - take these exams uh the grading for this
09:22 - one is um 700 out of a th000 points so
09:25 - basically it's around 70% I say around
09:28 - 70% because aure uses scaled scoring so
09:32 - um technically it's not exactly 70% it's
09:35 - 700 out of a th000 I know that's
09:37 - confusing but generally uh what you want
09:39 - to do is aim to go get 80 80% I always
09:43 - think if you have a margin of 10% above
09:46 - that you're passing for practice exams
09:48 - then you're giving yourself a buffer you
09:50 - might even want to go higher and try to
09:51 - Target
09:52 - 85% um and in the result you will
09:54 - actually probably get
09:56 - 70% um in terms of the the amount of
09:59 - questions there's between 40 and 60 so
10:02 - you should be able to get 12 and 18
10:04 - questions wrong though the thing is is
10:06 - that when we save 12 18 that's not
10:08 - exactly true because you have to
10:10 - understand the format of the questions
10:12 - for Microsoft Azure exams and they have
10:15 - a lot of different kinds of formatted
10:17 - questions you got multiple choice you
10:19 - got multiple answer you got drag and
10:20 - drop you got build list reorder you have
10:23 - active screen you got hot areas you have
10:25 - case studies you'll absolutely see case
10:27 - studies in this exam and then questions
10:29 - can have exhibits so it can get really
10:31 - complex um in terms of what you will see
10:34 - will be different from other folks you
10:35 - definitely will see multiple choice
10:36 - multiple answer you'll definitely see
10:38 - case studies and some of these other
10:39 - ones might show up some questions are
10:41 - worth more than one points there is no
10:44 - penalty for wrong questions so do not uh
10:47 - do not leave any questions blank some
10:49 - questions cannot be skipped and you have
10:52 - to fill them in um so yeah there's a lot
10:54 - going on there the duration is two hours
10:58 - um I could have swore that the 204 was
11:01 - three hours but I looked it up and maybe
11:03 - they changed the time but I really
11:05 - thought they would give you more time
11:06 - for this exam but yeah it's just two
11:08 - hours so it is still a stressor um you
11:11 - get two minutes per question basically
11:13 - so we're looking at an exam time of 120
11:15 - minutes but your seat time is 150
11:18 - minutes all we do is we add 30 minutes
11:20 - to to make the seat time the seat time
11:23 - refers to the amount uh uh that you
11:25 - should allocate for the exam um so this
11:29 - includes time to review instructions
11:30 - Show online Proctor your workspace read
11:33 - and accept the NDA complete the exam
11:35 - provide feedback at the end of the exam
11:38 - the reason I I include this here is just
11:40 - to remind you that um when you're when
11:42 - you are planning this you have to plan
11:44 - for all of the time not just the exam
11:46 - but also the time around the exam this
11:49 - exam is only valid for 12 months uh this
11:52 - is something different so Azure used to
11:55 - have two years and then you'd have to
11:57 - pay every time now they're doing every
11:59 - 12 months so every year you
12:02 - recertify um however the recertification
12:05 - process supposedly isn't as hard because
12:07 - renewals are free um and I think you can
12:10 - even check in like six months into it if
12:12 - you want to do uh uh if you want to do
12:14 - it sooner but we'll take a look at that
12:17 - because again those are new things to me
12:18 - and I want to uh share those with you
12:21 - yeah hopefully that gives you an idea of
12:23 - the exam guide itself but we'll see you
12:25 - in the next one okay
12:27 - ciao
12:31 - hey this is Andrew Brown and welcome to
12:34 - the marketing site on Microsoft for the
12:37 - a305 and you'll notice the name of this
12:39 - is actually called designing Microsoft
12:41 - Azure infrastructure Solutions and it
12:43 - gets you that badge the uh Solutions
12:46 - expert but the uh strange thing is that
12:49 - Microsoft does not name the badge the
12:51 - same thing as the exam uh it's not that
12:54 - strange if you understand the history of
12:55 - their certifications they like to
12:58 - sometimes have more than one exam that
13:00 - you have to pass in order to get a badge
13:02 - and so they don't name them the same as
13:04 - the badge so that's just what's there
13:07 - and the old exam used to have two the
13:08 - 304 and the 303 but we'll go down here
13:11 - below because the reason I'm on this
13:13 - page is I want to show you the sandbox
13:15 - if you go here to the sandbox and we'll
13:17 - open this up we can see the formatting
13:20 - of the questions now it's not example
13:22 - questions um we have example ones on our
13:25 - platform and we have we should have a
13:27 - free set but what we'll do we'll go
13:29 - ahead and click next and this is just
13:31 - like if you were to take it online or a
13:33 - test center and we'll click through and
13:37 - we'll go here and there's just 10
13:38 - questions and the purpose of the sandbox
13:40 - is to show you the formatting of the
13:41 - questions I'm going to get out of the
13:43 - way there we go and so the first one we
13:46 - have is a multiple choice pretty
13:47 - straightforward says what is your
13:49 - favorite sound we'll say a bell we'll
13:51 - click next this one is multiple choice
13:54 - so here we can checkbox two things we'll
13:56 - do that the next one here is drag and
13:58 - drop so you have these things you drag
14:00 - and then you drop them into the area on
14:02 - the right here notice that um I can
14:05 - actually fill in multiple so uh you know
14:08 - just it can vary based on the type of
14:11 - question so it might not just be one to
14:13 - one these things might still remain in
14:16 - here then we have um build order build
14:20 - list reorder so which five tasks should
14:22 - you do in the correct order so you say
14:23 - making a sandwich We'll add the pickles
14:25 - the Mayo the ketchup this that this
14:29 - uh whoops this and that okay and then
14:32 - you can move them around notice that
14:34 - this actually has more than five
14:35 - questions these are draggable here
14:37 - you'll notice these everywhere we'll go
14:39 - ahead and hit
14:40 - next um this question is an active
14:43 - screen so which option should you ACH uh
14:45 - choose to achieve this goal so we'll
14:47 - scroll on down
14:49 - here um to answer select the appropriate
14:51 - setting in the applications example
14:53 - properties window in the answer area so
14:55 - notice here if we hover over might be
14:58 - hard to see see but there is a blue line
15:00 - so it's showing like where do you click
15:02 - right so here that's the only one we
15:04 - drop it down then we choose an option
15:06 - here so it's kind of like
15:08 - simulating um a components without
15:10 - actually having the
15:11 - environment we'll go to the next one
15:13 - here so this one is an active screen so
15:16 - you need to implement self- serving
15:17 - provisions of virtual machines this
15:19 - actually kind of sounds like a proper
15:20 - question the solution must ensure that
15:22 - the user caner the virtual machines Etc
15:24 - what do you choose so we'll go here and
15:26 - we'll choose option one and then op
15:28 - option to notice that these are relaying
15:31 - to this table here but this is just you
15:34 - know another way that we could be
15:36 - working with stuff here we have a hot
15:39 - area so here it says which Services
15:42 - should you configure so we'll go take a
15:44 - look here and notice that we can select
15:46 - something from the hot area so it's very
15:48 - similar to that other one um active
15:52 - screen again you're just like clicking
15:53 - on stuff we'll go to the next one here
15:56 - and so this is a case study so uh case
15:59 - studies are uh pretty complex the idea
16:01 - is you can click around here and read
16:03 - all of this stuff okay so you read
16:06 - through all of this stuff and then what
16:08 - you're going to do is go back to your
16:10 - question at the top I know it's
16:11 - confusing but that's where it is and
16:13 - then you can go ahead and answer uh the
16:15 - question we'll go back and hit next then
16:18 - you have exhibits so exhibits are
16:19 - basically just tabs the idea is that you
16:22 - read the question and then you can go
16:23 - look at exhibit there can also be
16:25 - multiple exhibits so I think in this one
16:27 - here's an example where you have
16:29 - multiples and then you go back to your
16:30 - question and then you answer it okay so
16:34 - you know hopefully that uh makes things
16:36 - really clear um but you can see that
16:39 - they really do have a lot of different
16:41 - formatted questions and uh that can make
16:43 - this uh quite challenging if we want to
16:46 - read about the specifics of the exam we
16:48 - can go to the uh study guide now I
16:51 - remember there being at least in
16:53 - previous exams um or um in other exams
16:56 - you used to have a PDF you can download
16:58 - I can't seem to find that anymore all
17:00 - there is is the marketing site here
17:02 - maybe that's what they want to do but
17:04 - you can go through here and read about
17:05 - like the certification renewals the
17:08 - scoring all the stuff here we were
17:11 - mentioning about certification renewal
17:13 - so if you want to read a bit more about
17:14 - that where they talk about how renewals
17:16 - are free and you have a six-month
17:18 - renewal window and things like that you
17:20 - can read all about it um but yeah there
17:23 - you go and uh we'll see you in the next
17:27 - one
17:30 - hey this is Andrew Brown and before we
17:33 - get into Azure ad I need to point
17:35 - something out it's not called Azure ad
17:37 - anymore it's called Microsoft entra ID
17:40 - Microsoft decided to change the name of
17:42 - azure ad why nobody knows but I can tell
17:46 - you no customer likes this particular
17:47 - change um somebody just had a lot of
17:50 - time on their hands over at Microsoft
17:53 - but we do need to address this and I
17:54 - need to point out that I'm not refiling
17:57 - all of the content that I made just to
17:59 - change the name because that's crazy I
18:01 - will at some point when this when the
18:04 - content is stale but the content is not
18:06 - stale they just changed the name on us
18:08 - um but I wanted to just go over that
18:10 - quickly here so uh the names here we
18:13 - have Azure ad is now Microsoft entra ID
18:16 - then the Azure ad tiers is from P1 P2
18:20 - still P1 P2 the Azure ad external
18:22 - identities is now called Microsoft entra
18:25 - external identities and if we scroll on
18:27 - down we have a logo change so instead of
18:30 - this which by the way I really like the
18:32 - old
18:33 - logo they didn't need a muck with it but
18:35 - anyway we have uh the older ones here
18:37 - and so this is the new one
18:39 - here and so there are some name changes
18:42 - here Azure ad single sign on now
18:44 - Microsoft entra entra uh single sign on
18:47 - we'll go down below here and you can see
18:50 - well more name changes okay so um anyway
18:55 - yeah they renamed it and you know
18:57 - customers are just going to take a while
18:58 - to get used to it I still like calling
19:00 - azuread I know a lot of other people
19:02 - that like still calling it azuread but
19:03 - it's at some point we'll get moved over
19:06 - to it and we're just going to use both
19:08 - names okay now coming over to uh uh the
19:11 - portal I need to show you that if you
19:13 - type in Azure ad it's still going to
19:16 - pull up Microsoft Entre ID okay now you
19:19 - don't want Azure ad B to see which is
19:21 - interesting they didn't rename that U
19:25 - which is a it is part of azure kind of
19:28 - in a sense but it's more for um if
19:30 - you're building applications and you
19:31 - want to um have authentication into it
19:36 - so just understand that there's not
19:38 - consistency all over the place
19:39 - especially even their documentation the
19:41 - marketplace still says Azure ad all over
19:43 - the place um even down below uh you know
19:46 - Azure ad notification so you know
19:48 - there's just going to be that Legacy of
19:50 - azure ad but anyway yeah what you want
19:52 - to do is go go over to Microsoft Entre
19:54 - ID it all looks the same it's just some
19:56 - name changes okay but but yeah there you
19:59 - go see you in the next
20:01 - [Music]
20:04 - one hey this is Andrew Brown from exam
20:07 - Pro and we are looking at Azure active
20:08 - directory and this is a cloud-based
20:10 - identity and access management service
20:12 - to manage users sign-ins and access to
20:14 - ad related resources so Azure active
20:17 - directory is Microsoft's cloud-based
20:19 - identity and access management service
20:21 - which helps you your employees sign in
20:23 - and access resources so that could be
20:25 - external resources like Microsoft Office
20:27 - 365 Azure portal SAS applications or
20:30 - internal resources so applications
20:32 - within your internal networking or
20:34 - access to workstations on premise and
20:36 - you can use Azure ad to implement single
20:38 - sign on so you can see that Azure ad is
20:41 - basically like the the the one solution
20:43 - to log everything and uh we actually use
20:46 - it at exam Pro we use it with Microsoft
20:48 - teams or uh you know for the exam pro
20:51 - pro platform our mid panel is tied to it
20:53 - so when we want to log into the mid
20:55 - panel with credentials we have it there
20:57 - uh we use it with AWS to log into there
21:00 - and we use it to log into Azure so it
21:02 - has a lot of flexibility and if you're
21:03 - building out applications for
21:05 - Enterprises they're likely using ad and
21:07 - so this is the reason why everybody
21:09 - adopts it or needs to understand it so
21:10 - it's a service I really really do want
21:12 - you to understand and know as your
21:14 - active directory comes in for additions
21:16 - we have the free tier and by the way
21:18 - each uh uh uh tier that goes up has the
21:21 - features before it but uh free has MFA
21:24 - SSO basic security usage reports and
21:26 - user management then you have the Office
21:28 - 365 apps which is uh revolves around if
21:31 - you're using that Suite so you have
21:33 - company branding SLA two sync between on
21:36 - premise and cloud and then the premium
21:37 - tiers which really comes into Enterprise
21:40 - or or or on premise hybrid architecture
21:42 - so hybrid architectures Advanced group
21:45 - access conditional access premium 2
21:47 - identity protection and identity
21:49 - governance only thing I don't like about
21:51 - Azure ad is that uh you can't really
21:52 - create your uh custom access controls
21:55 - unless you have premium one or premium
21:56 - two but that's just how they do it so
21:57 - there you go
21:58 - [Music]
22:01 - go so let's take a look at the use case
22:04 - for Azure ad and we basically covered it
22:06 - in the introduction but I just want to
22:08 - reiterate it in a different way with a
22:10 - bit of a visual uh so that it really
22:12 - helps uh it sync into your uh brain
22:14 - there so Azure ad can authorize and
22:16 - authenticate to multiple sources so it
22:18 - can authenticate to your on- premise ad
22:20 - to your web application allow users to
22:22 - log in with uh ipds uh so identity
22:26 - providers could be like use Facebook or
22:28 - Google login uh you can use it with
22:30 - Office 365 or a Azure Microsoft and so
22:33 - just a visual here uh notice that uh we
22:36 - have Azure ad and using Azure ad connect
22:38 - we can connect to on premise through uh
22:41 - app registrations we're able to uh
22:43 - connect our web application to Azure ad
22:46 - with external identities we can um use
22:49 - Facebook or Google uh uh uh login and
22:53 - then for cloud applications we can
22:55 - connect to Office 365 or Microsoft
22:59 - [Music]
23:03 - Azure so active directory existed way
23:05 - before Azure and so let's just do a
23:07 - quick uh uh rundown of the history so we
23:09 - have an idea of what we're looking at so
23:12 - um Microsoft introduced active directory
23:14 - domain services in Windows 2000 to give
23:16 - organizations the ability to manage
23:18 - multiple on-premise infrastructure
23:19 - components and systems using a single
23:22 - identity per user so it's been around
23:23 - for 20 years and Azure ad takes this
23:26 - approach to the next level by providing
23:28 - organizations with identity as a service
23:30 - so idas solution for their apps across
23:34 - uh cloud and on premise and both
23:36 - versions are still used today because
23:38 - they just have different utility and so
23:40 - we have active directory which is for on
23:42 - premise and then you have Azure ad which
23:44 - is just the cloud hosted version and in
23:47 - many regards these can also be connected
23:49 - together um but there you
23:54 - go so remember that the domain
23:57 - controller is the server that users are
23:59 - going to be using to authenticate to the
24:00 - directory Service uh and so when you
24:03 - create an active directory Azure sets
24:05 - one up for you but there's some cases
24:07 - where you might want to set one up
24:08 - yourself and the reason why is that you
24:10 - could be like an on like an Enterprise
24:12 - where you already have your own active
24:14 - directory on premise but you've decided
24:16 - that you want to move it over to Azure
24:18 - ad uh because you just want a fully
24:20 - managed active directory and uh you want
24:22 - to tap into the cloud but uh the thing
24:25 - is that some domain Services those are
24:27 - features on your domain controller just
24:28 - might not be available and that's where
24:30 - you're going to need to set up your own
24:32 - domain controller and that's where Azure
24:33 - active director domain Services come
24:35 - into play because these provide managed
24:37 - domain services and so they have managed
24:40 - domain services such as domain joins uh
24:43 - group policies uh
24:45 - ldaps uh uh curb Ross Never Can Say That
24:49 - properly ntlm authentication and so the
24:52 - great thing is here is you can have
24:54 - these domain services but you're not
24:55 - going to have to deploy them manage them
24:57 - attach them they're just going to work
24:59 - so there you
25:04 - go so let's talk about um uh the term
25:07 - tenant and a tenant represents an
25:09 - organization in an active directory and
25:11 - a tenant is dedicated to the Azure ad
25:13 - service instance a tenant is
25:15 - automatically created when you sign up
25:17 - for either Microsoft Azure or Microsoft
25:20 - InTune or Microsoft 365 and each a or
25:24 - Azure ad tenant is distinct and separate
25:26 - from other azure ad tenants and so if
25:29 - you uh if you were in um uh Azure ad and
25:33 - you clicked on your tenant information
25:35 - that's that's basically what that is
25:36 - right so that's my exam Pro one and it
25:38 - has its own special tenant ID and we can
25:40 - see that it's licensed for Office 365
25:43 - and so that tells you that I'm using the
25:45 - Office 365 uh tier of azure
25:49 - [Music]
25:52 - ad so now let's take a look at some of
25:54 - the ad objects starting with users so
25:56 - users represent an identity for a person
25:59 - or employee in your domain and a user
26:00 - has login credentials and can use them
26:03 - to log into the Azure portal so here I
26:05 - am a user and you can see it shows how
26:07 - many times I've logged in and I'm part
26:08 - of different uh ad groups uh and so you
26:11 - can assign roles and administrative
26:13 - roles uh to users you can add users to
26:15 - groups you can enforce authentication by
26:18 - uh like with MFA you can track user
26:20 - sign-ins as you can see on the right
26:22 - hand side you can track device uh
26:24 - devices users login and uh and allow or
26:26 - deny devices
26:28 - uh you can assign Microsoft licenses
26:30 - Azure ad has two kinds of users we have
26:32 - users that's a user that belongs to an
26:34 - organization and guest users this is a
26:36 - guest uh is a user that belongs from
26:38 - another organization and we'll cover uh
26:41 - Azure ad roles uh in the r section here
26:44 - uh because that is what's that you're
26:46 - going to be using to apply to these
26:51 - users so groups in Azure ID lets
26:54 - resource owners assign a set of access
26:56 - permissions to all members of the group
26:58 - instead of having to provide the rights
27:00 - one by one and so on the right hand side
27:02 - here you can see I have a bunch of
27:03 - groups in exam Pro and groups can
27:05 - contain owners and owners have
27:06 - permissions to add or remove members and
27:09 - then the members have rights to do
27:10 - things okay and so for assignment you
27:13 - can assign roles directly to a group you
27:15 - can assign applications directly to a
27:17 - group and to request join groups so uh
27:20 - the group owner can let users find their
27:22 - own groups to join instead of assigning
27:24 - them to them and the owner can set up
27:27 - the group to automatically accept all
27:28 - users that join or require approval this
27:30 - is really great when uh you just want
27:33 - people to do the work themselves as
27:35 - opposed to having to do all that manual
27:36 - labor of adding them to
27:41 - groups let's talk about how we're going
27:43 - to uh give users rights to access uh
27:45 - resources and there are four different
27:47 - ways to do that the first is direct
27:48 - assignment and this is where the
27:50 - resource owner is going to directly
27:51 - assign the user to the resource then you
27:53 - have group assignment this is where the
27:55 - resource owner assigns an a a group to
27:58 - the resource which automatically gives
27:59 - all group members access to the resource
28:02 - then you have rulebase assignment this
28:04 - is resource owner uh this is where the
28:05 - resource owner creates a group and uses
28:07 - a rule to Define which users are
28:09 - assigned to a specific resource and then
28:11 - you have external uh Authority
28:13 - assignment this is this access comes
28:14 - from an external Source such as an on-
28:16 - premise directory or SAS application and
28:18 - I just want you to know that there's
28:19 - four different ways to do it so uh to
28:21 - get access to
28:25 - resources all right let's take a look at
28:28 - managed identities for Microsoft enter
28:30 - ID or Azure ad managed identities is a
28:33 - concept in Microsoft enter ID that
28:35 - Associates identities with internal
28:37 - resources where these identities have
28:39 - their own roles and tokens managed
28:41 - identities increases security by
28:43 - allowing you to link directly resources
28:44 - to other resources without having to
28:46 - share any security information over the
28:48 - network those resources will be
28:50 - authenticated against enter ID to see if
28:52 - they have the necessary permissions to
28:54 - manipulate other resources for example
28:57 - we can allow our applications to access
28:59 - Azure key vault in order to retrieve a
29:00 - secret without exposing any
29:03 - passwords managed identities is
29:05 - available in two types system aside
29:07 - identities are created and managed by
29:10 - enter ID when you create a managed
29:12 - identity in a service instance only that
29:14 - Azure resource can use this identity to
29:16 - request tokens from enter ID user
29:19 - assigned identities are created and
29:21 - managed manually the identity is managed
29:24 - separately from the resources that use
29:26 - it the table provides a comparison
29:29 - between system assigned and user
29:30 - assigned features in the context of
29:32 - azure creation for system assigned the
29:35 - identity is created as part of an Azure
29:37 - resource whereas user assign the
29:39 - identity is created as a standalone
29:41 - Azure resource life cycle for system
29:43 - assigned the identity shares its life
29:45 - cycle with the Azure resource It's
29:47 - associated with while user assigned the
29:49 - identity has an independent life cycle
29:52 - deletion for system assigned when the
29:54 - associated Azure resource is deleted the
29:56 - identity is also deleted whereas user
29:59 - assigned the identity must be deleted
30:01 - explicitly sharing across Azure
30:03 - resources for system assigned the
30:05 - identity cannot be shared and is
30:06 - associated with only a single Azure
30:08 - resource whereas user assign the
30:10 - identity can be shared and can be
30:12 - associated with more than one Azure
30:14 - resource managed identity is under the
30:17 - identity blade for an Azure resource you
30:20 - assign roles to provide permissions to a
30:22 - managed identity in summary managed
30:25 - identities enhance security through
30:26 - seamless resource integration
30:28 - eliminating exposed credentials with
30:30 - system assigned and user assigned
30:32 - options Azure bolsters efficient
30:34 - flexible Resource
30:36 - [Music]
30:39 - Management let's talk about external
30:41 - identities so external identities in aad
30:44 - allows people outside your organization
30:45 - to access your apps and resources while
30:47 - letting them sign in uh and use whatever
30:50 - identity they prefer so your partners
30:52 - Distributors suppliers vendors or other
30:54 - guests can bring their own identities
30:56 - such as uh Google or Facebook uh you can
30:59 - share apps with external users that's
31:00 - for B2B stuff uh if you develop apps and
31:03 - tender for Azure ad tenants uh for
31:05 - single tenant or multi-tenant you can do
31:07 - that as well uh you can develop white
31:08 - label apps for consumers and customers
31:11 - so this would be like Azure ad uh
31:13 - b2c so there you
31:18 - go hey this is Andrew Brown from exam
31:21 - Pro and the next topic will be exploring
31:23 - or access reviews in Microsoft enter ID
31:25 - or azuread access reviews in Microsoft
31:28 - enter ID allow you to regularly review
31:30 - and manage access to resources in your
31:33 - organization with access reviews you can
31:35 - review who has access to resources and
31:37 - determine whether their access is still
31:39 - necessary access reviews are useful in
31:42 - maintaining security and compliance by
31:44 - ensuring that only authorized
31:45 - individuals have access to sensitive
31:47 - resources access reviews can be
31:49 - conducted for various types of resources
31:52 - such as applications groups and
31:53 - SharePoint online sites you can
31:56 - configure access reviews to occur on a
31:58 - regular schedule and select reviewers to
32:00 - conduct the reviews reviewers can be
32:02 - internal or external to your
32:04 - organization during an access review the
32:07 - reviewer will be presented with a list
32:08 - of people who have access to the
32:09 - resource being reviewed they can choose
32:11 - to approve or revoke access for each
32:14 - individual access can be revoked
32:16 - immediately or scheduled for a later
32:17 - date reviewers can also provide a reason
32:20 - for their decision which can be useful
32:22 - for auditing purposes access reviews are
32:25 - crucial in large organizations to
32:27 - regularly identify and resolve access
32:29 - issues they ensure resources are
32:31 - accessed only by necessary users and
32:33 - that access is revoked when no longer
32:35 - needed overall access reviews maintain
32:38 - security by assessing user access
32:40 - ensuring compliance preventing breaches
32:42 - and safeguarding data promoting a
32:44 - security aware culture make sure to know
32:46 - access reviews because it did show up a
32:48 - few times on the
32:53 - exam the next topic will be covering his
32:56 - single side on on an enter ID single
32:58 - sign on an enter ID is a feature that
33:00 - allows users to authenticate once with
33:02 - enter ID and then access multiple
33:04 - applications and services without having
33:05 - to authenticate again when a user signs
33:08 - into enter ID with their credentials
33:10 - enter ID creates a security token that
33:12 - can be used to access other resources
33:14 - within the same organization this token
33:16 - can be used to authenticate the user to
33:18 - other cloud based or on premises
33:19 - applications that have been integrated
33:21 - with enter ID SSO supports a wide range
33:24 - of applications including cloud-based
33:26 - applications such as Microsoft 365
33:29 - Salesforce and Dropbox as well as on
33:31 - premises applications such as SharePoint
33:33 - and sap SSO can also be used with
33:36 - custombuilt applications using industry
33:38 - standard protocols such as saml open ID
33:41 - connect and
33:42 - oo there are several ways you can
33:44 - configure an application for SSO
33:47 - choosing an SSO method depends on how
33:49 - the application is configured for
33:51 - authentication Cloud applications can
33:53 - use open ID connect ooth saml
33:56 - password-based or or linked for SSO
33:58 - single sideon can also be disabled on
34:01 - premises applications can use
34:02 - password-based integrated Windows
34:04 - authentication header based or linked
34:06 - for SSO Deon premises choices work when
34:09 - applications are configured for
34:10 - application proxy this flowchart can
34:13 - help you decide which SSO method is best
34:15 - for your
34:17 - situation the main SSO protocol
34:19 - supported in Azure include open ID
34:22 - connect in ooth open ID connect is an
34:24 - identity layer built on top of ooth 2.0
34:27 - it allows for authentication and
34:28 - authorization of users in a secure and
34:30 - standardized manner it is saml saml is
34:34 - an XML based protocol used for
34:36 - exchanging authentication and
34:37 - authorization data between an identity
34:39 - provider and a service provider it is
34:41 - commonly used for Federated
34:42 - authentication
34:44 - scenarios password based authentication
34:46 - this refers to the traditional username
34:48 - password authentication method where
34:50 - users provide their credentials directly
34:52 - to authenticate linked authentication
34:55 - Azure provides the ability to l link
34:57 - multiple accounts from different
34:58 - identity providers to a single user
35:00 - identity this allows users to
35:02 - authenticate using any of their linked
35:03 - accounts integrated Windows
35:05 - authentication it will let users access
35:08 - applications using their Windows domain
35:09 - credentials utilizing their current
35:11 - Windows session for
35:13 - authentication header based
35:14 - authentication in this method the
35:16 - application accepts an authentication
35:18 - token in the form of a header in each
35:20 - request the token is validated by the
35:22 - application to authenticate the user
35:25 - you'll need to be familiar with these
35:26 - assets o protocols is there will be
35:28 - questions asking you which SSO protocol
35:30 - is best suited for a specific
35:33 - application what is multiactor
35:36 - authentication a security control where
35:38 - after you fill in your username email
35:40 - and password you have to use a second
35:42 - device such as a phone to confirm that
35:43 - it's you logging in MFA protects against
35:46 - people who have stolen your password MFA
35:49 - is an option in most Cloud providers and
35:51 - even social media websites such as
35:53 - Facebook so that's an overview of single
35:56 - sign on an ENT
36:01 - ID the next topic we'll be going over is
36:04 - conditional access conditional access
36:06 - provides an extra layer of security
36:08 - before allowing authenticated users to
36:10 - access data or other assets conditional
36:13 - access is implemented via conditional
36:15 - access policies which are a set of rules
36:17 - that specify the conditions under which
36:19 - sign-ins are evaluated and allowed for
36:21 - example you can create a conditional
36:23 - access policy that states if the user
36:26 - account name is member of a group for
36:27 - users that are assigned The Exchange
36:29 - user password security SharePoint or
36:31 - Global administrator roles require MFA
36:34 - before allowing access this policy
36:36 - enables MFA enforcement based on group
36:38 - membership simplifying the process
36:40 - compared to configuring MFA for
36:42 - individual users when roles
36:44 - change conditional access policy
36:47 - analyzes signals including user and
36:50 - location device application and
36:52 - real-time risk and verifies every access
36:54 - attempt via access controls this
36:56 - requires MFA block access and allow
37:00 - access signals or metadata associated
37:02 - with an identity attempting to gain
37:04 - access user or group membership policies
37:07 - Target specific users and groups giving
37:09 - admins find grain control over access
37:12 - named location information IP location
37:15 - information IP address ranges are used
37:17 - to permit or deny access based on
37:18 - geographical locations device policies
37:22 - can be applied based on the platform or
37:23 - status of a user's device application
37:26 - users attempting to access specific
37:28 - applications can trigger different
37:30 - conditional access policies real time
37:33 - sign and risk detection signals and
37:35 - Azure ad identity protection detect
37:37 - risky sign-ins if risks emerge policies
37:39 - can prompt actions such as password
37:41 - resets multifactor authentication or
37:43 - block access pending admin
37:45 - intervention Cloud apps or actions can
37:48 - include or exclude Cloud applications or
37:50 - user actions that will be subject to the
37:52 - policy user risk for customers with
37:55 - identity protection user risk risk can
37:56 - be evaluated as part of a conditional
37:58 - access policy user risk represents the
38:01 - probability that a given Identity or
38:02 - account is
38:04 - compromised common decisions Define the
38:07 - access controls that decide what level
38:08 - of access based on Signal
38:10 - information block access most
38:13 - restrictive decision Grant access least
38:15 - restrictive decision still require one
38:17 - or more of the following options require
38:20 - multiactor authentication require device
38:23 - to be marked as compliant require hybrid
38:25 - enter ID join device require approved
38:27 - Client app and require app protection
38:29 - policy conditional access policies are
38:32 - available and can be utilized with the
38:33 - following licensing plans Microsoft 365
38:37 - business premium Microsoft 365 E3 and E5
38:40 - enter ID premium P1 and enter ID premium
38:43 - P2
38:44 - licenses overall conditional access acts
38:47 - as a robust security measure in Azure
38:49 - ensuring that authenticated users can
38:50 - only Access Data under specific
38:55 - conditions
38:57 - [Music]
38:58 - hey this is Andrew Brown from exam Pro
39:00 - and in this section we'll be going over
39:02 - the types of azure roles roles can be a
39:04 - bit confusing this is because Azure has
39:06 - three types of roles that can serve the
39:08 - same purpose the first type of role are
39:11 - classic subscription administrator roles
39:13 - this is the original RO system next we
39:16 - have Azure roles this is an
39:17 - authorization system that's also known
39:19 - as role-based access controls and is
39:21 - built on top of azure resource manager
39:24 - then we have Azure active directory
39:25 - roles Azure ad roles are used to manage
39:28 - Azure a resources in a
39:30 - directory Azure active directory roles
39:32 - are used to manage Azure a resources in
39:34 - a directory such as creating or editing
39:36 - users assigning administrative roles to
39:38 - others resetting user passwords managing
39:40 - user licenses and managing domains among
39:43 - other tasks the rules follow a similar
39:45 - rbac model and include several built-in
39:48 - roles like Global admin application
39:50 - admin application developer and billing
39:52 - admin but also allow the creation of
39:54 - custom roles for more specific control
39:56 - Azure rules specifically Azure rule
39:58 - based access control is a system that
40:01 - provides fine grained access management
40:02 - for Azure resources allowing
40:04 - administrators to Grant users specific
40:06 - rights to resources there are several
40:08 - predefined roles in Azure like owner
40:10 - contributor reader and user access
40:12 - administrator each providing specific
40:14 - levels of access to Azure resources and
40:16 - custom roles can be defined as well
40:19 - classic subscription administrator roles
40:21 - refer to the older model of azure access
40:23 - control and include three types account
40:25 - administrator service ad administrator
40:26 - and co-administrator Azure roles and
40:29 - classic subscription administrator roles
40:31 - can have overlapping responsibilities
40:33 - for example the service administrator in
40:35 - the classic Model has a similar role to
40:37 - the owner role in the Azure rbac where
40:39 - both can manage resources in the
40:41 - subscription however Azure rbac roles
40:43 - provide a more granular level of control
40:45 - compared to Classic roles offering more
40:47 - specific access management they allow
40:49 - administrators to delegate specific
40:51 - tasks and Grant specific permissions
40:53 - reducing the need to give full
40:54 - administrative privileges and therefore
40:56 - for enhancing
40:59 - [Music]
41:02 - security the next topic will be covering
41:04 - are the access controls so identity
41:07 - access management essentially allows you
41:09 - to create and assign roles to users for
41:11 - the Azure rules or the rbac system rules
41:14 - restrict access to Resource actions
41:16 - which are also known as operations there
41:18 - are two types of rules the first type
41:21 - are built-in roles these roles refer to
41:23 - the set of predefined roles offered by
41:24 - Microsoft and azure the rules are read
41:26 - only and cannot be altered cover a wide
41:28 - array of standard scenarios to
41:30 - facilitate efficient and secure access
41:32 - management for Azure resources the
41:34 - second type of rule are custom rules
41:36 - these rules represent user defined roles
41:38 - in Azure tailored to incorporate unique
41:40 - permissions and logic based on specific
41:42 - requirements that are satisfied by the
41:44 - available built-in roles a role
41:46 - assignment is when you apply a role to a
41:48 - service principle which could be a user
41:51 - group service principle or managed
41:54 - identity deny assignments lock users
41:56 - from performing specific actions even if
41:58 - a role assignment grants them access the
42:01 - only way to apply deny assignments is
42:02 - through Azure blueprints so that's the
42:05 - access controls for
42:07 - [Music]
42:10 - Azure hey this is Andrew Brown from exam
42:13 - Pro and in this section we'll be
42:14 - covering classic administrators classic
42:17 - administrators refer to the original
42:19 - role system in Azure despite the Legacy
42:21 - status of classic administrators
42:23 - understanding these roles can still be
42:25 - beneficial like when work with older
42:26 - Azure setups however it's recommended to
42:29 - use the new rbac system for managing
42:31 - access to resources given its enhanced
42:33 - granularity and flexibility classic
42:36 - administrators have three types of roles
42:38 - the first type is a account
42:39 - administrator this role is essentially
42:41 - the billing owner of the Azure
42:42 - subscription it's responsible for
42:44 - managing subscriptions making payments
42:46 - and changing the billing details the
42:48 - account admin has no access to the Azure
42:50 - portal meaning they can't manage
42:52 - resources directly the second type is
42:54 - service administrator this Ro shares the
42:56 - same level of access as a user assigned
42:58 - the owner role at the subscription scope
43:00 - in the rbac model they have full access
43:02 - to the Azure portal meaning they can
43:04 - manage all resources within the
43:06 - subscription the third type is
43:08 - co-administrator this has the same
43:09 - access level of a user who is assigned
43:11 - the owner role at the subscription scope
43:13 - it's similar to a service admin but the
43:15 - main difference is that there can be
43:16 - multiple co- administrators offering
43:18 - shared management capabilities you
43:21 - shouldn't have to worry much about the
43:22 - classic admin roles because I don't
43:24 - think I encountered any exam questions
43:26 - on it it'll mostly be focused on the
43:28 - rbac and Azure ad roles also note that
43:31 - classic resources and classic
43:32 - administrators will be retired on August
43:34 - 31st
43:37 - [Music]
43:40 - 2024 the next topic we'll be covering is
43:42 - the Azure Ro based access control so
43:45 - Azure Ro based Access Control helps you
43:47 - manage who has access to Azure resources
43:49 - what actions they can perform on
43:51 - resources and in what scope which are
43:53 - the areas they have access to ro
43:55 - assignments are the way you control
43:56 - access to Resources by assigning a role
43:59 - to a security principle like a user
44:01 - group service principle or managed
44:03 - identity at a particular scope you
44:05 - define who can perform what actions on
44:06 - which resources our role assignment
44:09 - consists of these three elements scope
44:11 - and role definition and security
44:13 - principle there are four fundamental
44:15 - Azure roles which include owner
44:17 - contributor reader and user access
44:19 - administrator Azure rbac includes over
44:22 - 70 built-in roles these roles are
44:24 - designed to serve many common use case
44:26 - and range from brought to very specific
44:27 - permissions such as virtual machine
44:29 - contributor or network
44:31 - contributor scope is the set of
44:33 - resources that access for the role
44:35 - assignment applies to scope access
44:37 - controls at the management subscription
44:39 - Resource Group and resource level
44:41 - assigning a role at the Management Group
44:43 - level inherits it across all Associated
44:45 - subscriptions and resources at the
44:48 - subscription level it applies to all
44:49 - resource groups and resources within the
44:51 - specific subscription a role assigned at
44:54 - the resource Group level affects all
44:55 - resource within that group at the
44:57 - resource level a role assignment applies
44:59 - only to that specific resource a role
45:02 - definition is a set of permissions that
45:04 - determines what actions can be performed
45:06 - such as read write or delete on various
45:08 - resources roles range from broad such as
45:11 - owner with extensive management
45:12 - permissions to specific such as virtual
45:14 - machine reader with more targeted
45:16 - permissions Azure has built-in roles and
45:18 - you can Define custom roles this table
45:21 - shows the four fundamental built-in
45:23 - roles the owner role has full access to
45:25 - all resources including the right to
45:27 - delegate access to others the
45:29 - contributor role can manage all types of
45:31 - azure resources including the ability to
45:33 - create update and delete but can't Grant
45:35 - access to others the reader role has the
45:38 - ability to view existing Azure resources
45:40 - but can't make changes or Grant access
45:42 - to others the user access administrator
45:44 - role has the ability to manage user
45:46 - access to Azure resources including
45:48 - granting and revoking access but can't
45:50 - create update or delete
45:53 - resources a security principle
45:55 - represents the identities requesting
45:57 - access to an Azure resource such as a
45:59 - user which is an individual who has a
46:01 - profile in Azure active directory a
46:03 - group which is a set of users created in
46:05 - Azure active directory a service
46:08 - principle which is a security identity
46:10 - used by applications or services to
46:12 - access specific Azure resources or a
46:15 - managed identity which is an identity
46:17 - and Azure active directory that is
46:18 - automatically managed by Azure so that's
46:21 - an overview of azure role-based Access
46:24 - Control
46:29 - the last type of role we'll be covering
46:30 - are azuread roles azuread roles are used
46:33 - to manage azuread resources in a
46:35 - directory such as create or edit users
46:38 - asside administrative roles to others
46:40 - reset user passwords manage user
46:43 - licenses and manage domains we'll go
46:46 - over a few important built-in azuread
46:48 - roles so here are some that you should
46:49 - know the first one is global
46:51 - administrator this grants you full
46:53 - access to everything all the features in
46:55 - Azure a
46:56 - another important role is user
46:58 - administrator this grants you full
46:59 - access to create and manage users for
47:02 - the billing administrator role this role
47:04 - can make purchases manage subscriptions
47:06 - and support tickets including monitor
47:08 - service Health not all organizations
47:11 - needs can be satisfied by these
47:12 - predefined roles therefore you can
47:14 - create custom roles which are very
47:16 - flexible and can Define the exact set of
47:18 - permissions that you need keep in mind
47:20 - that you'll need to purchase either
47:22 - Azure a d premium P1 or P2 to create
47:24 - custom roles so that's a short overview
47:27 - of azurea D
47:29 - [Music]
47:33 - rolles hey this is Andrew Brown from
47:35 - exam Pro and in this section we'll be
47:37 - covering the anatomy of an Azure role so
47:40 - it's important to know that the property
47:41 - names in an Azure Ro definition can vary
47:43 - depending on whether you're using Azure
47:45 - Powershell or Azure CLI the two
47:47 - different interfaces use different
47:48 - syntax and terminology we have an
47:51 - example here on the right and we'll go
47:52 - through the properties together a ro
47:55 - definition and Azure consists of these
47:56 - properties name this is the display name
47:59 - of the custom role also note that the
48:01 - ones highlighted in blue is the Syntax
48:03 - for Azure power shell and bold black is
48:05 - for Azure CLI so it's name for Azure
48:08 - power shell and rle name is for Azure
48:09 - CLI the next property is ID this is the
48:12 - unique ID of the custom Ro and it is
48:14 - autogenerated for you is custom this
48:17 - indicates whether this is a custom role
48:19 - it can be either true or false
48:21 - description this property describes the
48:23 - role actions this is an AR of strings
48:26 - that specify the management operations
48:27 - that the role is allowed to perform not
48:30 - actions an array of strings that specify
48:32 - the management operations that are
48:33 - excluded from the allowed actions data
48:36 - actions this is an array of strings that
48:38 - specify data operations the role is
48:40 - allowed to perform to your data within
48:42 - that object not data actions this is an
48:45 - array of strings that specify the data
48:46 - operations that are excluded from the
48:48 - allowed data actions assignable Scopes
48:51 - this is an array of strings that specify
48:53 - the Scopes that the custom Ro is
48:54 - available for assignment you can only
48:56 - Define why Management Group and
48:57 - assignable scopes of a custom
48:59 - role so in this example we see that
49:02 - there is an asterisk symbol that's used
49:04 - in the specific actions and the actions
49:06 - property like storage Network compute
49:08 - Etc this is called the Wild Card
49:11 - permission symbol represented as an
49:12 - asterisk this is used in the actions not
49:15 - actions data actions and not data
49:17 - actions properties to represent all or
49:19 - any operations and wild card allows you
49:22 - to apply to match
49:24 - everything in the examp example of
49:26 - actions microsoft. storage asterisk slre
49:30 - in a rle definition this means that the
49:32 - role is granted the permission to
49:33 - perform read operations on all resource
49:35 - types under the Microsoft storage
49:37 - resource provider so the Wild Card
49:40 - essentially allows you to either Grant
49:41 - or deny a wide range of permissions with
49:43 - a single statement however it should be
49:46 - used with caution because it can grant
49:47 - or deny more permissions than intended
49:49 - if not properly
49:54 - managed
49:56 - it can be a bit confusing to distinguish
49:58 - between Azure policies and Azure rules
50:00 - so we'll do a little comparison to help
50:02 - you understand the key differences for
50:04 - the Azure policies they are used to
50:06 - ensure compliance of resources they
50:08 - evaluate the state by examining
50:10 - properties on resources that are
50:11 - represented in resource manager and
50:13 - properties of some resource provider it
50:15 - doesn't restrict actions which are also
50:17 - called operations they ensure that
50:20 - resource state is compliant to your
50:21 - business rules without concern for who
50:23 - made the change or who has permission to
50:25 - make a change even if an individual has
50:27 - access to perform an action if the
50:29 - result is a non-compliant resource Azure
50:31 - policy still blocks the create or update
50:34 - as for the Azure roles they are used to
50:36 - control access to Azure resources they
50:39 - focus on managing user actions at
50:41 - different levels of Scopes and Azure
50:43 - roles do restriction on Azure
50:46 - resources another thing people tend to
50:48 - get confused between our Azure ad roles
50:50 - versus Azure roles for Azure ad roles
50:53 - they are used for managing Azure ad
50:55 - resources for Azure roles they are used
50:58 - for fine grained Access Control to Azure
51:00 - resources active directory resources
51:03 - include users groups billing licensing
51:07 - application registration Etc Azure
51:10 - resources included virtual machines
51:13 - databases cloud storage Cloud networking
51:17 - Etc by default Azure roles and Azure a
51:20 - roles are separate and do not span Azure
51:22 - and Azure a by default the global
51:24 - administrator doesn't have permissions
51:26 - to manage Azure resources Global
51:28 - administrator can gain access to Azure
51:30 - resource if granted the user access
51:32 - administrator role so azuread roles are
51:35 - specifically for managing Azure a
51:37 - resources while Azure roles focus on
51:39 - access control to Azure
51:42 - [Music]
51:45 - resources hey this is Andrew Brown from
51:48 - exam Pro and in this section we'll be
51:50 - covering Azure policies Azure policies
51:52 - enforce organizational standards and
51:54 - assess compliance at scale policies do
51:57 - not restrict access they only observe
51:59 - for compliance here is an image with a
52:01 - list of built-in policies that you can
52:03 - use right away from Azure let's go over
52:05 - a few key aspects of azure policies the
52:08 - first one are policy definitions which
52:10 - is a Json file used to describe business
52:12 - rules to control access to resources
52:15 - then we have policy assignment this is
52:17 - the scope of a policy's effect it can be
52:19 - assigned to a user a resource Group or
52:21 - Management Group next we have policy
52:24 - parameters the these are the values you
52:26 - can pass into your policy definition
52:28 - which makes your policies more flexible
52:29 - for reuse and we have initiative
52:32 - definitions this is a collection of
52:34 - policy definitions that you can assign
52:36 - for example a group of policies to
52:38 - enforce pcss
52:40 - compliance next we'll look at viewing
52:42 - non-compliant resources once a policy is
52:46 - assigned it will periodically evaluate
52:48 - the compliance State you can see how
52:50 - compliant we are on the compliance tab
52:52 - according to the example in the image it
52:54 - shows that we we are non-compliant it
52:56 - can occur due to many factors but it's
52:58 - most likely because virtual machines
53:00 - should have Disaster Recovery
53:01 - enabled so let's look at some of the
53:04 - main use cases for Azure policy
53:06 - organizational compliance Azure policy
53:09 - enforces standards and assesses
53:10 - compliance at scale such as enforcing
53:12 - compliance labels on all resources cost
53:15 - control policies can prevent over
53:17 - provisioning to save costs like limiting
53:19 - the creation of high class VNS security
53:22 - enhancements policies can improve
53:24 - security by enforcing configurations for
53:26 - example requiring secure transfer for
53:28 - All Storage accounts resource
53:31 - consistency policies can enforce
53:32 - consistent configurations like a
53:34 - specific naming convention or tag
53:36 - structure Regulatory Compliance policies
53:39 - can ensure specific configurations for
53:41 - Regulatory Compliance such as data
53:43 - hosting in specific regions for data
53:46 - sovereignty so that's in brief overview
53:48 - of azure
53:51 - [Music]
53:54 - policies the scope of an Azure policy is
53:56 - the set of resources that the policy is
53:58 - applied to when you assign a policy you
54:01 - define the scope at which the policy is
54:03 - enforced this could be as broad as a
54:05 - management group or as specific as a
54:07 - single resource the hierarchy of Scopes
54:09 - and Azure are like other Scopes like
54:11 - asure resource manager or rbac it moves
54:13 - from brought to specific in the
54:15 - following order Management Group greater
54:17 - than subscription greater than Resource
54:18 - Group greater than resource at each
54:20 - level you can apply different policies
54:22 - as per your requirements policies apply
54:25 - higher levels of the hierarchy are
54:26 - inherited by all the lower levels this
54:29 - structure provides a powerful mechanism
54:31 - for applying broad organizational
54:33 - policies while still allowing for
54:34 - flexibility and customization at lower
54:36 - levels it's a key part of how Azure
54:39 - enables you to manage and control your
54:40 - resources effectively and in a way that
54:42 - suits your organizational
54:45 - [Music]
54:48 - needs hey this is Andrew Brown from
54:50 - exampro and we'll go over the anatomy of
54:52 - an Azure policy definition file display
54:55 - name this serves as an identifier for
54:57 - the policy and has a 128 character limit
55:01 - type this field which is red only
55:03 - indicates the source of the policy it
55:05 - could be guiltin maintained by Microsoft
55:08 - custom created by you or static
55:11 - Microsoft owned and typically pertains
55:12 - to Regulatory Compliance description
55:15 - this provides the context of the policy
55:18 - metadata this optional field is used to
55:20 - store key value information on the
55:22 - policy though this determines which res
55:25 - resource types are evaluated and changes
55:26 - whether resource provider or Azure
55:28 - resource manager is used resource
55:30 - manager modes include all this includes
55:33 - resource groups subscriptions and all
55:35 - resource types and indexed this only
55:37 - includes resource types that support
55:39 - tags and location resource provider
55:41 - modes were used in deprecated services
55:43 - like Microsoft container service data
55:45 - and are now primarily utilized in
55:47 - services like Microsoft kubernetes data
55:49 - and Microsoft key Vault data moving on
55:52 - to parameters parameters are values
55:55 - passed into the policy to improve its
55:56 - flexibility a parameter has the
55:58 - following properties name the identifier
56:01 - for the parameter type could be a string
56:03 - array object Boolean integer float or
56:06 - datetime metadata utilized by Azure to
56:09 - display userfriendly information such as
56:11 - description display name strong type
56:13 - assign permissions default value an
56:16 - optional field to set a default
56:17 - parameter value allowed values an
56:20 - optional field for setting accepted
56:22 - parameter values you reference
56:24 - parameters by using field and in next
56:27 - let's go over the policy rule so this
56:29 - consists of if and then blocks in the if
56:32 - block you define one or more conditions
56:34 - that specify when the policy is enforced
56:36 - you can apply logical operators to these
56:39 - conditions to precisely Define the
56:40 - scenario for a
56:42 - policy the next concept we'll look into
56:44 - is the policy Rule and policy effect
56:46 - which is important in determining the
56:48 - impact of the policy so we'll go over a
56:51 - list of common policy effects deny if a
56:54 - resource is created creation or update
56:55 - doesn't adhere to the policy it fails
56:58 - audit this creates a warning event in
56:59 - the activity log when evaluating a
57:01 - non-compliant resource but it doesn't
57:03 - stop the request upend this effect adds
57:06 - extra parameters or FS to the resource
57:08 - during its creation or update for
57:10 - example it could append tags on
57:11 - resources like cost center or specify
57:13 - allowed IP addresses for a storage
57:16 - resource audit if not exists similarly
57:18 - to the audit effect this creates a
57:20 - warning event in the activity log when a
57:22 - resource doesn't comply with the policy
57:24 - but it doesn't stop the request audit is
57:26 - used to audit the properties of a
57:27 - resource while audit if not exists is
57:29 - used to audit the existence of a related
57:31 - resource deploy if not exists this
57:34 - effect executes a template deployment
57:36 - when a specific condition is met for
57:38 - example if SQL encryption is enabled on
57:40 - a database a template can be executed
57:42 - after the database creation to configure
57:44 - it in a specific way and the last one is
57:47 - disabled this effect turns off the
57:49 - policy rule often used for testing
57:51 - purposes these policy effects provide a
57:54 - range of resp responses to
57:55 - non-compliance enabling you to manage
57:57 - your resources according to your
57:58 - organization specific
58:02 - [Music]
58:05 - requirements hey this is angrew Brown
58:07 - from exam Pro and we're going to take a
58:08 - look at how to keep things compliant
58:10 - within our Azure account with Azure
58:12 - policy so let's make our way over to the
58:14 - Azure policy portal by typing policy at
58:17 - the top here and right away you can see
58:19 - already that I have a policy assigned I
58:21 - didn't sign this Azure did this for me
58:23 - by default and it's a great way to start
58:26 - understanding how this is useful so I'm
58:27 - going to go ahead and just click into
58:29 - that one there and this is an initiative
58:31 - uh policy meaning that it's made up of a
58:33 - bunch of policies we'll go over that
58:35 - over in a moment but you can see here
58:37 - that's saying hey you should turn on all
58:38 - these things they're not turned on by
58:40 - default so you should turn them on and
58:42 - these are ones that you have not turned
58:43 - on you might not want to consider
58:45 - turning some of these on uh if you have
58:47 - to consider costs but I think most of
58:49 - these are free um but that's something
58:51 - that you'll have to decide on your own
58:53 - uh but that gives you a general idea
58:54 - there so let's go back to policy at the
58:56 - top on left hand side here we can see
58:58 - our assignments and here I have that
59:00 - initiative type there if I click into it
59:03 - allows me to edit it there uh and what's
59:05 - really nice is if I want to disable it I
59:07 - could just disable it uh but I think
59:09 - this is a default one so oh no I can
59:10 - disable it okay great and you can see it
59:13 - was assigned by who uh and then you can
59:15 - also uh take remediation actions uh here
59:18 - so by default this assignment will only
59:19 - take effect on newly created uh
59:21 - resources existing resources can be
59:22 - updated via a remediation task after the
59:24 - policy is assigned so if you need this
59:27 - apply to ones that are created or or
59:29 - prior or updated that's something you
59:32 - might want to do but anyway we'll get
59:33 - out of there and we'll go take a look at
59:35 - all the big list of predefined uh
59:38 - definitions uh that Azure gives us so
59:41 - this is a great way for us to get
59:42 - started um you'll notice on the right
59:45 - hand side we have initiative and we have
59:46 - policy again initiative is a collection
59:48 - of policies down below and these are
59:50 - individual ones let's just go take a
59:52 - look at some uh policies and maybe we we
59:54 - can go apply one to like a virtual
59:56 - machine um so what I'm going to do is
59:58 - just drop down um this here and we can
60:02 - just unselect here and let's just go
60:04 - take a look do we have one called
60:07 - virtual machines not really um but we we
60:11 - go for compute here and we can just take
60:14 - a look here so uh audit virtual machine
60:16 - without Disaster Recovery let's take a
60:19 - look at that
60:21 - one and I just want to see the full
60:23 - description here audit virtual machine
60:25 - without Disaster Recovery configured I
60:27 - think that sounds really good to me so
60:29 - I'm going to go ahead and assign that
60:31 - and we can choose a
60:33 - scope and so I'm going to choose my
60:35 - subscription here and then we can choose
60:36 - a resource Group and you're going to
60:38 - notice that I don't actually have any
60:39 - resources so uh to scope that within so
60:42 - what we'll go do is launch ourself a
60:44 - virtual machine so make our way over to
60:46 - the virtual machine um portal here we'll
60:49 - add a virtual
60:51 - machine and I want something really
60:53 - cheap and so here it's set to a more
60:55 - expensive one because I was launching a
60:57 - Windows Server uh previously but I want
60:59 - the cheapest server I can get actually
61:02 - before I do that I'm just going to go
61:03 - back and oh yeah it's on Ubuntu just
61:05 - making sure and the one I had that was
61:08 - really cheap is the B1
61:11 - LS and so I'm just going to go up here
61:13 - to the top we'll name this
61:16 - beour and uh we'll put in the beour
61:19 - group
61:21 - there um this size does not support
61:23 - Azure spot that's totally fine I don't
61:25 - need spot did I turn that on by accident
61:27 - oh I
61:28 - did and this all looks fine to me I'm
61:31 - not going to be logging into this
61:32 - machine we just want to launch it up as
61:34 - an example I'm going to move to standard
61:35 - SSD CU I don't need premium here and
61:38 - this looks fine to me we'll go ahead and
61:40 - hit review and create so uh we'll go
61:44 - ahead and create
61:47 - that and I guess we have to download
61:49 - that private key probably a good idea
61:50 - we're not going to use it for anything
61:51 - but that's
61:53 - okay
61:55 - and now it's deploying let's see if we
61:57 - can select the scope as it's creating
61:58 - I'm not sure if it'll let us do that I'm
61:59 - just going to hit cancel here and reopen
62:02 - here and let's see if it shows up now it
62:05 - doesn't so uh we'll just wait a moment
62:08 - here uh I'll go back to my policy
62:11 - here and I'm just going to wait for this
62:13 - server to create and once that is
62:15 - created um what we'll do is we'll just
62:18 - come back here and see if we can now
62:20 - scope that all right so after a short
62:22 - little wait um our server is ready here
62:24 - so let's make our way back to our policy
62:26 - here and uh I guess I lost it so I have
62:30 - to go back to
62:32 - definitions and we'll drop down
62:34 - categories deselect we'll go to
62:36 - compute and we'll go ahead and uh click
62:39 - on Disaster Recovery again here you can
62:41 - see the policy and its entirety so
62:43 - that's kind of nice uh we'll go ahead
62:44 - and assign that and we'll choose our
62:46 - scope so we'll go back here choose
62:48 - subscription one now we should have
62:50 - beour and I'll go ahead and hit select
62:53 - uh we can assign it exclusions not
62:55 - something I'm going to do today um there
62:57 - is the name of it that's totally fine
62:58 - then we can hit enabled and I'm the one
63:00 - who's enabling it so that sounds like a
63:02 - good idea we'll go next uh we're going
63:04 - to leave REM remediation off we don't
63:06 - need to do that today we'll hit create
63:08 - uh and so now this policy is assigned it
63:10 - says here please note that the
63:11 - assignment takes around 30 minutes to
63:13 - take effect so if we work our way back
63:15 - here and look at
63:16 - assignments um I don't see it here yet
63:19 - so there it
63:20 - is and um it's probably not going to
63:23 - show us on this there so we'll go back
63:25 - to our
63:26 - overview and so I want to see it hasn't
63:30 - started yet so we'll just wait for a
63:32 - while here might take 30 minutes might
63:33 - take 10 not sure and we'll just see what
63:36 - happens this should show up as
63:37 - non-compliant but let's see that
63:39 - actually happen all right so after
63:41 - waiting a little while here I think it
63:43 - was about 15 20 minutes we can now see
63:45 - that it's saying that it's non-compliant
63:47 - so we'll go ahead and click into
63:50 - there and we can see what exactly it is
63:53 - complaining about
63:54 - let me click over to that
63:56 - resource so there you go it's not too uh
63:59 - complicated there uh we'll go ahead and
64:01 - just uh delete that
64:05 - assignment and uh if you're wondering
64:07 - where blueprints are they're all the way
64:08 - over here we talked about blueprints in
64:10 - the actual course not something we
64:11 - actually have to do because it is
64:13 - probably a better way of uh doing things
64:16 - I just wanted to point that out uh to
64:18 - you um but yeah there you go so it's as
64:20 - simple as
64:23 - that
64:25 - hey this is Andrew Brown from exam Pro
64:28 - and in this section we'll be covering
64:29 - Azure resource manager Azure resource
64:31 - manager is a service that allows you to
64:33 - manage Azure resources Azure resource
64:36 - manager is a collection of services in
64:38 - the Azure portal so you can't simply
64:39 - type in Azure resource manager in the
64:41 - search tab it is a management layer that
64:43 - allows you to create update or delete
64:46 - resources apply management features such
64:48 - as access controls locks or tags and
64:51 - right infrastructure is code using Json
64:53 - templat
64:54 - we will be examining the following key
64:56 - components that form the Azure resource
64:58 - manager layer we have subscriptions
65:00 - management groups resource groups
65:02 - resource providers resource locks Azure
65:04 - blueprints as well as resource tags
65:07 - Access Control role-based access
65:09 - controls Azure policies and arm
65:12 - templates you can think of azure
65:14 - resource manager as a gatekeeper all of
65:16 - the requests flow through arm and it
65:18 - decides whether that request can be
65:19 - performed on a resource such as the
65:21 - creation updating and deletion of a
65:23 - virtual machine and its arm's
65:24 - responsibility to authenticate and
65:26 - authorize these requests arm uses
65:28 - azure's role-based Access Control to
65:30 - determine whether a user has the
65:32 - necessary permissions to carry out a
65:33 - request when a request is made arm
65:36 - checks the users assigned roles and the
65:38 - permissions associated with those roles
65:40 - if the user has the necessary
65:41 - permissions the request is allowed
65:43 - otherwise it is
65:45 - denied the next concept we'll go over is
65:48 - the scope for Azure resource manager
65:50 - we've briefly covered scope in Azure
65:51 - policy and Azure rbac but we'll go into
65:54 - more detail with them in the following
65:55 - sections for
65:56 - arm so scope is a boundary of control
65:59 - for Azure resources it is a way to
66:01 - govern your resource by placing
66:03 - resources within a logical grouping and
66:05 - applying logical restrictions in the
66:07 - form of rules management groups are a
66:09 - logical grouping of multiple
66:11 - subscriptions subscriptions Grant you
66:13 - access to Azure Services based on a
66:15 - billing and support agreement resource
66:17 - groups are a logical grouping of
66:19 - multiple resources and resources can be
66:21 - a specific Azure service such as Azure
66:23 - VM
66:25 - so that's an overview of azure resource
66:28 - [Music]
66:31 - manager Azure management groups provide
66:34 - a way to manage multiple subscriptions
66:36 - by organizing them into a hierarchical
66:38 - structure every directory is assigned a
66:40 - single top level management group known
66:42 - as The Root Management Group this root
66:44 - group forms the base of the hierarchy
66:46 - and can have multiple management groups
66:48 - or subscriptions nested under it one of
66:50 - the key benefits of using management
66:52 - groups is that all subscriptions within
66:54 - a Management Group automatically inherit
66:56 - the conditions apply to the Management
66:57 - Group some important facts about
67:00 - management groups you should know are a
67:02 - single directory can support up to
67:03 - 10,000 management groups the hierarchy
67:06 - of a Management Group tree can be up to
67:08 - six levels deep not including the root
67:10 - level or the subscription level each
67:12 - Management Group and subscription could
67:14 - have only one parent each Management
67:16 - Group could have multiple children all
67:19 - subscriptions and management groups
67:20 - exist within a single hierarchy in each
67:22 - directory so that's a short overview of
67:25 - azure management
67:27 - [Music]
67:30 - groups before you can do anything in
67:32 - your Azure account you'll need to have a
67:34 - subscription an Azure account can have
67:37 - multiple subscriptions and the most
67:38 - commonly used ones are free trial pay as
67:41 - you go and Azure for students for
67:44 - example if you wanted developer support
67:46 - you would add a developer support
67:47 - subscription to your account once a
67:49 - subscription is set up it provides you
67:51 - with the ability to configure various
67:53 - settings and features features such as
67:55 - resource tags these allow you to
67:56 - categorize your resources according to
67:58 - your organizational needs access
68:00 - controls helps manage access and
68:02 - permissions for your Azure resources
68:04 - resources groups are logical containers
68:07 - in which Azure resources are deployed
68:08 - and managed cost management and billing
68:11 - provides tools to track and manage your
68:12 - Cloud spending and more these features
68:15 - provide you with a high degree of
68:17 - flexibility and control over your Azure
68:19 - resources allowing you to manage your
68:20 - resources effectively and
68:22 - securely
68:27 - the next topic will be covering a
68:29 - resource groups so a resource Group is a
68:32 - container that holds related resources
68:33 - for an Azure solution for example you
68:36 - might have a resource Group that
68:37 - contains multiple virtual machines for a
68:39 - specific project or application as for
68:42 - resources these are manageable items
68:44 - available through Azure a resource could
68:46 - be an individual entity like a virtual
68:47 - machine next we have resource providers
68:50 - these are services that Supply Azure
68:52 - resources an example of a resource
68:54 - provider is Microsoft compute which
68:56 - provides compute resources like VMS in
69:00 - order to use Azure resources you have to
69:02 - register resource providers many
69:04 - resource providers are registered by
69:05 - default for you with your subscription
69:08 - however for certain resources you may
69:10 - need to manually register the resource
69:11 - provider this image shows a list of
69:13 - resource providers available in Azure
69:15 - and in the status you can see if they
69:17 - are either registered or not registered
69:19 - you can register resource providers
69:21 - under your subscription in the Azure
69:23 - portal through Azure Powershell or Azure
69:25 - CLI this ensures you have access to the
69:27 - latest resources and features provided
69:29 - by that
69:31 - [Music]
69:34 - service the next topic we'll explore our
69:37 - resource tags so a tag is a pair
69:39 - consisting of a key and a value that you
69:41 - can assign to Azure resources these tags
69:43 - can be used to categorize resources
69:45 - based on different criteria relevant to
69:47 - your organization here are some examples
69:49 - of tags Department equals Finance status
69:53 - equals approved approved team equals
69:55 - compliance environment equals production
69:58 - project equals Enterprise location
70:00 - equals West us tags allow you to
70:02 - organize your resources in the following
70:04 - ways Resource Management tags can help
70:07 - you sort and manage resources based on
70:09 - specific workloads or environments such
70:11 - as developer environments cost
70:13 - management and optimization tags can be
70:15 - used for cost tracking setting budgets
70:17 - and creating alerts operations
70:20 - management tags can be used to manage
70:22 - business commitments and service level
70:23 - agreement operations such as Mission
70:25 - critical Services security tags can be
70:28 - used for classifying data and assessing
70:30 - security impact helping you manage your
70:32 - security posture other helpful ways
70:35 - include governance and Regulatory
70:37 - Compliance Automation and workload
70:39 - optimization all in all tags provide a
70:42 - flexible customizable method for
70:43 - managing your Azure resources according
70:45 - to your specific
70:47 - [Music]
70:50 - needs the next topic will be covering a
70:53 - res resource locks resource locks are a
70:55 - critical feature in Azure that helps
70:57 - Safeguard important resources from
70:58 - accidental modifications or deletions as
71:01 - an admin you may need to lock a
71:03 - subscription Resource Group or resource
71:05 - to prevent other users from accidentally
71:07 - deleting or modifying critical resources
71:09 - especially in environments with multiple
71:11 - administrators or automated processes in
71:14 - the Azure portal you can set the
71:15 - following lock levels him not delete
71:18 - this lock ensures authorized users can
71:20 - still read and modify a resource but
71:22 - they can't delete the resource
71:24 - read only this lock ensures authorized
71:26 - users can read a resource but they can
71:28 - delete or update the resource there are
71:31 - a number of ways to manage locks here
71:33 - are some of them Azure portal you can
71:35 - easily create View and delete locks
71:37 - through the Azure portal Azure poers
71:39 - shell UCM dlet like new as resource lock
71:41 - to manage locks Azure CLI commands like
71:44 - as lock create help manage locks Azure
71:47 - resource manage your templates you can
71:48 - also Define locks in your arm
71:52 - templates
71:55 - [Music]
71:56 - hey this is Andrew Brown from exam Pro
71:58 - and in this section we'll be covering
71:59 - Azure blueprints Azure blueprints enable
72:02 - quick creation of govern subscriptions
72:04 - the key term here is governed while one
72:06 - can easily create a subscription in
72:08 - their account a govern subscription
72:09 - indicates there's a process and set
72:11 - expectations for how the subscription
72:13 - should be configured Azure blueprints
72:15 - allow you to compose artifacts based on
72:17 - common patterns or those specific to an
72:19 - organization into reusable blueprints
72:21 - the service is designed to help with
72:22 - environment set
72:24 - the service is designed to help with
72:25 - environment setup blueprints are a
72:27 - declarative way to orchestrate the
72:29 - deployment of various resource templates
72:31 - and other artifacts such as Ro
72:33 - assignments policy assignments Azure
72:36 - resource manager templates resource
72:38 - groups Azure blueprint Services powered
72:41 - by the globally distributed Azure Cosmos
72:43 - DB ensuring blueprint objects are
72:45 - replicated across multiple regions
72:47 - providing redundancy and
72:49 - resilience a common query is the
72:51 - difference between an arm template and
72:52 - an Azure blueprint
72:54 - nearly everything that you want to
72:55 - include for deployment in Azure
72:56 - blueprints can be accomplished with an
72:58 - arm template with arm templates you can
73:00 - store them either locally or in Source
73:02 - control there isn't an active connection
73:05 - or relationship to the arm template post
73:07 - deployment on the other hand Azure
73:09 - blueprints maintain a connection between
73:11 - the blueprint definition what should be
73:12 - deployed and the blueprint assignment
73:14 - what has been deployed Azure blueprints
73:16 - can upgrade multiple subscriptions
73:18 - simultaneously if they're governed by
73:20 - the same blueprint this means Azure
73:22 - blueprint supports Improv tracking and
73:24 - auditing of deployments so that's an
73:26 - overview of azure
73:28 - [Music]
73:31 - blueprints hey this is Andrew Brown from
73:34 - exam Pro and what we're going to be
73:35 - doing is moving resources from one
73:37 - Resource Group to another and
73:39 - understanding the limitations around
73:40 - that uh so I just have this page pulled
73:42 - up here because I just want to emphasize
73:44 - that I've seen exam questions on this
73:46 - stuff and there's a lot of little
73:48 - nitty-gritty things that could show up
73:50 - as a solution so the idea is that when
73:52 - you're talking about about moving
73:54 - resources um you know you're moving them
73:56 - uh into different regions or to
73:57 - different subscriptions or different
73:58 - resource groups and generally they're
74:00 - pretty straightforward but there are
74:02 - some edge cases where uh things will not
74:04 - work as expected and that's based on um
74:07 - some particular services so when we're
74:09 - looking at app Services devop Services
74:12 - classic deployment Network movement
74:14 - guidance Recovery Services virtual
74:15 - machines so I definitely know that for
74:18 - app Services you're going to run into
74:20 - issues uh like if you're moving from uh
74:23 - one subscription to another and you
74:25 - already have a web app uh service in the
74:27 - one that you're moving it into it won't
74:29 - allow you to do it and so there's a lot
74:31 - of little things like that
74:33 - okay um and also if you are migrating
74:35 - them you can go ahead and use the
74:37 - diagnostic tool to uh debug it and it
74:40 - will tell you some additional
74:42 - information so you will have to read
74:44 - through all of these I just can't show
74:46 - you all that in a follow along um but
74:49 - what we'll do is we'll just go through
74:51 - the basics here of moving things between
74:53 - between resource groups and so what
74:54 - we'll need is a few resource groups so
74:56 - I'm going to add a new one and we're
74:58 - going to call this one the USS fed or
75:01 - the Federation of
75:05 - planets and we'll put that uh East
75:09 - us and then we will make another
75:12 - Resource Group and we'll call this the
75:13 - Klingon
75:18 - Empire and for fun we will place it in
75:21 - uh West
75:27 - so we'll go ahead and create that so now
75:28 - we just need something we can move
75:30 - around back and forth so what I'm going
75:32 - to do is I'm going to make my way over
75:34 - to diss because that's a great example
75:36 - of something we can move around and
75:37 - change and so I'm going to add a new
75:40 - dis and uh I'm going to place this in
75:42 - the
75:45 - Federation and I'm just going to say um
75:51 - dilithium I can never spell that right
75:53 - right let's see if I can get the proper
75:55 - spelling for this thing that is not real
75:58 - di lithium
76:00 - here and so it's going to be over here I
76:03 - don't care about availability zones and
76:04 - stuff like that Source type this is
76:07 - fine and so we have the initial size I
76:10 - do not need a drive that big let's go
76:12 - super
76:13 - small uh we could even do an hcd because
76:16 - those are even cheaper and we'll just
76:18 - choose the small size down
76:21 - here um and so we'll pick
76:26 - 32 and so this should be alal encryption
76:29 - networking we don't really care we're
76:31 - not doing anything other than moving
76:32 - this around so we'll go ahead and review
76:34 - and
76:35 - create right and so then we'll go create
76:37 - that
76:40 - resource and we'll just wait a little
76:42 - while here it shouldn't take too
76:45 - long all right so the dis is ready so
76:48 - we'll go to this resource and here it is
76:50 - so now let's say we want to move that to
76:53 - our other resource Group which is in
76:55 - another region what we're going to do is
76:58 - we're going to go back to our resource
77:01 - groups
77:03 - here and if we go into the Federation of
77:06 - planets what we can do is uh go to
77:10 - overview select the dis go over here and
77:14 - say move to another Resource Group and
77:16 - I'll just click these other ones to show
77:17 - you I don't have any other subscriptions
77:18 - to move this to um so I'll just go back
77:22 - there I thought maybe could show you
77:23 - some stuff but I just realized I don't
77:24 - have much in this account or with other
77:27 - subscriptions so we'll just go back to
77:30 - overview here and just move it to
77:31 - another Resource Group and technically
77:33 - we're moving it to another region so
77:35 - we'll see how that
77:36 - goes um and so we will
77:39 - select um the klling on Empire I
77:42 - understand that the tools and scripts
77:44 - associated with the will not work uh etc
77:46 - etc so we'll hit
77:52 - okay
77:54 - and that's going to take a little bit of
77:56 - time to move so while that is going I'm
77:58 - going to make a new tab so we don't lose
77:59 - that history there I just want to talk
78:01 - about some settings you can put on your
78:03 - resource groups so if we go into the
78:05 - Kling on Empire and we go down below
78:08 - here we should have the ability to apply
78:10 - locks and there are a few different
78:13 - types of locks we have readon and delete
78:15 - so readon means it's a readon resource
78:18 - and delete means we cannot delete the
78:21 - resource which makes sense right so read
78:23 - only should mean we shouldn't be able to
78:24 - modify this so I'm going to say uh don't
78:27 - touch do not
78:31 - touch and I say okay here and so what
78:35 - that means is that I should not be able
78:36 - to
78:38 - modify the resource uh whether I can
78:40 - delete it or not is another story we'll
78:42 - find that out and another question is
78:44 - can I move that resource outside of this
78:46 - if it's set to those modes and that's
78:49 - what we're going to find out so moving
78:50 - back over here um has this finished
78:53 - moving it's still validating so we're
78:54 - going to have to wait a little bit here
78:56 - and I'll see you back in a moment all
78:58 - right so after waiting a little while
78:59 - here I believe that it's done moving so
79:01 - I'm still in my old Resource Group here
79:03 - what we'll do is click back into here go
79:05 - to overview and we see that it's no
79:07 - longer there but if we go over to the
79:09 - cing on Empire we go back over just
79:12 - clicking to overview there it is so we
79:14 - had no problems moving the dis to
79:16 - another region and another Resource
79:18 - Group now we did apply this lock over
79:21 - here so what is going to happen when we
79:24 - try to
79:25 - modify this dis I think you know the
79:28 - answer but let's give it a go just to
79:29 - see what happens so over here we're
79:32 - going to want to uh re resize it here
79:35 - I'm going to pick 64 I'm going to go hit
79:37 - resize and it says fail to update the
79:39 - dis because of that reason so um there
79:42 - you cannot perform a right operation
79:44 - because of the lock so there you go um
79:46 - now let's go back and we are going to go
79:50 - apply uh another lock here this time I'm
79:53 - going to make it so you can't delete it
79:55 - and so we say don't do not delete
80:00 - me all right and so what we'll do is
80:04 - we'll go back uh and we'll try to delete
80:07 - it so we'll hit the delete button we'll
80:10 - say Yes again it says you cannot do it
80:12 - now here's the next question can you
80:13 - move that resource out or in to Resource
80:16 - Group if it has lock or read only on uh
80:20 - and that's what I want you to guess
80:23 - whether you can or not CU you are in
80:25 - read only so would you be able to move
80:27 - something that's readon so we'll go
80:29 - ahead and move it we're going to move
80:30 - this to another Resource Group and I'm
80:32 - going to send it back to the Federation
80:35 - of planets I say I understand hit okay
80:39 - and it failed the check we'll see why
80:41 - cannot perform the right operation
80:43 - because there are locks please remove
80:44 - some of the locks so which is it is it
80:46 - delete is it the read only do I have to
80:48 - remove both that's what we need to go
80:50 - find out so back in our Resource Group
80:53 - I'm going to go ahead and first remove
80:56 - parent resource locks can't be edited
80:58 - here okay that's fine so we'll go back
81:00 - to here and we will go into the locks
81:04 - and we'll first delete the
81:08 - readon and then what we'll do is we will
81:11 - attempt
81:13 - to uh delete this
81:16 - again see what happens it looks like
81:19 - it's moving so it's not that you can't
81:22 - move it uh out if it's in readon or
81:25 - sorry in delete but you but you can't if
81:27 - it's read only because it's readon right
81:29 - now when we're talking about moving
81:30 - resources into a group that's set to
81:32 - read only I'm almost certain that you
81:34 - can absolutely do that um so uh we could
81:38 - stage that I guess just to make sure so
81:40 - what I'm going to do is I'm just going
81:42 - to set up another Resource Group just to
81:46 - make sure that we we know this for
81:48 - certain just in case I Wrong the star
81:51 - the Romulan star
81:53 - Empire Romulan star
81:56 - Empire and I'm going to just leave in
81:58 - the same region and so for this one in
82:02 - particular what I'm going to do is apply
82:05 - a readon I might as well just put both
82:07 - on here but I'll just I'm just going to
82:09 - do read only and we'll say do not delete
82:13 - me and then what I'll do is I'll go back
82:15 - to the Federation
82:17 - planets and I'm just going to actually
82:19 - I'll just go to discs and we'll create a
82:21 - new dis here
82:27 - um and we will place it in the
82:28 - Federation
82:30 - here and we will just say um dark
82:35 - matter or
82:40 - antimatter and we will just change this
82:43 - to hhd as small as we can go we'll
82:46 - review and create that and we'll go
82:48 - ahead and create
82:50 - that and this will not take long we do
82:53 - not have to wait that long for
82:57 - this and notice here that
83:01 - um we could not uh this is interesting
83:04 - so there was an error moving the
83:05 - resource moving resources failed because
83:08 - resource grou Federation has active
83:09 - deployment so if you are moving stuff uh
83:12 - and then you do deploy it's going to
83:14 - cancel that deploy so that's interesting
83:16 - to know um so I believe that this new
83:21 - one is deployed I cannot remember I
83:23 - think we set this one to have read only
83:26 - here on the um what do you call it the
83:30 - uh rulin star Empire which isn't showing
83:34 - up there it is just double check there
83:37 - so the question is can I move a resource
83:39 - into a
83:42 - readon we'll go
83:44 - here and uh I guess we need to actually
83:47 - go to the Federation of
83:50 - planets we'll go to overview view we
83:52 - have antimatter and I want to move
83:56 - that into
83:58 - Romulan star Empire say I
84:02 - understand it'll either say we can or we
84:04 - can't and we absolutely can otherwise it
84:07 - would aired out at this point so
84:08 - hopefully you can keep that straight so
84:10 - you can move a resource into a group
84:13 - that has readon you cannot move it out
84:16 - if it's set you can absolutely move it
84:18 - out if it's set to delete delete just
84:20 - protects against delete if you are
84:22 - moving your resource from one Resource
84:23 - Group to another uh and then you deploy
84:26 - something it's going to cause that
84:28 - movement to fail and then there's those
84:31 - edge cases for uh moving Resources with
84:35 - Azure and that's something you should
84:37 - spend some time reading up on those use
84:39 - cases or maybe I'll just uh pick out the
84:41 - most important ones and put it into a
84:42 - cheat sheet all right uh and so what I'm
84:45 - going to do is just go ahead and clean
84:46 - this stuff up um so what I'm need to do
84:50 - here is go and and remove the delete I
84:53 - think it's only on here so if I go to my
84:58 - locks I'm going to go back to Resource
85:00 - groups
85:02 - here and we will delete the locks
85:09 - here and so now I can go ahead and
85:11 - delete
85:14 - these groups I guess I have to do them
85:16 - one by one which is kind of annoying but
85:17 - that's just how it
85:21 - goes
85:33 - [Music]
85:34 - it says it's locked it's not locked
85:36 - anymore oh it's still there look at
85:40 - that
85:45 - okay could have swore I definitely uh
85:48 - did something there there we
85:51 - go
85:53 - and we will go ahead and delete the last
85:58 - one there we are so yeah hopefully you
86:00 - know a bit more about moving resources
86:02 - [Music]
86:06 - around hey this is Andrew Brown from
86:08 - exam Pro and in this segment we'll be
86:10 - diving into arm templates so what
86:12 - exactly is infrastructure is code
86:15 - infrastructure is code is the process of
86:17 - managing and provisioning computer data
86:18 - centers such as those in Azure using
86:20 - machine readable definition files like
86:22 - JSO n files rather than depending on
86:25 - physical Hardware configuration or
86:26 - interactive configuration tools you
86:29 - write a script that will set up cloud
86:30 - services for you there are two main
86:32 - approaches to IAC declarative here you
86:36 - describe your desired outcome and the
86:37 - system figures out how to achieve it
86:40 - imperative here you provide Specific
86:42 - Instructions detailing exactly how to
86:44 - reach the desired State arm templates or
86:46 - JSO n files that Define Azure resources
86:49 - you want to provision and Azure Services
86:51 - you want to configure with arm templates
86:53 - you can ensure a declarative approach
86:55 - meaning you merely Define your intended
86:57 - setup and the system handles the rest
87:00 - build remove or share entire
87:01 - architectures in minutes reduce
87:03 - configuration mistakes and know exactly
87:06 - what you have defined for a stack to
87:08 - establish an architecture Baseline for
87:10 - compliance or over arm templates Empower
87:13 - you to establish an architecture
87:15 - Baseline for compliance achieve
87:17 - modularity break up your architecture in
87:19 - multiple files and reuse them Ensure
87:22 - sensibility add Powershell and Bash
87:24 - scripts to your templates test using the
87:26 - arm template toolkit preview changes
87:29 - before you create infrastructure via
87:30 - template see what it will create
87:32 - built-in validation will only deploy
87:34 - your template if it passes track
87:36 - deployments keep track of changes to
87:38 - architecture over time policy is code
87:41 - apply Azure policies to ensure you
87:43 - remain compliant use Microsoft
87:45 - blueprints which Forge a connection
87:47 - between a resource and its template
87:49 - integrate with CI CD Pipelines utilize
87:52 - exportable code letting you capture the
87:54 - current state of resource groups and
87:56 - individual resources and benefit from
87:59 - Advanced authoring tools for instance
88:01 - Visual Studio code offers sophisticated
88:03 - features tailored for crafting arm
88:05 - templates so as you can see arm
88:07 - templates has quite a lot of
88:10 - [Music]
88:13 - uses all right moving forward let's
88:16 - delve into the structure or the skeleton
88:17 - of arm templates skeleton is a term used
88:20 - to describe the basic framework
88:22 - structure of an arm template think of it
88:24 - as the blueprint that guides how an arm
88:25 - template should be organized and what
88:27 - elements it should contain schema this
88:29 - describes the properties that are
88:31 - available within a template content
88:33 - version this denotes the version of your
88:35 - template you can provide any value for
88:37 - this element API profile use this value
88:40 - to avoid having to specify API versions
88:43 - for each resource in the template
88:45 - parameters these are the dynamic values
88:47 - you feed into your template when you're
88:48 - deploying or updating resources it
88:50 - offers flexibility and enabling you to
88:52 - use the same template in different
88:54 - scenarios or environments just by
88:55 - changing the parameter values variables
88:58 - this is where you can process or
89:00 - transform the parameters or resource
89:01 - Properties by using function Expressions
89:04 - you can manipulate input values making
89:06 - your template more Dynamic and adaptable
89:08 - functions within the arm template you
89:10 - can Define user specific functions this
89:13 - allows for reusable custom logic
89:15 - reducing redundancy and simplifying the
89:17 - template resources here you list out all
89:20 - the Azure resources you intend to deploy
89:22 - update it defines what your
89:23 - infrastructure looks like and how each
89:25 - component is configured outputs after a
89:28 - successful deployment you might want to
89:29 - retrieve specific values or results the
89:32 - output section is where you define these
89:33 - values be at the IP address of a newly
89:35 - created VM or the URL of a web app
89:38 - overall an arm template skeleton
89:40 - provides a structured and consistent
89:42 - approach to Define deploy and manage
89:44 - Azure
89:46 - [Music]
89:49 - resources moving forward let's just
89:51 - discuss one of the pivotal components of
89:53 - the arm template the resources resource
89:56 - this represents any Azure component or
89:58 - service you wish to provision it could
90:00 - be a virtual machine a database or a
90:02 - network interface Etc type this defines
90:05 - the kind of resource you're provisioning
90:07 - this typically follows the format of
90:09 - resource provider /resource type for
90:11 - instance if you're looking to create a
90:13 - storage account you'd use the type
90:15 - microsoft. storage SL storage accounts
90:18 - API version each resource type
90:20 - corresponds to an API version which is
90:22 - essentially the version of the rest API
90:24 - used for that particular resource it's
90:26 - important to note that each resource
90:28 - provider published has its own API
90:30 - versions so you need to ensure you're
90:31 - using the correct one for your chosen
90:33 - resource name this attribute specifies
90:36 - the unique name of the resource for
90:38 - example if you're setting up a virtual
90:39 - machine this could be my virtual machine
90:42 - location this is a common attribute for
90:44 - most resources it determines the Azure
90:46 - region where your resource will be
90:48 - deployed such as East us or west Europe
90:51 - other property beyond the basic
90:53 - attributes each resource type has its
90:54 - own set of properties that allow for
90:56 - deeper configuration these properties
90:58 - can very widely depending on the
91:00 - resource for a virtual machine it could
91:02 - be the size or the operating system for
91:04 - a database it might be the capacity or
91:06 - replication
91:08 - [Music]
91:11 - settings in this segment we'll delve
91:14 - into a fundamental component of arm
91:15 - templates the parameters parameters play
91:18 - a critical role in arm templates they
91:20 - allow you to pass specific values into
91:22 - your template thus allowing you to
91:23 - create more flexible and dynamic
91:25 - infrastructure
91:26 - configurations defining a parameter as
91:29 - shown in the example to define a
91:30 - parameter named storage name it's a
91:32 - tight string with a minimum length of
91:34 - five characters and a maximum length of
91:36 - 20 characters once you've defined a
91:38 - parameter you can then reference it in
91:40 - various parts of your template such as
91:42 - type API version name and so forth type
91:46 - dictates the expected data type for the
91:47 - input value common types include string
91:50 - secur string in bu object secure object
91:53 - and array default value if no value is
91:56 - provided it will be set to default value
91:59 - allowed values this is an array of
92:01 - allowed values Min value the minimal
92:04 - possible value max value the maximum
92:07 - possible value Min length the maximum
92:10 - length of characters or array max length
92:13 - the maximum length of characters or
92:15 - array description the description that
92:17 - will be displayed to the in the Azure
92:19 - portal in summary parameters are the The
92:21 - Gatekeepers of customization and arm
92:28 - templates the next topic we'll be
92:30 - covering are the arm template functions
92:33 - functions and arm templates are powerful
92:35 - tools that allow you to transform and
92:36 - manipulate your arm variables think of
92:38 - them as the building blocks that enable
92:40 - you to create more Dynamic and flexible
92:42 - configurations template functions these
92:44 - are built-in functions provided by Azure
92:46 - for a wide range of common tasks user
92:49 - defined functions these are custom
92:51 - functions you can create to cater to
92:52 - specific needs that aren't addressed by
92:54 - the built-in template functions
92:56 - functions are called using parentheses e
92:58 - such as the example shown here
93:01 - categories of template functions array
93:04 - functions tools for handling arrays some
93:06 - of these include array concat contains
93:09 - create array empty first Etc comparison
93:12 - functions for equating or contrasting
93:14 - values coess equals less lesser equals
93:17 - greater and greater or equals date
93:20 - functions to manipulate and time date
93:22 - time ad UTC now deployment functions
93:25 - pertaining to the deployment itself
93:27 - deployment environment parameters and
93:30 - variables logical functions for logical
93:33 - operations and or if not numeric
93:36 - functions mathematical and numeric
93:38 - operations add copy index div float int
93:42 - min max Etc object functions for object
93:46 - manipulation contains empty intersection
93:48 - Json length and Union resource functions
93:52 - related to Azure resources extension
93:54 - resource ID list account SAS list Keys
93:57 - list Secrets Etc string functions for
94:00 - string manipulation and evaluation base
94:02 - 64 Bas 64 to Json base 64 to string
94:06 - concat contains Etc we won't go over all
94:09 - of them but this is just to show you
94:11 - that there are a lot of functions
94:12 - available to
94:14 - [Music]
94:17 - you the next topic we'll cover are the
94:19 - arm template variables template
94:22 - variables are used to simplify your arm
94:24 - templates you transform parameters and
94:26 - resource properties using functions and
94:28 - then assign them into a reusable
94:29 - variable in this example the storage
94:32 - name variable is computed by combining a
94:34 - parameter named storage name prefix with
94:36 - a unique string derived from the
94:37 - resource groups ID to call a variable
94:40 - you use the variable function as shown
94:43 - in this
94:44 - example sometimes your templates might
94:46 - become more intricate and you may need a
94:48 - hierarchical structure to your variables
94:50 - that's where nested variable come in you
94:53 - can use JSO and object to have nested
94:55 - variables to scope your variables for
94:56 - multiple use cases scoping nesting
94:59 - variables based on environment consider
95:01 - scenarios where you have configurations
95:03 - That Vary based on the environment like
95:05 - test or prod you might want to neatly
95:07 - encapsulate variables specific to each
95:09 - environment within a JSO n object as
95:12 - shown in the example you can use
95:14 - parameters to choose the environment and
95:16 - then reference nested variables as
95:18 - followed in the example variables
95:20 - parentheses square brackets
95:23 - dotproperty overall variables and R
95:25 - templates are powerful tools that can
95:27 - simplify your template make it more
95:29 - adaptable and improve its
95:35 - maintainability the arm templates are
95:37 - just about defining and provisioning
95:38 - resources they also offer a way to fetch
95:41 - information about the deploy resources
95:43 - this is where the output section of your
95:44 - arm templates comes into play outputs
95:47 - returns values from deployed resources
95:49 - so you can use them programmatically
95:52 - for example you might want to know the
95:53 - static IP of a created VM or the
95:55 - connection string of a deployed database
95:57 - you specify the type and value under
96:00 - outputs here the output name resourc it
96:02 - is capturing the ID of a public IP
96:04 - address resource it's noteworthy that
96:06 - the type is explicitly mentioned
96:07 - ensuring type safety once your resources
96:10 - are deployed these outputs can be
96:12 - fetched using Azure CLI po shell or the
96:14 - Azure sdks for instance with Azure CLI
96:18 - this command retrieves the resourced
96:19 - output value from a specific deployment
96:21 - in a resource Group so that's a quick
96:24 - overview of outputs and arm
96:27 - [Music]
96:30 - templates hey this is Andrew Brown from
96:32 - exam Pro and we are looking at Azure
96:34 - resource manager templates also known as
96:36 - arm templates and this helps you uh
96:38 - deliver infrastructure as code meaning
96:40 - that when you have a resource such as a
96:42 - virtual machine or a storage account
96:44 - instead of manually configuring it every
96:45 - single time through um uh the portal
96:48 - what you can do is provide a
96:50 - configuration file that defines finds
96:51 - all the properties that you want it to
96:53 - be configured with and the idea is that
96:55 - you can uh keep this file and share with
96:57 - other uh other people so they can easily
97:00 - uh create the same resources as you and
97:02 - then you know exactly how your stuff is
97:04 - is configured so what we're going to do
97:07 - is uh launch a new template now you
97:09 - can't go up here and just type in arm
97:11 - because these arm templates are managed
97:14 - uh at different levels so at one level
97:16 - is subscription or the resource groups
97:18 - so when you have a resource Group you
97:20 - have deployments within them and that's
97:21 - where uh these templates are deployed uh
97:24 - but just to deploy one from here what
97:26 - we're going to do is type in deploy why
97:28 - they didn't make it so you can type an
97:29 - arm I do not know but if you go down
97:31 - here we have deploy a custom template
97:34 - and so from here we have some common
97:35 - templates so if I click into web app and
97:37 - I go edit a template we already have
97:39 - some stuff pre-filled in I'm just going
97:41 - to go back and discard that go back to
97:43 - select a template and we're going to
97:44 - build our own and by default we'll have
97:46 - that schema that content version which
97:48 - is 1.0.0 point0 our parameters and our
97:51 - resources so today I want to launch a
97:53 - virtual machine and uh what you normally
97:56 - would have to do is go here uh and look
97:58 - up what is that you want to create so if
98:00 - it's this uh Microsoft compute virtual
98:02 - machine you'd go through here and you'd
98:04 - have to make sure you have all these uh
98:06 - properties so you define the resource
98:09 - here right the type um and then you
98:12 - define the properties that you want and
98:14 - down below you can go through here and
98:15 - see them all that's a lot of work I
98:17 - don't want to do that so I'll go to add
98:19 - resource here drop this down and click
98:23 - uh where is it virtual machine where are
98:24 - you there you are and I'm going to call
98:27 - this one Warf and Warf and Warf because
98:30 - it's not just going to create a virtual
98:31 - machine it's going to create other
98:32 - things uh that I need with it as well
98:35 - such as the storage account the network
98:36 - interface and the virtual Network so you
98:40 - can see that we have a bunch of
98:41 - parameters here so the name the type the
98:44 - name the admin username the password and
98:47 - the OS version oh you know what I think
98:49 - I chose a window Windows one I do not
98:52 - want a Windows one I want a Linux one
98:54 - because that is easier for me to work
98:56 - with here so we choose ubun 2 so I'll
98:58 - just fill this in
99:01 - again all right and so um back up here
99:05 - you know we have the Ubuntu version
99:06 - between some versions here and then
99:08 - there's the type so that's for uh
99:10 - replication then we have variables here
99:13 - so if we go to VM size this is the VM it
99:15 - will it will set here uh variables are
99:18 - either you can have string values or you
99:20 - can use function to transform other
99:22 - parameters into other stuff that you'll
99:24 - reference throughout your template then
99:26 - down below we have those resources here
99:29 - so what we'll do is um actually I'm
99:32 - going to copy this because it's it's
99:34 - very highly likely we're going to want
99:35 - to make some kind of change and so I
99:37 - have vs code over here on the on the
99:39 - left right hand side I'm just going to
99:40 - paste that on in there um and what we
99:43 - will do this is a Json file make things
99:46 - a little bit easier here great and what
99:49 - I'll do is just move that off screen and
99:51 - we'll go ahead and we will save this and
99:54 - we'll see if we can deploy
99:56 - this so I'm going to type in Warf here
99:59 - and we'll launch in Canada east I'll
100:01 - name this Warf we will name the username
100:04 - Warf but lowercase and then we'll do
100:06 - testing 1 2 3 4 5 6 capital on the T
100:10 - notice that it is uh hidden there and
100:12 - then we will choose 14 which is
100:14 - defaulted here and lrs we'll go ahead
100:16 - and do review and
100:18 - create and we'll hit create here
100:22 - so this is going to fail I already know
100:24 - because it has a misconfiguration it'll
100:26 - tell us how but while that's going we'll
100:28 - take a look at our input so this is the
100:30 - values that were inputed these are the
100:32 - outputs if we had defined any which we
100:33 - have not um and if we go back to our
100:37 - template I just wanted to show you that
100:38 - we have that secure string so when we
100:40 - were typing our password that's why we
100:41 - didn't see it um so just things like
100:45 - that so I'll go back up here and our
100:47 - deploy failed why what happened so we
100:50 - open it up here the requested VM size
100:52 - standard D1 is not available in the
100:54 - current region so the template we have
100:56 - is not that great um it it needs some
100:59 - configuration because we can't use D1 I
101:01 - think that doesn't exist anymore and so
101:04 - what we really want to use is the
101:05 - standard B1 LS all right standard B1 LS
101:11 - so I'm going to cut that and for the
101:13 - time being I'm going to go back to our
101:15 - original
101:17 - template and this is one big template
101:22 - I'm going to look for those variables oh
101:23 - they're all the way at the bottom here
101:24 - nice and so I'm going to just go ahead
101:26 - and paste that in B1 LS just double
101:29 - check making sure I spelled that right
101:32 - standard uh standard B1 LS looks good to
101:36 - me so I'm going to move that off screen
101:39 - and the question is what do we do what
101:41 - do we do when a deploy fails so let's go
101:44 - take a look at what has happened here so
101:46 - this all got deployed into a resource
101:48 - Group and under here this is where our
101:50 - deployments are
101:51 - so when we look at this
101:53 - template it we can see that it failed we
101:55 - can click into here get the same
101:57 - information um and if we click into here
102:00 - it just brings us back to where we just
102:01 - were but if we go look at what was
102:04 - actually deployed uh under our Resource
102:06 - Group under the overview we'll notice
102:09 - that it created the virtual Network the
102:11 - storage account and the network
102:12 - interface so when it fails it creates
102:14 - what it can but it doesn't roll back
102:17 - okay so the question is is then how do
102:19 - you do cleanup so you might think I'll
102:21 - go to deployments and what I'll do is go
102:23 - ahead and delete that template and we
102:25 - can go ahead and do that which by the
102:27 - way you can't edit this template all you
102:29 - can do is um all we can do here see I
102:33 - just want to show you that you cannot
102:34 - edit it we can download it and stuff
102:37 - like that but uh so you might think well
102:40 - if I go ahead and delete that
102:43 - template just making sure we're in the
102:45 - right place
102:46 - here you might think that might roll
102:48 - back those resources but it doesn't it
102:50 - just deletes the template so if you
102:52 - really want to get rid of this stuff
102:53 - what you got to do is go ahead and
102:55 - delete all these resources
102:59 - manually so um I wish it kind of had a
103:03 - roll back feature but that's just how it
103:04 - is but there are some nice things that
103:07 - uh Azure does here which we'll talk
103:08 - about in a moment so I think we have
103:10 - adjusted it to the correct value now so
103:13 - hopefully this is going to be all we
103:14 - need to make it work so what we'll
103:18 - do is go to our deployments here
103:21 - uh and we can't do it here but so we'll
103:22 - go back to the top here and type in
103:24 - deploy and we'll go to custom
103:26 - template and what we'll do is build our
103:29 - own template in our editor and I'm just
103:31 - going to copy the contents
103:38 - here okay we'll go
103:40 - copy and I will go paste and we'll make
103:45 - sure that this is all good looks fine to
103:47 - me we'll go ahead and hit save and we
103:50 - will choose Warf so we don't have to
103:52 - make a new one and we will fill in uh
103:55 - the name as Warf username is Warf I'll
103:57 - call Warf 2 just in case helps us keep
104:00 - track of what we're doing here testing 1
104:03 - 2 3 4 5 6 with a capital on the
104:05 - T 14 L RLS lrs and we'll go ahead oh we
104:10 - have one issue here cannot deploy
104:13 - Resource Group worth
104:17 - deleting uh we'll go back
104:23 - and we will hit create here I don't
104:26 - think I deleted the resource Group let
104:27 - me just go double
104:29 - check I almost I'm almost certain I
104:31 - deleted all the contents of it
104:34 - right oh so there's already one here so
104:36 - we're just waiting for that to
104:39 - delete just going to go delete for us
104:41 - please thank
104:49 - you it failed to delete we'll go take a
104:52 - look as to
104:53 - why resources not
104:56 - found
105:02 - mhm uh we'll go back to our resource
105:05 - groups give this a refresh
105:09 - here okay so you know what I must have
105:11 - deleted the resource Group which is
105:12 - totally fine I I could have SW I only
105:14 - delete the contents of it but we'll just
105:17 - call this war regular then we'll go
105:20 - ahead and hit uh great here and so this
105:24 - time I have better feeling about
105:34 - this and so we will just have to wait a
105:37 - little bit it won't take too
105:41 - long I'll see you back in a moment okay
105:44 - so after waiting a little bit here our
105:46 - uh our thing seems to be deployed so if
105:47 - we go to Resource groups we can see that
105:49 - our virtual machine is deployed so uh
105:52 - that's pretty much all there is to it
105:54 - one other thing I'd like to show you is
105:55 - that whatever you have whatever is in
105:58 - your resource Group you can actually
105:59 - export the template so uh if you did
106:02 - configure something manually all you'd
106:03 - have to do is find the resource go up
106:06 - here to well um it is export template
106:10 - and there's your template and it just
106:12 - has that single resource in there I
106:13 - can't remember if if I go into here if I
106:16 - select multiples um I go X or where is
106:20 - it
106:21 - export
106:23 - template look it's going to include all
106:25 - that stuff so if you already have
106:27 - existing resources that you provisioned
106:29 - and you want to have them that's what
106:31 - you can do notice that some things won't
106:33 - be included in the template when you do
106:34 - that but you can just go ahead and
106:36 - download them and then you have them for
106:38 - later so yeah that's all there really is
106:40 - to um arm other than learning the uh the
106:44 - nitty gritties of the actual language
106:45 - that's just how you work with it there
106:47 - so what I'm going to do is make my way
106:49 - over to my Resource Group group here and
106:51 - I'm just going to go ahead and delete
106:54 - this
106:58 - here and we're all good to
107:02 - [Music]
107:06 - go hey this is Andrew Brown from exam
107:09 - Pro and in this section we'll be
107:10 - covering Azure monitor so Azure monitor
107:13 - is a comprehensive solution for
107:14 - collecting analyzing and acting on
107:16 - Telemetry from your cloud and on
107:18 - premises environments it serves as is
107:20 - the backbone for gaining insight into
107:22 - the performance and health of your
107:23 - applications infrastructure and even the
107:25 - network he features visual dashboards A
107:28 - visual representation of your data smart
107:31 - alerts intelligent notifications based
107:33 - on specific conditions automated actions
107:36 - set automation based on certain triggers
107:39 - log monitoring track and analyze event
107:41 - logs many Azure Services by default are
107:44 - already sending Telemetry data to Azure
107:46 - monitor what is observability it's the
107:49 - ability to to measure and understand how
107:51 - internal systems work in order to answer
107:53 - questions regarding performance
107:54 - tolerance security and faults with a
107:56 - system or application to obtain
107:59 - observability you need to use metrics
108:01 - logs and traces you have to use them
108:04 - together using them in isolation does
108:05 - not gain you observability metrics a
108:08 - number that is measured over a period of
108:10 - time for example if we measured the CPU
108:13 - usage and aggregated it over a period of
108:15 - time we could have an average CPU metric
108:17 - plugs a text file where each line
108:19 - contains event data about what happened
108:21 - at a certain time traces a history of
108:24 - request that travels through multiple
108:26 - apps or services so we can pinpoint
108:28 - performance or failure looks like they
108:30 - should have called it the Triforce of
108:32 - observability the sources of common
108:34 - monitoring data to populate data stores
108:36 - order by highest to lowest application
108:39 - operating system Azure resources Azure
108:42 - subscription Azure tenant custom sources
108:46 - the two fundamental data stores are
108:47 - metrics and logs Azure monitor monor
108:51 - functionalities insights this can be for
108:53 - applications containers VMS or other
108:56 - Monitoring Solutions visualize using
108:58 - dashboards views powerbi and workbooks
109:01 - you can create Rich visual presentations
109:03 - of your data Analyze This involves
109:05 - delving deep into metrics analytics and
109:07 - log analytics respond Based on data
109:10 - Azure monitor can alert you or even
109:11 - autoscale resources integrate extend the
109:14 - capabilities by using logic apps or
109:16 - export APS for more flexibility overall
109:19 - as your monitor is a comprehensive
109:21 - solution vital for ensuring that your
109:23 - applications and services run optimally
109:25 - and any issues are detected and dealt
109:27 - with
109:28 - [Music]
109:31 - properly the next topic we'll be
109:33 - covering are the various sources from
109:35 - which azzure monitor collects data
109:37 - application code Azure monitor
109:39 - application insights offers robust
109:41 - metrics about the performance and
109:42 - functionality of your applications and
109:44 - code you'll get performance traces
109:47 - application logs and even user Telemetry
109:49 - you'll need to install instrumentation
109:51 - package in your application to collect
109:53 - data for application insights
109:55 - availability tests measure your
109:57 - application's responsiveness from
109:58 - different locations on the public
110:00 - internet this helps in assessing the
110:02 - reliability and uptime of your services
110:04 - nrixs descriptive data regarding your
110:07 - application's performance operation and
110:09 - custom metrics log store operational
110:12 - data about your application including
110:14 - page views application requests
110:16 - exceptions and traces you can send
110:18 - application data to azure storage for
110:20 - archiving view the details of
110:22 - availability tests stored and debug
110:25 - snapshot data that is captured for a
110:26 - subset of exceptions is stored in Azure
110:29 - storage log analytics agent is installed
110:31 - for comprehensive monitoring dependency
110:34 - agent collects discovered data about
110:36 - processes running on the virtual machine
110:37 - and external process dependencies agents
110:40 - can be installed on the OS for VMS
110:42 - running in Azure on premises or other
110:44 - Cloud providers Diagnostics extension
110:47 - collect performance counters and store
110:49 - them in metrics
110:50 - application insights logs collect logs
110:52 - and performance counters from the
110:54 - compute resources supporting your
110:55 - application allowing them to be analyzed
110:57 - alongside other application data the
111:00 - Azure Diagnostics extension always
111:02 - writes to an Azure storage account while
111:04 - Azure monitor for VMS uses the log
111:06 - analytics agent to store Health State
111:07 - information in a custom location the
111:10 - Diagnostics extension can also stream
111:12 - data to other locations using a vet hubs
111:15 - resource logs provide insights into the
111:17 - internal operation of an Azure resource
111:19 - and are autom automatically created
111:21 - however you must create a diagnostic
111:22 - setting to specify a destination for
111:24 - each resource platform metrics will
111:27 - write to the Azure monitor metrics
111:28 - database with no configuration you can
111:31 - access platform metrics from metrics
111:33 - Explorer for trending and other analyzes
111:36 - use log analytics copy platform metrics
111:39 - to logs send resource logs to Azure
111:41 - storage for archiving stream metrics to
111:44 - other locations using event
111:46 - hubs Azure subscription this includes
111:49 - Telemetry related to to the health and
111:50 - operation of your Azure subscription
111:52 - Azure service Health provides
111:54 - information about the health of the
111:55 - Azure services in your subscription that
111:57 - your application and resources rely on
112:00 - Telemetry related to your Azure tenant
112:02 - is collected from tenant wide services
112:04 - such as Azure active directory Azure
112:07 - active directory reporting contains the
112:09 - history of sign and activity and audit
112:10 - trail of changes made within a
112:12 - particular tenant for resources that
112:14 - cannot be monitored using the other data
112:16 - sources write this data to either
112:18 - metrics or logs using an azure monitor
112:20 - API this will allow you to collect log
112:22 - data from any rest client and store it
112:24 - in log analytics in the Azure monitor
112:26 - metrics
112:28 - [Music]
112:31 - database Azure monitor is integral to
112:34 - maintaining the health and performance
112:35 - of your applications and resources
112:37 - collecting two fundamental types of data
112:39 - logs and metrics Azure monitor logs
112:42 - collects and organizes log in
112:43 - performance data from a variety of
112:45 - monitored resources data consolidation
112:48 - logs can be pulled from diverse sources
112:50 - such as platform logs from Azure
112:51 - services log and performance data from
112:53 - Agents on Virtual machines and usage and
112:56 - performance data from applications
112:58 - workspaces all these logs are organized
113:00 - into workspaces providing a centralized
113:02 - repository for in-depth analysis query
113:05 - language Azure monitor logs offers a
113:07 - sophisticated query language which can
113:09 - quickly analyze millions of Records
113:11 - making it an ideal choice for complex
113:13 - data analytics log analytics you can
113:16 - interactively work with log queries and
113:18 - their results using Azure log Analytics
113:20 - tool in contrast Azure monitor metrics
113:23 - collects numeric data and organizes it
113:25 - into a Time series database here's why
113:27 - that's important numeric data metrics
113:30 - are numerical values captured at regular
113:32 - intervals they are a snapshot that
113:34 - describes a particular aspect of a
113:36 - system at a specific Moment In Time
113:38 - lightweight metrics are designed to be
113:40 - lightweight allowing for near realtime
113:42 - data analysis this makes them
113:44 - particularly useful for alerting and the
113:45 - rapid detection of issues metrics
113:48 - Explorer the metrics Explorer tool
113:50 - allows for interactive analysis of
113:51 - metric data providing a more immediate
113:54 - understanding of your system's
113:55 - performance and
113:57 - [Music]
114:00 - health the next topic we'll cover are
114:02 - the data retention and archive policies
114:04 - of azure monitor logs this is an
114:06 - important aspect of your monitoring
114:08 - strategy as it allows you to control how
114:10 - long your data remains stored and
114:12 - accessible by default in the Azure
114:14 - portal you can set this retention time
114:16 - anywhere from 30 to 730 days for the
114:18 - whole workspace if you want you can also
114:21 - specify different storage durations for
114:23 - certain tables within your workspace
114:24 - letting you manage different types of
114:26 - data as needed this gives you the
114:28 - flexibility to meet any business or
114:30 - regulatory rules about data storage
114:33 - however note that to tweak these
114:34 - retention settings you have to be on the
114:36 - paage tier of azure monitor logs to set
114:39 - retention and archive policy by table
114:42 - why navigate to the Azure portal and go
114:44 - to the log analytics workspace where the
114:46 - data is stored to under the setting
114:48 - section select usage and estimated cost
114:51 - three then select data retention four in
114:55 - the data retention blade you can modify
114:57 - the retention period for each table by
114:59 - default it is set to 31 days but you can
115:01 - extend it up to 730 days five for
115:05 - archiving data you can use Azure data
115:07 - Explorer which lets you retain data
115:09 - beyond the 2-year limit and gives you a
115:10 - highly scalable analytic service so
115:13 - that's an overview of the data retention
115:15 - and archive policies of azure monitor
115:17 - logs you'll most likely encounter a
115:19 - question related to this on the exam so
115:21 - be sure to know
115:23 - [Music]
115:27 - this hey this is Andrew Brown from exam
115:29 - Pro and in this section we'll be
115:31 - covering Azure log analytics so log
115:34 - analytics is a tool in the Azure portal
115:35 - used to edit and run log queries with
115:37 - data in Azure monitor logs log analytics
115:40 - processes data from various sources and
115:42 - transforms it into actionable insights
115:45 - it ingests data from Azure monitor
115:47 - windows and Linux agents Azure services
115:49 - and other sources once the data is
115:51 - collected you can use log analytics
115:53 - query language to retrieve consolidate
115:55 - and analyze the data log analytics uses
115:58 - a query language called
116:00 - kql now we'll go over some of the
116:02 - benefits of log analytics centralized
116:05 - log management collect and analyze data
116:07 - from multiple sources both on premises
116:09 - and in the cloud in a centralized
116:11 - location powerful analytics utilize the
116:14 - custo query language to run Advanced
116:15 - analytics on large amounts of fast
116:17 - streaming data in real time custom
116:20 - dashboards create custom dashboards and
116:22 - visualizations to display real-time data
116:24 - and Trends integration seamless
116:26 - integration with other Azure services
116:28 - and Microsoft Solutions such as powerbi
116:30 - and Azure Automation and alerting set up
116:33 - alerts based on specific criteria to
116:35 - proactively identify and respond to
116:37 - potential issues before they affect your
116:39 - users log analytics workspace is a
116:42 - unique environment for Azure monitor log
116:44 - data each workspace has its own data
116:47 - repository and configuration and data
116:49 - source and solutions are configured to
116:51 - store their data in a particular
116:53 - workspace so that's an overview of azure
116:55 - log
116:58 - [Music]
117:01 - analytics the log analytics agent is a
117:03 - lightweight agent that can be installed
117:05 - on Windows and Linux machines to collect
117:07 - and send log data to Azure monitor it
117:09 - provides a way to centralize logs from
117:11 - various sources and enables the analysis
117:13 - of the data using tools like Azure
117:15 - monitor logs Azure dashboards and Azure
117:17 - monitor workbooks the agent can collect
117:20 - logs from various sources including
117:22 - Windows event logs custom logs
117:24 - performance counters and CIS log it
117:27 - supports both agent-based and agentless
117:29 - data collection and can be configured to
117:30 - collect data from on premises and
117:32 - cloud-based
117:33 - environments the log analytics agent is
117:35 - set up to monitor certain Windows event
117:37 - logs like security system or application
117:40 - logs the data from these logs is then
117:42 - gathered and sent to log analytics for
117:44 - analysis using queries and
117:46 - visualizations the log analytics agent
117:48 - is set up to monitor CIS log servers or
117:50 - network devices it collects data from
117:53 - these sources and sends it to log
117:54 - analytics allowing for detailed analysis
117:56 - and troubleshooting both methods for
117:59 - collecting log data allow for
118:00 - centralized management and Analysis of
118:02 - log data from multiple sources which can
118:04 - help to improve visibility and
118:06 - streamline troubleshooting and issue
118:08 - resolution you can expect to see a
118:10 - question related to log analytics agents
118:12 - and choosing either Windows event logs
118:14 - for a Windows agent or CIS log for Linux
118:16 - agent on the
118:18 - exam
118:19 - [Music]
118:23 - the next topic will be covering our
118:25 - application insights application
118:27 - insights is an application Performance
118:29 - Management Service and it's a subservice
118:31 - of azure monitor APM is all about the
118:34 - monitoring and management of performance
118:35 - and availability of software apps it
118:37 - strives to detect and diagnose complex
118:39 - application performance problems to
118:41 - maintain an expected level of service so
118:44 - yuse application insights automatic
118:47 - detection of performance anomalies
118:48 - application insights automatically
118:50 - identifies performance anomalies in your
118:52 - system powerful analytics tools it comes
118:55 - with robust analytics tools to help you
118:57 - diagnose issues and understand what
118:59 - users do with your app continuous
119:01 - Improvement it is designed to help you
119:03 - continuously improve performance and
119:04 - usability of your applications platform
119:07 - agnostic it works for apps on net
119:09 - node.js Java and python hosted on
119:12 - premises hybrid or any public Cloud
119:15 - devops integration it can be integrated
119:17 - into your devops process and mobile app
119:20 - monitoring it can monitor and analyze
119:22 - Telemetry from mobile apps by
119:23 - integrating with visual studio app
119:25 - center to use application insights you
119:28 - need to instrument your application this
119:30 - involves installing the instrument
119:32 - package or enabling application insights
119:34 - using the application insights agents
119:36 - were supported there are many ways to
119:38 - view your Telemetry data apps can be
119:41 - instrumented from anywhere when you set
119:43 - up application insights monitoring for
119:45 - your web app you create an application
119:47 - insights resource in Microsoft azure you
119:49 - open this resource in the azzure portal
119:51 - in order to see and analyze the
119:53 - Telemetry collected from your app the
119:55 - resource is identified by an
119:56 - instrumentation
119:58 - key what does application insights
120:01 - monitor request rates response times and
120:03 - failure rates dependency rates response
120:06 - times and failure rates exceptions page
120:09 - views and low performance ax calls user
120:12 - and session counts performance counters
120:14 - post Diagnostics diagnostic Trace logs
120:17 - and custom events and metrics
120:20 - where do I see my Telemetry smart
120:22 - detection and manual alerts application
120:25 - map profiler usage analysis diagnostic
120:28 - search for instance data metrics
120:30 - Explorer for aurad data dashboards live
120:33 - stream metrics analytics Visual Studio
120:36 - ET overall application insights is a
120:39 - comprehensive APM service that offers
120:41 - automatic detection of performance
120:43 - anomalies powerful analytics tools and
120:45 - is designed to help you continuously
120:47 - improve performance and usability
120:50 - [Music]
120:54 - in this segment we'll delve into the
120:55 - topic of application insites
120:57 - instrumentation so what is
120:59 - instrumentation in simple terms it's a
121:01 - way to make your application smarter by
121:03 - adding a few lines of code or in some
121:05 - cases none at all you can monitor how
121:07 - your app performs and where it might be
121:09 - running into issues you instrument your
121:11 - application by adding the Azure
121:13 - application insights SDK and
121:14 - implementing traces in the case of a
121:17 - node.js application you can install the
121:19 - Azure application insights SDK using npm
121:22 - with the following command npm install
121:24 - application insights hyphen save
121:26 - application insights this is the name of
121:28 - the package you are installing which is
121:30 - azure SDK for application insights
121:33 - hyphen save this flag saves the package
121:35 - as a dependency in your package.json n
121:37 - file here this piece of code lets you
121:40 - configure what you want to
121:41 - collect Azure supports the following
121:44 - languages net Java python node.js
121:48 - JavaScript
121:49 - Auto instrumentation allows you to
121:51 - enable application monitoring with
121:53 - application insights without changing
121:54 - your code this table shows which Azure
121:57 - Services support application insights
121:59 - and in what programming languages the
122:01 - services range from Azure app service on
122:03 - Windows and Linux to Azure functions
122:05 - Azure spring Cloud Azure kubernetes
122:07 - service and more GA General availability
122:10 - meaning it's fully supported and ready
122:12 - to use public preview still being tested
122:15 - but you can use it not supported you
122:17 - can't use application sites here through
122:20 - agent you need to install a special
122:22 - piece of software to use this service o
122:24 - NBD on by default meaning the feature is
122:26 - automatically enabled through extension
122:29 - available but needs an extension to work
122:31 - we won't go through the entire table but
122:33 - we'll give a few examples for
122:35 - applications written in.net and hosted
122:37 - on Azure app service on Windows
122:39 - application insights is generally
122:41 - available and enabled by default for
122:43 - applications written in Python and
122:45 - hosted on Azure functions application
122:47 - insights is available and enabled by
122:49 - default but for dependencies monitoring
122:51 - you will need to use an extension so
122:54 - that's an overview of application
122:55 - insights
122:58 - [Music]
123:02 - instrumentation hey this is Andrew Brown
123:04 - from exam proo and in this section we'll
123:06 - be covering Microsoft Sentinel formerly
123:08 - known as Azure Sentinel Microsoft
123:10 - Sentinel is a scalable Cloud native
123:12 - solution that encompasses two key
123:15 - functionalities security information
123:17 - event management this is all about
123:19 - collecting and analyzing security
123:20 - related data to provide real-time
123:22 - analysis of security alerts generated by
123:24 - applications and network Hardware
123:27 - security orchestration automated
123:28 - response this refers to the collection
123:30 - of tools that enable an organization to
123:32 - Define standardize measure and automate
123:35 - responses to security events Microsoft
123:37 - Sentinel delivers intelligent security
123:39 - analytics and threat intelligence across
123:41 - the Enterprise providing a single
123:43 - solution for alert detection threat
123:46 - visibility proactive hunting and threat
123:49 - response with Microsoft Sentinel you can
123:52 - collect data at Cloud scale across all
123:54 - users devices applications and
123:56 - infrastructure both on premises and in
123:58 - multiple clouds detect previously
124:00 - undetected threats and minimize false
124:02 - positives using Microsoft's analytics
124:04 - and unparalleled threat
124:06 - intelligence investigate threats with
124:08 - artificial intelligence and hunt for
124:10 - suspicious activities at scale tapping
124:12 - into years of cyber security work at
124:14 - Microsoft respond to incidents rapidly
124:17 - with built-in orchestration and
124:18 - automation of common
124:19 - tasks Microsoft Sentinel comes with a
124:22 - number of connectors for Microsoft
124:24 - Solutions such as Microsoft 365 Defender
124:28 - Office
124:29 - 365 Azure a or Microsoft enter ID
124:33 - Microsoft Defender for identity and
124:35 - Microsoft Defender for cloud apps you
124:37 - can use common event formats CIS logit
124:40 - rest API Windows event logs common event
124:44 - format and trusted automated the
124:46 - exchange of indicator information
124:49 - one notable feature of Microsoft
124:51 - Sentinel is the ability to create Azure
124:53 - monitor workbooks workbooks provide a
124:55 - flexible canvas for data analysis and
124:57 - the creation of Rich visual reports
124:59 - within the Azure portal they allow you
125:01 - to tap into multiple data sources from
125:03 - across Azure and combine them into
125:05 - unified interactive
125:06 - experiences it tells a story about the
125:09 - performance and availability about your
125:10 - applications and services workbooks are
125:13 - temporary workspaces to define a
125:15 - document like format with visualization
125:17 - intertwined to help investig and discuss
125:20 - performance Microsoft Sentinel uses
125:22 - analytics to correlate alerts into
125:24 - incidents incidents are groups of
125:26 - related alerts that together create an
125:28 - actionable possible threat that you can
125:30 - investigate and
125:31 - resolve Microsoft Sentinels Automation
125:34 - and orchestration solution provides a
125:36 - highly extensible architecture that
125:37 - enables scalable automation as new
125:39 - technologies and threats emerge built on
125:42 - the foundation of azure logic apps
125:44 - includes 200 plus connectors for
125:47 - services my Microsoft Sentinel also
125:49 - offers deep investigation tools that
125:51 - help you to understand the scope and
125:53 - find the root cause of a potential
125:54 - security threat you can choose an entity
125:56 - on the interactive graph to ask
125:58 - interesting questions for a specific
126:00 - entity and drill down into that entity
126:01 - and its connections to get to the root
126:03 - cause of the
126:04 - threat additionally Microsoft Sentinels
126:07 - powerful hunting search and query tools
126:09 - based on the miter framework enable you
126:11 - to proactively hunt for security threats
126:13 - across your organization's data sources
126:15 - before an alert is triggered after you
126:18 - discover which hunting query provides
126:20 - high value insights into possible
126:21 - attacks you can also create custom
126:23 - detection rules based on your query and
126:25 - surface those insights as alerts to your
126:27 - security incident
126:28 - responders while hunting you can create
126:31 - bookmarks for interesting events
126:32 - enabling you to return to them later
126:34 - share them with others and group them
126:35 - with other correlating events to create
126:37 - a compelling incident for
126:39 - investigation lastly let's talk about
126:41 - pricing Microsoft Sentinel has two
126:43 - different pricing models capacity
126:46 - reservations this involves being build a
126:48 - fix fee base on the selected tier
126:50 - enabling a predictable total cost for
126:52 - Microsoft Sentinel pay as you go with
126:54 - this option Bill per gigabyte for the
126:56 - volume of data ingested for analysis in
126:58 - Microsoft Sentinel and stored in the
127:00 - Azure monitor log analytics
127:02 - workspace and there you have it a
127:04 - comprehensive look at Microsoft Sentinel
127:06 - a robust seam and Source solution that
127:08 - can help protect your organization's
127:09 - infrastructure applications and
127:13 - [Music]
127:17 - data hey this this is Andrew Brown from
127:19 - exam Pro and in this section we'll be
127:21 - covering the identity management best
127:23 - practices we'll start things off by
127:25 - discussing the principle of lease
127:26 - privilege a term you may have come
127:28 - across before so the principle of lease
127:30 - privilege is a security principle that
127:32 - states that users applications and
127:34 - services should be granted only the
127:36 - minimum access necessary to perform
127:37 - their assigned tasks and no more in
127:40 - Microsoft Azure the principle of lease
127:41 - privilege is a critical aspect of
127:43 - security that helps prevent unauthorized
127:45 - access data breaches and other security
127:47 - incidents
127:49 - the principle of lease privilege in
127:50 - Azure involves limiting access to Azure
127:52 - resources such as virtual machines
127:54 - storage accounts and databases as well
127:57 - as Azure services such as Azure active
127:59 - directory and Azure key Vault Azure
128:01 - ro-based Access Control permits
128:03 - administrators to Grant roles to users
128:05 - groups and apps based on their access
128:07 - levels following the principle of lease
128:09 - privilege helps minimize unauthorized
128:11 - access risks and potential harm from
128:13 - compromised credentials thus reducing
128:15 - the Azure environment's attack
128:17 - surface following the principle of lease
128:19 - privilege can help organizations comply
128:21 - with regulatory requirements and best
128:23 - practices for security to implement the
128:26 - principle of lease privilege in Azure
128:28 - administrators should follow these best
128:30 - practices assign roles based on the
128:32 - leaste amount of privilege needed to
128:33 - perform the task monitor role
128:36 - assignments and permissions regularly to
128:38 - ensure they align with business
128:39 - requirements limit the use of shared
128:41 - accounts and use individual user
128:43 - accounts where possible Implement
128:45 - multiactor authentication to prevent
128:47 - unauthorized access access to user
128:49 - accounts use Azure policy to enforce
128:51 - compliance with organizational policies
128:53 - and Industry regulations Implement
128:56 - network security groups and firewalls to
128:58 - control traffic to in from aure
129:00 - resources and regularly review Access
129:02 - Control policies and adjust as necessary
129:04 - to ensure that they remain effective so
129:07 - that's an overview of the principle of
129:09 - lease
129:14 - privilege the next topic will cover our
129:16 - privileged identity management or Pim
129:19 - for short within Azure D now why should
129:21 - you care about Pim because it's like a
129:23 - VIP lounge for your most sensitive
129:25 - resources controlling who gets in when
129:28 - and what they can do once they're inside
129:30 - so privileged identity management is an
129:32 - Azure aervice enabling you to manage
129:34 - control and monitor access to important
129:36 - resources in your organization you can
129:39 - manage resources from Azure ad Azure
129:42 - Microsoft 365 Microsoft and two and more
129:46 - so what are the key features of P Im
129:49 - Just in Time access this feature allows
129:51 - you to Grant privileged access to Azure
129:53 - ad and Azure resources only when needed
129:55 - assign time bound access to resources
129:57 - using start and end dates required
129:59 - approval to activate privileged roles
130:02 - enforce multiactor authentication to
130:04 - activate any role use justification to
130:07 - understand why users activate get
130:09 - notifications when privileged roles are
130:11 - activated conduct access reviews to
130:13 - ensure users still need roles and
130:16 - download audit history for internal or
130:17 - external audit it's important to note
130:20 - that Pim is part of azurea D premium 2o
130:22 - so you'll need that subscription to
130:24 - access these features by taking
130:26 - advantage of Pim and its features you
130:28 - can ensure a more secure and controlled
130:30 - environment for your
130:33 - [Music]
130:36 - organization next we'll be diving into
130:38 - another critical topic Azure ad identity
130:41 - protection so what does it do identity
130:43 - protection is a feature of azuread that
130:45 - enables you to detect investigate
130:47 - remediate and Export identity based
130:49 - risks for future analysis Microsoft
130:52 - analyzes a staggering 6.5 trillion
130:54 - signals per day to identify and protect
130:56 - customers from threats identity
130:58 - protection can notice risky users risky
131:01 - sign-ins and risk
131:04 - detections let's take a closer look at
131:06 - the types of risks that identity
131:07 - protection can identify Anonymous IP
131:10 - address this is a sign in from an
131:12 - anonymous IP address like those used by
131:14 - the tour browser or anonymizer vpns a
131:17 - typical travel this refers to a sign in
131:19 - from a location that is atypical based
131:20 - on the users's recent sign-ins malware
131:23 - linked IP address a sign in from an IP
131:25 - address linked to malware unfamiliar
131:27 - signin properties a sign in with
131:29 - properties that haven't been seen
131:30 - recently for the given user leak
131:33 - credentials indicates that the user's
131:35 - valid credentials have been leaked
131:37 - password spray this involves multiple
131:39 - usernames being attacked using common
131:41 - passwords in a unified Brute Force
131:43 - manner Azure a de threat intelligence
131:45 - this is when Microsoft's internal and
131:47 - external threat threat intelligence
131:48 - sources have identified a known attack
131:50 - pattern and there are others detected by
131:53 - Microsoft Defender for cloud apps such
131:55 - as new country activity activity from
131:57 - Anonymous IP addresses and suspicious
131:59 - inbox
132:00 - forwarding the risk signals can trigger
132:02 - remediation efforts such as requiring
132:04 - users to use azuread multifactor
132:06 - authentication reset their password
132:08 - using self-service password reset or
132:10 - even blocking until an administrator
132:12 - takes
132:13 - action identity protection categorizes
132:16 - risk into three tiers low me medium and
132:18 - high and administrators can use key
132:21 - reports for investigations such as risky
132:23 - users risky sign-ins and risk detections
132:26 - reports in the risky users report you'll
132:28 - find comprehensive details about
132:30 - detected risks a complete history of all
132:32 - risky sign-ins as well as the users
132:34 - overall risk history to give you a full
132:36 - picture of security
132:37 - concerns in the risky signin report
132:40 - you'll see signin categorized as at risk
132:42 - confirmed compromised confirmed safe
132:44 - dismissed or remediate this report
132:46 - provides both real time and aggregate
132:48 - risk levels associated with each sign
132:50 - and attempt it also includes the types
132:52 - of risk detections triggered the
132:54 - conditional access policies applied
132:56 - multiactor authentication details and
132:58 - information about the device application
133:00 - and location involved in the
133:02 - signin risk detections offers filterable
133:05 - data covering up to the past 90 days it
133:07 - provides detailed information about each
133:09 - type of risk detected as well as other
133:11 - risks that were triggered simultaneously
133:13 - in the locations of the sign and
133:15 - attempts had been follow-up actions
133:18 - admins can take various actions like
133:20 - resetting the user password confirming a
133:23 - user compromise dismissing user risk
133:26 - blocking user side ends and investigate
133:29 - further using Azure app so with Azure ad
133:32 - identity protection you're not just
133:33 - identifying risks but you're also given
133:35 - powerful tools to act on
133:38 - [Music]
133:41 - them hey this is Andrew Brown from exam
133:44 - Pro in this segment we'll delve deep
133:46 - into Azure key vault pivotal tool to
133:48 - ensure the security of your Cloud
133:50 - applications and services Azure keyal
133:53 - helps you Safeguard cryptographic keys
133:54 - and other Secrets used by Cloud apps and
133:57 - services Azure key Vault focuses on
133:59 - three things certificate management this
134:02 - feature allows for easy provision
134:03 - management and deployment of both public
134:05 - and private SSL certificates these
134:08 - certificates can be used with Azure and
134:09 - internally connected resources Key
134:12 - Management this enables the creation and
134:14 - control of encryption Keys used to
134:15 - encrypt your data secrets manag
134:18 - here you have a secure space to store
134:20 - and tightly control access to tokens
134:22 - passwords certificates API keys and
134:24 - other Secrets note that certificates
134:27 - contain a key pair which is a
134:28 - combination of a key and a secret this
134:30 - should not be confused with key
134:32 - management and secrets management which
134:34 - are distinct
134:35 - functionalities moving forward let's
134:37 - talk about hsms or Hardware security
134:40 - modules these are dedicated Hardware
134:42 - devices specifically designed to
134:43 - securely store encryption Keys when it
134:46 - comes to adhering to standard standards
134:48 - we reference the federal information
134:49 - processing standard or fips this is a
134:52 - guideline recognized by the US and
134:53 - Canadian governments that specifies the
134:55 - security requirements for cryptographic
134:57 - modules that protect sensitive
134:59 - information in line with fips we have
135:01 - two levels of compliance for hsms fips
135:04 - 104 diminus 2 level two compliant this
135:07 - compliance level is for multi-tenant
135:09 - hsms where multiple customers are
135:10 - virtually isolated on a single HSM fips
135:14 - 104 diminus 2 level three compliant this
135:16 - level on the other hand pertains to
135:18 - single tenant hsms where One customer
135:20 - utilizes a dedicated HSM in essence
135:23 - Azure key vault is an indispensable tool
135:25 - for ensuring that your cloud data
135:27 - remains both accessible and secure
135:29 - whether you're working with certificates
135:31 - encryption keys or various Secrets Azure
135:33 - key Vault has you
135:36 - [Music]
135:39 - covered all right let's dive into the
135:41 - core of azure key Vault the Vault itself
135:43 - a vault is where your secrets and keys
135:45 - reside safeguarded either by software or
135:47 - by hsms validated to the standards of
135:49 - fips 1004 diminus 2 level two Azure key
135:52 - vaults provides two types of containers
135:55 - vaults these containers support both
135:57 - software and HSM back Keys HSM pools
136:00 - these are specialized containers solely
136:02 - for HSM back keys to activate your HSM
136:05 - you will need to provide a minimum of
136:08 - three RSA key pairs up to a maximum of
136:10 - 10 and specify the minimum number of
136:12 - keys required to decrypt the security
136:14 - domain called a quorum you do not choose
136:17 - the container on creation you just
136:18 - choose between standard and premium when
136:20 - you choose premium and create enough RSA
136:22 - key pairs you will begin to use HSM
136:25 - pools diving a bit into technicalities
136:28 - Azure key Vault rest API is used for
136:30 - programmatically managing Azure key
136:32 - Vault resources allowing you to perform
136:34 - operations such as create a key or
136:36 - secret import a key or secret revoke a
136:39 - key or secret delete a key or secret
136:42 - authorize user or apps to access its
136:44 - keys or secrets and monitor and manage
136:46 - key usage Azure key Vault rest API
136:49 - supports three different types of
136:50 - authentication managed identities and
136:53 - identity managed by Azure ad recommended
136:55 - is best practice service principle and
136:58 - certificate this method uses a
137:00 - certificate for authentication Service
137:02 - principle and secret a combination of a
137:04 - user identity and a secret key one
137:07 - feature to note is the soft delete
137:09 - functionality soft delete allows you to
137:11 - recover or permanently delete a key
137:12 - vault in secrets for the duration of the
137:14 - retention period this feature is enabled
137:16 - by default on cre
137:18 - mandatory retention period prevents the
137:20 - permanent deletion of key vaults or
137:22 - Secrets prior to the retention period
137:24 - elapsing furthermore enabling Purge
137:27 - protection safeguards your secrets from
137:28 - being prematurely purged either by users
137:30 - or by Microsoft bolstering the security
137:33 - of your
137:38 - Vault next up on our agenda is breaking
137:40 - down the pricing of azure key Vault
137:42 - knowing how your bill for this service
137:44 - can help you make informed decisions and
137:46 - optimize your costs Azure key Vault
137:48 - offers two pricing tiers standard and
137:50 - premium the notable distinction between
137:52 - the two is that while both tiers support
137:54 - software protected Keys only the premium
137:56 - tier allows for HSM protected Keys
137:59 - here's a closer look at the pricing
138:01 - tiers first 250 Keys regardless of
138:04 - whether you're on the standard or
138:05 - premium tier you'll be buillt $5 per key
138:08 - every month
138:10 - 25115 Keys the price drops to $2.50 per
138:14 - key monthly again consistent across both
138:16 - tiers 1501 to 4,000 Keys the cost
138:19 - further reduces to 90 cents for each key
138:21 - every month 4,1 plus keys for larger key
138:25 - volumes Beyond this point you'll be
138:27 - charged at a rate of 40 cents per key
138:28 - per month Secrets operations both tiers
138:31 - are priced at 3 cents for every 10,000
138:34 - transactions involving Secrets
138:36 - certificate operations exclusive to the
138:38 - premium tier each certificate renewal
138:40 - request is buil at $3 managed Azure
138:43 - storage account key rotation this
138:45 - service only available in the premium
138:47 - tier is priced at $1 per renewal HSM
138:50 - protected Keys specifically for HSM
138:52 - protected Keys the pricing is further
138:54 - broken down based on the key types for
138:57 - RSA 2048 bit Keys the cost is $1 per key
139:00 - per month along with an additional
139:02 - charge of 3 cents per 10,000
139:04 - transactions for RSA 3072 bit and
139:06 - 4096-bit keys as well as ECC Keys the
139:10 - first 250 keys are priced at $5 per key
139:13 - per month so that's an overview of the
139:15 - pricing model for Azure key vault
139:18 - [Music]
139:22 - the next topic we'll be covering is
139:24 - double encryption for Azure key Vault
139:26 - before we dive in let's quickly recap
139:28 - infrastructure encryption for storage
139:30 - accounts by default Azure ensures that
139:32 - your storage account data is encrypted
139:34 - when it's at rest infrastructure
139:35 - encryption adds a second layer of
139:37 - encryption to your storage accounts data
139:39 - now let's jump into Azure diss double
139:41 - encryption double encryption is
139:43 - precisely what it sounds like it's where
139:45 - two or more independent layers of En
139:47 - encryption are enable to protect against
139:49 - compromises of any one layer of
139:50 - encryption this strategy ensures that
139:53 - even if one encryption layer is
139:54 - compromised the data remains protected
139:56 - by the other Microsoft has a two-layered
139:59 - approach both for data at rest and data
140:01 - in transit for data at rest disk
140:04 - encryption this is achieved using
140:05 - customer managed keys and infrastructure
140:08 - encryption this uses platform managed
140:10 - Keys strengthening the base layer and
140:12 - for data in Transit Transit encryption
140:15 - using transport layer Security 1 2 the
140:17 - Safeguard data as it travels through
140:19 - networks and an additional layer of
140:21 - encryption provided at the
140:22 - infrastructure layer so that's a quick
140:25 - overview of double encryption for Azure
140:27 - key
140:31 - vault in this section we'll go into
140:34 - detail on the keys in Azure key Vault
140:36 - when it comes to creating a key in Azure
140:38 - you have three primary choices generate
140:41 - Azure will generate the key for you
140:43 - import import an existing RSA key that
140:45 - you already possess and restore backup
140:47 - restore a key from backup for Keys
140:50 - generated by Azure you can use either
140:52 - RSA or EC RSA or rivest shamier or
140:55 - Adelman this supports key sizes of 2048
140:58 - 3072 and 4096 bits EC or elliptic curve
141:02 - cryptography here you can select from
141:05 - p256 P 384 p521 or p256 k for Keys
141:11 - generated by azer you can set an
141:13 - activation and expiration date
141:15 - additionally you're not bound to a
141:16 - static version of a key you can create
141:18 - new versions of keys you can also
141:21 - download backups of keys but remember
141:23 - that backups can only be restored within
141:25 - the same Azure subscription and within
141:27 - Azure key Vault when you have a premium
141:29 - Vault you'll key options for HSM you can
141:32 - generate either an RSA or EC
141:34 - specifically for HSM or import an RSA
141:37 - key for HSM as shown in the
141:39 - example now let's talk about Key
141:42 - Management types Microsoft managed key
141:44 - or Keys managed by Microsoft they do not
141:47 - appear in your Vault and in most cases
141:48 - are used by default for many Azure
141:51 - services customer managed key are Keys
141:53 - you create in Azure key Vault you need
141:55 - to select a key from EV Vault for
141:56 - various Services sometimes customer
141:59 - managed means that the customer has
142:01 - imported cryptographic material and any
142:03 - generated or imported keys are
142:04 - considered cmk and Azure in order to use
142:07 - a key an Azure service needs an identity
142:09 - established with an Azure adid for
142:11 - permission to access the key from The
142:12 - Vault Additionally you have the option
142:15 - to implement infrastructure encryption
142:17 - while while Azure already encrypt
142:18 - storage account data at Rest by default
142:20 - opting for infrastructure encryption
142:22 - adds a second layer of security
142:23 - fortifying your storage accounts data
142:25 - even
142:30 - further the next topic will be covering
142:32 - our secrets in Azure key Vault Azure key
142:35 - Vault Secrets provide Secure Storage of
142:37 - generic Secrets such as passwords and
142:39 - database connection strings key Vault AP
142:42 - as accept and return secret values as
142:44 - strains internally key Vault stores and
142:46 - man of Secrets as sequences of octets
142:49 - with each secret having a maximum size
142:51 - of 25k bytes the key Vault service
142:54 - doesn't provide semantics for Secrets it
142:56 - accepts the data encrypts it stores it
142:58 - and returns a secret identifier for
143:01 - highly sensitive data clients should
143:02 - consider additional layers of protection
143:04 - for data for example encrypting your
143:06 - data using a separate protection key
143:08 - before storing it in the key Vault keyal
143:11 - also supports a content type field for
143:13 - Secrets allowing clients to specify the
143:15 - content type of a secret to a assist in
143:17 - interpreting the secret data when it's
143:19 - retrieved note that the maximum length
143:21 - of this field is 255
143:23 - characters every secret stored in your
143:26 - key vault is encrypted key vault encrypt
143:28 - Secrets at rest with a hierarchy of
143:30 - encryption Keys all keys in that
143:32 - hierarchy are protected by modules that
143:34 - are fips 14 minutes 2 compliant the
143:36 - encryption Leaf key is unique to each
143:38 - key Vault while the root key is unique
143:40 - to the entire security world the
143:42 - protection level may vary between
143:44 - regions for example chid uses fips 1004
143:47 - dimin 2 level one and all other regions
143:49 - use level two or higher diving into
143:52 - secret attributes we have exp this is
143:55 - the expiration time after which the
143:56 - secret data should not be retrieved NBF
143:59 - not before default value is now this
144:02 - defines the time before which the secret
144:03 - data should not be retrieved enable this
144:06 - tells us whether the secret data can be
144:08 - retrieved or not with its default set to
144:10 - True additionally there are readon
144:12 - attributes for created an update in
144:15 - order to access Secrets within your
144:16 - applic a code you can would use the
144:18 - Azure SDK for example we have a net
144:21 - example in this image here another
144:23 - option is to use tools like Azure CLI so
144:27 - that about covers the important details
144:28 - of secrets in Azure key
144:34 - Vault the next topic will be covering
144:37 - our x59 certificates first let's unravel
144:40 - what public key infrastructure is pki is
144:43 - a set of rules policies Hardware
144:45 - software and procedures needed to to
144:47 - create manage distribute use store and
144:49 - revoke digital certificates and manage
144:51 - public key encryption so what exactly is
144:54 - an x59 certificate it is a standard
144:57 - defined by the international
144:58 - telecommunication Union for public key
145:01 - certifications X 59 certificates are
145:03 - used in many internet protocols
145:05 - including SSL TLS and https signed and
145:09 - encrypted email and code signing and
145:11 - document signing a certificate contains
145:13 - an identity which could be a host name
145:15 - an organization or an individual along
145:17 - with a public key built on platforms
145:19 - like RSA DSA or
145:21 - ecda but who issues these certificates
145:24 - here comes the role of the certificate
145:26 - Authority a trusted entity that issues
145:27 - digital certificates a c acts is a
145:30 - trusted third part trusted both by the
145:32 - subject of the certificate and by the
145:33 - party relying upon the
145:35 - certificate a certificate Authority can
145:37 - issue multiple certificates in the form
145:39 - of tree structure known as a chain of
145:41 - trust root certificate Authority this is
145:43 - a self-signed certificate the private
145:45 - key associated with it signs other
145:47 - certificates it's important that the
145:49 - private key of root or protected
145:51 - intermediate certificate Authority these
145:53 - certificates are signed by the roots
145:55 - private key they protect the root
145:56 - certificate because the root certificate
145:58 - does not have to sign every issued
146:00 - certificate and entity certificate a
146:02 - certificate issued by the ICA used by
146:04 - the end entity the entity in the case is
146:06 - an SSL certificate for a
146:08 - website a certificate contains of
146:11 - metadata about version number the
146:13 - version of the x59 standard serial
146:16 - number a unique serial number assigned
146:18 - to the certificate by the certificate
146:19 - Authority signature algorithm ID the
146:22 - algorithm used to sign the certificate
146:24 - such as RSA or DSA issuer the name of
146:27 - the certificate Authority that issued
146:29 - this certificate validity period the
146:31 - start and end dates during which the
146:33 - certificate is valid subject the
146:35 - identifier for the individual or
146:37 - organization to whom the certificate was
146:38 - issued subject public key the public key
146:41 - that is authenticated by this
146:42 - certificate this field also names the
146:45 - algorithm used for public key generation
146:46 - ation ISS your unique identifier allows
146:49 - multiple Cas to operate as a single
146:51 - logical CL subject unique identifier
146:54 - allows multiple certificate holders to
146:56 - act as a single logical entity
146:58 - extensions allows a c to associate
147:00 - additional private information with a
147:02 - certificate all the metadata is publicly
147:04 - readable anyone can view it so that's an
147:07 - overview of x59
147:11 - [Music]
147:14 - certificates hey this is Andrew Brown
147:16 - from exam Pro and in this fall along
147:18 - we're going to be learning all about
147:20 - Azure Vault so let's get to it so what I
147:23 - want you to do is go on the top here and
147:25 - type in key Vault and here we'll have to
147:28 - go ahead and create ourselves a new
147:29 - Vault and so from there we're going to
147:31 - create a new Resource Group I'm going to
147:33 - call this Resource Group my example
147:37 - Vault and then we will make a vault key
147:40 - here so I'll say My Vault example which
147:43 - is kind of funny because this one's
147:44 - slightly different so you've seen I've
147:45 - done this before so I'm I'm going to do
147:47 - my example vault as the name here and
147:50 - for the region Us East is fine for
147:53 - pricing we'll keep it at standard soft
147:56 - delete is enabled um and then there's
147:58 - the option for Purge protection so we
148:01 - are going to enable Purge
148:04 - protection and uh this is going to play
148:06 - into other follows we'll explain that as
148:08 - it goes but Purge protection does not
148:10 - allow you to uh Purge things uh easily
148:13 - once it's enabled so what we'll do is go
148:16 - ahead and and review and
148:19 - create and we'll go ahead and go review
148:26 - create and we'll give it a moment
148:32 - here and we'll just wait till it's done
148:34 - deploying okay all right so after a
148:37 - short little wait our vault is created
148:39 - and so what I want you to do is go to
148:40 - the resource and we're going to be using
148:42 - this Vault a little bit in some of the
148:45 - Fall alongs and in some cases is not so
148:47 - much
149:05 - [Music]
149:08 - okay hey this is Andrew Brown and this
149:11 - fall along we're going to be doing some
149:12 - things with uh keys with an Azure key
149:14 - Vault so what I want you to do is make
149:16 - your way to the Keys blade on the left
149:18 - hand side here we're going to generate
149:19 - or slimport a new key we're going to
149:22 - choose the generate option in terms of
149:25 - naming we're going to call this my disk
149:28 - key and we are going to choose RSA 2048
149:31 - that seems totally fine to me everything
149:33 - else seems okay so we'll go ahead and
149:35 - create that key so we'll give it a
149:38 - moment to create doesn't take too long
149:40 - and then what we're going to do is go on
149:43 - the left hand side to I am access
149:45 - controls and what we're want we're going
149:47 - to want to do is add a new Ro assignment
149:50 - so we can go ahead and start using this
149:52 - uh key so what I want you to do is go
149:56 - and look for key Vault administrator
149:59 - which is here we'll go ahead and hit
150:01 - next and then for our uh user we will
150:05 - choose ourselves so under user I'm going
150:07 - to select the members I'm looking for
150:09 - the account I'm using there I am and
150:11 - your brown go ahead and select that
150:13 - there and so that is all we need to to
150:16 - assign it so that we can actually uh
150:19 - work with that key so I think a good
150:22 - idea is to use a key uh to encrypt a
150:25 - disk so what we'll do is make our way
150:27 - over to dis encryption sets because
150:29 - before you can encrypt a dis you need to
150:31 - have an encryption set so we'll go ahead
150:34 - and create ourselves a new encryption
150:35 - set we'll call we'll use the uh sorry
150:38 - the same um resource ccrypt so it's very
150:41 - easy clean up afterwards we'll call this
150:43 - my disk encrypt set here and in terms of
150:49 - the encryption type we're going to use
150:51 - double encryption because that's much
150:53 - better you have two keys that encrypt it
150:55 - so that's a lot better we are going to
150:57 - choose our vault so we have my example
150:59 - Vault there's only one option here and
151:02 - in terms of the key we'll select my dis
151:05 - key terms of the version uh we'll select
151:07 - the current version we'll go ahead and
151:09 - hit review
151:11 - create and then we will go and create
151:15 - that and we'll give give it a moment to
151:17 - create that encryption set shouldn't
151:18 - take too long here and after a short
151:20 - little wait uh our resource should be
151:22 - deployed it took about a minute for me
151:24 - and if we go here it's going to have
151:25 - this message up here it's very small but
151:26 - it says to associate disk image snapshot
151:29 - this dis encryption set you must Grant
151:30 - permissions to key Vault so all we have
151:32 - to do is click that uh alert and will
151:34 - grant permissions and so now we are able
151:37 - uh to use that key um or like to to
151:41 - we're going to have the permissions
151:42 - issues is solved so what we'll do is go
151:43 - to type and create a new disk and so we
151:45 - can apply
151:46 - this key to that encryption so we go
151:48 - ahead and create we're going to choose
151:50 - the same Resource Group here I'm going
151:52 - to call this my example Vault and um or
151:57 - sorry my
151:59 - example uh dis so that's a little bit
152:01 - more clear than that and for the
152:03 - availability Zone doesn't matter for the
152:05 - source type um it doesn't matter as well
152:08 - in terms of the size we want this to be
152:11 - cheap we're not really using this for
152:13 - real so we'll use standard HDD and we'll
152:16 - say okay in terms of encryption this is
152:19 - where things get fun we go to double
152:21 - encryption we choose our key here we'll
152:23 - go ahead review and
152:27 - create and we'll just give it a moment
152:29 - for that to well we'll hit create and
152:31 - we'll have to wait a little while here
152:32 - for that create that resource so we'll
152:34 - just wait until that is created okay and
152:38 - after a very short while the dis is
152:39 - ready so we'll go to that resource we'll
152:41 - go to the encryption tab to see that
152:43 - encryption is applied so that's all it
152:44 - takes to use a key to encrypt a dis so
152:48 - we are going to still use some of these
152:50 - accounts there's no clean up yet I'll go
152:51 - back here and I'll see you in the next
152:56 - [Music]
152:59 - one hey this is Andrew Brown and this
153:02 - follow along we're going to learn about
153:03 - backup and restore key so what I want
153:05 - you to do is go back into the uh
153:08 - Resource Group that we just recently
153:09 - created and we're going to make our way
153:12 - over to keys so I'm just or sorry we got
153:14 - to get into the Vault first then we'll
153:16 - go over to keys and the idea is that we
153:18 - have this key here and so um you can see
153:21 - that we have this current version so you
153:22 - can add additional versions but what's
153:24 - going to happen if we try to back this
153:26 - up so when you back this up you're going
153:28 - to get this file here and if you open up
153:30 - this file it's going to look like a
153:31 - bunch of gobbly goop so I'm just going
153:33 - to try to open it here um I have it up
153:36 - off screen here so I'm just trying to
153:37 - open it up within uh Visual Studio code
153:39 - so I'm just going to open up visual
153:41 - studio code again doing this off screen
153:43 - here just give me a moment all right and
153:47 - so this is the file um that we encrypted
153:50 - uh and you take a look here and it's
153:54 - it's doesn't look like anything but the
153:55 - idea is that it is our backup of our key
153:57 - so that we can rein ort
154:00 - that and just taking a look at the key
154:03 - name this is what it looks like so it
154:04 - says my example Vault my dis key then
154:06 - there's this um uh date and that's key
154:09 - backup so just recognize that's the
154:10 - format and the date is very useful to
154:12 - indicate when you backed it up so let's
154:14 - go ahead and delete this key because the
154:15 - idea is we want to uh restore that
154:18 - backup and so we have deleted that key
154:22 - there and uh what we're going to do is
154:25 - we're going to attempt a restore so I'm
154:27 - going to go ahead and
154:34 - go occurred while restoring the key the
154:37 - key you're trying to restore already
154:39 - exists why would it throw that error
154:42 - we've clearly deleted it and the reason
154:43 - why is that we have Purge protection on
154:46 - we did that in the um first first part
154:50 - when we set up this actual Vault here
154:52 - I'm going to just see if we can find the
154:53 - settings wherever that Purge protection
154:55 - is I'm trying to remember where it is
154:58 - Purge protection is enabled so we can go
155:00 - here and once you enable it you cannot
155:02 - turn it off it's going to retain it for
155:03 - a certain amount of days um and so all
155:06 - you can do is soft delete keys so this
155:09 - key is not actually deleted yet if you
155:11 - go to manage deleted Keys you can see
155:13 - the key is over here and if you try to
155:15 - click on Purge it is disabled because we
155:18 - cannot remove the key because we have
155:20 - Purge protection on but we can recover
155:21 - the key so we'll go ahead and
155:23 - recover uh and so that will allow us to
155:27 - recover the
155:31 - key and if we refresh here it's going to
155:34 - take a little bit time for that key to
155:36 - restore so we'll just have to uh wait a
155:39 - little bit and then it will show
155:41 - up here's one other thing I wanted to
155:44 - show you was under policy because you
155:46 - know um if you go under where's policies
155:50 - here um or access policies if you look
155:53 - under our user here and we look at the
155:55 - key permissions um there is an option to
155:58 - purge and we don't actually have that uh
156:00 - turned on right now but if we were to
156:02 - save this and we were to still go to
156:05 - that Purge option it would still say the
156:06 - same thing so even if you have Purge
156:07 - permissions it does not matter if Purge
156:09 - protections turned on it still will not
156:11 - let you purge but you would need a
156:13 - combination of those in order to uh you
156:16 - know be able to do things there so to
156:18 - really show you how to do that recovery
156:20 - I think what we should do I'm just going
156:22 - to delete our old key here because we
156:23 - don't care about it but we are going to
156:26 - well I guess we could try to import it
156:27 - into the other one so I'm just going to
156:29 - undo that for a second but we are going
156:31 - to go ahead and create ourselves another
156:34 - Vault so I'm going to go and type in
156:38 - Vault at the top here and we're going to
156:40 - be a little bit more careful when we
156:41 - create this Vault so we'll go here and
156:43 - we will choose um
156:46 - my example Vault I'm going to say My
156:49 - Vault no
156:52 - protect and the pricing tier will be
156:54 - standard one day we're going to leave it
156:57 - or well seven is the lowest and we'll
156:59 - say disable Purge protection because we
157:01 - don't want to have that
157:03 - enabled and we'll see if we can import
157:05 - the key into another Vault I'm not sure
157:07 - if we can do that worst case we'll make
157:09 - a new key download the key re-upload it
157:11 - but I'm just curious what would happen
157:12 - if we tried to upload the same key as
157:14 - it's still in another Vault I'm not
157:17 - exactly
157:19 - sure all right so this deployment is
157:21 - successful I'm going to go to this
157:22 - resource I'm going to go ahead to go to
157:24 - create and we're going to restore from
157:26 - backup and we're going to take this key
157:28 - and see if we can actually import it
157:29 - here so it looks like we can take a key
157:31 - and it can exist in multiple vaults I'm
157:34 - going to go ahead and delete this key
157:36 - and we're going to say are you sure you
157:38 - want to delete this key I'm going to say
157:40 - yes and if we go to manage
157:43 - keys and we refresh it takes a a little
157:45 - bit of time here so we'll just wait a
157:47 - moment for this to uh
157:49 - persist and after a short little wait
157:52 - like about 2 minutes I refresh and the
157:53 - key is here so if I go here you'll
157:55 - notice the purges option is still not
157:57 - available we can obviously recover um
157:59 - but we don't have Purge um protection on
158:03 - so if we go to access policies over here
158:05 - and we'll go ahead and scroll down and
158:06 - select Purge and save our changes we can
158:09 - then go back to Keys we'll give it a
158:11 - moment to save we go back to Keys we'll
158:14 - refresh it we'll manage our keys
158:16 - and we'll go ahead and Purge it and that
158:18 - will permanently Purge it there so
158:20 - that's all it takes uh to do that so
158:23 - there you
158:26 - [Music]
158:29 - go hey this is Andrew Brown from exampro
158:32 - and in this section we'll be going over
158:33 - the integration with on premises
158:35 - Solutions with Azure starting with Azure
158:37 - adconnect Azure adconnect is a tool
158:39 - provided by Microsoft that enables
158:41 - organizations to synchronize on premises
158:43 - active directory with Azure active
158:45 - directory C this synchronization enables
158:48 - organizations to extend their on
158:49 - premises identities and security
158:51 - policies to the cloud and enable
158:52 - seamless access to cloud-based
158:54 - applications Azure a DEC connect allows
158:57 - for seamless single side on from your on
158:59 - premises workstation to Microsoft Azure
159:02 - Azure adconnect has the following
159:03 - features password hash synchronization
159:06 - this feature syncs user password hashes
159:08 - from on premises active directory to
159:10 - Azure ID enabling the same signing
159:12 - method for both pass through
159:14 - authentication this allows you users to
159:16 - maintain the same password across on
159:17 - premises and Cloud platforms without
159:19 - needing a separate Federated environment
159:21 - setup Federation integration an optional
159:24 - feature it facilitates a hybrid setup
159:26 - using on premises a DFS infrastructure
159:28 - and provides management tools like
159:30 - certificate renewal and server
159:32 - deployment synchronization this is
159:34 - responsible for creating and aligning
159:36 - users groups and other objects between
159:38 - on premises and Cloud ensuring identity
159:40 - information matches across both Health
159:43 - monitoring Azure adconnect Health offers
159:45 - robust activity monitoring with a
159:46 - dedicated azzure portal section to
159:48 - review this data here are the steps for
159:51 - installing configuring and synchronizing
159:53 - on premises active directory with
159:55 - azuread using Azure adconnect Why
159:58 - install Azure adconnect install Azure
160:00 - adconnect on a server connected to both
160:02 - on premises a and Azure a to configure
160:05 - Azure ad connect use the wizard to set
160:07 - up synchronization settings Source
160:09 - Target directories and sync frequency
160:12 - three synchronized directories aure a
160:14 - DEC connect syn on premises a with
160:16 - azzure a d replicating changes from
160:18 - source to Target for Monitor and manage
160:21 - continuously Monitor and manage the
160:23 - synchronization process to ensure
160:25 - accuracy and meet business needs overall
160:28 - Azure a DEC connect is your bridge
160:29 - between the on premises world and Azure
160:31 - it ensures a synchronized coherent and
160:34 - seamless
160:39 - experience the next topic we'll be
160:41 - covering is azure a application proxy
160:44 - Azure a application proxy is a service
160:46 - provided by Microsoft Azure that allows
160:48 - organizations to provide remote access
160:50 - to their on premises web applications it
160:53 - allows users to access the applications
160:55 - securely from anywhere using any device
160:57 - without the need for complex network
160:59 - configuration or exposing the
161:00 - applications directly to the internet
161:03 - organizations can publish their on
161:04 - premises applications to the cloud
161:06 - providing secure remote access for their
161:08 - users the service allows organizations
161:11 - to use their existing on premises
161:13 - infrastructure and application
161:14 - architecture leveraging the benefits of
161:16 - the cloud it provides Advanced security
161:19 - features such as multiactor
161:20 - authentication and conditional access
161:22 - policies ensuring that only authorized
161:24 - users can access the
161:26 - applications the azuread application
161:29 - proxy service consists of two main
161:31 - components while an Azure a d
161:33 - application proxy connector a
161:35 - lightweight agent that is installed on a
161:36 - server within the organizations's on
161:38 - premises environment the connector
161:40 - establishes a secure outbound connection
161:42 - to the azuread application proxy service
161:45 - which which enables communication
161:46 - between the on premises application and
161:48 - the azuread service to Azure ad
161:51 - application proxy service a cloud-based
161:53 - service that manages the authentication
161:55 - and authorization of users who access
161:57 - the on premises web applications through
161:59 - the application proxy connector it also
162:01 - routes traffic to the appropriate pen
162:03 - servers and enforces policies set by the
162:06 - organization next let's look at the
162:08 - azuread application proxy architecture
162:11 - here's an image of the process and we'll
162:12 - go through each step why an user access
162:16 - the user accesses the application and
162:17 - gets redirected to azuread for sign in
162:20 - any set conditional access policies are
162:22 - checked two token issuance after
162:25 - successful signin azuread sends a token
162:27 - to the user's device three token
162:30 - interpretation the client sends this
162:32 - token to application proxy which
162:33 - extracts the user principal name and
162:35 - security principal name for request
162:38 - forwarding application proxy forwards
162:40 - the request to the connector installed
162:42 - on premises five additional
162:45 - authentication optionally the connector
162:47 - May perform additional authentication
162:49 - and then sends the request to the on-
162:50 - premises application six server response
162:54 - the application's response is sent back
162:56 - through the connector to the application
162:57 - proxy service seven response delivery
163:01 - finally the application proxy service
163:03 - delivers the response to the user's
163:05 - device next we'll go over some of the
163:07 - use cases for Azure a application proxy
163:11 - remote access Azure ad application proxy
163:13 - is commonly used to provide secure
163:15 - remote access to on premise applications
163:17 - employees working from home or other
163:19 - remote locations can securely access
163:21 - their internal applications just as if
163:23 - they were in the office single signon
163:25 - application proxy can integrate with
163:27 - azuread to provide single signon
163:29 - capabilities this allows users to
163:31 - authenticate once and then access
163:33 - multiple applications without needing to
163:34 - sign in again conditional access with
163:37 - application proxy you can leverage Azure
163:39 - ad's conditional access policies for
163:41 - your on premise applications this
163:43 - provides granular control over over
163:45 - access based on user location device
163:47 - status and other factors Legacy
163:50 - application modernization application
163:52 - proxy can help organizations expose
163:54 - Legacy on premise applications to the
163:56 - internet in a secure manner without
163:58 - changing the application code this can
164:00 - be a key part of a strategy to modernize
164:02 - Legacy
164:03 - applications scalability and performance
164:06 - Azure a d application proxy scales
164:08 - automatically to meet your
164:09 - organization's usage patterns and
164:11 - provides a global reach without needing
164:12 - to open additional firewall ports this
164:14 - can help help improve the performance
164:16 - and availability of your
164:18 - applications in conclusion Azure a
164:20 - application proxy is like a security
164:22 - guard in a bridge ensuring that your on
164:24 - premises applications are both
164:25 - accessible and protected be sure to know
164:28 - this as it'll definitely appear on the
164:31 - [Music]
164:34 - exam hey this is Andrew Brown from exam
164:37 - Pro and in this section we'll be going
164:39 - over storage accounts in Azure Azure
164:41 - storage offers several types of storage
164:43 - accounts each with different features
164:45 - and their own pricing models these
164:46 - storage account types include standard
164:49 - general purpose V1 which is now
164:51 - considered Legacy standard general
164:52 - purpose V2 blob storage block blob
164:55 - storage and file storage storage
164:57 - accounts vary with the following
164:59 - features supported Services essentially
165:02 - this answers the question what can I put
165:03 - in this storage account your options
165:05 - include blob file Q table disk and data
165:08 - Lake Gen 2 performance tears this
165:11 - focuses on the speed of your read and
165:13 - write operations Azure offers to tiers
165:15 - standard and premium access tiers how
165:18 - often do I need quick access to files
165:20 - the tiers are hot cool archive
165:23 - replication how many redundant copies
165:25 - should be made and where Azure provides
165:27 - various replication options including
165:29 - lrs GRS R GS zrs gzrs R gzrs deployment
165:35 - model Azure has two models resource
165:37 - manager and
165:38 - classic the table details different
165:41 - types of azure storage accounts their
165:42 - capabilities performance tiers access
165:45 - tiers replication methods and deployment
165:47 - models type this column describes the
165:50 - different types of storage accounts
165:52 - general purpose V2 this is the latest
165:54 - version of azure storage accounts and
165:55 - supports various services like blob file
165:58 - Q table disk and data Lake Gen 2 general
166:01 - purpose V1 this is the older version and
166:04 - supports blob file Q table and disk
166:07 - block blob storage designed for storing
166:09 - block blobs and aen blobs file storage
166:12 - specifically tailored for Azure file
166:14 - shares blob storage meant for storing
166:16 - block and the pen blobs service these
166:19 - are the storage Services each account
166:20 - type can provide performance tiers Azure
166:23 - offers two performance tiers standard
166:26 - back by hard disk drives and suitable
166:27 - for general purpose storage premium uses
166:30 - solid state drives and is optimized for
166:32 - high performance and low latency
166:34 - workloads access tiers these Define the
166:36 - data access frequency hot for frequently
166:39 - accessed data tool optimize for storing
166:42 - and frequently accessed data for a
166:44 - minimum of 30 days archive for rarely
166:47 - Access Data with a minimum of 180 days
166:49 - storage duration replication Azure
166:51 - provides multiple replication options
166:53 - for ensuring data durability and
166:55 - availability lrs locally redundant
166:58 - storage stores multiple copies of your
167:00 - data in a single data center GRS Geo
167:03 - redundant storage replicates your data
167:05 - to a secondary region raw GRS read
167:08 - access Geo redundant storage offers
167:10 - readon access to the data in the
167:11 - secondary location in addition to Geo
167:13 - replication zrs Zone redundant storage
167:17 - spreads data across multiple
167:18 - availability zones gzrs geozone
167:21 - redundant storage combines both zrs and
167:24 - GRS by spreading data across
167:25 - availability zones and replicating to a
167:27 - secondary region raw gzrs like gzrs but
167:31 - with read access to the secondary region
167:33 - deployment models resource manager this
167:36 - is azure's modern deployment model it
167:38 - allows you to group related resources
167:40 - together for easier management classic
167:42 - the older deployment model that existed
167:44 - before the the introduction of the Azure
167:46 - resource
167:47 - manager lastly let's explore the five
167:49 - core Services Azure provides in the
167:51 - storage domain Azure blob a massively
167:54 - scalable Object Store for text and
167:56 - binary data also includes support for
167:58 - big data analytics through data like
168:00 - storage Gen 2 Azure files manage file
168:03 - shares for cloud or on premises
168:05 - deployments Azure cues a messaging store
168:07 - for Reliable messaging between
168:09 - application components Azure tables a
168:12 - nsql store for schema Less storage of
168:14 - structured data Azure diss Block Level
168:17 - storage volumes for Azure VNS so that's
168:20 - an introduction to storage accounts in
168:26 - Azure the next topic we'll be exploring
168:29 - is azure blob storage in more detail
168:31 - blob storage is an object store that is
168:33 - optimized for storing massive amounts of
168:35 - unstructured data unstructured data is
168:38 - data that doesn't adhere to a particular
168:39 - data model or definition such as text or
168:42 - binary data Azure blobs are composed of
168:44 - the following core components storage
168:47 - account this is essentially your unique
168:48 - space or Nam space in Azure and it looks
168:50 - something like this HTTP colon doubleist
168:53 - account. blob. core. windows.net
168:57 - container this component functions
168:59 - similarly to a folder in a file system
169:02 - blobs here is where the actual data is
169:05 - stored Azure storage supports three
169:07 - types of blobs why block blobs these are
169:10 - ideal for storing text in binary data
169:12 - it's made up of blocks of data that can
169:14 - be maned individually and they can store
169:16 - up to about 4.75 tip of data two append
169:20 - blobs specially optimized for append
169:22 - operations these are ideal for scenarios
169:24 - such as logging data from virtual
169:26 - machine three page blobs capable of
169:28 - storing random access files up to 8
169:30 - terabytes in size and they are suited
169:32 - for store virtual hard drive files and
169:34 - serve as discs for Azure virtual
169:37 - machines when it comes to transferring
169:39 - data into Azure blob storage there are
169:41 - multiple Methods at your disposal
169:42 - including as copy and easy easy to use
169:45 - command line tool for Windows and ltic
169:47 - Azure storage data movement Library a
169:49 - net library that uses as copy in the
169:51 - background Azure data Factory an ETL
169:54 - service by Azure blop pews this virtual
169:57 - file system driver allows for direct
169:59 - data access through the Linux file
170:01 - system Azure data box a robust physical
170:04 - device designed to transport data to
170:05 - Azure securely Azure import export
170:08 - service a service where you ship your
170:10 - physical discs for data transfer onto
170:12 - Azure so that's an overview of azure
170:14 - blob
170:19 - storage the next topic we'll be covering
170:21 - are the performance tiers in Blob
170:23 - storage in a bit more detail there are
170:25 - two types of performance tiers for
170:27 - storage accounts standard and premium
170:30 - before we delve into the details it's
170:31 - essential to understand the term iops
170:34 - iops stands for input output operations
170:36 - per second the higher the iops the
170:39 - faster a drive can read and write
170:41 - premium performance tier in this tier
170:43 - data is stored on solid state drives
170:45 - these drives are optimized for low
170:47 - latency operations ensuring higher
170:48 - throughput and speed data access some of
170:51 - the ideal use cases include interactive
170:53 - workloads analytics AI or machine
170:55 - learning processes and data
170:57 - transformation tasks and SSD has no
171:00 - moving parts and data is distributed
171:02 - randomly this is why it can read and
171:04 - write so fast on the other hand standard
171:06 - performance the standard tier stores
171:08 - data on hard disk drives these drives
171:11 - offer VAR Performance Based on the
171:12 - access tier such as hot cool or archive
171:15 - the standard tier shines in the
171:17 - following use cases backup and Disaster
171:19 - Recovery operations storing media
171:21 - content and bulk data processing an HDD
171:24 - as moving Parts an arm that needs to
171:26 - read and write data sequential to a disk
171:28 - it is very good at writing or reading
171:30 - large amounts of data that is close
171:32 - together overall your choice between
171:34 - premium and standard largely depends on
171:36 - your specific requirements whether you
171:38 - prioritize Lightning Fast data access or
171:40 - more budget friendly voluminous data
171:43 - storage
171:48 - moving on to the next topic we'll be
171:49 - covering access tiers for blob storage
171:51 - in more detail so there are three types
171:54 - of access tiers for standard storage
171:56 - cool hot and archive hot tier ideal for
171:59 - data that's access frequently it has the
172:01 - highest storage cost but you get the
172:03 - lowest access cost use cases data that's
172:06 - in active use or expected to be access
172:08 - frequently and data that staged for
172:10 - processing an eventual migration to the
172:12 - cool access tier cool here best for data
172:15 - that's in frequently accessed and stored
172:17 - for at least 30 days it has lower
172:19 - storage cost but higher access cost use
172:22 - case great for short-term backup in
172:24 - Disaster Recovery data sets older media
172:26 - content not viewed frequently anymore
172:28 - but is expected to be available
172:29 - immediately when accessed and large data
172:32 - sets that need to be stored cost-
172:33 - effectively while more data is being
172:35 - gathered for future processing archive
172:37 - tier best for data that's rarely
172:39 - accessed and stored for at least 180
172:41 - days it has the lowest storage cost but
172:44 - the highest access cost use case best
172:46 - suited for long-term backup secondary
172:48 - backup and archival data sets original
172:51 - data that must be preserved even after
172:53 - it has been processed into final usable
172:55 - form and compliance and archival data
172:57 - that needs to be stored for a long time
172:59 - and is hardly ever
173:00 - accessed before we move on let's touch
173:03 - upon some essential technical aspects
173:05 - account level tiering any blob that
173:07 - doesn't have an explicitly assigned tier
173:09 - infers the tier from the storage account
173:10 - access tier setting blob level tiering
173:13 - you can upload a blob to the tier of
173:14 - your choice changing tiers happens
173:16 - instantly with the exception from moving
173:18 - out of archive rehydrating a blob when
173:21 - moving a blob out of archive into
173:22 - another tier it can take several hours
173:24 - this is known as rehydrating blob life
173:27 - cycle management here you can create
173:29 - rule-based policies to transition data
173:31 - to different tiers such as after 30 days
173:33 - move to Cool
173:34 - Storage when a blob is uploaded or moved
173:37 - to another tier it's charged at the new
173:39 - tier rate immediately upon tier change
173:41 - when moving from a cooler tier the
173:43 - operation is building is a right
173:44 - operation to the destination tier where
173:46 - the right operation and data right
173:48 - charges of the destination tier apply
173:51 - when moving from a hotter tier the
173:52 - operation is built as a read from the
173:54 - source tier where the read operation and
173:56 - data retrieval charges of the source
173:57 - tier apply early deletion charges for
173:59 - any blot moved out of the cool or
174:01 - archive tier May apply as well cool and
174:04 - archive early deletion any blob that is
174:06 - moved into the cool tier is subject to a
174:08 - cool early deletion period of 30 days
174:10 - any blob that is moved into the archive
174:12 - tier is subject to an archive early
174:13 - deletion per period of 180 days this
174:16 - charge is prated so that's a more
174:18 - in-depth look into the access tiers for
174:20 - blob
174:25 - storage the next topic we'll be covering
174:28 - is the replication and data redundancy
174:29 - for storage accounts so when you create
174:32 - a storage account you need to choose a
174:34 - replication type replication stores
174:36 - multiple copies of your data so that it
174:38 - is protected from planned events
174:40 - transient Hardware failures Network or
174:42 - power outages or even massive of natural
174:45 - disasters primary region redundancy
174:47 - these include locally redundant storage
174:49 - and Zone redundant storage secondary
174:51 - region redundancy this includes Geo
174:53 - redundant storage and geozone redundant
174:55 - storage secondary region redundancy with
174:58 - re access re access Geo redundant
175:00 - storage and reais geozone redundant
175:02 - storage as you can expect the greater
175:05 - level of redundancy the more expensive
175:06 - the cost of
175:08 - replication for redundancy in the
175:10 - primary region data is replicated three
175:12 - times in the primary region there are
175:14 - two options for storing in the primary
175:16 - region locally redundant storage copies
175:18 - data synchronously in primary region
175:22 - 99.999999999% that's 11 9's durability
175:26 - this is the cheapest option so redundant
175:28 - storage copies data synchronously across
175:30 - 3 A's in primary region
175:33 - 99.9% 12 9's
175:35 - durability for redundancy in the
175:37 - secondary region data is replicated to a
175:39 - secondary region in case of primary
175:41 - Regional disaster the secondary region
175:43 - is determined based on your primary's
175:45 - pair region secondary region isn't
175:47 - available for read or right access Geo
175:50 - redundant storage copies data
175:51 - synchronously in primary region copies
175:53 - data asynchronously to another region
175:56 - 99.9% that's 16 NES of durability gozone
176:00 - redundant storage similar to GRS but
176:03 - adds synchronous replication across
176:04 - three availability zones in the primary
176:06 - region before a synchronously
176:07 - replicating to another region
176:09 - maintaining the same
176:10 - 99.9% that 1699 durability level
176:15 - redundancy in the secondary region with
176:17 - read access data is replicated
176:19 - synchronously to primary region your
176:21 - data will be in sync with your primary
176:22 - and you'll have red access read access
176:25 - Geo redundant storage ensures
176:27 - synchronous data replication within the
176:28 - primary region into another region
176:30 - offering a high durability of
176:32 - 99.9% with
176:34 - 169 read AIS geozone redundant storage
176:37 - this goes a step further by replicating
176:39 - data synchronously across three
176:41 - availability zones in the primary region
176:43 - before synchronous L replicating to
176:45 - another region maintaining the same high
176:46 - durability level so choosing the right
176:49 - replication strategy depends on your
176:51 - business needs weighing costs against
176:53 - data durability and accessibility and
176:55 - that about covers main points for the
176:57 - replication and data redundancy for
176:58 - storage
177:04 - accounts the next topic we'll be
177:06 - exploring is a z copy so what is a z
177:08 - copy a z copy is a versatile command
177:11 - line tool designed specifically for
177:13 - copying blobs or files to or from Azure
177:15 - storage accounts it's a goto utility for
177:17 - many when they think of data transfer
177:19 - with Azure while download first things
177:22 - first to get started with a z copy
177:24 - you'll need to download the executable
177:25 - file compatible with your operating
177:27 - system be it windows litx or Macos to
177:31 - before you begin transferring data
177:33 - ensure you have the necessary level of
177:35 - authorization you will need to have the
177:37 - level of authorization via attached
177:38 - roles for downloading you'll require the
177:41 - storage blob data reader Ro for
177:43 - uploading the rules necessary are
177:45 - storage blob data contributor and
177:46 - storage blob data owner three you gain
177:49 - access either via a z copy login options
177:52 - for authentication include using Azure
177:54 - active directory or a shared access
177:56 - signature known commonly as SAS this
177:59 - prompts you deci side in you'll then be
178:00 - guided to use a web browser open a
178:02 - specific page and enter a given code to
178:05 - authenticate for copying data to move
178:07 - data use the straightforward copy
178:09 - command a z copy copy whether you're
178:11 - uploading or downloading this command is
178:13 - your way to data transfer so that's a
178:16 - quick summary of how to utilize a z
178:22 - copy the next topic we'll be covering is
178:25 - the life cycle Management in Azure
178:27 - storage Azure storage life cycle
178:29 - management offers a rule-based policy
178:31 - that you can use to transition blob data
178:33 - to the appropriate access tiers or to
178:35 - expire data at the end of the data life
178:36 - cycle with the life cycle management
178:39 - policy you can transition blobs from
178:41 - cool to hot immediately when they are
178:43 - accessed to optimize for performance
178:45 - transition blobs blob versions and blob
178:48 - snapshots to a cooler storage tier if
178:50 - the objects have not been accessed or
178:51 - modified for a period of time to
178:53 - optimize for cost delete blobs blob
178:56 - versions and blob snapshots at the end
178:58 - of their life cycles Define rules to be
179:00 - run once per day at the storage account
179:02 - level and apply rules to Containers or
179:04 - to a subset of blobs using name prefixes
179:07 - or blob index tags as
179:09 - filters so to manage the life cycle of
179:11 - our blobs inside containers a life cycle
179:14 - management rule must be created navigate
179:16 - to your Azure storage account go to life
179:18 - cycle management find and select blob
179:20 - service and click on add a rule from
179:22 - here decide whether to May apply this
179:24 - rule to all blobs inside the storage
179:26 - account or filter the blobs to have this
179:27 - rule applied in this storage account for
179:30 - example if Bas blobs were last modified
179:33 - for more than one day ago then delete
179:34 - the blob for example if bass blobs were
179:37 - last modified from than one day ago then
179:39 - delete the blob if base blobs were last
179:42 - modified more than 2 days ago then then
179:43 - move to Cool Storage overall Azure
179:46 - storage life cycle management gives you
179:48 - automated tools to handle data
179:49 - efficiently as it progresses through its
179:51 - life cycle balancing between performance
179:53 - needs and cost
179:56 - [Music]
179:59 - considerations hey this is Andrew Brown
180:01 - from exam Pro and in this section we'll
180:03 - be covering the Azure SQL offerings
180:05 - let's break them down SQL server on
180:08 - Azure VMS provides SQL server and Os
180:11 - level access supports various SQL n OS
180:14 - versions this is an ideal choice for
180:16 - migrations and applications that need
180:18 - direct interaction with the operating
180:20 - system managed instances single instance
180:23 - gives SQL server and Os access for
180:25 - robust needs supports various SQL and Os
180:28 - versions best for isolated SQL workloads
180:31 - needing dedicated resources instance
180:33 - pool enables pre-provisioning of
180:35 - resources for efficient migration host
180:37 - smaller cost-effective instances this is
180:40 - a perfect fit if you're looking to
180:41 - migrate several smaller databases in b
180:43 - es suitable for batch migration of
180:45 - smaller databases databases single
180:49 - database offers hyperscale storage up to
180:51 - 100 terabytes features demand-based
180:53 - serverless compute best for cloud
180:55 - applications needing a fully managed
180:57 - service elastic pool enables resource
181:00 - sharing among databases simplifies
181:03 - Performance Management with multiple
181:04 - databases best for managing and scaling
181:07 - databases with variable usage patterns
181:09 - so that's an overview of the Azure SQL
181:12 - offerings whether you're migrating
181:13 - scaling or starting fresh Azure has a
181:16 - SQL solution tailored for your
181:21 - needs let's talk about Azure SQL
181:24 - databases in more detail Azure SQL
181:27 - database is a fully managed relational
181:29 - database service provided by Microsoft
181:31 - Azure it's a cloud-based database
181:33 - service that offers a high level of
181:35 - scalability availability and security
181:38 - Azure SQL database is based on the
181:40 - latest version of Microsoft SQL server
181:42 - and it's designed to handle various
181:44 - workloads ranging from small web
181:45 - applications to large Enterprise
181:47 - workloads hazure SQL database supports
181:50 - popular relational database engines such
181:52 - as SQL Server MySQL and postgressql and
181:55 - offers a variety of deployment options
181:57 - including single database and elastic
181:59 - pool Azure SQL database offers several
182:02 - benefits that make it a popular choice
182:04 - for businesses looking to migrate their
182:06 - on premises databases to the
182:08 - cloud some of the key benefits include
182:11 - fully managed service Azure handles
182:13 - administrative tasks like patching and
182:15 - backups freeing businesses to focus on
182:17 - core tasks High availability built-in
182:20 - automatic failover and Disaster Recovery
182:22 - capabilities ensure data access even
182:24 - during outages scalability Azure SQL
182:27 - database easily scales resources
182:29 - according to workload optimizing costs
182:32 - security Advanced features like threat
182:34 - protection and data encryption ensure
182:35 - secure data storage integration Azure
182:38 - SQL database integrates seamlessly with
182:41 - other Azure Services supporting the
182:42 - development of modern datadriven
182:45 - applications the Azure SQL database
182:48 - service offers various tiers to cater to
182:49 - a range of requirements and workloads
182:52 - here's a breakdown basic tier this is
182:55 - the most economical tier optimized for
182:57 - lighter database workloads its best for
182:59 - tasks like testing and development as
183:01 - well as for other non-critical workloads
183:03 - you can store data up to 2 gb and it
183:06 - provides 5 dtus which stands for
183:08 - database transaction units that offer a
183:10 - combined measure of compute storage and
183:12 - EO resources
183:14 - standard tier this tier is designed to
183:16 - handle the majority of database
183:18 - workloads it's ideal for business
183:20 - critical production workloads the
183:22 - storage capacity is up to 1 tbte and it
183:24 - offers a range of 10 to 4,000
183:26 - dtus premium tier build for Mission
183:30 - critical databases this tier emphasizes
183:32 - High transactional rates it's
183:34 - particularly suitable for workloads with
183:36 - high volume transactions allows data
183:38 - storage up to 4 terabytes and it
183:40 - provides 125 to 20,000 dtus
183:44 - general purpose tier this tier is
183:46 - designed for customers with demanding
183:48 - database workloads fits best for
183:50 - moderate to heavy transactional
183:51 - workloads offers storage up to 4
183:53 - terabytes equipped with 5 to 80v cores
183:56 - providing robust computational capacity
183:59 - hyperscale tier optimized for extremely
184:02 - large data volumes High transaction
184:04 - rates and great concurrency this tier is
184:06 - the best fit for large volume of ltp
184:08 - workloads provides a massive storage
184:10 - capacity of up to 100 terabytes provides
184:13 - a compute range of 4 to 160 V cores in
184:16 - summary Azure SQL database offers a
184:19 - comprehensive set of tiers that cater to
184:21 - everything from lightweight development
184:22 - tasks to high demand Mission critical
184:24 - applications ensuring that organizations
184:27 - can pick the perfect blend of cost
184:28 - performance and capacity for their
184:34 - needs the next topic will be covering
184:37 - our Azure SQL database elastic pools
184:40 - Azure SQL database elastic pools are a
184:42 - simple cost-effective solution for
184:44 - managing and scaling multiple databases
184:46 - that have varying and unpredictable
184:48 - usage demands the databases in an
184:50 - elastic pool are on a single server and
184:52 - share a set number of resources at a set
184:54 - price the concept of elastic pools
184:56 - refers to a shared pool of resources
184:58 - such as CPU memory and storage allocated
185:01 - to a group of databases this shared set
185:03 - of resources can be automatically
185:05 - adjusted and distributed among the
185:07 - databases based on their varying demand
185:09 - Azure SQL database elastic pools is
185:12 - ideal for businesses with many databases
185:14 - experiencing varying workloads instead
185:16 - of allocating dedicated resources per
185:18 - database an elastic pool shares
185:20 - resources across databases for efficient
185:22 - use and cost
185:23 - reduction benefits of azure SQL database
185:26 - elastic pools coste effective share
185:29 - resources across databases pay only for
185:31 - what you use Performance Management
185:34 - resources are autom managed across a
185:36 - pool no manual adjustment required
185:38 - flexibility and scalability auto scales
185:41 - to meet demand handling traffic spikes
185:43 - smoothly simplified Administration
185:45 - easier management with shared resources
185:47 - across databases use cases for Azure SQL
185:51 - database elastic pools SAS providers
185:54 - manage varying customer database
185:56 - activity costs effectively development
185:58 - and test environments efficient less
186:01 - expensive process for regular database
186:03 - setup and teardown businesses with
186:05 - multiple apps optimize resource usage
186:07 - and cost by sharing resources among
186:09 - databases with varying activity so
186:12 - that's an overview of aure SQL database
186:14 - elastic
186:16 - [Music]
186:19 - pools next we'll be exploring Azure SQL
186:22 - managed instance in more detail Azure
186:25 - SQL managed instance is a fully managed
186:27 - database service offered by Microsoft
186:29 - Azure designed to provide an easy
186:31 - migration path for SQL Server workloads
186:33 - to Azure it provides a managed instance
186:35 - of SQL server in the cloud allowing you
186:38 - to run your existing applications with
186:39 - minimal changes it's built on top of the
186:42 - latest SQL server engine and supports
186:44 - all its features including complex
186:46 - queries and userdefined functions it
186:48 - offers various deployment options
186:50 - including Standalone automatic failover
186:52 - configurations and cross region
186:54 - replication for Disaster Recovery aure
186:57 - SQL managed instan is ideal for
186:59 - modernizing SQL Server workloads
187:01 - consolidating multiple SQL Server
187:03 - instances and building new Cloud native
187:06 - applications some of the main benefits
187:08 - of azure SQL managed instance include
187:11 - easy migration provides a Swift simple
187:13 - migration path for SQL Server workloads
187:16 - to Azure with minimal application
187:17 - changes fully managed Microsoft handles
187:20 - maintenance backups and updates allowing
187:23 - focus on applications High availability
187:26 - built-in capabilities for automatic
187:27 - failover and Disaster Recovery ensure
187:29 - constant application availability
187:32 - Security offers features like data
187:34 - encryption and threat detection for data
187:36 - protection performance supports large
187:38 - databases High transaction rates and low
187:41 - latency queries integration seamlessly
187:44 - integrates with other Azure services for
187:45 - easy Cloud native application deployment
187:48 - Azure SQL managed instance has two
187:51 - service tiers general purpose this tier
187:53 - is for light to medium IO applications
187:56 - using local storage and providing an
187:58 - economical and scalable option suitable
188:00 - for small and medium-sized businesses
188:02 - needing an affordable Cloud option
188:04 - business critical this tier is for high
188:06 - IO applications offering High
188:08 - availability automatic failover and
188:11 - premium storage ideal for critical
188:13 - applications in large Enterprises
188:15 - ensuring high performance availability
188:17 - and durability so that's a brief
188:20 - overview of azure SQL managed
188:26 - instances the next topic we'll be
188:28 - covering is database scalability both
188:31 - Azure SQL database and Azure SQL managed
188:33 - instance enable you to scale database
188:35 - Resources with minimal downtime
188:37 - adjusting quickly to workload or traffic
188:39 - changes to dynamically scale database
188:41 - resources you can use the the following
188:43 - options horizontal scaling this involves
188:46 - adding or removing replicas to adjust
188:48 - the capacity of your database both Azure
188:50 - SQL database and Azure SQL managed
188:52 - instance support horizontal scaling
188:55 - vertical scaling this involves adjusting
188:57 - the resources allocated to your database
188:59 - such as CPU or memory again both Azure
189:01 - SQL database and Azure SQL managed
189:04 - instance support vertical scaling the
189:07 - exact process for scaling your database
189:09 - resources may vary depending on which
189:10 - service you are using but in general the
189:13 - steps are as follows determine the
189:15 - resource needs of your database such as
189:17 - the required CPU and memory and the
189:19 - expected workload or traffic decide on
189:21 - the scaling option that best meets your
189:23 - needs whether that be horizontal or
189:25 - vertical scaling use the Azure portal po
189:28 - shell or the Azure CLI to configure the
189:30 - scaling settings for your database
189:32 - monitor the performance of your database
189:34 - to ensure that the scaling changes are
189:36 - providing the expected
189:37 - improvements the difference between
189:39 - Azure SQL database and Azure SQL managed
189:42 - instance when a when it comes to
189:43 - dynamically scaling database resources
189:46 - Azure SQL database offers a wide range
189:48 - of service tiers each with varying
189:50 - levels of performance and capabilities
189:52 - these service tiers allow you to choose
189:54 - the level of resources that best meets
189:55 - your needs and to easily scale up or
189:57 - down as needed as your SQL managed
190:00 - instance offers two service tiers
190:02 - general purpose and business critical
190:04 - these service tiers are designed to meet
190:05 - different needs with general purpose
190:07 - offering a balance of price and
190:08 - performance and business critical
190:10 - offering higher performance and
190:11 - availability for Miss critical workloads
190:14 - so that's a quick overview of database
190:16 - scalability focusing mainly on the
190:18 - commonly used Azure SQL database and
190:20 - Azure managed SQL
190:26 - instance the next topic we'll be
190:28 - covering is dynamic data masking Dynamic
190:31 - data masking or DDM an Azure is a
190:33 - feature that helps prevent unauthorized
190:35 - access to sensitive data it is a
190:37 - security feature of azure SQL database
190:39 - Azure synapse analytics and SQL Server
190:42 - that automatically conceal sensitive
190:43 - data in the result set of a query for
190:46 - example if you have a credit card number
190:48 - stored is 1 2 3 4 5 6 7 8 9101 11121 a
190:53 - dynamic data masking rule might conceal
190:55 - the numbers with all X's except for the
190:57 - ending four numbers in the query results
190:59 - ensuring most of the data stays
191:00 - protected and away from prying eyes
191:03 - Dynamic data masking is useful for
191:05 - scenarios where you want to provide a
191:06 - level of data security without needing
191:08 - to modify database operations it's often
191:11 - used in scenarios like reducing the
191:13 - exposure of sensitive data in your
191:15 - database when users are running reports
191:16 - or analytics and preventing accidental
191:19 - exposure of sensitive data especially
191:21 - when data is being used for development
191:23 - or testing
191:24 - purposes key features realtime masking
191:27 - data is masked in real time and does not
191:29 - affect the underlying data stored in the
191:31 - database the actual data remains intact
191:33 - and is not physically changed
191:35 - customizable masking patterns you can
191:38 - Define different types of masks
191:39 - depending on the nature of the data from
191:41 - partially hiding email addresses to
191:43 - fully masking credit card numbers except
191:45 - the last four digits the choices are
191:47 - vast and flexible role-based access
191:49 - control a masking permissions can be
191:51 - granted to users who need to access the
191:53 - actual data ease of use Dynamic data
191:56 - masking is simple to set up and doesn't
191:57 - require changes to the database or
191:59 - applications common use cases for
192:02 - dynamic data masking include protecting
192:04 - personally identifiable information
192:06 - financial data and other sensitive data
192:08 - types in non-production environments or
192:10 - in applications with user roles that
192:12 - require access access to a database but
192:13 - not all of its sensitive data so that's
192:16 - an overview of dynamic data
192:22 - masking Azure storage provides several
192:25 - security and encryption features to
192:26 - ensure the confidentiality integrity and
192:29 - availability of your data here are some
192:31 - of the key features encryption at rest
192:33 - Azure storage automatically encrypts all
192:35 - data at rest using Azure storage service
192:37 - encryption or customer managed Keys
192:39 - using Azure key Vault encryption and
192:42 - Transit all data transferred to INF aure
192:44 - storage is encrypted using secure SSL
192:46 - TLS protocols keeping your data secure
192:48 - during Transmissions rule-based Access
192:51 - Control Azure storage provides rbac
192:54 - which enables you to Grant permissions
192:55 - to users groups and applications at A
192:57 - fine grain level access keys and shared
193:00 - access signatures Azure storage provides
193:02 - two types of authentication mechanisms
193:04 - for accessing storage accounts access
193:06 - keys and shared access signatures access
193:09 - keys are account keys that allow full
193:10 - access to a storage account while
193:12 - provides granular Access Control to
193:14 - specific resources within a storage
193:17 - account Azure private link enables you
193:19 - to access Azure storage resources over a
193:21 - private end point in your virtual
193:23 - Network this ensures that traffic
193:25 - between your virtual Network and Azure
193:26 - storage remains on the Microsoft Azure
193:28 - backbone
193:29 - Network Azure virtual network service
193:32 - and points Azure virtual network service
193:34 - and points enable you to extend your
193:35 - virtual Network to Azure storage
193:37 - similarly to Azure private link this
193:39 - ensures traffic remains within the
193:41 - confines of the Microsoft azure Azure
193:43 - Network Azure firewall this is a managed
193:46 - cloud-based network security service
193:48 - that protects your Azure virtual network
193:50 - resources you can use Azure firewall to
193:52 - secure traffic between your virtual
193:54 - Network and Azure storage Azure Monitor
193:57 - and Azure security Center Azure Monitor
193:59 - and Azure security Center provide
194:00 - monitoring and security features for
194:02 - Azure storage Azure monitor allows you
194:04 - to monitor storage account metrics and
194:06 - logs Azure security Center provides
194:08 - security recommendations and threat
194:10 - detection for Azure storage overall
194:13 - Azure storage isn't just a storage
194:15 - solution it's a fortress designed to
194:16 - protect your data at all levels from
194:18 - encryption mechanisms and access
194:20 - controls to private networking and
194:22 - threat detection it's equipped to tackle
194:24 - diverse security
194:26 - [Music]
194:29 - challenges hey this is Andrew Brown from
194:32 - exam Pro and in this section we'll be
194:34 - covering Azure synapse analytics so
194:36 - Azure synaps analytics is a data
194:38 - warehouse and unified analytics platform
194:40 - it offers a code free visual environment
194:42 - that streamlines and simplifies the
194:44 - building of ETL elt processes it allows
194:47 - for easy ingestion of data via more than
194:49 - 95 native connectors ensuring
194:51 - comprehensive access to a wide variety
194:53 - of data sources the integration of
194:55 - Apache spark into Azure synapse
194:57 - analytics allows for the use of tsql
194:59 - queries across both the data warehouse
195:01 - and Spark engines tsql short for
195:04 - transact SQL is Microsoft's
195:06 - implementation of SQL it's used to
195:08 - interact with relational databases in
195:10 - Microsoft SQL Server in addition it
195:13 - supports a variety of languages
195:14 - including tsql python Scala spark SQL
195:18 - and net broadening its accessibility to
195:20 - different developers Azure synapse
195:23 - analytics is not limited to data
195:24 - warehousing it's also seamlessly
195:26 - integrated with both artificial
195:27 - intelligence and business intelligence
195:29 - tools for instance it works in harmony
195:31 - with Azure machine learning for AI
195:33 - purposes and leverages Azure cognitive
195:35 - services for identity and access
195:37 - management moreover it integrates
195:39 - effectively with Microsoft powerbi for
195:41 - efficient data visualiz Iz ation and
195:43 - business intelligence
195:44 - operations taking a look at this image
195:47 - it shows a simple process of azure
195:49 - synapse analytics you can ingest data
195:51 - from many data sources such as on
195:53 - premises data cloud data SAS data or
195:55 - streaming data the data is stored in
195:58 - object storage via data Lake storage Gen
196:00 - 2 you can manage Azure synapse analytics
196:03 - via the synapse studio interface you can
196:05 - output the data to various Azure
196:07 - services such as Azure purview Azure ml
196:10 - or
196:11 - powerbi let let's talk a bit about
196:13 - synaps SQL a vital component of this
196:15 - platform snaps SQL is a distributed
196:18 - version of tsql designed for data
196:19 - warehouse workloads it extends tsql to
196:23 - address streaming and machine learning
196:24 - scenarios you can use built-in streaming
196:27 - capabilities to land data from cloud
196:29 - data sources into SQL tables it
196:31 - integrates AI with SQL by using ml
196:34 - models to score data using the tsql
196:36 - predict function it offers both
196:38 - serverless and dedicated resource models
196:41 - for unpredictable workloads that are
196:43 - unplanned or bursty you can use the
196:44 - always available serverless SQL in point
196:47 - for predictable workloads create
196:49 - dedicated SQL pools to reserve
196:51 - processing power for data stored in SQL
196:54 - tables and there you have it a concise
196:56 - introduction to Azure synaps
197:02 - analytics the next topic we'll be
197:04 - covering is the dedicated SQL pool and
197:06 - serverless SQL pool both are crucial
197:09 - components in the Azure synapse
197:10 - analytics Suite but they serve distinct
197:12 - purposes let's break them down dedicated
197:15 - SQL pool is essentially a query service
197:18 - over the data in your data warehouse the
197:20 - unit of scale is an abstraction of
197:22 - compute power that is known as a data
197:23 - warehouse unit think of dwu as a measure
197:26 - of computational power ensuring your
197:28 - database operations run efficiently once
197:31 - your dedicated SQL pool is created you
197:33 - can import Big Data with simple poly
197:35 - based tsql queries and then use the
197:37 - power of the distributed query engine to
197:39 - run high performance analytics ensuring
197:41 - you derive meaningful insights from your
197:43 - data on the other hand serverless SQL
197:46 - pool is a query service over the data in
197:48 - your data Lake one of the beauties of
197:50 - the serverless Paradigm is its
197:52 - adaptability scaling is done
197:53 - automatically to accommodate each query
197:55 - resource requirements in the world of
197:58 - data change is the only constant whether
198:00 - you're adding nodes removing them or
198:02 - managing failovers the serverless SQL
198:04 - pool is resilient it constantly adapts
198:06 - to ensure every query receives the
198:08 - resources it needs guaranteeing
198:10 - successful execution a in essence if you
198:12 - want Power go with dedicated SQL pool if
198:15 - you value adaptability choose serverless
198:17 - SQL pool either way asure simplifies
198:20 - your data operations making it easy to
198:22 - manage complex
198:27 - tasks all right let's dive into Apache
198:30 - spark integration within Azure synapse
198:32 - Azure synapse can deeply and seamlessly
198:34 - integrate with Apache spark which is one
198:36 - of the most popular open-source Big Data
198:38 - engine used for data preparation data
198:41 - engineering ETL and even machine
198:43 - learning tasks it offers machine
198:45 - learning models with sparkml algorithms
198:47 - and azl integration for Apache spark 3.1
198:50 - with built-in support for Linux
198:52 - Foundation Delta Lake provides a
198:54 - simplified resource model that frees you
198:56 - from having to worry about managing
198:58 - clusters it has a rapid startup process
199:00 - along with aggressive autoscaling
199:02 - capabilities ensuring your system adapts
199:04 - swiftly to workload demands offers
199:06 - built-in support foret allowing you to
199:09 - easily incorporate your expertise and
199:10 - existing net code with in a spark
199:12 - application maximizing efficiency and
199:14 - resource
199:16 - utilization now let's talk about spark
199:18 - in conjunction with data Lake and Azure
199:20 - synapse Azure synapse removes the
199:22 - traditional technology barriers between
199:24 - using SQL and Spark together you can
199:26 - seamlessly mix and match based on your
199:28 - needs and expertise tables defined on
199:31 - files in the data Lake are seamlessly
199:33 - consumed by either spark or Hive SQL and
199:36 - Spark can directly explore and analyze
199:37 - paret CSV tsv and JSO n files stored in
199:41 - the data Lake
199:42 - and you benefit from fast and scalable
199:44 - data loading capabilities between SQL
199:46 - and Spark
199:48 - databases overall Azure setups coupled
199:50 - with Apache spark offers a Powerhouse of
199:52 - tools and capabilities making your data
199:55 - operation smoother and more
200:00 - efficient Azure synapse link is a
200:02 - feature in Azure synapse analytics that
200:04 - provides seamless integration and
200:06 - real-time analytics capabilities between
200:08 - Azure synapse analytics and operational
200:10 - data stored in Azure Cosmos DB
200:12 - it creates a tight integration between
200:14 - Azure Cosmos DB and Azure synapse
200:16 - analytics allowing users to explore and
200:18 - analyze their data with no extraction
200:20 - transformation and loading process
200:22 - required no data duplication and no
200:24 - impact on the performance of
200:25 - transactional workloads aszure synapse
200:28 - link forms a connection between Azure
200:30 - Cosmos DB's transactional database and
200:32 - Azure synapse analytics analytical
200:34 - capabilities it accomplishes this by
200:36 - creating a real-time updated columnar
200:38 - based analytical store within Cosmos DB
200:42 - benefits of azure synaps link for Azure
200:44 - Cosmos DB realtime analytics Azure
200:47 - synaps link enables realtime analytics
200:49 - by allowing direct querying of live
200:50 - operational data in Azure Cosmos DB
200:53 - without impacting its performance
200:55 - operational and analytical data cohesion
200:57 - it eliminates the need for complex ETL
200:59 - processes this simplifies the data
201:01 - architecture as it provides immediate
201:03 - and seamless access to analyze
201:05 - operational data cost efficiency Azure
201:08 - synaps link reduces costs by avoiding
201:10 - the need for additional storage and
201:12 - resources that are typically required
201:13 - for ETL processes increase productivity
201:17 - by removing the need for manual data
201:18 - extraction or synchronization processes
201:21 - Azure synaps link saves time and
201:22 - increases productivity for data
201:24 - scientists and
201:25 - developers improve data freshness with
201:28 - Azure synaps link the most upto-date
201:29 - data from Azure Cosmos DB can be
201:31 - accessed for analytics and Reporting in
201:33 - real time this ensures data freshness
201:35 - and
201:37 - accuracy so when should you use Azure
201:39 - synaps link for Azure Cosmos DB you
201:42 - should consider using Azure synaps link
201:43 - for Azure Cosmos DB when you are an
201:46 - Azure Cosmos DB user wanting to conduct
201:48 - analytics business intelligence and
201:50 - machine learning on your operational
201:51 - data you are currently running analytics
201:54 - or bi on your Azure Cosmos DB
201:56 - operational data using separate
201:57 - connectors or you are executing ETL
202:00 - processes to transfer operational data
202:02 - into a distinct analytic system in such
202:05 - cases AZ your synaps link offers a
202:06 - seamless analytics experience without
202:08 - impacting the performance of your
202:10 - transactional store however it's not
202:12 - ideal if you require traditional data
202:14 - warehouse capabilities like high
202:16 - concurrency workload management and
202:18 - persisting Aggregates across multiple
202:19 - data
202:21 - [Music]
202:24 - sources hey this is Andrew Brown from
202:27 - exam Pro and in this section we'll be
202:29 - covering Azure data Factory Azure data
202:31 - Factory is essentially a cloud-based
202:33 - managed service that plays a pivotal
202:35 - role in ETL elt and data integration
202:38 - process is it acts like a Powerhouse
202:40 - orchestrating data movement and and
202:41 - transforming data seamlessly on a large
202:43 - scale here's a breakdown you can create
202:46 - pipelines to schedule data driven
202:48 - workflows making data transformation and
202:50 - integration hassle-free build complex
202:53 - ETL processes that transform data
202:55 - visually with data flows this means you
202:57 - can transform data in a more intuitive
202:58 - and Visually appealing way use compute
203:01 - services such as Azure HD Insight Hado
203:03 - Azure data bricks and Azure SQL database
203:06 - thus offering flexibility and Power in
203:08 - how you handle your data once your data
203:11 - has undergone transformation you can
203:13 - publish your transform data to data
203:14 - stores such as Azure synapse analytics
203:16 - which can store and derive meaningful
203:18 - insights Azure data Factory has the
203:20 - ability to turn raw data into organized
203:23 - meaningful data stores and data likes
203:25 - ready for further analysis and Reporting
203:27 - so that's a quick introduction to Azure
203:29 - data
203:34 - Factory the next topic we'll be covering
203:37 - are the core components let's dive into
203:38 - each one and understand their
203:40 - significance Pipelines think of these as
203:42 - the assembly lines of your data Factory
203:44 - at pipeline is a logical grouping of
203:46 - activities that performs a unit of work
203:48 - it orchestrates and automates the flow
203:50 - of data activities these are the
203:52 - specific tasks within a pipeline an
203:54 - activity is essentially a processing
203:56 - step in a pipeline the action items or
203:58 - the work stages in your pipeline data
204:01 - sets these represent the data structures
204:03 - or the blueprints within the data store
204:05 - Link services they Define the connection
204:07 - information necessary for data Factory
204:09 - to access external resources like your
204:11 - data sources data flows data flows
204:14 - Define how data moves through a pipeline
204:16 - or under goes transformation offering a
204:18 - visual representation of data's journey
204:20 - and
204:21 - manipulations integration run times this
204:23 - is the engine under the hood the
204:25 - integration runtimes are the compute
204:26 - infrastructure used by Azure data
204:28 - Factory to facilitate the data movements
204:30 - and compute processes control flow the
204:33 - control flow orchestrates the sequence
204:35 - of activities in a pipeline it
204:36 - determines how activities are chained
204:38 - ordered or branched ensuring the
204:40 - systematic flow and process processing
204:41 - of data in a nutshell these seven
204:44 - components are the backbone of azure
204:46 - data Factory they work harmoniously
204:48 - ensuring your data is efficiently moved
204:50 - transformed and
204:55 - processed the next topic will cover in
204:58 - Azure data Factory is the data
204:59 - orchestration process data ingestion
205:02 - data is ingested from a variety of data
205:04 - sources this could be on premises SQL
205:07 - Server databases external data or any
205:09 - other supported data sources a DF
205:11 - supports a wide range of connectors that
205:13 - can be used to pull data from these
205:15 - sources data can be ingested in batch or
205:17 - realtime modes depending on the
205:19 - requirements data storage and
205:21 - transformation after data ingestion it's
205:23 - often stored in an intermediate storage
205:25 - for processing this could be Azure blob
205:27 - storage which is a scalable object
205:29 - storage for unstructured data for more
205:31 - structured analytics ready data Azure
205:33 - synapse analytics which is an analytics
205:35 - service can be used at this stage ADF
205:38 - pipelines can transform data by cleaning
205:40 - shaping and enriching using mapping data
205:42 - flows analysis once the data is ready
205:45 - it's then loaded into a data model for
205:47 - analysis this is where Azure analysis
205:49 - Services come in it allows you to build
205:51 - semantic models on your data which
205:53 - provide a Consolidated view of your
205:54 - business data and support high
205:56 - performance reporting and analytics
205:58 - visualization the data model can then be
206:00 - used by reporting tools like powerbi to
206:02 - create visualizations dashboards and
206:05 - reports that provide actionable business
206:07 - insights security and authentication
206:09 - throughout this process asure active
206:11 - directory is used for authenticating and
206:13 - authorizing users it provides identity
206:15 - and access Management Services ensuring
206:18 - that only authorized users have access
206:19 - to your resources and data and there you
206:22 - have it that's an overview of the data
206:23 - orchestration process in Azure data
206:25 - Factory from raw data to insightful
206:27 - visualizations all while being securely
206:34 - guarded the next topic we'll be covering
206:36 - is Microsoft SQL Server integration
206:39 - Services commonly known as ssis
206:42 - Microsoft SQL Server integration
206:44 - Services is a platform for building
206:45 - Enterprise level data integration and
206:47 - data transformation Solutions as sis can
206:51 - be used to automate SQL Server databases
206:53 - additionally it can be used as an
206:55 - integration runtime within Azure data
206:57 - Factory you can perform the following
206:59 - tasks with ssis copy or download files
207:02 - load data into Data warehouses cleansing
207:04 - data for better accuracy dive into Data
207:07 - Mining and managing SQL Server objects
207:09 - and data s sis can perform elt with
207:12 - variety of sources such as XML flat
207:15 - files and relational data sources s sis
207:18 - is built-in tasks and Transformations
207:20 - graphical tools for building packages
207:22 - and integration Services catalog
207:24 - database where you store run and manage
207:26 - packages you can use the graphical
207:29 - integration Services tools to integrate
207:31 - and transform data without having to
207:32 - write code s sis designer is a graphical
207:35 - tool that you can use to create and
207:37 - maintain integration Services packages
207:39 - picture a canvas where you lay out your
207:41 - data operations all with simple drag and
207:43 - drop
207:45 - functionalities here is an image that
207:46 - outlines the ssis designer in action
207:49 - with a data task flow example ssis
207:52 - allows you to drag out data
207:53 - Transformations with a variety of common
207:55 - tasks such as aggregate merge lookup and
207:58 - many more and here you can design
208:00 - different kinds of control or data flows
208:03 - so that sums up our overview on
208:04 - Microsoft SQL Server integration
208:10 - services
208:12 - next we'll quickly go through the
208:14 - pricing tiers of azure data bricks so
208:16 - you can get a better understanding of
208:17 - which one is best for your workload
208:19 - Azure data bricks offers two pricing
208:21 - tiers premium and standard here's what
208:23 - each tier brings to the table premium
208:26 - SKU the premium tier offers a full set
208:28 - of Advanced Data bricks features such as
208:30 - role-based Access Control integration
208:32 - with Azure active directory for identity
208:34 - management and datab Bricks Delta engine
208:36 - which is a high performance engine for
208:37 - large scale data Lakes it is the most
208:40 - appropriate for Big Data analytics
208:41 - workloads and organizations requiring
208:43 - Advanced security and team-based
208:45 - workflows standard SKU the standard tier
208:48 - offers a subset of the data bricks
208:50 - platform features and is more
208:51 - costeffective for smaller workloads or
208:53 - development in test environments it
208:55 - includes the basic data bricks runtime
208:57 - in the collaborative workspace but does
208:59 - not include role-based access control or
209:01 - Azure ad integration so that's an
209:04 - overview of the pricing tiers of azure
209:05 - data
209:06 - bricks hey this is Andrew Brown from
209:08 - exam Pro and in this section we're
209:10 - delving into to Azure datab bricks a
209:12 - game changer in the world of big data
209:14 - and artificial intelligence Solutions
209:16 - Azure datab bricks is an Apache
209:18 - spark-based analytics platform optimized
209:20 - for the Microsoft Azure cloud services
209:22 - platform it's designed to simplify the
209:25 - process of building big data and
209:26 - artificial intelligence solutions by
209:28 - providing a unified analytics platform
209:30 - that accelerates the preparation of data
209:32 - for analytics and machine learning key
209:35 - features collaborative environment Azure
209:37 - data bricks provides a shared workspace
209:39 - for collaboration among data Prof
209:41 - professionals it facilitates dashboard
209:43 - creation and project sharing across
209:45 - various languages Azure integration as a
209:48 - native service it integrates smoothly
209:50 - with Azure services like data Factory
209:51 - synapse analytics machine learning and
209:53 - powerbi simplifying analytics pipeline
209:56 - creation Apache spark integration it
209:59 - incorporates an optimized runtime for
210:01 - Superior performance offering a
210:02 - serverless Apache spark experience with
210:04 - Auto configurability and integrated
210:06 - Azure Security Auto scaling and
210:09 - performance it offers Auto scaling and
210:11 - speed optimization for faster processing
210:13 - of Big Data workloads scaling to
210:15 - thousands of nodes and handling diverse
210:17 - workloads robust security it delivers
210:20 - Enterprise grade security with data
210:21 - encryption active directory integration
210:24 - role base Access Control private network
210:26 - connectivity and Industry standard
210:29 - compliance use cases D data analytics
210:32 - Azure data bricks can process large
210:34 - volumes of data in parallel making it
210:36 - suitable for big data processing and
210:38 - analytics machine learning it provides
210:41 - provides a platform for preparing data
210:42 - and developing training and deploying
210:44 - machine learning models realtime
210:47 - analytics Azure data brick supports
210:48 - streaming analytics allowing for
210:50 - real-time insights from data ETL
210:53 - processes Azure data bricks can be used
210:55 - for building robust ETL pipelines data
210:58 - exploration and visualization the
211:00 - collaborative workspace in Azure data
211:01 - bricks allows for data exploration and
211:03 - visualization enabling data scientists
211:05 - and analysts to derive insights from
211:07 - data just to recap the key differences
211:10 - as your data focuses on analytics big
211:12 - data and machine learning while Azure
211:14 - data Factory primarily handles data
211:16 - integration and pipeline orchestration
211:18 - in the
211:20 - [Music]
211:27 - cloud hey this is Andrew Brown from exam
211:29 - Pro and in this but before we delve into
211:31 - the specifics of azure data Lake let's
211:33 - clarify what a data Lake actually is in
211:36 - simple terms a data lake is a
211:37 - centralized and scalable repository
211:39 - capable of storing a vast range of data
211:41 - including raw unstructured and
211:43 - semi-structured data it's designed to
211:45 - accommodate a massive volume of data
211:47 - commonly utilizing objects referred to
211:49 - as blobs or files as its primary mediums
211:52 - for storage here is a visual of a lake
211:54 - with ones and zeros representing the
211:56 - vast amount of data that it can
211:57 - accommodate the processing of this data
212:00 - can be simplified and broken down into a
212:02 - few pivotal steps collect this step
212:04 - involves Gathering data from various
212:06 - sources transform here data under goes
212:09 - Transformations or modifications through
212:11 - the usage of elt or ETL this process
212:14 - converts the raw data into a more
212:15 - organized semi-structured format ready
212:18 - for further analysis distribution this
212:20 - phase makes the transform data
212:22 - accessible to various programs or apis
212:25 - publish finally data sets are published
212:27 - to metadata cataloges making it easier
212:29 - for analysts to locate and tap into
212:31 - valuable data resources so that sums up
212:34 - a quick introduction to data lakes and
212:36 - the foundational process
212:39 - involved
212:42 - now that we have a solid grasp on what a
212:44 - data lake is let's delve into the
212:46 - specifics of azure data Lake Azure data
212:48 - lake is a highly scalable and secure
212:50 - data Lake that allows you to store and
212:52 - analyze large amounts of data it is
212:54 - composed of two main components Azure
212:56 - data Lake storage and Azure data Lake
212:58 - analytics Azure data Lake storage this
213:00 - is a massively scalable and secure data
213:02 - Lake that allows you to store all types
213:04 - of data there are two generations of
213:06 - azure data Lake storage Azure data Lake
213:08 - storage gen one it provides a single
213:10 - repository where you can capture data of
213:12 - any size type and Speed without forcing
213:15 - changes to your application as the data
213:16 - scales Azure data Lake storage Gen 2
213:19 - this is a set of capabilities dedicated
213:21 - to big data analytics built on Azure
213:23 - blob storage it combines the scalability
213:25 - and cost benefits of object storage with
213:27 - the reliability and performance of The
213:29 - Big Data file system capabilities to
213:32 - Azure data Lake analytics this is an on
213:34 - demand analytics job service that
213:36 - simplifies big data instead of deploying
213:38 - configuring and tuning Hardware you
213:40 - write queries to transform your data and
213:42 - extract valuable insights it includes
213:45 - usql a language that unifies the
213:47 - benefits of SQL with the expressive
213:49 - power of your own code it also allows
213:51 - you to dynamically scale the resources
213:52 - you need for your jobs making it coste
213:55 - effective as your data Lake storage gen
213:57 - one the first version of data Lake
213:59 - storage and will be retired in 2024 new
214:02 - users should use Gen 2 Azure data Lake
214:04 - storage Gen 2 Data Lake storage is azure
214:07 - blog storage which is has been extended
214:08 - to support big data analytics work loads
214:11 - designed to handle pedabytes of data and
214:13 - hundreds of gigabits of throughput in
214:15 - order to efficiently Access Data data
214:17 - Lake storage adds a hierarchical main
214:19 - space to Azure blob storage and there
214:21 - you have it a comprehensive overview of
214:23 - azure data
214:25 - [Music]
214:28 - like next we'll be exploring Azure data
214:31 - Lake storage Gen 2 in more detail given
214:33 - that the exam will feature questions
214:35 - asking you to identify the most suitable
214:37 - type of storage for specific scenarios
214:39 - it's essential to have a firm Gra grasp
214:40 - on which option is best suited for
214:42 - different tasks Azure data Lake storage
214:45 - Gen 2 plays a vital role in Azure data
214:47 - Lake by providing a secure scalable
214:49 - platform to store large data volumes it
214:51 - is tailored to support high performance
214:53 - analytics and machine learning
214:54 - operations here are some of its core
214:57 - features hierarchical mainspace enables
214:59 - organizing and managing data in a
215:01 - hierarchical file and folder structur
215:03 - similar to traditional file systems
215:05 - simplifying data organization and
215:07 - Transformations scalability designed to
215:10 - handle enormous amounts of data from
215:12 - pedabytes to exabytes with high
215:13 - throughput and low latency it can also
215:15 - manage High volumes of small rights
215:17 - common in Big Data scenarios security
215:20 - incorporates Azure active directory for
215:22 - identity and access management
215:24 - role-based Access Control firewall rules
215:26 - and virtual network service in points
215:28 - along with encryption at rest and and
215:30 - Transit it also supports Azure private
215:32 - link ensuring data travels over a
215:34 - private Network cost Effectiveness
215:37 - offers lowcost storage with life cycle
215:39 - management policies that a automatically
215:41 - move data to cheaper storage tiers or
215:42 - deleted after a specified period
215:44 - lowering costs performance provides the
215:47 - high performance Computing needed for
215:49 - big data analytics and allows choosing
215:51 - the best performance characteristics for
215:53 - specific workloads through its
215:54 - performance tiers overall Azure data
215:57 - Lake storage Gen 2 is an extended
215:59 - version of azure blob storage designed
216:00 - for big data analytics providing
216:02 - additional capabilities like
216:04 - hierarchical file systems and fine
216:05 - grained Access
216:09 - Control
216:12 - the next topic we'll be covering is the
216:13 - second core component to Azure data Lake
216:16 - which is azure data Lake analytics Azure
216:18 - data Lake analytics is an on demand
216:20 - analytics job service streamlining the
216:22 - complexities of Big Data operations
216:24 - instead of going through the cumbersome
216:26 - processes deploying configuring and
216:28 - tuning Hardware all you write queries
216:30 - using usql to transform your data and
216:32 - extract valuable insights effortlessly
216:35 - to illustrate its efficiency exporting
216:37 - approximately 2.8 billion rows of TP CDs
216:40 - store sales data which is around 500 GB
216:43 - into a CSV format file took less than 7
216:45 - minutes and importing a full 1 terab set
216:48 - of source data into Azure analysis
216:50 - Services by using the Azure data Lake
216:52 - connector took less than 6 hours now
216:55 - let's talk a bit about usql usql is a
216:58 - structured query language included
216:59 - within data Lake analytics to perform
217:01 - queries on your data Lake it's versatile
217:04 - allowing you to query and combine data
217:06 - from a variety of data sources including
217:08 - Azure data Lake storage Azure blob
217:10 - storage Azure sqldb Azure SQL data
217:13 - warehouse and even SQL Server instances
217:15 - running in Azure VMS for those who like
217:17 - Hands-On experiences you can install
217:19 - Azure data Lake tools for visual studio
217:21 - to perform usql jobs on your Azure data
217:24 - lake so that's an overview of azure data
217:27 - Lake
217:29 - [Music]
217:32 - analytics hey this is Andrew Brown from
217:34 - exam Pro and in this section we'll be
217:36 - covering is Cosmos DB but before we talk
217:38 - about Cosmos DB it's important to
217:41 - understand the key types of nosql
217:43 - databases first up let's talk about key
217:45 - value stores key value stores are simple
217:48 - and fast but they generally lack
217:50 - features like relationships indexes and
217:53 - aggregation in a key Value Store data is
217:55 - stored in pairs where a unique key is
217:57 - stored alongside a corresponding value a
218:00 - simple key Value Store will interpret
218:02 - this data resembling a dictionary also
218:04 - known as associative arrays or hash
218:06 - although this kind of data storage can
218:08 - resemble tabular data it doesn't require
218:10 - consistent columns per row making it
218:12 - schema less due to their simple design
218:15 - they can scale well beyond a relational
218:17 - database next let's explore what a
218:19 - document store is a document store is a
218:22 - nosql database that stores documents as
218:24 - its primary data structure these
218:27 - documents can be structured as XML but
218:29 - are more commonly found in JSO or
218:31 - similar formats essentially document
218:33 - stores are a subclass of key value
218:35 - stores but have a more intricate
218:37 - structure allowing for complex queries
218:39 - and operations here's a visual that
218:41 - displays the components of a document
218:42 - store compared to a relational
218:45 - database now let's move on to
218:48 - understanding graph databases a graph
218:50 - database is a database composed of a
218:52 - data structure that uses vertices also
218:54 - known as nodes or dots which form
218:56 - relationship to other vertices through
218:58 - edges arcs or lines this type of
219:00 - database is particularly powerful for
219:02 - mapping relationships and identifying
219:04 - patterns use cases for graph database
219:06 - fraud detection real-time recommendation
219:09 - engines Master data management Network
219:11 - and and operations identity and access
219:13 - management traceability and
219:14 - Manufacturing contact tracing data
219:17 - lineage for gdpr customer 360 analysis
219:21 - product recommendations social media
219:23 - graphing and feature engineering the
219:25 - nodes can contain data properties while
219:28 - the edges can hold relational data
219:30 - including directional information and
219:31 - other data properties so that's an
219:34 - overview of three key types of nosql
219:39 - databases
219:42 - now that we have a better understanding
219:44 - of the key types of nosql databases it's
219:46 - time to delve into the main topic Cosmos
219:48 - DB Azure Cosmos DB is a service for
219:51 - fully managed nosql databases that are
219:53 - designed to scale and have high
219:55 - performance attributes Cosmos TB
219:57 - facilitates interaction with different
219:59 - types of nosql database engines through
220:01 - distinct apis namely core SQL a document
220:05 - data store as your Cosmos dbapi for
220:08 - mongodb another document data store
220:10 - Azure table a key value data store
220:13 - Gremlin a graph data store based on
220:15 - aache Tinker pop these no SQL engines
220:18 - offer two specific capacities provision
220:20 - throughput where you pay for a
220:21 - guaranteed capacity and serverless where
220:23 - you pay only for what you use Cosmos DB
220:27 - shines with its incredibly quick
220:28 - response times and solid support for
220:30 - scalability it's a fully managed service
220:32 - meaning Azure takes charge of all
220:34 - automatic management updates and patches
220:37 - main advantages of azure Cosmos TB into
220:40 - integrates with many Azure services
220:41 - including Azure functions Azure
220:43 - kubernetes services and Azure app
220:45 - services integrates with many databases
220:48 - apis like the native core SQL mongodb
220:51 - Cassandra and Gremlin support for
220:53 - multiple development sdks spanning
220:55 - Donnet Java Python and
220:58 - nodejs offers a schema less service with
221:00 - automatic indexing of data ensuring
221:02 - rapid queries guaranteed uptime SLA of
221:06 - 99.999% availability data replication
221:09 - between Azure regions is automatic data
221:12 - protected with encryption at rest and
221:14 - ro-based access and autoscale is
221:16 - provided to handle a variety of workload
221:19 - sizes next let's talk about Cosmos DB
221:22 - apis different types of AIS are
221:24 - available in Azure Cosmos DB to support
221:26 - a wider range of applications these apis
221:29 - allow data to be delivered via documents
221:31 - key value pairs y columns or graph data
221:34 - for new projects the core SQL apis are
221:37 - strongly recommended whereas for
221:39 - existing databases is the specific
221:40 - database API is recommended the apis are
221:43 - as follows for SQL API the default API
221:46 - for utilizing Azure Cosmos DB and
221:48 - enables data querying with a language
221:50 - akin to SQL mongod DB API facilitates
221:54 - communication with mongod DB databases
221:56 - and document storage Cassandra API
221:59 - allows interaction with Cassandra using
222:01 - the Cassandra query language and
222:02 - supports data storage as a partition row
222:04 - store Azure table API a communication
222:07 - tool for Azure table storage it supports
222:09 - it indexing in the partition and row
222:11 - Keys Gremlin API helps in creating a
222:14 - graph-based data view that can be quered
222:16 - using graph traversal language so that's
222:18 - an introduction to Cosmos
222:24 - DB next up on our agenda is the cosmos
222:27 - DB Explorer Cosmos DB Explorer provides
222:30 - a userfriendly web interface that allows
222:32 - users to delve into an engage with their
222:34 - Cosmos DB accounts you can readily
222:36 - access this interface by heading to
222:38 - cosmos. azure.com
222:40 - here is a cosmos DB core SQL within
222:42 - Cosmos DB Explorer adding a document to
222:44 - the database which is very
222:46 - straightforward it's worth noting that
222:48 - when you journey through Azure to access
222:49 - a cosmos DB account under the data
222:51 - Explorer section you're essentially
222:53 - interacting with the same interface as
222:55 - cosmos DB Explorer furthermore the
222:58 - versatility of Cosmos DB Explorer shines
223:00 - through with its compatibility with
223:01 - other database types for example it's
223:04 - seamless integration with a graph
223:05 - database using Gremlin so that's a quick
223:08 - overview of Cosmos DB
223:14 - Explorer the next topic we'll be
223:16 - covering is partitioning schemas in
223:18 - Cosmos DB partitioning in Azure Cosmos
223:21 - DB plays a pivotal role in optimizing
223:23 - performance data in Cosmos DB indexes is
223:25 - strategically grouped by partition Keys
223:28 - ensuring quick and efficient data access
223:30 - main concepts of partitioning schemas in
223:32 - Azure Cosmos DB partition Keys think of
223:35 - these as the backbone of partitioning
223:37 - they are the keys used to group items
223:39 - together and can be like lik to primary
223:40 - keys in relational databases essentially
223:43 - they dictate how data is divided and
223:44 - organized within the system a logical
223:47 - partition is a group of items that all
223:49 - have the same partition key value
223:51 - physical partitions consists of a set of
223:53 - logical partitions Azure Cosmos DB
223:56 - manages logical partitions which can
223:58 - have one to many replica sets are made
224:00 - up of a group of physical partitions
224:02 - that are materialized as a self-managed
224:04 - dynamically load balanced group of
224:06 - replicas that span across multiple fault
224:08 - domains each physical partition is not
224:11 - alone it has a set of clones or replicas
224:13 - this set of replicas is what we refer to
224:15 - as a replica set replicas ensure data
224:18 - durability availability and
224:20 - consistency logical partitions are
224:22 - mapped to physical partitions and these
224:24 - physical entities are then spread
224:25 - globally ensuring data availability and
224:27 - low latency access across regions to
224:30 - clarify the term in the image a
224:32 - partition set refers to a collection of
224:34 - physical partitions these partitions
224:36 - collectively manage the same logical
224:38 - partition keys and they Ensure this
224:40 - consistent management across multiple
224:42 - regions in essence Cosmos DB's
224:44 - partitioning architecture is
224:45 - meticulously designed for scalability
224:47 - fault tolerance and Optimal Performance
224:50 - whether you're considering logical or
224:51 - physical partitions replica sets or
224:54 - partition Keys each plays a crucial role
224:56 - in ensuring that Cosmos DB remains one
224:58 - of the most efficient nosql databases on
225:00 - the
225:05 - market the next topic we'll be covering
225:07 - is choosing a partition key at partition
225:10 - key has two components partition key
225:12 - path and the partition key value for
225:14 - example you can consider an item user ID
225:16 - Andrew works for Microsoft if you choose
225:19 - user ID as the partition key the
225:21 - following are the two partition key
225:22 - components while in Partition key path
225:25 - this would be user ID this path
225:27 - essentially points to the property and
225:29 - your data item that holds the partition
225:30 - key value alpha numeric characters and
225:33 - underscores are accepted and you can
225:34 - navigate through nested objects using
225:36 - the standard path notation to partition
225:39 - key key value for our example this is
225:41 - Andrew it's the specific value found at
225:43 - the partition key path this value can be
225:46 - either a string or a number your
225:48 - partition key for all containers should
225:49 - be a property that has a value which
225:51 - does not change you can't change the
225:53 - value of a property if it's your
225:54 - partition key the partition key should
225:56 - have a wide range of possible values to
225:58 - distribute data and workload uniformly
226:00 - across various logical partitions spread
226:03 - request unit consumption and data
226:05 - storage evenly across all logical
226:06 - partitions this ensures even rot
226:08 - consumption and Storage distribution
226:10 - across your physical
226:12 - partitions let's talk about unique keys
226:15 - so unique Keys provide developers with
226:17 - the ability to add a layer of data
226:18 - Integrity to their database by creating
226:20 - a unique key policy when a container is
226:22 - created you ensure the uniqueness of one
226:24 - or more values per partition key a
226:27 - unique key is scoped to a logical
226:28 - partition if you partition the container
226:30 - based on the zip code you end up with
226:32 - duplicated items in each logical
226:34 - partition it's important to know that
226:36 - you can't update an existing container
226:38 - to use a different unique key
226:40 - a unique key policy can have a maximum
226:42 - of 16 path values each unique key policy
226:45 - can have a maximum of 10 unique key
226:46 - constraints or combination when a
226:48 - container has a unique key policy
226:50 - request unit charges to create update
226:52 - and delete an item are slightly higher
226:54 - in addition unique key names are case
227:01 - sensitive the next topic we'll be
227:03 - covering our containers in Cosmos DB
227:05 - Azure Cosmos containers are useful for
227:07 - scalability in Azure Cosmos DB both in
227:10 - terms of storage and throughput they are
227:12 - beneficial when you need a different set
227:13 - of configurations for each of your Azure
227:15 - Cosmos DBS because they allow you to
227:17 - customize each container individually
227:19 - some applications may require robust WR
227:21 - capabilities for logging While others
227:23 - prioritize reading due to data access
227:25 - needs with Cosmos DB containers each
227:28 - application can have a customized
227:29 - container that meets its demands
227:31 - balancing performance and cost asure
227:33 - Cosmos container has some container
227:35 - specific properties and those properties
227:37 - which can be system generated or user
227:39 - configurable vary according on the used
227:42 - API and Azure Cosmos container has a set
227:44 - of system defined properties depending
227:47 - on which API you use some properties
227:49 - might not be directly exposed the table
227:51 - lists various system defined properties
227:53 - in Cosmos DB and indicates which ones
227:55 - are system generated versus user
227:57 - configurable additionally it also
227:59 - denotes which properties are used by
228:01 - different apis for nosql Cassandra manga
228:04 - B Gremlin and table rid type system
228:07 - generated purpose acts as a unique
228:09 - identifier for a container supported by
228:11 - only the API for no SQL eag type system
228:15 - generated purpose utilized for
228:17 - optimistic concurrency control it
228:19 - ensures that only one client can change
228:21 - an item in the database at a time
228:23 - supported by only the API for no SQL TS
228:26 - type system generated purpose represents
228:29 - the timestamp when the container was
228:31 - last updated supported by only the API
228:33 - for nosql self type system generated
228:37 - purpose provides an addressable Yuri of
228:39 - the the container it's essentially a
228:40 - unique reference or link to the
228:42 - container supported by only the API for
228:45 - nosql ID type user configurable purpose
228:48 - denotes the name of the container
228:50 - supported by all the apis listed there
228:53 - are many more properties but we won't be
228:55 - able to list them all
229:00 - here the next topic we'll be covering
229:02 - are the capacity of Cosmos DB containers
229:05 - so what is capacity capacity defines the
229:08 - amount of underlying resources are
229:09 - available to support consumption of
229:11 - resources such as compute and storage as
229:14 - we've briefly touched upon before Cosmos
229:16 - dbs2 capacity modes provision throughput
229:19 - and serverless provision throughput in
229:22 - this mode you allocate a specific amount
229:23 - of throughput for your containers this
229:25 - throughput is Quantified in terms of
229:27 - request units per second this mode is
229:30 - suitable for workloads where traffic can
229:31 - be predicted it offers a high degree of
229:33 - flexibility letting you match your
229:35 - provision capacity to the expected
229:37 - demand serverless this mode is the
229:39 - opposite in its approach instead of
229:41 - provisioning in advance you simply run
229:43 - database operations without setting any
229:45 - predetermined capacity this mode is
229:47 - beneficial for smaller workloads or
229:49 - those that might experience
229:50 - unpredictable traffic spikes while it
229:52 - offers the advantage of Simplicity and
229:54 - configuration there are some inherent
229:56 - limitations to be aware of when it comes
229:59 - to geod distribution the provision
230:01 - throughput option in Cosmos DB offers
230:03 - unlimited multi- region support in
230:05 - contrast the serverless option is
230:07 - restricted to a single region in terms
230:09 - of storage capacity while provision
230:11 - throughput allows for unlimited storage
230:13 - per container serverless is Cap at 50 GB
230:16 - performance- wise both options deliver
230:18 - less than 10 ms latency for Point reads
230:21 - however while provision throughput
230:22 - guarantees less than 10 ms latency for
230:24 - rights under its SLA serverless offers
230:26 - less than 30 MS for rights as covered by
230:29 - its SLO regarding billing provision
230:31 - throughput charges per hour based on the
230:33 - set Ru s irrespective of actual R
230:36 - consumption on the other hand serverless
230:38 - charges are based on the actual Ru as
230:40 - consumed by your operations build on an
230:42 - hourly
230:44 - [Music]
230:47 - basis next we'll delve into the
230:49 - consistency levels in Cosmos DB these
230:52 - levels play a pivotal role shaping the
230:54 - availability latency and accuracy of
230:56 - database operations aure Cosmos DB
230:59 - provides five different consistency
231:00 - levels to maintain data availability and
231:03 - quering performance depending on your
231:05 - requirements the consistency levels in
231:07 - Cosmos DB can be visualized as the
231:09 - Spectrum this ranges from strong to
231:11 - eventual on the strong side it has
231:13 - higher latency lower availability but
231:15 - has worse read scalability but as you
231:17 - move towards eventual you gain lower
231:19 - legancy higher availability and better
231:21 - read scalability strong linearizability
231:24 - reads are guarantee to return the most
231:26 - recent version of an item bowed
231:28 - staleness consistent prefix reads lag
231:30 - behind writes by at most K prefixes or t
231:32 - interval session consistent prefix
231:35 - medonic reads medonic rights read your
231:38 - rights WR follows read reads consistent
231:40 - prefix updates returned or some prefix
231:42 - of all the updates with no gaps eventual
231:45 - outof order reads you set default
231:48 - consistently at the cosmo B account
231:49 - Level under default consistency
231:51 - blade strong consistency this level
231:54 - guarantees that read operations return
231:56 - the most recent data while it's Rec cost
231:58 - align with bounded tailess they are
231:59 - higher than those of session and
232:01 - eventual consistencies furthermore data
232:03 - written can only be read once the
232:04 - majority of replicas have successfully
232:06 - replicated it now it stailness causes re
232:09 - operations to lack behind rights due to
232:11 - time or version disparities despite
232:13 - having the same re cost as strong
232:15 - consistency it's pricier than session
232:17 - and eventual consistencies it stands out
232:19 - as the most consistent when compared to
232:21 - session consistent prefix and eventual
232:24 - this level is ideal for globally
232:25 - distributed applications that prioritize
232:27 - High availability and minimal
232:29 - latency session session consistency
232:32 - ensures that data read within a session
232:34 - matches the most recent write in that
232:36 - session however other sessions might see
232:38 - outdated or dirty data from recent wrs
232:40 - in different sessions it's the default
232:42 - consistency for new databases its Rec
232:44 - cost fall between those of bounded stal
232:46 - less strong and eventual
232:48 - consistencies with consistent prefix
232:50 - read operations fetch the latest data
232:52 - replicated among replicas although it
232:54 - may not be the absolute latest
232:56 - situations can arise where dirty data
232:58 - appears due to changes in one replica
233:00 - that haven't propagated to others its
233:02 - consistency is superior to eventual but
233:04 - Trails behind other levels eventual
233:06 - consistency offers the least Assurance
233:08 - with no guarantees on immediate data
233:10 - accuracy however it boasts the lowest
233:12 - latency Optimal Performance and the most
233:15 - cost-effective read operations among all
233:17 - levels so that's an overview of the
233:19 - consistency levels in Cosmos
233:22 - [Music]
233:25 - DB hey this is Andrew Brown from exam
233:28 - Pro and in this section we'll be
233:29 - covering Azure rid's cach but before we
233:32 - talk about Azure ridy cache we'll need
233:34 - to know what ridy is so ridy is an open
233:37 - source and memory database store Ries
233:39 - accs as caching layer or a very fast
233:41 - database since all data is stored in
233:43 - memory it's highly volatile meaning data
233:45 - loss is possible under certain
233:47 - conditions rides is very fast that it
233:49 - can deliver content from its store with
233:50 - single to double digit milliseconds such
233:52 - as 10ms despite its imemory nature
233:55 - ridies provides options for data
233:57 - persistence allowing you to balance
233:59 - performance with durability it can
234:01 - periodically save data snapshots to dis
234:03 - or append each command to AOG red is a
234:06 - key value store and it supports the
234:08 - following data structures sets and
234:10 - sorted sets collections of strings in
234:12 - which every item is unique lists a
234:14 - collection of strings sorted according
234:16 - to the order they were inserted hashes
234:18 - perfect for story objects these are maps
234:20 - between string fields and string values
234:23 - bit maps and bit Fields allow for
234:25 - operations at the bit level providing
234:27 - extremely efficient storage hyper log
234:29 - log a sophisticated algorithm to count
234:31 - unique values geospatial indexes Grant
234:34 - the ability to manage spatial items in
234:36 - query by position streams new in rides
234:39 - it offers a way to log in realtime
234:41 - stream data so that's an introduction of
234:48 - rid's the next topic we'll be covering
234:50 - is azure cash for rides Azure rid cash
234:53 - is based on the popular op Source rid's
234:55 - cache it gives you access to a secure
234:57 - dedicated rid's cache that Microsoft
234:59 - manages and that you can access from any
235:01 - Azure application Azure R's caches an in
235:04 - memory database that caches data in key
235:06 - value pairs it helps your application
235:08 - become more responsive even as the
235:09 - customer load increases it takes
235:12 - advantage of the redu engines low
235:13 - latency High througha capabilities this
235:16 - distributed cash layer allows your data
235:18 - tier to scale independently allowing for
235:20 - more efficient use of your application
235:21 - layers compute
235:23 - resources Azure red cash perfectly
235:25 - complements Azure database services such
235:27 - as cosmos DB and Azure SQL it provides a
235:30 - cost-effective solution to scale read
235:32 - and write throughput of your data tier
235:34 - using the caches side pattern you can
235:36 - store and share database query results
235:38 - session States static content and more
235:41 - Azure cash for stores session State and
235:44 - other data that needs low latency access
235:47 - diagram of cash aside pattern on Azure
235:49 - storage first we'll need to check the
235:51 - rid's cach to see if your item is
235:53 - available if the item is found we'll
235:55 - retrieve it if the item is not found
235:57 - we'll pull the item from the table
235:59 - storage recash it in redas and then
236:01 - return the results on the right we have
236:03 - an image that should clearly explain the
236:05 - workflow overview check if cash exists
236:08 - if true go to step two and obtain the
236:10 - data if cash could not be found go to
236:12 - step three and recash the item and
236:14 - return the results so that's a summary
236:16 - of azure cash redes and how it
236:22 - works the next topic will be covering
236:25 - our caching expiration policies for
236:27 - Azure rid's cach Azure rid's caches
236:29 - expiration policies are configured per
236:31 - each request so we could have an
236:33 - expiration policy different for each
236:34 - cach key for rid's distributed cach
236:37 - those expiration policies are as follows
236:40 - absolute expiration relative to now this
236:42 - policy sets the duration for the cash to
236:44 - live counting from the moment the item
236:45 - is added to the cash for example if you
236:48 - set this to 10 minutes the cash item
236:50 - will expire 10 minutes after it has been
236:52 - added absolute expiration this policy
236:55 - sets a specific date and time when the
236:56 - cash item will expire once that date and
236:59 - time are reached the cash item is
237:00 - evicted regardless of when it was added
237:03 - sliding expiration this policy defines
237:05 - an expiration time relative to the last
237:07 - access time of the particular cach item
237:09 - If an item has a slighty expiration of
237:11 - 10 minutes it will expire if it hasn't
237:13 - been accessed in those 10 minutes every
237:15 - time the item is accessed its expiration
237:17 - timer is reset so that's a brief summary
237:20 - of the Azure rid's caches expiration
237:24 - [Music]
237:27 - policies hey this is Andrew Brown from
237:30 - exam Pro and in this section we'll be
237:31 - covering non-relational data storage
237:33 - Solutions in Azure starting with Azure
237:35 - table storage Azure table storage isn't
237:38 - your regular relational database instead
237:40 - it's a nosql data store for
237:42 - semi-structured data needly housed
237:44 - within Azure storage accounts what makes
237:46 - it particularly appealing especially in
237:48 - today's fast-paced Tech world is its
237:50 - ability to handle vast amounts of
237:52 - unstructured or semi-structured data
237:53 - without being tied down by a fixed
237:55 - schema there are two ways to interact
237:57 - with Azure tables the first is azure
238:00 - table storage API a robust tool for
238:02 - developers to seamlessly integrate query
238:04 - and manage their data it provides you
238:06 - the flexibility to interact
238:07 - programmatically and customize according
238:09 - to your application's needs on the other
238:12 - hand we have Microsoft Azure storage
238:14 - Explorer it offers a userfriendly
238:16 - interface to browse manipulate and
238:18 - manage your data without writing a
238:19 - single line of code think of it as your
238:21 - visual window into the world of azure
238:24 - tables here is a visual of azure storage
238:26 - Explorer a standalone app that makes it
238:28 - easy to work with Azure storage data on
238:30 - Windows Mac OS and Linux you can create
238:33 - blob containers upload files create
238:35 - snapshots of disk and more so when
238:38 - should you use Azure table storage large
238:41 - amounts of semi-structured data if you
238:42 - have massive volumes of semi-structured
238:44 - or unstructured data but don't need the
238:46 - complexities of a relational database
238:49 - costeffective storage Azure table
238:50 - storage is a budget friendly solution
238:52 - especially when compared to full-fledged
238:54 - databases it's suitable for projects
238:56 - where lowcost storage is a priority
238:59 - scheme of flexibility if your
239:00 - application has evolving data structures
239:02 - where the schema can change over time
239:04 - the schema less design of azure table
239:06 - storage offers flexibility scalability
239:09 - requirements for applications that need
239:11 - to scale out by adding more data
239:12 - entities but don't require complex joins
239:14 - stored procedures or secondary indexes
239:17 - and fast access and high throughput if
239:19 - you need a storage solution with low
239:21 - latency and high availability
239:22 - characteristics for quick access to data
239:25 - so that's an overview of azure table
239:31 - storage the next topic we'll be covering
239:34 - is adding entries in Azure table storage
239:36 - when you enter data you must provide a
239:39 - partition key this is a unique
239:40 - identifier for each partition within a
239:42 - table this key enables Azure to
239:44 - distribute table data across multiple
239:46 - nodes for improved data access and load
239:48 - balancing for instance you might use the
239:50 - date and time as a partition key for log
239:52 - data ensuring entries are evenly
239:54 - distributed and easily sorted row key
239:57 - this key is a unique identifier within a
239:59 - partition allowing you to pinpoint a
240:01 - specific entity it's like the address of
240:03 - your data within the partition for
240:04 - instance in a list of customers a
240:06 - customer ID could serve as a r key Azure
240:09 - table supports a diverse set of data
240:11 - types string textual data such as names
240:14 - or addresses Boolean true or false
240:16 - values binary data such as file or image
240:20 - date time specific date and time
240:22 - information double floating Point
240:24 - numbers get Global unique identifiers in
240:27 - 32 and in 64 whole numbers both small
240:30 - and large when you need to retrieve data
240:33 - you can perform queries using both
240:35 - partition and row keys this dual Key
240:37 - System allows for Rob in flexible data
240:39 - retrieval operations for example if
240:41 - you're looking for a specific
240:42 - transaction in a financial database you
240:45 - can use the partition key to narrow down
240:46 - the date and the row key to find the
240:48 - exact transaction Azure table storage
240:51 - allows you to apply additional filters
240:53 - to your queries if you need to find all
240:55 - entries from a specific location or all
240:57 - entries falling under a particular
240:58 - category you can easily set up filters
241:00 - to refine your search so that's a quick
241:03 - overview of adding entries in Azure
241:05 - table
241:07 - storage
241:10 - the next topic we'll be covering is
241:12 - azure Q storage Azure Q storage is a
241:15 - robust and straightforward messaging
241:16 - broker that facilitates smooth and
241:18 - secure message exchange between various
241:20 - applications and services within the
241:22 - Azure environment key features simple
241:25 - message broker Azure Q storage allows
241:27 - Services running on cloud infrastructure
241:29 - to communicate with each other
241:30 - asynchronously it can handle large
241:32 - numbers of messages simultaneously
241:34 - ensuring your services remain highly
241:36 - responsive security Azure Q storage uses
241:39 - authenticated HTTP as protocols ensuring
241:42 - that data transmission is secure and
241:44 - reliable message size it can hold
241:46 - messages up to 64 kilobytes in size
241:49 - accommodating a wide range of data types
241:51 - and
241:51 - sizes for storage and access storage
241:55 - account Q storage is stored within an
241:57 - Azure storage account ensuring a secure
241:59 - and unified setting for All Storage
242:00 - needs access keys and connection strings
242:03 - utilize the same axis keys and
242:05 - connection strings for Q storage as with
242:07 - other resources in the storage account
242:09 - ensuring streamlined and consistent
242:10 - access management Azure Q storage offers
242:13 - three ways of handling messages on the
242:15 - que Peak this option allows you to
242:17 - preview a message in the queue without
242:19 - deleting or locking it it's useful for
242:21 - determining the next message to process
242:24 - delete after successfully processing a
242:26 - message an application will typically
242:28 - delete the message to ensure it isn't
242:29 - processed again receive and lock by
242:32 - locking a message it ensures that other
242:34 - parts of the system can process the
242:35 - message simultaneously after process ing
242:38 - the lock is either renewed or the
242:39 - message is
242:41 - deleted Azure portal easily create a
242:43 - Quee and send messages through the
242:45 - userfriendly interface of the Azure
242:46 - portal Azure SDK or CLI for more
242:49 - programmatic control most interactions
242:51 - with the queue including sending peing
242:54 - and deleting messages can be performed
242:56 - using the Azure SDK or command line
242:58 - interface here is a python example to
243:00 - help you understand what this looks like
243:02 - developers can create cues add messages
243:04 - and process them with just a few lines
243:06 - of code so that's an overview of azure Q
243:14 - storage the next topic will be covering
243:16 - are the key Concepts in Azure Q storage
243:19 - cues can be accessed by using the
243:20 - following URL format https storage
243:23 - account. q.c. windows.net Q the
243:27 - following URL addresses a Que in the
243:29 - diagram https by account. q.c cor.
243:33 - windows.net images to download here's a
243:36 - breakdown of the components store
243:38 - storage account a storage account is
243:39 - required for all Azure storage access
243:42 - think of it as your gateway to Azure
243:43 - storage Services q a q contains a set of
243:46 - messages keep in mind the Q name must be
243:49 - all lowercase message you can store any
243:51 - format of data in a message but it must
243:53 - not exceeds 64 Koby if for version 2017
243:57 - 0729 the maximum time to live allow is 7
244:00 - days for version 2017 0729 or later the
244:04 - maximum time to live can be any positive
244:06 - number or minus one indic that the
244:08 - message doesn't expire in addition the
244:10 - default time to live is 7 days if this
244:12 - parameter is not
244:14 - specified you can interact with the
244:16 - queue via the Azure CLI subcommand a Z
244:18 - Storage message action clear deletes all
244:21 - messages from the specified que delete
244:23 - deletes the specified message get
244:26 - retrieves one or more messages from the
244:27 - front of the queue peik retrieves one or
244:30 - more messages from the front of the
244:31 - queue but does not alter the visibility
244:33 - of the message put adds a new message to
244:36 - the back of the message queue update
244:38 - updates the visibility timeout of a
244:40 - message so that's an overview of the key
244:42 - Concepts in Azure Q
244:45 - [Music]
244:48 - storage hey this is Andrew Brown from
244:51 - exam Pro and in this segment we'll be
244:52 - covering Azure files Azure files is a
244:55 - fully managed file share in the cloud a
244:57 - file share is a centralized server for
244:59 - storage that allows multiple connections
245:01 - it's like having one big share drive
245:03 - that everyone or in this example
245:05 - multiple virtual machines can work on at
245:07 - the same time to connect to the file
245:09 - share you can use Network protocols like
245:11 - the server message block or network file
245:13 - system when a connection is established
245:15 - the file shares file system will be
245:17 - accessible in the specific directory
245:19 - within your own directory tree this
245:21 - process is known as
245:22 - mounting backups you shared snapshots to
245:25 - backup your file share these snapshots
245:27 - are readed only and incremental meaning
245:29 - they only contain data that has changed
245:31 - since the last snapshot you could have a
245:33 - maximum of 200 snapshots per file share
245:35 - and these can be retained for a whopping
245:37 - 10 years remember backups are stored
245:39 - within your file share if you delete the
245:41 - share you say goodbye to the backups as
245:43 - well soft delete you can prevent
245:45 - accidental deletion by turning on soft
245:47 - delete with soft delete your data isn't
245:49 - immediately removed instead it's marked
245:52 - for deletion and held for a certain
245:53 - period before it's permanently erased
245:56 - Advanced threat protection an additional
245:57 - layer of security intelligence that
245:59 - provides alerts when it detects
246:01 - suspicious activity on your storage
246:02 - account store tiers premium store on to
246:05 - SSD with singled digigit milliseconds
246:07 - for most EO operations transaction
246:10 - optimized store on HDD with transaction
246:12 - heavy workloads that don't need the
246:13 - latency offered by premium file shares
246:16 - hot optimized for general purpose file
246:18 - sharing scenarios such as team shares
246:20 - and Azure file sync cool stored on HDD
246:23 - for cost efficient storage optimized for
246:25 - online archade storage
246:27 - scenarios types of storage general
246:29 - purpose version 2 deployed onto HDD file
246:32 - storage deployed on the SSD identity on
246:35 - premises Azure storage can integrate
246:37 - with an on- premises active directory
246:39 - domain service managed Azure storage can
246:41 - be joined to Microsoft managed active
246:43 - directory domain service store account
246:45 - key use a combination of storage account
246:47 - name as the username and the account key
246:49 - as the password for mounting networking
246:52 - a aure files can be access from anywhere
246:54 - both inside and outside your Azure
246:56 - account through the storage accounts
246:57 - public in point remember SMB uses Port
247:01 - 445 if you face connection issues check
247:03 - if this port is open in your
247:05 - organization to mount your file share
247:07 - encryption Azure files is encrypted at
247:09 - rest using Azure storage service
247:11 - encryption Azure files is encrypted and
247:13 - Transit with SMB 3.0 plus with
247:16 - encryption or
247:17 - https and there we have it a
247:19 - comprehensive look into Azure files and
247:21 - its noteworthy
247:26 - features continuing from our discussion
247:28 - on Azure files let's now explore its
247:31 - main use cases use cases for Azure files
247:34 - completely replace or supplement on
247:36 - premises file servers Network network
247:37 - attached storage devices lift and shift
247:40 - your on premises storage to the cloud
247:42 - via classic lift or hybrid lift lift and
247:44 - shift means when you move workloads
247:45 - without rear tting for example you can
247:48 - directly import your local VMS to the
247:50 - clab classic lift both the application
247:52 - and its data are moved to Azure hybrid
247:54 - lift here only the application data gets
247:56 - moved to Azure files while the
247:58 - application remains operational on
248:00 - premises simplify Cloud development
248:03 - shared application settings multiple VMS
248:05 - and developer workstations need to
248:07 - access the same configuration files
248:09 - diagnostic share all VMS log to the file
248:11 - share developers can M and debug all
248:14 - logs in a centralized Place Dev test
248:16 - debug developers can quickly share
248:18 - essential tools needed for local
248:20 - environments containerization you can
248:22 - use Azure files to persist volumes for
248:24 - stateful containers while use Azure
248:27 - files instead of setting up your own
248:28 - file share server shared access Azure
248:31 - files is preconfigured to function with
248:33 - standard networking protocols like SMB
248:35 - and NFS fully managed Azure manages
248:37 - maintenance and security patches to
248:39 - ensure your file storage is secure and
248:41 - updated scripting and tooling you can
248:43 - automate the management and creation of
248:44 - files shared with Azure API and
248:46 - Powershell resiliency Azure files is
248:48 - built for durability ensuring that your
248:50 - data is always safe and accessible so
248:53 - these are the main reasons and scenarios
248:55 - for incorporating Azure files into your
249:02 - infrastructure the next topic we'll be
249:04 - covering is azure file sync Azure file
249:06 - sync is a service that it allows you to
249:08 - Cache azzure file shares on an on
249:09 - premises Windows server or Cloud VM how
249:12 - does it work caching Azure file sync
249:14 - transforms your Windows Server into a
249:16 - quick cach of your Azure file share this
249:18 - means that you can access the data you
249:20 - need faster than ever without having to
249:22 - rely on the cloud multiple protocols
249:24 - regardless of the protocol you're
249:26 - comfortable with be it SMB NFS or ftps
249:29 - aure file sync lets you access your data
249:31 - locally granting greater flexibility
249:34 - Global caches for businesses operating
249:36 - globally as your file s Sy allows
249:37 - setting up caches in multiple locations
249:39 - for faster data access key benefits
249:43 - centralized storage with Azure file sync
249:46 - you get centralized file services in
249:47 - Azure allowing multiple locations to use
249:49 - cloud tiering and direct Cloud access to
249:51 - store and access data integrated with
249:54 - Azure backup Azure file sync integrates
249:56 - seamlessly with Azure backup offering
249:58 - unified and streamlined backup Services
250:01 - Cloud tearing as your storage needs
250:03 - change older or rarely used files can be
250:05 - moved to Azure saving space on your
250:07 - local servers easy integration it's
250:09 - designed to integrate smoothly with your
250:11 - existing infrastructure meaning less
250:13 - learning curve and disruption so that's
250:15 - a brief overview of azure file
250:18 - [Music]
250:22 - sync hey this is Andie Brown from exam
250:24 - Pro and in this follow I'm going to show
250:26 - you how to set up a file share uh and
250:28 - mount it to a virtual machine so let's
250:30 - get to it so the first thing we're going
250:32 - to do is go all the way to the top and
250:33 - we're going to type in storage accounts
250:35 - because if you type in files or um Azure
250:38 - files you're just not going to get
250:39 - anything because it is a subservice uh
250:41 - within a storage account so we'll go
250:43 - ahead and hit add and I think I will
250:46 - name uh I'll make a new Resource Group
250:48 - as I always do I'm going to name this
250:50 - one as
250:51 - keos okay and then we'll name the
250:53 - account as such now I want to show you
250:55 - something here so under the account kind
250:57 - uh if you remember from our lecture
250:59 - content you can create a uh a file share
251:01 - under general purpose 2 um but if you go
251:04 - to premium 2 you'll have just a file
251:07 - storage type that means only your this
251:09 - storage account is only for creating a
251:11 - file storage and that's if you want to
251:12 - use the premium tier 4 uh um axis and we
251:17 - don't today um but I will show you uh
251:20 - when we get to that in a moment here uh
251:22 - just to point that out to you so what
251:24 - we'll do is go ahead and hit review
251:26 - create actually before we do that let's
251:27 - just double check if there's anything
251:29 - interesting here sometimes there is so
251:30 - under Azure files for large file shares
251:33 - um provides file share support up to 100
251:35 - terabytes so right now it's it's
251:37 - disabled but if you wanted larger files
251:39 - you could go ahead and enable that but
251:40 - we're just going to leave it off I'm
251:42 - going to go ahead and hit review and
251:43 - create now as this is creating and this
251:46 - goes pretty darn quick uh what I'm going
251:47 - to do is launch a new virtual machine
251:50 - because we need something uh to mount
251:52 - the file share too so make your way over
251:54 - to Virtual
251:55 - machines and what we're going to do is
251:57 - launch a Linux one just because that's a
251:59 - lot easier for me to do here and they're
252:01 - generally more inexpensive so we'll
252:03 - choose
252:04 - Kos I'm also going to name the virtual
252:06 - machiney evos and we'll choose Ubuntu as
252:09 - our image so just go down here make sure
252:12 - you choose 18 LTS generation 2 and under
252:16 - the sizes we'll expand it make sure you
252:18 - choose B1 LS because that costs around
252:20 - $6 a month under your password we will
252:23 - choose Azure user as the name and then
252:26 - for the password itself testing 1 two 3
252:28 - capital on that t so capital T testing 1
252:31 - 23 4 5 6 sorry it's always uh 1 2 3 4 5
252:34 - 6 we should probably open up Port 44
252:37 - five um but uh we'll get to that when um
252:40 - we get to the uh actually after we
252:42 - create it here so I'll hit review and
252:44 - create and we will go ahead and just
252:47 - give it a
252:50 - moment just takes a little bit of time
252:53 - and we'll just go and create that
252:55 - there so as that is creating we'll go
252:58 - back to our storage account we'll go
253:00 - into it and uh as as you notice these
253:03 - are the services the the things that we
253:05 - can launch in our storage account what
253:06 - we want is a file share that's Azure
253:08 - files um and notice here that the
253:10 - capacity is set to 5 terabytes and it
253:12 - has a soft delete for 7 days it's not
253:14 - configured with Azure Azure active
253:16 - directory as of yet if we wanted to use
253:18 - that to authenticate but what we'll do
253:20 - is click on file share on the right hand
253:21 - side I'm going to name our file share as
253:24 - keyos I'm going to set the total to uh 3
253:27 - gigabytes because we don't need a lot of
253:28 - data and remember that tier I talked
253:30 - about well here it is it's disabled but
253:31 - if we had created it a a a premium
253:34 - storage account and a file file storage
253:36 - we could could have chosen that all
253:38 - right so we'll go ahead and hit create
253:40 - and this doesn't take too long as that's
253:42 - going let's make our way back to our
253:43 - virtual machine it looks like it's
253:44 - deployed so we'll go to that resource
253:47 - and then on the left hand side we'll go
253:48 - to networking because I want to open up
253:50 - that Port Port Port 445 that's what ssnb
253:54 - SMB communicates on and so uh down below
253:58 - all this is fine but if we were to go
254:00 - here there isn't one for SMB so what
254:02 - we'll do is just put in 445 and we'll
254:05 - say TCP and I'll just write down your
254:07 - SMB and we'll go ahead and hit add and
254:09 - that only takes a moment there we'll
254:11 - make our way back to our storage account
254:13 - we'll click in or sorry into our files
254:15 - yeah Serge account in our files notice
254:17 - we have connect and upload and so what
254:19 - we're going to do is Click uh connect
254:21 - and it's going to give us some
254:23 - instructions here that we can run and
254:25 - this is going to be the mount point so
254:26 - I'm going to go make my way back to here
254:29 - and I see that that record has been
254:31 - created and I'm going to open up our uh
254:33 - Cloud
254:34 - shell and so if you've never opened up
254:36 - cloud Cloud shell before It'll ask you
254:38 - to make a storage account that is
254:39 - specific just for it just say yes make
254:41 - sure you're in bash mode we cannot do
254:43 - this in Powershell and uh within your
254:45 - virtual machine if you're on the right
254:46 - tab here you'll go to overview and we're
254:48 - going to grab that public IP address
254:50 - type SSH Azure user at sign paste in the
254:55 - IP address we'll hit enter type in yes
254:57 - to accept the fingerprint and then type
254:59 - your password capital T testing 1 23 4 5
255:02 - 6 okay and it'll let us in and what
255:06 - we're going to need to do is do a pseudo
255:08 - apt update and I'm going to do Ampersand
255:10 - Ampersand just so we can run this in one
255:12 - line we're going to type in pseudo apt
255:14 - install cifs utils this is the utility
255:17 - that we're going to use to do um that's
255:19 - part of the mounting so we need to make
255:20 - sure that is installed and so we'll let
255:23 - that go um up apt update can take a lot
255:26 - of time or a very little amount of time
255:28 - it just depends on how many updates
255:29 - there are but it shouldn't take too long
255:32 - um and so what we're going to do is
255:33 - prepare this because this is not the
255:36 - easiest to work with and so what we'll
255:38 - do I already have it open here because I
255:40 - was given this a go earlier this is the
255:43 - exact same thing but I'm going to delete
255:44 - it out so it's one: one because I think
255:47 - it might have changed a little bit here
255:50 - whoops and so I will copy this
255:54 - here and paste it in and we'll just give
255:56 - it a quick read so what it's going to do
255:58 - um I don't know if it would color this
256:00 - if I went to bash here shell script
256:04 - there we go it's a bit easier so it's
256:05 - going to uh create a Dory called Mount
256:08 - keos um and then it's going to create
256:10 - another directory for the SMB
256:12 - credentials it's going to store the
256:14 - username and then the password in these
256:16 - credentials we're going to chamod it so
256:17 - we have our permissions uh and then we
256:19 - have this bass script here um which is a
256:22 - little bit hard to read but it is using
256:25 - cifs so it's doing some something there
256:29 - and setting that stuff up and then it's
256:30 - using CFS it's actually mounting this
256:32 - case so that's what we're going to do so
256:35 - um just go going back over here and back
256:38 - to this other
256:39 - tab um what I'm going to do here
256:42 - is um oh sorry now this is ready we're
256:45 - going to just type in clear and I'm
256:48 - going to run each of these commands so
256:50 - I'm going to grab the make directory
256:52 - here and I'm going to paste that in if
256:54 - you paste all this in half the time it
256:55 - messes up when you have multi-line like
256:57 - that so it's not even worth trying uh so
256:59 - we'll do copy we'll make that other
257:01 - directory
257:02 - here and then uh we have this pseudo
257:04 - bash line here so we'll copy that
257:09 - okay hit enter and then we'll grab this
257:11 - next line here we'll copy that paste
257:13 - that in hit
257:15 - enter did not like the space in front of
257:17 - it I think it's just a really long line
257:20 - so what I'll do is I'll just grab that
257:22 - line and move to the wall make my life a
257:24 - little bit easier and we'll paste that
257:26 - in like
257:28 - that um it's saying the credentials does
257:30 - not
257:32 - exist hold on
257:35 - here so we'll just write clear it's okay
257:39 - if it happens so this directory should
257:42 - exist right so if I do um um
257:48 - PWD Etc or we should just be able to
257:50 - autocomplete it so CD Etc I'm hitting
257:53 - tab to autocomplete and then we want
257:56 - SMB and this says
257:59 - credential so it's missing the S so
258:02 - there's my mistake okay so I'm going to
258:04 - go ahead and remove that because if we
258:06 - don't have it named right it's just not
258:07 - going to work right so we'll go ahead
258:09 - here and make sure we copy the entire
258:11 - line that was my mistake and we'll paste
258:13 - that in there we'll go to the next line
258:16 - copy that paste that
258:18 - in copy the next
258:22 - line here paste that
258:26 - in and uh we will what we'll do is we'll
258:30 - chamod the
258:32 - file okay so if we want to check to make
258:34 - sure it's the correct permissions what
258:36 - we can do is is um do an LS paste in the
258:40 - or just write in the path uh which is
258:42 - SMB credentials here and here we can see
258:45 - the file if we do a hyphen La it will
258:47 - show us that it has the correct
258:49 - permission so see it's really locked
258:51 - down there that's what the Chabad did if
258:52 - we want to see the contents of that file
258:54 - to make sure that those things are in
258:55 - there uh what we can do is type in um
258:59 - cat and just grab that whole link here
259:02 - we might have to give it pseudo it might
259:03 - complain for that it does so we'll put
259:05 - pseudo in front of that and so we just
259:07 - want to make sure there's one username
259:08 - and one password um and everything looks
259:11 - okay there we'll type clear we'll make
259:14 - our way back here and so now we'll run
259:16 - these commands so that's the first
259:20 - one and then this say is the second
259:25 - one we'll make sure that that n matches
259:27 - up i o yep I O hit enter and so now it
259:31 - should work so if we were to upload a
259:33 - file uh that should be in good shape but
259:35 - if we want to get to that directory
259:36 - we'll just type in Mount if we keep on
259:39 - hitting tab it will show us there's
259:40 - keyos okay and so you know this is where
259:43 - we're going to create some files so I
259:44 - actually don't have a file prepared so
259:46 - give me a moment I'll be back in a
259:47 - moment with a file all right I'm back
259:48 - and I've uh prepared an image for our
259:50 - self load so going back to our uh file
259:53 - share in here we have the upload button
259:54 - just go ahead and click that and on the
259:56 - right hand side it's going to give you a
259:58 - popup and uh within that popup there um
260:01 - just go find your image I'm just
260:03 - offscreen grabbing it so this is uh my
260:06 - image what I'm uploading which is keas
260:08 - uh keas fagio it's who we named it after
260:11 - he's the collector in Star Trek Con
260:12 - generation and so what I'll do is go
260:15 - ahead and hit
260:16 - upload right and so that file is
260:19 - supposedly there if we go back here and
260:21 - do uh LS there's the file right um and
260:25 - so probably it should be reflective so
260:28 - if we were to go and uh delete this file
260:31 - so we'd say remove or maybe if we made a
260:33 - new directory let's see what happens if
260:34 - we say um you know uh the episode's
260:38 - called the most toys in Star
260:40 - Trek just see if it actually reflects
260:43 - back and forth so we do a refresh there
260:46 - it is can I drag no I can't drag but I
260:48 - could probably move this file over there
260:49 - so we'll say move uh keos fio into the
260:53 - most toys episode and if we go back here
260:57 - and
260:58 - refresh you get the idea right so yeah u
261:01 - i mean that's as simple it is uh with
261:03 - the file share there okay so now that
261:07 - we're all done here with Azure file
261:08 - share we can't or we can't use this to
261:10 - make a file sync because we would have
261:11 - to use Windows for that so this is all
261:13 - pretty much done so what I want you to
261:15 - do is go to your resource groups we'll
261:17 - find keyos and we'll go ahead and delete
261:19 - this and we'll call this uh part of our
261:22 - follow along 100% done here okay so see
261:25 - you in the next
261:26 - [Music]
261:30 - one hey this is angre Brown from exam
261:32 - Pro and we are going to take a look at
261:34 - using Azure file sync so uh previously
261:37 - we set up a file share with just Linux
261:40 - but we're going to take it a bit further
261:41 - and set up a sync so what we're going to
261:43 - do is we're going to do two things we're
261:45 - going need to set up a new storage
261:47 - account and we'll also have to launch a
261:49 - virtual machine since virtual machines
261:51 - take longer to launch than a storage
261:52 - account what I want you to do I still
261:54 - have the old tab here from the last uh
261:56 - follow along but what I want you to do
261:57 - is make your way over to Virtual
261:59 - machines open that a new tab and we'll
262:01 - go ahead and launch ourself a Windows
262:03 - Server so up in the top left corner hit
262:06 - uh ad virtual machine we'll give it a
262:08 - moment I'm going to call this new one uh
262:11 - well we we can use keos again so I'm
262:13 - going to go ahead and do that we'll say
262:14 - this is keyos and I'm going to choose
262:16 - this time Windows 2019 server so we have
262:19 - a Windows server and I want uh
262:21 - 2019 um it doesn't necessarily have to
262:23 - be let me just double check
262:25 - here um yeah we'll just do 2019
262:28 - generation 2 here and we'll scroll on
262:31 - down here and I'm going to put an Azure
262:33 - user and then we'll do um uh capital T
262:37 - testing 1 2 3 4 5 6 capital T testing 1
262:39 - 2 3 4 5 6 scroll on down we are
262:42 - definitely going to want Port 3389 open
262:46 - um we'll open up these two ports as well
262:49 - I think that's okay and we'll go ahead
262:51 - hit review and create well actually
262:53 - before we do that I don't think I choose
262:56 - the right size so um we'll just wait a
262:58 - moment here as it's complaining and
263:00 - we'll change this because you can't have
263:01 - a B1 LS we're going to have to go with
263:05 - um uh
263:07 - dsv uh SV uh DS V3 okay because it has
263:12 - two V vcpus at the minimum actually we
263:14 - can go with this one here because it
263:15 - just has to be two and four so I think
263:17 - we can get away with a B2 which is 50 50
263:19 - bucks a month which we're not going to
263:21 - keep that around for very long we'll go
263:22 - ahead and hit
263:24 - create um we might need actually before
263:29 - that I'm just going to add another disc
263:30 - just in case because I really don't want
263:31 - to have to do this if I have to make
263:32 - another one and so we're just going to
263:34 - say an empty disc here I
263:37 - suppose yeah that seems fine to me and
263:39 - I'm going to go ahead
263:41 - and hit uh yeah we should enable dis now
263:44 - we'll do nothing there and we'll say
263:46 - okay and go ahead and hit review
263:51 - create and we'll just give it a moment
263:53 - takes a little bit of time and go ahead
263:55 - and hit create there and while that's
263:56 - creating now we'll go over and make
263:58 - ourselves a storage account so I'll make
264:00 - a new one here and we'll call it uh
264:02 - we'll put it in our keos and we'll call
264:04 - it keos we'll stick with Standard
264:06 - General 2 that's totally fine for our
264:08 - use case we'll go ahead and hit create
264:10 - and this will not take
264:12 - long it's very fast to making storage
264:15 - accounts virtual Windows Virtual
264:17 - machines not as
264:19 - fast so we'll just give it a moment
264:35 - here
264:40 - and it looks like it's created so we'll
264:41 - go to the resource and then under file
264:44 - shares we'll click on that we'll create
264:46 - a new file share and this will be called
264:48 - keos and we'll set 5 gigabytes because
264:51 - we really don't need a large one we'll
264:53 - create it and so this can be used for
264:54 - Windows or Linux uh in this case we're
264:57 - going to be using it for um uh windows
265:00 - so what we can do uh we'll just have to
265:02 - wait for that to finish deploy but we're
265:04 - going to have to set up as your file so
265:05 - if we go files I think we type it in
265:07 - here will it show up no so if the thing
265:10 - is we need to get file sync and it's in
265:12 - a kind of a weird spot so if we go to
265:15 - all services and then we type in
265:19 - files whoops it a little bit too quick
265:23 - there uh I'm just looking for it
265:26 - here uh what if we type in
265:29 - sync there we go so what I'm looking for
265:31 - is storage sync
265:34 - Services can we type it up here
265:37 - sync yeah so that would have been the
265:39 - way I would have gone gone gone and
265:41 - found it there but we need to find uh as
265:43 - your files I'm just trying to
265:46 - remember how to get to
265:49 - there so I could have swore we could
265:51 - type in file
265:54 - sync yeah here it is a your file
265:57 - sync and so this is how we get to it I
265:59 - know it's quite a let me just double
266:01 - check to make sure if that's somewhat if
266:02 - we can actually type that up here file
266:04 - sync no okay so what we would want to do
266:08 - is go ahead and uh create this if we
266:10 - want to take a look we can check the
266:11 - plans this seems okay so we'll go ahead
266:13 - and hit create and under here we're
266:15 - going to put it into keyos and then for
266:18 - the provider name it will be the storage
266:20 - sync service name will just be keyos as
266:22 - well um and I guess we'll stick with
266:25 - Central us I guess it just really
266:27 - depends on where our virtual machine
266:28 - storage account is I hope everything's
266:29 - launching in the same place I don't
266:31 - think it matters but I'm just going to
266:32 - double check here so this one is in
266:34 - Canada east and then our storage account
266:37 - let's just go take a look where it is it
266:40 - is in um Canada east so maybe we could
266:44 - launch this in Canada east if it doesn't
266:45 - give us any grief it doesn't so that's
266:47 - great we'll go ahead and hit
266:52 - create and we'll give that a little bit
266:54 - of time there and if our instance is
266:57 - ready we should go ahead and connect to
266:59 - it because there's going to be a couple
267:00 - things we need to install okay so I'm
267:02 - going to go over to RDP here and we're
267:04 - going to uh download the TP file and
267:07 - that's going to open up that so we'll
267:09 - just double click it we'll say connect
267:11 - if you're on a Mac you got to download
267:13 - the client if you're on Windows it's
267:14 - already it should be pre-installed if
267:16 - you if you're on a Chromebook you'll
267:17 - have to use a Bastian but I'm going to
267:18 - type in Azure
267:21 - user and that password capital T testing
267:24 - 1 2 3 4 5 6 we'll hit okay I'll say yes
267:28 - I'll
267:33 - connect and we'll just give it a moment
267:36 - there to
267:37 - load we are going to need uh two things
267:40 - we need to install but just give me a
267:41 - moment here I'll go fetch
267:44 - them all right so now that we uh I have
267:47 - our Windows machine we're uh inside of
267:49 - it here what I want you to do is open up
267:52 - um Powershell so if you type in
267:54 - Powershell we do need to run this in
267:55 - administrator mode so just type it in
267:58 - it's a little bit slow when you're rping
268:00 - which is totally fine and what I want
268:01 - you to do is right click and run as an
268:03 - administrator uh and what we're going to
268:05 - do is install Azure RM because we need
268:08 - it in order to use um the Azure file
268:10 - sync agent okay so type in install
268:13 - hyphen module and then uh type in name
268:17 - Azure RM and then we'll say allow
268:20 - clobber don't ask me what clobber does I
268:22 - have no idea but that's what Microsoft
268:24 - or Azure tells us that we should uh
268:26 - whoops uh did I spell that right no it's
268:27 - all correct okay that we should do this
268:29 - thing is pretty darn slow so uh we'll
268:32 - just have to wait here a little while um
268:34 - and once that's done we'll we'll
268:35 - continue on to the next part
268:39 - okay okay so after waiting a little bit
268:41 - it asks us the question so we're just
268:42 - going to hit why to everything all right
268:44 - and uh as that's going here um it can
268:47 - take a little bit of time so what I'm
268:49 - going to do if I can even minimize this
268:51 - here um and I did not mean to minimize
268:54 - Azure uh or this here oh the other one
268:56 - already prompted that was pretty fast
268:58 - actually it's going really fast as of
269:01 - today um but as that's
269:04 - going um
269:07 - what we'll do is we'll make our way back
269:09 - over uh to sync storage here because or
269:12 - storage sync we were waiting for this to
269:13 - get set up uh because we're going to
269:15 - have to go uh set up a sync group so I'm
269:18 - going to just call this uh keyos make
269:20 - everything easy we'll stick with that
269:21 - Azure subscription we'll drop this down
269:24 - and we'll choose keyos in here uh and
269:26 - then for the Azure file share we'll
269:27 - choose
269:29 - keyos and once that's created we'll
269:31 - click into that and what we are trying
269:34 - to do is we're adding a cloud uh
269:36 - endpoint here so we would choose our
269:39 - storage account and we would choose uh
269:41 - this here and hit
269:46 - create okay and so now we have a cloud
269:48 - endpoint but we still have to uh
269:50 - continue on with our uh our virtual
269:53 - machine here so just wait until this is
269:56 - done this will take a little bit of time
269:59 - okay all right so now that uh we um uh
270:03 - we have that installed the the power
270:05 - shell for uh resource manager what we're
270:07 - going to need to install is the Azure
270:09 - file sync agent in order to do that
270:11 - we're going to have to go to the
270:11 - internet but before we do that we really
270:14 - want to uh uh turn off a particular
270:16 - feature um so turn back on protective
270:18 - mode I was fiddling with it earlier but
270:21 - if we were to go and it doesn't matter
270:22 - where we go we say Azure file uh sync
270:26 - agent
270:28 - okay and if we were to go see we started
270:30 - getting this pop up and drives us crazy
270:32 - so what we can do because this isn't a
270:34 - big deal it's just for uh testing here
270:35 - we're going to turn that off so um go to
270:38 - your local server where it says I I IE
270:40 - enhanced security configuration click on
270:42 - that and just turn it off you know
270:45 - because it'll drive you crazy and then
270:47 - we'll go back to inter Explorer
270:48 - completely close it reopen it and now uh
270:51 - what we can do is look for that aure
270:53 - file sync so we'll type in aure file
270:55 - sync uh agent
270:58 - download and that's is azure
271:00 - microsoft.com maybe later for that
271:02 - experience it's easier if you go to the
271:04 - Microsoft site and uh if you go here
271:07 - just give it a moment scroll on down
271:08 - click on the download button and we want
271:11 - uh 2019 because that's the server we
271:12 - launched right and that and we're trying
271:14 - to uh sync with storage sync so we'll go
271:16 - ahead and hit next this is 50 megabytes
271:18 - so it shouldn't take too
271:20 - long
271:22 - um and we're going to wait for it to
271:24 - download if it doesn't uh well we'll say
271:26 - allow once for the site here so maybe it
271:29 - was trying to download yeah there it
271:31 - goes and we will save
271:34 - it
271:37 - and we will always allow for Microsoft
271:39 - here it's totally fine and it's already
271:41 - finished downloading so we'll go ahead
271:43 - and run
271:45 - that and just give it a moment here
271:47 - we'll hit next we'll accept the terms uh
271:51 - we'll install it there that's totally
271:52 - fine we'll use the existing proxy uh
271:56 - configuration Uh custom configure proxy
271:58 - well we don't have any custom so we
272:00 - leave it alone we'll leave Microsoft
272:01 - updates on uh it' probably be good to
272:04 - checkbox that on but I'm just going to
272:05 - go install next
272:07 - here all right and so we'll let the
272:10 - agent
272:12 - install usually doesn't take too
272:21 - long any I'll see you back here in a
272:24 - moment all right so now that we have our
272:26 - St agent set up we'll just hit finish
272:29 - here uh and I don't know why it opened a
272:31 - bunch of windows but that's just what it
272:32 - does and so we're just going to wait for
272:35 - for it to pop back up here and then
272:37 - we'll say
272:39 - okay and then what it'll want to do is
272:41 - connect so it says Azure environment uh
272:44 - so it is azure Cloud right and I'm just
272:46 - going to sign into my account and I'm
272:49 - going to try and log in here so let me
272:52 - just go grab my
272:53 - credentials all right so I entered my
272:55 - credentials in here so we'll go ahead
272:56 - ahead and hit sign in and we'll give it
272:58 - a moment there we're going to choose our
273:00 - subscription we will choose our Resource
273:02 - Group which is keyos and our storage
273:03 - sync there hit register
273:08 - and I'll see you back here in a
273:12 - moment all right so it looks like our
273:14 - registration was successful so that
273:16 - means that we should be able to create a
273:17 - server endpoint uh and go from there
273:20 - okay so I'm going to just go ahead and
273:21 - hit close and uh I'll come back here to
273:25 - you in a moment all right so now what
273:27 - we're going to do is set up a folder for
273:29 - uh that that we're going to want to uh
273:31 - uh be synced okay because the idea is to
273:33 - back up or sync that directory there so
273:36 - uh what's going to happen here is we're
273:39 - going to make our way down to a file
273:40 - explorer and from this PC I'm just going
273:42 - to go to the C drive I made that other
273:44 - drive because I assumed we were just
273:45 - going to throw it on there but honestly
273:46 - I'm just going to put it in the window
273:47 - or the C drive here we'll make a new
273:49 - folder called keos we'll make it all
273:51 - lowercase and inside of here I'm just
273:53 - going to make a new file here called
273:56 - hello.txt nothing super exciting um and
274:00 - so uh probably we need to turn on
274:02 - sharing so that's just a habit of mine
274:04 - so we'll go to properties here sharing
274:07 - share and we'll just make sure that
274:10 - Azure Azure user has both that and it'll
274:13 - ask us to turn network discovery on
274:15 - we'll say okay keos keos okay I can't
274:19 - remember if we have to turn that on or
274:20 - not but uh you know as long as we get
274:22 - this working that's the most important
274:23 - part here so now that we have that going
274:25 - what we want to do is make our way back
274:27 - to um Azure here and so we had our uh
274:31 - what was the sync sync
274:34 - storages and what we'll need to do is go
274:37 - into our sync storage once it ever lets
274:39 - us uh get there we'll click on keyos and
274:43 - we'll go into sync groups we'll click
274:46 - into our keyos sync group we have a
274:48 - cloud endpoint we'll need to create an
274:50 - add server endpoint drop down here we
274:52 - have a registered server so you know how
274:53 - we typed in our we typed we typed in our
274:56 - we logged into the Azure file sync agent
274:59 - that's how this registered server is
275:00 - showing up we're going to give it a path
275:02 - so this is going to be C back SL keyos
275:08 - right um so this all looks
275:13 - okay and what we'll do is we will say
275:16 - let's just check this here this is all
275:18 - okay and we'll go ahead and hit uh
275:19 - connect all right and so this takes a
275:22 - little bit of time to provision it's
275:23 - going to show up here in a moment if I
275:25 - hit refresh it's provisioning so I'll
275:27 - see you back here in a
275:29 - bit you know I just remembered is that
275:32 - uh we do actually have to turn on cloud
275:33 - tering or it's going to stay pending
275:34 - forever so so I'm going to go to enable
275:36 - here and uh always preserve specific
275:39 - percentage of free space on the volume
275:42 - uh sure we can do 20% here we don't need
275:44 - a date policy here and this all looks
275:47 - okay so we'll go ahead and hit save uh
275:50 - because if we don't do that it's not
275:51 - going to actually it's not going to move
275:53 - it to the cloud right so we'll give that
275:55 - a go oh the health is good um and but
275:59 - we'll just give it a moment here
276:02 - okay so we give it a little bit of time
276:04 - here it says the Point failed which I
276:05 - really don't believe it because I just
276:07 - checked and it was working totally fine
276:08 - but what I want to do is show you if I
276:10 - go over to Microsoft Azure here and we
276:13 - go over to uh storage accounts and we go
276:15 - to keyos and we go into file shares and
276:19 - we click into this one we can actually
276:21 - see the files here so it clearly is
276:23 - syncing uh I'm not sure why we got that
276:25 - error I've never seen that error before
276:27 - um cloud is not supported for the
276:29 - specified path well that's fine okay so
276:32 - if the server path didn't work maybe
276:33 - it's the cloud cloud endpoint but but
276:35 - generally that is the workflow to uh get
276:37 - uh syncing working so you pretty much
276:39 - have all the working knowledge you need
276:41 - um but uh what we'll do is go ahead and
276:43 - tear all this stuff down because we are
276:45 - 100% done so I'm going to go to our
276:47 - resource groups
276:49 - here and uh what we'll do is go into
276:53 - keyos and we'll just make sure that
276:56 - everything is there even the storage
276:57 - sync service I just wanted to make sure
276:59 - it was all there and what we'll do is go
277:01 - ahead and write in keyos and go ahead
277:04 - and delete and that should take
277:06 - everything down no problem all right so
277:08 - there you
277:10 - go all right so just one more thing uh I
277:12 - was doing that cleanup and then today I
277:14 - woke up the next morning and when I
277:16 - checked my resource groups the keyos was
277:18 - still here so I went in here and we
277:20 - still had uh the storage sync service so
277:22 - what you're going to want to do is go in
277:24 - here and I think what it wants you to do
277:26 - actually tell us the message here if we
277:27 - go delete Resource Group we type in
277:32 - keyos and uh it will complain that uh
277:34 - it's it still need you have to get rid
277:36 - of the um the resources within it so I
277:38 - already know that's going to fail um but
277:42 - what I want you to do is delete the sync
277:43 - group a so if you select the sync group
277:46 - here and go ahead and delete
277:48 - that we might also have to delete the
277:50 - end points
277:52 - first usually uh usually a is really
277:54 - great about tearing everything down but
277:55 - in this case uh it's not giving us a lot
277:58 - of help here so we'll just say um yeah
278:02 - we want to delete this end point too
278:13 - let's delete all the server so you got
278:14 - to first delete the server endpoints to
278:16 - delete the cloud endpoint to delete the
278:17 - group then to delete the actual uh
278:20 - service so it's a little bit um
278:23 - convoluted but uh you know I guess for
278:25 - whatever reason this one is not
278:26 - automated like the other one so just go
278:28 - through those process make sure that you
278:30 - get everything deleted out okay um and
278:33 - then you should be in good shape
278:35 - [Music]
278:38 - okay hey this is Andrew Brown from exam
278:41 - Pro and in this section we'll be
278:43 - covering the core backup and Disaster
278:44 - Recovery Solutions in Azure starting
278:46 - with Azure site recovery Azure site
278:49 - recovery is a hybrid backup solution
278:51 - that facilitates sight to sight recovery
278:52 - from on premises to the cloud ASR is a
278:55 - critical component for your business
278:57 - continuity and Disaster Recovery
278:59 - strategy site recovery replicates
279:01 - workloads from a primary site to a
279:03 - secondary site in the event the primary
279:05 - site encounters a failure site recovery
279:07 - will fail over to the secondary site to
279:09 - ensure continuity of services Azure site
279:12 - recovery can replicate Azure VMS between
279:14 - different regions also known as cross
279:16 - region replication various OS such as
279:18 - Windows and Linux on premises to Azure
279:21 - seamlessly transition from your local
279:23 - servers to the cloud between other cloud
279:25 - service providers such as AZ to Azure
279:27 - different machines including VMware
279:29 - hyperv or physical machines recovery
279:32 - time objectives this is the amount of
279:34 - time your business business can afford
279:35 - to have its systems unavailable or
279:37 - offline before it significantly impacts
279:38 - your business recovery Point objectives
279:42 - this is the maximum amount of data loss
279:43 - your business can tolerate measured in
279:45 - time before a disaster occurs this
279:48 - overview highlights Azure site recovery
279:50 - a key tool for protecting business
279:51 - operations against unexpected
279:58 - disruptions the next topic we'll be
280:00 - covering is Recovery Solution for Azure
280:02 - site recovery a Recovery Solution for
280:05 - Azure hybrid and on premises workloads
280:07 - should be designed to meet specific
280:08 - recovery objectives including recovery
280:10 - time objective recovery Point objective
280:13 - and Recovery level objective recovery
280:15 - time objective RTO is the maximum amount
280:18 - of time that an application can be down
280:20 - before it starts causing significant
280:21 - business disruption a Recovery Solution
280:24 - should be able to restore the system and
280:25 - data within the specified RTO time frame
280:28 - for example if the RTO is 1 hour then
280:30 - the Recovery Solution should be able to
280:32 - restore the system and data within 1
280:34 - hour of an outage
280:35 - recovery Point objective RPO is the
280:37 - amount of data loss that is acceptable
280:39 - after an outage The Recovery Solution
280:41 - should be able to restore the system and
280:43 - data to the required RPO level for
280:45 - example if the RPO is 1 hour then the
280:47 - Recovery Solution should be able to
280:49 - restore the data to the most recent
280:51 - point in time within 1 hour of the
280:52 - outage recovery level objective R is the
280:55 - level of recovery that is required after
280:57 - an outage the rlo can vary depending on
281:00 - the type of data or application being
281:01 - recovered a Recovery Solution should be
281:03 - able to restore the data or application
281:05 - to the required rlo level for example if
281:08 - the rlo requires a point in time restore
281:10 - The Recovery Solution should be able to
281:12 - restore the data to the specific point
281:14 - in time when recommending a Recovery
281:17 - Solution for Azure hybrid and on
281:19 - premises workloads the following factors
281:21 - should be considered business
281:23 - requirements The Recovery Solution
281:24 - should align with the business
281:25 - requirements including the RTO rlo and
281:28 - RPO objectives these objectives should
281:30 - be identified during the planning phase
281:33 - workload types different types of
281:35 - workloads may have different RTO rlo and
281:37 - RPO requirements The Recovery Solution
281:40 - should be able to accommodate the
281:41 - requirements of each workload type
281:43 - hybrid or on premises The Recovery
281:45 - Solution should be able to handle hybrid
281:47 - or on premises workloads depending on
281:49 - the specific requirements of the
281:51 - organization data protection The
281:53 - Recovery Solution should provide data
281:55 - protection including backups and
281:57 - replication to ensure that the data can
281:59 - be recovered in case of an outage
282:01 - testing the Recovery Solution should be
282:03 - tested regularly to ensure that it meets
282:05 - the RTO rlo and RPO objectives testing
282:08 - should be conducted in a controlled
282:10 - environment to avoid any negative impact
282:12 - on production systems by considering
282:14 - these factors you can enhance your Azure
282:16 - site Recovery Solutions ensuring Swift
282:18 - and efficient data recovery to maintain
282:20 - business
282:25 - continuity in this section we're going
282:27 - to explore an example of azure site
282:29 - recovery architecture This Disaster
282:32 - Recovery Solution utilizes Azure site
282:34 - recovery alongside other managed
282:35 - services like traffic manager and
282:37 - virtual Network this combination
282:39 - provides a costeffective and high
282:41 - availability environment suitable for
282:43 - small to medium businesses this allows
282:45 - companies to focus more on their core
282:47 - Solutions benefiting various Industries
282:49 - including Health Care travel and
282:50 - hospitality and Manufacturing for
282:53 - example it can be used in portable
282:55 - healthcare clinics restaurant chains and
282:57 - local Logistics and Supply chains here
282:59 - is a breakdown of the architecture
283:01 - traffic manager this Azure service
283:03 - wrotes DNS traffic enabling easy
283:05 - redirection from one site to another
283:07 - based on policies set by your
283:09 - organization Azure site recovery this
283:11 - service handles the orchestration of
283:13 - machine replication and manages the
283:15 - setup of failback procedures virtual
283:17 - Network this is the location where the
283:19 - failover site is established when a
283:21 - disaster happens blot storage this is
283:23 - where the replica images of all machines
283:25 - protected by site recovery are stored so
283:28 - that's an overview of the example Azure
283:30 - site recovery architecture
283:33 - shown
283:37 - the next topic we'll be covering is the
283:38 - Azure Backup Service another crucial
283:40 - tool for backup and Disaster Recovery
283:42 - Solutions Azure backup service is a
283:45 - Cloud solution from Microsoft that
283:46 - offers secure scalable and simple data
283:49 - backup in recovery across various Azure
283:51 - Services you won't find it by searching
283:53 - based on the service name however Azure
283:56 - backup is seamlessly integrated within
283:58 - numerous Azure Services making its
284:00 - operation quite intuitive the five core
284:02 - components of azure backup Mars agent
284:05 - helps in backing up files folders and
284:07 - System state data from on premises
284:09 - machines and Azure VMS to a backup
284:11 - Recovery Services Vault and Azure
284:13 - Recovery Services Vault manages and
284:15 - organizes your backups in a cost
284:17 - effective secure and scalable manner
284:19 - Azure backup server MBS used to backup
284:22 - on premises data to Azure for hybrid
284:24 - protection VM extension allows the
284:27 - backup of azure VMS without the need to
284:29 - deploy any additional agents backup
284:31 - policy defines when and how your data is
284:33 - backed up
284:34 - what can be backed up on premises Azure
284:37 - VMS Azure manage discs Azure file shares
284:40 - SQL Server sap handed databases Azure
284:43 - database for postgressql servers and
284:45 - Azure blobs why use Azure backup offload
284:48 - on premises backups saf card your data
284:51 - by moving backups to Azure backup Azure
284:53 - iOS VMS ensure your Azure VM data is
284:56 - protected scale easily adjust your
284:58 - backup storage size based on your needs
285:01 - get unlimited data transfer no limits or
285:03 - charges for data trans transfer keep
285:05 - data secure ensure your data is secure
285:07 - both at rest in and Transit centralized
285:10 - monitoring and management have a unified
285:12 - View and manage your backups easily app
285:14 - consistent backups restore applications
285:16 - back to a precise State automatic
285:19 - storage management no need to manage
285:21 - Backup storage explicitly multiple
285:23 - storage options choose between different
285:25 - storage options based on your needs so
285:28 - that's an overview of azure backup
285:33 - service
285:35 - the next topic we'll be covering is
285:36 - azure Recovery Services Vault Azure
285:39 - Recovery Services vault is a storage
285:40 - entity in Azure that stores backup
285:42 - copies of data and configuration
285:44 - information over time this data can be
285:46 - related to various Azure resources like
285:48 - virtual machines workloads servers or
285:51 - workstations backup for Azure Services
285:53 - RS Vault protects data from various
285:55 - Azure services including iOS VMS Azure
285:58 - SQL databases Azure blob storage Azure
286:01 - file shares and Azure functions recovery
286:04 - Services Vault supports various
286:05 - platforms including system center data
286:08 - protection manager Windows server
286:11 - integrates with Azure backup server for
286:12 - application protection and other
286:15 - platforms Recovery Services vaults has
286:17 - the following features enhanced security
286:20 - Azure Recovery Services Vault encrypts
286:22 - data in transit and at rest for secure
286:24 - backup and restore processes Central
286:26 - monitoring the Azure portal allows
286:28 - centralized monitoring of all backup and
286:30 - restore tasks across hybrid environments
286:33 - azure role-based Access Control Azure
286:35 - rbac enables granular access control and
286:38 - management of Recovery Services vaults
286:40 - soft Elite this feature retains backup
286:42 - data for an additional 14 days after
286:44 - deletion protecting against accidental
286:46 - or malicious data loss cross region
286:49 - restore crr enables data restoration in
286:51 - a secondary region during a disaster in
286:53 - the primary region ensuring business
286:55 - continuity so that's an overview of
286:57 - azure Recovery Services
287:03 - vault
287:04 - next let's explore the Microsoft Azure
287:06 - Recovery Services agent the Mars agent
287:09 - plays an important role in backing up
287:11 - files folders and the system state from
287:13 - Windows based on premises machines and
287:15 - Azure VMS all backups facilitated by the
287:18 - Mars agent are securely stored in a
287:19 - Recovery Services vault in Azure Mars
287:22 - agent is also known as the Azure backup
287:24 - agent note that the Mars agent does not
287:26 - support Linux operating systems to get
287:29 - started with utilizing the Mars agent
287:31 - for backup purposes follow these steps
287:33 - create an Azure Recovery Services Vault
287:36 - this Vault will store all the backups
287:38 - create a backup policy within the Vault
287:40 - set the terms and conditions for backups
287:42 - ensuring they align with your
287:44 - requirements configure secure rub for
287:46 - backup depending on your security and
287:48 - performance needs you might opt for
287:50 - Pathways like Express routes or private
287:52 - end points download the Mars agent
287:54 - download the agent from Azure install
287:57 - and register the agent to your Windows
287:58 - machine once installed register it for
288:01 - Activation and functionality overall the
288:03 - Mars agent streamlines the backup
288:05 - process for windows-based systems
288:07 - ensuring that data is both safely stored
288:09 - in Azure and swiftly recoverable when
288:12 - [Music]
288:15 - needed next let's explore the Azure
288:17 - backup policy Azure backup policy allows
288:20 - users to Define and configure how data
288:22 - backups are managed on the Azure
288:24 - platform it lets you set the frequency
288:26 - retention duration and type of backups
288:28 - to ensure data protection and meet
288:30 - organizational requirements creating a
288:32 - backup policy and azure inv involves a
288:34 - few essential steps select a data source
288:37 - type choose the type of data you want to
288:38 - back up such as Azure virtual machines
288:40 - or posters ql data base determine the
288:43 - frequency decide how often you want the
288:45 - backups to occur this could be daily
288:47 - weekly or any other frequency that suits
288:49 - your needs set the retention details
288:52 - determine how many snapshots or backup
288:54 - copies you wish to retain and for how
288:55 - long choose the time range for retention
288:58 - specify the time range during which you
289:00 - want to retain the backups this setting
289:02 - helps in managing the life cycle of the
289:03 - back up data efficiently in summary
289:06 - Azure backup policy allows you to set
289:08 - customized backup in retention settings
289:10 - for Reliable and consistent data
289:16 - protection the next topic we'll be
289:18 - covering is azure VM backup Azure VM
289:21 - backup is a solution for backing up and
289:23 - restoring virtual machines running on
289:25 - Azure it allows organizations to protect
289:27 - their virtual machines and their data
289:28 - against various issues such as
289:30 - accidental deletion Hardware failure
289:32 - ransomware and other for forms of data
289:34 - loss how it works Azure VM backup
289:37 - utilizes Azure backup to offer a
289:39 - centralized backup solution manageable
289:41 - through the Azure portal Powershell or
289:43 - rest apis it supports both windows and
289:46 - Linux virtual machines allowing for
289:48 - backup and restore operations at both
289:49 - the dis and VM levels multiple backup
289:52 - options are available including full
289:54 - backups incremental backups and
289:56 - differential backups backup retention
289:58 - policies enable organizations to retain
290:01 - backups for specified durations
290:02 - enhancing data man management and
290:04 - compliance ke features the solution
290:07 - employs the robust infrastructure of
290:09 - azure backup ensuring encryption at rest
290:11 - and support for backing up VMS across
290:13 - different regions and availability zones
290:16 - the integration with Recovery Services
290:18 - vaults further strengthens the
290:19 - protection and management of backup data
290:21 - Disaster Recovery Beyond backup Azure VM
290:24 - backup bolsters Disaster Recovery
290:26 - strategies by enabling the replication
290:28 - of virtual machines to a secondary
290:30 - region this redundancy is vital in
290:32 - mitigating the impact of outages or
290:34 - disasters organizations can seamlessly
290:36 - fail over in fail back virtual machines
290:38 - ensuring minimal downtime and enhanced
290:40 - business continuity in conclusion Azure
290:43 - VM backup stands out as a resilient and
290:45 - dependable backup in recovery solution
290:47 - for Azure hosted virtual
290:50 - [Music]
290:53 - machines hey this is Andrew Brown from
290:55 - exam Pro and in this follow along we're
290:57 - going to be looking at backup solutions
290:58 - for Azure so let's make our way all the
291:00 - way to the top here and launch ourselves
291:02 - a virtual machine
291:04 - that we are going to be using uh as a
291:07 - means to back up so what I'll do is
291:09 - create a new group here um and we will
291:12 - call it
291:13 - peard uh and we'll call this virtual
291:15 - machine peard and scrolling on down here
291:18 - yeah I want to stick with a Windows
291:19 - Server 2019 Gen 2 so go over here select
291:22 - it uh as generation 2 there and for the
291:25 - machine you want to make sure it's a b2s
291:27 - cuz you need at least two vpcu and 4 GB
291:30 - Ram to launch a Windows uh server there
291:33 - for the the user is going to be Azure
291:34 - user for the password capital T testing
291:36 - 1 2 3 4 5 6 capital T testing 1 2 3 456
291:41 - uh we'll have RDP uh I'm not sure if
291:43 - we'll end up using it uh but if we do we
291:45 - have that open we'll go ahead and go to
291:47 - the review page we'll wait for it to
291:49 - allow us to hit create give it a
291:53 - moment great we'll hit
291:56 - create and then we'll give it a moment
291:59 - to see that it is deploying and if it's
292:00 - deploying I'll see you back here in a
292:02 - moment great so it looks like our
292:04 - virtual machine is ready to go here so
292:06 - what we'll do is go all the way to the
292:07 - top here and type in backups we can go
292:09 - to vaults directly but let's take a look
292:11 - here at the backup center uh where we
292:13 - can kind of have a bit of an overview of
292:15 - stuff so the idea is we have vaults
292:16 - where we're going to store our backups
292:18 - we can create backup jobs backup
292:20 - policies so let's get to it and and
292:21 - first create ourselves a vault so we'll
292:24 - go ahead here and create ourselves a new
292:25 - Vault and we have two options we have
292:26 - Recovery Services Vault or a backup
292:29 - Vault so uh the difference here is that
292:31 - one is just for backing up things like
292:33 - databases discs and Etc where Recovery
292:36 - Services is more like a like a sitewide
292:39 - recovery so you're going to notice it
292:41 - includes virtual machines and all these
292:42 - other stuff uh generally you'll probably
292:44 - want to go with Recovery Services Vault
292:46 - because it is just basically a new
292:48 - version of the backup Vault so let's go
292:50 - ahead and do
292:52 - that and uh we'll place this in our Bard
292:56 - um a resource Group there and we'll just
292:58 - say peard backup or vault Vault there we
293:02 - go and we'll go ahead and hit review and
293:04 - create and we'll go and hit create
293:08 - there and we'll just give it a moment
293:10 - there to deploy all right so our
293:12 - Recovery Services vault is ready and
293:14 - we'll go ahead and click back up and
293:16 - down below ask us what kind we are
293:18 - running so we have Azure stack and on
293:19 - premise it's just Azure and we have a
293:21 - virtual machine so we'll go ahead and
293:23 - hit backup and notice that we'll have to
293:25 - set up a backup policy and here we have
293:27 - a default one but let's actually go and
293:28 - make a new one so we can kind of learn
293:30 - some of the settings that we can set so
293:31 - we can say the frequency the time Etc
293:34 - probably want to set this to my time
293:36 - zone so I'm in Toronto if I can find it
293:39 - here which I believe is
293:41 - -5 uh I was trying to type to see if I
293:43 - could do that but I wasn't uh getting uh
293:46 - getting there too well um we'll just say
293:48 - central Canada there that's okay it's
293:50 - off by an hour but that's close enough
293:53 - um and you can have weekly monthly and
293:55 - yearly backups but we're going to stick
293:57 - with the daily backup for 30
293:59 - days um and this is okay we don't have
294:02 - to enter that in we'll hit all right
294:04 - okay and now we can add our virtual
294:06 - machine so we'll go here and select the
294:08 - card hit okay enable
294:11 - backups please select at least one that
294:13 - has backups enabled so what we'll do is
294:15 - open up our Azure tab here make our way
294:17 - over to our virtual machine and just
294:18 - make sure that that stuff is
294:22 - enabled so we'll go over
294:24 - here um and we'll go to
294:30 - backups and here we can actually even
294:33 - set set it from here so I guess we'll
294:35 - select our existing Vault here it'll say
294:38 - enable
294:41 - backups but that set it up with a
294:43 - default one right so I'm a bit surprised
294:45 - we couldn't do it that way
294:47 - but we'll make our way back
294:50 - here as long as it works that's the
294:52 - thing with Azure it's just you know
294:54 - you'll do something and then you'll find
294:55 - that uh you have to do it slightly
294:57 - different uh but we we'll just wait
294:58 - until that's done there so after waiting
295:01 - a very short while uh it looks like
295:03 - backups are turned on so if we were to
295:05 - go to our overview and go to backups you
295:07 - can see we have one backp item if we go
295:09 - over to our backup items we'll see under
295:10 - virtual machines we have Bard um we did
295:13 - set the default policy so I guess if we
295:15 - wanted to switch that out we probably
295:17 - could if we just go back and let's just
295:20 - go create a policy of our own just so
295:22 - that we are familiar with that there um
295:25 - and we'll say an Azure virtual machine
295:26 - one so my special
295:30 - policy
295:31 - okay and um this is for 180 days we go
295:36 - ahead and hit
295:38 - create and once that policy is deployed
295:41 - we'll be able to associate I can't
295:42 - remember if this will be super fast but
295:43 - we'll give it a moment here yep it is
295:45 - super fast and then once uh we have that
295:48 - poliy we can go into it and then start
295:50 - assigning things to it actually I think
295:52 - when I made the lecture content I don't
295:54 - even remember there being a backup
295:55 - center so to me this is all a little bit
295:57 - new um so maybe I'll go back and up to
296:00 - the slides cuz I I usually say there is
296:01 - no centralized service but they have one
296:03 - now so um that's uh pretty darn good um
296:08 - so for Associated items we don't fetch
296:10 - any data for services completed um so
296:12 - I'm just trying to figure out how we can
296:14 - change that policy there if we go to
296:16 - backups we just change it over
296:22 - here uh I'm not sure how to switch out
296:24 - the policy but I know at least if we
296:26 - want to do backup now we just press that
296:29 - there it's really not that important to
296:31 - know how to swap out a policy just more
296:33 - so the fact that you know what a backup
296:35 - policy is and you can apply to machines
296:36 - and things like
296:38 - that yeah so there we go we just had our
296:41 - own backup
296:43 - there and uh yeah there's not much else
296:47 - here to look at but uh yeah there you go
296:48 - set backup policies so I guess what
296:50 - we'll do is go ahead and tear all this
296:57 - down and so that was our book card there
296:59 - we'll go ahead and delete this Resource
297:02 - Group
297:04 - and there you
297:06 - [Music]
297:10 - go hey this is Andrew Brown from exam
297:12 - Pro in this section we're diving into an
297:15 - introduction to Azure kubernetes service
297:17 - or a KS Azure kubernetes service
297:19 - simplifies the process of deploying a
297:21 - managed kubernetes cluster in Azure one
297:24 - of the main benefits of using a KS is
297:26 - that Azure will take over the management
297:27 - of the kubernetes master notes for you
297:29 - including Health monitoring and routine
297:31 - maintenance your resp responsibility
297:33 - lies only in maintaining the agent nodes
297:36 - an additional benefit is the cost
297:38 - effectiveness of a KS the service itself
297:40 - is free you only incur charges for the
297:42 - agent nodes within the cluster not the
297:44 - Masters when you deploy in a KS cluster
297:46 - both the kubernetes master and all nodes
297:48 - are deployed and configured for you
297:50 - during the deployment process you can
297:52 - also configure additional features such
297:53 - as advanced networking Azure active
297:56 - directory integration to use kubernetes
297:59 - Ro based Access Control monitoring
298:01 - capabilities and Windows Server
298:03 - containers are supported in a KS so when
298:06 - should one opt for a KS well a KS is
298:09 - ideal in scenarios where you need full
298:10 - container orchestration this includes
298:13 - situations demanding seamless service
298:15 - Discovery across multiple containers
298:17 - automatic scalability to handle varying
298:19 - loads and coordinated and smooth
298:20 - application upgrades in summary a KS
298:23 - offers a streamline managed solution for
298:25 - container orchestration in Azure
298:27 - simplifying the setup and maintenance of
298:29 - a kubernetes
298:32 - cluster
298:35 - the next topic we'll be covering is
298:36 - called bridge to kubernetes bridge to
298:39 - kubernetes is an extension available for
298:41 - both visual studio and visual studio
298:43 - code that allows developers to write
298:45 - test and debug microservice code
298:47 - directly on their local development
298:49 - workstations with bridge to kubernetes
298:51 - you can integrate a service running
298:52 - locally with yours cluster this bypasses
298:55 - the need to create Docker and kubernetes
298:57 - configurations for the lifetime of this
298:59 - connection a proxy is added to your
299:01 - cluster in place of your kubernetes
299:03 - deployment that redirects requests to
299:05 - the service to your development computer
299:07 - when you disconnect the application
299:08 - deployment will revert to using the
299:10 - original version of the deployment
299:11 - running on the cluster it's important to
299:13 - note that Azure Dev spaces will be
299:15 - retired on October 31st 2023 existing
299:18 - users are encouraged to transition to
299:20 - using bridge to kubernetes as their
299:22 - client development tool ensuring a
299:23 - streamlined and integrated development
299:25 - experience on their local machines while
299:27 - interacting with the ks clusters so
299:30 - that's a brief overview bridge to
299:31 - kubernetes a very useful tool to
299:33 - streamline development and debugging for
299:40 - developers the next topic we'll be
299:42 - covering are the Recovery Solutions for
299:44 - Azure kubernetes service that's
299:46 - practices for business continuity and
299:48 - Disaster Recovery in Azure kubernetes
299:50 - service use multiple availability zones
299:53 - Azure AKs supports multiple availability
299:55 - zones which distribute your application
299:57 - across different data centers this
299:59 - ensures High availability and resiliency
300:01 - in the event of a data center failure
300:03 - when you create a new KS cluster you can
300:05 - choose to create it across multiple
300:06 - availability zones Implement backup and
300:09 - restore backing up your KS cluster
300:11 - ensures that you have a copy of your
300:12 - application and data in case of data
300:14 - loss or corruption Azure AAS supports
300:17 - backing up your application data and
300:18 - kubernetes resources such as deployments
300:21 - and services using the Valero backup and
300:23 - restore tool use Azure site recovery
300:26 - Azure site recovery is a disaster
300:28 - recovery solution that replicates your
300:29 - KS cluster to a secondary location this
300:32 - can be used useful in the event of a
300:33 - disaster or outage as you can fail over
300:35 - to the secondary location and resume
300:38 - operations for monitor your a KS cluster
300:41 - it is important to monitor your KS
300:43 - cluster for any potential issues or
300:45 - failures Azure provides a range of
300:46 - monitoring and alerting solutions such
300:48 - as Azure Monitor and Azure service
300:50 - Health which can help you proactively
300:52 - detect and respond to issues five test
300:55 - your Disaster Recovery plan to ensure
300:57 - that your Disaster Recovery plan is
300:59 - effective it is important to test it
301:00 - regularly you can use tools like azure
301:02 - site recovery to perform failover tests
301:04 - and ensure that your KS cluster can be
301:06 - recovered in the event of a disaster six
301:09 - use Azure kubernetes service with Azure
301:11 - Arc Azure Arc enables you to manage your
301:13 - KS cluster and other kubernetes clusters
301:15 - across multiple clouds and on premises
301:17 - environments from a single control plane
301:20 - this provides greater flexibility and
301:21 - resilience in the event of a disaster or
301:23 - outage by following these best practices
301:26 - you can ensure that your Azure
301:27 - kubernetes service cluster is resilient
301:30 - highly available and recoverable in the
301:31 - event of a disaster
301:34 - [Music]
301:37 - routage hey this is Andrew Brown from
301:39 - exam Pro and in this section we'll be
301:41 - covering Azure regions and availability
301:43 - zones starting with Azure regions Azure
301:46 - regions are physical locations around
301:47 - the world where Microsoft as data
301:49 - centers to provide cloud services each
301:51 - region is composed of multiple data
301:53 - centers that are geographically
301:54 - dispersed to provide redundancy
301:56 - resilience and high availability to
301:58 - customers P facts about Azure regions
302:01 - multiple data centers each region is
302:03 - made up of at least one data center but
302:05 - many regions have multiple data centers
302:07 - for added redundancy and availability
302:09 - Global presence Azure regions are
302:11 - distributed around the world covering
302:13 - almost every major continent and country
302:16 - unique identifiers each Azure region is
302:18 - designated by a distinct name such as
302:20 - East us or west Europe which indicates
302:22 - its geographical location resource
302:25 - deployment when creating resources in
302:26 - Azure you can choose which region to
302:28 - deploy them to based on factors such as
302:30 - proximity to users data sovereignty
302:32 - require requirements and service
302:34 - availability data residency data in a
302:36 - region stays within that region unless
302:38 - explicitly copied or replicated to
302:40 - another region for redundancy or
302:42 - Disaster Recovery purposes continuous
302:45 - expansion Microsoft continually expands
302:47 - its Azure footprint adding new regions
302:49 - to cater to emerging markets and
302:50 - customer needs in essence Azure regions
302:53 - are the Cornerstone of azure's global
302:55 - infrastructure guaranteeing High
302:56 - availability redundancy and adherence to
302:59 - Regional data
303:01 - norms
303:05 - the next topic we'll be covering our
303:06 - Azure availability zones Azure
303:09 - availability zones are physical data
303:11 - center locations within an Azure region
303:12 - that are typically located in separate
303:14 - buildings or regions but still close
303:16 - enough to provide low latency network
303:18 - connectivity these zones help protect
303:20 - applications and data from data center
303:22 - level failures by providing redundant
303:24 - Power Cooling and networking and Azure
303:26 - region is a geographic location that
303:28 - contains one or more data centers an
303:30 - availability zone is a unique physical
303:32 - location within an Azure region key
303:35 - points of azure availability zones each
303:37 - availability zone is composed of one or
303:39 - more data centers with independent Power
303:41 - Cooling and networking each data center
303:44 - within an availability zone is connected
303:46 - through a high-speed low latency Network
303:49 - availability zones are designed to
303:50 - provide High availability for critical
303:52 - applications by Distributing them across
303:54 - different zones by deploying
303:56 - applications across multiple zones it is
303:59 - possible to achieve a higher level of
304:00 - redundancy and fault tolerance virtual
304:03 - machines storage accounts and other
304:05 - Azure resources can be deployed to
304:06 - specific availability zones within a
304:08 - region to ensure High availability Azure
304:11 - traffic manager can be used to Route
304:12 - traffic between different zones based on
304:14 - various criteria such as geographic
304:16 - location latency and performance so
304:19 - that's an overview of azure availability
304:25 - zones the next topic we'll be covering
304:28 - are the availability options for Azure
304:30 - virtual machines while we've briefly
304:32 - mentioned these terms in the past we'll
304:33 - now explore them in detail specifically
304:35 - in the context of VMS Azure virtual
304:38 - machines provide several options for
304:40 - achieving High availability and ensuring
304:42 - business continuity here are the primary
304:44 - strategies availability sets an
304:47 - availability set is a logical grouping
304:49 - that informs Azure about application
304:51 - redundancy and availability requirements
304:53 - ideally to ensure High availability and
304:56 - meet azure's
304:57 - 99.95% SLA place at least 2 VMS within
305:00 - an availability set there are no es for
305:02 - the availability set itself costs arise
305:05 - only for the individual VM instances
305:06 - created within virtual machine scale
305:09 - sets these sets enable the automatic
305:11 - deployment and scalability of identical
305:13 - VMS VMS can auto adjust their capacity
305:16 - as per demand facilitating autoscaling
305:18 - for applications VM scale sets are built
305:21 - to integrate with Azure load balancer
305:23 - and application Gateway ensuring traffic
305:25 - is evenly distributed across VM
305:27 - instances VM distribution across fault
305:29 - domains and update domains further
305:31 - fortifies High availability ility Azure
305:34 - site recovery this service delivers
305:36 - Disaster Recovery capabilities for VMS
305:39 - it facilitates VM replication to a
305:41 - secondary location which could be
305:43 - another Azure region a different data
305:44 - center or an on premises site both
305:47 - physical servers and virtual ones
305:49 - including those on hyperv and VMware can
305:51 - be replicated using site recovery Azure
305:54 - backup Azure backup offers backup
305:56 - solutions for VMS their applications and
305:59 - data you can back up to the cloud
306:01 - ensuring backups are application
306:03 - consistent meaning they're usable and
306:04 - can be restored as needed these backups
306:07 - can either be retained in the
306:08 - originating region or transferred to
306:09 - another for Disaster Recovery
306:12 - additionally Azure backup provides
306:13 - extensive retention periods in backup
306:15 - archiving aligning with Regulatory
306:17 - Compliance needs overall these are the
306:20 - main options for availability options
306:22 - for Azure virtual
306:24 - [Music]
306:27 - machines hey this is Andrew Brown from
306:30 - exam Pro and in this section we'll be
306:31 - going over Azure virtual machines Azure
306:34 - virtual machines offer a highly
306:35 - configurable server experience through
306:38 - virtualization you can run a server
306:39 - without the hassles and expenses of
306:41 - maintaining physical Hardware however
306:43 - it's important to note that VMS are not
306:45 - entirely maintenance-free they still
306:47 - require OS patch applications and
306:49 - package installations and configurations
306:52 - key points about Azure VMS configuration
306:55 - and size the size of the VM is
306:57 - determined by its image this image
306:59 - defines the combination of vcpus memory
307:01 - and Storage capacity subscription limits
307:04 - as of now there's a limit of 20 VMS per
307:07 - region on a per subscription basis
307:09 - billing Azure VMS are buil at an hourly
307:11 - rate availability a single instance VM
307:14 - offers 99.9% availability when all its
307:17 - storage discs are of Premium quality to
307:19 - achieve a
307:21 - 99.95% availability deploy two instances
307:23 - in an availability set storage you can
307:26 - attach multiple manage discs to your
307:28 - Azure
307:29 - VMS networking components when you
307:31 - launch an Azure virtual machine other
307:33 - networking components will be either
307:35 - created or Associated to your virtual
307:36 - machine including Network Security Group
307:39 - a virtual firewall with rules concerning
307:41 - ports and protocols this is attached to
307:43 - the Nick network interface a device that
307:46 - handles IP protocols and network
307:48 - communication virtual machine instance
307:51 - the actual server that's running public
307:53 - IP address the address that you will use
307:55 - publicly access your VM virtual Network
307:58 - the network in which your VM is located
308:01 - overall as your virtual machines offer a
308:03 - versatile and strong cloud computing
308:05 - space allowing easy and seamless
308:06 - deployment of robust applications and
308:13 - systems the next topic will be covering
308:15 - our operation systems in Azure VMS so
308:18 - what is an operation system well I'm
308:20 - sure you already know this but the OS is
308:22 - the program that manages all other
308:23 - programs in a computer who most commonly
308:25 - know operation systems or Windows Mac OS
308:28 - and liux when you launch a virtual
308:30 - machine you need to choose an image
308:32 - which has a specific operation system
308:34 - Microsoft Works closely with Partners to
308:36 - ensure the images available are updated
308:38 - and optimized for an Azure runtime you
308:40 - can find most of these images in the
308:42 - Azure Marketplace including Sue liex
308:45 - Enterprise server Red Hat Enterprise
308:47 - liex auntu server Debian free bestd
308:52 - Azure Marketplace flat car container
308:54 - Linux Rancher OS thei library for Azure
308:58 - mesosphere DC OS on Azure Docker images
309:01 - Cloud B Jenkins platform if the
309:04 - available options do not meet your
309:05 - requirements you can bring your own
309:07 - Linux by creating a Linux virtual hard
309:09 - disk note that in Azure only the fixed
309:11 - vhd format is supported not the hyperv
309:14 - virtual hard disk format so that's an
309:16 - overview of the operating systems in
309:18 - Azure
309:23 - VMS the next topic we'll be covering are
309:25 - the sizes of azure VMS Azure VMS come in
309:28 - a variety of sizes that are also
309:30 - optimized for specific use cases is
309:32 - azure VMS are grouped into types such as
309:35 - general purposes and compute optimized
309:37 - and sizes such as B and dsv 3 also
309:40 - called series or SKU family general
309:42 - purpose balanced CPU to memory ratio
309:45 - testing and development small to medium
309:47 - databases and low to medium traffic web
309:50 - servers skus B dsv 3 DV3 D4 D4 dsv 2 and
309:55 - so on compute optimized High CPU to
309:58 - memory ratio good for medium traffic web
310:01 - servers work appliances batch processes
310:04 - and app servers skus ffs fsv 2 memory
310:08 - optimized High memory to CPU ratio best
310:11 - for relational database servers medium
310:13 - to large caches and in memory analytics
310:15 - has K's S3 F3 east4 E4 F4 S4 and so on
310:21 - storage optimized offers High disk
310:23 - throughput and IO ideal for Big Data SQL
310:26 - and osql databases data warehousing and
310:29 - large transactional databases skus lsv2
310:33 - GPU specialize VMS for heavy graphic
310:36 - rendering and video editing model
310:38 - training and inferencing with deep
310:39 - learning available with single or
310:41 - multiple gpus skus NC NC V2 ncv3 and
310:45 - cast T4 V3 ND and so on high performance
310:49 - compute features the fastest and most
310:51 - powerful CPU virtual machines with
310:53 - optional High throughput network
310:55 - interfaces skus HB hbv2 HC H there are
311:00 - previous series of virtual machines
311:02 - sizes not shown here like basic a the
311:05 - type of image May limit you to specific
311:07 - VM sizes use Azure sorting and filtering
311:10 - options to explore sizes based on
311:12 - various parameters such as
311:13 - cost Azure compute unit provides a way
311:16 - of comparing compute performance across
311:18 - Azure skus ACU is standardized on a
311:21 - small VM aside the value of 100 all
311:24 - other skus then represent approximately
311:26 - how much faster that SK you can run a
311:28 - standard Benchmark let's break down the
311:31 - provided information information A1 a4u
311:34 - family a CU vcpu 100 ACU stands for
311:38 - Azure compute unit which is a measure of
311:40 - the relative computational performance
311:42 - of different Azure skus and this family
311:44 - each virtual CPU is rated at 100 acus
311:48 - vcpu core 1:1 this means for every
311:51 - virtual CPU you allocate it corresponds
311:53 - to one physical core D1 D14 SKU family
311:58 - SCU vcpu ranges between 160 to 200 50
312:02 - this indicates that VMS in this family
312:04 - have a higher computational performance
312:06 - per vcpu compared to the Asser vcpu core
312:10 - 1:1 similar to the ass series each
312:12 - virtual CPU corresponds to one physical
312:14 - core in summary knowing Azure VM sizes
312:17 - helps pick the best option for your
312:19 - needs ensuring efficiency and cost
312:26 - Effectiveness the upcoming topic
312:28 - explores hyperv focusing on the
312:29 - distinctions between Generation 1 and
312:31 - generation 2 hyperv is a hardware
312:34 - virtualization product from Microsoft
312:36 - enabling the creation and management of
312:38 - virtual machines each VM functions as a
312:40 - separate computer equipped with its own
312:42 - operating system and software
312:44 - applications in many ways hyperv mirrors
312:47 - the functionalities of virtual box there
312:49 - are two generations of hyper vvms
312:51 - Generation 1 supports a wide range of
312:54 - guest operating systems generation 2
312:56 - primarily support 64-bit versions of
312:59 - Windows alongside more recent versions
313:01 - of litx and free bestd operating systems
313:04 - Azure offers both Generation 1 and
313:06 - generation 2 VMS aligning with hyperv
313:08 - and structure but bearing distinct
313:10 - characteristics key differences between
313:12 - Azure gen 1 and Gen 2 gen one based on
313:15 - bios architecture Gen 2 utilizes ufi
313:19 - based boot architecture enhancing Boot
313:21 - and installation times only Gen 2 VMS
313:23 - employ secure boot ensuring the boot
313:25 - loader is authenticated by a trustworthy
313:27 - Source gen 2vm support a substantially
313:30 - larger boot volume of up to 64 terab
313:33 - hyper vvms are packaged as either vhd or
313:36 - vhdx files consolidating their
313:38 - structural components for efficient
313:39 - management and deployment in conclusion
313:42 - understanding hyperv Generations AIDS in
313:44 - optimizing virtualization and ensuring
313:46 - OS
313:51 - compatibility the next topic we'll be
313:53 - covering is SSH RDP and Bastion Azure
313:57 - virtual machines offer multiple methods
313:59 - to connect including SSH RDP and Bastion
314:02 - let's dive into each secure shell is a
314:05 - protocol to establish a secure
314:06 - connection between a client and server
314:09 - this is used to remotely connect to your
314:10 - Azure VM via the terminal S sh operates
314:13 - on Port 22 via TCP RSA key pairs are
314:16 - commonly used to authorized access
314:19 - remote desktop protocol is a proprietary
314:21 - protocol developed by Microsoft which
314:23 - provides a user with a graphical
314:25 - interface to connect to another computer
314:26 - over a network connection this is how
314:29 - you can remotely connect to Windows
314:30 - Server via virtual virtual desktop RDP
314:33 - operates on Port 3389 utilizing both TCP
314:36 - and UDP Bastion Azure Bastion is a
314:40 - service you deploy that lets you connect
314:41 - to a virtual machine using your browser
314:43 - and the Azure portal it provides secure
314:46 - and seamless RDP SSH connectivity to
314:48 - your virtual machines directly from the
314:50 - Azure portal over TLS a Bastion is a
314:52 - hardened instance that is monitored
314:54 - users connect to this VM which then
314:56 - establishes a connection to the Target
314:58 - instance sometimes known as jump so to
315:01 - ensure op optimiz security and
315:02 - connectivity it's important to
315:04 - understand these methods we'll delve
315:06 - deeper into each and subsequent
315:12 - sections let's go into a bit more detail
315:14 - with SSH secure shell or SSH is a
315:17 - cryptographic network protocol commonly
315:19 - employed to securely access and manage
315:21 - servers remotely it is very common to
315:24 - use SSH key pairs as a means to
315:26 - authenticate to your VMS SSH key pairs
315:29 - is when you generate out two keys a
315:31 - private key this is the key that remains
315:33 - confidential and should never be shared
315:34 - or exposed it is stored securely on your
315:37 - local system and is used to initiate a
315:39 - connection to the VM a public key as the
315:41 - name suggests this key can be shared
315:43 - publicly it is added to the servers or
315:46 - vm's authorized Keys list how does SSH
315:49 - key authentication work when you attempt
315:51 - to SSH into a server your system uses
315:54 - the private key to send a cryptographic
315:55 - proof the server which has the
315:57 - corresponding public key verifies the
315:59 - authenticity of the cryptographic proof
316:01 - a if the proof is verified meaning the
316:03 - keys have matched your authenticated and
316:05 - granted access advantages of using SSH
316:08 - key pairs security SSH Keys provide a
316:11 - more secure method of authentication
316:13 - than traditional passwords without the
316:15 - correct private key unauthorized access
316:17 - attempts are effectively thwarted
316:19 - convenience once set up users can
316:21 - connect without needing to remember and
316:22 - input a password each time automation
316:25 - automated scripts and services can use
316:27 - key pairs to establish connections
316:29 - without human intervention so that's a
316:31 - more in-depth look into
316:37 - SSH let's dive into the remote desktop
316:40 - protocol commonly known as RDP RDP is a
316:43 - protocol developed by Microsoft that
316:45 - allows users to remotely connect to
316:46 - Windows systems when you want to use RDP
316:49 - to access your Windows Server you'll
316:50 - first need to download the RDP file for
316:53 - Windows 10 users the remote desktop
316:55 - client is pre-installed so there's no
316:57 - additional software to download however
317:00 - if you're using Mac OS you can easily
317:02 - get the Microsoft Remote Desktop app
317:04 - from the Apple Store once you've got the
317:06 - necessary tools simply open the
317:08 - downloaded RDP file during this process
317:10 - you'll be prompted to enter the username
317:12 - and password that you set up during the
317:13 - creation of your VM on the Azure portal
317:16 - RDP uses encryption to secure
317:18 - Communications ensuring confidentiality
317:20 - it also supports features like audio
317:22 - redirection clipboard sharing and
317:24 - printer redirection for a seamless
317:26 - remote experience in conclusion RDP
317:29 - provides an efficient way to remotely
317:30 - access Windows those
317:36 - systems the next topic we'll be covering
317:38 - is azure Bastion Azure Bastion serves as
317:41 - a secure Bridge enabling you to connect
317:43 - to your server via SSH or RDP without
317:45 - exposing it to the public it will
317:47 - provision a web-based interface for both
317:49 - RDP and SSH eliminating the need for
317:52 - external clients this can be especially
317:54 - useful for devices like Google
317:56 - Chromebooks which might not support
317:57 - traditional RDP clients when setting up
318:00 - azure you'll need to add a dedicated
318:03 - submit to your virtual Network named
318:04 - Azure basan submit this submit should
318:06 - have at least a 27 size equating to 32
318:10 - addresses if you have a Windows Server
318:12 - which requires RDP and have a Bastion in
318:14 - the same v-net you simply enter in your
318:17 - user name and password as you normally
318:19 - would if you have a Linux server you can
318:21 - SSH with the Bastion you can use SSH
318:24 - private key or password that you set
318:26 - when you created your VM key benefits
318:29 - single click access Rd p and SSH
318:32 - sessions available directly through the
318:33 - Azure portal secure session Azure
318:36 - Bastion offers RDP SSH over TLS with
318:39 - support for TLS 1.2 plus no public IP
318:42 - needed RDP SSH connects via the vm's
318:45 - private IP simplified NSG management no
318:48 - need for nsgs on the Azure Bastion
318:50 - subnet allows RDP SSH solely from Azure
318:53 - Bastion managed service Azure Bastion is
318:56 - a fully managed secure platform for RDP
318:59 - SSH protection from scans VMS aren't
319:02 - exposed to the internet preventing Port
319:04 - scanning centralized hardening Bastion
319:07 - at the vnet perimeter eliminates
319:08 - individual VM hardening so that's an
319:11 - overview of azure
319:16 - Bastion let's take a look at a
319:18 - comparison between Windows versus liux
319:20 - servers Azure VMS offer the flexibility
319:23 - to run both windows and liux based
319:25 - servers Windows licensing to run Windows
319:29 - you'll need a valid license if you don't
319:31 - act activate it certain features may be
319:32 - restricted Azure does offer a way to
319:34 - leverage existing licenses through its
319:36 - hybrid benefit program authentication
319:39 - typically you set up a username and
319:41 - password during the VM creation instance
319:43 - size to smoothly operate Windows you
319:46 - typically need a larger VM size starting
319:48 - at least with a B2 due to its
319:49 - comprehensive desktop environment
319:51 - environment Windows provides a full
319:53 - desktop environment complete with a
319:55 - graphical user interface making it more
319:57 - intuitive for those familiar with the
319:59 - windows ecosystem l
320:01 - licensing most Linux distributions are
320:04 - open source and don't require any
320:05 - licensing fees authentication Linux
320:08 - offers flexible authentication you can
320:10 - set up a username and password or more
320:12 - commonly use SSH key pairs for a more
320:14 - secure connection instance size Linux
320:17 - servers especially those without a
320:19 - graphical user interface have minimal
320:21 - system requirements this means you can
320:23 - run them on smaller VM sizes conserving
320:25 - resources and cost environment
320:28 - traditionally Linux systems operate with
320:30 - a terminal Bas and environment although
320:32 - there are distributions with graphical
320:34 - interfaces overall your choice between
320:36 - Windows and Linux will largely depend on
320:38 - the specific needs of your project
320:40 - familiarity with the operating system
320:42 - licensing costs and desired system
320:49 - resources the next topic will be
320:51 - covering is update Management in Azure
320:53 - update management allows you to manage
320:55 - and install operating system updates and
320:57 - patches for both windows and Linux
320:59 - virtual machines that are deployed in
321:00 - azure on premises or with other Cloud
321:03 - providers when you launch an Azure VM
321:05 - you can go to operations and turn on
321:07 - guest plus host Updates this will
321:09 - install the Microsoft monitoring agent
321:11 - that will be used to monitor your
321:12 - instances Azure automations is the
321:15 - underlying service that is installed the
321:16 - agent update management will perform a
321:19 - scan for update compliance by default a
321:22 - compliance scan is performed every 12
321:24 - hours on Windows and every 3 hours on
321:26 - Linux it can take between 30 minutes and
321:28 - 6 hours for the dashboard to display
321:30 - updated dat data from managed computers
321:32 - in Azure automation you can enable the
321:34 - update management change tracking and
321:36 - inventory and start or stop VMS during
321:38 - off hours features for your servers and
321:40 - virtual machines these features have a
321:42 - dependency on a log analytics workspace
321:44 - and therefore require linking the
321:46 - workspace with an automation account so
321:49 - Azure update management offers a
321:50 - comprehensive solution for ensuring your
321:52 - virtual machines are always up to date
321:54 - with the latest patches and
321:59 - updates oh right let's take a look at
322:02 - Azure virtual desktop formerly known as
322:04 - Windows Virtual desktop Azure virtual
322:07 - desktop on Microsoft Azure is a desktop
322:09 - and app virtualization service that runs
322:11 - on the cloud Azure virtual desktop works
322:14 - across devices like Windows Mac iOS
322:17 - Android and LX with apps that you can
322:18 - use to access remote desktops and apps
322:21 - you can use most modern browsers to
322:23 - access Azure virtual desktop hosted
322:25 - experiences use Azure virtual desktop
322:28 - for specific needs like when security is
322:29 - a concern because all day data is saved
322:31 - on the server and cannot be left on the
322:33 - device of a user key features and
322:35 - benefits enable secure and productive
322:38 - remote work on any device Azure virtual
322:40 - desktop provides full Windows 10 and
322:42 - Windows Server desktop and application
322:44 - virtualization on any personal device
322:47 - seamless integration with Microsoft 365
322:50 - apps for Enterprise and Microsoft teams
322:52 - reduce cost of Licensing and
322:54 - infrastructure use eligible windows or
322:57 - Microsoft 365 licenses to access Windows
323:00 - Virtual desk top and pay only for what
323:02 - you use protect against outages to stay
323:04 - productive help keep your team running
323:06 - during outages by leveraging built-in
323:08 - Azure site recovery and Azure backup
323:10 - Technologies simplify it Management
323:13 - Windows Virtual desktop manages the
323:15 - virtual desktop infrastructure for you
323:17 - so you can focus on users apps and Os
323:20 - images instead of hardware and
323:21 - maintenance Keep application and user
323:24 - data secure easily apply the right
323:26 - access controls to users and devices
323:28 - with Azure active directory conditional
323:30 - access ACC so that's an overview of
323:33 - azure virtual
323:35 - [Music]
323:38 - desktop hey this is Andrew Brown from
323:40 - exam Pro and we're going to be launching
323:42 - our own Bastion using Azure Bastion
323:44 - services so there's two ways to set this
323:46 - up we can go to bastions over here and
323:49 - create a Bastion this way or we can
323:51 - create one uh after we've created a
323:53 - virtual machine I prefer the ladder so
323:55 - let's go ahead and do that and launch
323:57 - ourselves a new virtual machine and so
324:00 - we can either use launch a Windows
324:02 - server or a Linux server today I'm going
324:04 - to be launching a Windows
324:06 - server and uh what we'll do is go down
324:09 - here I'll make a new group we'll call it
324:11 - the
324:14 - Enterprise and uh as we do that we'll
324:16 - just name this uh
324:19 - Enterprise D and we'll launch that in
324:22 - Canada C or us or Central us that's fine
324:25 - with me uh 2019 data center Gen 2 is
324:28 - totally fine if you go here you're
324:31 - trying to find it you go hit select and
324:33 - we'll choose Gen 2 Data Center it is
324:36 - expensive but uh we're not going to be
324:38 - using this for very long uh for the
324:40 - username I'm going to put uh data and
324:43 - for the I'll just make it Azure user to
324:45 - make our lives a bit easier and then
324:46 - we'll put testing capital T 1 2 3 4 5 6
324:50 - testing 1 2 3 4 5 6 and we will go down
324:53 - below we're fine with the settings here
324:55 - we're going to go next to dis we're
324:56 - going to leave the dis to premium that's
324:58 - fine we'll let it create a new network
325:00 - that's totally
325:01 - fine management is okay and we'll just
325:04 - actually go hit review and
325:11 - create and now we'll just hit create so
325:14 - that it will go ahead and do that it'll
325:16 - tell us that it's in
325:18 - progress and we'll just wait a little
325:20 - bit here I'll see you back in a
325:22 - moment all right so our instance is
325:24 - ready so let's go ahead and go to this
325:25 - resource here and then on the left hand
325:28 - side you'll have connect and so I'm
325:30 - going to going to show you and you don't
325:32 - it's not necessary for you to do uh this
325:33 - step because you're going to I'm going
325:35 - to show you how to connect VI the Bas in
325:36 - but I'm going to go ahead and download
325:38 - this file the RDP file and this will
325:40 - only work if you're on windows by the
325:42 - way well I guess it will work on Mac but
325:44 - you'd have to install the um uh the uh
325:48 - RDP uh service for that users with
325:50 - Windows there and so here this is azure
325:53 - user we're going to type in testing with
325:54 - a capital T just double check that there
325:57 - I'm going to log in make sure that this
325:59 - works uh do we'll do that one more time
326:02 - oh you know it's testing 1 2 3 4 5
326:06 - six there we go we'll say
326:13 - yes and we'll just make sure that we can
326:15 - uh remote desktop into this just before
326:17 - anything else and there we go so that's
326:19 - all good to me I don't need to see
326:20 - anymore we'll go over to Bastion we'll
326:22 - say use Bastion and this is going to set
326:24 - up a Bastion service in order to use
326:27 - Bastion you need to have another um uh
326:30 - address space uh defined for it it makes
326:32 - it really easy to uh make it here so I'm
326:35 - just going to go
326:36 - 10.0.1
326:38 - do0
326:40 - 24 and we'll go ahead and hit
326:45 - okay and so down below it's going to
326:47 - choose an address
326:49 - space um we have a security group um I'm
326:53 - just going to put it for none I don't
326:56 - think I want one on
326:59 - that
327:01 - and if we scroll on down here we have
327:03 - the resource Group so we're going to put
327:05 - it in the same Resource Group and we'll
327:07 - go ahead and create
327:09 - that so now before this they didn't have
327:12 - this really nice wizard you used to have
327:13 - to go and and create all those things
327:15 - individually in your virtual network but
327:17 - this is really nice it does take a bit
327:19 - of time for this to provision so I'll
327:21 - see you back here in a bit that took a
327:23 - bit of time for that to create that
327:25 - Bastion but it is ready to go and so now
327:27 - that we have it we can go ahead and
327:29 - utilize uh this connection here and so
327:32 - right away it I think it's setting up
327:33 - for RDP here so what we'll do is type in
327:37 - Azure user uh and then capital T testing
327:40 - 1 2 3 4 5 6 we'll go ahead and hit
327:45 - connect and so notice that I didn't have
327:47 - to use an external application I could
327:49 - just uh run it in right here it's all in
327:53 - the web browser so that's pretty much
327:55 - how uh the Bastion works I can't
327:57 - remember the pricing on Bastion I think
327:58 - it's a little bit of money so I don't
328:00 - want to keep this uh laying around here
328:01 - but this is great if you let's say
328:03 - you're on a Chromebook which are
328:04 - becoming really popular where you can't
328:06 - install native applications uh or you're
328:09 - just having issues because you're on
328:10 - like Linux or something like that so
328:12 - there you go that's all there is to it
328:13 - we'll go ahead and clean this up and so
328:16 - I'm just going to go here find the
328:17 - resource Group and we'll go ahead and
328:20 - delete I'm just making sure that
328:21 - bastion's within there so it is
328:28 - good and there we go
328:33 - [Music]
328:37 - so we just launched a virtual machine
328:38 - for Linux now let's go ahead and launch
328:40 - one for Windows so I'm going to go to
328:42 - the top here and type in Virtual
328:44 - machines we'll go to the first link I'm
328:46 - going to hit add add virtual
328:49 - machine and uh what we'll do is we'll
328:52 - create a new group the last one I had
328:53 - was called beor I'm going to call this
328:55 - one
328:56 - kardashia
328:59 - kardashia
329:03 - and I'm going to name this uh machine
329:05 - also named
329:07 - kardashia and this time what we want to
329:09 - do is is we want to move over to a
329:11 - Windows Server I find the easiest one to
329:13 - learn with is the Windows 10 Pro server
329:17 - uh just because I find these ones a
329:18 - little bit daunting so I'm going to go
329:19 - Windows 10 Pro uh and then what we're
329:22 - going to do is go choose a larger size
329:24 - this is not going to work we cannot run
329:26 - a Windows server on a B1 LS so we're
329:29 - going to have to go a little bit larger
329:31 - and uh we don't have to go too much
329:33 - larger here but the idea here is that
329:35 - there's going to be a a more expensive
329:37 - spend here so we're not going to be
329:38 - wanting to keep this running for long
329:40 - but here we have the b2s that is the
329:42 - appropriate size to run this anything
329:44 - smaller I don't think is going to
329:46 - work and we are going to put in a
329:49 - password here so I'm just going to put
329:50 - in
329:54 - kardashia and we'll do
329:56 - kardashia uh one
329:59 - two3 put a capital on it I guess we'll
330:02 - just do this
330:06 - here and I'm just going to go back and
330:08 - lowercase this
330:11 - one and uh we're going to allow the
330:14 - inbound Port of 3389 because that is
330:16 - what RDP needs are going to confirm that
330:19 - I have a Windows license I actually
330:20 - don't but the thing is you can still
330:21 - launch one for uh your test purposes
330:24 - it'll it'll just complain saying you're
330:26 - not activated uh so there are some
330:28 - limitations but it's good enough for us
330:30 - to uh learn okay and so now that that is
330:32 - all great we'll go next to diss we're
330:35 - going to go with pre or standard SSD
330:37 - this time uh we are going to go ahead
330:40 - and hit uh next and go to networking
330:41 - it's going to create us a new v-net
330:43 - which is a great idea we're going to let
330:45 - it create a uh a network security group
330:47 - on the Nick just like before we'll go
330:49 - ahead and hit
330:50 - next uh we'll leave all these options
330:52 - alone this all seems fine to me uh and
330:55 - we'll hit next review and
330:59 - create
331:01 - and we'll go ahead and create This
331:09 - Server all right and so that's going to
331:12 - go ahead and create it so I'll just see
331:13 - you back here in a moment when that's uh
331:15 - done deploying all right and so after a
331:17 - short little while here it looks like
331:18 - our Windows server is now deployed so
331:21 - what we can do is go to that resource
331:23 - and if you want to to see what it's
331:24 - deployed it's the same stuff as always
331:26 - you have your network interface card
331:27 - your virtual Network NSG the IP address
331:31 - but let's actually go to that resource
331:33 - now and so let's see how we can gain
331:35 - access to this virtual machine and so
331:38 - what we can do is use RDP luckily I am
331:40 - on a Windows machine and so um I already
331:43 - have the rdb client uh that I can use so
331:45 - all I got to do is download the rdb file
331:49 - and then once we have that file I can
331:50 - just double click it and I can open this
331:52 - up if you're on a Mac uh you can
331:54 - download uh the app in the app store and
331:57 - so I'll go ahead and type in my password
331:59 - so my username was
332:00 - kardashia and then my password was
332:03 - capital c a r d a SS I A1 23 we'll hit
332:08 - okay and then it'll give us another
332:10 - warning we'll say yes and now we are in
332:13 - our virtual machine so there you go how
332:15 - cool is that I'll give it a moment to
332:18 - load up but this is a full uh Windows 10
332:20 - Pro uh and as I said before you know we
332:23 - don't actually have a license so if
332:25 - you're afraid of spining up because you
332:26 - think you're going to get charged a
332:28 - license fee uh for Windows you do not
332:30 - have to worry that's not going to happen
332:31 - you have to do some manual intervention
332:34 - uh for that to happen so we'll just wait
332:36 - a little while here for this to load um
332:38 - it is not we're not using the most
332:40 - powerful machine so it does take a
332:41 - little bit of time and so we just hit
332:43 - accept
332:45 - here and here we are so we are on uh we
332:49 - have our nice Windows machine here
332:51 - whoops I don't know if it has any games
332:53 - let's go take a
332:56 - look maybe we play M
332:58 - sweeper
333:03 - um no maybe maybe you have to download
333:05 - in the store I'm not that familiar with
333:07 - Windows machines but um so there you go
333:09 - so we'll go ahead and close that and you
333:11 - know if we were using the Bastion it's
333:12 - the same process you saw how we used it
333:14 - with SSH but if we had the Bastion and
333:16 - it's so much work to set one up we
333:18 - already did that before uh but all you
333:19 - do is enter your credentials in on the
333:21 - page just as we did and it and it's just
333:23 - a lot easier that way uh so let's go
333:25 - ahead and just tear down this machine
333:27 - we're all done with it so I'm just going
333:28 - to hit um uh
333:32 - Delete and uh if we find that Resource
333:34 - Group we should be able to easily delete
333:36 - them all I find the easiest way is to go
333:37 - up here go to all resources and then
333:40 - there's the resource Group there and
333:42 - then hit delete Resource Group and then
333:44 - I'll type in the name of it which is
333:46 - kardashia and I'll delete all those
333:48 - resources but after that's done always
333:50 - just take a double check uh on your all
333:52 - resources Tab and just make sure that
333:55 - those resources are gone because
333:56 - sometimes they stick around but there
333:58 - you go that's as simple as it was to
333:59 - launch a Windows
334:03 - [Music]
334:06 - machine hey this is Andrew Brown from
334:08 - exam Pro and let's take a look here at
334:11 - virtual machines which I consider the
334:12 - backbone of most cloud service providers
334:15 - and Azure keeps it really simple by
334:16 - calling it um virtual machine so we can
334:19 - go up here and type in virtual machine
334:21 - and make our way over here but right now
334:23 - I don't have any options because I'm
334:24 - using a tenant uh that doesn't have a
334:26 - subscription applied to it so what I'm
334:28 - going to do is go switch back to my
334:29 - original tenant and this one has a sub
334:32 - subscription applied to
334:33 - it and so what I'll do is just click
334:36 - back up here and now we'll just type in
334:38 - Virtual
334:39 - machines and I can now see uh I have
334:42 - options of creating virtual machines so
334:43 - let's go ahead and go create a Linux one
334:45 - first and then we'll go ahead and create
334:47 - a Windows one and then we'll see how we
334:48 - can connect to it all right so first
334:50 - we'll go to the top here and hit add
334:52 - we'll click on virtual
334:55 - machine and we're going to be presented
334:56 - with a lot of options so we'll have to
334:58 - choose a subscription and so there is
335:00 - mine I want to probably create a new
335:02 - Resource Group here I'm going to call
335:04 - this one
335:06 - beour and we'll name this uh vour again
335:11 - and I'm going to launch this in Us East
335:14 - I'll just set it to one availability
335:15 - zone for the time being uh then here we
335:18 - have what we can choose as an image I
335:20 - can click on see all
335:21 - images and choose from a variety of them
335:24 - so if I didn't want to use Ubuntu I
335:26 - could launch something else like Debian
335:27 - or something like that uh but really I
335:29 - just want to uh stick with uh Ubuntu cuz
335:32 - I'm fine with that version with 18 uh
335:35 - then here's what what really matters is
335:36 - the size cuz that's going to affect our
335:38 - cost so if we click on see all sizes we
335:40 - have this uh Nifty um table where we can
335:43 - sort the cost it's just loading in the
335:44 - cost here it's Dynamic this is going to
335:46 - be based on uh what your base
335:48 - subscription is so if you're in Canada
335:51 - you're going to see Canadian prices
335:52 - you're in the US you're going to see us
335:53 - prices etc etc and I care a lot about
335:57 - cost here so I'm just going to sort this
336:00 - by cost and here we have the B1 LS which
336:03 - is very cost effective we have a ram of5
336:07 - GB and some other options there so we'll
336:10 - go ahead and select that there and we
336:13 - have a couple options we can use SSH
336:15 - public key or we can uh utilize a
336:18 - password and so I think what we'll do is
336:20 - use an SSH public key because that's uh
336:22 - pretty much the standard there we're
336:23 - going to name the username beig or if it
336:25 - lets
336:26 - us probably won't probably want some
336:29 - additional options there oh it's okay oh
336:31 - great and uh we'll go ahead and generate
336:33 - a new key pair and I'm just going to
336:36 - name that one
336:38 - beour and uh we have some options here
336:41 - for inbound rules uh so you could set to
336:43 - none this is just setting up the uh NSG
336:46 - for you uh but we're probably going to
336:48 - want to have that Port open uh for SSH
336:51 - because that's how we're going to make
336:52 - our way back in here if we're running a
336:54 - um like an apachi server we'd want to
336:57 - have Port 80 open so we can go ahead and
336:59 - do that
337:00 - we'll take a look at now
337:02 - diss so here we have an options between
337:04 - premium um standard and standard hhd um
337:09 - I just want this to be cost effective so
337:10 - I'm just going to go with standard HDD
337:12 - uh but generally you you want to have um
337:15 - at least uh standard or premium SSD when
337:17 - you're running uh real web development
337:19 - workloads uh then there's encryption
337:21 - here and so it's always turned on by
337:23 - default which is great they also have
337:25 - this option of double encryption with
337:26 - the platform manage and customer manage
337:28 - key uh we're just going to leave that
337:29 - that as default enable alteris
337:31 - compatibility that's not something we
337:32 - need to do here CU we are not using
337:34 - alter disk and here you can see that you
337:36 - can attach multiple uh diss here um so I
337:39 - can go and do that but that's not
337:41 - something I need to do today and some
337:43 - other Advanced options which we do not
337:45 - care about we'll go over to networking
337:47 - and so it's going to end up creating us
337:48 - a new v-net for us and we'll create a
337:51 - new subnet for us and assign it a IP
337:54 - address uh it will also set up a NYX uh
337:58 - network security group so the network
337:59 - Security Group is not going to be
338:00 - applied at the subnet level it's going
338:02 - to be applied at the Nick which is
338:04 - attached to the um dc2 there and so
338:08 - we'll just leave it to basic we're going
338:09 - to allow in inbound ports for Port 80 um
338:13 - and 22 that was carried over uh from
338:15 - earlier we can put this behind a load
338:17 - balancer but I don't think we're going
338:18 - to do that right now we'll go over to
338:21 - management uh we have some additional
338:23 - options here for monitoring uh this is
338:25 - all okay here we can set it to auto
338:28 - shutdown actually I'll leave that alone
338:29 - loone you can also enable backups here
338:33 - we'll go Advanced and now we have this
338:36 - option here for custom data uh I covered
338:39 - a section on cloud and nit and uh they
338:42 - don't call this user data but most other
338:44 - providers will call this user data so we
338:45 - could provide a bash script or
338:48 - additional information here if we wanted
338:50 - to then down below uh there's some host
338:53 - group options we're not going to worry
338:54 - about that and proximity placement group
338:56 - this is really important if you need to
338:57 - have um instances nearby uh I think this
339:01 - is pretty common with um what's it
339:04 - called high capacity workloads H hfc I
339:07 - can't remember the initialism right now
339:09 - but we covered in the core content then
339:11 - we can tag our resource here uh we'll
339:13 - just leave that alone I don't care about
339:15 - tagging too much but generally it's good
339:16 - to tag in practice and then we will get
339:19 - to review and create our server here
339:22 - we'll go ahead and hit
339:24 - create and then we'll have to download
339:26 - our private key so we can utilize it
339:28 - later and so that's downloaded there and
339:32 - now we're just waiting for it to deploy
339:34 - this and I'll see you back here in a
339:35 - moment so we had to wait a little bit
339:37 - there and finally um our deployment is
339:40 - complete and we can go ahead and just
339:42 - review all the things that it created so
339:44 - notice that it created the virtual
339:46 - machine it created a network interface a
339:48 - Nick for us the NSG the network security
339:52 - group the um virtual Network and also a
339:55 - public IP address um when I do cleanup a
339:57 - lot of things I always miss are these IP
339:59 - addresses and I know that um azzure gave
340:01 - me a warning that said hey you're about
340:03 - to spend uh $700 yearly on IP addresses
340:07 - because you weren't releasing them so
340:09 - when we do the cleanup step I'll
340:10 - definitely uh emphasize about deleting
340:12 - those IP addresses and how to go about
340:14 - that let's go take a look at the actual
340:16 - resource
340:17 - now so here we are and uh you can see we
340:20 - have a lot of options the left hand side
340:22 - such as the disk um so we can see the
340:24 - disk options there and there's
340:26 - additional Security Options let's go go
340:29 - take a look at how we can go ahead and
340:31 - connect to the server and so uh there's
340:34 - different options here so we have RDP
340:36 - SSH and Bastion since we are using a
340:39 - Linux machine we're not going to be
340:40 - using RDP that's really for
340:43 - Windows um but the trick here is that um
340:46 - I would need to have a client on my
340:48 - computer to connect like um I think it's
340:51 - called putty if you're on a Linux based
340:52 - machine it's a lot easier uh and
340:55 - certainly I have the uh Linux subsystem
340:57 - installed so I could probably um connect
340:59 - that
341:00 - way but I figured let's just go ahead
341:02 - and connect via Bastion because I think
341:04 - this is a pretty darn cool feature so
341:06 - let's go ahead and create ourselves a
341:09 - Bastion and this will take a little bit
341:11 - of time here but we'll go ahead here and
341:14 - just set up subnet so to associate a
341:15 - virtual Network to a Bastion it must
341:17 - contain a subnet with the Azure Bastion
341:19 - subnet so they actually have a special
341:21 - subnet for it so what we'll do is we'll
341:24 - just go back to our um our server here
341:28 - uh which we called beour
341:30 - and I think we can find the subnet
341:31 - through here so on the left hand side if
341:33 - we go to networking we probably could
341:35 - find it that way um so I'm just looking
341:38 - for that Security Group um in
341:42 - there it should be um maybe it's not
341:46 - there if it's not
341:48 - there um well you know we could just go
341:50 - over make our way over to subnets it's
341:52 - not a big
341:53 - deal cuz it's called beour it's pretty
341:56 - darn easy to find to begin with and so
341:58 - under subnet here what we need to do is
342:00 - add a a special one here and just got to
342:04 - remember how this works um so you need
342:07 - to create a a subnet called Azure
342:09 - Bastion subnet with a prefix of at least
342:11 - 27 so we'll go ahead and add a new
342:14 - subnet and we'll call it
342:16 - that uh we'll take out the space
342:19 - there and the range is 10 0 1 0 Etc so
342:23 - we'll just do it on two 0 do um
342:28 - 2.0
342:31 - sl27 uh 102 is not
342:38 - contained yeah it overlaps so we'll do
342:40 - two do not contain the virtual Network
342:43 - address space oh right so we have to add
342:44 - the address space
342:46 - first oops we'll just hit cancel
342:49 - here that's okay we'll discard that
342:51 - we'll make our way over to address space
342:53 - and we'll go ahead and add
342:54 - 10.0 uh
342:57 - 2.04 that'll give was a pretty darn uh
343:00 - large range
343:02 - there and so now what we'll do is go
343:05 - back to our
343:07 - subnet and we'll go ahead and create
343:09 - that
343:11 - there and it said only needed 27 so
343:14 - we'll just give it only 27 we don't need
343:15 - to go bigger than we need and that
343:18 - should be okay we'll go ahead and hit
343:22 - save and it shouldn't take too long so
343:25 - now that we have that we can go back
343:26 - here and we'll give give this another go
343:28 - here
343:35 - here uh it's there so it shouldn't be
343:38 - complaining maybe what we'll do is just
343:40 - start from the uh the start here
343:47 - again yeah there you go that the Azure
343:49 - portal is like that a lot where you'll
343:51 - have something set up and it has the Old
343:53 - State of it and so you just have to
343:55 - trust yourself that you know what you're
343:57 - doing and you have to go back but if you
343:59 - don't have a lot of confidence a lot of
344:00 - times you'll get stuck and you'll think
344:01 - okay I don't have it right but always
344:03 - just try again and hit refresh um
344:05 - because the Azure portal is very
344:07 - inconsistent so we're just going to have
344:08 - to wait for this to create this does
344:10 - take uh a little while to create so I'll
344:12 - see you back in a moment so after
344:14 - waiting five minutes our Bastion is now
344:16 - created and so what we can do is without
344:18 - even using a putty client or having to
344:20 - use Linux directly uh we can just uh
344:23 - connect through via the Bastion uh so
344:26 - here uh we'll see we have some options
344:27 - here so we want to do SS H uh private
344:30 - key from local
344:31 - file okay and what we can do is go ahead
344:35 - and select our beor key and then I'll
344:37 - just scroll down here and hit
344:39 - connect oh um and I think we made the
344:42 - username
344:43 - beour and we'll go ahead and connect
344:47 - now and it's complaining about a popup
344:49 - here so we'll go up here and
344:52 - say always
344:54 - allow and we'll try that
344:57 - again
344:59 - and then we'll say allow
345:02 - again and so now that we're into our
345:05 - server here let's go ahead and try to
345:06 - install aachi and see if we can get at
345:08 - least the default page running um so
345:10 - this is using Ubuntu if my memory serves
345:12 - me correctly should be app get install
345:14 - Apachi
345:16 - 2 and we'll just hit Y for
345:21 - continue and we'll just wait for this to
345:23 - install doesn't take too darn
345:27 - long
345:29 - and after a short little wait there uh
345:31 - it finally did install also if you
345:33 - notice this little icon here we have a a
345:35 - little clipboard here um I don't seem to
345:37 - ever use that there so that's fine um
345:40 - now when you install a Pochi we might
345:42 - have to go ahead and start it up um so
345:45 - let's just take a look to see if it
345:47 - actually is in the running directory
345:49 - here so we'll go to cdar www and so
345:52 - that's where the default directory is
345:55 - right um but we can just check to see if
345:57 - uh it's running by doing a PS Ox I think
346:00 - it's
346:01 - httpd or we can say Apache here and so
346:05 - it looks like it's already running so
346:08 - that's pretty great for us and since
346:10 - it's running on Port 80 and we've opened
346:12 - up Port 80 we should probably be able to
346:14 - access that um here so let's go back to
346:17 - our actual virtual machine so we'll go
346:19 - to Virtual
346:23 - machines and uh we have that virtual
346:26 - machine running I'm just going to click
346:27 - into it because I just want to find out
346:28 - it's public IP
346:31 - address so here it is there and for
346:34 - lucky this will just work just a copy to
346:35 - clipboard button right
346:37 - there and look at that we have the
346:39 - default page Isn't that cool um so
346:41 - that's all there really is to it and I
346:43 - could even update this page you don't
346:45 - have to do it but I'm just going to
346:46 - update it for fun actually I probably
346:48 - have to restart the server so maybe I
346:49 - won't do that um but yeah so we
346:51 - connected through the basion so that was
346:53 - pretty darn easy we probably could have
346:55 - also used um the cloud shell to connect
346:58 - um
347:00 - but maybe we should we could give that a
347:01 - go as
347:04 - well since we're all done here let's go
347:06 - ahead and do some cleanup the first
347:08 - thing I want to do is uh the easiest way
347:10 - is actually to go to all resources here
347:11 - at the left hand side and this really
347:13 - gives you an idea of everything that's
347:15 - running in your account so I actually
347:16 - have other stuff in here uh that's not
347:18 - relevant um but the idea is that all of
347:20 - our stuff is running within a resource
347:22 - Group um and so I'm just taking a look
347:25 - there I'm not seeing this is all
347:26 - resources here um see if we see Resource
347:29 - Group here yeah they're all there right
347:32 - there so I can go ahead and click that
347:34 - and so everything more or less should be
347:36 - self-contained within here see all that
347:38 - stuff um you can even see the v-net is
347:40 - part of it as well and so if I go ahead
347:43 - and delete this Resource Group it should
347:46 - delete all this stuff so I'm just going
347:47 - to type beour to
347:49 - confirm and we'll go ahead and
347:51 - delete and that should do a good job of
347:54 - cleaning up all those
347:56 - files I'm not sure if it will delete the
347:58 - IP it should right there but if it
348:00 - doesn't what I recommend is after
348:03 - everything is deleted just go back here
348:05 - to all resources and just double check
348:07 - to make sure they all vanish because
348:09 - when this is done they're all going to
348:11 - uh start to vanish from this list and if
348:12 - there's anything remaining you'll know
348:14 - because it's still here right so just be
348:16 - careful about that that's all I want you
348:18 - to know um and so that's the Linux part
348:21 - and so let's go ahead and actually now
348:22 - set up a Windows
348:27 - Server hey hey this is Andrew Brown from
348:29 - exampro and in this section we'll be
348:31 - covering Azure app service Azure app
348:34 - service is an HTTP based platform for
348:36 - web apps re estf apis and mobile bin
348:39 - Services you can choose your programming
348:41 - language in Python Java or any other
348:43 - language and run it in either a Windows
348:45 - or Linux environment it is a platform as
348:48 - service so it's the Heroku equivalent
348:50 - for Azure Azure app service takes care
348:52 - of the following underlying
348:54 - infrastructure OS and language security
348:56 - patches load balancing Auto scaling and
348:58 - infrastructure management azzure app
349:01 - service makes it easy to implement
349:02 - common Integrations and features such as
349:04 - Azure Dev Ops for deployments GitHub and
349:07 - dockerhub package Management Systems
349:09 - easy to set up staging environments
349:11 - custom domains and attaching TLS or SSL
349:14 - certificates you pay based on an Azure
349:16 - app service plan shared tier includes
349:19 - free and shared options litex isn't
349:21 - supported here dedicated tier includes
349:23 - basic standard premium premium 2 Premium
349:26 - 3 and there's isolated tier as your app
349:30 - service is versatile you can deploy
349:31 - single or multi-container Docker
349:33 - applications when you create your app
349:35 - you have to choose a unique name since
349:37 - it becomes a fully qualified domain
349:40 - overall Azure app service simplifies
349:42 - your web hosting needs ensuring you can
349:43 - focus on coding and let Azure do the
349:45 - heavy
349:50 - lifting let's delve into runtimes in
349:52 - Azure app service so what is a runtime
349:55 - environment a runtime environment refers
349:58 - to the softt sof Ware and settings
349:59 - needed for a program to run in a defined
350:01 - way at runtime a runtime generally means
350:03 - what programming language and libraries
350:05 - and framework you are using a runtime
350:07 - for Azure app services will be a
350:09 - predefined container that has your
350:10 - programming language and commonly used
350:12 - library for that language installed with
350:15 - Azure app Services you're presented with
350:17 - a range of runtimes to choose from
350:19 - including net. netcore javar Ruby
350:21 - node.js PHP and python moreover Azure
350:25 - app Services generally supports multiple
350:27 - versions of each prr programming
350:28 - language for example for Ruby you might
350:31 - find versions 2.6 and
350:33 - 2.7 it's worth noting that cloud
350:35 - providers including Azure May phase out
350:37 - support for older versions over time
350:39 - this not only ensures that they're
350:41 - offering the latest and most efficient
350:42 - tools but also promotes better security
350:44 - practices among users pushing them to
350:46 - keep up with the latest patches so
350:49 - that's an overview of runtimes and Azure
350:51 - app
350:55 - service the next thing we'll be covering
350:57 - are custom containers in Azure app
350:59 - service Azure app service gives you the
351:01 - flexibility to use custom containers for
351:04 - both windows and Linux the primary
351:06 - reason you might opt for a custom
351:07 - container is to use a distinct runtime
351:09 - that isn't natively supported or to
351:11 - incorporate specific packages and
351:13 - software here's a straightforward
351:15 - process to get started with custom
351:16 - containers and Azure app service design
351:19 - your container Begin by creating a
351:21 - Docker container tailored to your needs
351:22 - on your local Machine push to Azure once
351:25 - your container is ready push it to the
351:27 - Azure container registry this
351:28 - centralized repository ensures that your
351:30 - container is easily accessible within
351:32 - Azure deploy and go live finally deploy
351:35 - your container image directly to the
351:37 - Azure app service once deployed Azure
351:39 - takes care of scaling maintenance and
351:41 - updates another advantage of custom
351:43 - containers in Azure app services that
351:45 - they offer more granular control over
351:47 - your environment you can fine-tune
351:49 - performance security and other aspects
351:51 - of your application environment to suit
351:53 - your
351:57 - needs the next topic will be covering
351:59 - our deployment slots in Azure app
352:01 - service deployment slots allow you to
352:04 - create different environments of your
352:05 - web application Associated to a
352:07 - different host name this is useful when
352:09 - you require a testing staging or QA
352:11 - environment alongside your production
352:13 - setup deployment slots let you swiftly
352:15 - replicate your production setting for
352:17 - various purposes ensuring consistent
352:19 - testing environments you could also swap
352:22 - environments this is useful for
352:23 - executing blue green deployments by
352:26 - using swap you can promote your stage in
352:28 - environment to production with thesee
352:30 - you can promote our staging to
352:31 - production by swapping if something goes
352:33 - wrong you could swap them back this
352:35 - capability ensures minimal downtime and
352:37 - enhances the user experience since you
352:39 - can introduce changes in a controlled
352:40 - manner rolling them back if necessary in
352:43 - addition Azure ensures that when
352:45 - swapping the instances are warmed up
352:47 - before traffic is routed resulting in
352:49 - zero downtime so that's a quick overview
352:52 - of deployment
352:56 - slots the next topic we'll be covering
352:59 - is the app service environment in Azure
353:01 - app Service app service environment is
353:03 - an Azure app service feature that
353:05 - provides a fully isolated and dedicated
353:07 - environment for securely running app
353:09 - service apps at high scale this allow
353:11 - you to host windows and liux web apps
353:13 - Docker containers mobile apps and
353:16 - functions app service environments are
353:18 - appropriate for application workloads
353:20 - that require very high-scale isolation
353:22 - and secure network access and high
353:24 - memory utilization customers can create
353:26 - multiple ases within a a single Azure
353:28 - region or across multiple Azure regions
353:30 - making ases ideal for horizontally
353:32 - scaling stateless application tiers in
353:34 - support of high requests per second
353:36 - workloads ases comes with its own
353:39 - pricing tier called the isolated tier
353:41 - ases can be used to configure security
353:43 - architecture apps running on ases can
353:46 - have their access gated by Upstream
353:47 - devices such as web application
353:49 - firewalls app service environments can
353:52 - be deployed into availability zones
353:53 - using Zone pinning there are two
353:55 - deployment types for an app service
353:57 - environment external ass and
354:00 - ilbs external ass exposes the ass hosted
354:03 - apps on an internet accessible IP
354:04 - address if the v-net is connected to
354:07 - your on premises Network apps and your
354:09 - ass also have access to resources there
354:11 - without additional configuration because
354:13 - the ass is within the v-net it can also
354:15 - access resources within the v-net
354:17 - without any additional
354:19 - configuration ilbs exposes the ass
354:21 - hosted apps on an IP address inside your
354:23 - v-net the internal in point is an
354:25 - internal load balancer so that's an
354:28 - overview of app service environment and
354:30 - Azure app
354:35 - service the next thing we'll be going
354:37 - over is deployment in Azure app service
354:39 - so what is deployment well it's the
354:41 - action of pushing changes or updates
354:43 - from a local environment or repository
354:45 - into a remote environment Azure app
354:47 - Services provides many ways to deploy
354:49 - your applications including run from
354:52 - package deploy zip or War deploy via FTP
354:55 - deploy via Cloud sync such as Dropbox or
354:57 - one drive deploy continuously with
354:59 - GitHub bitbucket and Azure repos which
355:02 - using kudu and Azure pipelines deploy
355:04 - using a custom container CI CD pipeline
355:06 - deploy from local git deploy using
355:08 - GitHub actions deploy using GitHub
355:11 - actions containers and deploy with
355:13 - template run from a package is when the
355:15 - files in the package are not copied to
355:17 - the WW root directory instead the zip
355:19 - package itself gets mounted directly as
355:21 - the re only ww rout directory all other
355:24 - deployment methods in app service have
355:26 - deployed to the following directory for
355:28 - Windows Deon we use back slashes home
355:31 - site ww root for Linux we use forward
355:34 - sles home site ww rout since the same
355:37 - directory is used by your app at runtime
355:40 - it's possible for deployment to fail
355:41 - because of file lock conflicts and for
355:43 - the app to behave unpredictably because
355:45 - some of the files are not yet
355:47 - updated zip and War file deployment uses
355:50 - the same kuduo service that powers
355:51 - continuous integration based deployments
355:54 - kudu is the engine behind get
355:55 - deployments in Azure app service it's an
355:57 - open source project that can also R
355:59 - outside of azure kudu supports the
356:02 - following functionality for zip file
356:04 - deployment deletion of files left over
356:06 - from a previous deployment option to
356:08 - turn on the default build process which
356:10 - includes package restore deployment
356:12 - customization including running
356:13 - deployment scripts deployment logs and a
356:16 - file size limit of 248 megabytes you can
356:20 - deploy using Azure CLI Azure API via
356:22 - rest and Azure
356:24 - portal you can use file transfer
356:26 - protocol to upload files you will need
356:28 - your own FTP client you just drag and
356:31 - upload your files go to the deployment
356:33 - Center get the FTP credentials for your
356:35 - FTP client you can use dropbox or one
356:39 - drive to deploy using a Cloud sync
356:41 - Dropbox is a third-party cloud storage
356:43 - service one drive is Microsoft's cloud
356:46 - storage service you go to deployment
356:48 - Center configure for Dropbox or one
356:50 - drive when you turn on sync it will
356:52 - create a folder in your Dropbox Cloud
356:54 - Drive one drive apps as your web apps
356:57 - drop box apps Azure this will sync with
357:00 - your home site ww root so you just
357:02 - update files in that folder in summary
357:05 - Azure app service offers a range of
357:07 - deployment methods ensuring flexibility
357:09 - and ease for
357:14 - developers the next topic we'll be
357:16 - covering is autoscale and Azure app
357:18 - service autoscaling is the process of
357:20 - adjusting a server infrastructure
357:22 - capability to fulfill incoming requests
357:24 - from your web application it usually
357:26 - takes seconds for the changes to take
357:28 - effect and can be done automatically
357:29 - according to preconfigured metrics it
357:31 - does not need any new deployment or
357:33 - coding changes scaling options in Azure
357:36 - app service horizontal scaling this
357:38 - involves adding or removing servers from
357:40 - your infrastructure for example during
357:42 - hight traffic periods you might scale up
357:44 - from 1 to three virtual machines when
357:46 - demand decreases you can reduce the C to
357:48 - minimize cost vertical scaling this
357:51 - suggest the resources of an existing
357:53 - server such as CPU memory or storage for
357:56 - example if you find that your app
357:57 - application is processing a large amount
357:59 - of data and needs more storage for logs
358:01 - you might opt to increase the storage
358:03 - capacity of your existing server overall
358:06 - Azure app service autoscaling
358:07 - dynamically adjusts resources to meet
358:09 - real time application
358:15 - demands the next thing we'll be covering
358:17 - is the Azure app service plan Azure app
358:20 - service plan determines the region of
358:21 - the physical server where your web
358:23 - application will be hosted and defines
358:24 - the amount of storage RAM and CPU your
358:27 - application will will use it offers
358:28 - several pricing tiers shareed tiers
358:31 - there are two shared tiers free and
358:33 - shared free tier provides this tier
358:35 - offers 1 GB of disk space supports up to
358:38 - 10 apps on a single shared instance
358:40 - provides no availability SLA and allows
358:42 - each app a compute quoto 60 minutes per
358:44 - day shared tier provides hosting
358:47 - multiple apps up to 100 on a single
358:49 - shared instance no availability s SLA is
358:52 - offered and each app gets a compute
358:53 - quote of 240 minutes per day it's worth
358:56 - noting that litx based instances are
358:58 - supported in this
358:59 - tier dedicated tiers basic standard
359:03 - premium premium 2 Premium 3 Bic offers
359:06 - more disk space unlimited apps three
359:09 - levels in this tier that offer varying
359:10 - amounts of compute power memory and disk
359:13 - storage standard allows scaling out to
359:16 - three dedicated instances guarantees
359:18 - 99.95% availability and also has three
359:21 - levels with varying resources premium
359:24 - provides the ability to scale up to 10
359:26 - dedicated instances and ensures
359:29 - 99.95% availability and it includes
359:31 - multiple Hardware level
359:33 - options isolated tier dedicated Azure
359:36 - virtual Network full Network and compute
359:38 - isolation scale out to 100 instances and
359:41 - availability SLA of
359:44 - 99.95% so the Azure app service plan
359:47 - lets you tailor your hosting environment
359:49 - and budget to fit your application
359:55 - needs the next topic will be going over
359:57 - is enabling diagnostic logging in azzure
359:59 - app service Azure provides built-in
360:02 - Diagnostics to assist with debugging an
360:04 - app Service app Diagnostics logging is
360:06 - an important part of any web
360:08 - application's operation it allows you to
360:10 - troubleshoot exceptions not exception
360:12 - errors alerts and warnings as well as
360:14 - track and improve the user experience
360:17 - with Azure Diagnostics logging you may
360:19 - log application events generated by your
360:22 - application web server logging with a
360:24 - raw version of requests made to your app
360:26 - only available for for the windows
360:28 - platform detailed error Pages saving
360:30 - copies of the error Pages presented to
360:32 - your user only available for the windows
360:34 - platform failed request tracing with
360:37 - detailed information regarding failed
360:39 - requests deployment logging logging
360:41 - detailed information about the
360:42 - deployment process in order to
360:44 - troubleshoot when a deployment fails to
360:47 - enable application logging for Windows
360:48 - apps in the Azure portal navigate to
360:50 - your app and select app service logs
360:53 - select on for either application logging
360:55 - or application logging or both the file
360:58 - system option is for temporary debugging
361:00 - purposes and turns itself off in 12
361:02 - hours The Blob option is for long-term
361:04 - logging and needs a blob storage
361:05 - container to write logs to you can also
361:08 - set the level of details included in the
361:10 - log as shown in the table below disabled
361:12 - this level doesn't capture any logs
361:14 - error at this level only error and
361:16 - critical logs are captured warning this
361:19 - level captures logs that are warning
361:21 - error and critical information this
361:23 - level encompasses a broader range of
361:25 - logs capturing info warning error and
361:27 - critical categories verbose this is the
361:30 - most detailed level capturing all
361:32 - categories Trace debug info warning
361:35 - error and critical in essence As you
361:37 - move from disabled to verbose the range
361:39 - of logs captured increases with verbose
361:41 - capturing the most comprehensive set of
361:43 - logs enable application logging for
361:46 - Linux container in app service log set
361:49 - the application logging option to file
361:51 - system in Quota specify the dis quota
361:53 - for the application logs in retention
361:56 - period set the number of days the log
361:58 - should be retained when finished select
362:00 - save it so asure app Services diagnostic
362:03 - logging is essential for optimizing
362:05 - troubleshooting and monitoring your
362:12 - application let's take a look at
362:13 - configuring Azure web app settings Azure
362:16 - web app settings can be configured
362:17 - through the Azure portal or the Azure
362:19 - CLI the Azure portal also offers a bulk
362:21 - editing option the main settings that
362:24 - may be configured for your Azure web app
362:25 - are the following t SL SL SSL settings
362:28 - to have a secure and encrypted
362:30 - Communication channel API settings such
362:33 - as technology stacker platform Settings
362:35 - app settings you can override your
362:37 - configuration stored on the web config
362:40 - connection strings in order to do not
362:42 - have it written on the web config
362:44 - default documents displays default web
362:46 - pages when accessing the root of your
362:48 - website URL path mappings configure
362:51 - settings according to the user OS
362:53 - overall Azure web app settings offer a
362:55 - simplified way to optimize and here your
362:57 - Azure web
363:00 - [Music]
363:03 - application hey this is Andrew Brown
363:05 - from exam Pro and we are going to be
363:06 - learning about Azure app services in
363:08 - this follow along uh and it's a service
363:11 - that's supposed to make it easy for you
363:12 - to deploy web applications I say
363:14 - supposed to because it really depends on
363:16 - your stack Azure has more synergies with
363:18 - other stacks and others so like if
363:20 - you're like me and you like Ruby on
363:22 - Rails you're going to find a lot of
363:23 - friction with rails and Linux but if
363:25 - you're using something like Windows
363:27 - servers or python orn net you're going
363:30 - to have a much easier time still really
363:32 - great service just wish they'd make it a
363:33 - bit more broad there but let's hop into
363:35 - it so before we can go use that service
363:37 - let's make sure that it's activated and
363:40 - so we'll go over here and we'll go to as
363:43 - your subscription and then down below
363:45 - we're going to go to Resource provider
363:46 - now you think what you could do is just
363:49 - type in app Services uh and you'd be
363:51 - wrong because the the service is under a
363:54 - particular provider so if you want to
363:56 - figure out what provider it is we can go
363:58 - um Azure resource
364:01 - providers and they have a page on
364:03 - documentation here that lists them all
364:06 - so if I search for Azure app
364:09 - Services it's under web and domain
364:11 - registration so we're going to make sure
364:13 - this is registered if we're using a
364:14 - custom domain which we are not today we
364:17 - need this one activated so going back
364:20 - here I will type in web and you can see
364:22 - it's registered so if yours is not
364:24 - registered go ahead and hit that I
364:26 - believe this by default is generally
364:28 - registered with new Azure accounts so I
364:31 - don't think that is an issue for you but
364:33 - we'll go back up here close these
364:35 - additional tabs and we will type in
364:36 - Azure app
364:39 - services and we will look for that
364:41 - service so there it is and we'll go
364:42 - ahead and hit
364:44 - add um and so I'm going to give it a new
364:47 - name I just made it a moment ago but I'm
364:50 - going to try again and try to use the
364:51 - same name so we're going to call this
364:55 - Voyager Great and then I'm going to go
364:57 - ahead and name this Voyager and I
364:59 - already know that that is taken so I'm
365:00 - going to type in Delta
365:02 - Flyer and these are fully qualified
365:05 - domain so they are unique with Azure app
365:07 - Services you can run a Docker container
365:09 - we're doing code this time around and
365:11 - what I like to use is Ruby um but again
365:14 - you know if I want to use the cicd I'm
365:16 - not going to be able to use the
365:17 - deployment center with Ruby so that is
365:19 - not possible um and so we're going to go
365:21 - with python and run either a flask or a
365:23 - Django app I haven't decided yet I am in
365:26 - Canada so let's go to Canada east and uh
365:30 - down below here we have the plans
365:32 - generally the plans will tell you the
365:34 - cost underneath look you'll notice that
365:35 - it's loading but I just want to show you
365:37 - that there are some discrepancies in
365:39 - terms of pricing so if I was to go to
365:41 - Azure app Services
365:44 - pricing and we were to pull this up here
365:47 - we can kind of see the pricing
365:49 - here okay and if we scroll on down right
365:52 - now we're looking at a premium V2 uh and
365:56 - oh no I don't need help I'm okay you'll
365:58 - notice that it's 20 cents per hour so if
366:00 - I go here and do that times 730 because
366:03 - there's 730 hours in the year that's
366:06 - $146 I believe this is showing me in USD
366:09 - dollars yeah and in here it's showing
366:13 - me03 Canadian which is lower um so it
366:17 - could be that because I'm running in a
366:18 - Canada east region it's the price is
366:21 - different but you could imagine that if
366:23 - I had this at this cost at uh what did
366:26 - we say here um at 146
366:30 - USD to CAD I'd actually be paying
366:34 - $182 so you got to watch out for that
366:36 - kind of stuff but I'm pretty sure this
366:38 - is what the cost is so just be aware
366:40 - that if you look stuff up in here it's
366:42 - not necessarily reflective so you got to
366:44 - do a little bit more work to figure that
366:45 - out uh if we wanted to go here uh we
366:48 - cannot choose the free tier when we're
366:49 - using Linux if we're using Windows I
366:51 - believe we can use it we're working with
366:53 - Linux today so that's just how it's
366:54 - going to be um for the B1 this is
366:57 - totally fine but we want to utilize
366:59 - deployment slots deployment slots is an
367:01 - advanced feature of uh the production
367:03 - version and that's the only way we're
367:05 - going to be able to use it here this is
367:06 - 20 cents per hour again so I don't want
367:09 - to be doing this for too long but I
367:11 - think what we'll do is before we do that
367:13 - we can just do an upgrade to devb to
367:15 - prod so we can experience that I'm going
367:17 - to go and just choose B1 okay so we go
367:21 - next um we do not need any application
367:24 - insights for the time being and it will
367:25 - not let us so it's okay we'll go next
367:28 - review and
367:30 - create and we'll go ahead and create
367:32 - this resource here and I will see you
367:35 - back when this is done so um our
367:39 - resources Now set up we'll go to
367:40 - Resource and now that we're in here
367:43 - you'll notice if we hit browse we're not
367:44 - going to see anything because we do not
367:46 - have anything deployed which makes sense
367:48 - right uh so we're going to actually have
367:50 - to go ahead and deploy something so we
367:52 - are going to make our way over to the
367:53 - deployment
367:55 - Center and uh it's just going to tell us
367:57 - that we have yet to configure anything
367:58 - and that's totally fine we're going to
368:00 - go to
368:01 - settings it'll give it a moment and so
368:04 - the thing is is that we're going to need
368:06 - something to deploy um I did not create
368:08 - an app but the great thing uh is in the
368:11 - Azure documentation they have a bunch of
368:13 - quick starts here all right and
368:15 - apparently they have one for Ruby as
368:16 - well but today we are looking at python
368:19 - uh and so they actually have an example
368:22 - repository for us here which is
368:25 - github.com asure samples python docs
368:28 - hello world and I mean I could go make a
368:30 - repo for you but we might as well just
368:31 - use the one that is already provided to
368:33 - us so I'm just going to pull this up to
368:36 - show you what's in it it's a very very
368:37 - simple application even if you don't
368:39 - know anything about building web apps
368:41 - I'm going to walk you through really
368:42 - easily here okay so we're going to open
368:44 - up app.py so we are using flask if
368:47 - you've never heard of flask it is a very
368:49 - minimal python framework for creating
368:51 - web apps uh very uninspiring uh homepage
368:55 - here but it gets the job done it's going
368:57 - to create a default route for us which
369:00 - uh we have there we're going to call
369:02 - hello here and we're going to have hello
369:04 - world so that's all that's going on here
369:07 - very very simple and we have a
369:09 - requirements this is our package manager
369:11 - I don't know why python uses dxt files
369:13 - is very outdated to me but that's what
369:14 - they use and here we have
369:17 - flask all right so we're going to use
369:20 - that repo it's a public repo so it
369:22 - should be very easy for us to connect so
369:24 - we'll drop down go to GitHub
369:28 - and uh the next thing we need to do is
369:30 - authorize GitHub all right so I ran into
369:32 - a bit of trouble there because I could
369:34 - not uh authenticate my uh GitHub account
369:37 - but you know what I just made another
369:39 - GitHub account so that made it a lot
369:40 - easier I'm going to go ahead here hit
369:42 - GitHub and we're going to try to
369:43 - authorize it and so now I'm logged into
369:46 - this new one called exampro Dev and
369:48 - we'll go ahead and authorize this
369:50 - application and we're now in good shape
369:53 - this repository doesn't have anything in
369:55 - it so um if I want to clone something I
369:58 - guess I'll probably have to Fork that
370:00 - repo so we'll give it a moment to
370:02 - authorize and while that's going I think
370:04 - that's what I'm going to do I'm going to
370:05 - go and uh Fork the example repo if I can
370:09 - find the link again here uh
370:13 - myself uh I
370:17 - believe it
370:19 - is that's still authorizing over there
370:21 - I'm still looking for it so it was like
370:24 - examples or something samples or examp
370:28 - examples all right so I found a way
370:30 - around the problem I just made a new uh
370:32 - GitHub account so that's all I had to do
370:34 - um and I just won't be using my primary
370:36 - account until I get my phone back but um
370:38 - so what we'll do is go hit connect I'll
370:40 - hit
370:41 - authorize and it didn't prompt me
370:43 - because it already connected to this new
370:45 - one called exam prodev you might have to
370:46 - put your credentials in here and it's
370:48 - going to ask me to select some things
370:50 - it's a new account so there are no
370:51 - organizations there are no repositories
370:53 - there are no branches totally brand new
370:56 - so what I'm going to need to do is get a
370:58 - repo in there so we'll just go ahead and
371:00 - Fork the Azure samples one so that is
371:03 - azure samples
371:06 - python docs hello
371:09 - world and if I type that right we're in
371:11 - good shape I'm going to go ahead and
371:13 - Fork this
371:15 - repository I'll say got it and then I'll
371:18 - move this off screen here this is now
371:20 - cloned you should see it cloned here and
371:24 - we'll go back here and this probably
371:26 - isn't live so there's no refresh button
371:29 - here so we'll have to hit discard and we
371:31 - will give this another go here and we
371:34 - will select our organization which is
371:36 - our name there is the
371:38 - repository uh should be main branch this
371:40 - is kind of outdated I'm sorry but it's
371:42 - called Master that's what it is not my
371:44 - fault that's azure's fault okay um and I
371:49 - think that's it I don't know if we need
371:51 - a workflow configuration file I don't
371:54 - think
371:55 - so going to double check here no I don't
371:58 - think so and uh what we'll do is we'll
372:00 - just go ahead and save
372:06 - that and so now we are set up for
372:14 - [Music]
372:18 - deployment all right so now that that's
372:20 - all hooked up if we were to go to browse
372:22 - we're actually still seeing the default
372:23 - page a deployment hasn't been triggered
372:26 - just yet yet uh so the way it works is
372:28 - it's using GitHub action so if we click
372:30 - into our I'm going to call it main
372:31 - branch I know they got the wrong name
372:33 - but uh we're going to click into our
372:35 - GitHub workflows and then below here we
372:36 - can see we have a yaml file uh and this
372:39 - is for GitHub actions integration here
372:41 - and so what it's doing is it's
372:42 - specifying the branch uh what how it's
372:45 - going to uh build it's going to run onto
372:47 - buntu latest the steps it's going to do
372:49 - it's going to check it out it's going to
372:51 - set up the python version it's going to
372:53 - build it it's going to do that stuff and
372:55 - so in order for this to um take action
372:59 - we'd actually have to go ahead and make
373:01 - some kind of manual change which we have
373:03 - yet to do so okay so what we'll do is
373:06 - we'll go back to our main
373:09 - here and uh it should be as simple as uh
373:13 - just changing something here so it's not
373:16 - I'm not sure how it's supposed to know
373:17 - that it's supposed to be doing the hello
373:19 - we oh I guess yeah sorry so this means
373:21 - it's going to Route over to here um so
373:23 - I'm just going to make any kind of
373:24 - change here doesn't matter what it is
373:26 - just one space we'll go ahead and give
373:28 - it a
373:29 - commit and um if I go back to my latest
373:34 - commits we should see that I made that
373:35 - change there it is we'll go back over
373:37 - here and this should be
373:42 - deploying um so if we go over to logs
373:45 - here you can see one's in progress right
373:46 - now okay and so that's what we're
373:48 - waiting we're just going to see that
373:50 - finish there we could probably open the
373:51 - logs and get some more information there
373:53 - and so it just brings you back over to
373:55 - GitHub actions and say here's GitHub
373:57 - actions and it's performing the stuff
373:58 - here so we're just going to give it time
374:01 - here and I'll see you back in a moment
374:03 - so we didn't have to wait too long it
374:04 - only took 1 minute and 29 seconds if we
374:06 - go back over here um we might need to do
374:09 - a refresh and so we can see this is
374:11 - reflected over here and so if we go back
374:15 - to it doesn't really matter if we go to
374:16 - settings or logs here but I'm going to
374:17 - hit browse and see if my page is
374:19 - deployed it still is not so we do have a
374:22 - small little problem here and it's
374:23 - really going to just have to do with how
374:24 - the app is served so that's what we need
374:26 - to figure out next all right so our app
374:28 - is not currently working and uh there's
374:30 - a few approaches we can take and the
374:32 - thing I can think right away is we
374:33 - should go and SSH into that instance if
374:36 - you scroll on down here from developer
374:37 - tools you can go to SSH and click this
374:40 - button and that's going to SSH you right
374:42 - into that machine right away you can
374:44 - also uh access SSH via the um CLI
374:49 - command so I believe it's
374:51 - like it's like a web
374:55 - app
374:56 - um SSH it'll do the exact same thing you
374:59 - do that from the cloud shell but that's
375:01 - not what we're doing today if I give
375:03 - this an LS in here and we're in Linux we
375:05 - can see we have our app here and uh what
375:07 - I would do is I would see what's running
375:09 - so I I would do a puma uh or sorry not
375:12 - Puma PS Ox grep uh Python and you can
375:17 - notice that we have a g unicorn that's
375:19 - running so that is where our python
375:21 - instances are running so you're not
375:23 - looking for flas you're looking for
375:24 - python here and and if we wanted to make
375:27 - sure that was working we just type in
375:28 - curl Local
375:32 - Host um and so that is going to return
375:35 - up Port 80 so that tells me that because
375:37 - like curl just means like let's go look
375:39 - at that page um it should return some
375:41 - HTML like print out the HTML to us so
375:44 - that means the app is not running um so
375:46 - what you could do is run flask run and
375:48 - it's going to start
375:50 - on Port
375:52 - 5000 right so what I can
375:55 - do
375:56 - is I can go up uh back to my deployment
375:59 - Center
376:00 - here and I'm going to go get that link
376:04 - here I just ignore the fact that it's
376:06 - working uh it's it's not working right
376:08 - now I know for certain it's not um but
376:10 - if we do 5,000 that won't resolve
376:12 - because Port 5,000 isn't open so we
376:14 - can't really just uh put 5,000 in there
376:17 - and the default server here would be
376:19 - 5,000 so if I stop this and I specify
376:22 - Port
376:24 - 80 right
376:27 - then this will start up the app on Port
376:28 - 80 and so now when you go
376:31 - here okay it will work uh this is not a
376:34 - great way because of course as soon as
376:35 - you kill it here uh technically the S
376:38 - should stop running um and so you'll run
376:40 - into that step uh so what we need to do
376:42 - is provide a configuration to gunicorn
376:46 - which is a python thing again it's not
376:47 - so important that you know how like what
376:49 - these things are but the idea is that
376:50 - you understand as administrator you want
376:52 - to make sure you have an app that runs
376:54 - after you do a deploy so in this
376:56 - particular one we need a startup. txt uh
376:59 - and interestingly enough there is a
377:02 - example code by the same author of the
377:03 - other one we were looking at here I
377:05 - believe it's the same
377:06 - person or it might not be but uh they
377:09 - have a startup txt right and so in here
377:12 - you can see that it binds on Port 0000
377:14 - it starts up for workers starts up the
377:16 - app all right um and so that's something
377:19 - that we can go ahead and do so uh what I
377:23 - will do is I will go back to my GitHub
377:25 - repository
377:29 - that we have
377:30 - here and I can just go ahead and add a
377:33 - new file so I'm going to say
377:36 - um add a file create a new file here
377:40 - we'll call it startup.
377:42 - txt I'm going to copy this command here
377:45 - and paste it in there so gunicorn will
377:48 - bind the workers and startup on the app
377:51 - um startup app is being ran by uh
377:54 - something here so if I go back here I I
377:56 - think they have a startup High here and
377:58 - that's all that it is doing um I think I
378:02 - want
378:03 - to I could do it this way I suppose let
378:06 - me just see here there's is a slightly
378:09 - different I so they actually have like a
378:11 - full app going on
378:12 - here and I just want a very simple flask
378:15 - app so I think what I can
378:17 - do is put flask run
378:23 - here Port
378:24 - 80
378:26 - and that should start up the app there
378:28 - I'm going to go ahead and commit that
378:30 - file okay and as soon as I commit that
378:33 - if I go back to my actions it created
378:35 - that startup file there so it should
378:37 - trigger a
378:39 - build it's queued
378:41 - up um and I'll just put this tab up here
378:45 - so we'll be back here in two seconds and
378:47 - if I give this a nice refresh yeah you
378:49 - can see it deploys in progress so uh
378:52 - this doesn't take too long we'll just
378:53 - wait close that there we'll just wait a
378:55 - few minutes we click logs it just opens
378:57 - it back up here and we'll see how that
378:59 - goes all right so uh your deploy may
379:01 - have finished there but the thing is is
379:02 - that we're not going to really know if
379:04 - uh a change has taken effect unless we
379:06 - actually go ahead and update our code so
379:08 - what I want you to do is go to your code
379:10 - tab go to your app.py we'll hit edit and
379:13 - I'm going to go ahead and change this to
379:16 - Vulcan and then we'll scroll on down hit
379:19 - commit changes and we'll make our way
379:21 - back over to our deployment Center and
379:24 - we'll give it a refresh here and we're
379:26 - just going to wait until this one is
379:27 - complete and we will double check to
379:29 - make sure that that is changed if it is
379:30 - not we will take action to fix that okay
379:34 - all right so we just waited a little
379:35 - while there for that deploy to happen
379:37 - and if we go to our website here it is
379:41 - taking effect so that's all we had to do
379:42 - to get it working so that's pretty good
379:44 - um so that is uh
379:49 - deployment so let's talk about
379:51 - deployment slots in order to utilize
379:54 - this feature we're going to actually
379:55 - have to upgrade our account because we
379:56 - cannot utilize them at this uh the basic
379:59 - plan here we got to go to standard or
380:01 - premium so let's go ahead and give that
380:03 - an
380:03 - upgrade uh so here's the B1 we're going
380:06 - to go to production here um and I
380:10 - think yeah we're going to have to choose
380:13 - this one here uh very expensive so the
380:16 - thing is we're going to just upgrade it
380:17 - temporarily unless there's more options
380:18 - down below that are
380:21 - cheaper yeah these are the standard
380:23 - tiers let's go with this one here
380:26 - because it's only $880 again we're not
380:28 - going to be doing this for long but I
380:29 - want to show you how to do staging slots
380:31 - and auto scaling okay so we'll go ahead
380:33 - and apply that
380:35 - there and now it says that it's applied
380:38 - so if I go back to our app here and we
380:40 - click on deployment slots sometimes it
380:42 - doesn't show up right away if it doesn't
380:44 - that's not a big deal you just wait a
380:46 - bit but today it's super fast so we're
380:48 - going to go ahead and add a new slot
380:50 - we're going to call it uh staging we're
380:52 - going to deploy from our production
380:54 - Branch here and I'm going to go ahead
380:55 - and create that
381:00 - there and we'll just wait until that's
381:02 - done
381:04 - okay great so we waited a little bit
381:06 - there and uh our slot is created so I'm
381:09 - going to just hit close there and so now
381:12 - let's go take a look and see if we can
381:13 - actually see the application here so I
381:15 - just clicked into it I click browse and
381:17 - we're getting the default page so
381:18 - nothing is actually really deployed to
381:20 - it uh so how are we going to do that
381:23 - that's the the main question here um so
381:26 - what I'm going to do is I'm going to
381:28 - make my way over to the deployment
381:32 - Center and you can see that it's not
381:35 - configured for the slot so we are going
381:38 - to have to set it up all over again even
381:39 - though it copied over configuration
381:41 - settings it didn't copy over the code so
381:43 - we go to GitHub we'll choose our
381:45 - organization again I'm going to choose
381:47 - the repository we're going to choose
381:49 - that main branch again there we're going
381:51 - to let it add a workflow and notice that
381:53 - this time it's going to call it staging
381:54 - do yaml so there'll be separate workflow
381:56 - that gets created we're going to go
381:58 - ahead and save that
382:04 - there and what we can do is again click
382:06 - onto our Branch name there and if we
382:10 - click into our workflows we'll not now
382:12 - notice that we have a staging example
382:13 - it's the same thing um but it should be
382:16 - able to now deploy so the whole purpose
382:19 - of um these deployment branches is that
382:22 - it helps us uh we can deploy different
382:24 - versions of our apps but also um it's
382:26 - just a place where we can uh uh view
382:29 - things before we actually roll them out
382:31 - so we want to make sure 100% that they
382:32 - are working correctly um I don't think
382:35 - this will automatically push out let me
382:36 - just go to my actions to see if this is
382:38 - deploying notice that we have two
382:40 - workflows now we have staging here uh
382:42 - and yeah it looks like it's going to
382:43 - deploy here so we'll just wait a little
382:45 - bit um but maybe what we can do is try
382:48 - to have a slightly different version uh
382:50 - for each one here okay but we'll just
382:52 - let that finish and I'll see you back in
382:54 - a moment
382:58 - all right so our deploy finished there
383:00 - so now if we go back to our website here
383:02 - we go browse we should see that
383:04 - application it says hello Vulcan and if
383:06 - we go and take out this we still have
383:08 - hello Vulcan so how can we have a a
383:11 - variant of this so that we can push out
383:13 - to that so what I'm going to do is I'm
383:14 - going to go back to my application here
383:16 - I'm going to go to code and I'm just
383:18 - going to make a minor change um I don't
383:21 - say also is that spelled right
383:24 - startup that does look correct to me um
383:27 - so maybe I'll go ahead and adjust that
383:29 - file but it doesn't seem to be affecting
383:30 - anything which is I'm a bit surprised
383:32 - there so what I'll do is I'm going to go
383:36 - and edit that file and give it the
383:38 - proper name can I rename this file yes I
383:42 - can so we'll call that startup
383:44 - file I thought we need that for
383:46 - deployment I guess it just works without
383:47 - it which is nice uh if we go back here
383:50 - I'm going to go and I actually just want
383:51 - to edit my um app here again and I'm
383:55 - going to go and edit this and we'll say
383:59 - um hello
384:02 - Andor or hello andorians
384:07 - maybe and so if I go back to my actions
384:09 - the question what is it deploying is it
384:10 - going to deploy the production or the
384:13 - staging and it looks like it's going to
384:18 - do
384:19 - both looks like it's doing both here but
384:22 - one way we could tell is we can go to
384:24 - our logs here and we can see that um so
384:27 - we did two deploys so there's one change
384:29 - here uh if we go back to our main
384:31 - application and our deployment Center
384:35 - here and we go over to our
384:38 - logs you can see that they're both
384:40 - deploying so it doesn't seem like it's a
384:43 - great thing that that's how it works so
384:45 - the question is is then how would we um
384:48 - facilitate that deploy right how could
384:50 - we do that I suppose what we could do is
384:52 - just make a separate staging Branch um
384:55 - so if I go over to code
384:57 - here um I don't think we can just make
385:00 - branches through here so what I'm going
385:02 - to have to do is go ahead and oh I can
385:05 - create a branch right here so we'll just
385:06 - type in staging and we'll go create
385:08 - ourselves a new
385:11 - branch and now we are in this branch and
385:13 - what I'm going to do is go ahead and
385:15 - modify this and we're just going to call
385:17 - this
385:20 - um hello
385:23 - Klingons okay we'll go ahead and update
385:26 - that and so this should be a separate
385:27 - Branch so you think what we could do is
385:29 - go in and just change our settings so
385:31 - that it deploys from that one uh we'll
385:34 - go back to our deployment slots we'll
385:36 - click into staging
385:38 - here and we need to change our
385:41 - configuration settings um I think we
385:45 - could just do it
385:46 - [Music]
385:47 - from here hold on here I I could have
385:49 - swore it specified the branch if we go
385:51 - to deployment Center here I think it's
385:53 - set up on that other branch there I
385:56 - think we just adjust it here so yeah I
385:58 - think we could just um adjust these
386:02 - settings um we can't discard them but
386:07 - maybe what we can do is just go in and
386:10 - modify that file so we will go into our
386:13 - code
386:15 - here and uh we will go ahead and click
386:18 - into here go into staging and we'll just
386:21 - change what the branch is
386:23 - called so we'll just say staging
386:30 - and we'll hit start commit and we will
386:32 - save
386:33 - that and we'll see if it actually
386:35 - reflects those changes there so we will
386:37 - go here and hit
386:42 - refresh we'll see if it picks up staging
386:44 - now if we go to
386:46 - settings it's not picking it up so um
386:49 - I'm not sure I don't think perform a
386:51 - redeploy operation we don't want to
386:53 - redeploy so maybe what we'll do is just
386:55 - we'll have to do a disconnect here
386:56 - because it's collect it has the wrong
386:58 - one here so save workflow file
387:03 - um okay we'll just go ahead and delete
387:05 - it it's not a big deal we'll just have
387:06 - to make a new one
387:07 - here we'll go to GitHub we'll choose our
387:11 - uh organization again or repository our
387:14 - staging Branch this time around we'll
387:16 - let it add one see it says we can use an
387:18 - available workflow so we could have kept
387:19 - it there and added it there um and we'll
387:22 - go ahead and save that so now we'll have
387:25 - two separate branches there and we'll
387:28 - give that some time to deploy because
387:30 - that will now trigger a deploy off the
387:31 - bat and so I'll see you back here in a
387:33 - moment all right so after a short little
387:35 - wait here it looks like our app is done
387:37 - deploying so we'll go over here we'll
387:38 - make sure that this is our staging
387:39 - server is good and we want to see that
387:42 - our production is different perfect so
387:44 - we now have a way to deploy to each one
387:46 - but imagine that we want to swap our
387:48 - traffic so we're happy with our staging
387:50 - server we want to roll that out to
387:52 - production and that's where we can uh do
387:54 - some SWAT and so what we'll do is click
387:56 - the swap button and we're going to say
387:58 - the source is the staging and this is
388:00 - our Target production and we're going to
388:01 - perform that swap uh right now we can't
388:04 - do a preview because we don't have a
388:06 - particular setting set that's okay and
388:09 - it's kind of showing if there are any
388:10 - changes so set of configuration changes
388:12 - we don't have any so that's totally fine
388:14 - as well we'll go ahead and hit
388:17 - Swap and that's going to swap those two
388:19 - I believe it's has zero downtime so we
388:22 - will be in good shape if that happens
388:24 - there
388:27 - and we'll just give it a moment to do
388:31 - that great so after a short little wait
388:33 - there the swap is complete and so uh if
388:36 - we remember
388:37 - clearly this was our production right
388:40 - and so if I was to hit refresh it so now
388:42 - say Klingons and if I go to my staging
388:44 - server it should be the other way around
388:47 - right good so now imagine that I want to
388:50 - just split the traffic uh that's
388:51 - something else that we can do um so
388:53 - notice over here we have these percent
388:55 - is here um not sure why it won't let me
388:57 - change
388:59 - those so maybe I'll have to look into
389:01 - that so I'll be back so I'm not sure why
389:03 - it's not showing us that traffic slot
389:05 - there but what I'm going to do is just
389:06 - maybe try to trigger deploy back into
389:08 - our staging and maybe that's what it
389:10 - wants to see um so what I'm going to do
389:12 - is go back to my code here we'll be in
389:14 - our staging Branch here I'm going to go
389:16 - ahead and uh edit this file here and we
389:20 - will just change this to
389:23 - borans
389:26 - and we will hit
389:28 - update and we will let that go ahead and
389:31 - deploy so we go to actions here we can
389:33 - see that it is
389:34 - deploying um and we'll just give it some
389:36 - time okay so see you back here in a bit
389:39 - I mean the other reason could be that
389:40 - we're just not at the main level hold on
389:44 - here uh if we go back here to deployment
389:47 - slots you know what I think it's just
389:49 - because I was clicked into here and then
389:51 - I was clicked into deployment slots that
389:53 - they're both grayed out yeah it is so we
389:55 - can actually do it that top level there
389:57 - doesn't hurt to do another deploy though
389:58 - so um we'll just wait for I'll wait for
390:00 - that deploy to finish and then we'll
390:02 - come here and uh adjust that there okay
390:06 - all right so let's take a look at uh
390:07 - doing some traffic switching here so
390:09 - right now if we were to go to our
390:11 - production we have Klingons and if we
390:13 - were to uh go to our
390:19 - staging we have Boran so imagine that we
390:22 - only want 50% of that traffic to show up
390:24 - so what we can do is put in
390:26 - 50% and what I'm going to do is um do I
390:31 - hit enter here or oh sorry save up here
390:33 - there we go um and so what's going to
390:36 - happen is this should take effect I
390:38 - think right away yep uh and so now we
390:41 - have 50 50 50% chance of getting
390:43 - something else here um so I'm just going
390:45 - to keep on hitting enter here if that
390:47 - doesn't work we can try an incognito tab
390:50 - and there we go we got the opposite
390:52 - there and so this is serving up staging
390:54 - right
390:55 - uh and this is serving up production but
390:57 - they're both on the production URL so
390:59 - that's a way you can split the traffic
391:02 - so uh that's pretty much all I wanted to
391:04 - show you for deployment slots let's now
391:05 - talk about
391:09 - scaling hey this is Andrew Brown from
391:12 - exam Pro and in this section we'll be
391:13 - covering Azure container instances Azure
391:16 - container instances allow you to launch
391:18 - containers without the need to worry
391:19 - about configuring or managing the
391:21 - underlying virtual machine aure
391:23 - container instances is design designed
391:25 - for isolated containers they are
391:26 - tailored for simple applications task
391:28 - Automation and tasks like build jobs
391:31 - containers can be provisioned within
391:33 - seconds whereas VMS can take several
391:35 - minutes containers are built per second
391:37 - whereas VMS are built per hour providing
391:39 - potential cost savings containers have
391:41 - granular and custom sizing of vcpus
391:44 - memory and gpus whereas VM sizes are
391:47 - predetermined ACI can deploy both
391:49 - windows and Linux containers you can
391:51 - persist storage with Azure files for
391:53 - your ACI containers
391:55 - once deployed AIS are accessible via a
391:57 - fully qualified domain name like custom
391:59 - label. aure region. aure
392:02 - container. Azure provides quick start
392:04 - images to start launching example
392:06 - applications but you can also Source
392:08 - containers from Azure container registry
392:10 - Docker Hub or even privately hosted
392:12 - container
392:13 - registry container groups are collection
392:15 - of containers that get scheduled on the
392:17 - same host machine the containers in a
392:20 - container group share life cycle
392:21 - resources local network and storage
392:24 - volumes
392:25 - container groups are similar to a
392:26 - kubernetes pod multi-container groups
392:29 - currently support only Linux containers
392:31 - there are two ways to deploy a
392:33 - multi-container group to deploy a
392:35 - multi-container group you can use either
392:36 - a resource manage or template if
392:38 - deploying additional Azure service
392:40 - resources or a yml file for deployments
392:42 - involving only container instances
392:45 - overall Azure container instances
392:47 - simplify container deployment and
392:48 - scaling removing the complexities of
392:50 - infrastructure
392:53 - management
392:56 - the next topic we'll be going over our
392:58 - container restart policies a container
393:01 - restart policy specifies what a
393:02 - container should do when their process
393:04 - has completed these policies ensure that
393:06 - the container instances can handle
393:08 - different scenarios effectively based on
393:10 - the specific requirements of the
393:11 - application or task Azure container
393:14 - instances has three restart policy
393:16 - options always this policy ensures that
393:19 - the containers restart continuously
393:21 - regardless of whether they exit
393:22 - successfully or not it's useful for app
393:24 - applications that need to be constantly
393:26 - available such as web servers never with
393:29 - this policy containers do not restart
393:31 - once they've completed their execution
393:33 - this is ideal for tasks that are
393:35 - designed to run once and then terminate
393:36 - such as batch jobs or scheduled tasks on
393:39 - failure containers will only restart if
393:41 - they stop due to an error or unexpected
393:43 - termination this ensures that if a
393:45 - container crashes or faces an unexpected
393:47 - error it will try to restart and
393:49 - continue its operations overall choosing
393:52 - the appropriate restart policy is vital
393:54 - for the the stability and responsiveness
393:55 - of your
394:01 - applications the next topic we'll be
394:03 - covering our container environment
394:05 - variables environment variables are key
394:07 - value pairs that can be used to
394:08 - configure and manage the behavior of
394:10 - applications running inside containers
394:13 - environment variables allow you to pass
394:15 - configuration details to your containers
394:17 - which can be critical in guiding
394:18 - applications on how to connect to
394:20 - databases where to find certain
394:21 - resources or how to adjust their
394:23 - behavior based on the environment
394:24 - they're running in in Azure you can
394:26 - easily set up these environment
394:28 - variables for your containers using the
394:29 - Azure portal CLI or Powershell secured
394:33 - environment variables by default
394:35 - environment variables are stored in
394:36 - plain text to address this Azure offers
394:39 - the option to secure your environment
394:40 - variables instead of storing them in
394:42 - plain text which could expose sensitive
394:44 - information if breached you can Leverage
394:46 - The secure environment variables flag so
394:49 - that's a quick overview of container
394:51 - environment
394:53 - variables
394:57 - the next topic we'll be covering is
394:59 - container troubleshooting
395:00 - troubleshooting containers in Azure
395:02 - involves a series of commands that help
395:04 - diagnose and resolve issues as container
395:06 - logs this command lets you fetch logs
395:08 - from your container these logs can
395:10 - provide insights into application
395:12 - behavior and possible errors as
395:14 - container attach if you need diagnostic
395:16 - data during container startup use this
395:18 - command it helps in understanding issues
395:20 - that might arise during the
395:21 - initialization phase of a container as
395:24 - container EXA for a deeper dive into the
395:26 - Container this command starts an
395:28 - interactive session this is useful for
395:30 - live debugging and to inspect the
395:31 - container's current state as monitor
395:34 - metrics list this command gives you
395:35 - metrics related to your container such
395:37 - as CPU usage which can be essential for
395:39 - performance tuning or identifying bottle
395:41 - X so these are the commonly used
395:44 - commands for container
395:46 - [Music]
395:50 - troubleshooting hey this is Andrew Brown
395:52 - from exam Pro and we're going to take a
395:53 - look at a your container instances so
395:55 - here it is so all we got to do is go to
395:58 - container instances we'll hit add and
396:00 - the nice thing is that Azure provides us
396:02 - with a Hello World one so it's very easy
396:03 - for us to get started um it's a Linux
396:06 - machine and it looks like it's pretty
396:08 - inexpensive there so we'll stick with
396:10 - that I'm going to create a new group
396:12 - here we're going to call it banana um
396:15 - and we'll name the container instance
396:18 - banana and East Us 2 seems fine to me
396:21 - you'll notice we're on a quick start
396:23 - image if we wanted we could use
396:24 - something from the docker Hub and
396:26 - provide our own link but we'll just take
396:28 - with the quick uh start image for today
396:31 - we're going to go ahead and hit next to
396:33 - networking just to see what we have as
396:35 - options you can make it public or
396:37 - private we'll go to Advanced hold on
396:40 - here yep those are just the ports you
396:41 - can expose we'll go to advance and for
396:44 - the restart policy we can set on failure
396:46 - always or never we can pass in
396:48 - environment variables and I covered this
396:50 - a lot more in detail in the lecture
396:52 - content so we don't need to really dive
396:54 - deep into this um and we'll go ahead and
396:57 - create this
396:59 - instance and so we'll have to wait a
397:01 - little while here and I'll see you back
397:02 - in a moment okay and so after a short
397:05 - wait our container instance is ready
397:07 - we'll go to that resource there and take
397:10 - a look around so on the left hand side
397:12 - we can go to our containers and there we
397:13 - can see it running we can see the events
397:16 - down below of what's going on so you can
397:18 - see that it's pulled the image it
397:20 - successfully pulled it and it started
397:22 - the container some properties nothing
397:25 - interesting there the logs if we wanted
397:27 - to see stuff and if we wanted to connect
397:28 - to the instance we could also go here
397:30 - and hit connect which is kind of nice um
397:33 - I don't have any purpose to do that
397:34 - right now so and it's also not going to
397:36 - work the way we're doing it but I just
397:38 - wanted to show you you had those
397:40 - opportunities uh you can do identity so
397:42 - that means manage it with ro base access
397:45 - controls but what I want to see is
397:47 - actually this uh hello world working I'm
397:49 - assuming that must be a a hello page
397:51 - I've never looked at it before so we're
397:53 - going to go here grab the public IP
397:55 - address and paste it on in the top and
397:57 - there we go so we have deployed a
398:00 - instance onto Azure container instances
398:03 - or a container I should say so nothing
398:05 - super exciting to talk about here um but
398:08 - we do need to know the basics uh there
398:11 - um if we wanted to deploy other
398:13 - containers it's just the one there so
398:15 - that's all you really need to do um but
398:18 - yeah so yeah hopefully that uh gives you
398:20 - an idea there I'll just go back to the
398:22 - list here so we can see it and we'll go
398:24 - ahead and just uh delete that probably
398:26 - do it for the vi the resources on the
398:28 - left hand side like I always like to do
398:31 - uh and we will go into banana
398:33 - here and we will delete banana and there
398:37 - you
398:40 - [Music]
398:43 - go hey this is Andrew Brown from exam
398:46 - Pro in this section we're diving into
398:48 - the Azure container registry Azure
398:50 - container registry is a managed Docker
398:52 - registry service based on the open
398:54 - Source Docker registry 2.0 it's designed
398:56 - for building storing and managing
398:58 - containerized applications and images
399:00 - use Azure container Registries with your
399:02 - existing container development and
399:04 - deployment pipelines use Azure container
399:07 - registry tasks to automate image builds
399:09 - directly in Azure when you commit code
399:11 - you can seamlessly pull images from ACR
399:13 - for deployment to various orchestrators
399:15 - such as kubernetes DC OS and Docker
399:18 - swarm many Azure services including
399:21 - Azure kubernetes service Azure app
399:23 - service Azure B and Azure service fabric
399:25 - offer direct support to interface with
399:27 - ACR this ensures a cohesive workflow for
399:30 - deploying applications developers can
399:32 - also push container images to a
399:34 - container registry as part of a
399:35 - container development workflow with
399:37 - delivery tools such as Azure pipelines
399:39 - and Jenkins ACR offers various
399:42 - interfaces for interaction you can
399:44 - manage and configure it using the Azure
399:46 - CLI Azure Powershell Azure portal Azure
399:49 - SDK or even the docker extension for
399:51 - visual studio code in conclusion azure
399:54 - container registry securely and
399:55 - efficiently manages your Docker images
399:57 - streamlining build storage and
400:03 - deployment the next topic will be
400:05 - covering our Azure container registry
400:07 - tasks ACR tasks are designed to automate
400:10 - the process of patching the operating
400:12 - system and Frameworks within your Docker
400:14 - containers for quick tasks ACR tasks let
400:17 - you instantly push a specific container
400:19 - image to azure's container registry
400:21 - without needing a local Docker engine
400:22 - installation ACR tasks offer automation
400:25 - flexibility you can set up automated
400:27 - builds that are triggered by different
400:29 - events such as updates made to the
400:31 - source code updates to a containers
400:33 - based image or predetermined schedules
400:36 - or timers with ACR tasks multi-step
400:39 - workflows become easier for example you
400:41 - could build a web application image run
400:43 - the web application container build a
400:45 - web application test image and deploy
400:47 - the container that will run tests on the
400:49 - web application each ACR task has an
400:52 - Associated source code Contex the
400:54 - location of a set of source files used
400:56 - to build a container image or other
400:58 - artifact furthermore ACR tasks support
401:01 - the use of run variables making it
401:02 - possible to repurpose task definitions
401:04 - and enforce consistent image and
401:06 - artifact tagging standards in summary
401:09 - ACR tasks amplify the automation
401:11 - efficiency and consistency of container
401:13 - management and
401:18 - Azure the next topic we'll be going over
401:20 - is Docker Docker is a powerful platform
401:23 - design designed for automating the
401:24 - deployment scaling and management of
401:26 - applications using
401:28 - containerization Docker abstracts
401:30 - infrastructure and environment variables
401:32 - allowing you to create a controlled
401:33 - environment within your Docker container
401:35 - we can install Docker images into those
401:37 - Docker containers with each Docker image
401:39 - representing one or a group of common
401:41 - software the machine on which Docker is
401:44 - installed and running is usually
401:45 - referred to as a Docker host or host
401:47 - when you deploy an application on the
401:49 - host it will create a logical entity to
401:51 - host where the application called a
401:52 - container or Docker container container
401:54 - a Docker container does not have any OS
401:56 - installed and running on it it has a
401:58 - virtual copy of the process table
402:00 - network interface and the file system
402:02 - outpoint which have been inherited from
402:03 - the OS of the host on which the
402:05 - container is hosted in Runing the kernel
402:07 - of the host OS is shared across all the
402:09 - containers that are running on it this
402:12 - allows each container to be isolated
402:13 - from the other present on the same host
402:15 - it supports multiple containers with
402:17 - different application requirements and
402:19 - dependencies to run on the same host if
402:21 - the OS requirements are the same Docker
402:24 - key benefits Docker supports multiple
402:27 - applications with different requirements
402:28 - and dependencies to be hosted on the
402:30 - same host if the OS requirements are the
402:32 - same storage optimized containers are
402:35 - typically a few megabytes in size and
402:37 - consume very little disk space allowing
402:39 - a large number of applications to be
402:41 - hosted on the same host robustness
402:44 - robustness containers don't contain a
402:46 - full OS making them more lightweight
402:47 - than virtual machines as a result they
402:50 - use significantly less memory and can
402:51 - boot up in mere seconds whereas a might
402:54 - take several minutes reduces costs
402:57 - Docker is less demanding when it comes
402:58 - to the hardware required to run it
403:01 - better or Disaster Recovery you can back
403:03 - up a Docker image of the state of the
403:04 - container at a specific point in time
403:06 - and restore it later if serious issues
403:08 - arise faster configuration with
403:10 - consistency you can just put your
403:12 - configurations into code and deploy it
403:14 - saves a lot of time from preparing the
403:16 - setup and deployment
403:18 - documentation overall Docker provides a
403:20 - robust efficient and cost-effective
403:22 - solution for application deployment and
403:24 - management through
403:30 - containerization the next topic we'll be
403:32 - going over is a Docker file in Docker a
403:34 - Docker file is a text document that
403:36 - contains all the commands a user could
403:37 - call on the command line to assemble an
403:39 - image by using the docker Bill command
403:42 - users can automate the image creation
403:44 - process by executing the instructions
403:45 - laid out in the docker file here are
403:48 - some essential points to understand
403:49 - about Docker files the docker file is a
403:52 - text file that contains the instruction
403:53 - that you would execute on the command
403:55 - line to create an image a Docker file is
403:57 - a step-by-step set of instructions
404:00 - Docker offers standard commands for use
404:02 - within the docker file such as from
404:04 - specifies the base image to start with
404:07 - copy copies files from the host system
404:09 - into the image run executes a command
404:12 - EnV sets environment variables expose
404:15 - informs Docker that the container will
404:17 - listen on a specified Network Port at
404:18 - runtime CMD provides defaults for the
404:21 - executing container docker will build a
404:24 - Docker image automatically by reading
404:25 - these instructions from the docker file
404:28 - overall a Docker file provides a
404:29 - reproducible and consistent method to
404:31 - build Docker container
404:34 - [Music]
404:37 - images hey it's Andrew Brown from exam
404:39 - Pro and in this section we'll delve into
404:41 - Azure functions but first it's essential
404:44 - to understand the concepts of serverless
404:45 - and functions as a service so what is
404:48 - serverless serverless architecture
404:50 - generally describes fully managed cloud
404:52 - services that classification of a cloud
404:54 - service being serverless is not a
404:56 - Boolean answer but an answer on a scale
404:58 - where a cloud service has a degree of
405:00 - serverless a serverless service could
405:02 - have alter most of the following
405:04 - characteristics High elasticity and
405:06 - scalability high availability high
405:08 - durability and secure by default
405:10 - abstracts away the underlying
405:12 - infrastructure and are built based on
405:13 - the execution of your business task
405:15 - serverless can scale to zero meaning
405:17 - when not and use the serverless
405:18 - resources cost nothing pay for Value
405:21 - model where you only pay for actual use
405:23 - usage eliminating costs for idle
405:25 - resources an analogy of serverless could
405:27 - be similar to an energy rating labels
405:29 - which allows consumers to compare the
405:31 - Energy Efficiency of a product some
405:33 - services are more serverless than others
405:36 - what is function is a service FAS
405:38 - empowers developers to concentrate on
405:40 - Crafting specific pieces of code known
405:42 - as functions these functions can be
405:44 - event driven meaning they either trigger
405:46 - based on events or produce event data
405:48 - typically several functions are
405:49 - interwoven to form a serverless
405:51 - application and these functions are
405:52 - activated only when called upon function
405:55 - as a service is not serverless on its
405:57 - own f is only serverless if it's fully
405:59 - managed and scales to zero in conclusion
406:02 - serverless and F represent
406:04 - transformative approaches in cloud
406:05 - computing emphasizing efficiency
406:07 - scalability and cost-effective
406:13 - me the next topic we'll be covering is
406:16 - azure functions Azure functions is a
406:18 - function as a service offering that
406:20 - allows developers to focus on writing
406:22 - code and not worry about maintaining the
406:24 - underlying Computing infrastructure a
406:26 - function app defines the underlying
406:28 - compute for a collection of functions a
406:30 - function app defines the hosting runtime
406:32 - and other Global
406:33 - configurations a function represents
406:35 - code LA with application runtime
406:37 - configuration a trigger is the chosen
406:40 - event data that will cause function to
406:41 - execute you can only have one trigger
406:44 - input bindings or what are multiple data
406:46 - sources that will be pass to the
406:47 - function when a trigger occurs output
406:50 - bindings are one more data syns that
406:51 - will receive outputed data from the
406:53 - function on successful execution there
406:56 - are four versions of azure functions 1 x
406:58 - 2 x 3 x and 4 x we are currently using
407:02 - for
407:03 - X Azure functions storage
407:06 - considerations every function app
407:08 - requires a storage account to operate if
407:11 - that account is deleted your functions
407:12 - won't work Azure functions uses the
407:15 - following storage types in the storage
407:17 - account blob storage maintain binding
407:20 - State and function keys Azure files file
407:23 - share used to store and run your
407:24 - function app code in a consumption plan
407:26 - and Premium plan Azure files is set up
407:28 - by default but you can create an app
407:30 - without Azure files under certain
407:32 - conditions H Storage used by task hubs
407:35 - and durable functions and table storage
407:38 - also used by task hubs and durable
407:41 - functions Azure functions anatomy of a
407:43 - function function. JSO n configuration
407:47 - of a single function defining bindings
407:49 - code the code for your function Funk
407:52 - ignore to ignore host. JS n Global
407:56 - configuration of all functions at the
407:58 - function app level local project a place
408:00 - to locally store code in conclusion
408:03 - Azure functions offers developers an
408:05 - effortless way to build event driven
408:07 - Solutions without managing
408:13 - infrastructure the next topic we'll be
408:15 - overgo is the authorization levels for
408:17 - Azure functions HTTP triggers
408:19 - authorization level determines what keys
408:21 - if any need to be present on on the
408:23 - request in order to invoke the function
408:25 - the authorization level can be one of
408:27 - the following values Anonymous no API
408:30 - key is required function a specific API
408:32 - key for that function is needed admin
408:34 - the master key for all functions within
408:36 - that function app is required in this
408:39 - example we're creating an HTTP trigger
408:41 - the desired authorization level can be
408:43 - selected note that the authorization
408:45 - level can usually be changed after
408:47 - Creation in the portal and is set on the
408:49 - trigger Azure functions debugging you
408:52 - can can enable streaming logs for Azure
408:54 - functions to see your real-time logging
408:56 - when an error occurs there are two ways
408:58 - to view a stream of log files being
409:00 - generated by your function executions
409:03 - built-in log streaming the app service
409:05 - platform provides an inbuilt feature to
409:07 - stream application log files allowing
409:09 - developers to trace function executions
409:11 - live live metric stream when your
409:14 - function app is connected to application
409:15 - insights you can view log data and other
409:17 - metrics in near real time in the Azure
409:19 - portal using live metric stream it's
409:22 - worth highlighting that these lock
409:23 - stream can be viewed both in the portal
409:25 - and in most local development
409:26 - environments overall Azure functions
409:29 - provide adaptable HTTP trigger
409:31 - authorization and robust debugging tools
409:33 - enhancing security and developer
409:39 - troubleshooting the next let's break
409:41 - them down lightweight and serverless
409:44 - Azure functions are Compact and
409:45 - potentially serverless removing the need
409:47 - for heavy infrastructure efficiency and
409:50 - speed easy to write deploy and upgrade
409:52 - without effect other website components
409:54 - Azure functions are fast to execute
409:56 - because there is no large application
409:58 - startup time initialization and other
410:00 - events fired before the code is executed
410:02 - event driven execution ensures they run
410:04 - only when an event triggers them zero
410:07 - maintenance no need for active
410:09 - infrastructure management or Associated
410:10 - costs they scale automatically to meet
410:12 - traffic demands even scaling to zero
410:15 - cost when idle development e Azure
410:17 - functions can be built tested and
410:19 - deployed directly from the Azure portal
410:21 - they provide built-in CI CD through
410:23 - Azure devops and monitoring with Azure
410:25 - monitor streamline the development
410:27 - process cost effective you pay only for
410:29 - the actual runtime eliminating costs
410:31 - when functions are idle interoperability
410:34 - using industry standard protocols Azure
410:36 - functions can seamlessly communicate
410:38 - with various apis databases and
410:42 - libraries Azure functions use cases for
410:45 - business use cases Azure functions are
410:47 - great for scheduled tasks reminders and
410:49 - notifications lightweight web API
410:52 - sending background emails running
410:53 - background backup tasks and Performing
410:55 - bin
410:56 - calculations for technical use cases
410:59 - Azure functions are ideal in sending
411:01 - emails starting backup order processing
411:03 - and task scheduling such as database
411:05 - cleanup sending notifications messages
411:07 - and iot data processing Azure functions
411:11 - are best suited for smaller apps have
411:12 - events that can work independently of
411:14 - other websites in conclusion Azure
411:17 - functions are serverless and efficient
411:18 - streamlining development for both
411:20 - business and Technical tasks and event
411:22 - driven compaction
411:27 - applications the next topic we'll be
411:29 - covering is our Azure function templates
411:32 - Azure provides function templates to get
411:34 - you started with common function
411:35 - scenarios in Visual Studio code the
411:38 - selection of a function template occurs
411:40 - exclusively during the project creation
411:42 - phase HTTP triggered by an HTTP request
411:46 - and returns HTTP tyer triggered based on
411:49 - a predefined schedule blot storage
411:52 - triggered when files are either uploaded
411:54 - or updated in a blob storage container
411:56 - Cosmos DB executes in response to the
411:58 - addition or modification of documents in
412:00 - Cosmos DB Q storage triggered by Azure
412:04 - storage Q messages event grid triggered
412:07 - by event from event grid many Azure
412:09 - Services can trigger a function through
412:11 - event grid essentially event grid
412:13 - operates as a serverless event bus that
412:15 - is deeply integrated with various Azure
412:17 - Services event Hub triggered by event
412:20 - Hub event and is particularly effective
412:22 - for streaming scenar scenarios service
412:24 - bus Q triggered when there's a new
412:26 - message in a service bus que making it
412:28 - optimal for messaging systems service
412:30 - bus topics triggered by an event from
412:32 - bus topic aligning it with the pub
412:35 - submodel send grid specifically designed
412:37 - to be triggered by an email event within
412:39 - the third party service send grid
412:42 - overall Azure function templates offer
412:44 - developers a structured foundation for
412:46 - common scenarios enabling quicker
412:48 - deployment and integration within
412:51 - azure
412:55 - the next topic we'll be going over the
412:57 - functions configuration for Azure
412:58 - functions each function comes with a
413:01 - specific configuration file named
413:02 - function. JSO n this file serves an
413:05 - important role outlining the trigger
413:07 - bindings and additional configuration
413:09 - settings for the function the Essential
413:11 - Elements of this file include type
413:14 - specifies The Binding type Direction
413:16 - indicates whether the binding is for
413:18 - receiving data into the function or
413:19 - sending data from the function name
413:22 - represents the the data binding in the
413:23 - function in C this would be an argument
413:26 - name whereas in JavaScript it would
413:28 - manifest as a key and a key value
413:30 - pair let's take a look at the host
413:32 - configuration for Azure functions every
413:35 - function app has a host configuration
413:36 - file named host. Json this configuration
413:40 - file contains Global configurations
413:42 - options and parameters for all the
413:44 - functions within the function app the
413:46 - host has lot of configuration options
413:48 - these include aggregator application
413:50 - insights blobs console Cosmos d custom
413:53 - Handler durable task event Hub
413:55 - extensions extension bundle functions
413:57 - function timeout health monitor HTTP
414:00 - loging manage dependency cues retri send
414:03 - grid service bus Singleton version watch
414:06 - directories and watch file settings
414:08 - essentially these configurations offer
414:10 - developers a framework to adapt
414:12 - functions respond to event triggers and
414:14 - ensure consistency across the function
414:20 - app the next thing will be be covering
414:23 - are the plan services and Azure
414:24 - functions Azure functions offers three
414:27 - distinct plan Services each tailored to
414:29 - different needs consumption plan cold
414:31 - starts you only pay for the time your
414:33 - code or application is running billing
414:35 - is based on the number of executions the
414:37 - duration of each execution and the
414:39 - amount of memory used just pay while you
414:41 - have functions running and scale out
414:43 - automatically even through long loading
414:45 - times Premium plan pre-warmed the user
414:48 - has designated a set of pre-warmed cases
414:50 - which are already online and ready to
414:52 - react instantly Azure provides any
414:54 - additional Computing Services that are
414:56 - required when your function is running
414:57 - you pay for the constantly pre-warmed
414:59 - instances including any additional
415:01 - instances needed to scale the Azure app
415:03 - in out Azure functions host instances
415:05 - are added and removed based on the
415:07 - number of incoming events dedicated plan
415:10 - VM sharing when you use app service for
415:12 - other apps your functions will run on
415:14 - the same plan at no extra cost you may
415:16 - scale it up manually by adding more VM
415:18 - instances for an app service plan you
415:20 - may have autoscale enabled optimal when
415:22 - you have existing underutilized VMS
415:25 - which also operate other instances of
415:26 - the app service in summary when choosing
415:29 - an Azure functions plan we your budget
415:31 - responsiveness needs and current
415:33 - infrastructure for Optimal Performance
415:35 - and
415:39 - value the next thing we'll be covering
415:41 - are triggers and bindings and Azure
415:43 - functions triggers and bindings let you
415:45 - avoid hardcoding access to other
415:47 - services and abstracting Away boiler
415:49 - plate code keeping your functions lean
415:51 - what is a trigger a trigger is a
415:53 - specific type of event which causes the
415:55 - function to run it defines how a
415:57 - function is invoked and a function must
415:58 - only have one trigger triggers can have
416:00 - Associated data which is often provided
416:02 - as the payload of the function what is a
416:04 - binding binding is defined if your
416:06 - function is connected to another service
416:08 - the data from bindings is provided to
416:10 - the function as parameters bindings are
416:12 - optional and a function can have
416:14 - multiple input and output
416:16 - bindings Azure functions support a wide
416:18 - range of bindings to facilitate
416:20 - integration and data processing these
416:22 - include Storage Solutions like blob
416:24 - storage Azure Cosmos DB and Azure SQL
416:27 - Dapper event grid and event Hub cater to
416:30 - event driven architectures with iot Hub
416:32 - focusing on event data from Hardware
416:34 - devices HTTP and web hooks facilitate
416:37 - real-time data interaction while Kafka
416:40 - processes stream data mobile apps helps
416:42 - in Mobile development and notification
416:44 - hubs are for push notifications Hugh
416:46 - storage rabbit and Q a messaging broker
416:48 - and service bus ensure seamless data
416:50 - transfer and messaging send Grit
416:52 - optimizes email delivery SLE R an open
416:55 - source.net Library provides asynchronous
416:58 - notifications to web apps table storage
417:00 - is the nsql KE Value Store timer
417:03 - triggers are based on scheduled rejects
417:04 - expressions and twio offers a cloud
417:07 - platform for Voice and text messaging
417:09 - systems the table provides a breakdown
417:12 - of various Azure functions Integrations
417:13 - and their support across versions one
417:15 - Hax and 2 x and higher both blob storage
417:18 - and Azure Cosmos DB are fully supported
417:20 - across all categories for both versions
417:23 - Azure data Explorer Azure SQL Dapper
417:25 - cafka and Signal are supported from
417:27 - version 2 x Awards some Integrations
417:30 - such as mobile apps notification hubs
417:32 - and send grid have limited support
417:34 - across the versions specifically with
417:36 - triggers inputs or outputs others like
417:39 - event grid event hubs HTTP and web hooks
417:42 - iot Hub Q storage service bus and table
417:45 - storage have brought support but may
417:46 - lack in one or two categories overall
417:49 - Azure functions offer serverless
417:51 - computation and seamless service
417:52 - integration using triggers and bindings
417:55 - they eliminate redundant code the
417:56 - platform supports diverse bindings but
417:59 - some Integrations have
418:04 - limits the next topic we'll be covering
418:06 - into is The Binding Direction and Azure
418:08 - functions all triggers and bindings have
418:11 - a direction property in the function.
418:12 - JSO n file the direction of triggers is
418:15 - always in input and output bindings use
418:18 - in out or both some bindings support a
418:21 - special Direction and out out the
418:23 - trigger is defined alongside the input
418:25 - and output bindings trigger will have
418:27 - the same as the input type but with
418:28 - trigger appended for example an input
418:30 - binding name blob would have a trigger
418:32 - name blob trigger if you use an out only
418:34 - the advanced editor is available via the
418:36 - integrate tab in the portal if you use
418:38 - an out only the advanced editor is
418:40 - available via the integrate tab in the
418:42 - portal in scenarios requiring periodic
418:45 - data processing Azure functions offers
418:47 - tailored Solutions suppose every hour
418:50 - you want to read new log files delivered
418:51 - by your app application and you need to
418:53 - transform the data to be ingested in
418:55 - your nosql database that resides in
418:57 - Cosmos DB you'll use the trigger type
418:59 - timer because it's a scheduled job that
419:01 - will run at a specific time the blob
419:04 - storage would be an in binding as the
419:05 - function reads the data from it the
419:07 - cosmos DB would be an out binding
419:09 - enabling the function to write the
419:10 - processed data into the database in
419:13 - essence by understanding and effectively
419:15 - leveraging these binding directions
419:17 - developers can architect efficient
419:18 - event-driven solutions that seamlessly
419:20 - integrate various Azure services
419:23 - [Music]
419:27 - hey this is Andrew Brown from exam Pro
419:29 - and in this section we'll be covering
419:31 - Azure front door Azure front door is a
419:33 - traffic manager traffic accelerator
419:35 - Global load balancer and content
419:37 - distribution Network Azure front door is
419:40 - a modern application delivery Network
419:42 - platform providing a secure scalable CDN
419:45 - Dynamic site acceleration and Global
419:47 - HTTP load balancing for your Global web
419:50 - applications Azure front door featur
419:52 - fees caching similar to traditional cdns
419:55 - Azure front door offers caching with
419:57 - specified rules and expiration policies
420:00 - resiliency by Distributing incoming
420:02 - traffic across multiple or different
420:03 - Azure regions cookie based session
420:06 - affinity for restful applications when
420:08 - traffic needs to be redirected back to
420:10 - the same back end Health probe to
420:12 - determine the healthiest and closest
420:13 - backend to the client request web
420:16 - application firewall a crucial security
420:18 - measure protecting your back ends from
420:20 - malicious attacks and vulnerabilities
420:22 - URL redirect redirecting traffic based
420:25 - on protocol HTTP or https host name path
420:29 - and query string URL rewrite with a
420:32 - powerful engine for rewriting income
420:33 - requests to a different backend
420:35 - request an Azure front door is made up
420:38 - of frontends domains these front-ends
420:41 - domains are connected to backend pools
420:42 - where those connections are filtered by
420:44 - routing rules Azure front door core
420:47 - components profile containers all front
420:50 - door components and Point acts as a
420:52 - pathway or Bridge connecting the front
420:54 - end to the back end origin groups a
420:57 - grouping of Origins origin defining the
420:59 - road to back end rule sets a grouping of
421:02 - rules rules routing rules overall aure
421:05 - front door optimizes web application
421:07 - delivery within the Azure ecosystem by
421:10 - utilizing its features businesses can
421:12 - enhance web traffic management
421:18 - efficiency the next topic we'll be
421:20 - covering are the tiers for Azure front
421:21 - door door Azure front door offers two
421:24 - distinct tiers tailored for different
421:25 - needs first we have standard this
421:28 - auction optimizes content delivery
421:30 - offers both static and dynamic content
421:32 - acceleration offering Global load
421:34 - balancing SSL offload domain and
421:36 - certificate management enhanced traffic
421:38 - analytics and basic security
421:40 - capabilities and we have premium this
421:43 - option includes all the features of the
421:45 - standard tier asure front door premium
421:47 - tier offers extensive security
421:48 - capabilities with Waf by protection
421:51 - private links support and integration
421:53 - with Microsoft thread intelligence and
421:55 - security analytics in conclusion Azure
421:57 - front door provides standard for Content
421:59 - delivery and premium with added security
422:01 - ensuring Global application performance
422:03 - and
422:08 - protection the next topic we'll be going
422:10 - over is routing and Azure front door
422:13 - routing in Azure front door determines
422:14 - the path in HTTP request from a user
422:17 - takes to reach a configured backend
422:18 - service here's how it works an HTTP
422:21 - request from a user is directed to the
422:23 - nearest Edge location it then matches
422:25 - with an Azure front door profile and
422:27 - evaluates any web application firewall
422:29 - rules following this it matches with an
422:31 - Azure front door route evaluates engine
422:33 - rules and either returns cash content or
422:35 - selects the appropriate origin group
422:37 - finally the request is directed to the
422:39 - selected origin which then sends it to
422:41 - the corresponding back end Azure front
422:43 - door offers four distinct traffic
422:45 - routing methods latency routes requests
422:48 - to the backends with the lowest latency
422:50 - within a specified sensitivity range
422:52 - priority directs requests based on a
422:54 - user defined priority number weighted
422:57 - distributes requests to backends
422:59 - proportionally based on assigned weight
423:01 - coefficients session Affinity ensures
423:03 - requests from the same end user are
423:05 - directed to the same back end ideal for
423:07 - stateful backends in summary Azure front
423:09 - door enhances user experience and
423:11 - backend performance through its
423:12 - efficient routing
423:18 - capabilities the next topic will be
423:20 - covering is origin and origin grp groups
423:22 - in Azure front door the origin is what
423:24 - Azure front door will point at to the
423:26 - end user origin is the endpoint that
423:28 - points to your back end Azure front door
423:30 - provides robust support for a variety of
423:32 - Origins to seamlessly integrate with its
423:34 - application delivery Network among the
423:36 - supported origins or Azure blob storage
423:39 - which offers vast storage capabilities
423:41 - and Azure storage with static website
423:43 - hosting catering to web hosting needs
423:45 - additionally Azure supports cloud
423:47 - services app services and static web
423:49 - apps for diverse application deployments
423:51 - for comprehensive API management Azure
423:53 - front door integrates with API
423:55 - management furthermore application
423:57 - Gateway public IP address Azure traffic
423:59 - manager Azure spring cloud and Azure
424:01 - container instances are also supported
424:04 - notably there's flexibility for users as
424:06 - they can also add custom Origins by
424:08 - providing a host name priority and Azure
424:11 - front door determines who is the primary
424:13 - recipient of traffic or who who to send
424:14 - traffic to First you select a value
424:16 - ranging from 1 to 5 a lower value
424:19 - signifies a higher priority and multiple
424:21 - backends can share this number weights
424:23 - allow you to determine the split of
424:25 - traffic distribution between origins of
424:27 - the same priority a number between 1 to
424:29 - 1,000 the default value is
424:32 - 50 origin groups in Azure front door are
424:35 - collections of Origins every origin must
424:37 - be part of an origin Group by default as
424:39 - your front door profiles contain An
424:41 - Origin group named default origin group
424:44 - origin groups facilitate the application
424:46 - of Health probes to assess the condition
424:47 - of your Origins and load balancing
424:49 - settings to manage the distribution
424:51 - among them
424:52 - to direct inbound traffic to a
424:54 - particular origin group and inlo must
424:56 - link to the origin group through a
424:57 - designated route overall as your front
425:00 - door optimizes web traffic management
425:02 - with diverse Origins prioritization and
425:04 - weight mechanisms using origin groups it
425:07 - leverages health probes and load
425:08 - balancing for efficient application
425:15 - delivery the next topic will be covering
425:17 - our health checks in Azure front door
425:19 - Azure front door uses origin group
425:21 - Health probes to periodically ping a
425:23 - backend verifying whether it returns a
425:25 - healthy response typically a healthy
425:27 - response is determined by status 20 100
425:30 - if a backend fails to produce a healthy
425:32 - response aure front door will redirect
425:34 - the traffic to other available and
425:36 - healthy backends assuming other Origins
425:38 - are configured what is a HTTP Response
425:41 - Code when a user sends an HTTP request a
425:43 - HTTP response is returned and HTTP
425:47 - responses will have a response code to
425:48 - communicate how a backend server
425:50 - interpreted the request
425:52 - a response code is number that
425:53 - coordinates to what happened for example
425:56 - 200 signifies okay 4003 means forbidden
425:59 - 4004 indicates not found 500 represents
426:02 - internal server error Azure front door
426:05 - load balancing settings origin group
426:08 - load balancing settings and Azure front
426:10 - door enable you to specify the criteria
426:12 - determining the health of a back end
426:14 - essentially these settings Define which
426:15 - sample set should be used to classify a
426:17 - backend as either healthy or unhealthy
426:20 - when the latency sensitivity is set to
426:22 - zero aure front door prioritizes and
426:24 - routes traffic to the quickest available
426:26 - backend if it's set to any other value
426:28 - the system adopts a round robin approach
426:30 - Distributing traffic between the fastest
426:32 - back end and subsequent ones all within
426:34 - the bounds of the preconfigured latency
426:41 - sensitivity the next topic will be
426:43 - covering a routes and Azure front door
426:45 - routes and Azure front door serve as
426:47 - mapping tools linking your domains and
426:49 - corresponding URL path patterns to
426:51 - specific origin groups routes of caching
426:53 - and compression features applied rules
426:56 - from rule sets can be Associated to
426:57 - routes to apply intelligent
426:59 - routing one of the standout features of
427:02 - azure front door is its capability for
427:03 - traffic acceleration this feature
427:05 - ensures faster Global delivery of your
427:07 - application without necessitating any
427:09 - changes to your existing application
427:11 - code hazure achieves this traffic
427:13 - acceleration by directing traffic to the
427:15 - nearest Edge location to a ramp into the
427:17 - Azure Network traffic that is following
427:20 - within the internal Azure Network
427:21 - travels at accelerated speed while also
427:23 - taking the most direct path think of it
427:25 - as an expressway in essence asure front
427:28 - door routes link domains to origin
427:29 - groups incorporating caching compression
427:32 - and intelligent
427:37 - routing the next topic will be covering
427:39 - a rule sets in Azure front door Azure
427:42 - front door rules engine allows you to
427:44 - customize how HTTP requests gets handled
427:46 - at the edge and provides a more
427:47 - controlled Behavior to your web
427:49 - application this interface is a
427:51 - configuration screen for defining rules
427:53 - in Azure front door a Content delivery
427:55 - and application acceleration service
427:57 - let's break down what's presented rule
428:00 - Name by rule this is the name assigned
428:01 - to the current rule condition if
428:04 - represents the conditions under which
428:06 - the Rule's action should be executed
428:08 - request header the rule is triggered
428:10 - based on an HTTP header present in the
428:12 - incoming request header name my header
428:14 - the rule looks for this specific header
428:16 - in the incoming request operator equal
428:19 - specifies the comparison type here it
428:21 - checks if the header's value exactly
428:23 - matches the defined value header value
428:25 - my value the value that my header should
428:27 - have to meet the condition string
428:29 - transform to lowercase before checking
428:31 - the value it will convert the header's
428:33 - value to lowercase action then
428:36 - represents the action to be taken if the
428:38 - condition is met URL redirect the action
428:41 - taken will be a URL redirect redirect
428:44 - type found specifies the type of HTTP
428:46 - redirect 3002 means a temporary redirect
428:50 - redirect protocol match request the
428:52 - protocol of the redirected URL will
428:54 - match that of the original request
428:56 - destination host not specified in the
428:58 - image which means it'll redirect to the
429:00 - same host or domain destination path go
429:03 - here this is the path to which the user
429:04 - will be redirected if the condition is
429:07 - met some of the available conditions
429:09 - include device type HTTP version and
429:12 - request cookies it also examines post
429:14 - args the query string and the remote
429:16 - address of the requester details like
429:18 - the request body request file name
429:20 - request file extension and request
429:22 - header is scrutinized furthermore the
429:24 - request method request path request
429:26 - protocol and request URL are taken into
429:29 - account during processing aure front
429:31 - door utilizes rule sets to manage its
429:33 - operations offering a range of operators
429:36 - for precise control these operators
429:38 - include equal contains less than greater
429:40 - than less than or equal greater than or
429:42 - equal begins with ends with and re X
429:44 - additionally each of these operators has
429:46 - a corresponding knot variant providing
429:48 - enhanced flexibility and specificity in
429:50 - defining rule
429:51 - conditions action cache expiration cache
429:55 - Behavior bypass override set of missing
429:58 - cash key query string Behavior include
430:01 - cach every unique URL exclude ignore
430:03 - query string modify request header
430:06 - modify response header operator append
430:09 - overwrite delete URL redirect redirect
430:12 - type found moved temporary redirect
430:15 - permanent redirect redirect protocol
430:17 - match request HTTP
430:20 - https URL Rite Source pattern and
430:23 - destination and origin group overr right
430:26 - so that's an overview of rule sets for
430:28 - Azure front
430:30 - [Music]
430:33 - door hey this is angrew brown and this
430:36 - fall along we're going to be utilizing
430:38 - Azure front door so before we do that
430:40 - we're going to need ourselves a storage
430:42 - account to set up some static website uh
430:44 - storage so what I want you to do is go
430:46 - to storage accounts and there is a
430:48 - static web app host uh hosting that we
430:50 - can use but we're going to do it the old
430:52 - school way because it's always great to
430:54 - learn a few different ways to do things
430:55 - in Azure and so uh there's probably
430:58 - another fall along where we we use uh
431:00 - the static static site thing there um so
431:02 - what we're going to do is create a new
431:05 - uh we're going to create a new um
431:08 - Resource Group I'm going to call this
431:10 - one uh my uh Azure front door and we'll
431:14 - say okay and from here we'll have to
431:16 - name it something so we'll just say my
431:18 - Azure front door or sorry we'll do uh
431:20 - step static
431:22 - Storage storage 8888 if you can't get
431:25 - the 48s because I'm using it then just
431:27 - give it a different um number sequence
431:30 - because these are treated like fully
431:31 - qualified domains choose like a a US
431:34 - region so that we're all doing the same
431:35 - thing for performance we're going to
431:37 - stick with standard it doesn't matter if
431:39 - it's on Geo redundant we're not doing
431:40 - anything fancy there so what I want you
431:42 - to do is go ahead and hit review and
431:44 - create and then give it a moment and hit
431:46 - create and we're going to wait for that
431:48 - to uh finish deployment and from there
431:50 - we'll then have to enable uh static
431:53 - website so it doesn't take too long to
431:55 - deploy so we'll just give it a
431:59 - moment and while that's going uh we do
432:02 - need to create ourselves a um index HTML
432:06 - file so you're going to need some kind
432:07 - of editor so I'm just opening up visual
432:09 - studio code on my
432:10 - computer just give it a moment
432:16 - there and I'm just creating a new file
432:19 - here this is taking it Taking a moment
432:20 - to load
432:22 - okay so and what we need to do is just
432:24 - create an index HTML file these are
432:26 - really basic you can find them anywhere
432:28 - online but this one's going to say hello
432:29 - Mars hello Mars very very simple so I'm
432:32 - going to go ahead and save this to my
432:34 - desktop so save as and um show local
432:39 - maybe there we go that's a little bit
432:41 - more sayane and we'll just say
432:46 - index.html and we will now go to the
432:48 - resource and we're going to go to
432:50 - website static is a blade on left hand
432:52 - side so we can do St website stuff we're
432:55 - going to say index HTML I think we have
432:56 - to set that for it to work I cannot
432:58 - remember if that's the case we have a
432:59 - primary end point this is uh these
433:01 - matter for later but we'll come back to
433:03 - that and we're going to go over to our
433:05 - containers and we're going to have to
433:06 - create a new container so I'm just going
433:09 - to call that actually we don't because
433:10 - we're going to put it in web so when we
433:12 - turned on static website hosting it
433:13 - already gave us a blob or container a
433:16 - container for us to uh add our add our
433:18 - or upload our files there so we'll go to
433:21 - upload here and then I'm going to go
433:22 - ahead and select that index HTML file
433:24 - and upload it in place um we need to
433:28 - change our access level for this uh this
433:31 - uh container to um I think just
433:36 - blob so I'm just double checking
433:40 - here yeah I think it's blob access uh
433:42 - can only be anonymous request container
433:44 - data is not available so we'll say okay
433:46 - I believe that is
433:49 - correct okay okay uh sometimes you can
433:52 - like we're clicked into it but I think
433:54 - like in the my instructions I do it a
433:56 - little bit different so I'm just going
433:57 - to double check to see what the settings
433:58 - are there just so you can see it if
434:00 - you're following my instructions so
434:02 - change
434:03 - access private blob yeah okay so that's
434:05 - fine um so blob access should be okay
434:09 - and so now what we want to do is just
434:10 - test that our our page is working so if
434:12 - we go back to static website
434:14 - here on the left hand side we can grab
434:17 - this primary endpoint and we should be
434:18 - able to post it anywhere here see I have
434:21 - the old tutorial here this is a moment
434:22 - ago so that's why it's a bit confusing
434:24 - but um so hello Mars is working but this
434:26 - is on static stor this isn't in front of
434:28 - or behind aure front door uh which is
434:31 - what we'll want to do next so what I
434:34 - want you to do is just close a couple of
434:35 - these tabs out we're going to type in
434:37 - Azure front door or front door should be
434:40 - enough front there we go and we have a
434:43 - whole bunch of options we'll just hit
434:45 - create that's a front door we actually
434:47 - do use for the platform we have Azure
434:49 - front door we have create I always go
434:51 - custom I don't think I've ever done
434:53 - quick create I just I just have more
434:55 - trust in custom and from here we will
434:57 - choose front
434:59 - door and ECS is
435:03 - fine I'm just wondering where our
435:05 - storage account is that's why we should
435:07 - always just kind of set them to be the
435:09 - same place so go storage account I don't
435:12 - think it will matter but I'm just going
435:14 - to double check storage
435:19 - account
435:21 - and this one is in East us so we're
435:24 - going to be
435:25 - okay just make sure they match just so
435:27 - we have less problems and I'm going to
435:30 - call this one my Azure front
435:33 - door now there are more functionalities
435:36 - in the premium but standard is fine for
435:38 - us we'll go next to Secrets um this is
435:42 - if we want to add a certificate um like
435:46 - you could bring your own certificate um
435:49 - but uh we're not too worried about that
435:50 - we're going to have to add a new
435:51 - endpoint so I'm going to just say my
435:53 - endpoint here there's a lot of small
435:55 - steps in here so this should be fun
435:58 - hopefully we don't configure anything
435:59 - wrong here and then we need to add a
436:01 - route say my
436:03 - route and it's going to use the default
436:06 - domain that's totally fine uh we don't
436:08 - have to do anything it's going to just
436:09 - be on the for SL Aster there we do need
436:12 - to create an origin group so we'll go
436:14 - here and create a new origin group so my
436:16 - origin group and then from there we need
436:18 - to add an origin yes it is very squirely
436:21 - going through all this my origin the
436:24 - origin type is going to be Azure or
436:27 - sorry storage static website and from
436:29 - here we need to choose the right one we
436:31 - call it static storage uh this is fine
436:34 - this is all fine we'll go ahead and hit
436:36 - add and down below we have some load
436:39 - balancing we don't care about any of
436:40 - that uh so you can see it status is
436:42 - enabled all this should be okay the
436:44 - protocol here for the health probe
436:46 - should be HTTP um actually let me double
436:50 - check I remember this is where I ran
436:51 - into some
436:53 - trouble I believe yeah HTTP I think it's
436:57 - htps okay I think I read somewhere
437:00 - that's it's like um Stack Storage
437:01 - doesn't use HTTP but clearly it does
437:03 - because when we went to the link earlier
437:04 - it was htps so I might have the
437:07 - screenshots wrong but I've corrected
437:08 - them in the actual instructions on my
437:10 - site we'll go ahead and save that I
437:12 - think it's htps and then the the the
437:14 - protocol here um we'll just leave it
437:17 - matching it's totally fine origin path
437:19 - is fine we'll go ahead and add that
437:21 - there and we'll go review create we'll
437:24 - give it a moment and we'll
437:36 - create all right so after waiting um
437:39 - about a few minutes like 2 3 minutes it
437:41 - looks like uh as your front door set up
437:42 - so we'll go to the resource and I'm just
437:44 - hoping this works sometimes you have to
437:46 - play around with the settings but if you
437:47 - see the endpoint host name we'll go
437:48 - ahead and grab that there paste it on in
437:51 - and we get a 404 so something's not
437:53 - working just right so we'll have to go
437:56 - do some debugging which I was hoping we
437:57 - didn't have to do so we go to the front
437:59 - door manager this is the same setup we
438:01 - have here before there are like
438:03 - different ways to get to it um but we'll
438:05 - have to just kind of go through and
438:06 - debug it so we do have my route my
438:09 - origin group so we can click into our
438:11 - origin group and then from here we'll
438:13 - expand it and we have our route so
438:16 - there's
438:17 - something that is not correct and that's
438:20 - what we have to to figure
438:21 - out so I'm just going to double check my
438:24 - instructions cuz this was a bit tricky
438:26 - to figure
438:28 - out um and it really came down to like
438:30 - these protocols
438:32 - here
438:36 - so let me keep checking and I mean I
438:39 - think the probe is working correctly so
438:40 - if we go on the origin if this wasn't
438:42 - working correctly then it wouldn't it
438:44 - wouldn't show Green so go back up a step
438:47 - here you know what maybe we should just
438:49 - wait a little bit because sometimes it
438:51 - takes time to propagate and I don't I
438:53 - don't 100% trust that it's not working
438:55 - because I feel like we configured it
438:57 - exactly right so we'll open up a new tab
439:03 - here says 404
439:11 - still my origin
439:18 - group that's fine
439:25 - we'll go
439:26 - here yeah so what I'm going to do let go
439:28 - back to our storage account I just want
439:29 - to double check uh what that uh string
439:33 - was for the static website I'm pretty
439:35 - sure it was
439:37 - htps it is HPS so there's no reason that
439:40 - should not
439:49 - work
439:51 - this is what I was trying to uh spare
439:52 - you the uh the debugging of this and
439:55 - it's very common it's not just Azure
439:56 - anything that has a CDN sometimes it's
439:59 - it's difficult to uh figure that stuff
440:01 - out we'll go ahead and edit that route
440:03 - and we'll just carefully look at what we
440:05 - have here so patterns to match that is
440:08 - fine it's the correct end
440:11 - point both Protocols are accepted that's
440:14 - totally fine redirect HPS that's totally
440:16 - fine match that doesn't
440:19 - matter so it
440:21 - should
440:23 - just work the only thing we didn't do
440:26 - was enable caching which I think
440:27 - wouldn't hurt to do um because it is a
440:30 - CDN we don't take advantage of it if we
440:32 - don't turn it on so we'll do that and
440:34 - say ignore that query string I mean
440:35 - that's not going to fix this problem if
440:37 - the routing is messed up again I'm just
440:40 - hoping that maybe it just has to do with
440:48 - propagation go back to the overview
440:51 - here and we will open this
440:55 - again
440:58 - whoops grab this let's just make sure
441:01 - it's doing HPS we're working to restore
441:04 - all services as possible right now what
441:05 - do you mean our services aren't
441:06 - available right now let's go look at
441:08 - Azure
441:09 - status uh page
441:14 - here and we will take a look at front
441:19 - door
441:20 - as your front door it's saying that it's
441:24 - fine Us East there's no green beside
441:29 - it good so we go down here I guess it's
441:32 - a it's a non- regional service so if
441:34 - it's green it's
441:35 - green so you know what I'm going to play
441:37 - around with this for a little oh 404 web
441:39 - so this is better right it's just saying
441:41 - it can't find the
441:43 - content well I mean I guess that makes
441:45 - sense because that was there so we'll
441:46 - just clear that out oh now it's loading
441:48 - so I think it what the issue
441:50 - is there was no issue it's just
441:52 - propagating to all the servers and it
441:53 - took some time so uh I think that's what
441:56 - really threw me for a loop um when I was
441:58 - originally doing it so just give it some
441:59 - patience and it will work eventually so
442:01 - we are all done here so we can go ahead
442:04 - and clean this
442:09 - up and I'm just looking for the resource
442:12 - Group here we'll go ahead and delete
442:14 - it and there you go that's Azure front
442:16 - [Music]
442:18 - door and I'll see you in the next next
442:20 - one
442:27 - okay hey this is Andrew Brown from exam
442:29 - Pro and in this section we'll be
442:31 - covering Azure CDN Azure content
442:33 - delivery network is an Azure service
442:35 - that provides your applications with a
442:37 - distributed network of servers with
442:39 - Azure CDN you can improve your
442:41 - applications load time save bandwidth
442:43 - with caching strategies and speed up
442:45 - responsiveness with compressed files
442:47 - Azure CDN centralizes requests from your
442:49 - origin into a single location making it
442:52 - easier to manage your inbound and
442:53 - outbound traffic with features such as
442:55 - caching strategies these outline which
442:57 - requests or cach and specify their
442:59 - expiration timelines file compression by
443:02 - minimizing static file sizes Azure CDN
443:05 - ensures bandwidth the ficiency during
443:06 - requests Geo filtering disallows or
443:09 - denies requests based on geographical
443:11 - regions global distribution azures
443:13 - regions span across the globe ensuring
443:15 - widespread reach integration Azure CDN
443:18 - seamlessly integrates with other Azure
443:20 - Services robust security Advanced
443:23 - security measures are in place without
443:24 - any extra cost scalability and load
443:27 - balancing Azure CDN makes application
443:29 - scalability straightforward thanks to
443:31 - its built-in load
443:33 - balancing Azure CDN can help you reduce
443:36 - load times save bandwidth and improve
443:38 - responsiveness it's used to cach static
443:40 - content such as images CSS or HTML Azure
443:44 - CDN is ideal when you're developing or
443:46 - managing websites or mobile apps
443:48 - encoding and distributing streaming
443:49 - media gaming software firmware updates
443:52 - and iot and points in essence Azure CDN
443:55 - provides a robust framework to enhance
443:57 - the delivery and performance of your
443:58 - digital
444:03 - assets the next topic will delve into
444:06 - are the different tiers available for
444:07 - Azure CDN Azure CDN is available in the
444:10 - following tiers Microsoft CDN standard
444:13 - Verizon standard aami premium Verizon
444:17 - aami is one of the world's largest CD
444:20 - provider with a large distributed
444:21 - network of servers around the world on
444:24 - the other hand Verizon media operates a
444:26 - global CDN platform with a focus on
444:28 - media streaming delivery and security
444:31 - the CDN features greatly vary based on
444:33 - the chosen option and it requires
444:34 - exploring a large feature table
444:36 - comparison keep in mind that Azure CDN
444:38 - from acami is scheduled to be retired on
444:40 - October 31st
444:42 - 2023 so that's a quick overview of azure
444:45 - CDN
444:48 - tiers
444:50 - next we'll explore the purge feature in
444:52 - Azure CDN The Purge function in Azure
444:55 - CDN serves as an essential tool when you
444:57 - want to refresh cach content it enables
444:59 - the removal of cach content from all
445:01 - Edge points of presence ensuring that
445:03 - the latest assets are fetched directly
445:04 - from the origin when requested when
445:07 - you're looking to purge specific files
445:09 - within a directory while cards
445:10 - represented by an asterisk can be
445:12 - employed for example to clear all files
445:14 - in a directory you'd utilize this wild
445:16 - card feature however note that the
445:19 - capabilities like like Purge all and
445:20 - wild card Purge AR available for Azure
445:22 - CDN when sourc from acam overall The
445:25 - Purge feature in Azure CDM provides
445:27 - administrators with granular control
445:29 - over the content caching ensuring that
445:31 - users always have access to the most
445:32 - updated
445:34 - [Music]
445:37 - assets hey this is Andrew Brown from
445:40 - exam Pro and in this section we're
445:41 - diving into Azure service bus service
445:44 - bus is a fully managed Enterprise
445:46 - message broker that allows you to
445:47 - publish or subscribe to topics and Q
445:49 - messages it can scale your applications
445:52 - with asynchronous messages and built-in
445:54 - integration with Azure Services Azure
445:56 - service bus allows for single or batch
445:58 - messaging message load balancing topic
446:01 - subscriptions message sessions and
446:03 - transactions ensuring compliance with
446:05 - industry standards this includes
446:07 - protocols such as amqp 1.0 with JMS 2.0
446:10 - available for premium SKU and JMS 1.1
446:13 - for standard
446:14 - SKU service bus key Concepts the
446:17 - namespace works like a server with cues
446:20 - and topics Q contains the messages
446:23 - sender is who sends the message receiver
446:25 - is who receives the message topic is a
446:28 - queue with multiple receivers that works
446:30 - like a queue subscription is a receiver
446:32 - in a topic a batch is a group of
446:35 - messages safe batch validates if each
446:37 - message can be included the batch
446:39 - session allows you to use fifo and group
446:41 - your messages in a queue Peak returns a
446:44 - message to the que without removing it
446:46 - dead letter q a q for messages that were
446:48 - unable to be delivered through the
446:49 - normal queue Peak and lock retrieves a
446:52 - message from the queue without removing
446:53 - it and locks it so other receivers
446:55 - cannot receive it receive and delete
446:57 - retrieves and delete a message from the
446:58 - queue Auto delete on idol sets a time
447:01 - span to delete the queue if it is not
447:03 - used duplicate detection history checks
447:05 - if the message was not sent earlier
447:07 - before sending a message it overall this
447:10 - gives you a comprehensive overview of
447:11 - azure service bus and its various
447:13 - components and
447:18 - features the next next topic we'll be
447:20 - covering is a namespace in Azure service
447:22 - bus an Azure service bus namespace
447:25 - serves as a container for all messaging
447:26 - components including both cues and
447:28 - topics container for messaging
447:30 - components one namespace can house
447:32 - multiple cues and topics making them
447:34 - versatile structures within the Azure
447:36 - service bus they're commonly used as
447:38 - application containers capacity slice
447:41 - think of a service bus Nam space as a
447:43 - segment of a large scale cluster this
447:45 - cluster comprises numerous all active
447:46 - virtual machines that fall under your
447:48 - control Azure availability zones a
447:51 - namespace can potentially span up to
447:53 - three Azure availability zones offering
447:55 - enhanced availability and resilience
447:57 - benefits at scale using Azure service
447:59 - bus means you're using a messaging
448:01 - system built for large scale operations
448:03 - offering High reliability and strength
448:05 - serverless messaging with Azure service
448:07 - bus it is serverless messaging this
448:10 - means you get to use the messaging
448:11 - service without getting bogged down by
448:13 - the intricacies of the underlying
448:15 - infrastructure so that's a quick
448:16 - overview of a nam space in Azure service
448:18 - bus
448:20 - [Music]
448:24 - next let's talk about qes and Azure
448:26 - service bus cues are used to send and
448:28 - receive messages messages are stored in
448:30 - cues until the receiving application is
448:32 - ready to accept and process them
448:35 - messages and qes are ordered and
448:36 - timestamped on arrival once accepted by
448:38 - the broker the message is always held
448:40 - durably in triple redundant storage
448:42 - spread across availability zones if the
448:44 - name space is Zone enabled service bus
448:47 - never leaves messages in memory or
448:48 - volatile storage after after they've
448:49 - been reported to the client is accepted
448:52 - messages are delivered in pull mode only
448:54 - delivering messages when
448:56 - requested configuration aspects of azure
448:58 - service bus cues include time to live
449:01 - this determines the duration a message
449:03 - remains in the queue if it isn't
449:04 - processed within this time frame it
449:06 - either gets removed or is transferred to
449:08 - the dead letter Q lock duration this
449:10 - represents the period during which a
449:12 - message is locked by locking messages
449:14 - service bus ensures that no two users
449:16 - can read or process the same message
449:18 - simultaneously
449:20 - to sum it up q's and Azure service bus
449:22 - are efficient tools to handle message
449:23 - sending and receiving in a structured
449:25 - and reliable manner so that's an
449:27 - overview of q's and Azure service
449:33 - bus the next topic will be covering our
449:36 - topics in Azure service bus topics can
449:39 - be used to send and receive messages a q
449:41 - is often used for pointto point or one
449:43 - toone communication whereas topics are
449:45 - useful in publish subscribe or on to
449:47 - many communication it's important to
449:50 - endnote that topics are not available at
449:52 - the basic pricing tier you need to opt
449:54 - for either the standard or premium tier
449:56 - a unique feature of topics is their
449:57 - support for multiple independent
449:59 - subscriptions multiple independent
450:01 - subscriptions can be attached to a topic
450:03 - and work in the same way as cues from
450:05 - the receiver's side a subscriber to a
450:07 - topic can receive a copy of each message
450:09 - sent to that topic it's also worth
450:11 - noting that subscriptions or named
450:13 - entities providing an organizational
450:15 - structure you can Define rules on a
450:17 - subscription a subscription rule has a
450:19 - filter that specifies a condition for a
450:21 - message to be copied into the
450:23 - subscription as well as an optional
450:24 - action that modifies message
450:27 - metadata when creating a topic consider
450:29 - the following configuration settings Max
450:32 - topic size you can specify a size
450:34 - ranging from 1 to 5 GB time to live the
450:38 - setting determines the duration after
450:39 - which a message is removed from the
450:41 - topic duplicate message avoidance this
450:44 - ensures that duplicate messages are
450:46 - processed partitioning useful for
450:48 - efficiently managing a large influx of
450:51 - events a subscription in Azure service
450:53 - bus is a named entity associated with a
450:55 - topic that allows subscribers to receive
450:57 - copies of messages sent to that topic in
451:00 - a publish subscribe model of azure
451:02 - service bus topics can be thought of as
451:04 - the channels to send messages
451:06 - subscriptions are like the virtual cues
451:07 - to receive those messages there are also
451:10 - additional configuration settings for
451:12 - subscriptions Max delivery count this
451:14 - setting can be adjusted between 1 to
451:16 - 2,000 defining the number of delivery
451:18 - attempts for a particular message
451:20 - message sessions when sessions are
451:22 - activated a subscription can ensure that
451:24 - messages are delivered in a first in
451:26 - first out sequence so that's an overview
451:29 - of topics and Azure so that's an
451:30 - overview of topics and
451:35 - Azure the next topic we'll be covering
451:38 - is the different pricing tiers in Azure
451:40 - service bus Azure service bus has three
451:42 - different pricing tiers basic standard
451:44 - and premium the more expensive the tiers
451:46 - the more functionality it provides let
451:49 - break down the features and their
451:50 - availability across these tiers Hughes
451:53 - this is the foundational messaging
451:54 - structure in service bus allowing
451:56 - messages to be sent and received all
451:58 - three tiers offer support for cues
452:01 - schedule messages this feature allows
452:03 - users to set messages to be dispatched
452:04 - at a future specified time it's
452:06 - available across all three tiers topics
452:09 - topics support the publish subscribe
452:11 - messaging pattern this feature isn't
452:13 - available in the basic tier but is
452:15 - offered in both the standard and premium
452:17 - tiers transactions and uring a set of
452:19 - operations are completed successfully
452:21 - and in order transactions are not
452:23 - available in the basic tier however both
452:25 - the standard and premium tiers support
452:27 - this feature to duplication this ensures
452:30 - that duplicate messages are processed
452:32 - more than once only the standard and
452:33 - premium tiers offer this feature with
452:35 - the basic tier lacking it sessions
452:38 - ensuring ordered and related sets of
452:39 - messages are processed in the correct
452:41 - sequence this feature is excluded in the
452:43 - basic tier but is available in the
452:45 - standard and premium tiers forward to
452:47 - send via this facility Ates forwarding a
452:50 - message or routing it via a specific
452:51 - path it's not supported in the basic
452:53 - tier but is available in the standard
452:55 - and premium tiers message size dictates
452:58 - the maximum allowable size for an
453:00 - individual message the basic and
453:02 - standard tier support messages up to 256
453:05 - kilobytes while the premium tier
453:06 - significantly extends this limit to 100
453:09 - megabytes resource isolation this
453:11 - provides isolated computational
453:13 - resources to ensure better performance
453:15 - and reliability it's a feature exclusive
453:17 - to the premium tier G Geo Disaster
453:19 - Recovery in the event of significant
453:21 - geographical or infrastructure
453:23 - disruptions this recovery feature helps
453:25 - maintain service Integrity it's denoted
453:27 - with an asterisk suggesting there might
453:29 - be additional details or conditions this
453:31 - feature is reserved for the premium tear
453:34 - Java messaging service supporting JMS a
453:36 - standard messaging protocol for Java
453:38 - this feature is only available in the
453:40 - premium tier availability Zone support
453:43 - this ensures High resilience and
453:44 - availability by Distributing Services
453:46 - across multiple isolated data centers or
453:48 - zones it's exclusive to the premium
453:55 - tier the next topic we'll be going over
453:57 - is a dead letter q and Azure service bus
454:00 - the dead letter q and Azure service bus
454:02 - is a specialized queue that stores
454:03 - messages that couldn't be delivered or
454:05 - processed successfully these messages
454:07 - might fail due to various reasons
454:09 - message that is sent to a queue that
454:11 - does not exist Hue length limit exceeded
454:14 - message length limit exceeded message is
454:17 - rejected by another q Exchange
454:19 - message reaches a threshold read counter
454:21 - number because it is not consumed
454:23 - sometimes this is called a backout q the
454:26 - message expires due to per message TTL
454:28 - or the message is not processed
454:30 - successfully dead letter cues provide
454:32 - several benefits including monitoring
454:35 - failed message deliveries to understand
454:37 - and address the underlying issues ruing
454:40 - messages for another attempt at
454:41 - processing especially after resolving
454:43 - the reason for the initial failure
454:45 - initiating follow-up actions such as
454:47 - alerting remediation or alternative
454:50 - processing paths when specific failure
454:52 - patterns are detected in essence dead
454:54 - letter cues are vital for ensuring the
454:56 - reliability and resilience of a
454:57 - messaging
455:02 - system the next topic will be covering
455:05 - are the Azure CLI commands for Azure
455:07 - service bus Azure CLI offers various
455:09 - subcommands specifically tailored for
455:11 - Azure service bus these subcommands
455:13 - enable efficient management and
455:15 - configuration as serviceus geore
455:17 - recovery Alias as service bu migration
455:20 - as service bu Nam space as service bu Q
455:24 - as serviceus topic notably unlike Azure
455:27 - storage Q Azure service bus does not
455:29 - have direct CLI commands to send
455:31 - messages to cues or topics for sending
455:33 - messages to a que You' need to use the
455:35 - Azure SDK for example using the node.js
455:38 - SDK you can install the necessary
455:41 - package with npm install at Azure
455:43 - servicebus this will allow you to
455:45 - integrate Azure service bus
455:46 - functionalities directly into your
455:48 - application and send messages
455:50 - programmatically so that's an overview
455:52 - of the key Azure CLI commands for Azure
455:54 - service
455:56 - [Music]
455:59 - bus hey this is Andrew Brown from
456:01 - exampro and we are going to take a look
456:03 - at service bus so what I want you to do
456:05 - is type in servers bus at the top here
456:06 - what's interesting is that this is the
456:08 - old icon they have a new icon so just
456:10 - realize that there's some
456:11 - inconsistencies there and that's not my
456:12 - fault that's azure's fault um but the
456:14 - first thing we need to do is create a
456:16 - name space because a service bus is kind
456:18 - of like you you know storage account
456:19 - where you can have uh a variety of
456:21 - different kinds of storages well you can
456:23 - have uh more than one type of messaging
456:26 - system and so we have our traditional
456:28 - one uh like event messaging similar to
456:31 - storage CU but with first in first out
456:33 - functionality and we have Pub sub via
456:36 - topic so what you'll do is create a new
456:38 - service bus namespace and I'm going to
456:41 - create a new Resource Group I'm can call
456:43 - this the
456:44 - a204 um service
456:47 - bus and we'll say message or sorry Q
456:50 - because we're going to do a que and then
456:52 - we're going to do a topic separately and
456:54 - for this I'm going to call the Nam space
456:57 - um service bus Q to keep it simple we'll
457:00 - let it launch wherever it wants to
457:01 - launch and notice there are multiple
457:03 - pricing tiers depending on the tier
457:06 - affects the functionality so if we do
457:08 - basic we're only going to have access to
457:10 - Q we're not going to have access to um
457:13 - uh topics and so this is totally safe
457:15 - and fine to do um like even if we did
457:17 - premium it's fine because it's based on
457:19 - your consumption it's not based on um
457:22 - you know you just having holding around
457:24 - so we'll get the basic one here and
457:26 - we'll go to networking I don't think
457:27 - there's anything interesting there we'll
457:29 - go ahead and review and create and we'll
457:31 - let that create click create
457:34 - again and it's deploying as that is
457:36 - deploying which will not take too long
457:38 - what I've done is set up a private
457:40 - repository here you'll probably see me
457:42 - use this throughout the course it's
457:43 - literally an empty repository because I
457:45 - already have the code done I've been
457:46 - doing the follow alongs and document
457:49 - them here in the free Azure developer
457:50 - associate but when you're doing follow
457:52 - alongs with me you should do them with
457:53 - uh do them from scratch and then if you
457:55 - need to you can reference the stuff here
457:57 - so um I have this uh separate repository
458:00 - I have a g pod account which has a free
458:02 - tier you can totally do this in your own
458:04 - Visual Studio code on your little local
458:06 - machine the reason I'm doing a g pod is
458:08 - because I always want to show you how to
458:09 - set up the CLI and those other tools um
458:12 - and when you launch G pod gives you a
458:14 - blank environment so I'm just going to
458:16 - launch that up there as that's going
458:18 - we'll go back here and take a look and
458:20 - see if this is ready just hit refresh
458:22 - here it is still going but we already
458:25 - have our environment and while that is
458:27 - going in the background I want to go
458:28 - install the Azure CLI so we don't even
458:32 - have a single file here I'm just going
458:33 - to say readme.md so I can see what is
458:35 - going on here maybe we'll just dump
458:37 - things in here as we go I'm going to go
458:39 - get the Azure
458:41 - CLI uh Linux because this is running
458:44 - Linux
458:45 - Ubuntu uh here so something you should
458:47 - always check is like uh what Linux
458:50 - version am I running if you're on
458:52 - Windows of course this is going to be
458:53 - different but even Windows using the
458:55 - windows subsystem Linux is using Ubuntu
458:57 - as well so what I'll do here I'll go to
458:59 - the first link nope that's not the one I
459:01 - want maybe the second
459:04 - one there's usually like a command here
459:06 - I can run
459:08 - uh
459:09 - which Linux version am I
459:14 - running let's try this one
459:17 - here
459:20 - it's usually
459:21 - like so maybe it's this here cat proc
459:24 - version it really does vary based on
459:26 - what you're using and so here I'm going
459:28 - to go file uh or sorry terminal new
459:33 - terminal we'll paste that in there hit
459:35 - enter and so here it says Linux 513
459:38 - Ubuntu
459:40 - 11 uh 18
459:43 - so I know that this is
459:46 - auntu I don't really like that one there
459:49 - um let's try this
459:52 - instead because it just doesn't read
459:54 - very well there we go we're running
459:56 - Ubuntu 204 I already knew that um but I
459:59 - just wanted to double check and the
460:00 - reason that matters is that when you're
460:02 - installing the CLI it might matter what
460:04 - version you're using so we're going to
460:06 - go here and I'm going to go to Linux and
460:09 - the instructions might vary this one
460:11 - says 16 18 20 so they're all the same
460:13 - here and we have this one liner here
460:15 - that we'll
460:16 - install and what I'll do here is drop
460:18 - drop it in here and hit
460:22 - enter I'm not sure if this font is too
460:24 - small so while that's going I'm going to
460:25 - see if I can bump up our font
460:41 - here I'm looking for the terminal font
460:44 - size here
460:45 - terminal let's just say 20 here
460:49 - there we go and so the Azure CLI should
460:51 - be installed so I'll just type in clear
460:52 - so we say Azure or a to run it looks
460:57 - good to me so we'll type in Azure login
460:59 - um I don't want to log in with um that
461:02 - way I want to log with the device so
461:04 - we'll do ay login device
461:07 - because if you're on your regular
461:08 - computer you can just click a button and
461:10 - go to the browser but I'm not going to
461:11 - be able to do that so I going to have to
461:12 - do device
461:14 - login
461:17 - device
461:21 - well I'll have to do it the wrong way
461:22 - first to do it the right way soit enter
461:25 - Because the problem is if I go here it's
461:27 - going to go to Local Host because it's
461:28 - trying to launch in my local
461:30 - machine so it does that and that's no
461:32 - good and so here it says do the a login
461:34 - use device code okay so that's the one I
461:37 - really wanted to
461:39 - use so hyphen hyphen use device code
461:42 - enter and that will give us a code so
461:44 - what we do is we will need uh this link
461:46 - here so I'll have to expand that
461:49 - to
461:52 - here and then I'm going to go ahead and
461:55 - grab that
462:03 - code continue and so now go back this
462:06 - will authenticate it'll just take a
462:07 - second
462:12 - here close the tab here there we go may
462:14 - had to close the tab uh and so now I'm
462:17 - authenticated so I should should be able
462:18 - to uh do whatever I want um what I need
462:22 - to do
462:23 - next
462:25 - is create ourselves a message Q so we'll
462:28 - go to the resource here and notice here
462:30 - entities it only says Q now if we
462:32 - had uh other than the basic the standard
462:35 - plan then we would see topics here we'll
462:38 - go click into que click create a new
462:39 - queue I'm going to call it my qu we have
462:42 - some options here the Q size can go up
462:44 - to 5 gigabytes the max delivery count so
462:46 - this is the maximum deliveries time to
462:48 - live that is how long they live in the
462:50 - queue before they are dropped out or
462:52 - they are dropped into a dead letter a
462:56 - system there we have lock duration so
462:58 - the set the amount of time a message is
463:00 - locked for other receivers you can
463:02 - enable partitioning uh that's pretty
463:04 - complicated but we'll go ahead and
463:05 - create our que and so this should be
463:09 - pretty darn quick there is our queue
463:11 - we'll click into it and you'll notice
463:13 - that there isn't really a way to view
463:15 - messages there's not a way to add
463:18 - messages uh here we have the service bus
463:21 - Explorer um which I guess technically
463:24 - you can send and receive here I had not
463:26 - noticed this before at least it was not
463:27 - working for me so I suppose we could
463:29 - send a message here saying like Hello
463:31 - World this literally wasn't here last
463:33 - time I checked here um and we can go
463:37 - ahead and just hit
463:43 - send okay and notice here it says
463:45 - there's one active message and we can
463:47 - receive it say yes and so it says it
463:51 - received the message it's not showing us
463:53 - the
463:54 - answer so I guess they're kind of is
463:56 - something here I guess they're still
463:57 - working on it but uh mostly what we're
463:59 - going to have to do is uh do things
464:02 - programmatically so that is why we have
464:06 - this account so what I want you to do is
464:08 - open a new tab here and we're going to
464:10 - type in aure service bus documentation
464:14 - cuz we're going to grab some code there
464:15 - modify it make it our own so it's a bit
464:17 - easier to to work with so here I'm in
464:19 - the service bus we'll go to
464:21 - tutorials um I'm not sure if this one is
464:25 - the right
464:26 - one Azure service bus documentation
464:35 - Q it's the same thing
464:38 - here but this is doesn't look
464:42 - right what is service
464:47 - bus
464:52 - I mean it is the right page but it had a
464:54 - couple tutorials here uh that I had here
464:57 - so we'll type in like Azure service bus
465:00 - uh tutorial
465:04 - topics sometimes things aren't where you
465:06 - think they're supposed to
465:11 - be okay we'll type in service
465:17 - bus
465:18 - seems like the same page
465:20 - again ah it was quick starts sorry so we
465:23 - have tutorials here and then we have
465:24 - quick starts so under the quick starts
465:26 - this is where I was Finding uh the
465:27 - example code that I thought was okay
465:29 - notice that we could do everything via
465:31 - the CLI um that is not that fun but I
465:34 - mean this only does the creation of it
465:36 - doesn't necessarily do sending and
465:37 - receiving messages notice so that we
465:39 - only can use code so we'll use
465:41 - JavaScript because I think that will be
465:42 - the easiest to use uh so I already have
465:45 - node node comes pre-installed on git pod
465:47 - you'll have to figure that out for
465:48 - yourself on your own machine or you can
465:50 - just use get pod as well because it does
465:52 - have a generous free tier what I'll do
465:54 - is go ahead and paste on
465:56 - in this command it doesn't seem to want
465:59 - to paste today so I'll hit copy and then
466:02 - we'll go back here and go right click
466:04 - enter paste hit enter and so what that
466:07 - will do is install that Library if
466:11 - you're not very familiar with nodejs
466:12 - package Json is the package manager and
466:14 - this is showing that this requirement is
466:16 - there I want to install one other thing
466:19 - uh called EnV this will make our lives a
466:21 - lot
466:22 - easier um for node it comes for
466:25 - different things but I just want it
466:27 - for um JavaScript here so then we'll do
466:30 - mpm install. emvy
466:32 - save that's just a safe way for us to
466:35 - pass along our environment variables and
466:36 - so now both of these are installed so
466:39 - what we'll do is we'll go back over to
466:41 - this code and we'll scroll on down and
466:43 - they have one called send and they have
466:44 - one called receive so what I'll do is
466:46 - create myself a couple files here so
466:48 - we'll have send.
466:50 - Js and we'll have receive.
466:57 - Js and then what we'll do is go ahead
466:59 - and copy this is the send code so we'll
467:02 - put this in the sjs
467:06 - file and then down below we have the
467:09 - receive
467:12 - code and we will paste that on in there
467:15 - so I'll just make this bigger and we'll
467:17 - take a quick look look here at what it's
467:19 - doing so what this does is it Imports
467:21 - the SDK for service bus we need to set a
467:25 - connection string we need to set the Q
467:27 - name here is a bunch of messages that we
467:29 - are going to be passing along here we
467:31 - establish a service bus client very
467:33 - common in all sdks a set of a client
467:35 - first then we are creating a um sender
467:38 - and then here we are doing uh create
467:40 - batch messages so it's a way of sending
467:42 - messages in batch very efficiently so we
467:45 - have a for Loop here and uh so it says
467:47 - there's a batch and then it says try to
467:50 - add the message to the batch if it's not
467:52 - wait until it's ready then send the
467:55 - message okay so pretty straightforward
467:58 - for that code receive is going to be
468:00 - similar so connection string Q name
468:03 - create that client create a receiver and
468:06 - then from there we will set up a Handler
468:09 - for the receiver an error and so then
468:11 - we'll subscribe and we will listen for
468:13 - the message and handlers so even though
468:16 - we are doing cu's it's called a
468:18 - subscription still so just don't get too
468:20 - mixed up with that what I want to do is
468:23 - just make sure that we're passing our
468:24 - environment variables in safely or
468:26 - configuration so this is pretty standard
468:28 - or uh good uh best practices when
468:30 - working with any language the idea is
468:32 - you don't want to hardcode your values
468:34 - so I'm going to do process
468:35 - EnV uh and we'll do connection string
468:38 - here and then we'll do process EnV Q
468:43 - name this is the way you grab
468:45 - environment variables in JavaScript for
468:47 - every language it's a little bit
468:50 - different
468:53 - okay and I believe these are the same so
468:56 - I'm just going to go ahead and grab that
468:58 - there like
469:01 - this and I'll paste that on in
469:04 - here and I want to load environment
469:06 - variable so I'll make a new file here
469:07 - called EnV this is all part of that EnV
469:10 - dot thing they we're looking at EMV dot
469:13 - I'm just pulling that up again here or
469:16 - EnV you going to get the the right one
469:19 - because we need this line here requir
469:22 - Dov config that will load the
469:24 - environment variables uh it'll load it
469:26 - from that EnV
469:28 - file so we will go above here and hit
469:33 - paste and then we'll go to the receive
469:36 - here and do this as
469:40 - well and in here we need to Define these
469:42 - so I'm just going to copy this so I
469:44 - don't have to type it out by hand we
469:45 - will paste that on in here and so I just
469:47 - need need the Q Q
469:55 - name and connection string we'll just
469:57 - say equals and then
469:59 - equals so our Q was called my
470:05 - q and then we need to go grab the
470:08 - connection
470:09 - string so I'm just thinking here this is
470:13 - probably yeah it's at the names Space
470:15 - level and we'll go to share access
470:18 - policies notice it's called shared
470:19 - access policies remember when we were
470:21 - doing the storage key it was like called
470:23 - key access so it's totally different
470:25 - interface this is what I'm talking about
470:26 - where um Azure is inconsistent we're
470:28 - clicking on the root manage shared
470:30 - access key probably could create your
470:32 - own so it doesn't have full privileges
470:34 - but for this purpose we're just going to
470:35 - use this one on the Le hand side we have
470:38 - a primary and secondary we're going to
470:39 - use the primary one and we will go back
470:42 - here and we will paste on that value
470:46 - in so I'll paste that in there notice we
470:48 - don't have to do double quotations here
470:51 - it should uh already Escape in double
470:53 - quotations but when we're doing the CLI
470:54 - when we did the storage accounts that
470:56 - wasn't something that we could do um so
470:58 - we have these two values here so they
471:00 - should get loaded when we use them and
471:03 - this should all be good so we'll type in
471:05 - node sjs and hopefully it just works
471:08 - fingers crossed and so it sent a batch
471:10 - of messages to the queue so we'll go
471:12 - back over to our Q here and see if we
471:15 - can see
471:16 - anything
471:18 - and we'll click into
471:23 - here I'm just trying to see so there's
471:25 - 10 active messages that are here right
471:28 - now and so what we'll do is we'll
471:30 - receive all those messages so we'll go
471:31 - up and hit node receive
471:37 - JS so this code is now receiving those
471:40 - messages from the service bus q and
471:43 - we're just going to wait here because it
471:44 - takes time for whatever reason to uh
471:47 - finish here but we'll give it a little
471:48 - bit of time here to figure out that it's
471:53 - done still
471:57 - waiting there we go and so that's all
472:00 - there really is to it so that is q and
472:04 - we will do this again but next time with
472:06 - topics so what I want you to do is make
472:08 - your way over back to your resource
472:10 - groups we'll find the one that we just
472:12 - created which was uh this one here AZ
472:16 - 204 service bus Q we'll delete this
472:19 - Service
472:20 - Group hit delete delete and there you go
472:23 - and as always double check to make sure
472:26 - that you've uh for sure deleted that
472:27 - stuff and that's it for service bus Q
472:31 - we'll do topics next
472:33 - [Music]
472:36 - okay hey this is Andrew Brown from exam
472:38 - Pro and we are looking at service bus
472:40 - and this time we're looking at topics so
472:42 - what I want you to do just like before
472:44 - is go to the top we'll type in service
472:46 - bus and we'll go to the service bus uh
472:48 - service you can still see the old one is
472:50 - there it should be deleting that's how
472:53 - slow this thing is but we'll create a
472:55 - new one and we will create a new
472:57 - Resource Group and we will call it
473:01 - a204 uh service bus topic we'll say okay
473:06 - we'll name this service bus topic and
473:10 - this time it already exists and if it
473:12 - does just dump a bunch of numbers here
473:14 - on the end because it's Unique based on
473:16 - that so having a domain name if somebody
473:18 - has it you're going to have a problem
473:20 - and so here I'm going to go to standard
473:21 - because in order for us to use those
473:23 - additional features we will need to be
473:25 - on the standard plan so we'll go ahead
473:27 - and hit review and
473:28 - create and that'll take a little bit of
473:30 - time to create but while that's going
473:32 - I'm going to uh launch my environment
473:35 - here so this was the one I was just
473:36 - using a moment ago with uh GitHub and so
473:39 - what I'm going to
473:40 - do is I'm just going to go back to our
473:43 - repo here I'm just doing it off screen
473:45 - because I don't want to expose all my
473:46 - stuff here
473:48 - here and again if you want to you can do
473:51 - this in your local V Visual Studio code
473:54 - I just want to show everything from
473:55 - scratch every time so here's my M3 repo
473:57 - with get pod and so I'll just close this
473:58 - one and it will vanish all that code is
474:00 - now gone I'll H get pod this will launch
474:02 - a new environment it's trying to tell me
474:03 - to open the last one nope I'm going to
474:05 - make a new one for this workspace here
474:07 - and we'll get going here in a moment so
474:10 - this namespace has been created so we'll
474:12 - go ahead and hit
474:16 - create and uh I guess it's still making
474:21 - I I thought I already deployed it there
474:22 - but I guess I
474:26 - didn't okay while that's going we'll go
474:28 - ahead and install the Azure CLI we'll
474:30 - type in Azure CLI Linux because that is
474:33 - what we're using here
474:35 - today we'll go to Linux here we'll
474:38 - scroll on down grab this oneliner here
474:41 - to install it I'm going to open up my
474:43 - terminal your terminal might be
474:45 - somewhere else
474:48 - allow I'm going to go ahead and paste
474:50 - that on in there that's going to install
474:52 - our Azure CLI as that's installing it
474:55 - will be done here in a
474:57 - moment let's see if this is done this is
475:00 - still creating the uh namespace I
475:05 - think and this is still installing
475:07 - shouldn't take too long while this is
475:10 - going we can start grabbing the code for
475:12 - this so for this we'll type in Azure um
475:15 - Azure service bus
475:18 - documentation and this I found it under
475:20 - the quick
475:21 - start so we will go to Quick Start Ser
475:25 - topics and services because there's some
475:27 - code here that I want under the
475:30 - JavaScript and we have one for send to
475:33 - topic so we'll just grab that name there
475:36 - make a new file send to topic
475:39 - JS and we will go down here to this
475:42 - other one this
475:44 - is receive from subscription you're
475:48 - going to notice this is very similar to
475:49 - doing a q the difference is that you can
475:52 - have multiple subscriptions consume the
475:54 - same
475:55 - stuff uh our CLI is done so we'll type
475:57 - in a login um I can't remember what it
476:01 - is so we'll hit enter here I know this
476:02 - is the wrong way but I just can't
476:04 - remember what it is I want used device
476:06 - code that's what it is so I'll go ahead
476:07 - and grab this and I'll just hit contrl C
476:10 - to exit out of that and then paste that
476:12 - in there and then we'll do it the way
476:14 - that we actually want to do it so then
476:15 - we will go ahead and grab grab this for
476:17 - device
476:19 - login we will provide the code AS
476:22 - suggested
476:26 - here we will hit
476:28 - continue we will close
476:31 - this and it will say that we are
476:33 - authenticated so we are now
476:34 - authenticated we can use a or the Azure
476:38 - c
476:40 - um I don't know if we need the CLI well
476:43 - we have it anyway at least we had to
476:44 - authenticate so at least that was out of
476:46 - the way I'm not sure if we're actually
476:47 - going to use the CLI in this one um but
476:50 - what we'll do is go to the resource
476:54 - here and we want to create a topic this
476:57 - time around and I'm going to call this
476:58 - my topic we have a topic size between 1
477:01 - to five you have a TTL you can do U make
477:05 - sure there aren't duplicates we're going
477:07 - to go ahead and create this
477:13 - topic okay so just like last time we
477:16 - need to install install a couple things
477:18 - so if we go to the top here there should
477:20 - be like an npm install here it is npm
477:22 - install service bus so we'll paste that
477:25 - on in there we'll need ourv so look upv
477:28 - again and we will just install npm
477:30 - install. EnV for environment variables
477:33 - hit enter we'll create ourselves a newv
477:38 - file and we will need to look at what
477:40 - environment variables we are going to
477:42 - need so uh looks like we didn't copy the
477:44 - send to topic content at least I didn't
477:47 - so we will go back over to here and go
477:50 - up here and get sent to topic paste that
477:53 - on in there we'll go all the way to the
477:56 - top and I'm looking for what we need
477:57 - here so here we have one which is
478:00 - connection string
478:01 - whoops did not mean to delete all that
478:03 - so we'll hit process EnV connection
478:09 - string and we have to spell it right or
478:11 - it's going to have a
478:13 - problem and then we have process EnV
478:18 - topic name and I think in this one it
478:21 - actually also has subscription so we'll
478:23 - have to have a third one so I'm just
478:25 - going to copy these two over so I'm to
478:27 - type it 100
478:30 - times and this one will
478:33 - be process EnV subscription name you're
478:36 - noticing I'm not having the um semicolon
478:39 - that is optional in JavaScript so it
478:41 - won't break anything if I don't have
478:43 - them there subscription name
478:48 - and so these are the three that we'll
478:49 - need I'm going to just split this to
478:51 - make my life a little bit easier and I'm
478:53 - just going to copy this here
478:55 - paste not exactly how I wanted to paste
478:58 - it but that's totally
478:59 - fine I'm going to copy
479:02 - this I don't think we made a
479:04 - subscription yet but I know we're going
479:05 - to call it my subscription to make our
479:07 - lives easy so we'll say my
479:11 - topic my
479:13 - subscription and we'll grab this in a
479:15 - moment so we'll go back over
479:18 - to Azure we got too many tabs open here
479:21 - while we're here we might as well go
479:22 - grab this code um before we go back to
479:24 - Azure so I'm going grab grab this EMV
479:26 - config that is going to load our
479:29 - configur uh configuration environments
479:31 - so we go to the top here paste that on
479:33 - in there for both
479:37 - files again best practice never to
479:39 - hardcode your values always pass it in
479:41 - like that with environment variables
479:42 - we'll go back to our service bus the
479:44 - first thing we need well we made a topic
479:46 - we're going to need a subcription so we
479:47 - go to topic here and click into it we
479:50 - can create ourselves a subscription so
479:52 - we'll go here we'll say my subscription
479:55 - notice we have Max delivery count we
479:56 - have to set this between a value of 1
479:58 - and 2,000 I'm going to say 12 for fun we
480:01 - can set the idle notice here if we want
480:03 - to um have first in first out we would
480:06 - checkbox enable sessions we're going to
480:07 - leave that alone does not matter too
480:10 - much for our demo so we'll go ahead and
480:13 - create that subscription it is created
480:15 - now what I need you to do is go back to
480:16 - the the service bus topic namespace and
480:19 - from there on the left hand side we're
480:21 - going to Shared access policies we're
480:23 - going to click into the root manage
480:24 - shared access we're going to grab the
480:27 - primary connection string key the
480:28 - secondary would work too it's just a
480:30 - second optional one because they always
480:31 - give you a two we'll paste that on in
480:34 - there and if this is all correct these
480:36 - should just work so now what we'll do is
480:38 - type in
480:41 - node um actually we did want the CI
480:43 - installed because I wanted to show you
480:45 - uh that there was stuff in the queue
480:47 - whereas with um storage queue when we're
480:50 - doing Azure storage que I couldn't show
480:51 - you because um I just didn't know of a
480:53 - command I believe at least I think that
480:56 - was the case but any we'll take a look
480:57 - and see what we can see okay um
481:01 - so actually we didn't do it at any time
481:04 - I wonder which one I did that for let's
481:05 - just double check here I have um off
481:09 - screen here
481:10 - somewhere my instructions because I
481:12 - wrote it for one of these maybe it was
481:14 - for the que that we did
481:15 - it
481:18 - yeah I didn't do the last one but I did
481:19 - an Azure service bus Q show and the idea
481:22 - was to show you that there was a message
481:23 - count 10 so you could see the que but I
481:25 - think that since we saw it in the UI
481:27 - just wasn't too worried about it now did
481:29 - I do it for this one I'm not sure yeah
481:31 - we do a topic show so we'll do that for
481:33 - fun but first we need to insert our
481:36 - messages which are part of the topic
481:38 - send I don't think we read through these
481:39 - so let's just quickly read they look
481:41 - very very similar to the last one so you
481:44 - have your messages that you want to send
481:45 - you create your yourself a client uh you
481:48 - create yourself a sender you create a
481:50 - batch message send message like it's
481:53 - basically
481:55 - identical like I can't even tell the
481:57 - difference here except here we're
481:58 - supplying a topic name so
482:02 - um I mean I see it
482:05 - here ah here create Center the topic
482:07 - name is specified there so I just
482:09 - imagine that instead of providing a
482:11 - topic name you provide the Q name and
482:12 - that's how it knows the difference but
482:14 - anyway what we'll do is go ahead and
482:15 - execute this code we'll say node sent to
482:17 - topic uh
482:20 - JS okay and so it sent the stuff now we
482:22 - didn't do this last time so let's do it
482:24 - this time around so we'll type in Azure
482:25 - service bus topic show um and here we'll
482:29 - need to set the resource Group so this
482:31 - was called I don't know let's go take a
482:34 - look here what is our service group
482:38 - called it is called AZ
482:41 - 204 bring that down a bit here
482:45 - a 20
482:47 - for service bus topic we need to specify
482:51 - the
482:52 - namespace name so that's just called
482:57 - service bus topic hyphen
483:02 - 7238 49 because we couldn't get the
483:04 - number we want and then we need to
483:05 - specify the name so I assume it's the
483:08 - name of the topic so the top is my topic
483:11 - and I'll
483:14 - enter and it says service bus name is
483:19 - misspelled or not recognized by the
483:21 - service you mean service
483:24 - bus yeah I got to spell that right
483:28 - enter and that looks fine but I just
483:29 - want to specify it as output yaml just
483:32 - hit up on your keyboard if you want to
483:33 - go back to those previous commands with
483:35 - enter so this is a little bit easier to
483:37 - read and so what we're looking for here
483:39 - is just kind of like the message
483:43 - count does it show us here we
483:45 - subscription count
483:48 - I don't see it so I guess it's not
483:50 - visible in the same way as uh the que is
483:53 - like if we did this and we didn't do in
483:55 - the last one it would we just saw like
483:56 - that message count there but let's take
483:59 - a look at what we can see in the CLI to
484:01 - just see what information or a UI that
484:04 - or portal so we can see some information
484:05 - here so we have one subscription here
484:07 - we'll click into
484:08 - here um we have Max size incoming
484:12 - request
484:15 - 12
484:18 - um yeah I don't really see it message
484:21 - count 10 Max delivery count
484:25 - 12 okay so I guess there was 10 and
484:27 - that's the 12 so I guess that's where
484:29 - it's being counted let's go run the
484:31 - other one to receive I'm going to just
484:32 - double check to make sure that we set
484:34 - those we did that's all good so we'll
484:36 - type in node receive if you're wondering
484:39 - how I'm Auto completing without typing
484:40 - that I just hit tab on my keyboard so
484:42 - it's receiving
484:44 - those good and that finish I'm going to
484:47 - go hit refresh see if there's any
484:48 - difference here notice that the message
484:50 - counted zero so when the topics were
484:52 - there it was held in the subscription
484:54 - saying 10 is here and they've yet to be
484:56 - delivered when we ran it they were
484:59 - received and so that number cleared out
485:01 - that's all we really need to learn for
485:03 - um uh topics so we are done with topics
485:06 - so let's make our way over to Resource
485:09 - groups and we'll go over to our service
485:12 - bus
485:13 - topic go ahead and delete this here
485:24 - delete there it is deleting we are all
485:27 - good to go um and you know just as
485:29 - always don't ever trust Azure to delete
485:31 - these things come back and check in 3
485:33 - four minutes make sure it's deleted so
485:34 - you just don't have things lingering
485:36 - around um but uh yeah there you
485:42 - go hey this is Andrew Brown from exam
485:45 - Pro in this section we're diving into
485:47 - Azure event grid Azure event grid is a
485:49 - service that allows you to manage event
485:51 - routing from any source to any
485:53 - destination event grid provides a simple
485:55 - and strong customizable event delivery
485:57 - process that allows you to manage at a
485:59 - minimum level which types of events will
486:00 - be received as well as which subscribers
486:02 - will receive those events one of event
486:05 - grid strengths is its event fan out
486:07 - capability combined with 24-hour retry
486:09 - reliability ensuring your events are
486:11 - consistently delivered it's a
486:13 - costeffective serverless solution with
486:14 - dynamic scalability making it an
486:16 - excellent choice for businesses of all
486:18 - sizes asure event grid is ideal in
486:21 - event-driven architectures you can
486:23 - subscribe to events from Azure resources
486:25 - and then route them to an event handler
486:26 - or webbook Furthermore with custom
486:29 - topics you have the flexibility to craft
486:31 - and publish your own events within your
486:32 - event grid so that's an overview of
486:35 - azure event
486:40 - grid the next topic will be covering our
486:42 - event sources and handlers in Azure
486:44 - event grid Azure event grid is divided
486:47 - into two categories event sources and
486:49 - event handlers azure's event grid
486:52 - operates as a central Hub coordinating
486:54 - between various event sources and event
486:55 - handlers on the side of event sources
486:58 - which are services that emit data we
487:00 - have blob storage signal R resource
487:02 - groups Azure app configuration
487:04 - subscription Azure machine learning
487:05 - event how Azure communication Services
487:08 - media service Azure cash for Ries iot
487:10 - Hub Cloud events service bus Azure
487:13 - policy Azure Maps custom events and
487:15 - Azure container registry these sources
487:17 - send their data to the event grid on the
487:19 - other end we have event handlers which
487:21 - are the services that receive and act
487:23 - upon this data the serverless code
487:25 - category involves functions for workflow
487:28 - and integration the services user
487:29 - service bus and logic apps buffering and
487:32 - competing consumers handle the influx of
487:34 - data through event Hub and storage queue
487:37 - additionally there are other services
487:38 - and applications which incompass hybrid
487:40 - connections web sockets web hooks
487:43 - Automation and essentially any service
487:44 - or application
487:46 - these handlers effectively process or
487:48 - route the data sent by the event sources
487:50 - through event grid enabling a seamless
487:52 - flow of information across Azure
487:58 - Services the next topic will be going
488:00 - over are the key concepts of azure event
488:02 - grid domains are used to group event
488:04 - grid topics that are related to the same
488:06 - application for easier management topics
488:09 - these serve as the destinations to which
488:11 - events are dispatched system topics
488:13 - Azure services offer these built-in
488:15 - topics in contrast custom topics pertain
488:18 - to individual applications and
488:19 - thirdparty topics additionally partner
488:22 - events enable thirdparty software as a
488:24 - service providers to broadcast events
488:26 - events these encapsulate specific
488:28 - occurrences within a service Publishers
488:31 - is the service that published The Event
488:33 - Event sources are where the event took
488:35 - place event subscriptions are the
488:37 - mechanism that routes the events event
488:39 - subscription expiration this allows
488:41 - users to designate a lifespan for their
488:43 - event subscriptions after which they
488:45 - become valid event handlers these are
488:48 - the applications or Services tailored to
488:50 - process or act upon the received events
488:52 - event delivery is the delivery of events
488:54 - and batches or a single events batching
488:57 - is the sending of a group of events in a
488:58 - single request overall these are the key
489:02 - concepts of azure event
489:04 - [Music]
489:07 - grid hey this is angre Brown from exam
489:10 - Pro and we're going to take a look at
489:12 - event grid Basics so let's get to it so
489:15 - the first thing we're going to want to
489:16 - do is we're going to search up
489:18 - subscriptions because we're going to
489:19 - need to make sure that in uh like in
489:21 - order for us to use aent grid that we
489:24 - have it turned on and so we'll go into
489:27 - our subscription and we're going to go
489:28 - under um resource
489:32 - providers resource
489:34 - providers ah there it is and this is all
489:37 - the stuff that is registered all the
489:38 - providers that are registered so what
489:40 - we're doing is just make making sure
489:41 - that event grid is turned on because
489:42 - that one's not always turned on by
489:44 - default and just make sure that it's
489:46 - registered okay so you'll know that it's
489:48 - registered because it'll have a green uh
489:50 - check mark here and it'll say registered
489:52 - and so once that is done we can proceed
489:53 - to create a storage account because
489:56 - we're going to integrate a storage
489:59 - account into our event grid so we we'll
490:01 - do is create a new storage account and
490:04 - we're going to create a new Resource
490:06 - Group I'm going to call this Resource
490:08 - Group event grid
490:11 - Basics and we're going to name our
490:13 - storage account event
490:16 - event grid
490:20 - Basics all right and uh yes you can have
490:23 - hyphens you can't have anything else um
490:26 - and we'll just make sure we spelled that
490:27 - right
490:29 - Event Event grid Basics I can't remember
490:32 - if these are fully qualified domains if
490:34 - they are you might have to add some
490:35 - numbers on the end there but it is what
490:37 - it is we're doing uh USC I mean it just
490:40 - randomizes every time but this is where
490:42 - you should probably set it to we have
490:44 - standard premium we'll leave it as
490:45 - standard and everything else seems fine
490:47 - so we'll go ahead and create review and
490:50 - create and then it's going to allow us
490:53 - to review we'll go ahead and hit create
490:55 - and it's going to create the resource
490:57 - Group here we'll just wait for it finish
490:59 - uh deploying and then we'll go into the
491:01 - resource all right looks like it is
491:04 - finished deploying so we're going to go
491:05 - ahead and go to that resource we're
491:06 - going to go to containers and we're
491:07 - going to have to create a couple
491:08 - containers the first the first one is
491:10 - going to be called basic we're going to
491:11 - leave it as uh private just make sure
491:13 - it's basic not Basics and and we'll
491:16 - create another container called basic
491:17 - alt the idea is that we're going to use
491:19 - an event Grid in order to move one file
491:21 - from one storage account to another uh
491:24 - and that's going to be facilitated by
491:25 - log uh via a logic app because that's
491:27 - going to be the easiest way to use event
491:29 - grid um so what I want you to do is
491:31 - search for logic apps up here and we're
491:36 - going to go ahead and add a new logic
491:39 - app and what we want to do is choose our
491:43 - Resource Group and we'll just go event
491:44 - grid Basics here
491:46 - uh we are going to name uh this event
491:49 - grid
491:51 - Basics um maybe we'll do LG to indicate
491:54 - that it is a logic app or LG maybe uh L
491:59 - yeah LG is fine like short for logic and
492:02 - from here we have a workflow or a Docker
492:04 - container we're going to stick with a
492:07 - workflow um just to knock on wood we're
492:10 - going to just put in the same region as
492:12 - our um storage account so East US we
492:15 - have standard consumption I'd rather do
492:17 - consumption for this um so you pay only
492:20 - as much as you use because we don't need
492:22 - Enterprise level service application
492:23 - here we just need a consumption model
492:26 - we're going to leave this uh to disable
492:28 - so that is totally fine there's nothing
492:30 - else to do here so we'll go review plus
492:33 - create and we'll go ahead and create
492:36 - this logic app and we just have to wait
492:38 - for this to finish
492:41 - deploying all right so that should have
492:42 - been very very quick like under uh 10
492:45 - seconds there
492:46 - so we've gone into the resource I just
492:47 - click go to the resource and so we have
492:49 - this uh very fun interface and so what
492:52 - we need to do is start with a common
492:54 - trigger there's a few different ways to
492:55 - get to it but there should be something
492:58 - on the front
493:00 - here uh I don't know if they redesigned
493:02 - this
493:04 - recently so I'm just going to search
493:06 - start with a comment oh yeah it's up
493:08 - here okay I'm being silly um and so what
493:12 - we want to do is because this is an
493:14 - event grid follow along we want to click
493:17 - on when an event grid
493:19 - occurs and so this is the designer where
493:22 - we can make things a lot easier for
493:23 - ourself and so we're going to have to
493:24 - first sign in to authenticate so I'm
493:27 - this is my tenant exam Pro Training Inc
493:29 - so we'll go ahead and get connected
493:31 - there just give it a moment we'll select
493:35 - Andrew Brown which is totally fine and
493:38 - now that is connected so that is great
493:40 - so once we are signed in we can click
493:42 - continue and we're going to go ahead and
493:44 - add select our subscription here um and
493:48 - we need to choose a resource here so I
493:53 - guess in this case it's going to be
493:54 - event
493:56 - grid event
493:59 - Grid
494:06 - or could have swore yeah yeah I think
494:09 - that's what we want to do let me just
494:10 - double check here oh you know what it's
494:12 - just not for some reason I'm I'm
494:14 - searching it's not Auto completing
494:15 - properly okay I just wasn't sure there
494:18 - and as far as I can remember this would
494:20 - be probably an event grid
494:24 - topic and then we need to give this a
494:27 - resource name
494:30 - so
494:32 - um let me just think about this for a
494:34 - moment okay all right so I think I
494:37 - understand where my confusion was it was
494:38 - because we click continue and I never I
494:40 - didn't see event crit anymore so I
494:41 - thought we had to configure it when it
494:43 - was already configured right so this is
494:44 - where we were
494:45 - so we're not we at this stage like event
494:47 - Grid's already hooked up so it's ready
494:49 - to be triggered so this is the step that
494:51 - follows into it which is where we want
494:53 - to do our storage account so that's
494:55 - where I was getting confused so we'll
494:58 - choose our subscription here it's okay
495:01 - you know if you ever get confused just
495:02 - step uh step back a couple steps and
495:04 - just double check what you're doing
495:06 - happens to me all the time so um what we
495:10 - want to do is actually um connect
495:12 - storage uh storage accounts so we type
495:15 - in storage accounts here great uh we'll
495:18 - have that selected and then we need to
495:20 - select our storage account so this one's
495:21 - called event grid
495:23 - Basics and then we're going to have to
495:25 - enter in um some additional information
495:28 - event type so we want to have it happen
495:30 - when we add something to The Container
495:33 - so the basic container so we'll do blob
495:36 - created um and then from there we need
495:38 - to actually filter out the information
495:39 - so we need to add a new parameter and I
495:42 - think we'll have to do it on the prefix
495:44 - filter so a filter like whatever yeah so
495:46 - that's probably a good idea CU then we
495:49 - could place it into a particular place
495:51 - and I believe that um there are very
495:56 - specific uh filters that you can do for
496:00 - this because if I
496:03 - recall there's like standardized ones
496:05 - yeah see here like it's always going to
496:06 - be SL blob Services default containers
496:09 - Etc and you'll know that because you
496:11 - know if you read the documentation and
496:13 - you have to do that stuff you'll figure
496:14 - that out so I'm just just going to type
496:15 - it by hand here blob Services default
496:19 - containers and then we can put our
496:20 - container name so basic uh and I believe
496:24 - we have it without the S
496:26 - there
496:27 - so uh Services see I don't trust my
496:30 - writing here I'm just going to copy
496:31 - paste it
496:33 - in okay and that looks good to me um so
496:38 - I think that is what it needs to be so
496:40 - we'll go ahead and hit the next
496:42 - step and so the idea is anything in that
496:44 - folder
496:46 - uh like when something's added to that
496:48 - folder then follow up with this
496:50 - operation
496:53 - right um and maybe before we do that we
496:56 - should probably um you know observe that
497:00 - this stuff works that's probably be a
497:02 - good idea so what I'm going to do here
497:04 - is I'm going to make my way back over to
497:06 - our storage account so we'll just close
497:07 - this tab here I'm going to open a new
497:10 - tab and we're going to make our way over
497:12 - to storage
497:13 - accounts and we'll go event grid
497:16 - Basics and we will go to
497:19 - containers and we'll click into our
497:21 - basic container because I want to just
497:23 - see that this is working um and I'm
497:26 - going to need a file upload so let me
497:27 - just go grab an image really quick all
497:30 - right so I just grabbed an image off the
497:31 - internet so I just have data here but
497:33 - before we upload we probably should save
497:35 - what we have because if we don't save it
497:37 - we're not going to be able to observe it
497:38 - so I went back to logic app and we just
497:40 - hit save in the top left corner so we'll
497:43 - give it a moment to save and it looks
497:46 - like it's saved now I'm just going to go
497:48 - back over to here if we can um look at
497:52 - some of the code that gets executed I'm
497:54 - just trying to remember uh where it is
497:57 - because once it executes we want to um
497:59 - see what happened right so what I'll do
498:02 - is I'm going to go all the way back over
498:03 - here and I'm just going to go and drag
498:05 - or actually I'll have the upload button
498:07 - so I don't trust that that there and
498:08 - I'll drag it onto here nope I still
498:10 - don't trust it so what I'm going to do
498:12 - is just click the files and I'm just
498:14 - going to grab it this way and say open
498:16 - and we'll do
498:18 - upload and so that is now uploaded and
498:21 - so there is somewhere where we can
498:25 - observe um where stuff has happened so
498:27 - I'm just trying to remember where it
498:31 - is
498:32 - [Music]
498:34 - um I mean we could run the
498:36 - trigger we yeah we probably should run
498:39 - the trigger
498:41 - right
498:44 - run
498:54 - I think it's running so we'll just give
498:55 - it a moment
498:57 - okay you know I was thinking about it it
498:59 - doesn't make sense we shouldn't have to
499:01 - run it because it should just happen
499:02 - automatically I think it's on the
499:03 - overview page ah okay so if we look here
499:09 - we can see the Run history and so and
499:11 - there's also trigger history of when the
499:12 - things are triggered so we could we
499:14 - could manually but it doesn't make sense
499:15 - so I think this is the run that we just
499:17 - did if we click into here yes this is
499:20 - what it is so here we can see what what
499:21 - has happened so if we expand it we can
499:23 - see the inputs right so it's we have a
499:26 - blob
499:27 - created um it might show some
499:29 - information so here we can see data yep
499:32 - and it's a webp file and so it's gotten
499:35 - this far through and so that's a great
499:36 - way to kind of like debug so you can uh
499:38 - logic run app you can do it each step
499:40 - but right now we are using um event grid
499:42 - to do that integration right we're just
499:45 - doing through logic uh logic app because
499:47 - it's a lot easier so now that we have
499:50 - that what we should do is go back to our
499:53 - designer and we're going to have to add
499:55 - the follow-up step um so we have this oh
500:00 - yeah okay so that's the first step event
500:02 - grid doesn't show up there which is
500:03 - weird but um so we have this step here
500:06 - from our storage account and so the next
500:07 - step what we want to do is put it into
500:11 - another uh container so that will be the
500:14 - tricky part
500:16 - um so I'm just trying to remember what
500:18 - we do so we'll hit next
500:20 - step
500:24 - and
500:26 - um I think what we need to do is
500:28 - initialize a variable first because
500:30 - we're going to have to get some way to
500:32 - grab the name of the string because if
500:35 - we go back to our run over
500:38 - here just give it a moment
500:41 - here and we go enter a run again here
500:47 - we need to extract some data to pass
500:48 - along because there are some limitations
500:50 - in terms of how Json gets passed along
500:52 - or data gets passed along and so um what
500:55 - we want is we just want this part of the
500:56 - name we want to say take this name as
500:59 - the identifier so that when we're
501:01 - copying stuff over it will work and so
501:04 - what we'll have to do is store that into
501:05 - an intermediate variable so um we'll
501:08 - just type in variables
501:13 - here
501:19 - and I'm just seeing ah yeah so they look
501:22 - like this CU I can remember they might
501:23 - be in the built-in yeah that looks a lot
501:25 - better um and so we need a variable and
501:29 - it's an initialized variable and we're
501:31 - going to name this file
501:34 - name and this is a string of course and
501:37 - now we need to insert the value so in
501:40 - here what we need to do is write an
501:42 - expression in order to extract that
501:44 - information out
501:45 - um so what we'll do is go to the
501:47 - expression Tab and over here you can see
501:50 - we have all sorts of Expressions that we
501:51 - can use so going to type in last
501:54 - parentheses and then in there we'll do
501:56 - split
501:58 - parenthesis and then what we're looking
502:00 - for is trigger
502:02 - body and then we'll do question
502:06 - mark Square
502:08 - braces single quotations subject how did
502:12 - I know how to do that I I looked it up I
502:16 - looked it up somewhere and you know I
502:18 - just don't feel like there's much uh
502:20 - reason to to teach this part because uh
502:22 - you I mean if you really need to know
502:24 - you can go here um and learn all about
502:26 - it but a lot of times like if you need
502:28 - something you can just say I need this
502:30 - kind of function somebody's already done
502:32 - it right because there's so many common
502:33 - use cases so I probably search something
502:35 - like how do I get the name out of the
502:38 - the thing you know like for the blob and
502:40 - somebody had that there but it makes
502:42 - sense to me so let's hit okay here and
502:45 - it should turn purple because it is dime
502:47 - expression if you type it in here it
502:48 - probably won't work correctly you have
502:49 - to type it in here and then hit okay so
502:51 - it shows up like that uh but you notice
502:54 - we typed in like trigger body so if we
502:56 - go back over to our run here um this is
502:59 - the body here so when they say trigger
503:01 - body they're talking about here and then
503:03 - it was just grabbing that subject line
503:05 - there all right um so that would be the
503:09 - second step and that gets it into a
503:11 - variable but the next part is we need to
503:13 - actually um
503:15 - get the blob content and then insert it
503:17 - and then create a new blob so what we'll
503:19 - do is hit next step and we'll type in
503:21 - Blob and see if we can find anything
503:28 - here and from here we need to get the
503:32 - get Blob content using path version
503:35 - two so I'm just going to scroll down
503:37 - here and take a look for it there it is
503:40 - based on the
503:41 - path and we'll go down here and um
503:46 - um I guess it would be access key oh
503:49 - because we're setting up a connection
503:50 - for the first time
503:54 - so enter name for
503:59 - connection
504:00 - um I know what the storage account is
504:04 - but what is the connection connection
504:05 - name I do not remember give me two
504:07 - seconds okay there wasn't much to help
504:09 - me here cuz what I remember before was
504:11 - that you click it and you'd authenticate
504:12 - it like the event grid but it's not
504:13 - doing that SoBe maybe we just have to
504:15 - name it something so I don't know we'll
504:17 - just say Azure storage account maybe it
504:19 - just wants a
504:21 - name maybe it doesn't really
504:23 - matter oh yeah like they sign in that's
504:26 - what I want so connection name yeah so
504:29 - we'll say uh you know storage account
504:33 - event
504:34 - grid
504:35 - okay because if we can just single sign
504:38 - on let's do that that's super
504:42 - easy and we'll click that there
504:47 - okay so this is starting to look how I
504:48 - expect it to look and so we need the
504:51 - storage account name I don't know why
504:53 - it's not showing me any names here but
504:56 - that's okay we'll just go over back to
504:58 - our storage account here and it's called
505:00 - event grid
505:01 - Basics so we'll type in event grid
505:08 - Basics event grid
505:13 - basics
505:15 - uh that's custom value
505:18 - sure I mean that's what its name is I'm
505:20 - not sure why it's not autocom
505:22 - completing um but here what we need to
505:24 - do is we need to provide the path so
505:27 - it's going to be for SL basic ah and so
505:30 - now there's our environment V or that
505:32 - variable so we'll just click that there
505:34 - so that will make it super super
505:37 - easy now notice that it is showing basic
505:40 - now so I just clicked here the folder we
505:41 - typed it in manually but we could have
505:43 - clicked uh here and then put the
505:44 - environment variable in or the uh this
505:47 - this initialized variable in here but I
505:49 - did type that manually and it still did
505:51 - work correctly so we are okay here infer
505:53 - the content type sure why not um it
505:57 - doesn't matter if they do um so this
506:00 - gets the content so now this gets the
506:02 - path and so the next thing is actually
506:03 - to create the blob you can't like do an
506:06 - easy clone you have to do it this um
506:08 - intermediate step that's just how it
506:11 - works um and so what we'll need to do is
506:13 - go to our built-in once here we'll type
506:15 - in Blob again maybe
506:21 - standard and this time we want to create
506:24 - a
506:25 - blob so there it is uh block blob no we
506:29 - just want a
506:30 - blob and so what we'll do is um I guess
506:34 - we have to connect again I'm surprised
506:36 - it's not showing the name yeah it's just
506:38 - the name that's fine so we'll go back
506:39 - over
506:41 - here I just don't want to type it wrong
506:43 - so we'll just copy paste in event grid
506:47 - Basics event grid Basics enter custom
506:50 - value because it's giving us so much
506:51 - trouble for no particular reason make
506:53 - sure there's no space on the end there
506:54 - there now it works fine um and in this
506:57 - case what we want is basic
507:01 - alt and the blob name can be the file
507:04 - name which is totally
507:06 - fine and the blob content will be the
507:08 - file
507:09 - content and I don't think we need
507:12 - anything else so what we'll do is go
507:15 - ahead and click off and we will
507:19 - save all right so that's just the way
507:22 - we're going to have to do it um so what
507:25 - we can do is go back to our
507:28 - overview and we'll go back to our basic
507:32 - folder and we'll delete data say
507:37 - okay and we'll go
507:39 - upload we'll select our file again we'll
507:42 - grab it we will upload and then we will
507:45 - make our way back over to our logic
507:51 - app close this tab here so we don't get
507:53 - too mixed up refresh the page and it
507:57 - failed so it failed for some reason so
507:59 - something has not been configured
508:01 - correctly it failed on the initialized
508:03 - variable so something's wrong there so
508:06 - unable to process the template language
508:07 - expression in the actions initialized
508:09 - variable uh inputs at line Z column zero
508:12 - template function split is not defined
508:13 - is not valid so it's possible I just
508:15 - spelled it wrong so what we'll do is go
508:18 - back to our event grid we'll go back to
508:19 - our logic app designer here initialize
508:23 - variable we will click
508:25 - it and we probably just spiled it
508:29 - wrong
508:33 - spilt spilt what if we do lit spilt
508:38 - split um so if that's wrong we'll just
508:41 - scroll on down and we'll just take a
508:43 - look
508:46 - I could have swore that it autoc
508:48 - completed for
508:52 - us oh you know what it is spelled wrong
508:54 - it should be SP i l t
508:58 - spoit all right and I'm just double
509:01 - checking to see if there's any other
509:03 - problems here no looks fine to me so go
509:07 - ahead and say update we will save it in
509:08 - the top left corner we'll go back we'll
509:11 - delete our file here we'll say okay and
509:14 - we'll have to select a new file we'll
509:17 - click open just double check make sure
509:19 - that's been saved it looks like it's
509:20 - been saved we'll hit upload we'll go
509:21 - back to our overview page it's already
509:24 - running super fast by the way and we'll
509:27 - click into it and we'll see if we get
509:29 - any other failure so there's another
509:30 - failure that's totally okay so we'll
509:33 - just expand it this request is not
509:35 - authorized to perform this operation
509:36 - using the
509:38 - permissions um so it does not like the
509:40 - permissions I gave it totally fine so we
509:44 - will go back to our app
509:46 - designer we will go to this second step
509:49 - here even though it did select this
509:52 - properly so we'll change the
509:56 - connection uh I guess we'll add a new
510:01 - one so we did ad integrated oh let's do
510:04 - managed identity you you must enable
510:06 - managed identities in the logic app to
510:07 - use manage identities authentication you
510:09 - must Grant required access to the
510:12 - identity in the Target room
510:16 - resource
510:17 - okay uh there is an identity tab so we
510:20 - can go over there and take a look there
510:21 - quickly I don't remember it being that
510:23 - hard to do uh system assign manage
510:26 - identities restricted to one person one
510:28 - person resource is tied to life cycle
510:29 - you can grant permissions etc
510:32 - etc
510:34 - um can we just turn that on and hit
510:38 - save well it can be granted access
510:41 - resources protected by a sure let's give
510:43 - it a go
510:52 - all right so um it seems like we have to
510:54 - assign some Ro
510:56 - stuff so we can try and assign a role um
511:01 - can we do the subscription
511:02 - [Music]
511:04 - level
511:07 - contributor
511:09 - okay there's few different ways you can
511:11 - authenticate so hopefully this will be
511:13 - the EAS way to do it uh we'll refresh
511:16 - here did assign
511:32 - it and I don't think it said
511:36 - it all right so give me a moment and let
511:40 - me see what I can figure out
511:42 - okay you know what just to make this
511:44 - easier I think what we should do is just
511:46 - do the access key cuz that seems like
511:48 - the easiest way to do it I was just
511:50 - hoping that we could have you know just
511:52 - did a simple sign in here but it's not a
511:54 - big deal so we'll hit change connection
511:55 - we're going to add a new connection just
511:58 - say storage account um
512:02 - Event Event
512:05 - grid um key and so this is going to want
512:08 - the aure storage account name so this
512:10 - one will be the name of the storage
512:13 - account if we can find it it's called
512:16 - event grid
512:18 - Basics and then we need the Azure
512:20 - storage account access
512:23 - key so there's probably a tab called
512:26 - keys yep and we will show the key and we
512:30 - will copy the
512:34 - key if I don't have to pass along keys I
512:36 - like to not do that please check your
512:38 - account info again uh storage account
512:41 - access key should be a correct base 64
512:43 - encoded string come on give me a break
512:45 - here I am doing what you asked me to
512:49 - do so we will try this
512:57 - again this thing just hates me today
513:00 - give me a second okay you know what it
513:03 - was really short so I really don't trust
513:05 - it so let's just do I just cleared it
513:07 - out there I didn't do anything else what
513:09 - we're going to do is go back here click
513:10 - the copy and then rightclick
513:14 - and paste that's so much longer okay
513:16 - that has to be the right key we'll hit
513:19 - create and we'll give it a
513:23 - moment okay great so that's for that
513:27 - one um but this has to have the right
513:29 - connection as well so what we'll do is
513:32 - just change the
513:33 - connection you have a few here e um and
513:36 - the one we want
513:39 - is the one that's valid so we'll go to
513:42 - this one down below as you can see few
513:44 - attempts here and we'll save it and
513:48 - we'll go
513:49 - back to our overview here I'm just going
513:52 - to close that Tab out we're going to
513:54 - close this out we're going to go back
513:56 - into here we're going to go into our
513:58 - containers we're going to go into our
513:59 - basic we're going to go ahead and delete
514:02 - this we'll say
514:04 - okay and we will upload a new file we
514:07 - will choose the new file we'll choose
514:10 - data upload it we'll go back over here
514:14 - here and I want to see the latest run
514:16 - here we'll give this a
514:20 - refresh is it running it looks like it's
514:24 - running it's hard because this one looks
514:25 - like it just failed and and now the the
514:27 - messaging is getting really muddy
514:30 - here what is it doing so we'll click off
514:32 - here sometimes the portal is a bit funny
514:35 - is it just triggering over and over
514:37 - again did we make an infinite Loop
514:42 - uhoh okay think we have a problem
514:45 - here what if we go here is it basic or
514:48 - it's basic
514:49 - alt this one's
514:52 - basic
514:54 - so what's the
514:56 - problem we'll
515:01 - refresh failed why did it
515:05 - fail
515:07 - conflict another active upload session
515:10 - exists please retry after some time
515:16 - okay uh well let's just go take a look
515:19 - here go back it's here so it's it's
515:23 - here uh so it clearly has worked why
515:26 - it's triggering multiple times I don't
515:28 - know um don't particularly like
515:32 - that there we go I'm just go ahead and
515:34 - delete this one
515:35 - here and it's just it's just going over
515:37 - and over and over again so there's
515:40 - something wrong with my
515:42 - workflow
515:54 - so this looks fine to me that looks fine
515:57 - to
516:00 - me maybe it's triggering oh you know
516:03 - what the the parameters out of here so
516:06 - this is supposed to have a prefix here
516:07 - so what's happening is that it's
516:10 - triggering on anytime a basic one is set
516:13 - up or basic alt one and it's just stuck
516:15 - in an infinite Loop which is really
516:16 - really bad um so we did do this earlier
516:19 - but for whatever reason um the changes
516:22 - still are not here so what we'll need to
516:25 - do is set up that prefix so what we'll
516:27 - do is type in
516:30 - Blob Services
516:33 - default
516:35 - containers basic because we really don't
516:38 - want to trigger it on any but that that
516:40 - uh container there and uh
516:44 - did it save
516:47 - it it doesn't look yeah I mean it should
516:50 - be there so what we'll do is go
516:53 - ahead like why is it not
516:59 - filtering oh um I guess it wants to
517:01 - filter based on
517:06 - name but we gave it its name
517:11 - so I'm not sure what else
517:14 - we would have to type
517:16 - there okay
517:22 - um I'm just going to put um dot it's the
517:26 - prefix filter so data I guess I don't
517:28 - know like it's not letting me save okay
517:31 - there we go we'll save that I just want
517:33 - to stop the infinite Loop there for a
517:38 - moment so we'll go back over to the
517:41 - overview and we'll just make sure we're
517:43 - not uh running up our bill
517:45 - here and I'm just refreshing I just want
517:47 - to see it's not triggering anymore so it
517:49 - stopped triggering which is good and
517:51 - we'll go back over to here and we'll
517:54 - look at this prefix filter because I I
517:56 - don't remember having to do this so a
517:57 - filter like sample etc etc so we'll type
518:01 - it in again I guess blob Services
518:05 - default containers
518:12 - basic
518:15 - it's very oddd because like we typed it
518:17 - oh you know what I probably did I typed
518:19 - in the the filter parameter here we're
518:21 - supposed to add it then put it in there
518:22 - so it's just me getting confused by the
518:23 - UI silly me okay so what we'll do is go
518:27 - back to the overview and this time we
518:30 - just want to see it trigger once so we
518:32 - go back to basic alt we'll go ahead and
518:33 - delete this we'll say okay and we'll go
518:36 - back to our vent grid we go in or our
518:40 - event grid our basic our basic container
518:42 - we're going to go ahead and delete
518:43 - delete uh data here again and we're
518:46 - going to go upload one more
518:48 - time it's actually good that we had that
518:50 - problem because I got to show you uh why
518:53 - filters are so important um when we're
518:56 - dealing with uh the app logic there um
518:59 - or logic apps so we'll go ahead and hit
519:01 - upload we'll go make our way back over
519:03 - here we're going to give this a refresh
519:05 - and now we have a new one and it only
519:07 - happened once and that's what we wanted
519:09 - to happen so we'll go back over to
519:12 - here and we go to basic alt there it is
519:15 - so that's a means to which we can use
519:17 - event grid to integrate stuff you can
519:19 - see logic app is extremely useful for
519:21 - developers uh building uh all sorts of
519:24 - tools uh but we are all done here and
519:27 - what we'll do is make our way over to
519:28 - our Resource Group and we are going to
519:31 - just go ahead and clean up so we'll go
519:32 - into event grid Basics and we'll go
519:35 - ahead and delete this Resource
519:42 - Group
519:43 - there we go and it's going to go ahead
519:45 - and delete there uh yeah and there you
519:52 - [Music]
519:55 - go hey this is Andrew Brown from exam
519:58 - Pro and in this section we'll be
519:59 - covering Azure event Hub Azure event Hub
520:02 - serves as a critical component for event
520:04 - ingestion capable of consuming millions
520:06 - of events from mirriad sources and
520:08 - processing them in real time or via
520:10 - micro batching its potent Auto inflate
520:12 - feature automatic scales throughput
520:14 - units to accommodate varying demands
520:16 - ensuring seamless processing even during
520:18 - traffic spikes notably Azure event Hub
520:20 - offers seamless integration with Apache
520:22 - cafka applications and clients Bridging
520:24 - the capabilities of both
520:26 - platforms Azure event Hub helps you
520:29 - build your big data pipeline to analyze
520:31 - logging anomalies user and device
520:33 - Telemetry where you only pay for what
520:34 - you use breaking down the key concepts
520:36 - of azure event Hub Nam space is an
520:39 - endpoint for receiving and distributing
520:41 - events to event hubs event Hub is where
520:43 - your events will be delivered event Hub
520:45 - cluster is a dedicated event Hub with a
520:48 - 99.99% SLA event Hub capture allows you
520:51 - to automatically capture and save
520:53 - streaming events events hubs for Apache
520:56 - Kafka event Hub and points are
520:57 - compatible with Apache cafka event
521:00 - Publishers or applications or services
521:02 - that publish events to an event Hub
521:04 - publisher policy is a unique IDE used to
521:06 - identify Publishers partitions are used
521:09 - to organize the sequence of events in an
521:11 - event Hub event consumers or
521:13 - applications or services that read
521:15 - events from an event Hub consumer group
521:18 - enable consuming applications to each
521:20 - have a separate view of the event stream
521:22 - stream offset holds the position of an
521:24 - event inside a partition checkpointing
521:26 - is the process of distinguishing between
521:28 - read and unread
521:30 - events diving into scaling with Azure
521:32 - event Hub Auto inflate a dynamic feature
521:35 - that enables automatic scaling up to the
521:37 - Pinnacle of tus predicated on traffic
521:39 - exigencies however it's worth noting
521:42 - that this feature is not accessible
521:43 - within the basic pricing tier the image
521:46 - showcases various configuration options
521:48 - related to Azure event Hub pricing tier
521:51 - the standard tier is selected priced at
521:53 - approximately $22 USD per throughput
521:55 - unit monthly other plans can be browsed
521:58 - for more features throughput units the
522:00 - selected number is one in Azure event
522:02 - Hub throughput units dictate the events
522:04 - process per second and related data
522:06 - volume Auto inflate maximum throughput
522:09 - units the maximum number of throughput
522:11 - units that can be scaled to using the
522:12 - auto inflate feature is set to
522:18 - 12 the next topic will be covering are
522:21 - the pricing tiers for Azure event Hub
522:23 - plans and their key features basic Plan
522:26 - cost 1 cent divided by hour for every
522:28 - throughput unit data input charges 2
522:31 - cents for every million events storage
522:33 - holds up to 84 GB of data data hold time
522:36 - keeps data for only one day does not
522:38 - offer capture Apache Kafka schem of
522:41 - registry or extended retention standard
522:43 - Plan cost 3 cents divided hour per two
522:47 - data input same cost as Basic 2 cents
522:49 - per million events capture available at
522:52 - an additional $73 divided by month for
522:54 - each two storage same as basic 84 GB
522:58 - data hold time keeps data for 7 Days
523:00 - offers Apache Kafka and schem of
523:02 - registry however it lacks extended
523:04 - retention Premium plan cost a bit
523:08 - pricier at
523:09 - $123 divided hour but now it's for every
523:12 - processing unit data input included in
523:14 - the plan no extra cost capture included
523:17 - as well storage a lot more space with 1
523:19 - terab for each poo data whole time
523:22 - retains data for a longer 90 days offers
523:24 - Apache CFA schema registry and also has
523:27 - extended retention at 12 cents GB month
523:29 - with 1 terabyte already included
523:31 - dedicated Plan cost the highest at
523:35 - $684 divided our for each capacity unit
523:38 - data input included in the plan no extra
523:40 - cost capture also included storage huge
523:43 - space with 10 terab for each CU data
523:46 - hold time holds data for 90 days same as
523:49 - premium offers all the features of
523:50 - premium and its extended retention is
523:52 - also at 12 cents GB month but generously
523:55 - includes 10 terabytes terms capacity
523:58 - Unit A measure of capacity for the
524:00 - dedicated plan Processing Unit A measure
524:03 - of capacity for the Premium plan serit
524:05 - Unit A measure of capacity for both
524:07 - basic and standard
524:11 - plans
524:14 - the next thing we'll be going over is
524:15 - the producer in Azure event Hub also
524:17 - known as the publisher the producer is
524:20 - responsible for sending data to the
524:21 - stream Publishers can publish events
524:24 - using the following protocols https the
524:27 - majority of azure sdks prefer https amqp
524:31 - 1.0 Advanced message queuing protocol
524:34 - taffa protocol compatible with the
524:36 - Apache CFA ecosystem generally
524:39 - developers use the Azure SDK for
524:41 - publishing events you can publish an
524:43 - event either one at a time events can be
524:45 - published individually or batches
524:47 - multiple events can be grouped and
524:48 - published together however there's a
524:50 - size limit of 1 Megabyte for both
524:52 - individual and batched events any event
524:54 - or batch exceeding this limit will be
524:56 - rejected for authorization Publishers
524:59 - use either Azure ad with oo2 issued JWT
525:02 - tokens or shared access
525:04 - signature comparison https versus amqp
525:08 - for publishing events amqp requires the
525:11 - establishment of a persistent by
525:12 - directional socket in addition to
525:14 - transport level security or SSL TLS amqp
525:18 - is higher Network costs when
525:19 - initializing the session amqp is higher
525:21 - performance for frequent Publishers and
525:23 - can achieve much lower latencies when
525:25 - used with a synchronous publishing code
525:27 - https requires additional TLS overhead
525:30 - for every request publisher policies
525:34 - event hubs enables granular control over
525:36 - event Publishers through publisher
525:37 - policies publisher policies are runtime
525:40 - features designed to facilitate large
525:42 - numbers of independent event Publishers
525:44 - with Publisher policies each publisher
525:46 - uses its own unique identifier when
525:48 - publishing events to an event Hub
525:50 - ensuring that events are properly
525:51 - segregated authenticated and managed for
525:54 - each individual Source enhancing
525:55 - security and traceability within the
526:01 - system the next topic we'll be covering
526:04 - is a consumer in Azure event Hub the
526:06 - Azure event Hub consumer also commonly
526:08 - referred to as a reader is responsible
526:10 - for receiving and processing data from
526:12 - the the stream connection protocol all
526:15 - event hubs consumers Connect using the
526:16 - amqp 1.0 protocol as events become
526:20 - available they are delivered through
526:21 - this session this eliminates the need
526:23 - for the client to continuously check or
526:25 - pull for the availability of new data a
526:28 - consumer group represents a particular
526:30 - view like a state position or offset of
526:32 - an entire event Hub consumer groups
526:34 - enable multiple consuming applications
526:36 - to each have a separate view of the
526:38 - event stream and to read the stream
526:40 - independently at their own pace and with
526:41 - their own offsets
526:43 - typically in a stream processing
526:44 - architecture each Downstream application
526:46 - equates to a consumer group there's
526:49 - always a default consumer group in an
526:50 - event Hub and you can create up to the
526:52 - maximum number of consumer groups for
526:54 - the corresponding pricing tier there can
526:56 - be at most five concurrent readers on a
526:58 - partition per consumer group however
527:00 - it's recommended that there's only one
527:01 - active receiver on a partition per
527:03 - consumer group some clients offered by
527:06 - the Azure sdks are intelligent consumer
527:08 - agents that automatically manage the
527:09 - details of ensuring that each partition
527:11 - has a single reader and that all
527:12 - partitions for an event Hub are being
527:14 - read from this allows your code to focus
527:16 - on processing the events being read from
527:18 - the event Hub so it can ignore many of
527:20 - the details of the partitions so that's
527:23 - an overview of a consumer in Azure event
527:26 - [Music]
527:29 - Hub next let's talk about offsets in
527:32 - Azure event Hub in the realm of azure
527:34 - event Hub the term offset refers to the
527:36 - position of an event within a specific
527:38 - partition offsets enables a consumer to
527:40 - specify a point in the event stream for
527:42 - from which they want to begin reading
527:43 - events you have the option to specify
527:46 - the offset either as a distinct
527:47 - timestamp or as a numerical offset value
527:50 - consumers are responsible for storing
527:51 - their own offset values outside of the
527:53 - event Hub service every event situated
527:56 - within a partition comes equipped with
527:58 - an
527:58 - offset Azure event Hub checkpointing
528:02 - checkpointing is a process by which
528:03 - readers Mark or commit their position
528:05 - within a partition event sequence
528:07 - checkpointing is the responsibility of
528:09 - the consumer and occurs on a per
528:11 - partition basis within a cons consumer
528:12 - group the consumer is fully responsible
528:15 - for checkpointing this means that for
528:17 - each consumer group every individual
528:19 - partition reader must monitor its
528:21 - ongoing position within the event stream
528:23 - and notify the event Hub service once it
528:25 - recognizes the data stream to be
528:27 - complete or processed so that's an
528:29 - overview of offsets in Azure event
528:35 - Hub the next topic we'll be covering is
528:38 - the event retention for Azure event Hub
528:40 - published events are removed from an
528:42 - vent Hub based on a configurable
528:43 - timebase retention policy the default
528:46 - value in shortest possible retention
528:48 - period is 1 hour for event Hub standard
528:51 - the maximum retention period is 7 days
528:53 - for event hubs premium and dedicated the
528:56 - maximum retention period is 90 days if
528:58 - you change the retention period it
529:00 - applies to all messages including
529:02 - messages that are already in the event
529:03 - Hub it's important to note that
529:05 - individual events cannot be explicitly
529:07 - deleted the reason for event hubs limit
529:10 - on data retention based on time is to
529:11 - prevent large volumes of historic
529:13 - customer data getting trapped in a deep
529:15 - store that is only indexed by a
529:16 - timestamp and only allows for sequential
529:19 - access if you need to Archive events
529:21 - beyond the allowed retention period you
529:23 - can have them automatically stored in
529:24 - Azure storage or Azure data Lake by
529:26 - turning on the event hub's capture
529:28 - feature if you need to search or analyze
529:30 - such deep archives you can easily import
529:32 - them into Azure synaps or other similar
529:34 - stores and analytics platforms so that's
529:37 - an overview of vent retention for Azure
529:39 - event
529:41 - hub
529:45 - let's break down a comparison between
529:47 - event grid event Hub and service bus
529:49 - event grid event Hub and service bus all
529:52 - event driven services for application
529:54 - integration and use an event bus as
529:55 - means to work with event data Azure
529:58 - event grid provides the backbone for
530:00 - event-driven architectures without the
530:02 - need for infrastructure management Azure
530:04 - service to service communication
530:06 - primarily designed for communication
530:08 - between various Azure Services
530:10 - dynamically scalable cost efficient and
530:12 - guarantees at least once delivery of an
530:14 - event Azure event Hub streaming data
530:18 - ideal for ingesting massive amounts of
530:19 - streaming data low latency processes
530:22 - events with minimal delay High
530:24 - throughput capable of receiving and
530:26 - processing millions of events every
530:27 - second and guarantees at least once
530:30 - delivery of an event Azure service bus
530:33 - supports both cues and publish subscribe
530:35 - patterns making it suitable for a range
530:37 - of web applications reliable as
530:40 - synchronous message delivery that
530:41 - requires in advanced messaging features
530:44 - like first in and first out batching
530:46 - sessions transactions dead lettering
530:48 - temporal control routing and filtering
530:50 - and duplicate detection guarantees at
530:53 - least once delivery of a message and
530:54 - offers an optional feature to ensure
530:56 - messages are delivered in sequence the
530:59 - breakdown highlights the differences
531:00 - between Azure event grid event Hub and
531:02 - service bus showcasing their unique
531:04 - strengths tailored to different
531:07 - [Music]
531:11 - scenarios hey this is angrew Brown from
531:13 - exam Pro and this fall along we are
531:15 - going to learn all about Azure event Hub
531:18 - uh so what I want you to do is go to the
531:19 - top here and type in event Hub and we
531:21 - are going to create ourselves a new
531:22 - event Hub uh namespace so we'll go here
531:25 - and hit create and we'll create a new
531:27 - Resource Group as we always do and call
531:29 - it my event Hub and then for the
531:31 - namespace name we'll say um my event Hub
531:35 - if it doesn't let you do that you'll
531:37 - have to put some numbers on the end I'm
531:39 - just putting some numbers here because
531:40 - often these are taken
531:42 - doesn't matter what location is just
531:44 - choose whichever one we're going go with
531:46 - basic because there's not a huge
531:47 - difference between the pricing terms in
531:48 - terms of feature sets that we want to
531:50 - use today so we'll go ahead and create
531:53 - this namespace so we'll give it a moment
531:55 - and we'll go ahead and hit
531:57 - create and then we'll just wait for this
531:59 - namespace to provision all right so
532:01 - after waiting about a couple minutes
532:03 - there our namespace is deployed we're
532:05 - going to go ahead and create our CES and
532:06 - event Hub I'm going to call this one My
532:09 - Hub we'll go ahead and hit review and
532:11 - create
532:13 - and we'll create and these create very
532:16 - very quickly so we're not going to have
532:17 - to wait too long we need to set some
532:18 - shared access policies here so I'm going
532:21 - to go into the Hub here we going to hit
532:23 - manage I call this my SAS SAS Shar
532:27 - access policy and there we will now have
532:30 - the ability to have a primary key and
532:32 - connection key so we can actually
532:34 - connect uh to it so what we're going to
532:36 - do is go to GitHub and I want to create
532:38 - a new
532:40 - Repository and we'll go ahead
532:44 - here and we'll go down below go to exam
532:46 - Pro we'll say my event Hub that's
532:50 - already taken because I've done this
532:51 - before so for M I'm going to call it new
532:54 - and we'll go down to private we're going
532:55 - to use nodejs so we'll just type in
532:56 - nodejs so that it ignores the node
532:59 - modules you're going to want to have git
533:01 - pod or a visual studio code installed on
533:03 - your computer the easiest way is
533:05 - honestly get pod because these
533:06 - environments are temporary and it's free
533:08 - to uh utilize it so if you can go get
533:11 - the Chrome extension or uh if you don't
533:13 - want to install the Chrome extension all
533:14 - you got to do is attach this to the end
533:16 - of the repo to launch a GitHub or git
533:19 - pod environment so I'll give that a
533:20 - moment to
533:21 - launch and there we go so I do have some
533:25 - code uh for
533:27 - this so I'm just looking for it off
533:29 - screen
533:32 - here and uh we're going to need to have
533:35 - a couple files here we're going to need
533:36 - a new file called send JS and we're
533:39 - going to need a new file here called
533:41 - receive Js
533:42 - I'm not typing the full word receive
533:44 - because I'm always really bad at
533:45 - spelling it so I'm just trying to save
533:46 - myself some
533:48 - trouble and uh we're also going to have
533:51 - to initialize a
533:53 - new package Json file so there we go and
533:57 - we're going to have to get um a couple
533:59 - things installed yeah so we'll need mpm
534:03 - install Azure Event Event
534:07 - hubs Azure storage blob Azure event
534:14 - hubs
534:17 - checkpoint store
534:19 - blob save Dev and it make our lives a
534:22 - bit easier seems like I typed something
534:24 - wrong here I'll just hit up I forgot the
534:26 - Ford slash
534:28 - here and so we'll just go event
534:31 - Hub um Azure
534:36 - JavaScript because I believe
534:39 - yeah I kind of use this one but I
534:42 - modified it to make it a little bit
534:43 - easier I think this is the one I was
534:45 - doing no I don't think so
534:50 - tutorial might be this
534:53 - one yeah it looks like this um so this
534:56 - is just the JavaScript there so for send
534:58 - we will grab this code here we're not
535:01 - going to do exactly the way they do it
535:02 - but pretty close and then there is a
535:05 - receive code so we'll go down below and
535:07 - we'll grab this as you can see there's a
535:08 - lot going on
535:10 - here
535:14 - that will be our receive couple things
535:15 - we need to properly set so these will be
535:17 - all environment variables so what we'll
535:19 - do is go to the top here and we'll just
535:20 - do constant process requires process
535:24 - that's going to allow us to import our
535:26 - environment
535:29 - variables so we just
535:31 - say copy let's just save that we'll
535:35 - paste that in there as well and then
535:37 - this is where we need to replace all
535:39 - environment variables as you can see I
535:41 - always have things pcking up on me here
535:42 - so just close my teams
535:49 - out and just Lin to make this a little
535:52 - bit
535:53 - faster so default will be what we'll
535:55 - keep here um this will be our storage
535:58 - connection
536:00 - string say process
536:03 - EnV uh
536:06 - storage connection
536:08 - string then this one will be process
536:12 - EnV container
536:14 - name then we need process EnV event Hub
536:20 - name and up here we'll have process
536:24 - EnV um event Hub connection string and
536:29 - then we'll go over to our send here
536:30 - we'll do something very similar so
536:33 - process
536:34 - EnV event Hub connection string and then
536:39 - we'll have process EnV event
536:43 - Hub
536:44 - name so we need to set all of these I'm
536:47 - just going to copy this for a moment
536:48 - here I'm going to make a new
536:50 - file file new file sure it doesn't
536:54 - really matter we're just using this as a
536:55 - quick scratch pad and so uh what I want
536:58 - to do here is just delete out this
537:02 - part and we're going to do export on the
537:05 - end here we'll take out this one
537:07 - here and the idea is that we'll just set
537:09 - them all here and then we will
537:12 - make our lives a little bit easier when
537:14 - we have to mass set Mass set these so we
537:17 - do have the connection string because we
537:19 - saw it over here so we we'll grab the
537:22 - primary one doesn't matter which one
537:24 - primary secondary and that is for
537:28 - the uh vent Hub we called
537:32 - [Music]
537:33 - the the vent Hub was called My Hub I
537:37 - believe we'll just double check what the
537:40 - Hub was called yeah it's called My Hub
537:42 - up there we'll need a storage account so
537:45 - what I'm going to do wish this thing we
537:47 - get out of here get out of here I'm not
537:50 - I'm not trying to save a freaking file
537:52 - there we go hit escape a bunch of times
537:54 - and we'll go back here and we'll create
537:56 - ourselves a new storage account I'm
537:57 - actually make this in a new tab so we
537:59 - can see what we're
538:01 - doing so we go over to storage
538:06 - accounts and we'll create ourselveses a
538:08 - new storage account we'll create this uh
538:10 - storage account in the same name space
538:12 - so we'll go down to my event Hub we'll
538:15 - just say my um or we'll just say my
538:19 - event Hub 8888 again you might have to
538:22 - change it based on your standards but um
538:24 - or like what is available to you we'll
538:27 - go ahead and go review
538:31 - create and for the container name uh
538:34 - we'll probably just call
538:35 - container maybe container
538:39 - one we just have to wait for this to
538:40 - create to grab that connection
538:43 - string so this usually doesn't take too
538:45 - long just a couple
538:50 - seconds okay so there we are um we'll go
538:53 - over to access keys and we'll grab the
538:56 - connection string from here I believe
538:58 - this one should work U let me just
539:00 - double
539:03 - check yeah I think this will work so
539:05 - what we'll do is go back over here if it
539:07 - doesn't we'll find out pretty soon and
539:08 - we'll just generate out a a shared
539:10 - policy uh we'll go ahead and paste that
539:12 - on in
539:13 - here and just double checking that this
539:16 - is the correct one this looks identical
539:18 - to that one that can't be correct so
539:20 - we'll go back here this is the storage
539:22 - account so we'll go oh I have to
539:23 - actually hit the copy button that's what
539:25 - it didn't do we'll go ahead and paste
539:27 - that in and so theoretically this should
539:29 - work so we'll go ahead and copy these
539:32 - we'll drag our terminal up a bit we'll
539:33 - paste these in
539:36 - here and uh what I'll do is just double
539:39 - check that they're here so we'll say EnV
539:41 - GP event
539:44 - Hub so those are both set and then we'll
539:47 - do
539:48 - storage that one is set and we'll do
539:51 - container that one is set so these are
539:53 - all in good shape for our storage
539:55 - account we still have to create the
539:57 - container so go here and create a new
539:59 - container say container one we'll go
540:02 - ahead and create that we'll make our way
540:04 - back over here and then instead of just
540:06 - having export we'll do GP EnV this is
540:09 - just in case we have to restart the
540:10 - environment for for any reason so that
540:12 - these environment variables get exported
540:14 - twice so we paste that in there I
540:16 - believe those are all set go ahead and
540:17 - hit enter on the last one there and
540:20 - let's see if our code works so we'll do
540:22 - Ascend um actually we have to set up two
540:27 - scripts here so that we can actually
540:29 - call them so I one call here is called
540:31 - send this will be node send
540:35 - JS and we'll have receive so re just
540:38 - because I always spell receive wrong and
540:39 - I just don't want to have to type it a
540:40 - thousand times times we'll have that
540:42 - there so now what we'll do is do mpm run
540:45 - send see if that
540:49 - works it says a batch of three events
540:51 - have been sent we'll go and confirm that
540:53 - over in event Hub if it worked if we go
540:54 - to the overview it should show us some
540:56 - messages were
540:58 - received sometimes there's a bit of
541:00 - delay so we'll just give it a teeny tiny
541:03 - amount of time we'll hit refresh
541:04 - here because we know we sent them but
541:07 - while we're waiting for those to kind of
541:08 - propagate what we'll go back here and
541:09 - just kind of look at the code because we
541:11 - did didn't really look at it so the way
541:13 - it works is you are defining a client
541:16 - and it will be the producer client um
541:19 - and then down below so we say Okay a
541:21 - producer is someone that produces events
541:23 - it's very common in the messaging system
541:25 - to have a producer and consumer we're
541:26 - going to create a batch job and we're
541:28 - going to add them all to the batch job
541:30 - then we're going to send them all at
541:31 - once and it's going to close and it'll
541:34 - and it will complete and if there are
541:35 - any errors it will alert us about it so
541:38 - we'll go back over here and we'll do a
541:40 - refresh so I want to see messages
541:44 - messages would normally show up here so
541:47 - since I don't trust it I'm just going to
541:48 - run it again I mean clearly worked
541:50 - because there was no
541:54 - errors and we'll go back over here and
541:57 - um not here but we'll go back here we'll
542:02 - refresh and I'm just waiting to
542:05 - see something
542:10 - here
542:15 - processing data this is like something
542:17 - that's really powerful with
542:19 - um vent Hub here
542:22 - so still don't see the messages just
542:26 - give me a second to debug this I've done
542:27 - this lab like four times so it should
542:30 - work but uh you know sometimes sometimes
542:32 - it's trouble so just a
542:34 - moment all right so I literally did
542:36 - nothing and now it's actually showing up
542:38 - in the uh messaging queue so you know or
542:41 - The Hub the funnel uh so that's just
542:43 - something you have to consider is that
542:44 - sometimes you just have to be a little
542:45 - bit patient let's see if we can go
542:46 - receive those messages now by running
542:48 - the other uh script so what I'm going to
542:50 - do here is do mpm run
542:53 - receive and it should receive the
542:56 - messages as long as something isn't
542:57 - typed incorrectly so we'll go back over
542:59 - here we've seem to introduce a little
543:01 - mistake so I'll go ahead and save that
543:03 - we'll hit
543:04 - up and it should receive the events so
543:06 - it should print the three out so there
543:09 - we go and so we are streaming that
543:11 - through the consumer is technically um
543:13 - Storage storage accounts but if we go to
543:15 - the storage account there's nothing
543:16 - really uh um intelligible in terms of
543:20 - what's in here so like there are stuff
543:22 - in here
543:23 - checkpoints I guess it's saved to
543:25 - checkpoint I personally don't know what
543:27 - I'm looking at so I'm not exactly sure
543:29 - what the point of doing that I guess
543:31 - it's just saying the checkpoint is like
543:32 - the last point it it wrote but if we
543:34 - just take a look at the code here
543:36 - quickly you can see this called consumer
543:38 - so we hit consumer client there's a blob
543:40 - check Point store then we have the
543:43 - consumer client we are subscribing so
543:45 - it's saying hey are there any events
543:46 - let's consume them if there are no
543:48 - events throw a console log so tell us
543:51 - about it iterate through them then
543:53 - update the checkpoint so move it to the
543:54 - next Point um just say hey this is where
543:57 - it is now and that's pretty much all it
543:59 - wanted you to do we can go ahead and
544:01 - save this code we so we'll say um event
544:04 - Hub code doesn't matter what you name it
544:06 - there we'll sync the changes and we'll
544:08 - go ahead and clean up so we'll go back
544:10 - to our resour resource groups we'll go
544:12 - to vent Hub and we will then go and
544:16 - delete this Resource Group and we'll go
544:19 - here
544:20 - and there you
544:23 - [Music]
544:26 - go hey this is Andrew Brown from exam
544:29 - Pro and in this section we'll be
544:30 - covering the Azure Cloud adoption
544:32 - framework Cloud adoption framework is a
544:35 - white paper that is a step-by-step
544:36 - process to help organizations plan and
544:38 - migrate their workloads to Azure the
544:41 - image outlines the Microsoft cloud
544:42 - adoption framework for Azure a
544:44 - systematic approach to transitioning to
544:46 - the Azure Cloud the process is
544:48 - categorized into stages defined strategy
544:51 - here the focus is on understanding
544:53 - motivations grasping the reasons for
544:55 - cloud adoption business outcomes
544:57 - identifying the desired results business
545:00 - justification validating the moves
545:02 - reasons first adoption project
545:04 - kickstarting the cloud Journey plan this
545:07 - stage includes rationalizing digital
545:09 - estate evaluating current digital assets
545:12 - initial organization alignment ensuring
545:14 - everyone is aligned with the migration
545:16 - goals skills Readiness Plan equipping
545:18 - teams with necessary Cloud skills Cloud
545:21 - adoption plan laying out a road map for
545:23 - the cloud transition ready this phase
545:25 - ensures preparedness Azure Readiness
545:27 - guide preparing the environment for
545:29 - Azure First Landing Zone setting up an
545:31 - initial secure Azure environment
545:34 - expanding the blueprint broadening the
545:35 - Azure setup as per requirements best
545:38 - practice validation ensuring adherence
545:40 - to azure best practices adopt the
545:43 - actionable phase where migrate existing
545:45 - workloads or moved to Azure this entails
545:47 - the first workload migration
545:49 - understanding expanded scenarios
545:51 - validating best practices and making
545:53 - process improvements innovate transform
545:56 - services in the Azure environment using
545:58 - Innovation guides exploring new
546:00 - scenarios validating best practices and
546:02 - furthering process improvements govern
546:05 - this is about oversight and management
546:07 - establish a methodology and Benchmark
546:09 - for governance Implement initial best
546:12 - practices standards for Azure use
546:14 - measure governance maturity how well
546:16 - governance rules are followed manage the
546:19 - deals with ongoing operations ensure
546:21 - business commitments are met during the
546:23 - transition set and assess the operations
546:25 - Baseline determine operations maturity
546:28 - gauge the efficiency of cloud
546:31 - operations now let's take a look at the
546:33 - security roles and responsibilities of
546:35 - the Azure Cloud adoption framework
546:38 - Business and Technology outcomes go and
546:41 - results expected from security functions
546:43 - security outcomes results an
546:45 - organization aims for including
546:47 - governance prevention and response role
546:50 - types security leadership provides
546:52 - security Direction and strategy security
546:55 - architect designs and implements
546:57 - security blueprints platform app
546:59 - security Engineers ensures security of
547:01 - platforms and applications security
547:04 - operations manages real-time security
547:06 - threats
547:08 - responsibilities security leadership
547:10 - sets security strategy security
547:12 - architecture design secure systems
547:15 - security compliance ensures adherence to
547:18 - regulations policy and standards set
547:20 - security policies posture management
547:23 - manages overall security stance phases
547:26 - of security implementation plan
547:29 - identifies security needs build
547:31 - Implement strategies including access
547:33 - control and asset protection run manages
547:36 - ongoing operations including prevention
547:38 - and response feedback loop
547:41 - continuous Improvement cycle and
547:42 - security operations in summary the
547:45 - framework offers a structured way to
547:47 - transition to Azure ensuring strategy
547:49 - alignment preparation adoption
547:51 - governance and effective
547:56 - management let's talk about the Azure
547:59 - well architected framework Azure well
548:01 - architected framework provides best
548:03 - practices for Designing and implementing
548:05 - Solutions on Azure it is structured
548:07 - around five key pillars cost
548:09 - optimization this pillar focuses on
548:11 - maximizing the value delivered by
548:13 - managing and controlling costs proper
548:15 - cost management can lead to significant
548:17 - savings without compromising function or
548:20 - performance operational excellence this
548:22 - emphasizes implementing and maintaining
548:24 - system processes to ensure smooth and
548:26 - efficient operations in a production
548:28 - environment it involves routine
548:29 - operations deployment practices
548:31 - monitoring and iterative Improvement
548:34 - performance efficiency refers to
548:36 - ensuring that systems can scale
548:37 - appropriately and adapt to varying loads
548:39 - both expect and unexpected it's not just
548:42 - about speed but ensuring resources are
548:44 - used efficiently to meet performance
548:46 - requirements reliability concerns the
548:48 - system's ability to recover from
548:50 - interruptions such as failures or
548:52 - outages and continue to operate without
548:54 - significant degradation this pillar
548:55 - stresses designing for high availability
548:57 - and disaster recovery security Central
549:00 - to the Azure well architected framework
549:02 - this pillar emphasizes the protection of
549:04 - data and applications from potential
549:06 - threats it covers a broad spectrum from
549:08 - access controls and encryption to threat
549:10 - detection and response strategies so
549:13 - that's an overview of the Azure well
549:15 - architected
549:20 - framework the next topic we'll be
549:22 - covering is azure migrate Azure migrate
549:24 - offers a streamlined service for
549:26 - migration modernization and optimization
549:28 - on Azure it simplifies the pre-migration
549:31 - processes like discovering assessing and
549:33 - appropriately sizing on premises
549:35 - resources for infrastructure data and
549:37 - applications with an extensible
549:39 - framework as migrate easily integrates
549:41 - with thirdparty tools broadening its
549:43 - range of supported scenarios here's what
549:45 - it offers unified migration platform a
549:48 - centralized portal to initiate execute
549:51 - and monitor your Azure migration Journey
549:54 - diverse tool set Azure migrate provides
549:56 - a suite of tools for both assessment and
549:57 - migration it features tools such as
549:59 - Azure migrate Discovery and assessment
550:01 - in migration and modernization
550:03 - furthermore it seamlessly integrates
550:05 - with other Azure Services tools and
550:07 - third-party offerings from independent
550:09 - software vendors
550:11 - comprehensive migration and
550:12 - modernization capabilities in the Azure
550:15 - migrate Hub you can assess migrate and
550:17 - modernize servers databases and web apps
550:20 - assess and migrate on premises servers
550:22 - web apps and SQL Server instances to
550:25 - Azure databases analyze on premises SQL
550:28 - Server instances and databases and
550:30 - migrate them to Azure SQL on a VM Azure
550:33 - SQL managed instance or Azure SQL
550:36 - database web applications evaluate on
550:38 - premises web applications and transition
550:40 - them to the azzure app service or Azure
550:42 - kubernets service virtual desktops
550:45 - review your onsite virtual desktop
550:47 - infrastructure and move it to Azure
550:48 - virtual desktop data transfer
550:51 - efficiently and affordably transfer vast
550:53 - data volumes to Azure using Azure data
550:55 - box products by using Azure migrate
550:58 - organizations can streamline and
551:00 - simplify their migration process reduce
551:02 - downtime and improve the overall
551:04 - efficiency and cost effectiveness of
551:05 - their Cloud
551:09 - migration
551:11 - the next thing we'll be covering are the
551:13 - integrated Tools in Azure migrate the
551:15 - Azure migrate Hub includes these tools
551:18 - Azure migrate Discovery and assessment
551:20 - discover and assess servers including
551:22 - SQL and web apps discover and assess on
551:25 - premises servers running on VMware
551:27 - hyperv and physical servers in
551:29 - preparation for migration to Azure
551:31 - migration and modernization migrate
551:33 - servers migrate VMware VMS hyperv VMS
551:37 - physical servers other virtualized
551:39 - servers and public cloud VMS to Azure
551:41 - data migration assistant assess SQL
551:44 - Server databases for migration to Azure
551:46 - SQL database Azure SQL managed instance
551:49 - or Azure vm's running SQL Server data
551:52 - migration assistant assesses SQL servers
551:54 - identifies potential migration problems
551:56 - unsupported features and suggest the
551:58 - best path for database migration Azure
552:01 - database migration service migrate on
552:03 - premises databases to Azure vm's running
552:06 - SQL Server Azure SQL database or SQL
552:09 - managed instances is azure database
552:11 - migration service is a managed service
552:13 - for seamless migrations to Azure data
552:15 - platforms with minimal downtime maneuver
552:18 - assess servers mover is a SAS platform
552:20 - that enhances business intelligence by
552:22 - accurately depicting it environments
552:24 - within a day web app migration assistant
552:27 - assess on premises web apps and migrate
552:29 - them to Azure Azure app service
552:31 - migration assistant is a standalone tool
552:33 - to assess on premises websites for
552:35 - migration to Azure app service Azure
552:37 - data box migrate offline data use Azure
552:40 - datab box products to move large amounts
552:42 - of offline data to Azure so that's an
552:45 - overview of the integrated Tools in
552:46 - Azure
552:51 - migrate the next type of migration
552:53 - solution we'll be going over is azure
552:55 - database migration service Azure
552:57 - database migration service is a fully
552:59 - managed service that enables seamless
553:01 - migrations from various database sources
553:03 - to Azure data platforms with minimal
553:05 - downtime it simplifies the process of
553:07 - moving databases to the cloud and
553:09 - reduces the risk associated with
553:11 - migration the service supports various
553:13 - Source database engines such as SQL
553:16 - Server MySQL Oracle and postgressql and
553:19 - targets Azure database platforms such as
553:21 - Azure SQL database Azure SQL managed
553:24 - instance and Azure database for posters
553:26 - ql and
553:27 - nql Azure database migration service
553:30 - currently offers two versions database
553:33 - migration service via Azure SQL
553:35 - migration extension for Azure data
553:37 - Studio Azure portal Powershell and Azure
553:39 - CLI
553:40 - database migration service via Azure
553:42 - portal Powershell and Azure
553:44 - CLI use cases migrations of on premises
553:48 - databases to Azure SQL Services database
553:52 - consolidation and migration to a single
553:54 - platform in Azure Cloud bursting and
553:56 - Disaster Recovery Solutions features
553:59 - support for a wide range of source and
554:01 - Target database platforms automated
554:03 - schema and data migration with minimal
554:05 - downtime migration assessment to
554:07 - identify potential issues and guidance
554:09 - for successful migration real-time
554:12 - migration monitoring and error
554:14 - notifications pre and post migration
554:16 - validation and cleanup tools integration
554:19 - with Azure services such as Azure
554:20 - security Center and Azure monitor for
554:22 - enhanced security and monitoring
554:25 - capabilities how it works set up the
554:27 - migration project create a new migration
554:30 - project in Azure DMS Define the source
554:32 - and Target environments and specify the
554:34 - database objects to migrate configure
554:36 - the source and Target configure the
554:38 - network connectivity and security
554:40 - settings for the source and Target
554:41 - environments and install the Azure DMS
554:43 - extension on the source database server
554:46 - start the migration start the migration
554:48 - and monitor the progress in the Azure
554:49 - DMS portal the service automatically
554:52 - replicates The Source database to the
554:53 - Target environment and tracks the
554:55 - changes that occur during the migration
554:58 - perform ceed over and post migration
554:59 - tasks after replication initiate the
555:02 - ceed over to transition the application
555:04 - to the Target environment Azure DMS
555:06 - offers validation and cleanup tools post
555:09 - migration
555:10 - [Music]
555:13 - the next migration tool we'll be
555:15 - covering is storage migration service
555:17 - the storage migration service simplifies
555:19 - the process of migrating storage to
555:21 - Windows server or Azure it comes with a
555:23 - userfriendly graphical tool that takes
555:25 - stock of data on Windows litex and net
555:28 - appy ifs servers and facilitates its
555:30 - transfer to newer servers or Azure
555:32 - virtual machines the service also
555:34 - enables the migration of a server's
555:35 - identity to the destination server
555:37 - ensuring that applications and users can
555:39 - access access their data without any
555:41 - changes to Links or paths why use
555:43 - storage migration service use storage
555:46 - migration service because you've got a
555:47 - server or multiple servers that you want
555:49 - to migrate to newer Hardware or virtual
555:51 - machines storage migration service is
555:53 - designed to help by doing the following
555:55 - tasks inventory multiple servers and
555:57 - their data rapidly transfer files file
556:00 - shares and security configuration from
556:02 - The Source servers optionally take over
556:05 - the identity of the source servers also
556:07 - known as cutting over so that users and
556:09 - apps don't have to change CH anything to
556:10 - access existing data manage one or
556:12 - multiple migrations from the windows
556:14 - admin center user interface migration is
556:17 - a three-step process wi inventory
556:20 - servers to gather info about their files
556:22 - and configuration shown in the following
556:24 - figure to transfer data from The Source
556:26 - servers to the destination servers three
556:29 - cut over to the new servers the
556:31 - destination servers assume the source
556:33 - servers former identities so that apps
556:35 - and users don't have to change anything
556:37 - although the source servers retain their
556:39 - files they enter a maintenance mode and
556:40 - become inaccessible at a suitable time
556:43 - these servers can be decommissioned so
556:45 - that's an overview of storage migration
556:51 - service the next migration solution
556:54 - we'll be covering is azure data box the
556:56 - Microsoft Azure data box Cloud solution
556:59 - lets you send terabytes of data into and
557:00 - out of azure in a quick inexpensive and
557:03 - reliable way each storage device has a
557:05 - maximum usable storage capacity of 80
557:07 - terab and is transported to your data
557:09 - Center through a regional carrier it is
557:12 - designed to help customers with slow or
557:13 - limited internet connectivity to move
557:15 - large volumes of data to the
557:17 - cloud let's take a look at some of azure
557:19 - data Box's use cases data box is used to
557:23 - import data to Azure for onetime
557:25 - migrations moving large on premises data
557:27 - transitioning offline tapes relocating
557:30 - BMS SQL servers applications and
557:32 - transferring historical data for Azure
557:34 - based analysis initial bulk transfers
557:37 - large scale transfers using data box
557:39 - followed by incremental Network
557:41 - transfers for example moving vast
557:43 - backups with Partners like convolt
557:45 - periodic uploads transferring large
557:47 - volumes of data generated periodically
557:49 - like video content from oil rigs or
557:51 - windmill farms for exporting from Azure
557:54 - data boxes used for Disaster Recovery
557:57 - restoring Azure data on premises quickly
558:00 - security requirements meeting mandates
558:02 - that require data extraction from meure
558:04 - Storage tiers Like Us Secret migration
558:07 - moving data back to on premises or to a
558:09 - different cloud provider here's how
558:11 - Azure data box works customers order a
558:14 - data box from the Azure portal when the
558:16 - data box arrives customers connect it to
558:18 - their Network and configure it using the
558:20 - Azure portal customers copy data to the
558:23 - data box using standard file transfer
558:25 - protocols such as SMB or NFS once the
558:28 - data transfer is complete customers ship
558:30 - the data box back to Azure Azure copies
558:33 - the data from the data box to the
558:35 - customer's Azure storage account so
558:37 - that's an overview of azure data box
558:39 - it's cases and
558:42 - [Music]
558:45 - workflow let's talk about networking for
558:47 - Azure and so everything kind of revolves
558:49 - around the virtual Network also known as
558:51 - the v-net and this is a logically
558:52 - isolated section of your Azure Network
558:54 - where you launch your Azure resources
558:56 - and here's a very simple uh diagram of
559:00 - using v-net but there's a lot of
559:02 - networking components uh that you're
559:04 - going to be utilizing and we're not
559:05 - going to go through the exhaustive list
559:07 - here but let's just go through some of
559:08 - them just to give you an idea of like
559:11 - all the things you can do within Azure
559:13 - networking so you have Azure DNS this
559:15 - manages your uh your DNS domain then you
559:18 - have the v-net itself and so underneath
559:20 - that it'll have like address spaces
559:21 - route table subnets then you have
559:23 - network security groups this acts as a
559:25 - virtual firewall at the subnet or Nick
559:27 - level you have express route this helps
559:28 - you create a very fast connection
559:30 - between your on-prem to your v-net then
559:34 - you have virtual Wan this is a
559:35 - centralized Network to rote different
559:37 - network connections then you have
559:39 - virtual Network Gateway this is a sight
559:42 - tosite vpm connection between v-ets and
559:44 - local networks then you have your NYX or
559:47 - your network interfaces and these are
559:48 - virtual network devices to allow VMS to
559:50 - communicate using IP protocols and then
559:53 - you have like all your load balancers
559:55 - and other things like that so again not
559:58 - an exhaustive list uh but just shows you
560:00 - that there's a lot you can do uh within
560:02 - networking on
560:03 - [Music]
560:06 - Azure so as your private links allow you
560:09 - to establish secure connections between
560:11 - Azure resources so traffic remains
560:13 - within your Azure Network and so I got
560:16 - this big old graphic here and so let's
560:18 - just uh Define a few things I'm going to
560:20 - pull out my pen here and so imagine you
560:22 - have workloads on your on Prem or you
560:24 - have your own virtual Network on Azure
560:27 - uh and so you have some VMS doing some
560:29 - stuff and the idea is that you want to
560:31 - connect them to some other services over
560:33 - here uh but the thing is is that if you
560:36 - wanted to connect them they might try to
560:38 - transverse the internet so if you're on
560:40 - Prem it's going out here and it's going
560:42 - over here and so the issue with this is
560:45 - that um it's not necessarily secure uh
560:49 - another issue could be um you know it's
560:52 - just faster if it would to stay within
560:53 - the actual Network because if you're
560:56 - already connected you you know your
560:58 - stuff over here then why wouldn't it
560:59 - just go through uh the actual Network
561:02 - another thing is like data transfer cost
561:03 - so it's always going out the internet
561:05 - and coming back in and that's not a
561:07 - great scenario uh so I'm just going to
561:09 - erase all that stuff there uh the idea
561:12 - with private link is that it's just
561:13 - keeping everything within the Azure
561:15 - Network um and so that's what it's doing
561:18 - so if you want to uh you know um connect
561:22 - your workloads in your v-net or your on
561:25 - Prem that connects to a specific v-net
561:27 - the idea is that you can launch a
561:29 - private link endpoint which is a network
561:30 - interface and this is what's
561:31 - establishing that connection and you're
561:33 - going to have to give that private
561:34 - endpoint a private IP address from your
561:36 - v-net and so then the idea is that on
561:38 - the right hand side that's what you want
561:40 - to connect to there's a lot of azure
561:41 - Services by default works with private
561:43 - link so there's no additional
561:44 - configuration you'll just be able to uh
561:46 - say use private link and they will uh
561:49 - same thing with some thirdparty
561:50 - providers um in the marketplace uh they
561:52 - might be powered by private link and
561:55 - then there's a third case where let's
561:56 - say you have like a private subnet and
561:58 - you have some VMS there uh so what you
562:00 - could do is um if you have a load
562:03 - Bouncer and you need to have an internal
562:05 - load balcer there you can uh launch the
562:07 - private link service and this is going
562:09 - to basically make your uh your workload
562:12 - in that v-net become compatible or
562:15 - powered by private link so there you
562:21 - go let's take a look at subnets and a
562:24 - subnet is a logical division of an
562:26 - address space so we just looked at
562:27 - address spaces so we're cutting it up
562:29 - one step further and subnets help you
562:31 - define different kinds of workloads and
562:33 - allows you to apply virtual is isolation
562:36 - within your network so when you launch
562:37 - an aure resource you choose the subnet
562:40 - you want to launch with within and an IP
562:42 - from that subnet is assigned to your
562:44 - resource so uh the thing is is that uh
562:48 - when you create route tables that's how
562:49 - you are associating uh the subnet so it
562:52 - can access the internet or access
562:53 - anything uh and then there's public and
562:56 - private subnets but this thing is
562:57 - interesting because like again if you if
562:59 - you're using AWS uh those are very
563:02 - clearly defined things but in Azure uh
563:05 - they don't have this concept of public
563:07 - and private you have to Define it
563:08 - yourself so really a public or private
563:11 - subnet is just a subnet that doesn't
563:13 - have access to the internet and so we
563:14 - saw that when we had the route table and
563:16 - we over over uh Road um the access to
563:20 - the internet to none that M essentially
563:23 - made it a private subnet uh another
563:26 - thing you can do with subnets is you can
563:27 - associate a network security group and
563:29 - this is going to help protect uh protect
563:30 - your traffic entering and leaving the
563:32 - subnet so you're making rules based on
563:34 - the IP address port and protocol uh and
563:37 - then there's a special thing called a
563:38 - Gateway subnet so uh it's a specialized
563:41 - uh type of subnet and it's just for
563:43 - Azure virtual Network gateways and so um
563:46 - it's interesting you'll go in and
563:48 - there'll be a separate section just for
563:50 - it and so you'll create it it's just for
563:52 - that
563:53 - [Music]
563:57 - service so uh virtual private networks
563:59 - also know as vpns what they do is they
564:01 - extend a private Network across a public
564:04 - network and enables your users to send
564:06 - and receive data across shared or public
564:09 - networks as if their Computing devices
564:11 - were directly connected to the private
564:13 - Network all right uh and so now we have
564:16 - to talk about what is a virtual Network
564:19 - Gateway and so virtual Network Gateway
564:21 - is the software a VPN device uh for your
564:24 - Azure virtual Network so that's how
564:26 - these uh devices are going to connect to
564:29 - your network so when you deploy a
564:30 - virtual Network Gateway it will deploy
564:32 - to or uh two or more specialized uh VMS
564:36 - in specific subnets you need to create a
564:39 - Gateway subnet and these deployed VMS
564:42 - contain routing tables and run specific
564:44 - Gateway Services you can uh choose the
564:46 - type of Gateway you want it to be and
564:48 - this is going to determine whether You'
564:49 - use something like VPN Gateway or
564:51 - express route Gateway so it's as simple
564:53 - as an option like that but to really
564:55 - understand uh these virtual Network
564:57 - gateways we should just look at some VP
564:58 - and Gateway design so you understand why
565:00 - you're creating these
565:05 - things so to understand VPN Gateway we
565:07 - need to understand the utility of the
565:10 - service and so we'll look at some
565:11 - different topologies and the first one
565:13 - is site to site this is when you connect
565:15 - Azure to on- premise data center or vice
565:18 - versa and the idea is you create a VPN
565:20 - Gateway that's going to establish
565:22 - connection to your on- premise
565:23 - environment creating IPC tunnel which is
565:26 - a secure connection IP uh connection
565:28 - there but you might be asking well isn't
565:30 - that what Express rout is for well
565:31 - Express rout goes through a part um a
565:34 - edge partner uh so there's a lot more
565:36 - work involved in that setup this one
565:37 - just goes over the internet so it's not
565:39 - going to be as fast um and there's some
565:41 - other uh downsides but the idea is it's
565:43 - the easiest way to get connected from
565:45 - your uh your on premise to uh Azure the
565:48 - next one is multi sight this is when you
565:49 - connect Azure to multiple on premise
565:51 - data centers um so it's just like the
565:53 - same model except there's more than one
565:56 - uh tunnel here to more than one uh uh
565:58 - site then you have point to site this is
566:01 - when you connect Azure to multiple
566:03 - individual computers so imagine you have
566:04 - a bunch of employees around the world
566:06 - and they have laptops and they just need
566:08 - to connect
566:09 - uh securely to the um to the the private
566:12 - v-net or just the v-net in general and
566:15 - so that's where we're going to need uh
566:17 - VPN Gateway and the last one is v-net to
566:19 - vnet this is where you're going to
566:20 - connect two v-ets in two different
566:21 - regions and subscriptions uh or have
566:24 - different deployment models uh and so uh
566:27 - that's again those are transversing the
566:29 - internet uh in all these cases here but
566:31 - yeah that's pretty much it and I guess I
566:33 - think that if you were to set up Express
566:35 - Ro I think you'd have to set up VPN
566:36 - gateways anyway uh but anyway that's
566:39 - what that is there so hopefully that now
566:41 - makes
566:41 - [Music]
566:45 - sense so Azure Express Roes creates
566:47 - private connections between Azure data
566:49 - centers and infrastructure on your
566:51 - premises or in collocation environments
566:53 - and so uh connections don't go over the
566:55 - public internet and as a result offer
566:56 - more reliability faster speeds
566:58 - consistent latencies and higher security
567:01 - so here's a big old graphic here kind of
567:03 - representing what's going on here and
567:05 - the idea is that you're going to have
567:06 - connectivity from different things like
567:09 - from any to any so that's ipvpn Network
567:12 - a point-to-point ethernet Network a
567:14 - virtual cross connection and this is all
567:16 - going to be going through a connectivity
567:18 - provider at a co- location facility so
567:20 - this example would be a uh an edge
567:22 - partner and so you would be the customer
567:25 - Network that' be your on premise or
567:27 - whatever you want to connect so that you
567:29 - can make it all the way to your Azure
567:31 - services and the way you would establish
567:33 - a connection is you create these express
567:35 - route circuits uh it's pretty common to
567:37 - have uh more than one because because uh
567:39 - you want to have high availability just
567:40 - in case the other one goes out uh the
567:42 - idea is that you can route them uh route
567:44 - this traffic to a couple different
567:46 - places so the first one would be things
567:48 - like peering to office uh 365 Dynamics
567:52 - 365 or even your Azure public services
567:54 - so when we say Public Services imagine
567:57 - you just launch a um a virtual machine
567:59 - that is in a public subnet that has a an
568:02 - IP that's reachable from the internet uh
568:04 - that could just be to that or maybe
568:06 - you're using like cosmod DB or like you
568:08 - have fully qualified domains for other
568:10 - services uh so that's just a way that
568:12 - you can get there and then let's talk
568:14 - about the other side which is uh um
568:16 - doing private peering for uh v-ets so
568:19 - the idea is that you have a private
568:21 - v-net so the subnets are private and so
568:23 - the only way is going to be access it uh
568:25 - you want a direct way to access it that
568:26 - way so uh that's the two ways there uh
568:29 - and just to note there is express route
568:32 - direct and so this is like express route
568:35 - with an additional benefit that has
568:36 - greater bandwidth connections from 50
568:39 - megabytes per second up to 10 GB per
568:42 - second and this is really ideal if you
568:44 - have hybrid Solutions where you uh
568:45 - you're removing massive amounts of uh
568:47 - data or where latency matters because
568:49 - you you want uh to feel like these uh
568:52 - cloud services are right there with your
568:54 - on- premise environment you're going to
568:55 - need a lot of speed for
568:57 - [Music]
569:01 - that aure firewall is a managed
569:03 - cloud-based network security service
569:05 - that protects Azure vnet's resources and
569:07 - it's a fully stateful firewall as a
569:09 - service so you're going to get built-in
569:10 - High availability and unrestricted cloud
569:12 - scalability and what you can do is uh is
569:15 - essentially create enforce and log
569:17 - application network connectivity
569:18 - policies across subscription and virtual
569:21 - networks so Azure firewall uses a static
569:23 - public IP address for your v-net
569:25 - resources allowing outside firewalls to
569:29 - identify originating from your virtual
569:31 - Network and the service is fully
569:33 - integrated with Azure monitor for
569:35 - logging and analytics so here is a
569:37 - representation of azure firewall and so
569:40 - the idea is that you're going to launch
569:41 - an Azure firewall into its own v-net and
569:44 - then other v-ets and your on premise or
569:47 - other things are going to pass through
569:48 - that Central v-net onto wherever they
569:50 - want to go and the idea is that uh we're
569:53 - going to be able to uh do things like
569:56 - utilize Microsoft threat intelligence
569:57 - this is going to block known malicious
569:59 - IPS in
570:00 - fqdns that stands for fully qualified
570:03 - domain names and by default the traffic
570:05 - is uh set to deny uh but you can set
570:09 - connectivity policies to filter out
570:11 - traffic in a variety of ways to make
570:13 - sure that you are protected uh there so
570:16 - yeah that's all there is to
570:17 - [Music]
570:21 - it so network security groups filter
570:24 - Network traffic to and from the Azure
570:26 - resource
570:27 - naet uh and so an NSG is composed of
570:30 - many security rules and so here's an
570:33 - example of setting an inbound security
570:35 - Rule and each security rule has the
570:38 - following properties so you can give it
570:39 - a unique name which is all the way at
570:41 - the bottom there uh you can set the
570:43 - source or destination so you're going to
570:45 - set an IP address cider block service
570:47 - tag application group things like that
570:50 - uh then you set the port range so it
570:53 - could just be Port 80 could be all ports
570:55 - where it shows an aster it could be a
570:57 - range of ports you're going to set the
570:58 - protocol so you got TCP UDP icmp you set
571:02 - the action whe whether it's allowed or
571:04 - denied and then you set the priority
571:06 - which is a number between 149 96 and we
571:09 - have two types of security rules we got
571:12 - inbound rules which applies to traffic
571:14 - entering the NSG and outbound rules that
571:16 - apply to traffic leaving the
571:19 - NSG let's take a look at some of the
571:21 - default security rules that NSG sets for
571:23 - you so when you create an NSG you're
571:24 - going to have some created for you by
571:26 - default uh and that's going to be for
571:28 - outbound rules and inbound rules so uh
571:30 - for inbound rules uh you're going to be
571:32 - able to uh accept any virtual networks
571:35 - into your um through the uh NS
571:39 - uh it's going to allow Azure load
571:41 - balancer to make its way in and it's
571:42 - going to deny uh everything else all
571:45 - right then for your outbound rules it's
571:48 - allowed to uh go to any other virtual
571:50 - Network it's allowed to make its way out
571:52 - to the internet and then everything else
571:53 - is denied so there you
571:56 - [Music]
571:59 - go let's take a look here at the logic
572:01 - for security rules and there's a lot of
572:03 - logic here so we'll work our way through
572:05 - it unfortunately there's no fun way to
572:07 - visualize this so we just got work
572:08 - through the textt so you may not create
572:10 - two security rules with the same prior
572:12 - priority and Direction you can have
572:14 - 5,000 nsgs per subscription and 1,000
572:16 - nsgs rules per NSG for priority rules
572:20 - are processed in priority order with
572:22 - lowest number uh process before the
572:24 - higher number and network security
572:26 - groups uh rules are valuated By Priority
572:28 - using a five Tuple information to allow
572:30 - or deny traffic based on source source
572:33 - ports destination destination ports and
572:35 - protocol honestly I don't know what they
572:37 - mean by that uh and I couldn't make any
572:40 - sense it's in the documentation so I'm
572:41 - just showing it to you here then there's
572:43 - flow records They don't exactly explain
572:46 - what they are but I assume the idea is
572:48 - that uh when a request throws uh flows
572:51 - through the NSG they're attaching
572:52 - additional information to it uh so the
572:54 - flow record allows a network security
572:56 - group to be stateful a flow record is
572:58 - created for existing connections uh
573:01 - communication is a Lowder denied based
573:03 - on the connection state of the flow
573:05 - record and so let's talk about
573:07 - statefulness so if you if you specify an
573:09 - outbound Security Port you don't need to
573:11 - set the inbound Port since it will be
573:13 - set for you you only need to specify an
573:15 - inbound uh security rule if the
573:17 - communication is init initially
573:19 - externally uh initiated externally and
573:21 - the opposite is also true if inbound
573:23 - traffic is allowed over a port it's not
573:25 - necessary to specify an outbound
573:27 - security rule to respond to traffic over
573:29 - the port uh and the reason I know this
573:31 - when a when another Port is set with
573:33 - another one that's when it's stateful if
573:35 - it didn't do that then it would be
573:36 - considered stateless but it's state F uh
573:39 - let's talk about Interruption so
573:41 - existing connections may not be
573:42 - interrupted when you remove a security
573:44 - rule that uh enabled the flow and
573:47 - traffic flows are interrupted when
573:49 - connections are stopped and no traffic
573:50 - is flowing in either direction for at
573:52 - least a few minutes so there's the rules
573:55 - uh there a lot of stuff there honestly I
573:57 - won't even remember uh 90% of the stuff
573:59 - but I just wanted to go uh through it
574:01 - with you so there you
574:05 - go hey this is Andrew Brown from exam
574:08 - Pro and in this section we'll be
574:10 - covering Azure DDOS protection what is a
574:12 - DDOS attack a malicious attempt to
574:15 - disrupt normal traffic by flooding a
574:17 - website with large amounts of fake
574:18 - traffic DDOS attacks are big worries for
574:21 - people moving their apps to the cloud
574:23 - these attacks try to use up all of an
574:25 - app's resources so real users can access
574:27 - it any online Point can be a target for
574:29 - these attacks Azure DDOS protection
574:32 - offers Advanced features to counteract
574:33 - DDOS attacks when integrated with
574:35 - recommended application design practices
574:38 - this service is specifically designed
574:40 - for Azure resources within a virtual
574:42 - Network enabling this protection on both
574:44 - new and established virtual networks is
574:46 - simple and doesn't require any changes
574:47 - to apps or
574:49 - resources most frequent types of DDOS
574:51 - attack volumetric attacks these are
574:54 - volume driven attacks that deluse the
574:56 - network with seemingly legitimate
574:57 - traffic by doing so they exhaust
574:59 - available bandwidth leaving legitimate
575:01 - users unable to access the website these
575:04 - are typically measured in bits per
575:05 - second protocol attacks these tax
575:08 - exhaust server Resources by sending fake
575:10 - protocol requests that exploit
575:12 - vulnerabilities examples include UDP and
575:14 - TCP Flooding at layers 3 and four these
575:17 - are measured in packets per second
575:19 - application layer attacks these attacks
575:21 - Target the application layer examples
575:23 - include HTTP floods SQL injections
575:26 - cross- site scripting parameter
575:28 - tampering and slis attacks to defend
575:31 - against these web application firewalls
575:33 - are often
575:34 - employed Azure offers two tiers of DDOS
575:37 - protection
575:38 - DDOS Network protection when combined
575:41 - with best practices in application
575:42 - design Azure DDOS Network protection
575:45 - offers Advanced DDOS mitigation tools
575:47 - it's automatically configured to
575:49 - safeguard specific Azure resources in a
575:51 - virtual Network DDOS IP protection this
575:54 - is a paper protected IP model while it
575:57 - shares core features with DDOS Network
575:59 - protection it provides additional
576:00 - services like rapid DDOS response
576:03 - support cost protection and discounts on
576:06 - Waf so that's an overview of azure DDOS
576:11 - [Music]
576:18 - protection so Azure has uh a few
576:21 - different kinds of load balancers and
576:22 - one in particular is the application
576:24 - Gateway and this is for application
576:26 - Level routing and load bouncing service
576:29 - so application Gateway operates at the
576:30 - OS I layer 7 uh which is also known as
576:33 - the application layer and the idea here
576:36 - is that when you're working about
576:38 - applications you're working with HTTP
576:40 - requests that's what it is uh and so the
576:43 - idea is that it can look at the contents
576:45 - of the HTP request and do some
576:47 - interesting things so maybe it's looking
576:49 - at the path and saying okay you're uh if
576:51 - you have a path um and it's payments go
576:54 - to the VM that has our payment system
576:56 - and then if it's for admin go to the VM
577:00 - that does that uh maybe it has to do
577:01 - something with cookies or um maybe you
577:04 - want to apply a Waf policy to to it and
577:08 - so you can look at those HB requests and
577:11 - determine if it is malicious traffic and
577:12 - filter it out uh so yeah it's all about
577:15 - the application which has to do with HP
577:17 - requests to configure this thing you
577:19 - need to set up a front ends uh ring
577:22 - rules and backend pools and there are
577:24 - two configurations for the front end you
577:26 - can either have it with a private IP
577:28 - which makes it now an internal load
577:29 - balcer or a public IP which makes it
577:31 - either we would call a public or
577:33 - external load balancer uh and there's a
577:35 - both I've never used that option before
577:37 - um but I mean you just really need to
577:39 - know the public and private for the
577:41 - backends you create backend pools and a
577:43 - backend pool is just a collection of
577:44 - resources to which your application
577:46 - Gateway sends traffic uh and so a
577:49 - backend pool can contain virtual
577:50 - machines virtual machine scale sets IP
577:52 - addresses domain names app
577:55 - Service uh and I think you can also send
577:57 - it to like on Prem uh so let's just talk
578:00 - about routing rules which connects the
578:01 - front end and the back end together and
578:03 - so uh here is a more complex example
578:06 - gives you a better idea of this Service
578:08 - uh uh at full uh and so the idea is that
578:12 - you have your application Gateway it's
578:13 - going to send uh traffic or sorry like a
578:16 - DNS is going to send to application
578:17 - Gateway and then you have these
578:18 - listeners and they they listen for
578:20 - incoming traffic uh and then what it
578:22 - will do it will pass on that to a rule
578:26 - and a rule just really says who should
578:28 - we pass the data to uh and then if you
578:30 - are defining a backend pool then you
578:32 - need to have um an HTTP setting that
578:34 - says how do we handle the HTP requests
578:37 - okay so now we'll just look at more
578:39 - granular detail about request or routing
578:45 - rules let's take a closer look at
578:47 - routing rules this is where all the
578:48 - magic happens so a listener listens on a
578:51 - specified Port an IP address for traffic
578:53 - that uses a specified protocol if the
578:55 - listener criteria are met the
578:57 - application Gateway will apply the
578:58 - routing Rule and these come in two
579:00 - flavors we got basic which forwards all
579:02 - requests for any domain to backend pools
579:04 - and multisite forward requests to
579:06 - different backend pools based on host
579:08 - header and host name so the thing is is
579:10 - that uh you uh you can have multiple
579:12 - listeners and you can have an order of
579:13 - them and the idea is that you really
579:15 - want the basic to be on the uh be in
579:17 - last in priority because if it's a first
579:20 - it's going to capture everything because
579:22 - that's its job it's like a catchall uh
579:24 - so just make sure that um if you're
579:25 - using multiple listeners you put basic
579:27 - last uh so that's just something you
579:29 - need to know then for those backend
579:31 - Targets this is where we either Define a
579:33 - back and pool or redirection redirection
579:35 - is just an HTP redirection like uh 403
579:37 - or whatever you have their temporary or
579:39 - permanent very simple uh but for backend
579:42 - pools you have to create HTP settings uh
579:44 - and this tells us a little bit more
579:46 - about how we want to handle HTP you know
579:48 - cookies cook uh connection draining Port
579:50 - requests Etc let's talk about a little
579:52 - bit more so here are the actual options
579:54 - that we can configure for HTP settings
579:57 - so you have your backend Port uh so
579:59 - generally it's either Port 80 or 443 it
580:02 - just depends on where you're doing your
580:04 - SSL termination uh but generally
580:08 - um if you need end end encryption it'll
580:09 - be 443 or uh and then if you are doing
580:12 - connect or if you're doing SSL
580:14 - termination at the blow balancer then
580:15 - it's
580:16 - 80 um then you have cookie base Affinity
580:19 - this allows you to keep a user session
580:21 - on the same server so if you need to
580:22 - persist cookies and use them for
580:24 - authentication you'll want to enable
580:25 - that you got connection draining this
580:27 - gracefully uh removes backend pool
580:28 - members during plan service updates so
580:31 - you is that you know when there is an
580:33 - update it's not just going to abruptly
580:35 - cut over it's going to wait until a
580:37 - connection disconnects from a server
580:39 - before uh not no longer sending more
580:41 - requests there because you don't want a
580:42 - a connection dropping in the middle of
580:44 - an update it's not great for a user then
580:46 - you have request timeouts this is the
580:48 - number of seconds the application
580:49 - Gateway will will wait to receive a
580:51 - response from the backend pool before it
580:52 - returns a connection timeout error
580:54 - message and last our um oh sorry we
580:57 - still have override back in path these
580:59 - allow you to override the path in the
581:00 - URL so the request for spe a specific
581:02 - path can be red to another path the idea
581:04 - is imagine you want to send it to um but
581:07 - bananas and you want bananas actually
581:09 - internally route to oranges I don't know
581:11 - why you want to do that or maybe to
581:12 - plantains that' make a lot more sense so
581:14 - that's something you can do uh and the
581:16 - last one is the override the host name
581:17 - so application Gateway normally leaves
581:19 - the host name alone uh but you know if
581:22 - you're using multi-tenant services like
581:24 - apps uh like app service or API
581:27 - management it needs very specific host
581:29 - header set so this is where you can
581:31 - override them and change them for those
581:32 - services so hopefully that really gives
581:34 - you a good picture of application
581:36 - Gateway but I feel that that's all you
581:37 - need to
581:41 - know so let's take a look here at Azure
581:43 - scale sets and these are used to
581:45 - automatically increase or decrease your
581:47 - virtual machine capacity so imagine uh
581:50 - you have a web application behind an
581:52 - application load balancer and you have
581:54 - an increase in traffic you want to be
581:56 - able to quickly add more capacity by
581:58 - adding another identical virtual machine
582:01 - uh or when uh there's a decrease in
582:04 - traffic remove it to save cost and that
582:06 - is what Azure skill sets do
582:08 - um so you're creating uh scale policies
582:10 - to automatically add remove based on
582:13 - host metrics host metrics could be like
582:15 - CPU utilization or uh Network in uh
582:18 - you're going to create health checks and
582:20 - you can also uh set a repair policy to
582:23 - replace unhealthy instances you can
582:25 - associate a load balancer to distribute
582:27 - virtual machines across uh availability
582:30 - zones and you can scale to 100 or even a
582:33 - th000 VMS using scale
582:36 - sets
582:40 - so one thing you're going to probably
582:41 - want to do with your scale set is
582:42 - associate with a load Bouncer and in
582:44 - honest I don't run any kind of
582:46 - application workload uh unless it is in
582:48 - a scale set and behind a load balancer
582:51 - because it's just good practice to do
582:52 - that and the reason why you're going to
582:53 - want a load balancer is that it's going
582:55 - to help you evenly distribute your VMS
582:57 - across multiple availability zones and
582:59 - that's going to give you high
583:00 - availability uh because you definitely
583:02 - want to try to run uh 3 VMS across three
583:05 - azs to get that a high Veil ability uh
583:08 - you can also use a load balancer probe
583:10 - checks and these are more robust health
583:12 - checks than what skill set provides you
583:15 - uh so that's just an added benefit there
583:18 - uh and when we're talking about um load
583:20 - balancers we have two different choices
583:21 - here we have the application Gateway and
583:24 - so this is for HTTP htps web traffic
583:27 - load balancer uh and then you also have
583:30 - Azure load balancer and this is going to
583:31 - support TCP and UDP Network traffic and
583:35 - things like that so depending on what oi
583:37 - layer you need to operate on is going to
583:39 - determine what kind of load balcer you
583:41 - need to
583:45 - use so let's take a look at scaling
583:47 - policies and these determine when a
583:49 - virtual machine should be added or
583:50 - removed to meet the current requirements
583:52 - and you have this uh Little Wizard here
583:54 - this is what the What wizard you see or
583:56 - form you see when you're creating a
583:57 - scale set initially with very simple
583:59 - features but let's just focus on two
584:01 - things scaling out and scaling in so
584:03 - scaling out is when an instance should
584:05 - be added to a skill set to increase
584:06 - capacity so you choose your metric uh
584:09 - which is hardcode in this case to CPU
584:11 - threshold so you choose your metric the
584:13 - duration how many VMS you want to add
584:15 - and then the scale in is the opposite
584:17 - that's when you're removing uh uh uh
584:20 - something from the scale set to decrease
584:21 - your capacity and again that interface
584:24 - is very simple uh when you actually go
584:26 - ahead and create a scale set after you
584:28 - go and update it you're going to end up
584:29 - with a lot more options and here are the
584:32 - options you can see they're extremely
584:34 - robust and so uh you have the option to
584:36 - choose one of the built-in host metrics
584:38 - that are already be being collected on
584:40 - your virtual machine so uh the most
584:42 - popular here and there's more than this
584:44 - but uh the most popular here is CPU
584:46 - Network in network out dis read and
584:48 - write uh then you have your Aggregates
584:50 - so you can decide how you want to group
584:52 - or uh collect that data uh before you
584:55 - apply your final operations on it uh
584:57 - then you have your operator so you can
584:59 - say greater than or greater than or
585:01 - equal to then you have your actions you
585:03 - can say increase the count of so add uh
585:06 - x amount of of servers or you can say
585:09 - increase by percentage so imagine you
585:11 - had um uh 10 servers and you want to
585:14 - increase the load by 30% that would add
585:16 - three additional servers if you want
585:18 - more metrics than just the built-in host
585:20 - metrics you can absolutely get more uh
585:22 - but you're going to have to install a
585:23 - couple things uh for app uh if you want
585:25 - more app specific metrics like page
585:27 - performance or page load performance and
585:29 - sessions count you would uh install a
585:32 - small instrumentation package for app
585:33 - insights and uh if you want to have more
585:36 - detailed host metric so you might have
585:38 - host metrics but you want them in more
585:39 - detail or more available host metrics
585:41 - you'd install the Azure diagnostic
585:43 - extension within your VM um there are a
585:46 - few other uh scaling policy options that
585:49 - are well they're not exactly in that
585:50 - form but they are around or are
585:52 - associated with them one is the scale in
585:55 - policy not to be confused with the scale
585:56 - in
585:57 - option um and this determines what
586:00 - virtual machine should be removed when
586:01 - you decrease capacity so it's just a
586:03 - simple drop down and you have a few
586:05 - different options so there's the default
586:06 - option this is where it deletes the VM
586:08 - with the highest instance ID and it's
586:10 - going to do that b uh uh take in
586:12 - consideration uh the balancing across
586:14 - A's and ads or it can delete the newest
586:16 - virtual machine so delete the newest VM
586:19 - again uh this one's only across uh virt
586:21 - or availability zones or delete the
586:24 - oldest VM this is going to delete the
586:26 - oldest VM and balance across
586:28 - availability zones then you have an
586:30 - update policy and this determines how uh
586:34 - how VM instances are brought up to date
586:36 - with the latest scale that model so
586:37 - again it's another drop down and you
586:39 - have a a few options here so you have
586:41 - automatic so increasing the start
586:43 - upgrade immediately in random over uh
586:45 - order or or manual so existent or
586:48 - existing instances must be manually
586:50 - upgraded or rolling and so this update
586:53 - upgrades roll out uh in batches with
586:56 - optional pause uh and just one other
586:58 - note here is that if you want to do
587:00 - automatic OS upgrades you can enable uh
587:02 - this to help ease update management by
587:04 - safely and automatically upgrading the
587:05 - OS dis for all instances so there you
587:09 - [Music]
587:12 - go Health monitoring is a feature with
587:15 - scill sets you're absolutely going to
587:16 - want to turn on uh and what it does is
587:18 - it determines whether your instance your
587:20 - virtual machine instance is healthy or
587:22 - unhealthy so you have that option to
587:24 - disable or enable it and it comes in two
587:26 - different modes we're going to have the
587:28 - application Health extension which is
587:29 - what we're seeing on the right hand side
587:31 - here and this is where you ping an HTTP
587:33 - or htps request with a specific path and
587:37 - expect a back a specific status so the
587:41 - status here would be 200 so the idea is
587:43 - that you could say ping the homepage and
587:44 - if the homepage appears then therefore
587:46 - the server must be healthy or you can
587:48 - make it your own custom uh page that's
587:50 - what I like to do I have like a health
587:52 - check page the other mode is load
587:54 - balancer probe uh this is only going to
587:56 - work if you have an Associated load
587:57 - balancer and uh here you can check based
588:01 - on TCP UDP HTTP request so this is a
588:03 - little bit more robust um um so I
588:07 - generally would recommend using this
588:09 - mode over um the the first mode because
588:12 - generally when you have a scale set
588:14 - you're going to also want to have a load
588:16 - balancer uh now if you want to replace
588:19 - unhealthy instances they have an
588:20 - automatic repair policy so this is an
588:22 - automatic I mean like it's not turned on
588:24 - by default so you have to uh explicitly
588:26 - say you want to turn it on and what that
588:28 - will do is that if it finds an instance
588:30 - that is unhealthy it's just going to uh
588:32 - terminate it and then launch a new
588:33 - instance all right and so there you go
588:39 - [Music]
588:40 - all right so now that we have our scale
588:42 - set we're going to continue on learning
588:44 - more about availability by creating an
588:45 - application Gateway so this is actually
588:47 - a type of load balancer even though it
588:49 - has Gateway in the name then there's
588:51 - Azure load balancers and the difference
588:52 - between these two is this one's layer
588:54 - seven for the applications and this is
588:56 - layer four uh so TCP UDP like at a lower
588:59 - layer so uh you know since we have a web
589:02 - application which is a simple apoi page
589:04 - we're going to want to serve it up via
589:06 - the application Gateway all right so
589:08 - make your way over to the top here and
589:10 - we'll click on application Gateway or
589:12 - just type that in and we'll go ahead and
589:13 - add ourselves a new application Gateway
589:16 - we'll choose wolf here I'm going to name
589:17 - this wolf um uh application
589:21 - agw and then we do not yeah we don't
589:25 - need any Autos scaling right now so
589:27 - we'll just leave that alone I'm going to
589:28 - set that to one we're going to choose
589:29 - two one and three because the rule of
589:31 - three counts here we always want three
589:33 - and then down below make sure you choose
589:34 - the correct virtual network if you're
589:36 - not conf
589:37 - make sure you go over to your scale set
589:39 - and just double check to make sure where
589:41 - it resides so if I go over here into my
589:43 - scale set and check it's in wolfnet uh
589:47 - wolf vet
589:48 - 499 how I ended up with more than one I
589:50 - don't know I just the tutorial I've done
589:52 - this a few revisions so I've ended up
589:53 - with an additional one there notice it's
589:55 - complaining because it's trying to uh
589:57 - put this in the default but it needs its
589:58 - own uh subnet so here under subnets
590:01 - we're going to create one just for the
590:03 - vgw so vgw and we're going to need a
590:06 - range first so go over to address uh
590:08 - space ranges and we'll pick out the next
590:10 - number so
590:11 - 10.0 uh. 2.02 24 we'll hit
590:17 - save and once that has created we'll
590:19 - make our way over to the subnet and when
590:21 - we hit subnet it will automatically
590:22 - select the next one for us I'm going to
590:24 - put vgw in here we'll hit
590:27 - save and then once that's saved we'll
590:29 - make our way back to the wizard notice
590:31 - up here the bread comes if we click one
590:33 - back we can make our way uh back to
590:35 - where we are and then choose that vgw
590:37 - subnet we'll go to the front to the
590:39 - front end uh section here we got public
590:41 - private or both we want public today and
590:43 - we will need a new IP address so I'm
590:45 - going to call this uh wolf
590:49 - vgw we'll go ahead and hit okay we'll go
590:52 - to backends here add a backend pool I'm
590:54 - just going to call this backend and
590:56 - we'll drop down and choose a virtual
590:59 - machine scale set if you do not see it
591:00 - here it's because they're not in the
591:02 - same v-net make sure that's the case and
591:04 - then we'll go to configuration so we got
591:06 - a front the back end now we got to glue
591:08 - them together with routing rules we'll
591:10 - just say my rules we have listeners and
591:12 - backend targets so we'll name our
591:14 - listener name my listener if you can
591:16 - spell it that's a hard one for me so I'm
591:18 - just going to copy paste that in take
591:20 - out the space there uh the front end IP
591:24 - is it's going to be public we're going
591:25 - to have HTTP on Port 80 we'll have basic
591:28 - if we had multi sight so if you have
591:30 - more than one destination but we do not
591:33 - we'll go to backend targets and we will
591:35 - choose the backend pool that we created
591:37 - and then we have to choose or add um
591:40 - HTTP settings this is all kind of
591:41 - crammed together it doesn't really line
591:43 - up very well so I get a bit confused
591:44 - here so I think what we want to do is
591:46 - hit add new here and then we'll say my
591:49 - HTTP
591:50 - settings uh Port 80 is good we do not
591:53 - have cookies we do not care about
591:54 - connection draining all these options
591:56 - are not important to us uh for our very
591:58 - simple application um but if you were
592:01 - creating a real web application that uh
592:03 - that has a session you probably would
592:05 - want to turn these both on okay we'll go
592:07 - ahead and hit add and we do have
592:09 - path-based routing but this is not
592:11 - something we need to worry about right
592:13 - now this is where you could say you
592:15 - could have multiple targets so you could
592:16 - say Okay anytime it is uh cookie then go
592:20 - to this virtual machine and then if it's
592:22 - another one you could say go somewhere
592:23 - else or even do a redirect that's a
592:25 - great way of attaching things to your um
592:28 - uh virtual machine there we'll go ahead
592:29 - and hit add and so now we have all of
592:31 - our information we'll hit tags and we
592:35 - will go hit create and I'll see you back
592:37 - here uh when this is done deploying
592:40 - okay all right so I've been waiting a
592:42 - while here for uh this to complete but
592:44 - uh it looks like that we're still having
592:46 - a bit of trouble uh so what I'm going to
592:48 - do is make my way over to the actual uh
592:50 - load balancer here because I just
592:52 - remembered and this happens because I
592:54 - don't normally make a scale set
592:55 - separately I usually make it with the
592:56 - load balcer but if we make our way over
592:58 - to the backend pool and we're to click
593:00 - into here it actually has a a thing that
593:02 - says down here virtual machine skill set
593:04 - was added to this backend pool upgrade
593:06 - all in es of wolf scale set for this
593:08 - change to work so uh those the scale set
593:10 - needs to get updated is the is the
593:11 - problem here so what I'm going to do is
593:13 - make my way over to the scale
593:16 - set and we'll see if we can do an
593:19 - upgrade here so if we just checkbox them
593:21 - all on there I'm going to go hit
593:23 - upgrade and then this will perform an
593:25 - upgrade on the
593:26 - instances and we'll just give that some
593:28 - time and I'll see you back here in a
593:31 - moment okay so that didn't take too long
593:33 - for the upgrade and if you just hit the
593:34 - refresh here you're going to wait for
593:35 - the status was running and now they're
593:37 - all healthy so if we make our way back
593:39 - to um our load balancer tab here if you
593:41 - still have something open just go to
593:43 - backend pools you should see three
593:45 - targets if we go up to our overview over
593:47 - here we'll go grab whoops our IP address
593:50 - now why the uh they copy the text there
593:53 - I have no idea but if we go paste that
593:54 - up into here and take that out we get
593:57 - our page so there you go that's all it
593:59 - took to get the availability um or the
594:02 - actual server
594:03 - Runing if we go on the left hand side
594:06 - and we go over to health probes this is
594:09 - interesting to look at we're not going
594:10 - to set it up but I just want to show you
594:11 - that you'd write in your uh health check
594:13 - here and then you could specify um a
594:16 - different path so if you had like a page
594:17 - called and this is pretty common for a
594:19 - lot of places they'll have a page called
594:20 - health check and so that's a just kind
594:22 - of like a way to configure a custom
594:24 - health check uh for your instances but
594:27 - this is all I really wanted to
594:28 - accomplish here today um I don't think
594:31 - there's really much of anything else
594:32 - that I'd like to do here um so what
594:34 - we'll do is we'll go and make make our
594:36 - way back to all of our resources and
594:39 - we'll try to find our original Resource
594:41 - Group here if we can uh find it's
594:44 - probably easier if we just go to
594:44 - Resource
594:51 - groups great and I'll just go ahead
594:54 - and delete this one
595:00 - here there we go it's going to delete a
595:02 - whole lot of stuff uh but yeah there we
595:04 - go

Cleaned transcript:

hey this is Andrew Brown over here at free Camp bringing you another free Cloud certification study course and this time it's the Azure Solutions architect expert also known as the a305 and the way we're going to achieve uh Azure certification is through lectur content HandsOn labs and as always I provide you a free practice exam that way you can go get that certification to put it on your resume or LinkedIn to go get that Solutions architecture role if you like these kind of free Cloud study courses the best way to support more of these materials is by purchasing the optional paid materials over on the exampro platform exampro doco uh if you don't know me I'm Andrew Brown and I've taught a lot of different Cloud certification study courses here Azure adabs gcp kubernetes terraform uh you name it I've taught it so you're in great hands and I will see you in class uh in a moment ciaoo hey everyone it's Andrew Brown and we're at the start of our journey asking the most important question first which is what is the a305 so the a305 um is a certification or an exam uh to get the certification Microsoft Azure solution architect expert I use a funny wording there because um previously you had to take two separate exams the 303 and the 304 and then that would get you the certification but uh Microsoft has seen the light and now we just have one exam for a certification it looks like it's mostly like that now but just understand the future they might change that but yeah the a305 more or less means Microsoft Azure Solutions architect expert going to get off screen here now but uh yeah it is an expert level certification and it's focusing on comprehensive strategies for Designing Solutions on Azure it has a deeper dive into advanced networking configurations you don't see me doing this but I'm doing quotations air quotations here sorry air quotations for the word advance because it's not as indepth as something like the um uh the A700 Advanced networking but yes you do need to know your networking your Cloud net working very well for the certification it has a broad examination of data and Security Solutions in Azure uh the sc900 is a great recommendation for that but we'll talk about that when we look at the road map here and this has extensive HandsOn knowledge you should know how to do things in the Azure portal arm templates and other uh uh resilient architectural tools all Azure exams are very code and script driven so having great developer experience is a must make sure you do things in the um the portal um now this one the 305 is not as um technically heavy like the 104 in terms of HandsOn but it still is a great challenge because of how broad uh the information is being covered in the 305 who's the certification for well consider the certification if you're uh looking to get an Azure Solutions architect expert role uh you're designing Solutions using Microsoft Azure you're implementing and monitoring Azure infrastructure you're creating configuring resources for Azure applications uh enjoy crafting resilient and scalable Cloud architectures I want to warn you this is a hard exam um it is very common to fail this exam even if you practice a lot and you do your best job uh it may just come down to the type of questions you get so you know we do the best to give you practice exams and all the information here just understand when you get to expert level or professional level certifications it's not you it's just the questions and you might have to try multiple times so don't get discouraged if you uh if you take an exam and you're just like one point shy away that is what it's like doing these professional or expert certifications let's take a look at our Azure road map um and I'm right away I'm already noticing that this is a bit buggy this is supposed to just say 305 but just you know ignore that one one little bug there that says 305 but anyway uh we have a bunch of uh Azure certifications Microsoft certifications this isn't the full list um in fact there are some that I probably would even recommend on Route there but definitely before you go for expert you should have your A9 a900 gives you a good broad view of all services uh the a 104 is generally what is uh or absolutely recommended uh on path to the 305 I would also pick up the 204 on the way um in fact in this course a lot of the content not a lot of the content but a good chunk of the content came from our 104 and our 204 and then we had to round it out with other things so understand that these two are are part of the course if you've done the 104 204 you're going to be in really good shape or well aligned for the 305 and then you can spend time trying to figure out case studies and things like that I would probably also add the sc9 00 on here uh that is a fundamental certification the dp900 would be also a good one um I'm just kind of running out of space for all these things that's why I don't have them all listed like here you can see I have the networking one shoved down in here um but Azure just has a lot a lot of certifications okay um but anyway how long would it take to uh pass uh for studying well if you're beginner don't do this exam it is not a beginner certification it would take you uh well over a month if you're a beginner start in the fundamentals do the associates um and work your way up to it if you're experienced you already have that 204 that that or and the 104 and you already have multiple uh years working experience it might not take you that long to study but you know on average I think 50 hours is for somebody in the midtier area we're looking at 50% lecture 50% uh practice or sorry 50% lecture in Labs 50% practice exams I actually probably would even bump this up a bit more more try to get your hands on as many practice exams as you can the huge challenge with um uh Microsoft certifications is they have a rich uh type of exam questions and not everyone can emulate those questions so a lot of times you are uh needing to overstudy uh with simpler exam type questions in order to try to be able to tackle the harder exam so just understand that that is something you have to consider um 30 days absolutely you need a whole month for this exam uh for for uh for studying a recommended study one to two hours really Max your time out and so that you are in the best shape possible um what other things should we consider well make sure you watch the video lecture content do the HandsOn Labs absolutely do all of them and do the best that you can in fact there are Labs that we wanted to record but they were so hard to do and this course was already so long that we didn't even record them and put them in here but we actually do have um I believe additional Labs with the instructions on our platform so you know max out as many Labs as you can to make sure or or HandsOn work make sure you're in good shape absolutely do paid online practice exams we have our own sets here um just ignore these numbers here I just did not feel like taking a new screenshot um but uh yeah just the the times are a little bit different here because you actually get 120 minutes and it's more around 60 Questions so 57 makes sense um but anyway yeah you definitely want to look into that in terms of the actual exam guide outline there are four domains and each domain has its own waiting this determines how many questions in a domain that will show up uh Microsoft exams they like to do arrange of questions which it's not great for the test taker but I guess it makes the exam harder I you know I don't personally like this I don't like that they do ranges like this but uh you know I guess they think that makes it if it's more more confusing then therefore the exam must be more worth it for the end result I don't know so the first domain um or section whatever you want to call it it's between 25 and 30% of the exams so this design design identity governance Monitoring Solutions the next one is 20 to 25% for design data storage solutions the next one is 12 to 20% so design business continuity Solutions and then 30 to 35% to design infrastructure Solutions where are you going to take this exam it's going to be with Pearson view um previously Microsoft I think they offered it I think it had PSI online before um even adab us is doing the same thing they're just doing Pearson so Pearson and Pearson view um so you can do it at inperson test center or online for the convenience your home uh from your own home so what we're talking about is Pearson view online this is the online product exam system and then you have the Pearson view uh network of test centers this is where you do it in person the word Proctor means a supervisor visor so someone is going to monitor you during this exam um sometimes when you uh go for your exam they're uh uh like online they will go and actually call and talk to you and ask you to show the room and it can be very involved so just understand that uh there is somebody watching while you take these exams uh the grading for this one is um 700 out of a th000 points so basically it's around 70% I say around 70% because aure uses scaled scoring so um technically it's not exactly 70% it's 700 out of a th000 I know that's confusing but generally uh what you want to do is aim to go get 80 80% I always think if you have a margin of 10% above that you're passing for practice exams then you're giving yourself a buffer you might even want to go higher and try to Target 85% um and in the result you will actually probably get 70% um in terms of the the amount of questions there's between 40 and 60 so you should be able to get 12 and 18 questions wrong though the thing is is that when we save 12 18 that's not exactly true because you have to understand the format of the questions for Microsoft Azure exams and they have a lot of different kinds of formatted questions you got multiple choice you got multiple answer you got drag and drop you got build list reorder you have active screen you got hot areas you have case studies you'll absolutely see case studies in this exam and then questions can have exhibits so it can get really complex um in terms of what you will see will be different from other folks you definitely will see multiple choice multiple answer you'll definitely see case studies and some of these other ones might show up some questions are worth more than one points there is no penalty for wrong questions so do not uh do not leave any questions blank some questions cannot be skipped and you have to fill them in um so yeah there's a lot going on there the duration is two hours um I could have swore that the 204 was three hours but I looked it up and maybe they changed the time but I really thought they would give you more time for this exam but yeah it's just two hours so it is still a stressor um you get two minutes per question basically so we're looking at an exam time of 120 minutes but your seat time is 150 minutes all we do is we add 30 minutes to to make the seat time the seat time refers to the amount uh uh that you should allocate for the exam um so this includes time to review instructions Show online Proctor your workspace read and accept the NDA complete the exam provide feedback at the end of the exam the reason I I include this here is just to remind you that um when you're when you are planning this you have to plan for all of the time not just the exam but also the time around the exam this exam is only valid for 12 months uh this is something different so Azure used to have two years and then you'd have to pay every time now they're doing every 12 months so every year you recertify um however the recertification process supposedly isn't as hard because renewals are free um and I think you can even check in like six months into it if you want to do uh uh if you want to do it sooner but we'll take a look at that because again those are new things to me and I want to uh share those with you yeah hopefully that gives you an idea of the exam guide itself but we'll see you in the next one okay ciao hey this is Andrew Brown and welcome to the marketing site on Microsoft for the a305 and you'll notice the name of this is actually called designing Microsoft Azure infrastructure Solutions and it gets you that badge the uh Solutions expert but the uh strange thing is that Microsoft does not name the badge the same thing as the exam uh it's not that strange if you understand the history of their certifications they like to sometimes have more than one exam that you have to pass in order to get a badge and so they don't name them the same as the badge so that's just what's there and the old exam used to have two the 304 and the 303 but we'll go down here below because the reason I'm on this page is I want to show you the sandbox if you go here to the sandbox and we'll open this up we can see the formatting of the questions now it's not example questions um we have example ones on our platform and we have we should have a free set but what we'll do we'll go ahead and click next and this is just like if you were to take it online or a test center and we'll click through and we'll go here and there's just 10 questions and the purpose of the sandbox is to show you the formatting of the questions I'm going to get out of the way there we go and so the first one we have is a multiple choice pretty straightforward says what is your favorite sound we'll say a bell we'll click next this one is multiple choice so here we can checkbox two things we'll do that the next one here is drag and drop so you have these things you drag and then you drop them into the area on the right here notice that um I can actually fill in multiple so uh you know just it can vary based on the type of question so it might not just be one to one these things might still remain in here then we have um build order build list reorder so which five tasks should you do in the correct order so you say making a sandwich We'll add the pickles the Mayo the ketchup this that this uh whoops this and that okay and then you can move them around notice that this actually has more than five questions these are draggable here you'll notice these everywhere we'll go ahead and hit next um this question is an active screen so which option should you ACH uh choose to achieve this goal so we'll scroll on down here um to answer select the appropriate setting in the applications example properties window in the answer area so notice here if we hover over might be hard to see see but there is a blue line so it's showing like where do you click right so here that's the only one we drop it down then we choose an option here so it's kind of like simulating um a components without actually having the environment we'll go to the next one here so this one is an active screen so you need to implement self serving provisions of virtual machines this actually kind of sounds like a proper question the solution must ensure that the user caner the virtual machines Etc what do you choose so we'll go here and we'll choose option one and then op option to notice that these are relaying to this table here but this is just you know another way that we could be working with stuff here we have a hot area so here it says which Services should you configure so we'll go take a look here and notice that we can select something from the hot area so it's very similar to that other one um active screen again you're just like clicking on stuff we'll go to the next one here and so this is a case study so uh case studies are uh pretty complex the idea is you can click around here and read all of this stuff okay so you read through all of this stuff and then what you're going to do is go back to your question at the top I know it's confusing but that's where it is and then you can go ahead and answer uh the question we'll go back and hit next then you have exhibits so exhibits are basically just tabs the idea is that you read the question and then you can go look at exhibit there can also be multiple exhibits so I think in this one here's an example where you have multiples and then you go back to your question and then you answer it okay so you know hopefully that uh makes things really clear um but you can see that they really do have a lot of different formatted questions and uh that can make this uh quite challenging if we want to read about the specifics of the exam we can go to the uh study guide now I remember there being at least in previous exams um or um in other exams you used to have a PDF you can download I can't seem to find that anymore all there is is the marketing site here maybe that's what they want to do but you can go through here and read about like the certification renewals the scoring all the stuff here we were mentioning about certification renewal so if you want to read a bit more about that where they talk about how renewals are free and you have a sixmonth renewal window and things like that you can read all about it um but yeah there you go and uh we'll see you in the next one hey this is Andrew Brown and before we get into Azure ad I need to point something out it's not called Azure ad anymore it's called Microsoft entra ID Microsoft decided to change the name of azure ad why nobody knows but I can tell you no customer likes this particular change um somebody just had a lot of time on their hands over at Microsoft but we do need to address this and I need to point out that I'm not refiling all of the content that I made just to change the name because that's crazy I will at some point when this when the content is stale but the content is not stale they just changed the name on us um but I wanted to just go over that quickly here so uh the names here we have Azure ad is now Microsoft entra ID then the Azure ad tiers is from P1 P2 still P1 P2 the Azure ad external identities is now called Microsoft entra external identities and if we scroll on down we have a logo change so instead of this which by the way I really like the old logo they didn't need a muck with it but anyway we have uh the older ones here and so this is the new one here and so there are some name changes here Azure ad single sign on now Microsoft entra entra uh single sign on we'll go down below here and you can see well more name changes okay so um anyway yeah they renamed it and you know customers are just going to take a while to get used to it I still like calling azuread I know a lot of other people that like still calling it azuread but it's at some point we'll get moved over to it and we're just going to use both names okay now coming over to uh uh the portal I need to show you that if you type in Azure ad it's still going to pull up Microsoft Entre ID okay now you don't want Azure ad B to see which is interesting they didn't rename that U which is a it is part of azure kind of in a sense but it's more for um if you're building applications and you want to um have authentication into it so just understand that there's not consistency all over the place especially even their documentation the marketplace still says Azure ad all over the place um even down below uh you know Azure ad notification so you know there's just going to be that Legacy of azure ad but anyway yeah what you want to do is go go over to Microsoft Entre ID it all looks the same it's just some name changes okay but but yeah there you go see you in the next one hey this is Andrew Brown from exam Pro and we are looking at Azure active directory and this is a cloudbased identity and access management service to manage users signins and access to ad related resources so Azure active directory is Microsoft's cloudbased identity and access management service which helps you your employees sign in and access resources so that could be external resources like Microsoft Office 365 Azure portal SAS applications or internal resources so applications within your internal networking or access to workstations on premise and you can use Azure ad to implement single sign on so you can see that Azure ad is basically like the the the one solution to log everything and uh we actually use it at exam Pro we use it with Microsoft teams or uh you know for the exam pro pro platform our mid panel is tied to it so when we want to log into the mid panel with credentials we have it there uh we use it with AWS to log into there and we use it to log into Azure so it has a lot of flexibility and if you're building out applications for Enterprises they're likely using ad and so this is the reason why everybody adopts it or needs to understand it so it's a service I really really do want you to understand and know as your active directory comes in for additions we have the free tier and by the way each uh uh uh tier that goes up has the features before it but uh free has MFA SSO basic security usage reports and user management then you have the Office 365 apps which is uh revolves around if you're using that Suite so you have company branding SLA two sync between on premise and cloud and then the premium tiers which really comes into Enterprise or or or on premise hybrid architecture so hybrid architectures Advanced group access conditional access premium 2 identity protection and identity governance only thing I don't like about Azure ad is that uh you can't really create your uh custom access controls unless you have premium one or premium two but that's just how they do it so there you go go so let's take a look at the use case for Azure ad and we basically covered it in the introduction but I just want to reiterate it in a different way with a bit of a visual uh so that it really helps uh it sync into your uh brain there so Azure ad can authorize and authenticate to multiple sources so it can authenticate to your on premise ad to your web application allow users to log in with uh ipds uh so identity providers could be like use Facebook or Google login uh you can use it with Office 365 or a Azure Microsoft and so just a visual here uh notice that uh we have Azure ad and using Azure ad connect we can connect to on premise through uh app registrations we're able to uh connect our web application to Azure ad with external identities we can um use Facebook or Google uh uh uh login and then for cloud applications we can connect to Office 365 or Microsoft Azure so active directory existed way before Azure and so let's just do a quick uh uh rundown of the history so we have an idea of what we're looking at so um Microsoft introduced active directory domain services in Windows 2000 to give organizations the ability to manage multiple onpremise infrastructure components and systems using a single identity per user so it's been around for 20 years and Azure ad takes this approach to the next level by providing organizations with identity as a service so idas solution for their apps across uh cloud and on premise and both versions are still used today because they just have different utility and so we have active directory which is for on premise and then you have Azure ad which is just the cloud hosted version and in many regards these can also be connected together um but there you go so remember that the domain controller is the server that users are going to be using to authenticate to the directory Service uh and so when you create an active directory Azure sets one up for you but there's some cases where you might want to set one up yourself and the reason why is that you could be like an on like an Enterprise where you already have your own active directory on premise but you've decided that you want to move it over to Azure ad uh because you just want a fully managed active directory and uh you want to tap into the cloud but uh the thing is that some domain Services those are features on your domain controller just might not be available and that's where you're going to need to set up your own domain controller and that's where Azure active director domain Services come into play because these provide managed domain services and so they have managed domain services such as domain joins uh group policies uh ldaps uh uh curb Ross Never Can Say That properly ntlm authentication and so the great thing is here is you can have these domain services but you're not going to have to deploy them manage them attach them they're just going to work so there you go so let's talk about um uh the term tenant and a tenant represents an organization in an active directory and a tenant is dedicated to the Azure ad service instance a tenant is automatically created when you sign up for either Microsoft Azure or Microsoft InTune or Microsoft 365 and each a or Azure ad tenant is distinct and separate from other azure ad tenants and so if you uh if you were in um uh Azure ad and you clicked on your tenant information that's that's basically what that is right so that's my exam Pro one and it has its own special tenant ID and we can see that it's licensed for Office 365 and so that tells you that I'm using the Office 365 uh tier of azure ad so now let's take a look at some of the ad objects starting with users so users represent an identity for a person or employee in your domain and a user has login credentials and can use them to log into the Azure portal so here I am a user and you can see it shows how many times I've logged in and I'm part of different uh ad groups uh and so you can assign roles and administrative roles uh to users you can add users to groups you can enforce authentication by uh like with MFA you can track user signins as you can see on the right hand side you can track device uh devices users login and uh and allow or deny devices uh you can assign Microsoft licenses Azure ad has two kinds of users we have users that's a user that belongs to an organization and guest users this is a guest uh is a user that belongs from another organization and we'll cover uh Azure ad roles uh in the r section here uh because that is what's that you're going to be using to apply to these users so groups in Azure ID lets resource owners assign a set of access permissions to all members of the group instead of having to provide the rights one by one and so on the right hand side here you can see I have a bunch of groups in exam Pro and groups can contain owners and owners have permissions to add or remove members and then the members have rights to do things okay and so for assignment you can assign roles directly to a group you can assign applications directly to a group and to request join groups so uh the group owner can let users find their own groups to join instead of assigning them to them and the owner can set up the group to automatically accept all users that join or require approval this is really great when uh you just want people to do the work themselves as opposed to having to do all that manual labor of adding them to groups let's talk about how we're going to uh give users rights to access uh resources and there are four different ways to do that the first is direct assignment and this is where the resource owner is going to directly assign the user to the resource then you have group assignment this is where the resource owner assigns an a a group to the resource which automatically gives all group members access to the resource then you have rulebase assignment this is resource owner uh this is where the resource owner creates a group and uses a rule to Define which users are assigned to a specific resource and then you have external uh Authority assignment this is this access comes from an external Source such as an on premise directory or SAS application and I just want you to know that there's four different ways to do it so uh to get access to resources all right let's take a look at managed identities for Microsoft enter ID or Azure ad managed identities is a concept in Microsoft enter ID that Associates identities with internal resources where these identities have their own roles and tokens managed identities increases security by allowing you to link directly resources to other resources without having to share any security information over the network those resources will be authenticated against enter ID to see if they have the necessary permissions to manipulate other resources for example we can allow our applications to access Azure key vault in order to retrieve a secret without exposing any passwords managed identities is available in two types system aside identities are created and managed by enter ID when you create a managed identity in a service instance only that Azure resource can use this identity to request tokens from enter ID user assigned identities are created and managed manually the identity is managed separately from the resources that use it the table provides a comparison between system assigned and user assigned features in the context of azure creation for system assigned the identity is created as part of an Azure resource whereas user assign the identity is created as a standalone Azure resource life cycle for system assigned the identity shares its life cycle with the Azure resource It's associated with while user assigned the identity has an independent life cycle deletion for system assigned when the associated Azure resource is deleted the identity is also deleted whereas user assigned the identity must be deleted explicitly sharing across Azure resources for system assigned the identity cannot be shared and is associated with only a single Azure resource whereas user assign the identity can be shared and can be associated with more than one Azure resource managed identity is under the identity blade for an Azure resource you assign roles to provide permissions to a managed identity in summary managed identities enhance security through seamless resource integration eliminating exposed credentials with system assigned and user assigned options Azure bolsters efficient flexible Resource Management let's talk about external identities so external identities in aad allows people outside your organization to access your apps and resources while letting them sign in uh and use whatever identity they prefer so your partners Distributors suppliers vendors or other guests can bring their own identities such as uh Google or Facebook uh you can share apps with external users that's for B2B stuff uh if you develop apps and tender for Azure ad tenants uh for single tenant or multitenant you can do that as well uh you can develop white label apps for consumers and customers so this would be like Azure ad uh b2c so there you go hey this is Andrew Brown from exam Pro and the next topic will be exploring or access reviews in Microsoft enter ID or azuread access reviews in Microsoft enter ID allow you to regularly review and manage access to resources in your organization with access reviews you can review who has access to resources and determine whether their access is still necessary access reviews are useful in maintaining security and compliance by ensuring that only authorized individuals have access to sensitive resources access reviews can be conducted for various types of resources such as applications groups and SharePoint online sites you can configure access reviews to occur on a regular schedule and select reviewers to conduct the reviews reviewers can be internal or external to your organization during an access review the reviewer will be presented with a list of people who have access to the resource being reviewed they can choose to approve or revoke access for each individual access can be revoked immediately or scheduled for a later date reviewers can also provide a reason for their decision which can be useful for auditing purposes access reviews are crucial in large organizations to regularly identify and resolve access issues they ensure resources are accessed only by necessary users and that access is revoked when no longer needed overall access reviews maintain security by assessing user access ensuring compliance preventing breaches and safeguarding data promoting a security aware culture make sure to know access reviews because it did show up a few times on the exam the next topic will be covering his single side on on an enter ID single sign on an enter ID is a feature that allows users to authenticate once with enter ID and then access multiple applications and services without having to authenticate again when a user signs into enter ID with their credentials enter ID creates a security token that can be used to access other resources within the same organization this token can be used to authenticate the user to other cloud based or on premises applications that have been integrated with enter ID SSO supports a wide range of applications including cloudbased applications such as Microsoft 365 Salesforce and Dropbox as well as on premises applications such as SharePoint and sap SSO can also be used with custombuilt applications using industry standard protocols such as saml open ID connect and oo there are several ways you can configure an application for SSO choosing an SSO method depends on how the application is configured for authentication Cloud applications can use open ID connect ooth saml passwordbased or or linked for SSO single sideon can also be disabled on premises applications can use passwordbased integrated Windows authentication header based or linked for SSO Deon premises choices work when applications are configured for application proxy this flowchart can help you decide which SSO method is best for your situation the main SSO protocol supported in Azure include open ID connect in ooth open ID connect is an identity layer built on top of ooth 2.0 it allows for authentication and authorization of users in a secure and standardized manner it is saml saml is an XML based protocol used for exchanging authentication and authorization data between an identity provider and a service provider it is commonly used for Federated authentication scenarios password based authentication this refers to the traditional username password authentication method where users provide their credentials directly to authenticate linked authentication Azure provides the ability to l link multiple accounts from different identity providers to a single user identity this allows users to authenticate using any of their linked accounts integrated Windows authentication it will let users access applications using their Windows domain credentials utilizing their current Windows session for authentication header based authentication in this method the application accepts an authentication token in the form of a header in each request the token is validated by the application to authenticate the user you'll need to be familiar with these assets o protocols is there will be questions asking you which SSO protocol is best suited for a specific application what is multiactor authentication a security control where after you fill in your username email and password you have to use a second device such as a phone to confirm that it's you logging in MFA protects against people who have stolen your password MFA is an option in most Cloud providers and even social media websites such as Facebook so that's an overview of single sign on an ENT ID the next topic we'll be going over is conditional access conditional access provides an extra layer of security before allowing authenticated users to access data or other assets conditional access is implemented via conditional access policies which are a set of rules that specify the conditions under which signins are evaluated and allowed for example you can create a conditional access policy that states if the user account name is member of a group for users that are assigned The Exchange user password security SharePoint or Global administrator roles require MFA before allowing access this policy enables MFA enforcement based on group membership simplifying the process compared to configuring MFA for individual users when roles change conditional access policy analyzes signals including user and location device application and realtime risk and verifies every access attempt via access controls this requires MFA block access and allow access signals or metadata associated with an identity attempting to gain access user or group membership policies Target specific users and groups giving admins find grain control over access named location information IP location information IP address ranges are used to permit or deny access based on geographical locations device policies can be applied based on the platform or status of a user's device application users attempting to access specific applications can trigger different conditional access policies real time sign and risk detection signals and Azure ad identity protection detect risky signins if risks emerge policies can prompt actions such as password resets multifactor authentication or block access pending admin intervention Cloud apps or actions can include or exclude Cloud applications or user actions that will be subject to the policy user risk for customers with identity protection user risk risk can be evaluated as part of a conditional access policy user risk represents the probability that a given Identity or account is compromised common decisions Define the access controls that decide what level of access based on Signal information block access most restrictive decision Grant access least restrictive decision still require one or more of the following options require multiactor authentication require device to be marked as compliant require hybrid enter ID join device require approved Client app and require app protection policy conditional access policies are available and can be utilized with the following licensing plans Microsoft 365 business premium Microsoft 365 E3 and E5 enter ID premium P1 and enter ID premium P2 licenses overall conditional access acts as a robust security measure in Azure ensuring that authenticated users can only Access Data under specific conditions hey this is Andrew Brown from exam Pro and in this section we'll be going over the types of azure roles roles can be a bit confusing this is because Azure has three types of roles that can serve the same purpose the first type of role are classic subscription administrator roles this is the original RO system next we have Azure roles this is an authorization system that's also known as rolebased access controls and is built on top of azure resource manager then we have Azure active directory roles Azure ad roles are used to manage Azure a resources in a directory Azure active directory roles are used to manage Azure a resources in a directory such as creating or editing users assigning administrative roles to others resetting user passwords managing user licenses and managing domains among other tasks the rules follow a similar rbac model and include several builtin roles like Global admin application admin application developer and billing admin but also allow the creation of custom roles for more specific control Azure rules specifically Azure rule based access control is a system that provides fine grained access management for Azure resources allowing administrators to Grant users specific rights to resources there are several predefined roles in Azure like owner contributor reader and user access administrator each providing specific levels of access to Azure resources and custom roles can be defined as well classic subscription administrator roles refer to the older model of azure access control and include three types account administrator service ad administrator and coadministrator Azure roles and classic subscription administrator roles can have overlapping responsibilities for example the service administrator in the classic Model has a similar role to the owner role in the Azure rbac where both can manage resources in the subscription however Azure rbac roles provide a more granular level of control compared to Classic roles offering more specific access management they allow administrators to delegate specific tasks and Grant specific permissions reducing the need to give full administrative privileges and therefore for enhancing security the next topic will be covering are the access controls so identity access management essentially allows you to create and assign roles to users for the Azure rules or the rbac system rules restrict access to Resource actions which are also known as operations there are two types of rules the first type are builtin roles these roles refer to the set of predefined roles offered by Microsoft and azure the rules are read only and cannot be altered cover a wide array of standard scenarios to facilitate efficient and secure access management for Azure resources the second type of rule are custom rules these rules represent user defined roles in Azure tailored to incorporate unique permissions and logic based on specific requirements that are satisfied by the available builtin roles a role assignment is when you apply a role to a service principle which could be a user group service principle or managed identity deny assignments lock users from performing specific actions even if a role assignment grants them access the only way to apply deny assignments is through Azure blueprints so that's the access controls for Azure hey this is Andrew Brown from exam Pro and in this section we'll be covering classic administrators classic administrators refer to the original role system in Azure despite the Legacy status of classic administrators understanding these roles can still be beneficial like when work with older Azure setups however it's recommended to use the new rbac system for managing access to resources given its enhanced granularity and flexibility classic administrators have three types of roles the first type is a account administrator this role is essentially the billing owner of the Azure subscription it's responsible for managing subscriptions making payments and changing the billing details the account admin has no access to the Azure portal meaning they can't manage resources directly the second type is service administrator this Ro shares the same level of access as a user assigned the owner role at the subscription scope in the rbac model they have full access to the Azure portal meaning they can manage all resources within the subscription the third type is coadministrator this has the same access level of a user who is assigned the owner role at the subscription scope it's similar to a service admin but the main difference is that there can be multiple co administrators offering shared management capabilities you shouldn't have to worry much about the classic admin roles because I don't think I encountered any exam questions on it it'll mostly be focused on the rbac and Azure ad roles also note that classic resources and classic administrators will be retired on August 31st 2024 the next topic we'll be covering is the Azure Ro based access control so Azure Ro based Access Control helps you manage who has access to Azure resources what actions they can perform on resources and in what scope which are the areas they have access to ro assignments are the way you control access to Resources by assigning a role to a security principle like a user group service principle or managed identity at a particular scope you define who can perform what actions on which resources our role assignment consists of these three elements scope and role definition and security principle there are four fundamental Azure roles which include owner contributor reader and user access administrator Azure rbac includes over 70 builtin roles these roles are designed to serve many common use case and range from brought to very specific permissions such as virtual machine contributor or network contributor scope is the set of resources that access for the role assignment applies to scope access controls at the management subscription Resource Group and resource level assigning a role at the Management Group level inherits it across all Associated subscriptions and resources at the subscription level it applies to all resource groups and resources within the specific subscription a role assigned at the resource Group level affects all resource within that group at the resource level a role assignment applies only to that specific resource a role definition is a set of permissions that determines what actions can be performed such as read write or delete on various resources roles range from broad such as owner with extensive management permissions to specific such as virtual machine reader with more targeted permissions Azure has builtin roles and you can Define custom roles this table shows the four fundamental builtin roles the owner role has full access to all resources including the right to delegate access to others the contributor role can manage all types of azure resources including the ability to create update and delete but can't Grant access to others the reader role has the ability to view existing Azure resources but can't make changes or Grant access to others the user access administrator role has the ability to manage user access to Azure resources including granting and revoking access but can't create update or delete resources a security principle represents the identities requesting access to an Azure resource such as a user which is an individual who has a profile in Azure active directory a group which is a set of users created in Azure active directory a service principle which is a security identity used by applications or services to access specific Azure resources or a managed identity which is an identity and Azure active directory that is automatically managed by Azure so that's an overview of azure rolebased Access Control the last type of role we'll be covering are azuread roles azuread roles are used to manage azuread resources in a directory such as create or edit users asside administrative roles to others reset user passwords manage user licenses and manage domains we'll go over a few important builtin azuread roles so here are some that you should know the first one is global administrator this grants you full access to everything all the features in Azure a another important role is user administrator this grants you full access to create and manage users for the billing administrator role this role can make purchases manage subscriptions and support tickets including monitor service Health not all organizations needs can be satisfied by these predefined roles therefore you can create custom roles which are very flexible and can Define the exact set of permissions that you need keep in mind that you'll need to purchase either Azure a d premium P1 or P2 to create custom roles so that's a short overview of azurea D rolles hey this is Andrew Brown from exam Pro and in this section we'll be covering the anatomy of an Azure role so it's important to know that the property names in an Azure Ro definition can vary depending on whether you're using Azure Powershell or Azure CLI the two different interfaces use different syntax and terminology we have an example here on the right and we'll go through the properties together a ro definition and Azure consists of these properties name this is the display name of the custom role also note that the ones highlighted in blue is the Syntax for Azure power shell and bold black is for Azure CLI so it's name for Azure power shell and rle name is for Azure CLI the next property is ID this is the unique ID of the custom Ro and it is autogenerated for you is custom this indicates whether this is a custom role it can be either true or false description this property describes the role actions this is an AR of strings that specify the management operations that the role is allowed to perform not actions an array of strings that specify the management operations that are excluded from the allowed actions data actions this is an array of strings that specify data operations the role is allowed to perform to your data within that object not data actions this is an array of strings that specify the data operations that are excluded from the allowed data actions assignable Scopes this is an array of strings that specify the Scopes that the custom Ro is available for assignment you can only Define why Management Group and assignable scopes of a custom role so in this example we see that there is an asterisk symbol that's used in the specific actions and the actions property like storage Network compute Etc this is called the Wild Card permission symbol represented as an asterisk this is used in the actions not actions data actions and not data actions properties to represent all or any operations and wild card allows you to apply to match everything in the examp example of actions microsoft. storage asterisk slre in a rle definition this means that the role is granted the permission to perform read operations on all resource types under the Microsoft storage resource provider so the Wild Card essentially allows you to either Grant or deny a wide range of permissions with a single statement however it should be used with caution because it can grant or deny more permissions than intended if not properly managed it can be a bit confusing to distinguish between Azure policies and Azure rules so we'll do a little comparison to help you understand the key differences for the Azure policies they are used to ensure compliance of resources they evaluate the state by examining properties on resources that are represented in resource manager and properties of some resource provider it doesn't restrict actions which are also called operations they ensure that resource state is compliant to your business rules without concern for who made the change or who has permission to make a change even if an individual has access to perform an action if the result is a noncompliant resource Azure policy still blocks the create or update as for the Azure roles they are used to control access to Azure resources they focus on managing user actions at different levels of Scopes and Azure roles do restriction on Azure resources another thing people tend to get confused between our Azure ad roles versus Azure roles for Azure ad roles they are used for managing Azure ad resources for Azure roles they are used for fine grained Access Control to Azure resources active directory resources include users groups billing licensing application registration Etc Azure resources included virtual machines databases cloud storage Cloud networking Etc by default Azure roles and Azure a roles are separate and do not span Azure and Azure a by default the global administrator doesn't have permissions to manage Azure resources Global administrator can gain access to Azure resource if granted the user access administrator role so azuread roles are specifically for managing Azure a resources while Azure roles focus on access control to Azure resources hey this is Andrew Brown from exam Pro and in this section we'll be covering Azure policies Azure policies enforce organizational standards and assess compliance at scale policies do not restrict access they only observe for compliance here is an image with a list of builtin policies that you can use right away from Azure let's go over a few key aspects of azure policies the first one are policy definitions which is a Json file used to describe business rules to control access to resources then we have policy assignment this is the scope of a policy's effect it can be assigned to a user a resource Group or Management Group next we have policy parameters the these are the values you can pass into your policy definition which makes your policies more flexible for reuse and we have initiative definitions this is a collection of policy definitions that you can assign for example a group of policies to enforce pcss compliance next we'll look at viewing noncompliant resources once a policy is assigned it will periodically evaluate the compliance State you can see how compliant we are on the compliance tab according to the example in the image it shows that we we are noncompliant it can occur due to many factors but it's most likely because virtual machines should have Disaster Recovery enabled so let's look at some of the main use cases for Azure policy organizational compliance Azure policy enforces standards and assesses compliance at scale such as enforcing compliance labels on all resources cost control policies can prevent over provisioning to save costs like limiting the creation of high class VNS security enhancements policies can improve security by enforcing configurations for example requiring secure transfer for All Storage accounts resource consistency policies can enforce consistent configurations like a specific naming convention or tag structure Regulatory Compliance policies can ensure specific configurations for Regulatory Compliance such as data hosting in specific regions for data sovereignty so that's in brief overview of azure policies the scope of an Azure policy is the set of resources that the policy is applied to when you assign a policy you define the scope at which the policy is enforced this could be as broad as a management group or as specific as a single resource the hierarchy of Scopes and Azure are like other Scopes like asure resource manager or rbac it moves from brought to specific in the following order Management Group greater than subscription greater than Resource Group greater than resource at each level you can apply different policies as per your requirements policies apply higher levels of the hierarchy are inherited by all the lower levels this structure provides a powerful mechanism for applying broad organizational policies while still allowing for flexibility and customization at lower levels it's a key part of how Azure enables you to manage and control your resources effectively and in a way that suits your organizational needs hey this is Andrew Brown from exampro and we'll go over the anatomy of an Azure policy definition file display name this serves as an identifier for the policy and has a 128 character limit type this field which is red only indicates the source of the policy it could be guiltin maintained by Microsoft custom created by you or static Microsoft owned and typically pertains to Regulatory Compliance description this provides the context of the policy metadata this optional field is used to store key value information on the policy though this determines which res resource types are evaluated and changes whether resource provider or Azure resource manager is used resource manager modes include all this includes resource groups subscriptions and all resource types and indexed this only includes resource types that support tags and location resource provider modes were used in deprecated services like Microsoft container service data and are now primarily utilized in services like Microsoft kubernetes data and Microsoft key Vault data moving on to parameters parameters are values passed into the policy to improve its flexibility a parameter has the following properties name the identifier for the parameter type could be a string array object Boolean integer float or datetime metadata utilized by Azure to display userfriendly information such as description display name strong type assign permissions default value an optional field to set a default parameter value allowed values an optional field for setting accepted parameter values you reference parameters by using field and in next let's go over the policy rule so this consists of if and then blocks in the if block you define one or more conditions that specify when the policy is enforced you can apply logical operators to these conditions to precisely Define the scenario for a policy the next concept we'll look into is the policy Rule and policy effect which is important in determining the impact of the policy so we'll go over a list of common policy effects deny if a resource is created creation or update doesn't adhere to the policy it fails audit this creates a warning event in the activity log when evaluating a noncompliant resource but it doesn't stop the request upend this effect adds extra parameters or FS to the resource during its creation or update for example it could append tags on resources like cost center or specify allowed IP addresses for a storage resource audit if not exists similarly to the audit effect this creates a warning event in the activity log when a resource doesn't comply with the policy but it doesn't stop the request audit is used to audit the properties of a resource while audit if not exists is used to audit the existence of a related resource deploy if not exists this effect executes a template deployment when a specific condition is met for example if SQL encryption is enabled on a database a template can be executed after the database creation to configure it in a specific way and the last one is disabled this effect turns off the policy rule often used for testing purposes these policy effects provide a range of resp responses to noncompliance enabling you to manage your resources according to your organization specific requirements hey this is angrew Brown from exam Pro and we're going to take a look at how to keep things compliant within our Azure account with Azure policy so let's make our way over to the Azure policy portal by typing policy at the top here and right away you can see already that I have a policy assigned I didn't sign this Azure did this for me by default and it's a great way to start understanding how this is useful so I'm going to go ahead and just click into that one there and this is an initiative uh policy meaning that it's made up of a bunch of policies we'll go over that over in a moment but you can see here that's saying hey you should turn on all these things they're not turned on by default so you should turn them on and these are ones that you have not turned on you might not want to consider turning some of these on uh if you have to consider costs but I think most of these are free um but that's something that you'll have to decide on your own uh but that gives you a general idea there so let's go back to policy at the top on left hand side here we can see our assignments and here I have that initiative type there if I click into it allows me to edit it there uh and what's really nice is if I want to disable it I could just disable it uh but I think this is a default one so oh no I can disable it okay great and you can see it was assigned by who uh and then you can also uh take remediation actions uh here so by default this assignment will only take effect on newly created uh resources existing resources can be updated via a remediation task after the policy is assigned so if you need this apply to ones that are created or or prior or updated that's something you might want to do but anyway we'll get out of there and we'll go take a look at all the big list of predefined uh definitions uh that Azure gives us so this is a great way for us to get started um you'll notice on the right hand side we have initiative and we have policy again initiative is a collection of policies down below and these are individual ones let's just go take a look at some uh policies and maybe we we can go apply one to like a virtual machine um so what I'm going to do is just drop down um this here and we can just unselect here and let's just go take a look do we have one called virtual machines not really um but we we go for compute here and we can just take a look here so uh audit virtual machine without Disaster Recovery let's take a look at that one and I just want to see the full description here audit virtual machine without Disaster Recovery configured I think that sounds really good to me so I'm going to go ahead and assign that and we can choose a scope and so I'm going to choose my subscription here and then we can choose a resource Group and you're going to notice that I don't actually have any resources so uh to scope that within so what we'll go do is launch ourself a virtual machine so make our way over to the virtual machine um portal here we'll add a virtual machine and I want something really cheap and so here it's set to a more expensive one because I was launching a Windows Server uh previously but I want the cheapest server I can get actually before I do that I'm just going to go back and oh yeah it's on Ubuntu just making sure and the one I had that was really cheap is the B1 LS and so I'm just going to go up here to the top we'll name this beour and uh we'll put in the beour group there um this size does not support Azure spot that's totally fine I don't need spot did I turn that on by accident oh I did and this all looks fine to me I'm not going to be logging into this machine we just want to launch it up as an example I'm going to move to standard SSD CU I don't need premium here and this looks fine to me we'll go ahead and hit review and create so uh we'll go ahead and create that and I guess we have to download that private key probably a good idea we're not going to use it for anything but that's okay and now it's deploying let's see if we can select the scope as it's creating I'm not sure if it'll let us do that I'm just going to hit cancel here and reopen here and let's see if it shows up now it doesn't so uh we'll just wait a moment here uh I'll go back to my policy here and I'm just going to wait for this server to create and once that is created um what we'll do is we'll just come back here and see if we can now scope that all right so after a short little wait um our server is ready here so let's make our way back to our policy here and uh I guess I lost it so I have to go back to definitions and we'll drop down categories deselect we'll go to compute and we'll go ahead and uh click on Disaster Recovery again here you can see the policy and its entirety so that's kind of nice uh we'll go ahead and assign that and we'll choose our scope so we'll go back here choose subscription one now we should have beour and I'll go ahead and hit select uh we can assign it exclusions not something I'm going to do today um there is the name of it that's totally fine then we can hit enabled and I'm the one who's enabling it so that sounds like a good idea we'll go next uh we're going to leave REM remediation off we don't need to do that today we'll hit create uh and so now this policy is assigned it says here please note that the assignment takes around 30 minutes to take effect so if we work our way back here and look at assignments um I don't see it here yet so there it is and um it's probably not going to show us on this there so we'll go back to our overview and so I want to see it hasn't started yet so we'll just wait for a while here might take 30 minutes might take 10 not sure and we'll just see what happens this should show up as noncompliant but let's see that actually happen all right so after waiting a little while here I think it was about 15 20 minutes we can now see that it's saying that it's noncompliant so we'll go ahead and click into there and we can see what exactly it is complaining about let me click over to that resource so there you go it's not too uh complicated there uh we'll go ahead and just uh delete that assignment and uh if you're wondering where blueprints are they're all the way over here we talked about blueprints in the actual course not something we actually have to do because it is probably a better way of uh doing things I just wanted to point that out uh to you um but yeah there you go so it's as simple as that hey this is Andrew Brown from exam Pro and in this section we'll be covering Azure resource manager Azure resource manager is a service that allows you to manage Azure resources Azure resource manager is a collection of services in the Azure portal so you can't simply type in Azure resource manager in the search tab it is a management layer that allows you to create update or delete resources apply management features such as access controls locks or tags and right infrastructure is code using Json templat we will be examining the following key components that form the Azure resource manager layer we have subscriptions management groups resource groups resource providers resource locks Azure blueprints as well as resource tags Access Control rolebased access controls Azure policies and arm templates you can think of azure resource manager as a gatekeeper all of the requests flow through arm and it decides whether that request can be performed on a resource such as the creation updating and deletion of a virtual machine and its arm's responsibility to authenticate and authorize these requests arm uses azure's rolebased Access Control to determine whether a user has the necessary permissions to carry out a request when a request is made arm checks the users assigned roles and the permissions associated with those roles if the user has the necessary permissions the request is allowed otherwise it is denied the next concept we'll go over is the scope for Azure resource manager we've briefly covered scope in Azure policy and Azure rbac but we'll go into more detail with them in the following sections for arm so scope is a boundary of control for Azure resources it is a way to govern your resource by placing resources within a logical grouping and applying logical restrictions in the form of rules management groups are a logical grouping of multiple subscriptions subscriptions Grant you access to Azure Services based on a billing and support agreement resource groups are a logical grouping of multiple resources and resources can be a specific Azure service such as Azure VM so that's an overview of azure resource manager Azure management groups provide a way to manage multiple subscriptions by organizing them into a hierarchical structure every directory is assigned a single top level management group known as The Root Management Group this root group forms the base of the hierarchy and can have multiple management groups or subscriptions nested under it one of the key benefits of using management groups is that all subscriptions within a Management Group automatically inherit the conditions apply to the Management Group some important facts about management groups you should know are a single directory can support up to 10,000 management groups the hierarchy of a Management Group tree can be up to six levels deep not including the root level or the subscription level each Management Group and subscription could have only one parent each Management Group could have multiple children all subscriptions and management groups exist within a single hierarchy in each directory so that's a short overview of azure management groups before you can do anything in your Azure account you'll need to have a subscription an Azure account can have multiple subscriptions and the most commonly used ones are free trial pay as you go and Azure for students for example if you wanted developer support you would add a developer support subscription to your account once a subscription is set up it provides you with the ability to configure various settings and features features such as resource tags these allow you to categorize your resources according to your organizational needs access controls helps manage access and permissions for your Azure resources resources groups are logical containers in which Azure resources are deployed and managed cost management and billing provides tools to track and manage your Cloud spending and more these features provide you with a high degree of flexibility and control over your Azure resources allowing you to manage your resources effectively and securely the next topic will be covering a resource groups so a resource Group is a container that holds related resources for an Azure solution for example you might have a resource Group that contains multiple virtual machines for a specific project or application as for resources these are manageable items available through Azure a resource could be an individual entity like a virtual machine next we have resource providers these are services that Supply Azure resources an example of a resource provider is Microsoft compute which provides compute resources like VMS in order to use Azure resources you have to register resource providers many resource providers are registered by default for you with your subscription however for certain resources you may need to manually register the resource provider this image shows a list of resource providers available in Azure and in the status you can see if they are either registered or not registered you can register resource providers under your subscription in the Azure portal through Azure Powershell or Azure CLI this ensures you have access to the latest resources and features provided by that service the next topic we'll explore our resource tags so a tag is a pair consisting of a key and a value that you can assign to Azure resources these tags can be used to categorize resources based on different criteria relevant to your organization here are some examples of tags Department equals Finance status equals approved approved team equals compliance environment equals production project equals Enterprise location equals West us tags allow you to organize your resources in the following ways Resource Management tags can help you sort and manage resources based on specific workloads or environments such as developer environments cost management and optimization tags can be used for cost tracking setting budgets and creating alerts operations management tags can be used to manage business commitments and service level agreement operations such as Mission critical Services security tags can be used for classifying data and assessing security impact helping you manage your security posture other helpful ways include governance and Regulatory Compliance Automation and workload optimization all in all tags provide a flexible customizable method for managing your Azure resources according to your specific needs the next topic will be covering a res resource locks resource locks are a critical feature in Azure that helps Safeguard important resources from accidental modifications or deletions as an admin you may need to lock a subscription Resource Group or resource to prevent other users from accidentally deleting or modifying critical resources especially in environments with multiple administrators or automated processes in the Azure portal you can set the following lock levels him not delete this lock ensures authorized users can still read and modify a resource but they can't delete the resource read only this lock ensures authorized users can read a resource but they can delete or update the resource there are a number of ways to manage locks here are some of them Azure portal you can easily create View and delete locks through the Azure portal Azure poers shell UCM dlet like new as resource lock to manage locks Azure CLI commands like as lock create help manage locks Azure resource manage your templates you can also Define locks in your arm templates hey this is Andrew Brown from exam Pro and in this section we'll be covering Azure blueprints Azure blueprints enable quick creation of govern subscriptions the key term here is governed while one can easily create a subscription in their account a govern subscription indicates there's a process and set expectations for how the subscription should be configured Azure blueprints allow you to compose artifacts based on common patterns or those specific to an organization into reusable blueprints the service is designed to help with environment set the service is designed to help with environment setup blueprints are a declarative way to orchestrate the deployment of various resource templates and other artifacts such as Ro assignments policy assignments Azure resource manager templates resource groups Azure blueprint Services powered by the globally distributed Azure Cosmos DB ensuring blueprint objects are replicated across multiple regions providing redundancy and resilience a common query is the difference between an arm template and an Azure blueprint nearly everything that you want to include for deployment in Azure blueprints can be accomplished with an arm template with arm templates you can store them either locally or in Source control there isn't an active connection or relationship to the arm template post deployment on the other hand Azure blueprints maintain a connection between the blueprint definition what should be deployed and the blueprint assignment what has been deployed Azure blueprints can upgrade multiple subscriptions simultaneously if they're governed by the same blueprint this means Azure blueprint supports Improv tracking and auditing of deployments so that's an overview of azure blueprints hey this is Andrew Brown from exam Pro and what we're going to be doing is moving resources from one Resource Group to another and understanding the limitations around that uh so I just have this page pulled up here because I just want to emphasize that I've seen exam questions on this stuff and there's a lot of little nittygritty things that could show up as a solution so the idea is that when you're talking about about moving resources um you know you're moving them uh into different regions or to different subscriptions or different resource groups and generally they're pretty straightforward but there are some edge cases where uh things will not work as expected and that's based on um some particular services so when we're looking at app Services devop Services classic deployment Network movement guidance Recovery Services virtual machines so I definitely know that for app Services you're going to run into issues uh like if you're moving from uh one subscription to another and you already have a web app uh service in the one that you're moving it into it won't allow you to do it and so there's a lot of little things like that okay um and also if you are migrating them you can go ahead and use the diagnostic tool to uh debug it and it will tell you some additional information so you will have to read through all of these I just can't show you all that in a follow along um but what we'll do is we'll just go through the basics here of moving things between between resource groups and so what we'll need is a few resource groups so I'm going to add a new one and we're going to call this one the USS fed or the Federation of planets and we'll put that uh East us and then we will make another Resource Group and we'll call this the Klingon Empire and for fun we will place it in uh West so we'll go ahead and create that so now we just need something we can move around back and forth so what I'm going to do is I'm going to make my way over to diss because that's a great example of something we can move around and change and so I'm going to add a new dis and uh I'm going to place this in the Federation and I'm just going to say um dilithium I can never spell that right right let's see if I can get the proper spelling for this thing that is not real di lithium here and so it's going to be over here I don't care about availability zones and stuff like that Source type this is fine and so we have the initial size I do not need a drive that big let's go super small uh we could even do an hcd because those are even cheaper and we'll just choose the small size down here um and so we'll pick 32 and so this should be alal encryption networking we don't really care we're not doing anything other than moving this around so we'll go ahead and review and create right and so then we'll go create that resource and we'll just wait a little while here it shouldn't take too long all right so the dis is ready so we'll go to this resource and here it is so now let's say we want to move that to our other resource Group which is in another region what we're going to do is we're going to go back to our resource groups here and if we go into the Federation of planets what we can do is uh go to overview select the dis go over here and say move to another Resource Group and I'll just click these other ones to show you I don't have any other subscriptions to move this to um so I'll just go back there I thought maybe could show you some stuff but I just realized I don't have much in this account or with other subscriptions so we'll just go back to overview here and just move it to another Resource Group and technically we're moving it to another region so we'll see how that goes um and so we will select um the klling on Empire I understand that the tools and scripts associated with the will not work uh etc etc so we'll hit okay and that's going to take a little bit of time to move so while that is going I'm going to make a new tab so we don't lose that history there I just want to talk about some settings you can put on your resource groups so if we go into the Kling on Empire and we go down below here we should have the ability to apply locks and there are a few different types of locks we have readon and delete so readon means it's a readon resource and delete means we cannot delete the resource which makes sense right so read only should mean we shouldn't be able to modify this so I'm going to say uh don't touch do not touch and I say okay here and so what that means is that I should not be able to modify the resource uh whether I can delete it or not is another story we'll find that out and another question is can I move that resource outside of this if it's set to those modes and that's what we're going to find out so moving back over here um has this finished moving it's still validating so we're going to have to wait a little bit here and I'll see you back in a moment all right so after waiting a little while here I believe that it's done moving so I'm still in my old Resource Group here what we'll do is click back into here go to overview and we see that it's no longer there but if we go over to the cing on Empire we go back over just clicking to overview there it is so we had no problems moving the dis to another region and another Resource Group now we did apply this lock over here so what is going to happen when we try to modify this dis I think you know the answer but let's give it a go just to see what happens so over here we're going to want to uh re resize it here I'm going to pick 64 I'm going to go hit resize and it says fail to update the dis because of that reason so um there you cannot perform a right operation because of the lock so there you go um now let's go back and we are going to go apply uh another lock here this time I'm going to make it so you can't delete it and so we say don't do not delete me all right and so what we'll do is we'll go back uh and we'll try to delete it so we'll hit the delete button we'll say Yes again it says you cannot do it now here's the next question can you move that resource out or in to Resource Group if it has lock or read only on uh and that's what I want you to guess whether you can or not CU you are in read only so would you be able to move something that's readon so we'll go ahead and move it we're going to move this to another Resource Group and I'm going to send it back to the Federation of planets I say I understand hit okay and it failed the check we'll see why cannot perform the right operation because there are locks please remove some of the locks so which is it is it delete is it the read only do I have to remove both that's what we need to go find out so back in our Resource Group I'm going to go ahead and first remove parent resource locks can't be edited here okay that's fine so we'll go back to here and we will go into the locks and we'll first delete the readon and then what we'll do is we will attempt to uh delete this again see what happens it looks like it's moving so it's not that you can't move it uh out if it's in readon or sorry in delete but you but you can't if it's read only because it's readon right now when we're talking about moving resources into a group that's set to read only I'm almost certain that you can absolutely do that um so uh we could stage that I guess just to make sure so what I'm going to do is I'm just going to set up another Resource Group just to make sure that we we know this for certain just in case I Wrong the star the Romulan star Empire Romulan star Empire and I'm going to just leave in the same region and so for this one in particular what I'm going to do is apply a readon I might as well just put both on here but I'll just I'm just going to do read only and we'll say do not delete me and then what I'll do is I'll go back to the Federation planets and I'm just going to actually I'll just go to discs and we'll create a new dis here um and we will place it in the Federation here and we will just say um dark matter or antimatter and we will just change this to hhd as small as we can go we'll review and create that and we'll go ahead and create that and this will not take long we do not have to wait that long for this and notice here that um we could not uh this is interesting so there was an error moving the resource moving resources failed because resource grou Federation has active deployment so if you are moving stuff uh and then you do deploy it's going to cancel that deploy so that's interesting to know um so I believe that this new one is deployed I cannot remember I think we set this one to have read only here on the um what do you call it the uh rulin star Empire which isn't showing up there it is just double check there so the question is can I move a resource into a readon we'll go here and uh I guess we need to actually go to the Federation of planets we'll go to overview view we have antimatter and I want to move that into Romulan star Empire say I understand it'll either say we can or we can't and we absolutely can otherwise it would aired out at this point so hopefully you can keep that straight so you can move a resource into a group that has readon you cannot move it out if it's set you can absolutely move it out if it's set to delete delete just protects against delete if you are moving your resource from one Resource Group to another uh and then you deploy something it's going to cause that movement to fail and then there's those edge cases for uh moving Resources with Azure and that's something you should spend some time reading up on those use cases or maybe I'll just uh pick out the most important ones and put it into a cheat sheet all right uh and so what I'm going to do is just go ahead and clean this stuff up um so what I'm need to do here is go and and remove the delete I think it's only on here so if I go to my locks I'm going to go back to Resource groups here and we will delete the locks here and so now I can go ahead and delete these groups I guess I have to do them one by one which is kind of annoying but that's just how it goes it says it's locked it's not locked anymore oh it's still there look at that okay could have swore I definitely uh did something there there we go and we will go ahead and delete the last one there we are so yeah hopefully you know a bit more about moving resources around hey this is Andrew Brown from exam Pro and in this segment we'll be diving into arm templates so what exactly is infrastructure is code infrastructure is code is the process of managing and provisioning computer data centers such as those in Azure using machine readable definition files like JSO n files rather than depending on physical Hardware configuration or interactive configuration tools you write a script that will set up cloud services for you there are two main approaches to IAC declarative here you describe your desired outcome and the system figures out how to achieve it imperative here you provide Specific Instructions detailing exactly how to reach the desired State arm templates or JSO n files that Define Azure resources you want to provision and Azure Services you want to configure with arm templates you can ensure a declarative approach meaning you merely Define your intended setup and the system handles the rest build remove or share entire architectures in minutes reduce configuration mistakes and know exactly what you have defined for a stack to establish an architecture Baseline for compliance or over arm templates Empower you to establish an architecture Baseline for compliance achieve modularity break up your architecture in multiple files and reuse them Ensure sensibility add Powershell and Bash scripts to your templates test using the arm template toolkit preview changes before you create infrastructure via template see what it will create builtin validation will only deploy your template if it passes track deployments keep track of changes to architecture over time policy is code apply Azure policies to ensure you remain compliant use Microsoft blueprints which Forge a connection between a resource and its template integrate with CI CD Pipelines utilize exportable code letting you capture the current state of resource groups and individual resources and benefit from Advanced authoring tools for instance Visual Studio code offers sophisticated features tailored for crafting arm templates so as you can see arm templates has quite a lot of uses all right moving forward let's delve into the structure or the skeleton of arm templates skeleton is a term used to describe the basic framework structure of an arm template think of it as the blueprint that guides how an arm template should be organized and what elements it should contain schema this describes the properties that are available within a template content version this denotes the version of your template you can provide any value for this element API profile use this value to avoid having to specify API versions for each resource in the template parameters these are the dynamic values you feed into your template when you're deploying or updating resources it offers flexibility and enabling you to use the same template in different scenarios or environments just by changing the parameter values variables this is where you can process or transform the parameters or resource Properties by using function Expressions you can manipulate input values making your template more Dynamic and adaptable functions within the arm template you can Define user specific functions this allows for reusable custom logic reducing redundancy and simplifying the template resources here you list out all the Azure resources you intend to deploy update it defines what your infrastructure looks like and how each component is configured outputs after a successful deployment you might want to retrieve specific values or results the output section is where you define these values be at the IP address of a newly created VM or the URL of a web app overall an arm template skeleton provides a structured and consistent approach to Define deploy and manage Azure resources moving forward let's just discuss one of the pivotal components of the arm template the resources resource this represents any Azure component or service you wish to provision it could be a virtual machine a database or a network interface Etc type this defines the kind of resource you're provisioning this typically follows the format of resource provider /resource type for instance if you're looking to create a storage account you'd use the type microsoft. storage SL storage accounts API version each resource type corresponds to an API version which is essentially the version of the rest API used for that particular resource it's important to note that each resource provider published has its own API versions so you need to ensure you're using the correct one for your chosen resource name this attribute specifies the unique name of the resource for example if you're setting up a virtual machine this could be my virtual machine location this is a common attribute for most resources it determines the Azure region where your resource will be deployed such as East us or west Europe other property beyond the basic attributes each resource type has its own set of properties that allow for deeper configuration these properties can very widely depending on the resource for a virtual machine it could be the size or the operating system for a database it might be the capacity or replication settings in this segment we'll delve into a fundamental component of arm templates the parameters parameters play a critical role in arm templates they allow you to pass specific values into your template thus allowing you to create more flexible and dynamic infrastructure configurations defining a parameter as shown in the example to define a parameter named storage name it's a tight string with a minimum length of five characters and a maximum length of 20 characters once you've defined a parameter you can then reference it in various parts of your template such as type API version name and so forth type dictates the expected data type for the input value common types include string secur string in bu object secure object and array default value if no value is provided it will be set to default value allowed values this is an array of allowed values Min value the minimal possible value max value the maximum possible value Min length the maximum length of characters or array max length the maximum length of characters or array description the description that will be displayed to the in the Azure portal in summary parameters are the The Gatekeepers of customization and arm templates the next topic we'll be covering are the arm template functions functions and arm templates are powerful tools that allow you to transform and manipulate your arm variables think of them as the building blocks that enable you to create more Dynamic and flexible configurations template functions these are builtin functions provided by Azure for a wide range of common tasks user defined functions these are custom functions you can create to cater to specific needs that aren't addressed by the builtin template functions functions are called using parentheses e such as the example shown here categories of template functions array functions tools for handling arrays some of these include array concat contains create array empty first Etc comparison functions for equating or contrasting values coess equals less lesser equals greater and greater or equals date functions to manipulate and time date time ad UTC now deployment functions pertaining to the deployment itself deployment environment parameters and variables logical functions for logical operations and or if not numeric functions mathematical and numeric operations add copy index div float int min max Etc object functions for object manipulation contains empty intersection Json length and Union resource functions related to Azure resources extension resource ID list account SAS list Keys list Secrets Etc string functions for string manipulation and evaluation base 64 Bas 64 to Json base 64 to string concat contains Etc we won't go over all of them but this is just to show you that there are a lot of functions available to you the next topic we'll cover are the arm template variables template variables are used to simplify your arm templates you transform parameters and resource properties using functions and then assign them into a reusable variable in this example the storage name variable is computed by combining a parameter named storage name prefix with a unique string derived from the resource groups ID to call a variable you use the variable function as shown in this example sometimes your templates might become more intricate and you may need a hierarchical structure to your variables that's where nested variable come in you can use JSO and object to have nested variables to scope your variables for multiple use cases scoping nesting variables based on environment consider scenarios where you have configurations That Vary based on the environment like test or prod you might want to neatly encapsulate variables specific to each environment within a JSO n object as shown in the example you can use parameters to choose the environment and then reference nested variables as followed in the example variables parentheses square brackets dotproperty overall variables and R templates are powerful tools that can simplify your template make it more adaptable and improve its maintainability the arm templates are just about defining and provisioning resources they also offer a way to fetch information about the deploy resources this is where the output section of your arm templates comes into play outputs returns values from deployed resources so you can use them programmatically for example you might want to know the static IP of a created VM or the connection string of a deployed database you specify the type and value under outputs here the output name resourc it is capturing the ID of a public IP address resource it's noteworthy that the type is explicitly mentioned ensuring type safety once your resources are deployed these outputs can be fetched using Azure CLI po shell or the Azure sdks for instance with Azure CLI this command retrieves the resourced output value from a specific deployment in a resource Group so that's a quick overview of outputs and arm templates hey this is Andrew Brown from exam Pro and we are looking at Azure resource manager templates also known as arm templates and this helps you uh deliver infrastructure as code meaning that when you have a resource such as a virtual machine or a storage account instead of manually configuring it every single time through um uh the portal what you can do is provide a configuration file that defines finds all the properties that you want it to be configured with and the idea is that you can uh keep this file and share with other uh other people so they can easily uh create the same resources as you and then you know exactly how your stuff is is configured so what we're going to do is uh launch a new template now you can't go up here and just type in arm because these arm templates are managed uh at different levels so at one level is subscription or the resource groups so when you have a resource Group you have deployments within them and that's where uh these templates are deployed uh but just to deploy one from here what we're going to do is type in deploy why they didn't make it so you can type an arm I do not know but if you go down here we have deploy a custom template and so from here we have some common templates so if I click into web app and I go edit a template we already have some stuff prefilled in I'm just going to go back and discard that go back to select a template and we're going to build our own and by default we'll have that schema that content version which is 1.0.0 point0 our parameters and our resources so today I want to launch a virtual machine and uh what you normally would have to do is go here uh and look up what is that you want to create so if it's this uh Microsoft compute virtual machine you'd go through here and you'd have to make sure you have all these uh properties so you define the resource here right the type um and then you define the properties that you want and down below you can go through here and see them all that's a lot of work I don't want to do that so I'll go to add resource here drop this down and click uh where is it virtual machine where are you there you are and I'm going to call this one Warf and Warf and Warf because it's not just going to create a virtual machine it's going to create other things uh that I need with it as well such as the storage account the network interface and the virtual Network so you can see that we have a bunch of parameters here so the name the type the name the admin username the password and the OS version oh you know what I think I chose a window Windows one I do not want a Windows one I want a Linux one because that is easier for me to work with here so we choose ubun 2 so I'll just fill this in again all right and so um back up here you know we have the Ubuntu version between some versions here and then there's the type so that's for uh replication then we have variables here so if we go to VM size this is the VM it will it will set here uh variables are either you can have string values or you can use function to transform other parameters into other stuff that you'll reference throughout your template then down below we have those resources here so what we'll do is um actually I'm going to copy this because it's it's very highly likely we're going to want to make some kind of change and so I have vs code over here on the on the left right hand side I'm just going to paste that on in there um and what we will do this is a Json file make things a little bit easier here great and what I'll do is just move that off screen and we'll go ahead and we will save this and we'll see if we can deploy this so I'm going to type in Warf here and we'll launch in Canada east I'll name this Warf we will name the username Warf but lowercase and then we'll do testing 1 2 3 4 5 6 capital on the T notice that it is uh hidden there and then we will choose 14 which is defaulted here and lrs we'll go ahead and do review and create and we'll hit create here so this is going to fail I already know because it has a misconfiguration it'll tell us how but while that's going we'll take a look at our input so this is the values that were inputed these are the outputs if we had defined any which we have not um and if we go back to our template I just wanted to show you that we have that secure string so when we were typing our password that's why we didn't see it um so just things like that so I'll go back up here and our deploy failed why what happened so we open it up here the requested VM size standard D1 is not available in the current region so the template we have is not that great um it it needs some configuration because we can't use D1 I think that doesn't exist anymore and so what we really want to use is the standard B1 LS all right standard B1 LS so I'm going to cut that and for the time being I'm going to go back to our original template and this is one big template I'm going to look for those variables oh they're all the way at the bottom here nice and so I'm going to just go ahead and paste that in B1 LS just double check making sure I spelled that right standard uh standard B1 LS looks good to me so I'm going to move that off screen and the question is what do we do what do we do when a deploy fails so let's go take a look at what has happened here so this all got deployed into a resource Group and under here this is where our deployments are so when we look at this template it we can see that it failed we can click into here get the same information um and if we click into here it just brings us back to where we just were but if we go look at what was actually deployed uh under our Resource Group under the overview we'll notice that it created the virtual Network the storage account and the network interface so when it fails it creates what it can but it doesn't roll back okay so the question is is then how do you do cleanup so you might think I'll go to deployments and what I'll do is go ahead and delete that template and we can go ahead and do that which by the way you can't edit this template all you can do is um all we can do here see I just want to show you that you cannot edit it we can download it and stuff like that but uh so you might think well if I go ahead and delete that template just making sure we're in the right place here you might think that might roll back those resources but it doesn't it just deletes the template so if you really want to get rid of this stuff what you got to do is go ahead and delete all these resources manually so um I wish it kind of had a roll back feature but that's just how it is but there are some nice things that uh Azure does here which we'll talk about in a moment so I think we have adjusted it to the correct value now so hopefully this is going to be all we need to make it work so what we'll do is go to our deployments here uh and we can't do it here but so we'll go back to the top here and type in deploy and we'll go to custom template and what we'll do is build our own template in our editor and I'm just going to copy the contents here okay we'll go copy and I will go paste and we'll make sure that this is all good looks fine to me we'll go ahead and hit save and we will choose Warf so we don't have to make a new one and we will fill in uh the name as Warf username is Warf I'll call Warf 2 just in case helps us keep track of what we're doing here testing 1 2 3 4 5 6 with a capital on the T 14 L RLS lrs and we'll go ahead oh we have one issue here cannot deploy Resource Group worth deleting uh we'll go back and we will hit create here I don't think I deleted the resource Group let me just go double check I almost I'm almost certain I deleted all the contents of it right oh so there's already one here so we're just waiting for that to delete just going to go delete for us please thank you it failed to delete we'll go take a look as to why resources not found mhm uh we'll go back to our resource groups give this a refresh here okay so you know what I must have deleted the resource Group which is totally fine I I could have SW I only delete the contents of it but we'll just call this war regular then we'll go ahead and hit uh great here and so this time I have better feeling about this and so we will just have to wait a little bit it won't take too long I'll see you back in a moment okay so after waiting a little bit here our uh our thing seems to be deployed so if we go to Resource groups we can see that our virtual machine is deployed so uh that's pretty much all there is to it one other thing I'd like to show you is that whatever you have whatever is in your resource Group you can actually export the template so uh if you did configure something manually all you'd have to do is find the resource go up here to well um it is export template and there's your template and it just has that single resource in there I can't remember if if I go into here if I select multiples um I go X or where is it export template look it's going to include all that stuff so if you already have existing resources that you provisioned and you want to have them that's what you can do notice that some things won't be included in the template when you do that but you can just go ahead and download them and then you have them for later so yeah that's all there really is to um arm other than learning the uh the nitty gritties of the actual language that's just how you work with it there so what I'm going to do is make my way over to my Resource Group group here and I'm just going to go ahead and delete this here and we're all good to go hey this is Andrew Brown from exam Pro and in this section we'll be covering Azure monitor so Azure monitor is a comprehensive solution for collecting analyzing and acting on Telemetry from your cloud and on premises environments it serves as is the backbone for gaining insight into the performance and health of your applications infrastructure and even the network he features visual dashboards A visual representation of your data smart alerts intelligent notifications based on specific conditions automated actions set automation based on certain triggers log monitoring track and analyze event logs many Azure Services by default are already sending Telemetry data to Azure monitor what is observability it's the ability to to measure and understand how internal systems work in order to answer questions regarding performance tolerance security and faults with a system or application to obtain observability you need to use metrics logs and traces you have to use them together using them in isolation does not gain you observability metrics a number that is measured over a period of time for example if we measured the CPU usage and aggregated it over a period of time we could have an average CPU metric plugs a text file where each line contains event data about what happened at a certain time traces a history of request that travels through multiple apps or services so we can pinpoint performance or failure looks like they should have called it the Triforce of observability the sources of common monitoring data to populate data stores order by highest to lowest application operating system Azure resources Azure subscription Azure tenant custom sources the two fundamental data stores are metrics and logs Azure monitor monor functionalities insights this can be for applications containers VMS or other Monitoring Solutions visualize using dashboards views powerbi and workbooks you can create Rich visual presentations of your data Analyze This involves delving deep into metrics analytics and log analytics respond Based on data Azure monitor can alert you or even autoscale resources integrate extend the capabilities by using logic apps or export APS for more flexibility overall as your monitor is a comprehensive solution vital for ensuring that your applications and services run optimally and any issues are detected and dealt with properly the next topic we'll be covering are the various sources from which azzure monitor collects data application code Azure monitor application insights offers robust metrics about the performance and functionality of your applications and code you'll get performance traces application logs and even user Telemetry you'll need to install instrumentation package in your application to collect data for application insights availability tests measure your application's responsiveness from different locations on the public internet this helps in assessing the reliability and uptime of your services nrixs descriptive data regarding your application's performance operation and custom metrics log store operational data about your application including page views application requests exceptions and traces you can send application data to azure storage for archiving view the details of availability tests stored and debug snapshot data that is captured for a subset of exceptions is stored in Azure storage log analytics agent is installed for comprehensive monitoring dependency agent collects discovered data about processes running on the virtual machine and external process dependencies agents can be installed on the OS for VMS running in Azure on premises or other Cloud providers Diagnostics extension collect performance counters and store them in metrics application insights logs collect logs and performance counters from the compute resources supporting your application allowing them to be analyzed alongside other application data the Azure Diagnostics extension always writes to an Azure storage account while Azure monitor for VMS uses the log analytics agent to store Health State information in a custom location the Diagnostics extension can also stream data to other locations using a vet hubs resource logs provide insights into the internal operation of an Azure resource and are autom automatically created however you must create a diagnostic setting to specify a destination for each resource platform metrics will write to the Azure monitor metrics database with no configuration you can access platform metrics from metrics Explorer for trending and other analyzes use log analytics copy platform metrics to logs send resource logs to Azure storage for archiving stream metrics to other locations using event hubs Azure subscription this includes Telemetry related to to the health and operation of your Azure subscription Azure service Health provides information about the health of the Azure services in your subscription that your application and resources rely on Telemetry related to your Azure tenant is collected from tenant wide services such as Azure active directory Azure active directory reporting contains the history of sign and activity and audit trail of changes made within a particular tenant for resources that cannot be monitored using the other data sources write this data to either metrics or logs using an azure monitor API this will allow you to collect log data from any rest client and store it in log analytics in the Azure monitor metrics database Azure monitor is integral to maintaining the health and performance of your applications and resources collecting two fundamental types of data logs and metrics Azure monitor logs collects and organizes log in performance data from a variety of monitored resources data consolidation logs can be pulled from diverse sources such as platform logs from Azure services log and performance data from Agents on Virtual machines and usage and performance data from applications workspaces all these logs are organized into workspaces providing a centralized repository for indepth analysis query language Azure monitor logs offers a sophisticated query language which can quickly analyze millions of Records making it an ideal choice for complex data analytics log analytics you can interactively work with log queries and their results using Azure log Analytics tool in contrast Azure monitor metrics collects numeric data and organizes it into a Time series database here's why that's important numeric data metrics are numerical values captured at regular intervals they are a snapshot that describes a particular aspect of a system at a specific Moment In Time lightweight metrics are designed to be lightweight allowing for near realtime data analysis this makes them particularly useful for alerting and the rapid detection of issues metrics Explorer the metrics Explorer tool allows for interactive analysis of metric data providing a more immediate understanding of your system's performance and health the next topic we'll cover are the data retention and archive policies of azure monitor logs this is an important aspect of your monitoring strategy as it allows you to control how long your data remains stored and accessible by default in the Azure portal you can set this retention time anywhere from 30 to 730 days for the whole workspace if you want you can also specify different storage durations for certain tables within your workspace letting you manage different types of data as needed this gives you the flexibility to meet any business or regulatory rules about data storage however note that to tweak these retention settings you have to be on the paage tier of azure monitor logs to set retention and archive policy by table why navigate to the Azure portal and go to the log analytics workspace where the data is stored to under the setting section select usage and estimated cost three then select data retention four in the data retention blade you can modify the retention period for each table by default it is set to 31 days but you can extend it up to 730 days five for archiving data you can use Azure data Explorer which lets you retain data beyond the 2year limit and gives you a highly scalable analytic service so that's an overview of the data retention and archive policies of azure monitor logs you'll most likely encounter a question related to this on the exam so be sure to know this hey this is Andrew Brown from exam Pro and in this section we'll be covering Azure log analytics so log analytics is a tool in the Azure portal used to edit and run log queries with data in Azure monitor logs log analytics processes data from various sources and transforms it into actionable insights it ingests data from Azure monitor windows and Linux agents Azure services and other sources once the data is collected you can use log analytics query language to retrieve consolidate and analyze the data log analytics uses a query language called kql now we'll go over some of the benefits of log analytics centralized log management collect and analyze data from multiple sources both on premises and in the cloud in a centralized location powerful analytics utilize the custo query language to run Advanced analytics on large amounts of fast streaming data in real time custom dashboards create custom dashboards and visualizations to display realtime data and Trends integration seamless integration with other Azure services and Microsoft Solutions such as powerbi and Azure Automation and alerting set up alerts based on specific criteria to proactively identify and respond to potential issues before they affect your users log analytics workspace is a unique environment for Azure monitor log data each workspace has its own data repository and configuration and data source and solutions are configured to store their data in a particular workspace so that's an overview of azure log analytics the log analytics agent is a lightweight agent that can be installed on Windows and Linux machines to collect and send log data to Azure monitor it provides a way to centralize logs from various sources and enables the analysis of the data using tools like Azure monitor logs Azure dashboards and Azure monitor workbooks the agent can collect logs from various sources including Windows event logs custom logs performance counters and CIS log it supports both agentbased and agentless data collection and can be configured to collect data from on premises and cloudbased environments the log analytics agent is set up to monitor certain Windows event logs like security system or application logs the data from these logs is then gathered and sent to log analytics for analysis using queries and visualizations the log analytics agent is set up to monitor CIS log servers or network devices it collects data from these sources and sends it to log analytics allowing for detailed analysis and troubleshooting both methods for collecting log data allow for centralized management and Analysis of log data from multiple sources which can help to improve visibility and streamline troubleshooting and issue resolution you can expect to see a question related to log analytics agents and choosing either Windows event logs for a Windows agent or CIS log for Linux agent on the exam the next topic will be covering our application insights application insights is an application Performance Management Service and it's a subservice of azure monitor APM is all about the monitoring and management of performance and availability of software apps it strives to detect and diagnose complex application performance problems to maintain an expected level of service so yuse application insights automatic detection of performance anomalies application insights automatically identifies performance anomalies in your system powerful analytics tools it comes with robust analytics tools to help you diagnose issues and understand what users do with your app continuous Improvement it is designed to help you continuously improve performance and usability of your applications platform agnostic it works for apps on net node.js Java and python hosted on premises hybrid or any public Cloud devops integration it can be integrated into your devops process and mobile app monitoring it can monitor and analyze Telemetry from mobile apps by integrating with visual studio app center to use application insights you need to instrument your application this involves installing the instrument package or enabling application insights using the application insights agents were supported there are many ways to view your Telemetry data apps can be instrumented from anywhere when you set up application insights monitoring for your web app you create an application insights resource in Microsoft azure you open this resource in the azzure portal in order to see and analyze the Telemetry collected from your app the resource is identified by an instrumentation key what does application insights monitor request rates response times and failure rates dependency rates response times and failure rates exceptions page views and low performance ax calls user and session counts performance counters post Diagnostics diagnostic Trace logs and custom events and metrics where do I see my Telemetry smart detection and manual alerts application map profiler usage analysis diagnostic search for instance data metrics Explorer for aurad data dashboards live stream metrics analytics Visual Studio ET overall application insights is a comprehensive APM service that offers automatic detection of performance anomalies powerful analytics tools and is designed to help you continuously improve performance and usability in this segment we'll delve into the topic of application insites instrumentation so what is instrumentation in simple terms it's a way to make your application smarter by adding a few lines of code or in some cases none at all you can monitor how your app performs and where it might be running into issues you instrument your application by adding the Azure application insights SDK and implementing traces in the case of a node.js application you can install the Azure application insights SDK using npm with the following command npm install application insights hyphen save application insights this is the name of the package you are installing which is azure SDK for application insights hyphen save this flag saves the package as a dependency in your package.json n file here this piece of code lets you configure what you want to collect Azure supports the following languages net Java python node.js JavaScript Auto instrumentation allows you to enable application monitoring with application insights without changing your code this table shows which Azure Services support application insights and in what programming languages the services range from Azure app service on Windows and Linux to Azure functions Azure spring Cloud Azure kubernetes service and more GA General availability meaning it's fully supported and ready to use public preview still being tested but you can use it not supported you can't use application sites here through agent you need to install a special piece of software to use this service o NBD on by default meaning the feature is automatically enabled through extension available but needs an extension to work we won't go through the entire table but we'll give a few examples for applications written in.net and hosted on Azure app service on Windows application insights is generally available and enabled by default for applications written in Python and hosted on Azure functions application insights is available and enabled by default but for dependencies monitoring you will need to use an extension so that's an overview of application insights instrumentation hey this is Andrew Brown from exam proo and in this section we'll be covering Microsoft Sentinel formerly known as Azure Sentinel Microsoft Sentinel is a scalable Cloud native solution that encompasses two key functionalities security information event management this is all about collecting and analyzing security related data to provide realtime analysis of security alerts generated by applications and network Hardware security orchestration automated response this refers to the collection of tools that enable an organization to Define standardize measure and automate responses to security events Microsoft Sentinel delivers intelligent security analytics and threat intelligence across the Enterprise providing a single solution for alert detection threat visibility proactive hunting and threat response with Microsoft Sentinel you can collect data at Cloud scale across all users devices applications and infrastructure both on premises and in multiple clouds detect previously undetected threats and minimize false positives using Microsoft's analytics and unparalleled threat intelligence investigate threats with artificial intelligence and hunt for suspicious activities at scale tapping into years of cyber security work at Microsoft respond to incidents rapidly with builtin orchestration and automation of common tasks Microsoft Sentinel comes with a number of connectors for Microsoft Solutions such as Microsoft 365 Defender Office 365 Azure a or Microsoft enter ID Microsoft Defender for identity and Microsoft Defender for cloud apps you can use common event formats CIS logit rest API Windows event logs common event format and trusted automated the exchange of indicator information one notable feature of Microsoft Sentinel is the ability to create Azure monitor workbooks workbooks provide a flexible canvas for data analysis and the creation of Rich visual reports within the Azure portal they allow you to tap into multiple data sources from across Azure and combine them into unified interactive experiences it tells a story about the performance and availability about your applications and services workbooks are temporary workspaces to define a document like format with visualization intertwined to help investig and discuss performance Microsoft Sentinel uses analytics to correlate alerts into incidents incidents are groups of related alerts that together create an actionable possible threat that you can investigate and resolve Microsoft Sentinels Automation and orchestration solution provides a highly extensible architecture that enables scalable automation as new technologies and threats emerge built on the foundation of azure logic apps includes 200 plus connectors for services my Microsoft Sentinel also offers deep investigation tools that help you to understand the scope and find the root cause of a potential security threat you can choose an entity on the interactive graph to ask interesting questions for a specific entity and drill down into that entity and its connections to get to the root cause of the threat additionally Microsoft Sentinels powerful hunting search and query tools based on the miter framework enable you to proactively hunt for security threats across your organization's data sources before an alert is triggered after you discover which hunting query provides high value insights into possible attacks you can also create custom detection rules based on your query and surface those insights as alerts to your security incident responders while hunting you can create bookmarks for interesting events enabling you to return to them later share them with others and group them with other correlating events to create a compelling incident for investigation lastly let's talk about pricing Microsoft Sentinel has two different pricing models capacity reservations this involves being build a fix fee base on the selected tier enabling a predictable total cost for Microsoft Sentinel pay as you go with this option Bill per gigabyte for the volume of data ingested for analysis in Microsoft Sentinel and stored in the Azure monitor log analytics workspace and there you have it a comprehensive look at Microsoft Sentinel a robust seam and Source solution that can help protect your organization's infrastructure applications and data hey this this is Andrew Brown from exam Pro and in this section we'll be covering the identity management best practices we'll start things off by discussing the principle of lease privilege a term you may have come across before so the principle of lease privilege is a security principle that states that users applications and services should be granted only the minimum access necessary to perform their assigned tasks and no more in Microsoft Azure the principle of lease privilege is a critical aspect of security that helps prevent unauthorized access data breaches and other security incidents the principle of lease privilege in Azure involves limiting access to Azure resources such as virtual machines storage accounts and databases as well as Azure services such as Azure active directory and Azure key Vault Azure robased Access Control permits administrators to Grant roles to users groups and apps based on their access levels following the principle of lease privilege helps minimize unauthorized access risks and potential harm from compromised credentials thus reducing the Azure environment's attack surface following the principle of lease privilege can help organizations comply with regulatory requirements and best practices for security to implement the principle of lease privilege in Azure administrators should follow these best practices assign roles based on the leaste amount of privilege needed to perform the task monitor role assignments and permissions regularly to ensure they align with business requirements limit the use of shared accounts and use individual user accounts where possible Implement multiactor authentication to prevent unauthorized access access to user accounts use Azure policy to enforce compliance with organizational policies and Industry regulations Implement network security groups and firewalls to control traffic to in from aure resources and regularly review Access Control policies and adjust as necessary to ensure that they remain effective so that's an overview of the principle of lease privilege the next topic will cover our privileged identity management or Pim for short within Azure D now why should you care about Pim because it's like a VIP lounge for your most sensitive resources controlling who gets in when and what they can do once they're inside so privileged identity management is an Azure aervice enabling you to manage control and monitor access to important resources in your organization you can manage resources from Azure ad Azure Microsoft 365 Microsoft and two and more so what are the key features of P Im Just in Time access this feature allows you to Grant privileged access to Azure ad and Azure resources only when needed assign time bound access to resources using start and end dates required approval to activate privileged roles enforce multiactor authentication to activate any role use justification to understand why users activate get notifications when privileged roles are activated conduct access reviews to ensure users still need roles and download audit history for internal or external audit it's important to note that Pim is part of azurea D premium 2o so you'll need that subscription to access these features by taking advantage of Pim and its features you can ensure a more secure and controlled environment for your organization next we'll be diving into another critical topic Azure ad identity protection so what does it do identity protection is a feature of azuread that enables you to detect investigate remediate and Export identity based risks for future analysis Microsoft analyzes a staggering 6.5 trillion signals per day to identify and protect customers from threats identity protection can notice risky users risky signins and risk detections let's take a closer look at the types of risks that identity protection can identify Anonymous IP address this is a sign in from an anonymous IP address like those used by the tour browser or anonymizer vpns a typical travel this refers to a sign in from a location that is atypical based on the users's recent signins malware linked IP address a sign in from an IP address linked to malware unfamiliar signin properties a sign in with properties that haven't been seen recently for the given user leak credentials indicates that the user's valid credentials have been leaked password spray this involves multiple usernames being attacked using common passwords in a unified Brute Force manner Azure a de threat intelligence this is when Microsoft's internal and external threat threat intelligence sources have identified a known attack pattern and there are others detected by Microsoft Defender for cloud apps such as new country activity activity from Anonymous IP addresses and suspicious inbox forwarding the risk signals can trigger remediation efforts such as requiring users to use azuread multifactor authentication reset their password using selfservice password reset or even blocking until an administrator takes action identity protection categorizes risk into three tiers low me medium and high and administrators can use key reports for investigations such as risky users risky signins and risk detections reports in the risky users report you'll find comprehensive details about detected risks a complete history of all risky signins as well as the users overall risk history to give you a full picture of security concerns in the risky signin report you'll see signin categorized as at risk confirmed compromised confirmed safe dismissed or remediate this report provides both real time and aggregate risk levels associated with each sign and attempt it also includes the types of risk detections triggered the conditional access policies applied multiactor authentication details and information about the device application and location involved in the signin risk detections offers filterable data covering up to the past 90 days it provides detailed information about each type of risk detected as well as other risks that were triggered simultaneously in the locations of the sign and attempts had been followup actions admins can take various actions like resetting the user password confirming a user compromise dismissing user risk blocking user side ends and investigate further using Azure app so with Azure ad identity protection you're not just identifying risks but you're also given powerful tools to act on them hey this is Andrew Brown from exam Pro in this segment we'll delve deep into Azure key vault pivotal tool to ensure the security of your Cloud applications and services Azure keyal helps you Safeguard cryptographic keys and other Secrets used by Cloud apps and services Azure key Vault focuses on three things certificate management this feature allows for easy provision management and deployment of both public and private SSL certificates these certificates can be used with Azure and internally connected resources Key Management this enables the creation and control of encryption Keys used to encrypt your data secrets manag here you have a secure space to store and tightly control access to tokens passwords certificates API keys and other Secrets note that certificates contain a key pair which is a combination of a key and a secret this should not be confused with key management and secrets management which are distinct functionalities moving forward let's talk about hsms or Hardware security modules these are dedicated Hardware devices specifically designed to securely store encryption Keys when it comes to adhering to standard standards we reference the federal information processing standard or fips this is a guideline recognized by the US and Canadian governments that specifies the security requirements for cryptographic modules that protect sensitive information in line with fips we have two levels of compliance for hsms fips 104 diminus 2 level two compliant this compliance level is for multitenant hsms where multiple customers are virtually isolated on a single HSM fips 104 diminus 2 level three compliant this level on the other hand pertains to single tenant hsms where One customer utilizes a dedicated HSM in essence Azure key vault is an indispensable tool for ensuring that your cloud data remains both accessible and secure whether you're working with certificates encryption keys or various Secrets Azure key Vault has you covered all right let's dive into the core of azure key Vault the Vault itself a vault is where your secrets and keys reside safeguarded either by software or by hsms validated to the standards of fips 1004 diminus 2 level two Azure key vaults provides two types of containers vaults these containers support both software and HSM back Keys HSM pools these are specialized containers solely for HSM back keys to activate your HSM you will need to provide a minimum of three RSA key pairs up to a maximum of 10 and specify the minimum number of keys required to decrypt the security domain called a quorum you do not choose the container on creation you just choose between standard and premium when you choose premium and create enough RSA key pairs you will begin to use HSM pools diving a bit into technicalities Azure key Vault rest API is used for programmatically managing Azure key Vault resources allowing you to perform operations such as create a key or secret import a key or secret revoke a key or secret delete a key or secret authorize user or apps to access its keys or secrets and monitor and manage key usage Azure key Vault rest API supports three different types of authentication managed identities and identity managed by Azure ad recommended is best practice service principle and certificate this method uses a certificate for authentication Service principle and secret a combination of a user identity and a secret key one feature to note is the soft delete functionality soft delete allows you to recover or permanently delete a key vault in secrets for the duration of the retention period this feature is enabled by default on cre mandatory retention period prevents the permanent deletion of key vaults or Secrets prior to the retention period elapsing furthermore enabling Purge protection safeguards your secrets from being prematurely purged either by users or by Microsoft bolstering the security of your Vault next up on our agenda is breaking down the pricing of azure key Vault knowing how your bill for this service can help you make informed decisions and optimize your costs Azure key Vault offers two pricing tiers standard and premium the notable distinction between the two is that while both tiers support software protected Keys only the premium tier allows for HSM protected Keys here's a closer look at the pricing tiers first 250 Keys regardless of whether you're on the standard or premium tier you'll be buillt $5 per key every month 25115 Keys the price drops to $2.50 per key monthly again consistent across both tiers 1501 to 4,000 Keys the cost further reduces to 90 cents for each key every month 4,1 plus keys for larger key volumes Beyond this point you'll be charged at a rate of 40 cents per key per month Secrets operations both tiers are priced at 3 cents for every 10,000 transactions involving Secrets certificate operations exclusive to the premium tier each certificate renewal request is buil at $3 managed Azure storage account key rotation this service only available in the premium tier is priced at $1 per renewal HSM protected Keys specifically for HSM protected Keys the pricing is further broken down based on the key types for RSA 2048 bit Keys the cost is $1 per key per month along with an additional charge of 3 cents per 10,000 transactions for RSA 3072 bit and 4096bit keys as well as ECC Keys the first 250 keys are priced at $5 per key per month so that's an overview of the pricing model for Azure key vault the next topic we'll be covering is double encryption for Azure key Vault before we dive in let's quickly recap infrastructure encryption for storage accounts by default Azure ensures that your storage account data is encrypted when it's at rest infrastructure encryption adds a second layer of encryption to your storage accounts data now let's jump into Azure diss double encryption double encryption is precisely what it sounds like it's where two or more independent layers of En encryption are enable to protect against compromises of any one layer of encryption this strategy ensures that even if one encryption layer is compromised the data remains protected by the other Microsoft has a twolayered approach both for data at rest and data in transit for data at rest disk encryption this is achieved using customer managed keys and infrastructure encryption this uses platform managed Keys strengthening the base layer and for data in Transit Transit encryption using transport layer Security 1 2 the Safeguard data as it travels through networks and an additional layer of encryption provided at the infrastructure layer so that's a quick overview of double encryption for Azure key vault in this section we'll go into detail on the keys in Azure key Vault when it comes to creating a key in Azure you have three primary choices generate Azure will generate the key for you import import an existing RSA key that you already possess and restore backup restore a key from backup for Keys generated by Azure you can use either RSA or EC RSA or rivest shamier or Adelman this supports key sizes of 2048 3072 and 4096 bits EC or elliptic curve cryptography here you can select from p256 P 384 p521 or p256 k for Keys generated by azer you can set an activation and expiration date additionally you're not bound to a static version of a key you can create new versions of keys you can also download backups of keys but remember that backups can only be restored within the same Azure subscription and within Azure key Vault when you have a premium Vault you'll key options for HSM you can generate either an RSA or EC specifically for HSM or import an RSA key for HSM as shown in the example now let's talk about Key Management types Microsoft managed key or Keys managed by Microsoft they do not appear in your Vault and in most cases are used by default for many Azure services customer managed key are Keys you create in Azure key Vault you need to select a key from EV Vault for various Services sometimes customer managed means that the customer has imported cryptographic material and any generated or imported keys are considered cmk and Azure in order to use a key an Azure service needs an identity established with an Azure adid for permission to access the key from The Vault Additionally you have the option to implement infrastructure encryption while while Azure already encrypt storage account data at Rest by default opting for infrastructure encryption adds a second layer of security fortifying your storage accounts data even further the next topic will be covering our secrets in Azure key Vault Azure key Vault Secrets provide Secure Storage of generic Secrets such as passwords and database connection strings key Vault AP as accept and return secret values as strains internally key Vault stores and man of Secrets as sequences of octets with each secret having a maximum size of 25k bytes the key Vault service doesn't provide semantics for Secrets it accepts the data encrypts it stores it and returns a secret identifier for highly sensitive data clients should consider additional layers of protection for data for example encrypting your data using a separate protection key before storing it in the key Vault keyal also supports a content type field for Secrets allowing clients to specify the content type of a secret to a assist in interpreting the secret data when it's retrieved note that the maximum length of this field is 255 characters every secret stored in your key vault is encrypted key vault encrypt Secrets at rest with a hierarchy of encryption Keys all keys in that hierarchy are protected by modules that are fips 14 minutes 2 compliant the encryption Leaf key is unique to each key Vault while the root key is unique to the entire security world the protection level may vary between regions for example chid uses fips 1004 dimin 2 level one and all other regions use level two or higher diving into secret attributes we have exp this is the expiration time after which the secret data should not be retrieved NBF not before default value is now this defines the time before which the secret data should not be retrieved enable this tells us whether the secret data can be retrieved or not with its default set to True additionally there are readon attributes for created an update in order to access Secrets within your applic a code you can would use the Azure SDK for example we have a net example in this image here another option is to use tools like Azure CLI so that about covers the important details of secrets in Azure key Vault the next topic will be covering our x59 certificates first let's unravel what public key infrastructure is pki is a set of rules policies Hardware software and procedures needed to to create manage distribute use store and revoke digital certificates and manage public key encryption so what exactly is an x59 certificate it is a standard defined by the international telecommunication Union for public key certifications X 59 certificates are used in many internet protocols including SSL TLS and https signed and encrypted email and code signing and document signing a certificate contains an identity which could be a host name an organization or an individual along with a public key built on platforms like RSA DSA or ecda but who issues these certificates here comes the role of the certificate Authority a trusted entity that issues digital certificates a c acts is a trusted third part trusted both by the subject of the certificate and by the party relying upon the certificate a certificate Authority can issue multiple certificates in the form of tree structure known as a chain of trust root certificate Authority this is a selfsigned certificate the private key associated with it signs other certificates it's important that the private key of root or protected intermediate certificate Authority these certificates are signed by the roots private key they protect the root certificate because the root certificate does not have to sign every issued certificate and entity certificate a certificate issued by the ICA used by the end entity the entity in the case is an SSL certificate for a website a certificate contains of metadata about version number the version of the x59 standard serial number a unique serial number assigned to the certificate by the certificate Authority signature algorithm ID the algorithm used to sign the certificate such as RSA or DSA issuer the name of the certificate Authority that issued this certificate validity period the start and end dates during which the certificate is valid subject the identifier for the individual or organization to whom the certificate was issued subject public key the public key that is authenticated by this certificate this field also names the algorithm used for public key generation ation ISS your unique identifier allows multiple Cas to operate as a single logical CL subject unique identifier allows multiple certificate holders to act as a single logical entity extensions allows a c to associate additional private information with a certificate all the metadata is publicly readable anyone can view it so that's an overview of x59 certificates hey this is Andrew Brown from exam Pro and in this fall along we're going to be learning all about Azure Vault so let's get to it so what I want you to do is go on the top here and type in key Vault and here we'll have to go ahead and create ourselves a new Vault and so from there we're going to create a new Resource Group I'm going to call this Resource Group my example Vault and then we will make a vault key here so I'll say My Vault example which is kind of funny because this one's slightly different so you've seen I've done this before so I'm I'm going to do my example vault as the name here and for the region Us East is fine for pricing we'll keep it at standard soft delete is enabled um and then there's the option for Purge protection so we are going to enable Purge protection and uh this is going to play into other follows we'll explain that as it goes but Purge protection does not allow you to uh Purge things uh easily once it's enabled so what we'll do is go ahead and and review and create and we'll go ahead and go review create and we'll give it a moment here and we'll just wait till it's done deploying okay all right so after a short little wait our vault is created and so what I want you to do is go to the resource and we're going to be using this Vault a little bit in some of the Fall alongs and in some cases is not so much okay hey this is Andrew Brown and this fall along we're going to be doing some things with uh keys with an Azure key Vault so what I want you to do is make your way to the Keys blade on the left hand side here we're going to generate or slimport a new key we're going to choose the generate option in terms of naming we're going to call this my disk key and we are going to choose RSA 2048 that seems totally fine to me everything else seems okay so we'll go ahead and create that key so we'll give it a moment to create doesn't take too long and then what we're going to do is go on the left hand side to I am access controls and what we're want we're going to want to do is add a new Ro assignment so we can go ahead and start using this uh key so what I want you to do is go and look for key Vault administrator which is here we'll go ahead and hit next and then for our uh user we will choose ourselves so under user I'm going to select the members I'm looking for the account I'm using there I am and your brown go ahead and select that there and so that is all we need to to assign it so that we can actually uh work with that key so I think a good idea is to use a key uh to encrypt a disk so what we'll do is make our way over to dis encryption sets because before you can encrypt a dis you need to have an encryption set so we'll go ahead and create ourselves a new encryption set we'll call we'll use the uh sorry the same um resource ccrypt so it's very easy clean up afterwards we'll call this my disk encrypt set here and in terms of the encryption type we're going to use double encryption because that's much better you have two keys that encrypt it so that's a lot better we are going to choose our vault so we have my example Vault there's only one option here and in terms of the key we'll select my dis key terms of the version uh we'll select the current version we'll go ahead and hit review create and then we will go and create that and we'll give give it a moment to create that encryption set shouldn't take too long here and after a short little wait uh our resource should be deployed it took about a minute for me and if we go here it's going to have this message up here it's very small but it says to associate disk image snapshot this dis encryption set you must Grant permissions to key Vault so all we have to do is click that uh alert and will grant permissions and so now we are able uh to use that key um or like to to we're going to have the permissions issues is solved so what we'll do is go to type and create a new disk and so we can apply this key to that encryption so we go ahead and create we're going to choose the same Resource Group here I'm going to call this my example Vault and um or sorry my example uh dis so that's a little bit more clear than that and for the availability Zone doesn't matter for the source type um it doesn't matter as well in terms of the size we want this to be cheap we're not really using this for real so we'll use standard HDD and we'll say okay in terms of encryption this is where things get fun we go to double encryption we choose our key here we'll go ahead review and create and we'll just give it a moment for that to well we'll hit create and we'll have to wait a little while here for that create that resource so we'll just wait until that is created okay and after a very short while the dis is ready so we'll go to that resource we'll go to the encryption tab to see that encryption is applied so that's all it takes to use a key to encrypt a dis so we are going to still use some of these accounts there's no clean up yet I'll go back here and I'll see you in the next one hey this is Andrew Brown and this follow along we're going to learn about backup and restore key so what I want you to do is go back into the uh Resource Group that we just recently created and we're going to make our way over to keys so I'm just or sorry we got to get into the Vault first then we'll go over to keys and the idea is that we have this key here and so um you can see that we have this current version so you can add additional versions but what's going to happen if we try to back this up so when you back this up you're going to get this file here and if you open up this file it's going to look like a bunch of gobbly goop so I'm just going to try to open it here um I have it up off screen here so I'm just trying to open it up within uh Visual Studio code so I'm just going to open up visual studio code again doing this off screen here just give me a moment all right and so this is the file um that we encrypted uh and you take a look here and it's it's doesn't look like anything but the idea is that it is our backup of our key so that we can rein ort that and just taking a look at the key name this is what it looks like so it says my example Vault my dis key then there's this um uh date and that's key backup so just recognize that's the format and the date is very useful to indicate when you backed it up so let's go ahead and delete this key because the idea is we want to uh restore that backup and so we have deleted that key there and uh what we're going to do is we're going to attempt a restore so I'm going to go ahead and go occurred while restoring the key the key you're trying to restore already exists why would it throw that error we've clearly deleted it and the reason why is that we have Purge protection on we did that in the um first first part when we set up this actual Vault here I'm going to just see if we can find the settings wherever that Purge protection is I'm trying to remember where it is Purge protection is enabled so we can go here and once you enable it you cannot turn it off it's going to retain it for a certain amount of days um and so all you can do is soft delete keys so this key is not actually deleted yet if you go to manage deleted Keys you can see the key is over here and if you try to click on Purge it is disabled because we cannot remove the key because we have Purge protection on but we can recover the key so we'll go ahead and recover uh and so that will allow us to recover the key and if we refresh here it's going to take a little bit time for that key to restore so we'll just have to uh wait a little bit and then it will show up here's one other thing I wanted to show you was under policy because you know um if you go under where's policies here um or access policies if you look under our user here and we look at the key permissions um there is an option to purge and we don't actually have that uh turned on right now but if we were to save this and we were to still go to that Purge option it would still say the same thing so even if you have Purge permissions it does not matter if Purge protections turned on it still will not let you purge but you would need a combination of those in order to uh you know be able to do things there so to really show you how to do that recovery I think what we should do I'm just going to delete our old key here because we don't care about it but we are going to well I guess we could try to import it into the other one so I'm just going to undo that for a second but we are going to go ahead and create ourselves another Vault so I'm going to go and type in Vault at the top here and we're going to be a little bit more careful when we create this Vault so we'll go here and we will choose um my example Vault I'm going to say My Vault no protect and the pricing tier will be standard one day we're going to leave it or well seven is the lowest and we'll say disable Purge protection because we don't want to have that enabled and we'll see if we can import the key into another Vault I'm not sure if we can do that worst case we'll make a new key download the key reupload it but I'm just curious what would happen if we tried to upload the same key as it's still in another Vault I'm not exactly sure all right so this deployment is successful I'm going to go to this resource I'm going to go ahead to go to create and we're going to restore from backup and we're going to take this key and see if we can actually import it here so it looks like we can take a key and it can exist in multiple vaults I'm going to go ahead and delete this key and we're going to say are you sure you want to delete this key I'm going to say yes and if we go to manage keys and we refresh it takes a a little bit of time here so we'll just wait a moment for this to uh persist and after a short little wait like about 2 minutes I refresh and the key is here so if I go here you'll notice the purges option is still not available we can obviously recover um but we don't have Purge um protection on so if we go to access policies over here and we'll go ahead and scroll down and select Purge and save our changes we can then go back to Keys we'll give it a moment to save we go back to Keys we'll refresh it we'll manage our keys and we'll go ahead and Purge it and that will permanently Purge it there so that's all it takes uh to do that so there you go hey this is Andrew Brown from exampro and in this section we'll be going over the integration with on premises Solutions with Azure starting with Azure adconnect Azure adconnect is a tool provided by Microsoft that enables organizations to synchronize on premises active directory with Azure active directory C this synchronization enables organizations to extend their on premises identities and security policies to the cloud and enable seamless access to cloudbased applications Azure a DEC connect allows for seamless single side on from your on premises workstation to Microsoft Azure Azure adconnect has the following features password hash synchronization this feature syncs user password hashes from on premises active directory to Azure ID enabling the same signing method for both pass through authentication this allows you users to maintain the same password across on premises and Cloud platforms without needing a separate Federated environment setup Federation integration an optional feature it facilitates a hybrid setup using on premises a DFS infrastructure and provides management tools like certificate renewal and server deployment synchronization this is responsible for creating and aligning users groups and other objects between on premises and Cloud ensuring identity information matches across both Health monitoring Azure adconnect Health offers robust activity monitoring with a dedicated azzure portal section to review this data here are the steps for installing configuring and synchronizing on premises active directory with azuread using Azure adconnect Why install Azure adconnect install Azure adconnect on a server connected to both on premises a and Azure a to configure Azure ad connect use the wizard to set up synchronization settings Source Target directories and sync frequency three synchronized directories aure a DEC connect syn on premises a with azzure a d replicating changes from source to Target for Monitor and manage continuously Monitor and manage the synchronization process to ensure accuracy and meet business needs overall Azure a DEC connect is your bridge between the on premises world and Azure it ensures a synchronized coherent and seamless experience the next topic we'll be covering is azure a application proxy Azure a application proxy is a service provided by Microsoft Azure that allows organizations to provide remote access to their on premises web applications it allows users to access the applications securely from anywhere using any device without the need for complex network configuration or exposing the applications directly to the internet organizations can publish their on premises applications to the cloud providing secure remote access for their users the service allows organizations to use their existing on premises infrastructure and application architecture leveraging the benefits of the cloud it provides Advanced security features such as multiactor authentication and conditional access policies ensuring that only authorized users can access the applications the azuread application proxy service consists of two main components while an Azure a d application proxy connector a lightweight agent that is installed on a server within the organizations's on premises environment the connector establishes a secure outbound connection to the azuread application proxy service which which enables communication between the on premises application and the azuread service to Azure ad application proxy service a cloudbased service that manages the authentication and authorization of users who access the on premises web applications through the application proxy connector it also routes traffic to the appropriate pen servers and enforces policies set by the organization next let's look at the azuread application proxy architecture here's an image of the process and we'll go through each step why an user access the user accesses the application and gets redirected to azuread for sign in any set conditional access policies are checked two token issuance after successful signin azuread sends a token to the user's device three token interpretation the client sends this token to application proxy which extracts the user principal name and security principal name for request forwarding application proxy forwards the request to the connector installed on premises five additional authentication optionally the connector May perform additional authentication and then sends the request to the on premises application six server response the application's response is sent back through the connector to the application proxy service seven response delivery finally the application proxy service delivers the response to the user's device next we'll go over some of the use cases for Azure a application proxy remote access Azure ad application proxy is commonly used to provide secure remote access to on premise applications employees working from home or other remote locations can securely access their internal applications just as if they were in the office single signon application proxy can integrate with azuread to provide single signon capabilities this allows users to authenticate once and then access multiple applications without needing to sign in again conditional access with application proxy you can leverage Azure ad's conditional access policies for your on premise applications this provides granular control over over access based on user location device status and other factors Legacy application modernization application proxy can help organizations expose Legacy on premise applications to the internet in a secure manner without changing the application code this can be a key part of a strategy to modernize Legacy applications scalability and performance Azure a d application proxy scales automatically to meet your organization's usage patterns and provides a global reach without needing to open additional firewall ports this can help help improve the performance and availability of your applications in conclusion Azure a application proxy is like a security guard in a bridge ensuring that your on premises applications are both accessible and protected be sure to know this as it'll definitely appear on the exam hey this is Andrew Brown from exam Pro and in this section we'll be going over storage accounts in Azure Azure storage offers several types of storage accounts each with different features and their own pricing models these storage account types include standard general purpose V1 which is now considered Legacy standard general purpose V2 blob storage block blob storage and file storage storage accounts vary with the following features supported Services essentially this answers the question what can I put in this storage account your options include blob file Q table disk and data Lake Gen 2 performance tears this focuses on the speed of your read and write operations Azure offers to tiers standard and premium access tiers how often do I need quick access to files the tiers are hot cool archive replication how many redundant copies should be made and where Azure provides various replication options including lrs GRS R GS zrs gzrs R gzrs deployment model Azure has two models resource manager and classic the table details different types of azure storage accounts their capabilities performance tiers access tiers replication methods and deployment models type this column describes the different types of storage accounts general purpose V2 this is the latest version of azure storage accounts and supports various services like blob file Q table disk and data Lake Gen 2 general purpose V1 this is the older version and supports blob file Q table and disk block blob storage designed for storing block blobs and aen blobs file storage specifically tailored for Azure file shares blob storage meant for storing block and the pen blobs service these are the storage Services each account type can provide performance tiers Azure offers two performance tiers standard back by hard disk drives and suitable for general purpose storage premium uses solid state drives and is optimized for high performance and low latency workloads access tiers these Define the data access frequency hot for frequently accessed data tool optimize for storing and frequently accessed data for a minimum of 30 days archive for rarely Access Data with a minimum of 180 days storage duration replication Azure provides multiple replication options for ensuring data durability and availability lrs locally redundant storage stores multiple copies of your data in a single data center GRS Geo redundant storage replicates your data to a secondary region raw GRS read access Geo redundant storage offers readon access to the data in the secondary location in addition to Geo replication zrs Zone redundant storage spreads data across multiple availability zones gzrs geozone redundant storage combines both zrs and GRS by spreading data across availability zones and replicating to a secondary region raw gzrs like gzrs but with read access to the secondary region deployment models resource manager this is azure's modern deployment model it allows you to group related resources together for easier management classic the older deployment model that existed before the the introduction of the Azure resource manager lastly let's explore the five core Services Azure provides in the storage domain Azure blob a massively scalable Object Store for text and binary data also includes support for big data analytics through data like storage Gen 2 Azure files manage file shares for cloud or on premises deployments Azure cues a messaging store for Reliable messaging between application components Azure tables a nsql store for schema Less storage of structured data Azure diss Block Level storage volumes for Azure VNS so that's an introduction to storage accounts in Azure the next topic we'll be exploring is azure blob storage in more detail blob storage is an object store that is optimized for storing massive amounts of unstructured data unstructured data is data that doesn't adhere to a particular data model or definition such as text or binary data Azure blobs are composed of the following core components storage account this is essentially your unique space or Nam space in Azure and it looks something like this HTTP colon doubleist account. blob. core. windows.net container this component functions similarly to a folder in a file system blobs here is where the actual data is stored Azure storage supports three types of blobs why block blobs these are ideal for storing text in binary data it's made up of blocks of data that can be maned individually and they can store up to about 4.75 tip of data two append blobs specially optimized for append operations these are ideal for scenarios such as logging data from virtual machine three page blobs capable of storing random access files up to 8 terabytes in size and they are suited for store virtual hard drive files and serve as discs for Azure virtual machines when it comes to transferring data into Azure blob storage there are multiple Methods at your disposal including as copy and easy easy to use command line tool for Windows and ltic Azure storage data movement Library a net library that uses as copy in the background Azure data Factory an ETL service by Azure blop pews this virtual file system driver allows for direct data access through the Linux file system Azure data box a robust physical device designed to transport data to Azure securely Azure import export service a service where you ship your physical discs for data transfer onto Azure so that's an overview of azure blob storage the next topic we'll be covering are the performance tiers in Blob storage in a bit more detail there are two types of performance tiers for storage accounts standard and premium before we delve into the details it's essential to understand the term iops iops stands for input output operations per second the higher the iops the faster a drive can read and write premium performance tier in this tier data is stored on solid state drives these drives are optimized for low latency operations ensuring higher throughput and speed data access some of the ideal use cases include interactive workloads analytics AI or machine learning processes and data transformation tasks and SSD has no moving parts and data is distributed randomly this is why it can read and write so fast on the other hand standard performance the standard tier stores data on hard disk drives these drives offer VAR Performance Based on the access tier such as hot cool or archive the standard tier shines in the following use cases backup and Disaster Recovery operations storing media content and bulk data processing an HDD as moving Parts an arm that needs to read and write data sequential to a disk it is very good at writing or reading large amounts of data that is close together overall your choice between premium and standard largely depends on your specific requirements whether you prioritize Lightning Fast data access or more budget friendly voluminous data storage moving on to the next topic we'll be covering access tiers for blob storage in more detail so there are three types of access tiers for standard storage cool hot and archive hot tier ideal for data that's access frequently it has the highest storage cost but you get the lowest access cost use cases data that's in active use or expected to be access frequently and data that staged for processing an eventual migration to the cool access tier cool here best for data that's in frequently accessed and stored for at least 30 days it has lower storage cost but higher access cost use case great for shortterm backup in Disaster Recovery data sets older media content not viewed frequently anymore but is expected to be available immediately when accessed and large data sets that need to be stored cost effectively while more data is being gathered for future processing archive tier best for data that's rarely accessed and stored for at least 180 days it has the lowest storage cost but the highest access cost use case best suited for longterm backup secondary backup and archival data sets original data that must be preserved even after it has been processed into final usable form and compliance and archival data that needs to be stored for a long time and is hardly ever accessed before we move on let's touch upon some essential technical aspects account level tiering any blob that doesn't have an explicitly assigned tier infers the tier from the storage account access tier setting blob level tiering you can upload a blob to the tier of your choice changing tiers happens instantly with the exception from moving out of archive rehydrating a blob when moving a blob out of archive into another tier it can take several hours this is known as rehydrating blob life cycle management here you can create rulebased policies to transition data to different tiers such as after 30 days move to Cool Storage when a blob is uploaded or moved to another tier it's charged at the new tier rate immediately upon tier change when moving from a cooler tier the operation is building is a right operation to the destination tier where the right operation and data right charges of the destination tier apply when moving from a hotter tier the operation is built as a read from the source tier where the read operation and data retrieval charges of the source tier apply early deletion charges for any blot moved out of the cool or archive tier May apply as well cool and archive early deletion any blob that is moved into the cool tier is subject to a cool early deletion period of 30 days any blob that is moved into the archive tier is subject to an archive early deletion per period of 180 days this charge is prated so that's a more indepth look into the access tiers for blob storage the next topic we'll be covering is the replication and data redundancy for storage accounts so when you create a storage account you need to choose a replication type replication stores multiple copies of your data so that it is protected from planned events transient Hardware failures Network or power outages or even massive of natural disasters primary region redundancy these include locally redundant storage and Zone redundant storage secondary region redundancy this includes Geo redundant storage and geozone redundant storage secondary region redundancy with re access re access Geo redundant storage and reais geozone redundant storage as you can expect the greater level of redundancy the more expensive the cost of replication for redundancy in the primary region data is replicated three times in the primary region there are two options for storing in the primary region locally redundant storage copies data synchronously in primary region 99.999999999% that's 11 9's durability this is the cheapest option so redundant storage copies data synchronously across 3 A's in primary region 99.9% 12 9's durability for redundancy in the secondary region data is replicated to a secondary region in case of primary Regional disaster the secondary region is determined based on your primary's pair region secondary region isn't available for read or right access Geo redundant storage copies data synchronously in primary region copies data asynchronously to another region 99.9% that's 16 NES of durability gozone redundant storage similar to GRS but adds synchronous replication across three availability zones in the primary region before a synchronously replicating to another region maintaining the same 99.9% that 1699 durability level redundancy in the secondary region with read access data is replicated synchronously to primary region your data will be in sync with your primary and you'll have red access read access Geo redundant storage ensures synchronous data replication within the primary region into another region offering a high durability of 99.9% with 169 read AIS geozone redundant storage this goes a step further by replicating data synchronously across three availability zones in the primary region before synchronous L replicating to another region maintaining the same high durability level so choosing the right replication strategy depends on your business needs weighing costs against data durability and accessibility and that about covers main points for the replication and data redundancy for storage accounts the next topic we'll be exploring is a z copy so what is a z copy a z copy is a versatile command line tool designed specifically for copying blobs or files to or from Azure storage accounts it's a goto utility for many when they think of data transfer with Azure while download first things first to get started with a z copy you'll need to download the executable file compatible with your operating system be it windows litx or Macos to before you begin transferring data ensure you have the necessary level of authorization you will need to have the level of authorization via attached roles for downloading you'll require the storage blob data reader Ro for uploading the rules necessary are storage blob data contributor and storage blob data owner three you gain access either via a z copy login options for authentication include using Azure active directory or a shared access signature known commonly as SAS this prompts you deci side in you'll then be guided to use a web browser open a specific page and enter a given code to authenticate for copying data to move data use the straightforward copy command a z copy copy whether you're uploading or downloading this command is your way to data transfer so that's a quick summary of how to utilize a z copy the next topic we'll be covering is the life cycle Management in Azure storage Azure storage life cycle management offers a rulebased policy that you can use to transition blob data to the appropriate access tiers or to expire data at the end of the data life cycle with the life cycle management policy you can transition blobs from cool to hot immediately when they are accessed to optimize for performance transition blobs blob versions and blob snapshots to a cooler storage tier if the objects have not been accessed or modified for a period of time to optimize for cost delete blobs blob versions and blob snapshots at the end of their life cycles Define rules to be run once per day at the storage account level and apply rules to Containers or to a subset of blobs using name prefixes or blob index tags as filters so to manage the life cycle of our blobs inside containers a life cycle management rule must be created navigate to your Azure storage account go to life cycle management find and select blob service and click on add a rule from here decide whether to May apply this rule to all blobs inside the storage account or filter the blobs to have this rule applied in this storage account for example if Bas blobs were last modified for more than one day ago then delete the blob for example if bass blobs were last modified from than one day ago then delete the blob if base blobs were last modified more than 2 days ago then then move to Cool Storage overall Azure storage life cycle management gives you automated tools to handle data efficiently as it progresses through its life cycle balancing between performance needs and cost considerations hey this is Andrew Brown from exam Pro and in this section we'll be covering the Azure SQL offerings let's break them down SQL server on Azure VMS provides SQL server and Os level access supports various SQL n OS versions this is an ideal choice for migrations and applications that need direct interaction with the operating system managed instances single instance gives SQL server and Os access for robust needs supports various SQL and Os versions best for isolated SQL workloads needing dedicated resources instance pool enables preprovisioning of resources for efficient migration host smaller costeffective instances this is a perfect fit if you're looking to migrate several smaller databases in b es suitable for batch migration of smaller databases databases single database offers hyperscale storage up to 100 terabytes features demandbased serverless compute best for cloud applications needing a fully managed service elastic pool enables resource sharing among databases simplifies Performance Management with multiple databases best for managing and scaling databases with variable usage patterns so that's an overview of the Azure SQL offerings whether you're migrating scaling or starting fresh Azure has a SQL solution tailored for your needs let's talk about Azure SQL databases in more detail Azure SQL database is a fully managed relational database service provided by Microsoft Azure it's a cloudbased database service that offers a high level of scalability availability and security Azure SQL database is based on the latest version of Microsoft SQL server and it's designed to handle various workloads ranging from small web applications to large Enterprise workloads hazure SQL database supports popular relational database engines such as SQL Server MySQL and postgressql and offers a variety of deployment options including single database and elastic pool Azure SQL database offers several benefits that make it a popular choice for businesses looking to migrate their on premises databases to the cloud some of the key benefits include fully managed service Azure handles administrative tasks like patching and backups freeing businesses to focus on core tasks High availability builtin automatic failover and Disaster Recovery capabilities ensure data access even during outages scalability Azure SQL database easily scales resources according to workload optimizing costs security Advanced features like threat protection and data encryption ensure secure data storage integration Azure SQL database integrates seamlessly with other Azure Services supporting the development of modern datadriven applications the Azure SQL database service offers various tiers to cater to a range of requirements and workloads here's a breakdown basic tier this is the most economical tier optimized for lighter database workloads its best for tasks like testing and development as well as for other noncritical workloads you can store data up to 2 gb and it provides 5 dtus which stands for database transaction units that offer a combined measure of compute storage and EO resources standard tier this tier is designed to handle the majority of database workloads it's ideal for business critical production workloads the storage capacity is up to 1 tbte and it offers a range of 10 to 4,000 dtus premium tier build for Mission critical databases this tier emphasizes High transactional rates it's particularly suitable for workloads with high volume transactions allows data storage up to 4 terabytes and it provides 125 to 20,000 dtus general purpose tier this tier is designed for customers with demanding database workloads fits best for moderate to heavy transactional workloads offers storage up to 4 terabytes equipped with 5 to 80v cores providing robust computational capacity hyperscale tier optimized for extremely large data volumes High transaction rates and great concurrency this tier is the best fit for large volume of ltp workloads provides a massive storage capacity of up to 100 terabytes provides a compute range of 4 to 160 V cores in summary Azure SQL database offers a comprehensive set of tiers that cater to everything from lightweight development tasks to high demand Mission critical applications ensuring that organizations can pick the perfect blend of cost performance and capacity for their needs the next topic will be covering our Azure SQL database elastic pools Azure SQL database elastic pools are a simple costeffective solution for managing and scaling multiple databases that have varying and unpredictable usage demands the databases in an elastic pool are on a single server and share a set number of resources at a set price the concept of elastic pools refers to a shared pool of resources such as CPU memory and storage allocated to a group of databases this shared set of resources can be automatically adjusted and distributed among the databases based on their varying demand Azure SQL database elastic pools is ideal for businesses with many databases experiencing varying workloads instead of allocating dedicated resources per database an elastic pool shares resources across databases for efficient use and cost reduction benefits of azure SQL database elastic pools coste effective share resources across databases pay only for what you use Performance Management resources are autom managed across a pool no manual adjustment required flexibility and scalability auto scales to meet demand handling traffic spikes smoothly simplified Administration easier management with shared resources across databases use cases for Azure SQL database elastic pools SAS providers manage varying customer database activity costs effectively development and test environments efficient less expensive process for regular database setup and teardown businesses with multiple apps optimize resource usage and cost by sharing resources among databases with varying activity so that's an overview of aure SQL database elastic pools next we'll be exploring Azure SQL managed instance in more detail Azure SQL managed instance is a fully managed database service offered by Microsoft Azure designed to provide an easy migration path for SQL Server workloads to Azure it provides a managed instance of SQL server in the cloud allowing you to run your existing applications with minimal changes it's built on top of the latest SQL server engine and supports all its features including complex queries and userdefined functions it offers various deployment options including Standalone automatic failover configurations and cross region replication for Disaster Recovery aure SQL managed instan is ideal for modernizing SQL Server workloads consolidating multiple SQL Server instances and building new Cloud native applications some of the main benefits of azure SQL managed instance include easy migration provides a Swift simple migration path for SQL Server workloads to Azure with minimal application changes fully managed Microsoft handles maintenance backups and updates allowing focus on applications High availability builtin capabilities for automatic failover and Disaster Recovery ensure constant application availability Security offers features like data encryption and threat detection for data protection performance supports large databases High transaction rates and low latency queries integration seamlessly integrates with other Azure services for easy Cloud native application deployment Azure SQL managed instance has two service tiers general purpose this tier is for light to medium IO applications using local storage and providing an economical and scalable option suitable for small and mediumsized businesses needing an affordable Cloud option business critical this tier is for high IO applications offering High availability automatic failover and premium storage ideal for critical applications in large Enterprises ensuring high performance availability and durability so that's a brief overview of azure SQL managed instances the next topic we'll be covering is database scalability both Azure SQL database and Azure SQL managed instance enable you to scale database Resources with minimal downtime adjusting quickly to workload or traffic changes to dynamically scale database resources you can use the the following options horizontal scaling this involves adding or removing replicas to adjust the capacity of your database both Azure SQL database and Azure SQL managed instance support horizontal scaling vertical scaling this involves adjusting the resources allocated to your database such as CPU or memory again both Azure SQL database and Azure SQL managed instance support vertical scaling the exact process for scaling your database resources may vary depending on which service you are using but in general the steps are as follows determine the resource needs of your database such as the required CPU and memory and the expected workload or traffic decide on the scaling option that best meets your needs whether that be horizontal or vertical scaling use the Azure portal po shell or the Azure CLI to configure the scaling settings for your database monitor the performance of your database to ensure that the scaling changes are providing the expected improvements the difference between Azure SQL database and Azure SQL managed instance when a when it comes to dynamically scaling database resources Azure SQL database offers a wide range of service tiers each with varying levels of performance and capabilities these service tiers allow you to choose the level of resources that best meets your needs and to easily scale up or down as needed as your SQL managed instance offers two service tiers general purpose and business critical these service tiers are designed to meet different needs with general purpose offering a balance of price and performance and business critical offering higher performance and availability for Miss critical workloads so that's a quick overview of database scalability focusing mainly on the commonly used Azure SQL database and Azure managed SQL instance the next topic we'll be covering is dynamic data masking Dynamic data masking or DDM an Azure is a feature that helps prevent unauthorized access to sensitive data it is a security feature of azure SQL database Azure synapse analytics and SQL Server that automatically conceal sensitive data in the result set of a query for example if you have a credit card number stored is 1 2 3 4 5 6 7 8 9101 11121 a dynamic data masking rule might conceal the numbers with all X's except for the ending four numbers in the query results ensuring most of the data stays protected and away from prying eyes Dynamic data masking is useful for scenarios where you want to provide a level of data security without needing to modify database operations it's often used in scenarios like reducing the exposure of sensitive data in your database when users are running reports or analytics and preventing accidental exposure of sensitive data especially when data is being used for development or testing purposes key features realtime masking data is masked in real time and does not affect the underlying data stored in the database the actual data remains intact and is not physically changed customizable masking patterns you can Define different types of masks depending on the nature of the data from partially hiding email addresses to fully masking credit card numbers except the last four digits the choices are vast and flexible rolebased access control a masking permissions can be granted to users who need to access the actual data ease of use Dynamic data masking is simple to set up and doesn't require changes to the database or applications common use cases for dynamic data masking include protecting personally identifiable information financial data and other sensitive data types in nonproduction environments or in applications with user roles that require access access to a database but not all of its sensitive data so that's an overview of dynamic data masking Azure storage provides several security and encryption features to ensure the confidentiality integrity and availability of your data here are some of the key features encryption at rest Azure storage automatically encrypts all data at rest using Azure storage service encryption or customer managed Keys using Azure key Vault encryption and Transit all data transferred to INF aure storage is encrypted using secure SSL TLS protocols keeping your data secure during Transmissions rulebased Access Control Azure storage provides rbac which enables you to Grant permissions to users groups and applications at A fine grain level access keys and shared access signatures Azure storage provides two types of authentication mechanisms for accessing storage accounts access keys and shared access signatures access keys are account keys that allow full access to a storage account while provides granular Access Control to specific resources within a storage account Azure private link enables you to access Azure storage resources over a private end point in your virtual Network this ensures that traffic between your virtual Network and Azure storage remains on the Microsoft Azure backbone Network Azure virtual network service and points Azure virtual network service and points enable you to extend your virtual Network to Azure storage similarly to Azure private link this ensures traffic remains within the confines of the Microsoft azure Azure Network Azure firewall this is a managed cloudbased network security service that protects your Azure virtual network resources you can use Azure firewall to secure traffic between your virtual Network and Azure storage Azure Monitor and Azure security Center Azure Monitor and Azure security Center provide monitoring and security features for Azure storage Azure monitor allows you to monitor storage account metrics and logs Azure security Center provides security recommendations and threat detection for Azure storage overall Azure storage isn't just a storage solution it's a fortress designed to protect your data at all levels from encryption mechanisms and access controls to private networking and threat detection it's equipped to tackle diverse security challenges hey this is Andrew Brown from exam Pro and in this section we'll be covering Azure synapse analytics so Azure synaps analytics is a data warehouse and unified analytics platform it offers a code free visual environment that streamlines and simplifies the building of ETL elt processes it allows for easy ingestion of data via more than 95 native connectors ensuring comprehensive access to a wide variety of data sources the integration of Apache spark into Azure synapse analytics allows for the use of tsql queries across both the data warehouse and Spark engines tsql short for transact SQL is Microsoft's implementation of SQL it's used to interact with relational databases in Microsoft SQL Server in addition it supports a variety of languages including tsql python Scala spark SQL and net broadening its accessibility to different developers Azure synapse analytics is not limited to data warehousing it's also seamlessly integrated with both artificial intelligence and business intelligence tools for instance it works in harmony with Azure machine learning for AI purposes and leverages Azure cognitive services for identity and access management moreover it integrates effectively with Microsoft powerbi for efficient data visualiz Iz ation and business intelligence operations taking a look at this image it shows a simple process of azure synapse analytics you can ingest data from many data sources such as on premises data cloud data SAS data or streaming data the data is stored in object storage via data Lake storage Gen 2 you can manage Azure synapse analytics via the synapse studio interface you can output the data to various Azure services such as Azure purview Azure ml or powerbi let let's talk a bit about synaps SQL a vital component of this platform snaps SQL is a distributed version of tsql designed for data warehouse workloads it extends tsql to address streaming and machine learning scenarios you can use builtin streaming capabilities to land data from cloud data sources into SQL tables it integrates AI with SQL by using ml models to score data using the tsql predict function it offers both serverless and dedicated resource models for unpredictable workloads that are unplanned or bursty you can use the always available serverless SQL in point for predictable workloads create dedicated SQL pools to reserve processing power for data stored in SQL tables and there you have it a concise introduction to Azure synaps analytics the next topic we'll be covering is the dedicated SQL pool and serverless SQL pool both are crucial components in the Azure synapse analytics Suite but they serve distinct purposes let's break them down dedicated SQL pool is essentially a query service over the data in your data warehouse the unit of scale is an abstraction of compute power that is known as a data warehouse unit think of dwu as a measure of computational power ensuring your database operations run efficiently once your dedicated SQL pool is created you can import Big Data with simple poly based tsql queries and then use the power of the distributed query engine to run high performance analytics ensuring you derive meaningful insights from your data on the other hand serverless SQL pool is a query service over the data in your data Lake one of the beauties of the serverless Paradigm is its adaptability scaling is done automatically to accommodate each query resource requirements in the world of data change is the only constant whether you're adding nodes removing them or managing failovers the serverless SQL pool is resilient it constantly adapts to ensure every query receives the resources it needs guaranteeing successful execution a in essence if you want Power go with dedicated SQL pool if you value adaptability choose serverless SQL pool either way asure simplifies your data operations making it easy to manage complex tasks all right let's dive into Apache spark integration within Azure synapse Azure synapse can deeply and seamlessly integrate with Apache spark which is one of the most popular opensource Big Data engine used for data preparation data engineering ETL and even machine learning tasks it offers machine learning models with sparkml algorithms and azl integration for Apache spark 3.1 with builtin support for Linux Foundation Delta Lake provides a simplified resource model that frees you from having to worry about managing clusters it has a rapid startup process along with aggressive autoscaling capabilities ensuring your system adapts swiftly to workload demands offers builtin support foret allowing you to easily incorporate your expertise and existing net code with in a spark application maximizing efficiency and resource utilization now let's talk about spark in conjunction with data Lake and Azure synapse Azure synapse removes the traditional technology barriers between using SQL and Spark together you can seamlessly mix and match based on your needs and expertise tables defined on files in the data Lake are seamlessly consumed by either spark or Hive SQL and Spark can directly explore and analyze paret CSV tsv and JSO n files stored in the data Lake and you benefit from fast and scalable data loading capabilities between SQL and Spark databases overall Azure setups coupled with Apache spark offers a Powerhouse of tools and capabilities making your data operation smoother and more efficient Azure synapse link is a feature in Azure synapse analytics that provides seamless integration and realtime analytics capabilities between Azure synapse analytics and operational data stored in Azure Cosmos DB it creates a tight integration between Azure Cosmos DB and Azure synapse analytics allowing users to explore and analyze their data with no extraction transformation and loading process required no data duplication and no impact on the performance of transactional workloads aszure synapse link forms a connection between Azure Cosmos DB's transactional database and Azure synapse analytics analytical capabilities it accomplishes this by creating a realtime updated columnar based analytical store within Cosmos DB benefits of azure synaps link for Azure Cosmos DB realtime analytics Azure synaps link enables realtime analytics by allowing direct querying of live operational data in Azure Cosmos DB without impacting its performance operational and analytical data cohesion it eliminates the need for complex ETL processes this simplifies the data architecture as it provides immediate and seamless access to analyze operational data cost efficiency Azure synaps link reduces costs by avoiding the need for additional storage and resources that are typically required for ETL processes increase productivity by removing the need for manual data extraction or synchronization processes Azure synaps link saves time and increases productivity for data scientists and developers improve data freshness with Azure synaps link the most uptodate data from Azure Cosmos DB can be accessed for analytics and Reporting in real time this ensures data freshness and accuracy so when should you use Azure synaps link for Azure Cosmos DB you should consider using Azure synaps link for Azure Cosmos DB when you are an Azure Cosmos DB user wanting to conduct analytics business intelligence and machine learning on your operational data you are currently running analytics or bi on your Azure Cosmos DB operational data using separate connectors or you are executing ETL processes to transfer operational data into a distinct analytic system in such cases AZ your synaps link offers a seamless analytics experience without impacting the performance of your transactional store however it's not ideal if you require traditional data warehouse capabilities like high concurrency workload management and persisting Aggregates across multiple data sources hey this is Andrew Brown from exam Pro and in this section we'll be covering Azure data Factory Azure data Factory is essentially a cloudbased managed service that plays a pivotal role in ETL elt and data integration process is it acts like a Powerhouse orchestrating data movement and and transforming data seamlessly on a large scale here's a breakdown you can create pipelines to schedule data driven workflows making data transformation and integration hasslefree build complex ETL processes that transform data visually with data flows this means you can transform data in a more intuitive and Visually appealing way use compute services such as Azure HD Insight Hado Azure data bricks and Azure SQL database thus offering flexibility and Power in how you handle your data once your data has undergone transformation you can publish your transform data to data stores such as Azure synapse analytics which can store and derive meaningful insights Azure data Factory has the ability to turn raw data into organized meaningful data stores and data likes ready for further analysis and Reporting so that's a quick introduction to Azure data Factory the next topic we'll be covering are the core components let's dive into each one and understand their significance Pipelines think of these as the assembly lines of your data Factory at pipeline is a logical grouping of activities that performs a unit of work it orchestrates and automates the flow of data activities these are the specific tasks within a pipeline an activity is essentially a processing step in a pipeline the action items or the work stages in your pipeline data sets these represent the data structures or the blueprints within the data store Link services they Define the connection information necessary for data Factory to access external resources like your data sources data flows data flows Define how data moves through a pipeline or under goes transformation offering a visual representation of data's journey and manipulations integration run times this is the engine under the hood the integration runtimes are the compute infrastructure used by Azure data Factory to facilitate the data movements and compute processes control flow the control flow orchestrates the sequence of activities in a pipeline it determines how activities are chained ordered or branched ensuring the systematic flow and process processing of data in a nutshell these seven components are the backbone of azure data Factory they work harmoniously ensuring your data is efficiently moved transformed and processed the next topic will cover in Azure data Factory is the data orchestration process data ingestion data is ingested from a variety of data sources this could be on premises SQL Server databases external data or any other supported data sources a DF supports a wide range of connectors that can be used to pull data from these sources data can be ingested in batch or realtime modes depending on the requirements data storage and transformation after data ingestion it's often stored in an intermediate storage for processing this could be Azure blob storage which is a scalable object storage for unstructured data for more structured analytics ready data Azure synapse analytics which is an analytics service can be used at this stage ADF pipelines can transform data by cleaning shaping and enriching using mapping data flows analysis once the data is ready it's then loaded into a data model for analysis this is where Azure analysis Services come in it allows you to build semantic models on your data which provide a Consolidated view of your business data and support high performance reporting and analytics visualization the data model can then be used by reporting tools like powerbi to create visualizations dashboards and reports that provide actionable business insights security and authentication throughout this process asure active directory is used for authenticating and authorizing users it provides identity and access Management Services ensuring that only authorized users have access to your resources and data and there you have it that's an overview of the data orchestration process in Azure data Factory from raw data to insightful visualizations all while being securely guarded the next topic we'll be covering is Microsoft SQL Server integration Services commonly known as ssis Microsoft SQL Server integration Services is a platform for building Enterprise level data integration and data transformation Solutions as sis can be used to automate SQL Server databases additionally it can be used as an integration runtime within Azure data Factory you can perform the following tasks with ssis copy or download files load data into Data warehouses cleansing data for better accuracy dive into Data Mining and managing SQL Server objects and data s sis can perform elt with variety of sources such as XML flat files and relational data sources s sis is builtin tasks and Transformations graphical tools for building packages and integration Services catalog database where you store run and manage packages you can use the graphical integration Services tools to integrate and transform data without having to write code s sis designer is a graphical tool that you can use to create and maintain integration Services packages picture a canvas where you lay out your data operations all with simple drag and drop functionalities here is an image that outlines the ssis designer in action with a data task flow example ssis allows you to drag out data Transformations with a variety of common tasks such as aggregate merge lookup and many more and here you can design different kinds of control or data flows so that sums up our overview on Microsoft SQL Server integration services next we'll quickly go through the pricing tiers of azure data bricks so you can get a better understanding of which one is best for your workload Azure data bricks offers two pricing tiers premium and standard here's what each tier brings to the table premium SKU the premium tier offers a full set of Advanced Data bricks features such as rolebased Access Control integration with Azure active directory for identity management and datab Bricks Delta engine which is a high performance engine for large scale data Lakes it is the most appropriate for Big Data analytics workloads and organizations requiring Advanced security and teambased workflows standard SKU the standard tier offers a subset of the data bricks platform features and is more costeffective for smaller workloads or development in test environments it includes the basic data bricks runtime in the collaborative workspace but does not include rolebased access control or Azure ad integration so that's an overview of the pricing tiers of azure data bricks hey this is Andrew Brown from exam Pro and in this section we're delving into to Azure datab bricks a game changer in the world of big data and artificial intelligence Solutions Azure datab bricks is an Apache sparkbased analytics platform optimized for the Microsoft Azure cloud services platform it's designed to simplify the process of building big data and artificial intelligence solutions by providing a unified analytics platform that accelerates the preparation of data for analytics and machine learning key features collaborative environment Azure data bricks provides a shared workspace for collaboration among data Prof professionals it facilitates dashboard creation and project sharing across various languages Azure integration as a native service it integrates smoothly with Azure services like data Factory synapse analytics machine learning and powerbi simplifying analytics pipeline creation Apache spark integration it incorporates an optimized runtime for Superior performance offering a serverless Apache spark experience with Auto configurability and integrated Azure Security Auto scaling and performance it offers Auto scaling and speed optimization for faster processing of Big Data workloads scaling to thousands of nodes and handling diverse workloads robust security it delivers Enterprise grade security with data encryption active directory integration role base Access Control private network connectivity and Industry standard compliance use cases D data analytics Azure data bricks can process large volumes of data in parallel making it suitable for big data processing and analytics machine learning it provides provides a platform for preparing data and developing training and deploying machine learning models realtime analytics Azure data brick supports streaming analytics allowing for realtime insights from data ETL processes Azure data bricks can be used for building robust ETL pipelines data exploration and visualization the collaborative workspace in Azure data bricks allows for data exploration and visualization enabling data scientists and analysts to derive insights from data just to recap the key differences as your data focuses on analytics big data and machine learning while Azure data Factory primarily handles data integration and pipeline orchestration in the cloud hey this is Andrew Brown from exam Pro and in this but before we delve into the specifics of azure data Lake let's clarify what a data Lake actually is in simple terms a data lake is a centralized and scalable repository capable of storing a vast range of data including raw unstructured and semistructured data it's designed to accommodate a massive volume of data commonly utilizing objects referred to as blobs or files as its primary mediums for storage here is a visual of a lake with ones and zeros representing the vast amount of data that it can accommodate the processing of this data can be simplified and broken down into a few pivotal steps collect this step involves Gathering data from various sources transform here data under goes Transformations or modifications through the usage of elt or ETL this process converts the raw data into a more organized semistructured format ready for further analysis distribution this phase makes the transform data accessible to various programs or apis publish finally data sets are published to metadata cataloges making it easier for analysts to locate and tap into valuable data resources so that sums up a quick introduction to data lakes and the foundational process involved now that we have a solid grasp on what a data lake is let's delve into the specifics of azure data Lake Azure data lake is a highly scalable and secure data Lake that allows you to store and analyze large amounts of data it is composed of two main components Azure data Lake storage and Azure data Lake analytics Azure data Lake storage this is a massively scalable and secure data Lake that allows you to store all types of data there are two generations of azure data Lake storage Azure data Lake storage gen one it provides a single repository where you can capture data of any size type and Speed without forcing changes to your application as the data scales Azure data Lake storage Gen 2 this is a set of capabilities dedicated to big data analytics built on Azure blob storage it combines the scalability and cost benefits of object storage with the reliability and performance of The Big Data file system capabilities to Azure data Lake analytics this is an on demand analytics job service that simplifies big data instead of deploying configuring and tuning Hardware you write queries to transform your data and extract valuable insights it includes usql a language that unifies the benefits of SQL with the expressive power of your own code it also allows you to dynamically scale the resources you need for your jobs making it coste effective as your data Lake storage gen one the first version of data Lake storage and will be retired in 2024 new users should use Gen 2 Azure data Lake storage Gen 2 Data Lake storage is azure blog storage which is has been extended to support big data analytics work loads designed to handle pedabytes of data and hundreds of gigabits of throughput in order to efficiently Access Data data Lake storage adds a hierarchical main space to Azure blob storage and there you have it a comprehensive overview of azure data like next we'll be exploring Azure data Lake storage Gen 2 in more detail given that the exam will feature questions asking you to identify the most suitable type of storage for specific scenarios it's essential to have a firm Gra grasp on which option is best suited for different tasks Azure data Lake storage Gen 2 plays a vital role in Azure data Lake by providing a secure scalable platform to store large data volumes it is tailored to support high performance analytics and machine learning operations here are some of its core features hierarchical mainspace enables organizing and managing data in a hierarchical file and folder structur similar to traditional file systems simplifying data organization and Transformations scalability designed to handle enormous amounts of data from pedabytes to exabytes with high throughput and low latency it can also manage High volumes of small rights common in Big Data scenarios security incorporates Azure active directory for identity and access management rolebased Access Control firewall rules and virtual network service in points along with encryption at rest and and Transit it also supports Azure private link ensuring data travels over a private Network cost Effectiveness offers lowcost storage with life cycle management policies that a automatically move data to cheaper storage tiers or deleted after a specified period lowering costs performance provides the high performance Computing needed for big data analytics and allows choosing the best performance characteristics for specific workloads through its performance tiers overall Azure data Lake storage Gen 2 is an extended version of azure blob storage designed for big data analytics providing additional capabilities like hierarchical file systems and fine grained Access Control the next topic we'll be covering is the second core component to Azure data Lake which is azure data Lake analytics Azure data Lake analytics is an on demand analytics job service streamlining the complexities of Big Data operations instead of going through the cumbersome processes deploying configuring and tuning Hardware all you write queries using usql to transform your data and extract valuable insights effortlessly to illustrate its efficiency exporting approximately 2.8 billion rows of TP CDs store sales data which is around 500 GB into a CSV format file took less than 7 minutes and importing a full 1 terab set of source data into Azure analysis Services by using the Azure data Lake connector took less than 6 hours now let's talk a bit about usql usql is a structured query language included within data Lake analytics to perform queries on your data Lake it's versatile allowing you to query and combine data from a variety of data sources including Azure data Lake storage Azure blob storage Azure sqldb Azure SQL data warehouse and even SQL Server instances running in Azure VMS for those who like HandsOn experiences you can install Azure data Lake tools for visual studio to perform usql jobs on your Azure data lake so that's an overview of azure data Lake analytics hey this is Andrew Brown from exam Pro and in this section we'll be covering is Cosmos DB but before we talk about Cosmos DB it's important to understand the key types of nosql databases first up let's talk about key value stores key value stores are simple and fast but they generally lack features like relationships indexes and aggregation in a key Value Store data is stored in pairs where a unique key is stored alongside a corresponding value a simple key Value Store will interpret this data resembling a dictionary also known as associative arrays or hash although this kind of data storage can resemble tabular data it doesn't require consistent columns per row making it schema less due to their simple design they can scale well beyond a relational database next let's explore what a document store is a document store is a nosql database that stores documents as its primary data structure these documents can be structured as XML but are more commonly found in JSO or similar formats essentially document stores are a subclass of key value stores but have a more intricate structure allowing for complex queries and operations here's a visual that displays the components of a document store compared to a relational database now let's move on to understanding graph databases a graph database is a database composed of a data structure that uses vertices also known as nodes or dots which form relationship to other vertices through edges arcs or lines this type of database is particularly powerful for mapping relationships and identifying patterns use cases for graph database fraud detection realtime recommendation engines Master data management Network and and operations identity and access management traceability and Manufacturing contact tracing data lineage for gdpr customer 360 analysis product recommendations social media graphing and feature engineering the nodes can contain data properties while the edges can hold relational data including directional information and other data properties so that's an overview of three key types of nosql databases now that we have a better understanding of the key types of nosql databases it's time to delve into the main topic Cosmos DB Azure Cosmos DB is a service for fully managed nosql databases that are designed to scale and have high performance attributes Cosmos TB facilitates interaction with different types of nosql database engines through distinct apis namely core SQL a document data store as your Cosmos dbapi for mongodb another document data store Azure table a key value data store Gremlin a graph data store based on aache Tinker pop these no SQL engines offer two specific capacities provision throughput where you pay for a guaranteed capacity and serverless where you pay only for what you use Cosmos DB shines with its incredibly quick response times and solid support for scalability it's a fully managed service meaning Azure takes charge of all automatic management updates and patches main advantages of azure Cosmos TB into integrates with many Azure services including Azure functions Azure kubernetes services and Azure app services integrates with many databases apis like the native core SQL mongodb Cassandra and Gremlin support for multiple development sdks spanning Donnet Java Python and nodejs offers a schema less service with automatic indexing of data ensuring rapid queries guaranteed uptime SLA of 99.999% availability data replication between Azure regions is automatic data protected with encryption at rest and robased access and autoscale is provided to handle a variety of workload sizes next let's talk about Cosmos DB apis different types of AIS are available in Azure Cosmos DB to support a wider range of applications these apis allow data to be delivered via documents key value pairs y columns or graph data for new projects the core SQL apis are strongly recommended whereas for existing databases is the specific database API is recommended the apis are as follows for SQL API the default API for utilizing Azure Cosmos DB and enables data querying with a language akin to SQL mongod DB API facilitates communication with mongod DB databases and document storage Cassandra API allows interaction with Cassandra using the Cassandra query language and supports data storage as a partition row store Azure table API a communication tool for Azure table storage it supports it indexing in the partition and row Keys Gremlin API helps in creating a graphbased data view that can be quered using graph traversal language so that's an introduction to Cosmos DB next up on our agenda is the cosmos DB Explorer Cosmos DB Explorer provides a userfriendly web interface that allows users to delve into an engage with their Cosmos DB accounts you can readily access this interface by heading to cosmos. azure.com here is a cosmos DB core SQL within Cosmos DB Explorer adding a document to the database which is very straightforward it's worth noting that when you journey through Azure to access a cosmos DB account under the data Explorer section you're essentially interacting with the same interface as cosmos DB Explorer furthermore the versatility of Cosmos DB Explorer shines through with its compatibility with other database types for example it's seamless integration with a graph database using Gremlin so that's a quick overview of Cosmos DB Explorer the next topic we'll be covering is partitioning schemas in Cosmos DB partitioning in Azure Cosmos DB plays a pivotal role in optimizing performance data in Cosmos DB indexes is strategically grouped by partition Keys ensuring quick and efficient data access main concepts of partitioning schemas in Azure Cosmos DB partition Keys think of these as the backbone of partitioning they are the keys used to group items together and can be like lik to primary keys in relational databases essentially they dictate how data is divided and organized within the system a logical partition is a group of items that all have the same partition key value physical partitions consists of a set of logical partitions Azure Cosmos DB manages logical partitions which can have one to many replica sets are made up of a group of physical partitions that are materialized as a selfmanaged dynamically load balanced group of replicas that span across multiple fault domains each physical partition is not alone it has a set of clones or replicas this set of replicas is what we refer to as a replica set replicas ensure data durability availability and consistency logical partitions are mapped to physical partitions and these physical entities are then spread globally ensuring data availability and low latency access across regions to clarify the term in the image a partition set refers to a collection of physical partitions these partitions collectively manage the same logical partition keys and they Ensure this consistent management across multiple regions in essence Cosmos DB's partitioning architecture is meticulously designed for scalability fault tolerance and Optimal Performance whether you're considering logical or physical partitions replica sets or partition Keys each plays a crucial role in ensuring that Cosmos DB remains one of the most efficient nosql databases on the market the next topic we'll be covering is choosing a partition key at partition key has two components partition key path and the partition key value for example you can consider an item user ID Andrew works for Microsoft if you choose user ID as the partition key the following are the two partition key components while in Partition key path this would be user ID this path essentially points to the property and your data item that holds the partition key value alpha numeric characters and underscores are accepted and you can navigate through nested objects using the standard path notation to partition key key value for our example this is Andrew it's the specific value found at the partition key path this value can be either a string or a number your partition key for all containers should be a property that has a value which does not change you can't change the value of a property if it's your partition key the partition key should have a wide range of possible values to distribute data and workload uniformly across various logical partitions spread request unit consumption and data storage evenly across all logical partitions this ensures even rot consumption and Storage distribution across your physical partitions let's talk about unique keys so unique Keys provide developers with the ability to add a layer of data Integrity to their database by creating a unique key policy when a container is created you ensure the uniqueness of one or more values per partition key a unique key is scoped to a logical partition if you partition the container based on the zip code you end up with duplicated items in each logical partition it's important to know that you can't update an existing container to use a different unique key a unique key policy can have a maximum of 16 path values each unique key policy can have a maximum of 10 unique key constraints or combination when a container has a unique key policy request unit charges to create update and delete an item are slightly higher in addition unique key names are case sensitive the next topic we'll be covering our containers in Cosmos DB Azure Cosmos containers are useful for scalability in Azure Cosmos DB both in terms of storage and throughput they are beneficial when you need a different set of configurations for each of your Azure Cosmos DBS because they allow you to customize each container individually some applications may require robust WR capabilities for logging While others prioritize reading due to data access needs with Cosmos DB containers each application can have a customized container that meets its demands balancing performance and cost asure Cosmos container has some container specific properties and those properties which can be system generated or user configurable vary according on the used API and Azure Cosmos container has a set of system defined properties depending on which API you use some properties might not be directly exposed the table lists various system defined properties in Cosmos DB and indicates which ones are system generated versus user configurable additionally it also denotes which properties are used by different apis for nosql Cassandra manga B Gremlin and table rid type system generated purpose acts as a unique identifier for a container supported by only the API for no SQL eag type system generated purpose utilized for optimistic concurrency control it ensures that only one client can change an item in the database at a time supported by only the API for no SQL TS type system generated purpose represents the timestamp when the container was last updated supported by only the API for nosql self type system generated purpose provides an addressable Yuri of the the container it's essentially a unique reference or link to the container supported by only the API for nosql ID type user configurable purpose denotes the name of the container supported by all the apis listed there are many more properties but we won't be able to list them all here the next topic we'll be covering are the capacity of Cosmos DB containers so what is capacity capacity defines the amount of underlying resources are available to support consumption of resources such as compute and storage as we've briefly touched upon before Cosmos dbs2 capacity modes provision throughput and serverless provision throughput in this mode you allocate a specific amount of throughput for your containers this throughput is Quantified in terms of request units per second this mode is suitable for workloads where traffic can be predicted it offers a high degree of flexibility letting you match your provision capacity to the expected demand serverless this mode is the opposite in its approach instead of provisioning in advance you simply run database operations without setting any predetermined capacity this mode is beneficial for smaller workloads or those that might experience unpredictable traffic spikes while it offers the advantage of Simplicity and configuration there are some inherent limitations to be aware of when it comes to geod distribution the provision throughput option in Cosmos DB offers unlimited multi region support in contrast the serverless option is restricted to a single region in terms of storage capacity while provision throughput allows for unlimited storage per container serverless is Cap at 50 GB performance wise both options deliver less than 10 ms latency for Point reads however while provision throughput guarantees less than 10 ms latency for rights under its SLA serverless offers less than 30 MS for rights as covered by its SLO regarding billing provision throughput charges per hour based on the set Ru s irrespective of actual R consumption on the other hand serverless charges are based on the actual Ru as consumed by your operations build on an hourly basis next we'll delve into the consistency levels in Cosmos DB these levels play a pivotal role shaping the availability latency and accuracy of database operations aure Cosmos DB provides five different consistency levels to maintain data availability and quering performance depending on your requirements the consistency levels in Cosmos DB can be visualized as the Spectrum this ranges from strong to eventual on the strong side it has higher latency lower availability but has worse read scalability but as you move towards eventual you gain lower legancy higher availability and better read scalability strong linearizability reads are guarantee to return the most recent version of an item bowed staleness consistent prefix reads lag behind writes by at most K prefixes or t interval session consistent prefix medonic reads medonic rights read your rights WR follows read reads consistent prefix updates returned or some prefix of all the updates with no gaps eventual outof order reads you set default consistently at the cosmo B account Level under default consistency blade strong consistency this level guarantees that read operations return the most recent data while it's Rec cost align with bounded tailess they are higher than those of session and eventual consistencies furthermore data written can only be read once the majority of replicas have successfully replicated it now it stailness causes re operations to lack behind rights due to time or version disparities despite having the same re cost as strong consistency it's pricier than session and eventual consistencies it stands out as the most consistent when compared to session consistent prefix and eventual this level is ideal for globally distributed applications that prioritize High availability and minimal latency session session consistency ensures that data read within a session matches the most recent write in that session however other sessions might see outdated or dirty data from recent wrs in different sessions it's the default consistency for new databases its Rec cost fall between those of bounded stal less strong and eventual consistencies with consistent prefix read operations fetch the latest data replicated among replicas although it may not be the absolute latest situations can arise where dirty data appears due to changes in one replica that haven't propagated to others its consistency is superior to eventual but Trails behind other levels eventual consistency offers the least Assurance with no guarantees on immediate data accuracy however it boasts the lowest latency Optimal Performance and the most costeffective read operations among all levels so that's an overview of the consistency levels in Cosmos DB hey this is Andrew Brown from exam Pro and in this section we'll be covering Azure rid's cach but before we talk about Azure ridy cache we'll need to know what ridy is so ridy is an open source and memory database store Ries accs as caching layer or a very fast database since all data is stored in memory it's highly volatile meaning data loss is possible under certain conditions rides is very fast that it can deliver content from its store with single to double digit milliseconds such as 10ms despite its imemory nature ridies provides options for data persistence allowing you to balance performance with durability it can periodically save data snapshots to dis or append each command to AOG red is a key value store and it supports the following data structures sets and sorted sets collections of strings in which every item is unique lists a collection of strings sorted according to the order they were inserted hashes perfect for story objects these are maps between string fields and string values bit maps and bit Fields allow for operations at the bit level providing extremely efficient storage hyper log log a sophisticated algorithm to count unique values geospatial indexes Grant the ability to manage spatial items in query by position streams new in rides it offers a way to log in realtime stream data so that's an introduction of rid's the next topic we'll be covering is azure cash for rides Azure rid cash is based on the popular op Source rid's cache it gives you access to a secure dedicated rid's cache that Microsoft manages and that you can access from any Azure application Azure R's caches an in memory database that caches data in key value pairs it helps your application become more responsive even as the customer load increases it takes advantage of the redu engines low latency High througha capabilities this distributed cash layer allows your data tier to scale independently allowing for more efficient use of your application layers compute resources Azure red cash perfectly complements Azure database services such as cosmos DB and Azure SQL it provides a costeffective solution to scale read and write throughput of your data tier using the caches side pattern you can store and share database query results session States static content and more Azure cash for stores session State and other data that needs low latency access diagram of cash aside pattern on Azure storage first we'll need to check the rid's cach to see if your item is available if the item is found we'll retrieve it if the item is not found we'll pull the item from the table storage recash it in redas and then return the results on the right we have an image that should clearly explain the workflow overview check if cash exists if true go to step two and obtain the data if cash could not be found go to step three and recash the item and return the results so that's a summary of azure cash redes and how it works the next topic will be covering our caching expiration policies for Azure rid's cach Azure rid's caches expiration policies are configured per each request so we could have an expiration policy different for each cach key for rid's distributed cach those expiration policies are as follows absolute expiration relative to now this policy sets the duration for the cash to live counting from the moment the item is added to the cash for example if you set this to 10 minutes the cash item will expire 10 minutes after it has been added absolute expiration this policy sets a specific date and time when the cash item will expire once that date and time are reached the cash item is evicted regardless of when it was added sliding expiration this policy defines an expiration time relative to the last access time of the particular cach item If an item has a slighty expiration of 10 minutes it will expire if it hasn't been accessed in those 10 minutes every time the item is accessed its expiration timer is reset so that's a brief summary of the Azure rid's caches expiration policies hey this is Andrew Brown from exam Pro and in this section we'll be covering nonrelational data storage Solutions in Azure starting with Azure table storage Azure table storage isn't your regular relational database instead it's a nosql data store for semistructured data needly housed within Azure storage accounts what makes it particularly appealing especially in today's fastpaced Tech world is its ability to handle vast amounts of unstructured or semistructured data without being tied down by a fixed schema there are two ways to interact with Azure tables the first is azure table storage API a robust tool for developers to seamlessly integrate query and manage their data it provides you the flexibility to interact programmatically and customize according to your application's needs on the other hand we have Microsoft Azure storage Explorer it offers a userfriendly interface to browse manipulate and manage your data without writing a single line of code think of it as your visual window into the world of azure tables here is a visual of azure storage Explorer a standalone app that makes it easy to work with Azure storage data on Windows Mac OS and Linux you can create blob containers upload files create snapshots of disk and more so when should you use Azure table storage large amounts of semistructured data if you have massive volumes of semistructured or unstructured data but don't need the complexities of a relational database costeffective storage Azure table storage is a budget friendly solution especially when compared to fullfledged databases it's suitable for projects where lowcost storage is a priority scheme of flexibility if your application has evolving data structures where the schema can change over time the schema less design of azure table storage offers flexibility scalability requirements for applications that need to scale out by adding more data entities but don't require complex joins stored procedures or secondary indexes and fast access and high throughput if you need a storage solution with low latency and high availability characteristics for quick access to data so that's an overview of azure table storage the next topic we'll be covering is adding entries in Azure table storage when you enter data you must provide a partition key this is a unique identifier for each partition within a table this key enables Azure to distribute table data across multiple nodes for improved data access and load balancing for instance you might use the date and time as a partition key for log data ensuring entries are evenly distributed and easily sorted row key this key is a unique identifier within a partition allowing you to pinpoint a specific entity it's like the address of your data within the partition for instance in a list of customers a customer ID could serve as a r key Azure table supports a diverse set of data types string textual data such as names or addresses Boolean true or false values binary data such as file or image date time specific date and time information double floating Point numbers get Global unique identifiers in 32 and in 64 whole numbers both small and large when you need to retrieve data you can perform queries using both partition and row keys this dual Key System allows for Rob in flexible data retrieval operations for example if you're looking for a specific transaction in a financial database you can use the partition key to narrow down the date and the row key to find the exact transaction Azure table storage allows you to apply additional filters to your queries if you need to find all entries from a specific location or all entries falling under a particular category you can easily set up filters to refine your search so that's a quick overview of adding entries in Azure table storage the next topic we'll be covering is azure Q storage Azure Q storage is a robust and straightforward messaging broker that facilitates smooth and secure message exchange between various applications and services within the Azure environment key features simple message broker Azure Q storage allows Services running on cloud infrastructure to communicate with each other asynchronously it can handle large numbers of messages simultaneously ensuring your services remain highly responsive security Azure Q storage uses authenticated HTTP as protocols ensuring that data transmission is secure and reliable message size it can hold messages up to 64 kilobytes in size accommodating a wide range of data types and sizes for storage and access storage account Q storage is stored within an Azure storage account ensuring a secure and unified setting for All Storage needs access keys and connection strings utilize the same axis keys and connection strings for Q storage as with other resources in the storage account ensuring streamlined and consistent access management Azure Q storage offers three ways of handling messages on the que Peak this option allows you to preview a message in the queue without deleting or locking it it's useful for determining the next message to process delete after successfully processing a message an application will typically delete the message to ensure it isn't processed again receive and lock by locking a message it ensures that other parts of the system can process the message simultaneously after process ing the lock is either renewed or the message is deleted Azure portal easily create a Quee and send messages through the userfriendly interface of the Azure portal Azure SDK or CLI for more programmatic control most interactions with the queue including sending peing and deleting messages can be performed using the Azure SDK or command line interface here is a python example to help you understand what this looks like developers can create cues add messages and process them with just a few lines of code so that's an overview of azure Q storage the next topic will be covering are the key Concepts in Azure Q storage cues can be accessed by using the following URL format https storage account. q.c. windows.net Q the following URL addresses a Que in the diagram https by account. q.c cor. windows.net images to download here's a breakdown of the components store storage account a storage account is required for all Azure storage access think of it as your gateway to Azure storage Services q a q contains a set of messages keep in mind the Q name must be all lowercase message you can store any format of data in a message but it must not exceeds 64 Koby if for version 2017 0729 the maximum time to live allow is 7 days for version 2017 0729 or later the maximum time to live can be any positive number or minus one indic that the message doesn't expire in addition the default time to live is 7 days if this parameter is not specified you can interact with the queue via the Azure CLI subcommand a Z Storage message action clear deletes all messages from the specified que delete deletes the specified message get retrieves one or more messages from the front of the queue peik retrieves one or more messages from the front of the queue but does not alter the visibility of the message put adds a new message to the back of the message queue update updates the visibility timeout of a message so that's an overview of the key Concepts in Azure Q storage hey this is Andrew Brown from exam Pro and in this segment we'll be covering Azure files Azure files is a fully managed file share in the cloud a file share is a centralized server for storage that allows multiple connections it's like having one big share drive that everyone or in this example multiple virtual machines can work on at the same time to connect to the file share you can use Network protocols like the server message block or network file system when a connection is established the file shares file system will be accessible in the specific directory within your own directory tree this process is known as mounting backups you shared snapshots to backup your file share these snapshots are readed only and incremental meaning they only contain data that has changed since the last snapshot you could have a maximum of 200 snapshots per file share and these can be retained for a whopping 10 years remember backups are stored within your file share if you delete the share you say goodbye to the backups as well soft delete you can prevent accidental deletion by turning on soft delete with soft delete your data isn't immediately removed instead it's marked for deletion and held for a certain period before it's permanently erased Advanced threat protection an additional layer of security intelligence that provides alerts when it detects suspicious activity on your storage account store tiers premium store on to SSD with singled digigit milliseconds for most EO operations transaction optimized store on HDD with transaction heavy workloads that don't need the latency offered by premium file shares hot optimized for general purpose file sharing scenarios such as team shares and Azure file sync cool stored on HDD for cost efficient storage optimized for online archade storage scenarios types of storage general purpose version 2 deployed onto HDD file storage deployed on the SSD identity on premises Azure storage can integrate with an on premises active directory domain service managed Azure storage can be joined to Microsoft managed active directory domain service store account key use a combination of storage account name as the username and the account key as the password for mounting networking a aure files can be access from anywhere both inside and outside your Azure account through the storage accounts public in point remember SMB uses Port 445 if you face connection issues check if this port is open in your organization to mount your file share encryption Azure files is encrypted at rest using Azure storage service encryption Azure files is encrypted and Transit with SMB 3.0 plus with encryption or https and there we have it a comprehensive look into Azure files and its noteworthy features continuing from our discussion on Azure files let's now explore its main use cases use cases for Azure files completely replace or supplement on premises file servers Network network attached storage devices lift and shift your on premises storage to the cloud via classic lift or hybrid lift lift and shift means when you move workloads without rear tting for example you can directly import your local VMS to the clab classic lift both the application and its data are moved to Azure hybrid lift here only the application data gets moved to Azure files while the application remains operational on premises simplify Cloud development shared application settings multiple VMS and developer workstations need to access the same configuration files diagnostic share all VMS log to the file share developers can M and debug all logs in a centralized Place Dev test debug developers can quickly share essential tools needed for local environments containerization you can use Azure files to persist volumes for stateful containers while use Azure files instead of setting up your own file share server shared access Azure files is preconfigured to function with standard networking protocols like SMB and NFS fully managed Azure manages maintenance and security patches to ensure your file storage is secure and updated scripting and tooling you can automate the management and creation of files shared with Azure API and Powershell resiliency Azure files is built for durability ensuring that your data is always safe and accessible so these are the main reasons and scenarios for incorporating Azure files into your infrastructure the next topic we'll be covering is azure file sync Azure file sync is a service that it allows you to Cache azzure file shares on an on premises Windows server or Cloud VM how does it work caching Azure file sync transforms your Windows Server into a quick cach of your Azure file share this means that you can access the data you need faster than ever without having to rely on the cloud multiple protocols regardless of the protocol you're comfortable with be it SMB NFS or ftps aure file sync lets you access your data locally granting greater flexibility Global caches for businesses operating globally as your file s Sy allows setting up caches in multiple locations for faster data access key benefits centralized storage with Azure file sync you get centralized file services in Azure allowing multiple locations to use cloud tiering and direct Cloud access to store and access data integrated with Azure backup Azure file sync integrates seamlessly with Azure backup offering unified and streamlined backup Services Cloud tearing as your storage needs change older or rarely used files can be moved to Azure saving space on your local servers easy integration it's designed to integrate smoothly with your existing infrastructure meaning less learning curve and disruption so that's a brief overview of azure file sync hey this is Andie Brown from exam Pro and in this follow I'm going to show you how to set up a file share uh and mount it to a virtual machine so let's get to it so the first thing we're going to do is go all the way to the top and we're going to type in storage accounts because if you type in files or um Azure files you're just not going to get anything because it is a subservice uh within a storage account so we'll go ahead and hit add and I think I will name uh I'll make a new Resource Group as I always do I'm going to name this one as keos okay and then we'll name the account as such now I want to show you something here so under the account kind uh if you remember from our lecture content you can create a uh a file share under general purpose 2 um but if you go to premium 2 you'll have just a file storage type that means only your this storage account is only for creating a file storage and that's if you want to use the premium tier 4 uh um axis and we don't today um but I will show you uh when we get to that in a moment here uh just to point that out to you so what we'll do is go ahead and hit review create actually before we do that let's just double check if there's anything interesting here sometimes there is so under Azure files for large file shares um provides file share support up to 100 terabytes so right now it's it's disabled but if you wanted larger files you could go ahead and enable that but we're just going to leave it off I'm going to go ahead and hit review and create now as this is creating and this goes pretty darn quick uh what I'm going to do is launch a new virtual machine because we need something uh to mount the file share too so make your way over to Virtual machines and what we're going to do is launch a Linux one just because that's a lot easier for me to do here and they're generally more inexpensive so we'll choose Kos I'm also going to name the virtual machiney evos and we'll choose Ubuntu as our image so just go down here make sure you choose 18 LTS generation 2 and under the sizes we'll expand it make sure you choose B1 LS because that costs around $6 a month under your password we will choose Azure user as the name and then for the password itself testing 1 two 3 capital on that t so capital T testing 1 23 4 5 6 sorry it's always uh 1 2 3 4 5 6 we should probably open up Port 44 five um but uh we'll get to that when um we get to the uh actually after we create it here so I'll hit review and create and we will go ahead and just give it a moment just takes a little bit of time and we'll just go and create that there so as that is creating we'll go back to our storage account we'll go into it and uh as as you notice these are the services the the things that we can launch in our storage account what we want is a file share that's Azure files um and notice here that the capacity is set to 5 terabytes and it has a soft delete for 7 days it's not configured with Azure Azure active directory as of yet if we wanted to use that to authenticate but what we'll do is click on file share on the right hand side I'm going to name our file share as keyos I'm going to set the total to uh 3 gigabytes because we don't need a lot of data and remember that tier I talked about well here it is it's disabled but if we had created it a a a premium storage account and a file file storage we could could have chosen that all right so we'll go ahead and hit create and this doesn't take too long as that's going let's make our way back to our virtual machine it looks like it's deployed so we'll go to that resource and then on the left hand side we'll go to networking because I want to open up that Port Port Port 445 that's what ssnb SMB communicates on and so uh down below all this is fine but if we were to go here there isn't one for SMB so what we'll do is just put in 445 and we'll say TCP and I'll just write down your SMB and we'll go ahead and hit add and that only takes a moment there we'll make our way back to our storage account we'll click in or sorry into our files yeah Serge account in our files notice we have connect and upload and so what we're going to do is Click uh connect and it's going to give us some instructions here that we can run and this is going to be the mount point so I'm going to go make my way back to here and I see that that record has been created and I'm going to open up our uh Cloud shell and so if you've never opened up cloud Cloud shell before It'll ask you to make a storage account that is specific just for it just say yes make sure you're in bash mode we cannot do this in Powershell and uh within your virtual machine if you're on the right tab here you'll go to overview and we're going to grab that public IP address type SSH Azure user at sign paste in the IP address we'll hit enter type in yes to accept the fingerprint and then type your password capital T testing 1 23 4 5 6 okay and it'll let us in and what we're going to need to do is do a pseudo apt update and I'm going to do Ampersand Ampersand just so we can run this in one line we're going to type in pseudo apt install cifs utils this is the utility that we're going to use to do um that's part of the mounting so we need to make sure that is installed and so we'll let that go um up apt update can take a lot of time or a very little amount of time it just depends on how many updates there are but it shouldn't take too long um and so what we're going to do is prepare this because this is not the easiest to work with and so what we'll do I already have it open here because I was given this a go earlier this is the exact same thing but I'm going to delete it out so it's one one because I think it might have changed a little bit here whoops and so I will copy this here and paste it in and we'll just give it a quick read so what it's going to do um I don't know if it would color this if I went to bash here shell script there we go it's a bit easier so it's going to uh create a Dory called Mount keos um and then it's going to create another directory for the SMB credentials it's going to store the username and then the password in these credentials we're going to chamod it so we have our permissions uh and then we have this bass script here um which is a little bit hard to read but it is using cifs so it's doing some something there and setting that stuff up and then it's using CFS it's actually mounting this case so that's what we're going to do so um just go going back over here and back to this other tab um what I'm going to do here is um oh sorry now this is ready we're going to just type in clear and I'm going to run each of these commands so I'm going to grab the make directory here and I'm going to paste that in if you paste all this in half the time it messes up when you have multiline like that so it's not even worth trying uh so we'll do copy we'll make that other directory here and then uh we have this pseudo bash line here so we'll copy that okay hit enter and then we'll grab this next line here we'll copy that paste that in hit enter did not like the space in front of it I think it's just a really long line so what I'll do is I'll just grab that line and move to the wall make my life a little bit easier and we'll paste that in like that um it's saying the credentials does not exist hold on here so we'll just write clear it's okay if it happens so this directory should exist right so if I do um um PWD Etc or we should just be able to autocomplete it so CD Etc I'm hitting tab to autocomplete and then we want SMB and this says credential so it's missing the S so there's my mistake okay so I'm going to go ahead and remove that because if we don't have it named right it's just not going to work right so we'll go ahead here and make sure we copy the entire line that was my mistake and we'll paste that in there we'll go to the next line copy that paste that in copy the next line here paste that in and uh we will what we'll do is we'll chamod the file okay so if we want to check to make sure it's the correct permissions what we can do is is um do an LS paste in the or just write in the path uh which is SMB credentials here and here we can see the file if we do a hyphen La it will show us that it has the correct permission so see it's really locked down there that's what the Chabad did if we want to see the contents of that file to make sure that those things are in there uh what we can do is type in um cat and just grab that whole link here we might have to give it pseudo it might complain for that it does so we'll put pseudo in front of that and so we just want to make sure there's one username and one password um and everything looks okay there we'll type clear we'll make our way back here and so now we'll run these commands so that's the first one and then this say is the second one we'll make sure that that n matches up i o yep I O hit enter and so now it should work so if we were to upload a file uh that should be in good shape but if we want to get to that directory we'll just type in Mount if we keep on hitting tab it will show us there's keyos okay and so you know this is where we're going to create some files so I actually don't have a file prepared so give me a moment I'll be back in a moment with a file all right I'm back and I've uh prepared an image for our self load so going back to our uh file share in here we have the upload button just go ahead and click that and on the right hand side it's going to give you a popup and uh within that popup there um just go find your image I'm just offscreen grabbing it so this is uh my image what I'm uploading which is keas uh keas fagio it's who we named it after he's the collector in Star Trek Con generation and so what I'll do is go ahead and hit upload right and so that file is supposedly there if we go back here and do uh LS there's the file right um and so probably it should be reflective so if we were to go and uh delete this file so we'd say remove or maybe if we made a new directory let's see what happens if we say um you know uh the episode's called the most toys in Star Trek just see if it actually reflects back and forth so we do a refresh there it is can I drag no I can't drag but I could probably move this file over there so we'll say move uh keos fio into the most toys episode and if we go back here and refresh you get the idea right so yeah u i mean that's as simple it is uh with the file share there okay so now that we're all done here with Azure file share we can't or we can't use this to make a file sync because we would have to use Windows for that so this is all pretty much done so what I want you to do is go to your resource groups we'll find keyos and we'll go ahead and delete this and we'll call this uh part of our follow along 100% done here okay so see you in the next one hey this is angre Brown from exam Pro and we are going to take a look at using Azure file sync so uh previously we set up a file share with just Linux but we're going to take it a bit further and set up a sync so what we're going to do is we're going to do two things we're going need to set up a new storage account and we'll also have to launch a virtual machine since virtual machines take longer to launch than a storage account what I want you to do I still have the old tab here from the last uh follow along but what I want you to do is make your way over to Virtual machines open that a new tab and we'll go ahead and launch ourself a Windows Server so up in the top left corner hit uh ad virtual machine we'll give it a moment I'm going to call this new one uh well we we can use keos again so I'm going to go ahead and do that we'll say this is keyos and I'm going to choose this time Windows 2019 server so we have a Windows server and I want uh 2019 um it doesn't necessarily have to be let me just double check here um yeah we'll just do 2019 generation 2 here and we'll scroll on down here and I'm going to put an Azure user and then we'll do um uh capital T testing 1 2 3 4 5 6 capital T testing 1 2 3 4 5 6 scroll on down we are definitely going to want Port 3389 open um we'll open up these two ports as well I think that's okay and we'll go ahead hit review and create well actually before we do that I don't think I choose the right size so um we'll just wait a moment here as it's complaining and we'll change this because you can't have a B1 LS we're going to have to go with um uh dsv uh SV uh DS V3 okay because it has two V vcpus at the minimum actually we can go with this one here because it just has to be two and four so I think we can get away with a B2 which is 50 50 bucks a month which we're not going to keep that around for very long we'll go ahead and hit create um we might need actually before that I'm just going to add another disc just in case because I really don't want to have to do this if I have to make another one and so we're just going to say an empty disc here I suppose yeah that seems fine to me and I'm going to go ahead and hit uh yeah we should enable dis now we'll do nothing there and we'll say okay and go ahead and hit review create and we'll just give it a moment takes a little bit of time and go ahead and hit create there and while that's creating now we'll go over and make ourselves a storage account so I'll make a new one here and we'll call it uh we'll put it in our keos and we'll call it keos we'll stick with Standard General 2 that's totally fine for our use case we'll go ahead and hit create and this will not take long it's very fast to making storage accounts virtual Windows Virtual machines not as fast so we'll just give it a moment here and it looks like it's created so we'll go to the resource and then under file shares we'll click on that we'll create a new file share and this will be called keos and we'll set 5 gigabytes because we really don't need a large one we'll create it and so this can be used for Windows or Linux uh in this case we're going to be using it for um uh windows so what we can do uh we'll just have to wait for that to finish deploy but we're going to have to set up as your file so if we go files I think we type it in here will it show up no so if the thing is we need to get file sync and it's in a kind of a weird spot so if we go to all services and then we type in files whoops it a little bit too quick there uh I'm just looking for it here uh what if we type in sync there we go so what I'm looking for is storage sync Services can we type it up here sync yeah so that would have been the way I would have gone gone gone and found it there but we need to find uh as your files I'm just trying to remember how to get to there so I could have swore we could type in file sync yeah here it is a your file sync and so this is how we get to it I know it's quite a let me just double check to make sure if that's somewhat if we can actually type that up here file sync no okay so what we would want to do is go ahead and uh create this if we want to take a look we can check the plans this seems okay so we'll go ahead and hit create and under here we're going to put it into keyos and then for the provider name it will be the storage sync service name will just be keyos as well um and I guess we'll stick with Central us I guess it just really depends on where our virtual machine storage account is I hope everything's launching in the same place I don't think it matters but I'm just going to double check here so this one is in Canada east and then our storage account let's just go take a look where it is it is in um Canada east so maybe we could launch this in Canada east if it doesn't give us any grief it doesn't so that's great we'll go ahead and hit create and we'll give that a little bit of time there and if our instance is ready we should go ahead and connect to it because there's going to be a couple things we need to install okay so I'm going to go over to RDP here and we're going to uh download the TP file and that's going to open up that so we'll just double click it we'll say connect if you're on a Mac you got to download the client if you're on Windows it's already it should be preinstalled if you if you're on a Chromebook you'll have to use a Bastian but I'm going to type in Azure user and that password capital T testing 1 2 3 4 5 6 we'll hit okay I'll say yes I'll connect and we'll just give it a moment there to load we are going to need uh two things we need to install but just give me a moment here I'll go fetch them all right so now that we uh I have our Windows machine we're uh inside of it here what I want you to do is open up um Powershell so if you type in Powershell we do need to run this in administrator mode so just type it in it's a little bit slow when you're rping which is totally fine and what I want you to do is right click and run as an administrator uh and what we're going to do is install Azure RM because we need it in order to use um the Azure file sync agent okay so type in install hyphen module and then uh type in name Azure RM and then we'll say allow clobber don't ask me what clobber does I have no idea but that's what Microsoft or Azure tells us that we should uh whoops uh did I spell that right no it's all correct okay that we should do this thing is pretty darn slow so uh we'll just have to wait here a little while um and once that's done we'll we'll continue on to the next part okay okay so after waiting a little bit it asks us the question so we're just going to hit why to everything all right and uh as that's going here um it can take a little bit of time so what I'm going to do if I can even minimize this here um and I did not mean to minimize Azure uh or this here oh the other one already prompted that was pretty fast actually it's going really fast as of today um but as that's going um what we'll do is we'll make our way back over uh to sync storage here because or storage sync we were waiting for this to get set up uh because we're going to have to go uh set up a sync group so I'm going to just call this uh keyos make everything easy we'll stick with that Azure subscription we'll drop this down and we'll choose keyos in here uh and then for the Azure file share we'll choose keyos and once that's created we'll click into that and what we are trying to do is we're adding a cloud uh endpoint here so we would choose our storage account and we would choose uh this here and hit create okay and so now we have a cloud endpoint but we still have to uh continue on with our uh our virtual machine here so just wait until this is done this will take a little bit of time okay all right so now that uh we um uh we have that installed the the power shell for uh resource manager what we're going to need to install is the Azure file sync agent in order to do that we're going to have to go to the internet but before we do that we really want to uh uh turn off a particular feature um so turn back on protective mode I was fiddling with it earlier but if we were to go and it doesn't matter where we go we say Azure file uh sync agent okay and if we were to go see we started getting this pop up and drives us crazy so what we can do because this isn't a big deal it's just for uh testing here we're going to turn that off so um go to your local server where it says I I IE enhanced security configuration click on that and just turn it off you know because it'll drive you crazy and then we'll go back to inter Explorer completely close it reopen it and now uh what we can do is look for that aure file sync so we'll type in aure file sync uh agent download and that's is azure microsoft.com maybe later for that experience it's easier if you go to the Microsoft site and uh if you go here just give it a moment scroll on down click on the download button and we want uh 2019 because that's the server we launched right and that and we're trying to uh sync with storage sync so we'll go ahead and hit next this is 50 megabytes so it shouldn't take too long um and we're going to wait for it to download if it doesn't uh well we'll say allow once for the site here so maybe it was trying to download yeah there it goes and we will save it and we will always allow for Microsoft here it's totally fine and it's already finished downloading so we'll go ahead and run that and just give it a moment here we'll hit next we'll accept the terms uh we'll install it there that's totally fine we'll use the existing proxy uh configuration Uh custom configure proxy well we don't have any custom so we leave it alone we'll leave Microsoft updates on uh it' probably be good to checkbox that on but I'm just going to go install next here all right and so we'll let the agent install usually doesn't take too long any I'll see you back here in a moment all right so now that we have our St agent set up we'll just hit finish here uh and I don't know why it opened a bunch of windows but that's just what it does and so we're just going to wait for for it to pop back up here and then we'll say okay and then what it'll want to do is connect so it says Azure environment uh so it is azure Cloud right and I'm just going to sign into my account and I'm going to try and log in here so let me just go grab my credentials all right so I entered my credentials in here so we'll go ahead ahead and hit sign in and we'll give it a moment there we're going to choose our subscription we will choose our Resource Group which is keyos and our storage sync there hit register and I'll see you back here in a moment all right so it looks like our registration was successful so that means that we should be able to create a server endpoint uh and go from there okay so I'm going to just go ahead and hit close and uh I'll come back here to you in a moment all right so now what we're going to do is set up a folder for uh that that we're going to want to uh uh be synced okay because the idea is to back up or sync that directory there so uh what's going to happen here is we're going to make our way down to a file explorer and from this PC I'm just going to go to the C drive I made that other drive because I assumed we were just going to throw it on there but honestly I'm just going to put it in the window or the C drive here we'll make a new folder called keos we'll make it all lowercase and inside of here I'm just going to make a new file here called hello.txt nothing super exciting um and so uh probably we need to turn on sharing so that's just a habit of mine so we'll go to properties here sharing share and we'll just make sure that Azure Azure user has both that and it'll ask us to turn network discovery on we'll say okay keos keos okay I can't remember if we have to turn that on or not but uh you know as long as we get this working that's the most important part here so now that we have that going what we want to do is make our way back to um Azure here and so we had our uh what was the sync sync storages and what we'll need to do is go into our sync storage once it ever lets us uh get there we'll click on keyos and we'll go into sync groups we'll click into our keyos sync group we have a cloud endpoint we'll need to create an add server endpoint drop down here we have a registered server so you know how we typed in our we typed we typed in our we logged into the Azure file sync agent that's how this registered server is showing up we're going to give it a path so this is going to be C back SL keyos right um so this all looks okay and what we'll do is we will say let's just check this here this is all okay and we'll go ahead and hit uh connect all right and so this takes a little bit of time to provision it's going to show up here in a moment if I hit refresh it's provisioning so I'll see you back here in a bit you know I just remembered is that uh we do actually have to turn on cloud tering or it's going to stay pending forever so so I'm going to go to enable here and uh always preserve specific percentage of free space on the volume uh sure we can do 20% here we don't need a date policy here and this all looks okay so we'll go ahead and hit save uh because if we don't do that it's not going to actually it's not going to move it to the cloud right so we'll give that a go oh the health is good um and but we'll just give it a moment here okay so we give it a little bit of time here it says the Point failed which I really don't believe it because I just checked and it was working totally fine but what I want to do is show you if I go over to Microsoft Azure here and we go over to uh storage accounts and we go to keyos and we go into file shares and we click into this one we can actually see the files here so it clearly is syncing uh I'm not sure why we got that error I've never seen that error before um cloud is not supported for the specified path well that's fine okay so if the server path didn't work maybe it's the cloud cloud endpoint but but generally that is the workflow to uh get uh syncing working so you pretty much have all the working knowledge you need um but uh what we'll do is go ahead and tear all this stuff down because we are 100% done so I'm going to go to our resource groups here and uh what we'll do is go into keyos and we'll just make sure that everything is there even the storage sync service I just wanted to make sure it was all there and what we'll do is go ahead and write in keyos and go ahead and delete and that should take everything down no problem all right so there you go all right so just one more thing uh I was doing that cleanup and then today I woke up the next morning and when I checked my resource groups the keyos was still here so I went in here and we still had uh the storage sync service so what you're going to want to do is go in here and I think what it wants you to do actually tell us the message here if we go delete Resource Group we type in keyos and uh it will complain that uh it's it still need you have to get rid of the um the resources within it so I already know that's going to fail um but what I want you to do is delete the sync group a so if you select the sync group here and go ahead and delete that we might also have to delete the end points first usually uh usually a is really great about tearing everything down but in this case uh it's not giving us a lot of help here so we'll just say um yeah we want to delete this end point too let's delete all the server so you got to first delete the server endpoints to delete the cloud endpoint to delete the group then to delete the actual uh service so it's a little bit um convoluted but uh you know I guess for whatever reason this one is not automated like the other one so just go through those process make sure that you get everything deleted out okay um and then you should be in good shape okay hey this is Andrew Brown from exam Pro and in this section we'll be covering the core backup and Disaster Recovery Solutions in Azure starting with Azure site recovery Azure site recovery is a hybrid backup solution that facilitates sight to sight recovery from on premises to the cloud ASR is a critical component for your business continuity and Disaster Recovery strategy site recovery replicates workloads from a primary site to a secondary site in the event the primary site encounters a failure site recovery will fail over to the secondary site to ensure continuity of services Azure site recovery can replicate Azure VMS between different regions also known as cross region replication various OS such as Windows and Linux on premises to Azure seamlessly transition from your local servers to the cloud between other cloud service providers such as AZ to Azure different machines including VMware hyperv or physical machines recovery time objectives this is the amount of time your business business can afford to have its systems unavailable or offline before it significantly impacts your business recovery Point objectives this is the maximum amount of data loss your business can tolerate measured in time before a disaster occurs this overview highlights Azure site recovery a key tool for protecting business operations against unexpected disruptions the next topic we'll be covering is Recovery Solution for Azure site recovery a Recovery Solution for Azure hybrid and on premises workloads should be designed to meet specific recovery objectives including recovery time objective recovery Point objective and Recovery level objective recovery time objective RTO is the maximum amount of time that an application can be down before it starts causing significant business disruption a Recovery Solution should be able to restore the system and data within the specified RTO time frame for example if the RTO is 1 hour then the Recovery Solution should be able to restore the system and data within 1 hour of an outage recovery Point objective RPO is the amount of data loss that is acceptable after an outage The Recovery Solution should be able to restore the system and data to the required RPO level for example if the RPO is 1 hour then the Recovery Solution should be able to restore the data to the most recent point in time within 1 hour of the outage recovery level objective R is the level of recovery that is required after an outage the rlo can vary depending on the type of data or application being recovered a Recovery Solution should be able to restore the data or application to the required rlo level for example if the rlo requires a point in time restore The Recovery Solution should be able to restore the data to the specific point in time when recommending a Recovery Solution for Azure hybrid and on premises workloads the following factors should be considered business requirements The Recovery Solution should align with the business requirements including the RTO rlo and RPO objectives these objectives should be identified during the planning phase workload types different types of workloads may have different RTO rlo and RPO requirements The Recovery Solution should be able to accommodate the requirements of each workload type hybrid or on premises The Recovery Solution should be able to handle hybrid or on premises workloads depending on the specific requirements of the organization data protection The Recovery Solution should provide data protection including backups and replication to ensure that the data can be recovered in case of an outage testing the Recovery Solution should be tested regularly to ensure that it meets the RTO rlo and RPO objectives testing should be conducted in a controlled environment to avoid any negative impact on production systems by considering these factors you can enhance your Azure site Recovery Solutions ensuring Swift and efficient data recovery to maintain business continuity in this section we're going to explore an example of azure site recovery architecture This Disaster Recovery Solution utilizes Azure site recovery alongside other managed services like traffic manager and virtual Network this combination provides a costeffective and high availability environment suitable for small to medium businesses this allows companies to focus more on their core Solutions benefiting various Industries including Health Care travel and hospitality and Manufacturing for example it can be used in portable healthcare clinics restaurant chains and local Logistics and Supply chains here is a breakdown of the architecture traffic manager this Azure service wrotes DNS traffic enabling easy redirection from one site to another based on policies set by your organization Azure site recovery this service handles the orchestration of machine replication and manages the setup of failback procedures virtual Network this is the location where the failover site is established when a disaster happens blot storage this is where the replica images of all machines protected by site recovery are stored so that's an overview of the example Azure site recovery architecture shown the next topic we'll be covering is the Azure Backup Service another crucial tool for backup and Disaster Recovery Solutions Azure backup service is a Cloud solution from Microsoft that offers secure scalable and simple data backup in recovery across various Azure Services you won't find it by searching based on the service name however Azure backup is seamlessly integrated within numerous Azure Services making its operation quite intuitive the five core components of azure backup Mars agent helps in backing up files folders and System state data from on premises machines and Azure VMS to a backup Recovery Services Vault and Azure Recovery Services Vault manages and organizes your backups in a cost effective secure and scalable manner Azure backup server MBS used to backup on premises data to Azure for hybrid protection VM extension allows the backup of azure VMS without the need to deploy any additional agents backup policy defines when and how your data is backed up what can be backed up on premises Azure VMS Azure manage discs Azure file shares SQL Server sap handed databases Azure database for postgressql servers and Azure blobs why use Azure backup offload on premises backups saf card your data by moving backups to Azure backup Azure iOS VMS ensure your Azure VM data is protected scale easily adjust your backup storage size based on your needs get unlimited data transfer no limits or charges for data trans transfer keep data secure ensure your data is secure both at rest in and Transit centralized monitoring and management have a unified View and manage your backups easily app consistent backups restore applications back to a precise State automatic storage management no need to manage Backup storage explicitly multiple storage options choose between different storage options based on your needs so that's an overview of azure backup service the next topic we'll be covering is azure Recovery Services Vault Azure Recovery Services vault is a storage entity in Azure that stores backup copies of data and configuration information over time this data can be related to various Azure resources like virtual machines workloads servers or workstations backup for Azure Services RS Vault protects data from various Azure services including iOS VMS Azure SQL databases Azure blob storage Azure file shares and Azure functions recovery Services Vault supports various platforms including system center data protection manager Windows server integrates with Azure backup server for application protection and other platforms Recovery Services vaults has the following features enhanced security Azure Recovery Services Vault encrypts data in transit and at rest for secure backup and restore processes Central monitoring the Azure portal allows centralized monitoring of all backup and restore tasks across hybrid environments azure rolebased Access Control Azure rbac enables granular access control and management of Recovery Services vaults soft Elite this feature retains backup data for an additional 14 days after deletion protecting against accidental or malicious data loss cross region restore crr enables data restoration in a secondary region during a disaster in the primary region ensuring business continuity so that's an overview of azure Recovery Services vault next let's explore the Microsoft Azure Recovery Services agent the Mars agent plays an important role in backing up files folders and the system state from Windows based on premises machines and Azure VMS all backups facilitated by the Mars agent are securely stored in a Recovery Services vault in Azure Mars agent is also known as the Azure backup agent note that the Mars agent does not support Linux operating systems to get started with utilizing the Mars agent for backup purposes follow these steps create an Azure Recovery Services Vault this Vault will store all the backups create a backup policy within the Vault set the terms and conditions for backups ensuring they align with your requirements configure secure rub for backup depending on your security and performance needs you might opt for Pathways like Express routes or private end points download the Mars agent download the agent from Azure install and register the agent to your Windows machine once installed register it for Activation and functionality overall the Mars agent streamlines the backup process for windowsbased systems ensuring that data is both safely stored in Azure and swiftly recoverable when needed next let's explore the Azure backup policy Azure backup policy allows users to Define and configure how data backups are managed on the Azure platform it lets you set the frequency retention duration and type of backups to ensure data protection and meet organizational requirements creating a backup policy and azure inv involves a few essential steps select a data source type choose the type of data you want to back up such as Azure virtual machines or posters ql data base determine the frequency decide how often you want the backups to occur this could be daily weekly or any other frequency that suits your needs set the retention details determine how many snapshots or backup copies you wish to retain and for how long choose the time range for retention specify the time range during which you want to retain the backups this setting helps in managing the life cycle of the back up data efficiently in summary Azure backup policy allows you to set customized backup in retention settings for Reliable and consistent data protection the next topic we'll be covering is azure VM backup Azure VM backup is a solution for backing up and restoring virtual machines running on Azure it allows organizations to protect their virtual machines and their data against various issues such as accidental deletion Hardware failure ransomware and other for forms of data loss how it works Azure VM backup utilizes Azure backup to offer a centralized backup solution manageable through the Azure portal Powershell or rest apis it supports both windows and Linux virtual machines allowing for backup and restore operations at both the dis and VM levels multiple backup options are available including full backups incremental backups and differential backups backup retention policies enable organizations to retain backups for specified durations enhancing data man management and compliance ke features the solution employs the robust infrastructure of azure backup ensuring encryption at rest and support for backing up VMS across different regions and availability zones the integration with Recovery Services vaults further strengthens the protection and management of backup data Disaster Recovery Beyond backup Azure VM backup bolsters Disaster Recovery strategies by enabling the replication of virtual machines to a secondary region this redundancy is vital in mitigating the impact of outages or disasters organizations can seamlessly fail over in fail back virtual machines ensuring minimal downtime and enhanced business continuity in conclusion Azure VM backup stands out as a resilient and dependable backup in recovery solution for Azure hosted virtual machines hey this is Andrew Brown from exam Pro and in this follow along we're going to be looking at backup solutions for Azure so let's make our way all the way to the top here and launch ourselves a virtual machine that we are going to be using uh as a means to back up so what I'll do is create a new group here um and we will call it peard uh and we'll call this virtual machine peard and scrolling on down here yeah I want to stick with a Windows Server 2019 Gen 2 so go over here select it uh as generation 2 there and for the machine you want to make sure it's a b2s cuz you need at least two vpcu and 4 GB Ram to launch a Windows uh server there for the the user is going to be Azure user for the password capital T testing 1 2 3 4 5 6 capital T testing 1 2 3 456 uh we'll have RDP uh I'm not sure if we'll end up using it uh but if we do we have that open we'll go ahead and go to the review page we'll wait for it to allow us to hit create give it a moment great we'll hit create and then we'll give it a moment to see that it is deploying and if it's deploying I'll see you back here in a moment great so it looks like our virtual machine is ready to go here so what we'll do is go all the way to the top here and type in backups we can go to vaults directly but let's take a look here at the backup center uh where we can kind of have a bit of an overview of stuff so the idea is we have vaults where we're going to store our backups we can create backup jobs backup policies so let's get to it and and first create ourselves a vault so we'll go ahead here and create ourselves a new Vault and we have two options we have Recovery Services Vault or a backup Vault so uh the difference here is that one is just for backing up things like databases discs and Etc where Recovery Services is more like a like a sitewide recovery so you're going to notice it includes virtual machines and all these other stuff uh generally you'll probably want to go with Recovery Services Vault because it is just basically a new version of the backup Vault so let's go ahead and do that and uh we'll place this in our Bard um a resource Group there and we'll just say peard backup or vault Vault there we go and we'll go ahead and hit review and create and we'll go and hit create there and we'll just give it a moment there to deploy all right so our Recovery Services vault is ready and we'll go ahead and click back up and down below ask us what kind we are running so we have Azure stack and on premise it's just Azure and we have a virtual machine so we'll go ahead and hit backup and notice that we'll have to set up a backup policy and here we have a default one but let's actually go and make a new one so we can kind of learn some of the settings that we can set so we can say the frequency the time Etc probably want to set this to my time zone so I'm in Toronto if I can find it here which I believe is 5 uh I was trying to type to see if I could do that but I wasn't uh getting uh getting there too well um we'll just say central Canada there that's okay it's off by an hour but that's close enough um and you can have weekly monthly and yearly backups but we're going to stick with the daily backup for 30 days um and this is okay we don't have to enter that in we'll hit all right okay and now we can add our virtual machine so we'll go here and select the card hit okay enable backups please select at least one that has backups enabled so what we'll do is open up our Azure tab here make our way over to our virtual machine and just make sure that that stuff is enabled so we'll go over here um and we'll go to backups and here we can actually even set set it from here so I guess we'll select our existing Vault here it'll say enable backups but that set it up with a default one right so I'm a bit surprised we couldn't do it that way but we'll make our way back here as long as it works that's the thing with Azure it's just you know you'll do something and then you'll find that uh you have to do it slightly different uh but we we'll just wait until that's done there so after waiting a very short while uh it looks like backups are turned on so if we were to go to our overview and go to backups you can see we have one backp item if we go over to our backup items we'll see under virtual machines we have Bard um we did set the default policy so I guess if we wanted to switch that out we probably could if we just go back and let's just go create a policy of our own just so that we are familiar with that there um and we'll say an Azure virtual machine one so my special policy okay and um this is for 180 days we go ahead and hit create and once that policy is deployed we'll be able to associate I can't remember if this will be super fast but we'll give it a moment here yep it is super fast and then once uh we have that poliy we can go into it and then start assigning things to it actually I think when I made the lecture content I don't even remember there being a backup center so to me this is all a little bit new um so maybe I'll go back and up to the slides cuz I I usually say there is no centralized service but they have one now so um that's uh pretty darn good um so for Associated items we don't fetch any data for services completed um so I'm just trying to figure out how we can change that policy there if we go to backups we just change it over here uh I'm not sure how to switch out the policy but I know at least if we want to do backup now we just press that there it's really not that important to know how to swap out a policy just more so the fact that you know what a backup policy is and you can apply to machines and things like that yeah so there we go we just had our own backup there and uh yeah there's not much else here to look at but uh yeah there you go set backup policies so I guess what we'll do is go ahead and tear all this down and so that was our book card there we'll go ahead and delete this Resource Group and there you go hey this is Andrew Brown from exam Pro in this section we're diving into an introduction to Azure kubernetes service or a KS Azure kubernetes service simplifies the process of deploying a managed kubernetes cluster in Azure one of the main benefits of using a KS is that Azure will take over the management of the kubernetes master notes for you including Health monitoring and routine maintenance your resp responsibility lies only in maintaining the agent nodes an additional benefit is the cost effectiveness of a KS the service itself is free you only incur charges for the agent nodes within the cluster not the Masters when you deploy in a KS cluster both the kubernetes master and all nodes are deployed and configured for you during the deployment process you can also configure additional features such as advanced networking Azure active directory integration to use kubernetes Ro based Access Control monitoring capabilities and Windows Server containers are supported in a KS so when should one opt for a KS well a KS is ideal in scenarios where you need full container orchestration this includes situations demanding seamless service Discovery across multiple containers automatic scalability to handle varying loads and coordinated and smooth application upgrades in summary a KS offers a streamline managed solution for container orchestration in Azure simplifying the setup and maintenance of a kubernetes cluster the next topic we'll be covering is called bridge to kubernetes bridge to kubernetes is an extension available for both visual studio and visual studio code that allows developers to write test and debug microservice code directly on their local development workstations with bridge to kubernetes you can integrate a service running locally with yours cluster this bypasses the need to create Docker and kubernetes configurations for the lifetime of this connection a proxy is added to your cluster in place of your kubernetes deployment that redirects requests to the service to your development computer when you disconnect the application deployment will revert to using the original version of the deployment running on the cluster it's important to note that Azure Dev spaces will be retired on October 31st 2023 existing users are encouraged to transition to using bridge to kubernetes as their client development tool ensuring a streamlined and integrated development experience on their local machines while interacting with the ks clusters so that's a brief overview bridge to kubernetes a very useful tool to streamline development and debugging for developers the next topic we'll be covering are the Recovery Solutions for Azure kubernetes service that's practices for business continuity and Disaster Recovery in Azure kubernetes service use multiple availability zones Azure AKs supports multiple availability zones which distribute your application across different data centers this ensures High availability and resiliency in the event of a data center failure when you create a new KS cluster you can choose to create it across multiple availability zones Implement backup and restore backing up your KS cluster ensures that you have a copy of your application and data in case of data loss or corruption Azure AAS supports backing up your application data and kubernetes resources such as deployments and services using the Valero backup and restore tool use Azure site recovery Azure site recovery is a disaster recovery solution that replicates your KS cluster to a secondary location this can be used useful in the event of a disaster or outage as you can fail over to the secondary location and resume operations for monitor your a KS cluster it is important to monitor your KS cluster for any potential issues or failures Azure provides a range of monitoring and alerting solutions such as Azure Monitor and Azure service Health which can help you proactively detect and respond to issues five test your Disaster Recovery plan to ensure that your Disaster Recovery plan is effective it is important to test it regularly you can use tools like azure site recovery to perform failover tests and ensure that your KS cluster can be recovered in the event of a disaster six use Azure kubernetes service with Azure Arc Azure Arc enables you to manage your KS cluster and other kubernetes clusters across multiple clouds and on premises environments from a single control plane this provides greater flexibility and resilience in the event of a disaster or outage by following these best practices you can ensure that your Azure kubernetes service cluster is resilient highly available and recoverable in the event of a disaster routage hey this is Andrew Brown from exam Pro and in this section we'll be covering Azure regions and availability zones starting with Azure regions Azure regions are physical locations around the world where Microsoft as data centers to provide cloud services each region is composed of multiple data centers that are geographically dispersed to provide redundancy resilience and high availability to customers P facts about Azure regions multiple data centers each region is made up of at least one data center but many regions have multiple data centers for added redundancy and availability Global presence Azure regions are distributed around the world covering almost every major continent and country unique identifiers each Azure region is designated by a distinct name such as East us or west Europe which indicates its geographical location resource deployment when creating resources in Azure you can choose which region to deploy them to based on factors such as proximity to users data sovereignty require requirements and service availability data residency data in a region stays within that region unless explicitly copied or replicated to another region for redundancy or Disaster Recovery purposes continuous expansion Microsoft continually expands its Azure footprint adding new regions to cater to emerging markets and customer needs in essence Azure regions are the Cornerstone of azure's global infrastructure guaranteeing High availability redundancy and adherence to Regional data norms the next topic we'll be covering our Azure availability zones Azure availability zones are physical data center locations within an Azure region that are typically located in separate buildings or regions but still close enough to provide low latency network connectivity these zones help protect applications and data from data center level failures by providing redundant Power Cooling and networking and Azure region is a geographic location that contains one or more data centers an availability zone is a unique physical location within an Azure region key points of azure availability zones each availability zone is composed of one or more data centers with independent Power Cooling and networking each data center within an availability zone is connected through a highspeed low latency Network availability zones are designed to provide High availability for critical applications by Distributing them across different zones by deploying applications across multiple zones it is possible to achieve a higher level of redundancy and fault tolerance virtual machines storage accounts and other Azure resources can be deployed to specific availability zones within a region to ensure High availability Azure traffic manager can be used to Route traffic between different zones based on various criteria such as geographic location latency and performance so that's an overview of azure availability zones the next topic we'll be covering are the availability options for Azure virtual machines while we've briefly mentioned these terms in the past we'll now explore them in detail specifically in the context of VMS Azure virtual machines provide several options for achieving High availability and ensuring business continuity here are the primary strategies availability sets an availability set is a logical grouping that informs Azure about application redundancy and availability requirements ideally to ensure High availability and meet azure's 99.95% SLA place at least 2 VMS within an availability set there are no es for the availability set itself costs arise only for the individual VM instances created within virtual machine scale sets these sets enable the automatic deployment and scalability of identical VMS VMS can auto adjust their capacity as per demand facilitating autoscaling for applications VM scale sets are built to integrate with Azure load balancer and application Gateway ensuring traffic is evenly distributed across VM instances VM distribution across fault domains and update domains further fortifies High availability ility Azure site recovery this service delivers Disaster Recovery capabilities for VMS it facilitates VM replication to a secondary location which could be another Azure region a different data center or an on premises site both physical servers and virtual ones including those on hyperv and VMware can be replicated using site recovery Azure backup Azure backup offers backup solutions for VMS their applications and data you can back up to the cloud ensuring backups are application consistent meaning they're usable and can be restored as needed these backups can either be retained in the originating region or transferred to another for Disaster Recovery additionally Azure backup provides extensive retention periods in backup archiving aligning with Regulatory Compliance needs overall these are the main options for availability options for Azure virtual machines hey this is Andrew Brown from exam Pro and in this section we'll be going over Azure virtual machines Azure virtual machines offer a highly configurable server experience through virtualization you can run a server without the hassles and expenses of maintaining physical Hardware however it's important to note that VMS are not entirely maintenancefree they still require OS patch applications and package installations and configurations key points about Azure VMS configuration and size the size of the VM is determined by its image this image defines the combination of vcpus memory and Storage capacity subscription limits as of now there's a limit of 20 VMS per region on a per subscription basis billing Azure VMS are buil at an hourly rate availability a single instance VM offers 99.9% availability when all its storage discs are of Premium quality to achieve a 99.95% availability deploy two instances in an availability set storage you can attach multiple manage discs to your Azure VMS networking components when you launch an Azure virtual machine other networking components will be either created or Associated to your virtual machine including Network Security Group a virtual firewall with rules concerning ports and protocols this is attached to the Nick network interface a device that handles IP protocols and network communication virtual machine instance the actual server that's running public IP address the address that you will use publicly access your VM virtual Network the network in which your VM is located overall as your virtual machines offer a versatile and strong cloud computing space allowing easy and seamless deployment of robust applications and systems the next topic will be covering our operation systems in Azure VMS so what is an operation system well I'm sure you already know this but the OS is the program that manages all other programs in a computer who most commonly know operation systems or Windows Mac OS and liux when you launch a virtual machine you need to choose an image which has a specific operation system Microsoft Works closely with Partners to ensure the images available are updated and optimized for an Azure runtime you can find most of these images in the Azure Marketplace including Sue liex Enterprise server Red Hat Enterprise liex auntu server Debian free bestd Azure Marketplace flat car container Linux Rancher OS thei library for Azure mesosphere DC OS on Azure Docker images Cloud B Jenkins platform if the available options do not meet your requirements you can bring your own Linux by creating a Linux virtual hard disk note that in Azure only the fixed vhd format is supported not the hyperv virtual hard disk format so that's an overview of the operating systems in Azure VMS the next topic we'll be covering are the sizes of azure VMS Azure VMS come in a variety of sizes that are also optimized for specific use cases is azure VMS are grouped into types such as general purposes and compute optimized and sizes such as B and dsv 3 also called series or SKU family general purpose balanced CPU to memory ratio testing and development small to medium databases and low to medium traffic web servers skus B dsv 3 DV3 D4 D4 dsv 2 and so on compute optimized High CPU to memory ratio good for medium traffic web servers work appliances batch processes and app servers skus ffs fsv 2 memory optimized High memory to CPU ratio best for relational database servers medium to large caches and in memory analytics has K's S3 F3 east4 E4 F4 S4 and so on storage optimized offers High disk throughput and IO ideal for Big Data SQL and osql databases data warehousing and large transactional databases skus lsv2 GPU specialize VMS for heavy graphic rendering and video editing model training and inferencing with deep learning available with single or multiple gpus skus NC NC V2 ncv3 and cast T4 V3 ND and so on high performance compute features the fastest and most powerful CPU virtual machines with optional High throughput network interfaces skus HB hbv2 HC H there are previous series of virtual machines sizes not shown here like basic a the type of image May limit you to specific VM sizes use Azure sorting and filtering options to explore sizes based on various parameters such as cost Azure compute unit provides a way of comparing compute performance across Azure skus ACU is standardized on a small VM aside the value of 100 all other skus then represent approximately how much faster that SK you can run a standard Benchmark let's break down the provided information information A1 a4u family a CU vcpu 100 ACU stands for Azure compute unit which is a measure of the relative computational performance of different Azure skus and this family each virtual CPU is rated at 100 acus vcpu core 11 this means for every virtual CPU you allocate it corresponds to one physical core D1 D14 SKU family SCU vcpu ranges between 160 to 200 50 this indicates that VMS in this family have a higher computational performance per vcpu compared to the Asser vcpu core 11 similar to the ass series each virtual CPU corresponds to one physical core in summary knowing Azure VM sizes helps pick the best option for your needs ensuring efficiency and cost Effectiveness the upcoming topic explores hyperv focusing on the distinctions between Generation 1 and generation 2 hyperv is a hardware virtualization product from Microsoft enabling the creation and management of virtual machines each VM functions as a separate computer equipped with its own operating system and software applications in many ways hyperv mirrors the functionalities of virtual box there are two generations of hyper vvms Generation 1 supports a wide range of guest operating systems generation 2 primarily support 64bit versions of Windows alongside more recent versions of litx and free bestd operating systems Azure offers both Generation 1 and generation 2 VMS aligning with hyperv and structure but bearing distinct characteristics key differences between Azure gen 1 and Gen 2 gen one based on bios architecture Gen 2 utilizes ufi based boot architecture enhancing Boot and installation times only Gen 2 VMS employ secure boot ensuring the boot loader is authenticated by a trustworthy Source gen 2vm support a substantially larger boot volume of up to 64 terab hyper vvms are packaged as either vhd or vhdx files consolidating their structural components for efficient management and deployment in conclusion understanding hyperv Generations AIDS in optimizing virtualization and ensuring OS compatibility the next topic we'll be covering is SSH RDP and Bastion Azure virtual machines offer multiple methods to connect including SSH RDP and Bastion let's dive into each secure shell is a protocol to establish a secure connection between a client and server this is used to remotely connect to your Azure VM via the terminal S sh operates on Port 22 via TCP RSA key pairs are commonly used to authorized access remote desktop protocol is a proprietary protocol developed by Microsoft which provides a user with a graphical interface to connect to another computer over a network connection this is how you can remotely connect to Windows Server via virtual virtual desktop RDP operates on Port 3389 utilizing both TCP and UDP Bastion Azure Bastion is a service you deploy that lets you connect to a virtual machine using your browser and the Azure portal it provides secure and seamless RDP SSH connectivity to your virtual machines directly from the Azure portal over TLS a Bastion is a hardened instance that is monitored users connect to this VM which then establishes a connection to the Target instance sometimes known as jump so to ensure op optimiz security and connectivity it's important to understand these methods we'll delve deeper into each and subsequent sections let's go into a bit more detail with SSH secure shell or SSH is a cryptographic network protocol commonly employed to securely access and manage servers remotely it is very common to use SSH key pairs as a means to authenticate to your VMS SSH key pairs is when you generate out two keys a private key this is the key that remains confidential and should never be shared or exposed it is stored securely on your local system and is used to initiate a connection to the VM a public key as the name suggests this key can be shared publicly it is added to the servers or vm's authorized Keys list how does SSH key authentication work when you attempt to SSH into a server your system uses the private key to send a cryptographic proof the server which has the corresponding public key verifies the authenticity of the cryptographic proof a if the proof is verified meaning the keys have matched your authenticated and granted access advantages of using SSH key pairs security SSH Keys provide a more secure method of authentication than traditional passwords without the correct private key unauthorized access attempts are effectively thwarted convenience once set up users can connect without needing to remember and input a password each time automation automated scripts and services can use key pairs to establish connections without human intervention so that's a more indepth look into SSH let's dive into the remote desktop protocol commonly known as RDP RDP is a protocol developed by Microsoft that allows users to remotely connect to Windows systems when you want to use RDP to access your Windows Server you'll first need to download the RDP file for Windows 10 users the remote desktop client is preinstalled so there's no additional software to download however if you're using Mac OS you can easily get the Microsoft Remote Desktop app from the Apple Store once you've got the necessary tools simply open the downloaded RDP file during this process you'll be prompted to enter the username and password that you set up during the creation of your VM on the Azure portal RDP uses encryption to secure Communications ensuring confidentiality it also supports features like audio redirection clipboard sharing and printer redirection for a seamless remote experience in conclusion RDP provides an efficient way to remotely access Windows those systems the next topic we'll be covering is azure Bastion Azure Bastion serves as a secure Bridge enabling you to connect to your server via SSH or RDP without exposing it to the public it will provision a webbased interface for both RDP and SSH eliminating the need for external clients this can be especially useful for devices like Google Chromebooks which might not support traditional RDP clients when setting up azure you'll need to add a dedicated submit to your virtual Network named Azure basan submit this submit should have at least a 27 size equating to 32 addresses if you have a Windows Server which requires RDP and have a Bastion in the same vnet you simply enter in your user name and password as you normally would if you have a Linux server you can SSH with the Bastion you can use SSH private key or password that you set when you created your VM key benefits single click access Rd p and SSH sessions available directly through the Azure portal secure session Azure Bastion offers RDP SSH over TLS with support for TLS 1.2 plus no public IP needed RDP SSH connects via the vm's private IP simplified NSG management no need for nsgs on the Azure Bastion subnet allows RDP SSH solely from Azure Bastion managed service Azure Bastion is a fully managed secure platform for RDP SSH protection from scans VMS aren't exposed to the internet preventing Port scanning centralized hardening Bastion at the vnet perimeter eliminates individual VM hardening so that's an overview of azure Bastion let's take a look at a comparison between Windows versus liux servers Azure VMS offer the flexibility to run both windows and liux based servers Windows licensing to run Windows you'll need a valid license if you don't act activate it certain features may be restricted Azure does offer a way to leverage existing licenses through its hybrid benefit program authentication typically you set up a username and password during the VM creation instance size to smoothly operate Windows you typically need a larger VM size starting at least with a B2 due to its comprehensive desktop environment environment Windows provides a full desktop environment complete with a graphical user interface making it more intuitive for those familiar with the windows ecosystem l licensing most Linux distributions are open source and don't require any licensing fees authentication Linux offers flexible authentication you can set up a username and password or more commonly use SSH key pairs for a more secure connection instance size Linux servers especially those without a graphical user interface have minimal system requirements this means you can run them on smaller VM sizes conserving resources and cost environment traditionally Linux systems operate with a terminal Bas and environment although there are distributions with graphical interfaces overall your choice between Windows and Linux will largely depend on the specific needs of your project familiarity with the operating system licensing costs and desired system resources the next topic will be covering is update Management in Azure update management allows you to manage and install operating system updates and patches for both windows and Linux virtual machines that are deployed in azure on premises or with other Cloud providers when you launch an Azure VM you can go to operations and turn on guest plus host Updates this will install the Microsoft monitoring agent that will be used to monitor your instances Azure automations is the underlying service that is installed the agent update management will perform a scan for update compliance by default a compliance scan is performed every 12 hours on Windows and every 3 hours on Linux it can take between 30 minutes and 6 hours for the dashboard to display updated dat data from managed computers in Azure automation you can enable the update management change tracking and inventory and start or stop VMS during off hours features for your servers and virtual machines these features have a dependency on a log analytics workspace and therefore require linking the workspace with an automation account so Azure update management offers a comprehensive solution for ensuring your virtual machines are always up to date with the latest patches and updates oh right let's take a look at Azure virtual desktop formerly known as Windows Virtual desktop Azure virtual desktop on Microsoft Azure is a desktop and app virtualization service that runs on the cloud Azure virtual desktop works across devices like Windows Mac iOS Android and LX with apps that you can use to access remote desktops and apps you can use most modern browsers to access Azure virtual desktop hosted experiences use Azure virtual desktop for specific needs like when security is a concern because all day data is saved on the server and cannot be left on the device of a user key features and benefits enable secure and productive remote work on any device Azure virtual desktop provides full Windows 10 and Windows Server desktop and application virtualization on any personal device seamless integration with Microsoft 365 apps for Enterprise and Microsoft teams reduce cost of Licensing and infrastructure use eligible windows or Microsoft 365 licenses to access Windows Virtual desk top and pay only for what you use protect against outages to stay productive help keep your team running during outages by leveraging builtin Azure site recovery and Azure backup Technologies simplify it Management Windows Virtual desktop manages the virtual desktop infrastructure for you so you can focus on users apps and Os images instead of hardware and maintenance Keep application and user data secure easily apply the right access controls to users and devices with Azure active directory conditional access ACC so that's an overview of azure virtual desktop hey this is Andrew Brown from exam Pro and we're going to be launching our own Bastion using Azure Bastion services so there's two ways to set this up we can go to bastions over here and create a Bastion this way or we can create one uh after we've created a virtual machine I prefer the ladder so let's go ahead and do that and launch ourselves a new virtual machine and so we can either use launch a Windows server or a Linux server today I'm going to be launching a Windows server and uh what we'll do is go down here I'll make a new group we'll call it the Enterprise and uh as we do that we'll just name this uh Enterprise D and we'll launch that in Canada C or us or Central us that's fine with me uh 2019 data center Gen 2 is totally fine if you go here you're trying to find it you go hit select and we'll choose Gen 2 Data Center it is expensive but uh we're not going to be using this for very long uh for the username I'm going to put uh data and for the I'll just make it Azure user to make our lives a bit easier and then we'll put testing capital T 1 2 3 4 5 6 testing 1 2 3 4 5 6 and we will go down below we're fine with the settings here we're going to go next to dis we're going to leave the dis to premium that's fine we'll let it create a new network that's totally fine management is okay and we'll just actually go hit review and create and now we'll just hit create so that it will go ahead and do that it'll tell us that it's in progress and we'll just wait a little bit here I'll see you back in a moment all right so our instance is ready so let's go ahead and go to this resource here and then on the left hand side you'll have connect and so I'm going to going to show you and you don't it's not necessary for you to do uh this step because you're going to I'm going to show you how to connect VI the Bas in but I'm going to go ahead and download this file the RDP file and this will only work if you're on windows by the way well I guess it will work on Mac but you'd have to install the um uh the uh RDP uh service for that users with Windows there and so here this is azure user we're going to type in testing with a capital T just double check that there I'm going to log in make sure that this works uh do we'll do that one more time oh you know it's testing 1 2 3 4 5 six there we go we'll say yes and we'll just make sure that we can uh remote desktop into this just before anything else and there we go so that's all good to me I don't need to see anymore we'll go over to Bastion we'll say use Bastion and this is going to set up a Bastion service in order to use Bastion you need to have another um uh address space uh defined for it it makes it really easy to uh make it here so I'm just going to go 10.0.1 do0 24 and we'll go ahead and hit okay and so down below it's going to choose an address space um we have a security group um I'm just going to put it for none I don't think I want one on that and if we scroll on down here we have the resource Group so we're going to put it in the same Resource Group and we'll go ahead and create that so now before this they didn't have this really nice wizard you used to have to go and and create all those things individually in your virtual network but this is really nice it does take a bit of time for this to provision so I'll see you back here in a bit that took a bit of time for that to create that Bastion but it is ready to go and so now that we have it we can go ahead and utilize uh this connection here and so right away it I think it's setting up for RDP here so what we'll do is type in Azure user uh and then capital T testing 1 2 3 4 5 6 we'll go ahead and hit connect and so notice that I didn't have to use an external application I could just uh run it in right here it's all in the web browser so that's pretty much how uh the Bastion works I can't remember the pricing on Bastion I think it's a little bit of money so I don't want to keep this uh laying around here but this is great if you let's say you're on a Chromebook which are becoming really popular where you can't install native applications uh or you're just having issues because you're on like Linux or something like that so there you go that's all there is to it we'll go ahead and clean this up and so I'm just going to go here find the resource Group and we'll go ahead and delete I'm just making sure that bastion's within there so it is good and there we go so we just launched a virtual machine for Linux now let's go ahead and launch one for Windows so I'm going to go to the top here and type in Virtual machines we'll go to the first link I'm going to hit add add virtual machine and uh what we'll do is we'll create a new group the last one I had was called beor I'm going to call this one kardashia kardashia and I'm going to name this uh machine also named kardashia and this time what we want to do is is we want to move over to a Windows Server I find the easiest one to learn with is the Windows 10 Pro server uh just because I find these ones a little bit daunting so I'm going to go Windows 10 Pro uh and then what we're going to do is go choose a larger size this is not going to work we cannot run a Windows server on a B1 LS so we're going to have to go a little bit larger and uh we don't have to go too much larger here but the idea here is that there's going to be a a more expensive spend here so we're not going to be wanting to keep this running for long but here we have the b2s that is the appropriate size to run this anything smaller I don't think is going to work and we are going to put in a password here so I'm just going to put in kardashia and we'll do kardashia uh one two3 put a capital on it I guess we'll just do this here and I'm just going to go back and lowercase this one and uh we're going to allow the inbound Port of 3389 because that is what RDP needs are going to confirm that I have a Windows license I actually don't but the thing is you can still launch one for uh your test purposes it'll it'll just complain saying you're not activated uh so there are some limitations but it's good enough for us to uh learn okay and so now that that is all great we'll go next to diss we're going to go with pre or standard SSD this time uh we are going to go ahead and hit uh next and go to networking it's going to create us a new vnet which is a great idea we're going to let it create a uh a network security group on the Nick just like before we'll go ahead and hit next uh we'll leave all these options alone this all seems fine to me uh and we'll hit next review and create and we'll go ahead and create This Server all right and so that's going to go ahead and create it so I'll just see you back here in a moment when that's uh done deploying all right and so after a short little while here it looks like our Windows server is now deployed so what we can do is go to that resource and if you want to to see what it's deployed it's the same stuff as always you have your network interface card your virtual Network NSG the IP address but let's actually go to that resource now and so let's see how we can gain access to this virtual machine and so what we can do is use RDP luckily I am on a Windows machine and so um I already have the rdb client uh that I can use so all I got to do is download the rdb file and then once we have that file I can just double click it and I can open this up if you're on a Mac uh you can download uh the app in the app store and so I'll go ahead and type in my password so my username was kardashia and then my password was capital c a r d a SS I A1 23 we'll hit okay and then it'll give us another warning we'll say yes and now we are in our virtual machine so there you go how cool is that I'll give it a moment to load up but this is a full uh Windows 10 Pro uh and as I said before you know we don't actually have a license so if you're afraid of spining up because you think you're going to get charged a license fee uh for Windows you do not have to worry that's not going to happen you have to do some manual intervention uh for that to happen so we'll just wait a little while here for this to load um it is not we're not using the most powerful machine so it does take a little bit of time and so we just hit accept here and here we are so we are on uh we have our nice Windows machine here whoops I don't know if it has any games let's go take a look maybe we play M sweeper um no maybe maybe you have to download in the store I'm not that familiar with Windows machines but um so there you go so we'll go ahead and close that and you know if we were using the Bastion it's the same process you saw how we used it with SSH but if we had the Bastion and it's so much work to set one up we already did that before uh but all you do is enter your credentials in on the page just as we did and it and it's just a lot easier that way uh so let's go ahead and just tear down this machine we're all done with it so I'm just going to hit um uh Delete and uh if we find that Resource Group we should be able to easily delete them all I find the easiest way is to go up here go to all resources and then there's the resource Group there and then hit delete Resource Group and then I'll type in the name of it which is kardashia and I'll delete all those resources but after that's done always just take a double check uh on your all resources Tab and just make sure that those resources are gone because sometimes they stick around but there you go that's as simple as it was to launch a Windows machine hey this is Andrew Brown from exam Pro and let's take a look here at virtual machines which I consider the backbone of most cloud service providers and Azure keeps it really simple by calling it um virtual machine so we can go up here and type in virtual machine and make our way over here but right now I don't have any options because I'm using a tenant uh that doesn't have a subscription applied to it so what I'm going to do is go switch back to my original tenant and this one has a sub subscription applied to it and so what I'll do is just click back up here and now we'll just type in Virtual machines and I can now see uh I have options of creating virtual machines so let's go ahead and go create a Linux one first and then we'll go ahead and create a Windows one and then we'll see how we can connect to it all right so first we'll go to the top here and hit add we'll click on virtual machine and we're going to be presented with a lot of options so we'll have to choose a subscription and so there is mine I want to probably create a new Resource Group here I'm going to call this one beour and we'll name this uh vour again and I'm going to launch this in Us East I'll just set it to one availability zone for the time being uh then here we have what we can choose as an image I can click on see all images and choose from a variety of them so if I didn't want to use Ubuntu I could launch something else like Debian or something like that uh but really I just want to uh stick with uh Ubuntu cuz I'm fine with that version with 18 uh then here's what what really matters is the size cuz that's going to affect our cost so if we click on see all sizes we have this uh Nifty um table where we can sort the cost it's just loading in the cost here it's Dynamic this is going to be based on uh what your base subscription is so if you're in Canada you're going to see Canadian prices you're in the US you're going to see us prices etc etc and I care a lot about cost here so I'm just going to sort this by cost and here we have the B1 LS which is very cost effective we have a ram of5 GB and some other options there so we'll go ahead and select that there and we have a couple options we can use SSH public key or we can uh utilize a password and so I think what we'll do is use an SSH public key because that's uh pretty much the standard there we're going to name the username beig or if it lets us probably won't probably want some additional options there oh it's okay oh great and uh we'll go ahead and generate a new key pair and I'm just going to name that one beour and uh we have some options here for inbound rules uh so you could set to none this is just setting up the uh NSG for you uh but we're probably going to want to have that Port open uh for SSH because that's how we're going to make our way back in here if we're running a um like an apachi server we'd want to have Port 80 open so we can go ahead and do that we'll take a look at now diss so here we have an options between premium um standard and standard hhd um I just want this to be cost effective so I'm just going to go with standard HDD uh but generally you you want to have um at least uh standard or premium SSD when you're running uh real web development workloads uh then there's encryption here and so it's always turned on by default which is great they also have this option of double encryption with the platform manage and customer manage key uh we're just going to leave that that as default enable alteris compatibility that's not something we need to do here CU we are not using alter disk and here you can see that you can attach multiple uh diss here um so I can go and do that but that's not something I need to do today and some other Advanced options which we do not care about we'll go over to networking and so it's going to end up creating us a new vnet for us and we'll create a new subnet for us and assign it a IP address uh it will also set up a NYX uh network security group so the network Security Group is not going to be applied at the subnet level it's going to be applied at the Nick which is attached to the um dc2 there and so we'll just leave it to basic we're going to allow in inbound ports for Port 80 um and 22 that was carried over uh from earlier we can put this behind a load balancer but I don't think we're going to do that right now we'll go over to management uh we have some additional options here for monitoring uh this is all okay here we can set it to auto shutdown actually I'll leave that alone loone you can also enable backups here we'll go Advanced and now we have this option here for custom data uh I covered a section on cloud and nit and uh they don't call this user data but most other providers will call this user data so we could provide a bash script or additional information here if we wanted to then down below uh there's some host group options we're not going to worry about that and proximity placement group this is really important if you need to have um instances nearby uh I think this is pretty common with um what's it called high capacity workloads H hfc I can't remember the initialism right now but we covered in the core content then we can tag our resource here uh we'll just leave that alone I don't care about tagging too much but generally it's good to tag in practice and then we will get to review and create our server here we'll go ahead and hit create and then we'll have to download our private key so we can utilize it later and so that's downloaded there and now we're just waiting for it to deploy this and I'll see you back here in a moment so we had to wait a little bit there and finally um our deployment is complete and we can go ahead and just review all the things that it created so notice that it created the virtual machine it created a network interface a Nick for us the NSG the network security group the um virtual Network and also a public IP address um when I do cleanup a lot of things I always miss are these IP addresses and I know that um azzure gave me a warning that said hey you're about to spend uh $700 yearly on IP addresses because you weren't releasing them so when we do the cleanup step I'll definitely uh emphasize about deleting those IP addresses and how to go about that let's go take a look at the actual resource now so here we are and uh you can see we have a lot of options the left hand side such as the disk um so we can see the disk options there and there's additional Security Options let's go go take a look at how we can go ahead and connect to the server and so uh there's different options here so we have RDP SSH and Bastion since we are using a Linux machine we're not going to be using RDP that's really for Windows um but the trick here is that um I would need to have a client on my computer to connect like um I think it's called putty if you're on a Linux based machine it's a lot easier uh and certainly I have the uh Linux subsystem installed so I could probably um connect that way but I figured let's just go ahead and connect via Bastion because I think this is a pretty darn cool feature so let's go ahead and create ourselves a Bastion and this will take a little bit of time here but we'll go ahead here and just set up subnet so to associate a virtual Network to a Bastion it must contain a subnet with the Azure Bastion subnet so they actually have a special subnet for it so what we'll do is we'll just go back to our um our server here uh which we called beour and I think we can find the subnet through here so on the left hand side if we go to networking we probably could find it that way um so I'm just looking for that Security Group um in there it should be um maybe it's not there if it's not there um well you know we could just go over make our way over to subnets it's not a big deal cuz it's called beour it's pretty darn easy to find to begin with and so under subnet here what we need to do is add a a special one here and just got to remember how this works um so you need to create a a subnet called Azure Bastion subnet with a prefix of at least 27 so we'll go ahead and add a new subnet and we'll call it that uh we'll take out the space there and the range is 10 0 1 0 Etc so we'll just do it on two 0 do um 2.0 sl27 uh 102 is not contained yeah it overlaps so we'll do two do not contain the virtual Network address space oh right so we have to add the address space first oops we'll just hit cancel here that's okay we'll discard that we'll make our way over to address space and we'll go ahead and add 10.0 uh 2.04 that'll give was a pretty darn uh large range there and so now what we'll do is go back to our subnet and we'll go ahead and create that there and it said only needed 27 so we'll just give it only 27 we don't need to go bigger than we need and that should be okay we'll go ahead and hit save and it shouldn't take too long so now that we have that we can go back here and we'll give give this another go here here uh it's there so it shouldn't be complaining maybe what we'll do is just start from the uh the start here again yeah there you go that the Azure portal is like that a lot where you'll have something set up and it has the Old State of it and so you just have to trust yourself that you know what you're doing and you have to go back but if you don't have a lot of confidence a lot of times you'll get stuck and you'll think okay I don't have it right but always just try again and hit refresh um because the Azure portal is very inconsistent so we're just going to have to wait for this to create this does take uh a little while to create so I'll see you back in a moment so after waiting five minutes our Bastion is now created and so what we can do is without even using a putty client or having to use Linux directly uh we can just uh connect through via the Bastion uh so here uh we'll see we have some options here so we want to do SS H uh private key from local file okay and what we can do is go ahead and select our beor key and then I'll just scroll down here and hit connect oh um and I think we made the username beour and we'll go ahead and connect now and it's complaining about a popup here so we'll go up here and say always allow and we'll try that again and then we'll say allow again and so now that we're into our server here let's go ahead and try to install aachi and see if we can get at least the default page running um so this is using Ubuntu if my memory serves me correctly should be app get install Apachi 2 and we'll just hit Y for continue and we'll just wait for this to install doesn't take too darn long and after a short little wait there uh it finally did install also if you notice this little icon here we have a a little clipboard here um I don't seem to ever use that there so that's fine um now when you install a Pochi we might have to go ahead and start it up um so let's just take a look to see if it actually is in the running directory here so we'll go to cdar www and so that's where the default directory is right um but we can just check to see if uh it's running by doing a PS Ox I think it's httpd or we can say Apache here and so it looks like it's already running so that's pretty great for us and since it's running on Port 80 and we've opened up Port 80 we should probably be able to access that um here so let's go back to our actual virtual machine so we'll go to Virtual machines and uh we have that virtual machine running I'm just going to click into it because I just want to find out it's public IP address so here it is there and for lucky this will just work just a copy to clipboard button right there and look at that we have the default page Isn't that cool um so that's all there really is to it and I could even update this page you don't have to do it but I'm just going to update it for fun actually I probably have to restart the server so maybe I won't do that um but yeah so we connected through the basion so that was pretty darn easy we probably could have also used um the cloud shell to connect um but maybe we should we could give that a go as well since we're all done here let's go ahead and do some cleanup the first thing I want to do is uh the easiest way is actually to go to all resources here at the left hand side and this really gives you an idea of everything that's running in your account so I actually have other stuff in here uh that's not relevant um but the idea is that all of our stuff is running within a resource Group um and so I'm just taking a look there I'm not seeing this is all resources here um see if we see Resource Group here yeah they're all there right there so I can go ahead and click that and so everything more or less should be selfcontained within here see all that stuff um you can even see the vnet is part of it as well and so if I go ahead and delete this Resource Group it should delete all this stuff so I'm just going to type beour to confirm and we'll go ahead and delete and that should do a good job of cleaning up all those files I'm not sure if it will delete the IP it should right there but if it doesn't what I recommend is after everything is deleted just go back here to all resources and just double check to make sure they all vanish because when this is done they're all going to uh start to vanish from this list and if there's anything remaining you'll know because it's still here right so just be careful about that that's all I want you to know um and so that's the Linux part and so let's go ahead and actually now set up a Windows Server hey hey this is Andrew Brown from exampro and in this section we'll be covering Azure app service Azure app service is an HTTP based platform for web apps re estf apis and mobile bin Services you can choose your programming language in Python Java or any other language and run it in either a Windows or Linux environment it is a platform as service so it's the Heroku equivalent for Azure Azure app service takes care of the following underlying infrastructure OS and language security patches load balancing Auto scaling and infrastructure management azzure app service makes it easy to implement common Integrations and features such as Azure Dev Ops for deployments GitHub and dockerhub package Management Systems easy to set up staging environments custom domains and attaching TLS or SSL certificates you pay based on an Azure app service plan shared tier includes free and shared options litex isn't supported here dedicated tier includes basic standard premium premium 2 Premium 3 and there's isolated tier as your app service is versatile you can deploy single or multicontainer Docker applications when you create your app you have to choose a unique name since it becomes a fully qualified domain overall Azure app service simplifies your web hosting needs ensuring you can focus on coding and let Azure do the heavy lifting let's delve into runtimes in Azure app service so what is a runtime environment a runtime environment refers to the softt sof Ware and settings needed for a program to run in a defined way at runtime a runtime generally means what programming language and libraries and framework you are using a runtime for Azure app services will be a predefined container that has your programming language and commonly used library for that language installed with Azure app Services you're presented with a range of runtimes to choose from including net. netcore javar Ruby node.js PHP and python moreover Azure app Services generally supports multiple versions of each prr programming language for example for Ruby you might find versions 2.6 and 2.7 it's worth noting that cloud providers including Azure May phase out support for older versions over time this not only ensures that they're offering the latest and most efficient tools but also promotes better security practices among users pushing them to keep up with the latest patches so that's an overview of runtimes and Azure app service the next thing we'll be covering are custom containers in Azure app service Azure app service gives you the flexibility to use custom containers for both windows and Linux the primary reason you might opt for a custom container is to use a distinct runtime that isn't natively supported or to incorporate specific packages and software here's a straightforward process to get started with custom containers and Azure app service design your container Begin by creating a Docker container tailored to your needs on your local Machine push to Azure once your container is ready push it to the Azure container registry this centralized repository ensures that your container is easily accessible within Azure deploy and go live finally deploy your container image directly to the Azure app service once deployed Azure takes care of scaling maintenance and updates another advantage of custom containers in Azure app services that they offer more granular control over your environment you can finetune performance security and other aspects of your application environment to suit your needs the next topic will be covering our deployment slots in Azure app service deployment slots allow you to create different environments of your web application Associated to a different host name this is useful when you require a testing staging or QA environment alongside your production setup deployment slots let you swiftly replicate your production setting for various purposes ensuring consistent testing environments you could also swap environments this is useful for executing blue green deployments by using swap you can promote your stage in environment to production with thesee you can promote our staging to production by swapping if something goes wrong you could swap them back this capability ensures minimal downtime and enhances the user experience since you can introduce changes in a controlled manner rolling them back if necessary in addition Azure ensures that when swapping the instances are warmed up before traffic is routed resulting in zero downtime so that's a quick overview of deployment slots the next topic we'll be covering is the app service environment in Azure app Service app service environment is an Azure app service feature that provides a fully isolated and dedicated environment for securely running app service apps at high scale this allow you to host windows and liux web apps Docker containers mobile apps and functions app service environments are appropriate for application workloads that require very highscale isolation and secure network access and high memory utilization customers can create multiple ases within a a single Azure region or across multiple Azure regions making ases ideal for horizontally scaling stateless application tiers in support of high requests per second workloads ases comes with its own pricing tier called the isolated tier ases can be used to configure security architecture apps running on ases can have their access gated by Upstream devices such as web application firewalls app service environments can be deployed into availability zones using Zone pinning there are two deployment types for an app service environment external ass and ilbs external ass exposes the ass hosted apps on an internet accessible IP address if the vnet is connected to your on premises Network apps and your ass also have access to resources there without additional configuration because the ass is within the vnet it can also access resources within the vnet without any additional configuration ilbs exposes the ass hosted apps on an IP address inside your vnet the internal in point is an internal load balancer so that's an overview of app service environment and Azure app service the next thing we'll be going over is deployment in Azure app service so what is deployment well it's the action of pushing changes or updates from a local environment or repository into a remote environment Azure app Services provides many ways to deploy your applications including run from package deploy zip or War deploy via FTP deploy via Cloud sync such as Dropbox or one drive deploy continuously with GitHub bitbucket and Azure repos which using kudu and Azure pipelines deploy using a custom container CI CD pipeline deploy from local git deploy using GitHub actions deploy using GitHub actions containers and deploy with template run from a package is when the files in the package are not copied to the WW root directory instead the zip package itself gets mounted directly as the re only ww rout directory all other deployment methods in app service have deployed to the following directory for Windows Deon we use back slashes home site ww root for Linux we use forward sles home site ww rout since the same directory is used by your app at runtime it's possible for deployment to fail because of file lock conflicts and for the app to behave unpredictably because some of the files are not yet updated zip and War file deployment uses the same kuduo service that powers continuous integration based deployments kudu is the engine behind get deployments in Azure app service it's an open source project that can also R outside of azure kudu supports the following functionality for zip file deployment deletion of files left over from a previous deployment option to turn on the default build process which includes package restore deployment customization including running deployment scripts deployment logs and a file size limit of 248 megabytes you can deploy using Azure CLI Azure API via rest and Azure portal you can use file transfer protocol to upload files you will need your own FTP client you just drag and upload your files go to the deployment Center get the FTP credentials for your FTP client you can use dropbox or one drive to deploy using a Cloud sync Dropbox is a thirdparty cloud storage service one drive is Microsoft's cloud storage service you go to deployment Center configure for Dropbox or one drive when you turn on sync it will create a folder in your Dropbox Cloud Drive one drive apps as your web apps drop box apps Azure this will sync with your home site ww root so you just update files in that folder in summary Azure app service offers a range of deployment methods ensuring flexibility and ease for developers the next topic we'll be covering is autoscale and Azure app service autoscaling is the process of adjusting a server infrastructure capability to fulfill incoming requests from your web application it usually takes seconds for the changes to take effect and can be done automatically according to preconfigured metrics it does not need any new deployment or coding changes scaling options in Azure app service horizontal scaling this involves adding or removing servers from your infrastructure for example during hight traffic periods you might scale up from 1 to three virtual machines when demand decreases you can reduce the C to minimize cost vertical scaling this suggest the resources of an existing server such as CPU memory or storage for example if you find that your app application is processing a large amount of data and needs more storage for logs you might opt to increase the storage capacity of your existing server overall Azure app service autoscaling dynamically adjusts resources to meet real time application demands the next thing we'll be covering is the Azure app service plan Azure app service plan determines the region of the physical server where your web application will be hosted and defines the amount of storage RAM and CPU your application will will use it offers several pricing tiers shareed tiers there are two shared tiers free and shared free tier provides this tier offers 1 GB of disk space supports up to 10 apps on a single shared instance provides no availability SLA and allows each app a compute quoto 60 minutes per day shared tier provides hosting multiple apps up to 100 on a single shared instance no availability s SLA is offered and each app gets a compute quote of 240 minutes per day it's worth noting that litx based instances are supported in this tier dedicated tiers basic standard premium premium 2 Premium 3 Bic offers more disk space unlimited apps three levels in this tier that offer varying amounts of compute power memory and disk storage standard allows scaling out to three dedicated instances guarantees 99.95% availability and also has three levels with varying resources premium provides the ability to scale up to 10 dedicated instances and ensures 99.95% availability and it includes multiple Hardware level options isolated tier dedicated Azure virtual Network full Network and compute isolation scale out to 100 instances and availability SLA of 99.95% so the Azure app service plan lets you tailor your hosting environment and budget to fit your application needs the next topic will be going over is enabling diagnostic logging in azzure app service Azure provides builtin Diagnostics to assist with debugging an app Service app Diagnostics logging is an important part of any web application's operation it allows you to troubleshoot exceptions not exception errors alerts and warnings as well as track and improve the user experience with Azure Diagnostics logging you may log application events generated by your application web server logging with a raw version of requests made to your app only available for for the windows platform detailed error Pages saving copies of the error Pages presented to your user only available for the windows platform failed request tracing with detailed information regarding failed requests deployment logging logging detailed information about the deployment process in order to troubleshoot when a deployment fails to enable application logging for Windows apps in the Azure portal navigate to your app and select app service logs select on for either application logging or application logging or both the file system option is for temporary debugging purposes and turns itself off in 12 hours The Blob option is for longterm logging and needs a blob storage container to write logs to you can also set the level of details included in the log as shown in the table below disabled this level doesn't capture any logs error at this level only error and critical logs are captured warning this level captures logs that are warning error and critical information this level encompasses a broader range of logs capturing info warning error and critical categories verbose this is the most detailed level capturing all categories Trace debug info warning error and critical in essence As you move from disabled to verbose the range of logs captured increases with verbose capturing the most comprehensive set of logs enable application logging for Linux container in app service log set the application logging option to file system in Quota specify the dis quota for the application logs in retention period set the number of days the log should be retained when finished select save it so asure app Services diagnostic logging is essential for optimizing troubleshooting and monitoring your application let's take a look at configuring Azure web app settings Azure web app settings can be configured through the Azure portal or the Azure CLI the Azure portal also offers a bulk editing option the main settings that may be configured for your Azure web app are the following t SL SL SSL settings to have a secure and encrypted Communication channel API settings such as technology stacker platform Settings app settings you can override your configuration stored on the web config connection strings in order to do not have it written on the web config default documents displays default web pages when accessing the root of your website URL path mappings configure settings according to the user OS overall Azure web app settings offer a simplified way to optimize and here your Azure web application hey this is Andrew Brown from exam Pro and we are going to be learning about Azure app services in this follow along uh and it's a service that's supposed to make it easy for you to deploy web applications I say supposed to because it really depends on your stack Azure has more synergies with other stacks and others so like if you're like me and you like Ruby on Rails you're going to find a lot of friction with rails and Linux but if you're using something like Windows servers or python orn net you're going to have a much easier time still really great service just wish they'd make it a bit more broad there but let's hop into it so before we can go use that service let's make sure that it's activated and so we'll go over here and we'll go to as your subscription and then down below we're going to go to Resource provider now you think what you could do is just type in app Services uh and you'd be wrong because the the service is under a particular provider so if you want to figure out what provider it is we can go um Azure resource providers and they have a page on documentation here that lists them all so if I search for Azure app Services it's under web and domain registration so we're going to make sure this is registered if we're using a custom domain which we are not today we need this one activated so going back here I will type in web and you can see it's registered so if yours is not registered go ahead and hit that I believe this by default is generally registered with new Azure accounts so I don't think that is an issue for you but we'll go back up here close these additional tabs and we will type in Azure app services and we will look for that service so there it is and we'll go ahead and hit add um and so I'm going to give it a new name I just made it a moment ago but I'm going to try again and try to use the same name so we're going to call this Voyager Great and then I'm going to go ahead and name this Voyager and I already know that that is taken so I'm going to type in Delta Flyer and these are fully qualified domain so they are unique with Azure app Services you can run a Docker container we're doing code this time around and what I like to use is Ruby um but again you know if I want to use the cicd I'm not going to be able to use the deployment center with Ruby so that is not possible um and so we're going to go with python and run either a flask or a Django app I haven't decided yet I am in Canada so let's go to Canada east and uh down below here we have the plans generally the plans will tell you the cost underneath look you'll notice that it's loading but I just want to show you that there are some discrepancies in terms of pricing so if I was to go to Azure app Services pricing and we were to pull this up here we can kind of see the pricing here okay and if we scroll on down right now we're looking at a premium V2 uh and oh no I don't need help I'm okay you'll notice that it's 20 cents per hour so if I go here and do that times 730 because there's 730 hours in the year that's $146 I believe this is showing me in USD dollars yeah and in here it's showing me03 Canadian which is lower um so it could be that because I'm running in a Canada east region it's the price is different but you could imagine that if I had this at this cost at uh what did we say here um at 146 USD to CAD I'd actually be paying $182 so you got to watch out for that kind of stuff but I'm pretty sure this is what the cost is so just be aware that if you look stuff up in here it's not necessarily reflective so you got to do a little bit more work to figure that out uh if we wanted to go here uh we cannot choose the free tier when we're using Linux if we're using Windows I believe we can use it we're working with Linux today so that's just how it's going to be um for the B1 this is totally fine but we want to utilize deployment slots deployment slots is an advanced feature of uh the production version and that's the only way we're going to be able to use it here this is 20 cents per hour again so I don't want to be doing this for too long but I think what we'll do is before we do that we can just do an upgrade to devb to prod so we can experience that I'm going to go and just choose B1 okay so we go next um we do not need any application insights for the time being and it will not let us so it's okay we'll go next review and create and we'll go ahead and create this resource here and I will see you back when this is done so um our resources Now set up we'll go to Resource and now that we're in here you'll notice if we hit browse we're not going to see anything because we do not have anything deployed which makes sense right uh so we're going to actually have to go ahead and deploy something so we are going to make our way over to the deployment Center and uh it's just going to tell us that we have yet to configure anything and that's totally fine we're going to go to settings it'll give it a moment and so the thing is is that we're going to need something to deploy um I did not create an app but the great thing uh is in the Azure documentation they have a bunch of quick starts here all right and apparently they have one for Ruby as well but today we are looking at python uh and so they actually have an example repository for us here which is github.com asure samples python docs hello world and I mean I could go make a repo for you but we might as well just use the one that is already provided to us so I'm just going to pull this up to show you what's in it it's a very very simple application even if you don't know anything about building web apps I'm going to walk you through really easily here okay so we're going to open up app.py so we are using flask if you've never heard of flask it is a very minimal python framework for creating web apps uh very uninspiring uh homepage here but it gets the job done it's going to create a default route for us which uh we have there we're going to call hello here and we're going to have hello world so that's all that's going on here very very simple and we have a requirements this is our package manager I don't know why python uses dxt files is very outdated to me but that's what they use and here we have flask all right so we're going to use that repo it's a public repo so it should be very easy for us to connect so we'll drop down go to GitHub and uh the next thing we need to do is authorize GitHub all right so I ran into a bit of trouble there because I could not uh authenticate my uh GitHub account but you know what I just made another GitHub account so that made it a lot easier I'm going to go ahead here hit GitHub and we're going to try to authorize it and so now I'm logged into this new one called exampro Dev and we'll go ahead and authorize this application and we're now in good shape this repository doesn't have anything in it so um if I want to clone something I guess I'll probably have to Fork that repo so we'll give it a moment to authorize and while that's going I think that's what I'm going to do I'm going to go and uh Fork the example repo if I can find the link again here uh myself uh I believe it is that's still authorizing over there I'm still looking for it so it was like examples or something samples or examp examples all right so I found a way around the problem I just made a new uh GitHub account so that's all I had to do um and I just won't be using my primary account until I get my phone back but um so what we'll do is go hit connect I'll hit authorize and it didn't prompt me because it already connected to this new one called exam prodev you might have to put your credentials in here and it's going to ask me to select some things it's a new account so there are no organizations there are no repositories there are no branches totally brand new so what I'm going to need to do is get a repo in there so we'll just go ahead and Fork the Azure samples one so that is azure samples python docs hello world and if I type that right we're in good shape I'm going to go ahead and Fork this repository I'll say got it and then I'll move this off screen here this is now cloned you should see it cloned here and we'll go back here and this probably isn't live so there's no refresh button here so we'll have to hit discard and we will give this another go here and we will select our organization which is our name there is the repository uh should be main branch this is kind of outdated I'm sorry but it's called Master that's what it is not my fault that's azure's fault okay um and I think that's it I don't know if we need a workflow configuration file I don't think so going to double check here no I don't think so and uh what we'll do is we'll just go ahead and save that and so now we are set up for deployment all right so now that that's all hooked up if we were to go to browse we're actually still seeing the default page a deployment hasn't been triggered just yet yet uh so the way it works is it's using GitHub action so if we click into our I'm going to call it main branch I know they got the wrong name but uh we're going to click into our GitHub workflows and then below here we can see we have a yaml file uh and this is for GitHub actions integration here and so what it's doing is it's specifying the branch uh what how it's going to uh build it's going to run onto buntu latest the steps it's going to do it's going to check it out it's going to set up the python version it's going to build it it's going to do that stuff and so in order for this to um take action we'd actually have to go ahead and make some kind of manual change which we have yet to do so okay so what we'll do is we'll go back to our main here and uh it should be as simple as uh just changing something here so it's not I'm not sure how it's supposed to know that it's supposed to be doing the hello we oh I guess yeah sorry so this means it's going to Route over to here um so I'm just going to make any kind of change here doesn't matter what it is just one space we'll go ahead and give it a commit and um if I go back to my latest commits we should see that I made that change there it is we'll go back over here and this should be deploying um so if we go over to logs here you can see one's in progress right now okay and so that's what we're waiting we're just going to see that finish there we could probably open the logs and get some more information there and so it just brings you back over to GitHub actions and say here's GitHub actions and it's performing the stuff here so we're just going to give it time here and I'll see you back in a moment so we didn't have to wait too long it only took 1 minute and 29 seconds if we go back over here um we might need to do a refresh and so we can see this is reflected over here and so if we go back to it doesn't really matter if we go to settings or logs here but I'm going to hit browse and see if my page is deployed it still is not so we do have a small little problem here and it's really going to just have to do with how the app is served so that's what we need to figure out next all right so our app is not currently working and uh there's a few approaches we can take and the thing I can think right away is we should go and SSH into that instance if you scroll on down here from developer tools you can go to SSH and click this button and that's going to SSH you right into that machine right away you can also uh access SSH via the um CLI command so I believe it's like it's like a web app um SSH it'll do the exact same thing you do that from the cloud shell but that's not what we're doing today if I give this an LS in here and we're in Linux we can see we have our app here and uh what I would do is I would see what's running so I I would do a puma uh or sorry not Puma PS Ox grep uh Python and you can notice that we have a g unicorn that's running so that is where our python instances are running so you're not looking for flas you're looking for python here and and if we wanted to make sure that was working we just type in curl Local Host um and so that is going to return up Port 80 so that tells me that because like curl just means like let's go look at that page um it should return some HTML like print out the HTML to us so that means the app is not running um so what you could do is run flask run and it's going to start on Port 5000 right so what I can do is I can go up uh back to my deployment Center here and I'm going to go get that link here I just ignore the fact that it's working uh it's it's not working right now I know for certain it's not um but if we do 5,000 that won't resolve because Port 5,000 isn't open so we can't really just uh put 5,000 in there and the default server here would be 5,000 so if I stop this and I specify Port 80 right then this will start up the app on Port 80 and so now when you go here okay it will work uh this is not a great way because of course as soon as you kill it here uh technically the S should stop running um and so you'll run into that step uh so what we need to do is provide a configuration to gunicorn which is a python thing again it's not so important that you know how like what these things are but the idea is that you understand as administrator you want to make sure you have an app that runs after you do a deploy so in this particular one we need a startup. txt uh and interestingly enough there is a example code by the same author of the other one we were looking at here I believe it's the same person or it might not be but uh they have a startup txt right and so in here you can see that it binds on Port 0000 it starts up for workers starts up the app all right um and so that's something that we can go ahead and do so uh what I will do is I will go back to my GitHub repository that we have here and I can just go ahead and add a new file so I'm going to say um add a file create a new file here we'll call it startup. txt I'm going to copy this command here and paste it in there so gunicorn will bind the workers and startup on the app um startup app is being ran by uh something here so if I go back here I I think they have a startup High here and that's all that it is doing um I think I want to I could do it this way I suppose let me just see here there's is a slightly different I so they actually have like a full app going on here and I just want a very simple flask app so I think what I can do is put flask run here Port 80 and that should start up the app there I'm going to go ahead and commit that file okay and as soon as I commit that if I go back to my actions it created that startup file there so it should trigger a build it's queued up um and I'll just put this tab up here so we'll be back here in two seconds and if I give this a nice refresh yeah you can see it deploys in progress so uh this doesn't take too long we'll just wait close that there we'll just wait a few minutes we click logs it just opens it back up here and we'll see how that goes all right so uh your deploy may have finished there but the thing is is that we're not going to really know if uh a change has taken effect unless we actually go ahead and update our code so what I want you to do is go to your code tab go to your app.py we'll hit edit and I'm going to go ahead and change this to Vulcan and then we'll scroll on down hit commit changes and we'll make our way back over to our deployment Center and we'll give it a refresh here and we're just going to wait until this one is complete and we will double check to make sure that that is changed if it is not we will take action to fix that okay all right so we just waited a little while there for that deploy to happen and if we go to our website here it is taking effect so that's all we had to do to get it working so that's pretty good um so that is uh deployment so let's talk about deployment slots in order to utilize this feature we're going to actually have to upgrade our account because we cannot utilize them at this uh the basic plan here we got to go to standard or premium so let's go ahead and give that an upgrade uh so here's the B1 we're going to go to production here um and I think yeah we're going to have to choose this one here uh very expensive so the thing is we're going to just upgrade it temporarily unless there's more options down below that are cheaper yeah these are the standard tiers let's go with this one here because it's only $880 again we're not going to be doing this for long but I want to show you how to do staging slots and auto scaling okay so we'll go ahead and apply that there and now it says that it's applied so if I go back to our app here and we click on deployment slots sometimes it doesn't show up right away if it doesn't that's not a big deal you just wait a bit but today it's super fast so we're going to go ahead and add a new slot we're going to call it uh staging we're going to deploy from our production Branch here and I'm going to go ahead and create that there and we'll just wait until that's done okay great so we waited a little bit there and uh our slot is created so I'm going to just hit close there and so now let's go take a look and see if we can actually see the application here so I just clicked into it I click browse and we're getting the default page so nothing is actually really deployed to it uh so how are we going to do that that's the the main question here um so what I'm going to do is I'm going to make my way over to the deployment Center and you can see that it's not configured for the slot so we are going to have to set it up all over again even though it copied over configuration settings it didn't copy over the code so we go to GitHub we'll choose our organization again I'm going to choose the repository we're going to choose that main branch again there we're going to let it add a workflow and notice that this time it's going to call it staging do yaml so there'll be separate workflow that gets created we're going to go ahead and save that there and what we can do is again click onto our Branch name there and if we click into our workflows we'll not now notice that we have a staging example it's the same thing um but it should be able to now deploy so the whole purpose of um these deployment branches is that it helps us uh we can deploy different versions of our apps but also um it's just a place where we can uh uh view things before we actually roll them out so we want to make sure 100% that they are working correctly um I don't think this will automatically push out let me just go to my actions to see if this is deploying notice that we have two workflows now we have staging here uh and yeah it looks like it's going to deploy here so we'll just wait a little bit um but maybe what we can do is try to have a slightly different version uh for each one here okay but we'll just let that finish and I'll see you back in a moment all right so our deploy finished there so now if we go back to our website here we go browse we should see that application it says hello Vulcan and if we go and take out this we still have hello Vulcan so how can we have a a variant of this so that we can push out to that so what I'm going to do is I'm going to go back to my application here I'm going to go to code and I'm just going to make a minor change um I don't say also is that spelled right startup that does look correct to me um so maybe I'll go ahead and adjust that file but it doesn't seem to be affecting anything which is I'm a bit surprised there so what I'll do is I'm going to go and edit that file and give it the proper name can I rename this file yes I can so we'll call that startup file I thought we need that for deployment I guess it just works without it which is nice uh if we go back here I'm going to go and I actually just want to edit my um app here again and I'm going to go and edit this and we'll say um hello Andor or hello andorians maybe and so if I go back to my actions the question what is it deploying is it going to deploy the production or the staging and it looks like it's going to do both looks like it's doing both here but one way we could tell is we can go to our logs here and we can see that um so we did two deploys so there's one change here uh if we go back to our main application and our deployment Center here and we go over to our logs you can see that they're both deploying so it doesn't seem like it's a great thing that that's how it works so the question is is then how would we um facilitate that deploy right how could we do that I suppose what we could do is just make a separate staging Branch um so if I go over to code here um I don't think we can just make branches through here so what I'm going to have to do is go ahead and oh I can create a branch right here so we'll just type in staging and we'll go create ourselves a new branch and now we are in this branch and what I'm going to do is go ahead and modify this and we're just going to call this um hello Klingons okay we'll go ahead and update that and so this should be a separate Branch so you think what we could do is go in and just change our settings so that it deploys from that one uh we'll go back to our deployment slots we'll click into staging here and we need to change our configuration settings um I think we could just do it from here hold on here I I could have swore it specified the branch if we go to deployment Center here I think it's set up on that other branch there I think we just adjust it here so yeah I think we could just um adjust these settings um we can't discard them but maybe what we can do is just go in and modify that file so we will go into our code here and uh we will go ahead and click into here go into staging and we'll just change what the branch is called so we'll just say staging and we'll hit start commit and we will save that and we'll see if it actually reflects those changes there so we will go here and hit refresh we'll see if it picks up staging now if we go to settings it's not picking it up so um I'm not sure I don't think perform a redeploy operation we don't want to redeploy so maybe what we'll do is just we'll have to do a disconnect here because it's collect it has the wrong one here so save workflow file um okay we'll just go ahead and delete it it's not a big deal we'll just have to make a new one here we'll go to GitHub we'll choose our uh organization again or repository our staging Branch this time around we'll let it add one see it says we can use an available workflow so we could have kept it there and added it there um and we'll go ahead and save that so now we'll have two separate branches there and we'll give that some time to deploy because that will now trigger a deploy off the bat and so I'll see you back here in a moment all right so after a short little wait here it looks like our app is done deploying so we'll go over here we'll make sure that this is our staging server is good and we want to see that our production is different perfect so we now have a way to deploy to each one but imagine that we want to swap our traffic so we're happy with our staging server we want to roll that out to production and that's where we can uh do some SWAT and so what we'll do is click the swap button and we're going to say the source is the staging and this is our Target production and we're going to perform that swap uh right now we can't do a preview because we don't have a particular setting set that's okay and it's kind of showing if there are any changes so set of configuration changes we don't have any so that's totally fine as well we'll go ahead and hit Swap and that's going to swap those two I believe it's has zero downtime so we will be in good shape if that happens there and we'll just give it a moment to do that great so after a short little wait there the swap is complete and so uh if we remember clearly this was our production right and so if I was to hit refresh it so now say Klingons and if I go to my staging server it should be the other way around right good so now imagine that I want to just split the traffic uh that's something else that we can do um so notice over here we have these percent is here um not sure why it won't let me change those so maybe I'll have to look into that so I'll be back so I'm not sure why it's not showing us that traffic slot there but what I'm going to do is just maybe try to trigger deploy back into our staging and maybe that's what it wants to see um so what I'm going to do is go back to my code here we'll be in our staging Branch here I'm going to go ahead and uh edit this file here and we will just change this to borans and we will hit update and we will let that go ahead and deploy so we go to actions here we can see that it is deploying um and we'll just give it some time okay so see you back here in a bit I mean the other reason could be that we're just not at the main level hold on here uh if we go back here to deployment slots you know what I think it's just because I was clicked into here and then I was clicked into deployment slots that they're both grayed out yeah it is so we can actually do it that top level there doesn't hurt to do another deploy though so um we'll just wait for I'll wait for that deploy to finish and then we'll come here and uh adjust that there okay all right so let's take a look at uh doing some traffic switching here so right now if we were to go to our production we have Klingons and if we were to uh go to our staging we have Boran so imagine that we only want 50% of that traffic to show up so what we can do is put in 50% and what I'm going to do is um do I hit enter here or oh sorry save up here there we go um and so what's going to happen is this should take effect I think right away yep uh and so now we have 50 50 50% chance of getting something else here um so I'm just going to keep on hitting enter here if that doesn't work we can try an incognito tab and there we go we got the opposite there and so this is serving up staging right uh and this is serving up production but they're both on the production URL so that's a way you can split the traffic so uh that's pretty much all I wanted to show you for deployment slots let's now talk about scaling hey this is Andrew Brown from exam Pro and in this section we'll be covering Azure container instances Azure container instances allow you to launch containers without the need to worry about configuring or managing the underlying virtual machine aure container instances is design designed for isolated containers they are tailored for simple applications task Automation and tasks like build jobs containers can be provisioned within seconds whereas VMS can take several minutes containers are built per second whereas VMS are built per hour providing potential cost savings containers have granular and custom sizing of vcpus memory and gpus whereas VM sizes are predetermined ACI can deploy both windows and Linux containers you can persist storage with Azure files for your ACI containers once deployed AIS are accessible via a fully qualified domain name like custom label. aure region. aure container. Azure provides quick start images to start launching example applications but you can also Source containers from Azure container registry Docker Hub or even privately hosted container registry container groups are collection of containers that get scheduled on the same host machine the containers in a container group share life cycle resources local network and storage volumes container groups are similar to a kubernetes pod multicontainer groups currently support only Linux containers there are two ways to deploy a multicontainer group to deploy a multicontainer group you can use either a resource manage or template if deploying additional Azure service resources or a yml file for deployments involving only container instances overall Azure container instances simplify container deployment and scaling removing the complexities of infrastructure management the next topic we'll be going over our container restart policies a container restart policy specifies what a container should do when their process has completed these policies ensure that the container instances can handle different scenarios effectively based on the specific requirements of the application or task Azure container instances has three restart policy options always this policy ensures that the containers restart continuously regardless of whether they exit successfully or not it's useful for app applications that need to be constantly available such as web servers never with this policy containers do not restart once they've completed their execution this is ideal for tasks that are designed to run once and then terminate such as batch jobs or scheduled tasks on failure containers will only restart if they stop due to an error or unexpected termination this ensures that if a container crashes or faces an unexpected error it will try to restart and continue its operations overall choosing the appropriate restart policy is vital for the the stability and responsiveness of your applications the next topic we'll be covering our container environment variables environment variables are key value pairs that can be used to configure and manage the behavior of applications running inside containers environment variables allow you to pass configuration details to your containers which can be critical in guiding applications on how to connect to databases where to find certain resources or how to adjust their behavior based on the environment they're running in in Azure you can easily set up these environment variables for your containers using the Azure portal CLI or Powershell secured environment variables by default environment variables are stored in plain text to address this Azure offers the option to secure your environment variables instead of storing them in plain text which could expose sensitive information if breached you can Leverage The secure environment variables flag so that's a quick overview of container environment variables the next topic we'll be covering is container troubleshooting troubleshooting containers in Azure involves a series of commands that help diagnose and resolve issues as container logs this command lets you fetch logs from your container these logs can provide insights into application behavior and possible errors as container attach if you need diagnostic data during container startup use this command it helps in understanding issues that might arise during the initialization phase of a container as container EXA for a deeper dive into the Container this command starts an interactive session this is useful for live debugging and to inspect the container's current state as monitor metrics list this command gives you metrics related to your container such as CPU usage which can be essential for performance tuning or identifying bottle X so these are the commonly used commands for container troubleshooting hey this is Andrew Brown from exam Pro and we're going to take a look at a your container instances so here it is so all we got to do is go to container instances we'll hit add and the nice thing is that Azure provides us with a Hello World one so it's very easy for us to get started um it's a Linux machine and it looks like it's pretty inexpensive there so we'll stick with that I'm going to create a new group here we're going to call it banana um and we'll name the container instance banana and East Us 2 seems fine to me you'll notice we're on a quick start image if we wanted we could use something from the docker Hub and provide our own link but we'll just take with the quick uh start image for today we're going to go ahead and hit next to networking just to see what we have as options you can make it public or private we'll go to Advanced hold on here yep those are just the ports you can expose we'll go to advance and for the restart policy we can set on failure always or never we can pass in environment variables and I covered this a lot more in detail in the lecture content so we don't need to really dive deep into this um and we'll go ahead and create this instance and so we'll have to wait a little while here and I'll see you back in a moment okay and so after a short wait our container instance is ready we'll go to that resource there and take a look around so on the left hand side we can go to our containers and there we can see it running we can see the events down below of what's going on so you can see that it's pulled the image it successfully pulled it and it started the container some properties nothing interesting there the logs if we wanted to see stuff and if we wanted to connect to the instance we could also go here and hit connect which is kind of nice um I don't have any purpose to do that right now so and it's also not going to work the way we're doing it but I just wanted to show you you had those opportunities uh you can do identity so that means manage it with ro base access controls but what I want to see is actually this uh hello world working I'm assuming that must be a a hello page I've never looked at it before so we're going to go here grab the public IP address and paste it on in the top and there we go so we have deployed a instance onto Azure container instances or a container I should say so nothing super exciting to talk about here um but we do need to know the basics uh there um if we wanted to deploy other containers it's just the one there so that's all you really need to do um but yeah so yeah hopefully that uh gives you an idea there I'll just go back to the list here so we can see it and we'll go ahead and just uh delete that probably do it for the vi the resources on the left hand side like I always like to do uh and we will go into banana here and we will delete banana and there you go hey this is Andrew Brown from exam Pro in this section we're diving into the Azure container registry Azure container registry is a managed Docker registry service based on the open Source Docker registry 2.0 it's designed for building storing and managing containerized applications and images use Azure container Registries with your existing container development and deployment pipelines use Azure container registry tasks to automate image builds directly in Azure when you commit code you can seamlessly pull images from ACR for deployment to various orchestrators such as kubernetes DC OS and Docker swarm many Azure services including Azure kubernetes service Azure app service Azure B and Azure service fabric offer direct support to interface with ACR this ensures a cohesive workflow for deploying applications developers can also push container images to a container registry as part of a container development workflow with delivery tools such as Azure pipelines and Jenkins ACR offers various interfaces for interaction you can manage and configure it using the Azure CLI Azure Powershell Azure portal Azure SDK or even the docker extension for visual studio code in conclusion azure container registry securely and efficiently manages your Docker images streamlining build storage and deployment the next topic will be covering our Azure container registry tasks ACR tasks are designed to automate the process of patching the operating system and Frameworks within your Docker containers for quick tasks ACR tasks let you instantly push a specific container image to azure's container registry without needing a local Docker engine installation ACR tasks offer automation flexibility you can set up automated builds that are triggered by different events such as updates made to the source code updates to a containers based image or predetermined schedules or timers with ACR tasks multistep workflows become easier for example you could build a web application image run the web application container build a web application test image and deploy the container that will run tests on the web application each ACR task has an Associated source code Contex the location of a set of source files used to build a container image or other artifact furthermore ACR tasks support the use of run variables making it possible to repurpose task definitions and enforce consistent image and artifact tagging standards in summary ACR tasks amplify the automation efficiency and consistency of container management and Azure the next topic we'll be going over is Docker Docker is a powerful platform design designed for automating the deployment scaling and management of applications using containerization Docker abstracts infrastructure and environment variables allowing you to create a controlled environment within your Docker container we can install Docker images into those Docker containers with each Docker image representing one or a group of common software the machine on which Docker is installed and running is usually referred to as a Docker host or host when you deploy an application on the host it will create a logical entity to host where the application called a container or Docker container container a Docker container does not have any OS installed and running on it it has a virtual copy of the process table network interface and the file system outpoint which have been inherited from the OS of the host on which the container is hosted in Runing the kernel of the host OS is shared across all the containers that are running on it this allows each container to be isolated from the other present on the same host it supports multiple containers with different application requirements and dependencies to run on the same host if the OS requirements are the same Docker key benefits Docker supports multiple applications with different requirements and dependencies to be hosted on the same host if the OS requirements are the same storage optimized containers are typically a few megabytes in size and consume very little disk space allowing a large number of applications to be hosted on the same host robustness robustness containers don't contain a full OS making them more lightweight than virtual machines as a result they use significantly less memory and can boot up in mere seconds whereas a might take several minutes reduces costs Docker is less demanding when it comes to the hardware required to run it better or Disaster Recovery you can back up a Docker image of the state of the container at a specific point in time and restore it later if serious issues arise faster configuration with consistency you can just put your configurations into code and deploy it saves a lot of time from preparing the setup and deployment documentation overall Docker provides a robust efficient and costeffective solution for application deployment and management through containerization the next topic we'll be going over is a Docker file in Docker a Docker file is a text document that contains all the commands a user could call on the command line to assemble an image by using the docker Bill command users can automate the image creation process by executing the instructions laid out in the docker file here are some essential points to understand about Docker files the docker file is a text file that contains the instruction that you would execute on the command line to create an image a Docker file is a stepbystep set of instructions Docker offers standard commands for use within the docker file such as from specifies the base image to start with copy copies files from the host system into the image run executes a command EnV sets environment variables expose informs Docker that the container will listen on a specified Network Port at runtime CMD provides defaults for the executing container docker will build a Docker image automatically by reading these instructions from the docker file overall a Docker file provides a reproducible and consistent method to build Docker container images hey it's Andrew Brown from exam Pro and in this section we'll delve into Azure functions but first it's essential to understand the concepts of serverless and functions as a service so what is serverless serverless architecture generally describes fully managed cloud services that classification of a cloud service being serverless is not a Boolean answer but an answer on a scale where a cloud service has a degree of serverless a serverless service could have alter most of the following characteristics High elasticity and scalability high availability high durability and secure by default abstracts away the underlying infrastructure and are built based on the execution of your business task serverless can scale to zero meaning when not and use the serverless resources cost nothing pay for Value model where you only pay for actual use usage eliminating costs for idle resources an analogy of serverless could be similar to an energy rating labels which allows consumers to compare the Energy Efficiency of a product some services are more serverless than others what is function is a service FAS empowers developers to concentrate on Crafting specific pieces of code known as functions these functions can be event driven meaning they either trigger based on events or produce event data typically several functions are interwoven to form a serverless application and these functions are activated only when called upon function as a service is not serverless on its own f is only serverless if it's fully managed and scales to zero in conclusion serverless and F represent transformative approaches in cloud computing emphasizing efficiency scalability and costeffective me the next topic we'll be covering is azure functions Azure functions is a function as a service offering that allows developers to focus on writing code and not worry about maintaining the underlying Computing infrastructure a function app defines the underlying compute for a collection of functions a function app defines the hosting runtime and other Global configurations a function represents code LA with application runtime configuration a trigger is the chosen event data that will cause function to execute you can only have one trigger input bindings or what are multiple data sources that will be pass to the function when a trigger occurs output bindings are one more data syns that will receive outputed data from the function on successful execution there are four versions of azure functions 1 x 2 x 3 x and 4 x we are currently using for X Azure functions storage considerations every function app requires a storage account to operate if that account is deleted your functions won't work Azure functions uses the following storage types in the storage account blob storage maintain binding State and function keys Azure files file share used to store and run your function app code in a consumption plan and Premium plan Azure files is set up by default but you can create an app without Azure files under certain conditions H Storage used by task hubs and durable functions and table storage also used by task hubs and durable functions Azure functions anatomy of a function function. JSO n configuration of a single function defining bindings code the code for your function Funk ignore to ignore host. JS n Global configuration of all functions at the function app level local project a place to locally store code in conclusion Azure functions offers developers an effortless way to build event driven Solutions without managing infrastructure the next topic we'll be overgo is the authorization levels for Azure functions HTTP triggers authorization level determines what keys if any need to be present on on the request in order to invoke the function the authorization level can be one of the following values Anonymous no API key is required function a specific API key for that function is needed admin the master key for all functions within that function app is required in this example we're creating an HTTP trigger the desired authorization level can be selected note that the authorization level can usually be changed after Creation in the portal and is set on the trigger Azure functions debugging you can can enable streaming logs for Azure functions to see your realtime logging when an error occurs there are two ways to view a stream of log files being generated by your function executions builtin log streaming the app service platform provides an inbuilt feature to stream application log files allowing developers to trace function executions live live metric stream when your function app is connected to application insights you can view log data and other metrics in near real time in the Azure portal using live metric stream it's worth highlighting that these lock stream can be viewed both in the portal and in most local development environments overall Azure functions provide adaptable HTTP trigger authorization and robust debugging tools enhancing security and developer troubleshooting the next let's break them down lightweight and serverless Azure functions are Compact and potentially serverless removing the need for heavy infrastructure efficiency and speed easy to write deploy and upgrade without effect other website components Azure functions are fast to execute because there is no large application startup time initialization and other events fired before the code is executed event driven execution ensures they run only when an event triggers them zero maintenance no need for active infrastructure management or Associated costs they scale automatically to meet traffic demands even scaling to zero cost when idle development e Azure functions can be built tested and deployed directly from the Azure portal they provide builtin CI CD through Azure devops and monitoring with Azure monitor streamline the development process cost effective you pay only for the actual runtime eliminating costs when functions are idle interoperability using industry standard protocols Azure functions can seamlessly communicate with various apis databases and libraries Azure functions use cases for business use cases Azure functions are great for scheduled tasks reminders and notifications lightweight web API sending background emails running background backup tasks and Performing bin calculations for technical use cases Azure functions are ideal in sending emails starting backup order processing and task scheduling such as database cleanup sending notifications messages and iot data processing Azure functions are best suited for smaller apps have events that can work independently of other websites in conclusion Azure functions are serverless and efficient streamlining development for both business and Technical tasks and event driven compaction applications the next topic we'll be covering is our Azure function templates Azure provides function templates to get you started with common function scenarios in Visual Studio code the selection of a function template occurs exclusively during the project creation phase HTTP triggered by an HTTP request and returns HTTP tyer triggered based on a predefined schedule blot storage triggered when files are either uploaded or updated in a blob storage container Cosmos DB executes in response to the addition or modification of documents in Cosmos DB Q storage triggered by Azure storage Q messages event grid triggered by event from event grid many Azure Services can trigger a function through event grid essentially event grid operates as a serverless event bus that is deeply integrated with various Azure Services event Hub triggered by event Hub event and is particularly effective for streaming scenar scenarios service bus Q triggered when there's a new message in a service bus que making it optimal for messaging systems service bus topics triggered by an event from bus topic aligning it with the pub submodel send grid specifically designed to be triggered by an email event within the third party service send grid overall Azure function templates offer developers a structured foundation for common scenarios enabling quicker deployment and integration within azure the next topic we'll be going over the functions configuration for Azure functions each function comes with a specific configuration file named function. JSO n this file serves an important role outlining the trigger bindings and additional configuration settings for the function the Essential Elements of this file include type specifies The Binding type Direction indicates whether the binding is for receiving data into the function or sending data from the function name represents the the data binding in the function in C this would be an argument name whereas in JavaScript it would manifest as a key and a key value pair let's take a look at the host configuration for Azure functions every function app has a host configuration file named host. Json this configuration file contains Global configurations options and parameters for all the functions within the function app the host has lot of configuration options these include aggregator application insights blobs console Cosmos d custom Handler durable task event Hub extensions extension bundle functions function timeout health monitor HTTP loging manage dependency cues retri send grid service bus Singleton version watch directories and watch file settings essentially these configurations offer developers a framework to adapt functions respond to event triggers and ensure consistency across the function app the next thing will be be covering are the plan services and Azure functions Azure functions offers three distinct plan Services each tailored to different needs consumption plan cold starts you only pay for the time your code or application is running billing is based on the number of executions the duration of each execution and the amount of memory used just pay while you have functions running and scale out automatically even through long loading times Premium plan prewarmed the user has designated a set of prewarmed cases which are already online and ready to react instantly Azure provides any additional Computing Services that are required when your function is running you pay for the constantly prewarmed instances including any additional instances needed to scale the Azure app in out Azure functions host instances are added and removed based on the number of incoming events dedicated plan VM sharing when you use app service for other apps your functions will run on the same plan at no extra cost you may scale it up manually by adding more VM instances for an app service plan you may have autoscale enabled optimal when you have existing underutilized VMS which also operate other instances of the app service in summary when choosing an Azure functions plan we your budget responsiveness needs and current infrastructure for Optimal Performance and value the next thing we'll be covering are triggers and bindings and Azure functions triggers and bindings let you avoid hardcoding access to other services and abstracting Away boiler plate code keeping your functions lean what is a trigger a trigger is a specific type of event which causes the function to run it defines how a function is invoked and a function must only have one trigger triggers can have Associated data which is often provided as the payload of the function what is a binding binding is defined if your function is connected to another service the data from bindings is provided to the function as parameters bindings are optional and a function can have multiple input and output bindings Azure functions support a wide range of bindings to facilitate integration and data processing these include Storage Solutions like blob storage Azure Cosmos DB and Azure SQL Dapper event grid and event Hub cater to event driven architectures with iot Hub focusing on event data from Hardware devices HTTP and web hooks facilitate realtime data interaction while Kafka processes stream data mobile apps helps in Mobile development and notification hubs are for push notifications Hugh storage rabbit and Q a messaging broker and service bus ensure seamless data transfer and messaging send Grit optimizes email delivery SLE R an open source.net Library provides asynchronous notifications to web apps table storage is the nsql KE Value Store timer triggers are based on scheduled rejects expressions and twio offers a cloud platform for Voice and text messaging systems the table provides a breakdown of various Azure functions Integrations and their support across versions one Hax and 2 x and higher both blob storage and Azure Cosmos DB are fully supported across all categories for both versions Azure data Explorer Azure SQL Dapper cafka and Signal are supported from version 2 x Awards some Integrations such as mobile apps notification hubs and send grid have limited support across the versions specifically with triggers inputs or outputs others like event grid event hubs HTTP and web hooks iot Hub Q storage service bus and table storage have brought support but may lack in one or two categories overall Azure functions offer serverless computation and seamless service integration using triggers and bindings they eliminate redundant code the platform supports diverse bindings but some Integrations have limits the next topic we'll be covering into is The Binding Direction and Azure functions all triggers and bindings have a direction property in the function. JSO n file the direction of triggers is always in input and output bindings use in out or both some bindings support a special Direction and out out the trigger is defined alongside the input and output bindings trigger will have the same as the input type but with trigger appended for example an input binding name blob would have a trigger name blob trigger if you use an out only the advanced editor is available via the integrate tab in the portal if you use an out only the advanced editor is available via the integrate tab in the portal in scenarios requiring periodic data processing Azure functions offers tailored Solutions suppose every hour you want to read new log files delivered by your app application and you need to transform the data to be ingested in your nosql database that resides in Cosmos DB you'll use the trigger type timer because it's a scheduled job that will run at a specific time the blob storage would be an in binding as the function reads the data from it the cosmos DB would be an out binding enabling the function to write the processed data into the database in essence by understanding and effectively leveraging these binding directions developers can architect efficient eventdriven solutions that seamlessly integrate various Azure services hey this is Andrew Brown from exam Pro and in this section we'll be covering Azure front door Azure front door is a traffic manager traffic accelerator Global load balancer and content distribution Network Azure front door is a modern application delivery Network platform providing a secure scalable CDN Dynamic site acceleration and Global HTTP load balancing for your Global web applications Azure front door featur fees caching similar to traditional cdns Azure front door offers caching with specified rules and expiration policies resiliency by Distributing incoming traffic across multiple or different Azure regions cookie based session affinity for restful applications when traffic needs to be redirected back to the same back end Health probe to determine the healthiest and closest backend to the client request web application firewall a crucial security measure protecting your back ends from malicious attacks and vulnerabilities URL redirect redirecting traffic based on protocol HTTP or https host name path and query string URL rewrite with a powerful engine for rewriting income requests to a different backend request an Azure front door is made up of frontends domains these frontends domains are connected to backend pools where those connections are filtered by routing rules Azure front door core components profile containers all front door components and Point acts as a pathway or Bridge connecting the front end to the back end origin groups a grouping of Origins origin defining the road to back end rule sets a grouping of rules rules routing rules overall aure front door optimizes web application delivery within the Azure ecosystem by utilizing its features businesses can enhance web traffic management efficiency the next topic we'll be covering are the tiers for Azure front door door Azure front door offers two distinct tiers tailored for different needs first we have standard this auction optimizes content delivery offers both static and dynamic content acceleration offering Global load balancing SSL offload domain and certificate management enhanced traffic analytics and basic security capabilities and we have premium this option includes all the features of the standard tier asure front door premium tier offers extensive security capabilities with Waf by protection private links support and integration with Microsoft thread intelligence and security analytics in conclusion Azure front door provides standard for Content delivery and premium with added security ensuring Global application performance and protection the next topic we'll be going over is routing and Azure front door routing in Azure front door determines the path in HTTP request from a user takes to reach a configured backend service here's how it works an HTTP request from a user is directed to the nearest Edge location it then matches with an Azure front door profile and evaluates any web application firewall rules following this it matches with an Azure front door route evaluates engine rules and either returns cash content or selects the appropriate origin group finally the request is directed to the selected origin which then sends it to the corresponding back end Azure front door offers four distinct traffic routing methods latency routes requests to the backends with the lowest latency within a specified sensitivity range priority directs requests based on a user defined priority number weighted distributes requests to backends proportionally based on assigned weight coefficients session Affinity ensures requests from the same end user are directed to the same back end ideal for stateful backends in summary Azure front door enhances user experience and backend performance through its efficient routing capabilities the next topic will be covering is origin and origin grp groups in Azure front door the origin is what Azure front door will point at to the end user origin is the endpoint that points to your back end Azure front door provides robust support for a variety of Origins to seamlessly integrate with its application delivery Network among the supported origins or Azure blob storage which offers vast storage capabilities and Azure storage with static website hosting catering to web hosting needs additionally Azure supports cloud services app services and static web apps for diverse application deployments for comprehensive API management Azure front door integrates with API management furthermore application Gateway public IP address Azure traffic manager Azure spring cloud and Azure container instances are also supported notably there's flexibility for users as they can also add custom Origins by providing a host name priority and Azure front door determines who is the primary recipient of traffic or who who to send traffic to First you select a value ranging from 1 to 5 a lower value signifies a higher priority and multiple backends can share this number weights allow you to determine the split of traffic distribution between origins of the same priority a number between 1 to 1,000 the default value is 50 origin groups in Azure front door are collections of Origins every origin must be part of an origin Group by default as your front door profiles contain An Origin group named default origin group origin groups facilitate the application of Health probes to assess the condition of your Origins and load balancing settings to manage the distribution among them to direct inbound traffic to a particular origin group and inlo must link to the origin group through a designated route overall as your front door optimizes web traffic management with diverse Origins prioritization and weight mechanisms using origin groups it leverages health probes and load balancing for efficient application delivery the next topic will be covering our health checks in Azure front door Azure front door uses origin group Health probes to periodically ping a backend verifying whether it returns a healthy response typically a healthy response is determined by status 20 100 if a backend fails to produce a healthy response aure front door will redirect the traffic to other available and healthy backends assuming other Origins are configured what is a HTTP Response Code when a user sends an HTTP request a HTTP response is returned and HTTP responses will have a response code to communicate how a backend server interpreted the request a response code is number that coordinates to what happened for example 200 signifies okay 4003 means forbidden 4004 indicates not found 500 represents internal server error Azure front door load balancing settings origin group load balancing settings and Azure front door enable you to specify the criteria determining the health of a back end essentially these settings Define which sample set should be used to classify a backend as either healthy or unhealthy when the latency sensitivity is set to zero aure front door prioritizes and routes traffic to the quickest available backend if it's set to any other value the system adopts a round robin approach Distributing traffic between the fastest back end and subsequent ones all within the bounds of the preconfigured latency sensitivity the next topic will be covering a routes and Azure front door routes and Azure front door serve as mapping tools linking your domains and corresponding URL path patterns to specific origin groups routes of caching and compression features applied rules from rule sets can be Associated to routes to apply intelligent routing one of the standout features of azure front door is its capability for traffic acceleration this feature ensures faster Global delivery of your application without necessitating any changes to your existing application code hazure achieves this traffic acceleration by directing traffic to the nearest Edge location to a ramp into the Azure Network traffic that is following within the internal Azure Network travels at accelerated speed while also taking the most direct path think of it as an expressway in essence asure front door routes link domains to origin groups incorporating caching compression and intelligent routing the next topic will be covering a rule sets in Azure front door Azure front door rules engine allows you to customize how HTTP requests gets handled at the edge and provides a more controlled Behavior to your web application this interface is a configuration screen for defining rules in Azure front door a Content delivery and application acceleration service let's break down what's presented rule Name by rule this is the name assigned to the current rule condition if represents the conditions under which the Rule's action should be executed request header the rule is triggered based on an HTTP header present in the incoming request header name my header the rule looks for this specific header in the incoming request operator equal specifies the comparison type here it checks if the header's value exactly matches the defined value header value my value the value that my header should have to meet the condition string transform to lowercase before checking the value it will convert the header's value to lowercase action then represents the action to be taken if the condition is met URL redirect the action taken will be a URL redirect redirect type found specifies the type of HTTP redirect 3002 means a temporary redirect redirect protocol match request the protocol of the redirected URL will match that of the original request destination host not specified in the image which means it'll redirect to the same host or domain destination path go here this is the path to which the user will be redirected if the condition is met some of the available conditions include device type HTTP version and request cookies it also examines post args the query string and the remote address of the requester details like the request body request file name request file extension and request header is scrutinized furthermore the request method request path request protocol and request URL are taken into account during processing aure front door utilizes rule sets to manage its operations offering a range of operators for precise control these operators include equal contains less than greater than less than or equal greater than or equal begins with ends with and re X additionally each of these operators has a corresponding knot variant providing enhanced flexibility and specificity in defining rule conditions action cache expiration cache Behavior bypass override set of missing cash key query string Behavior include cach every unique URL exclude ignore query string modify request header modify response header operator append overwrite delete URL redirect redirect type found moved temporary redirect permanent redirect redirect protocol match request HTTP https URL Rite Source pattern and destination and origin group overr right so that's an overview of rule sets for Azure front door hey this is angrew brown and this fall along we're going to be utilizing Azure front door so before we do that we're going to need ourselves a storage account to set up some static website uh storage so what I want you to do is go to storage accounts and there is a static web app host uh hosting that we can use but we're going to do it the old school way because it's always great to learn a few different ways to do things in Azure and so uh there's probably another fall along where we we use uh the static static site thing there um so what we're going to do is create a new uh we're going to create a new um Resource Group I'm going to call this one uh my uh Azure front door and we'll say okay and from here we'll have to name it something so we'll just say my Azure front door or sorry we'll do uh step static Storage storage 8888 if you can't get the 48s because I'm using it then just give it a different um number sequence because these are treated like fully qualified domains choose like a a US region so that we're all doing the same thing for performance we're going to stick with standard it doesn't matter if it's on Geo redundant we're not doing anything fancy there so what I want you to do is go ahead and hit review and create and then give it a moment and hit create and we're going to wait for that to uh finish deployment and from there we'll then have to enable uh static website so it doesn't take too long to deploy so we'll just give it a moment and while that's going uh we do need to create ourselves a um index HTML file so you're going to need some kind of editor so I'm just opening up visual studio code on my computer just give it a moment there and I'm just creating a new file here this is taking it Taking a moment to load okay so and what we need to do is just create an index HTML file these are really basic you can find them anywhere online but this one's going to say hello Mars hello Mars very very simple so I'm going to go ahead and save this to my desktop so save as and um show local maybe there we go that's a little bit more sayane and we'll just say index.html and we will now go to the resource and we're going to go to website static is a blade on left hand side so we can do St website stuff we're going to say index HTML I think we have to set that for it to work I cannot remember if that's the case we have a primary end point this is uh these matter for later but we'll come back to that and we're going to go over to our containers and we're going to have to create a new container so I'm just going to call that actually we don't because we're going to put it in web so when we turned on static website hosting it already gave us a blob or container a container for us to uh add our add our or upload our files there so we'll go to upload here and then I'm going to go ahead and select that index HTML file and upload it in place um we need to change our access level for this uh this uh container to um I think just blob so I'm just double checking here yeah I think it's blob access uh can only be anonymous request container data is not available so we'll say okay I believe that is correct okay okay uh sometimes you can like we're clicked into it but I think like in the my instructions I do it a little bit different so I'm just going to double check to see what the settings are there just so you can see it if you're following my instructions so change access private blob yeah okay so that's fine um so blob access should be okay and so now what we want to do is just test that our our page is working so if we go back to static website here on the left hand side we can grab this primary endpoint and we should be able to post it anywhere here see I have the old tutorial here this is a moment ago so that's why it's a bit confusing but um so hello Mars is working but this is on static stor this isn't in front of or behind aure front door uh which is what we'll want to do next so what I want you to do is just close a couple of these tabs out we're going to type in Azure front door or front door should be enough front there we go and we have a whole bunch of options we'll just hit create that's a front door we actually do use for the platform we have Azure front door we have create I always go custom I don't think I've ever done quick create I just I just have more trust in custom and from here we will choose front door and ECS is fine I'm just wondering where our storage account is that's why we should always just kind of set them to be the same place so go storage account I don't think it will matter but I'm just going to double check storage account and this one is in East us so we're going to be okay just make sure they match just so we have less problems and I'm going to call this one my Azure front door now there are more functionalities in the premium but standard is fine for us we'll go next to Secrets um this is if we want to add a certificate um like you could bring your own certificate um but uh we're not too worried about that we're going to have to add a new endpoint so I'm going to just say my endpoint here there's a lot of small steps in here so this should be fun hopefully we don't configure anything wrong here and then we need to add a route say my route and it's going to use the default domain that's totally fine uh we don't have to do anything it's going to just be on the for SL Aster there we do need to create an origin group so we'll go here and create a new origin group so my origin group and then from there we need to add an origin yes it is very squirely going through all this my origin the origin type is going to be Azure or sorry storage static website and from here we need to choose the right one we call it static storage uh this is fine this is all fine we'll go ahead and hit add and down below we have some load balancing we don't care about any of that uh so you can see it status is enabled all this should be okay the protocol here for the health probe should be HTTP um actually let me double check I remember this is where I ran into some trouble I believe yeah HTTP I think it's htps okay I think I read somewhere that's it's like um Stack Storage doesn't use HTTP but clearly it does because when we went to the link earlier it was htps so I might have the screenshots wrong but I've corrected them in the actual instructions on my site we'll go ahead and save that I think it's htps and then the the the protocol here um we'll just leave it matching it's totally fine origin path is fine we'll go ahead and add that there and we'll go review create we'll give it a moment and we'll create all right so after waiting um about a few minutes like 2 3 minutes it looks like uh as your front door set up so we'll go to the resource and I'm just hoping this works sometimes you have to play around with the settings but if you see the endpoint host name we'll go ahead and grab that there paste it on in and we get a 404 so something's not working just right so we'll have to go do some debugging which I was hoping we didn't have to do so we go to the front door manager this is the same setup we have here before there are like different ways to get to it um but we'll have to just kind of go through and debug it so we do have my route my origin group so we can click into our origin group and then from here we'll expand it and we have our route so there's something that is not correct and that's what we have to to figure out so I'm just going to double check my instructions cuz this was a bit tricky to figure out um and it really came down to like these protocols here so let me keep checking and I mean I think the probe is working correctly so if we go on the origin if this wasn't working correctly then it wouldn't it wouldn't show Green so go back up a step here you know what maybe we should just wait a little bit because sometimes it takes time to propagate and I don't I don't 100% trust that it's not working because I feel like we configured it exactly right so we'll open up a new tab here says 404 still my origin group that's fine we'll go here yeah so what I'm going to do let go back to our storage account I just want to double check uh what that uh string was for the static website I'm pretty sure it was htps it is HPS so there's no reason that should not work this is what I was trying to uh spare you the uh the debugging of this and it's very common it's not just Azure anything that has a CDN sometimes it's it's difficult to uh figure that stuff out we'll go ahead and edit that route and we'll just carefully look at what we have here so patterns to match that is fine it's the correct end point both Protocols are accepted that's totally fine redirect HPS that's totally fine match that doesn't matter so it should just work the only thing we didn't do was enable caching which I think wouldn't hurt to do um because it is a CDN we don't take advantage of it if we don't turn it on so we'll do that and say ignore that query string I mean that's not going to fix this problem if the routing is messed up again I'm just hoping that maybe it just has to do with propagation go back to the overview here and we will open this again whoops grab this let's just make sure it's doing HPS we're working to restore all services as possible right now what do you mean our services aren't available right now let's go look at Azure status uh page here and we will take a look at front door as your front door it's saying that it's fine Us East there's no green beside it good so we go down here I guess it's a it's a non regional service so if it's green it's green so you know what I'm going to play around with this for a little oh 404 web so this is better right it's just saying it can't find the content well I mean I guess that makes sense because that was there so we'll just clear that out oh now it's loading so I think it what the issue is there was no issue it's just propagating to all the servers and it took some time so uh I think that's what really threw me for a loop um when I was originally doing it so just give it some patience and it will work eventually so we are all done here so we can go ahead and clean this up and I'm just looking for the resource Group here we'll go ahead and delete it and there you go that's Azure front door and I'll see you in the next next one okay hey this is Andrew Brown from exam Pro and in this section we'll be covering Azure CDN Azure content delivery network is an Azure service that provides your applications with a distributed network of servers with Azure CDN you can improve your applications load time save bandwidth with caching strategies and speed up responsiveness with compressed files Azure CDN centralizes requests from your origin into a single location making it easier to manage your inbound and outbound traffic with features such as caching strategies these outline which requests or cach and specify their expiration timelines file compression by minimizing static file sizes Azure CDN ensures bandwidth the ficiency during requests Geo filtering disallows or denies requests based on geographical regions global distribution azures regions span across the globe ensuring widespread reach integration Azure CDN seamlessly integrates with other Azure Services robust security Advanced security measures are in place without any extra cost scalability and load balancing Azure CDN makes application scalability straightforward thanks to its builtin load balancing Azure CDN can help you reduce load times save bandwidth and improve responsiveness it's used to cach static content such as images CSS or HTML Azure CDN is ideal when you're developing or managing websites or mobile apps encoding and distributing streaming media gaming software firmware updates and iot and points in essence Azure CDN provides a robust framework to enhance the delivery and performance of your digital assets the next topic will delve into are the different tiers available for Azure CDN Azure CDN is available in the following tiers Microsoft CDN standard Verizon standard aami premium Verizon aami is one of the world's largest CD provider with a large distributed network of servers around the world on the other hand Verizon media operates a global CDN platform with a focus on media streaming delivery and security the CDN features greatly vary based on the chosen option and it requires exploring a large feature table comparison keep in mind that Azure CDN from acami is scheduled to be retired on October 31st 2023 so that's a quick overview of azure CDN tiers next we'll explore the purge feature in Azure CDN The Purge function in Azure CDN serves as an essential tool when you want to refresh cach content it enables the removal of cach content from all Edge points of presence ensuring that the latest assets are fetched directly from the origin when requested when you're looking to purge specific files within a directory while cards represented by an asterisk can be employed for example to clear all files in a directory you'd utilize this wild card feature however note that the capabilities like like Purge all and wild card Purge AR available for Azure CDN when sourc from acam overall The Purge feature in Azure CDM provides administrators with granular control over the content caching ensuring that users always have access to the most updated assets hey this is Andrew Brown from exam Pro and in this section we're diving into Azure service bus service bus is a fully managed Enterprise message broker that allows you to publish or subscribe to topics and Q messages it can scale your applications with asynchronous messages and builtin integration with Azure Services Azure service bus allows for single or batch messaging message load balancing topic subscriptions message sessions and transactions ensuring compliance with industry standards this includes protocols such as amqp 1.0 with JMS 2.0 available for premium SKU and JMS 1.1 for standard SKU service bus key Concepts the namespace works like a server with cues and topics Q contains the messages sender is who sends the message receiver is who receives the message topic is a queue with multiple receivers that works like a queue subscription is a receiver in a topic a batch is a group of messages safe batch validates if each message can be included the batch session allows you to use fifo and group your messages in a queue Peak returns a message to the que without removing it dead letter q a q for messages that were unable to be delivered through the normal queue Peak and lock retrieves a message from the queue without removing it and locks it so other receivers cannot receive it receive and delete retrieves and delete a message from the queue Auto delete on idol sets a time span to delete the queue if it is not used duplicate detection history checks if the message was not sent earlier before sending a message it overall this gives you a comprehensive overview of azure service bus and its various components and features the next next topic we'll be covering is a namespace in Azure service bus an Azure service bus namespace serves as a container for all messaging components including both cues and topics container for messaging components one namespace can house multiple cues and topics making them versatile structures within the Azure service bus they're commonly used as application containers capacity slice think of a service bus Nam space as a segment of a large scale cluster this cluster comprises numerous all active virtual machines that fall under your control Azure availability zones a namespace can potentially span up to three Azure availability zones offering enhanced availability and resilience benefits at scale using Azure service bus means you're using a messaging system built for large scale operations offering High reliability and strength serverless messaging with Azure service bus it is serverless messaging this means you get to use the messaging service without getting bogged down by the intricacies of the underlying infrastructure so that's a quick overview of a nam space in Azure service bus next let's talk about qes and Azure service bus cues are used to send and receive messages messages are stored in cues until the receiving application is ready to accept and process them messages and qes are ordered and timestamped on arrival once accepted by the broker the message is always held durably in triple redundant storage spread across availability zones if the name space is Zone enabled service bus never leaves messages in memory or volatile storage after after they've been reported to the client is accepted messages are delivered in pull mode only delivering messages when requested configuration aspects of azure service bus cues include time to live this determines the duration a message remains in the queue if it isn't processed within this time frame it either gets removed or is transferred to the dead letter Q lock duration this represents the period during which a message is locked by locking messages service bus ensures that no two users can read or process the same message simultaneously to sum it up q's and Azure service bus are efficient tools to handle message sending and receiving in a structured and reliable manner so that's an overview of q's and Azure service bus the next topic will be covering our topics in Azure service bus topics can be used to send and receive messages a q is often used for pointto point or one toone communication whereas topics are useful in publish subscribe or on to many communication it's important to endnote that topics are not available at the basic pricing tier you need to opt for either the standard or premium tier a unique feature of topics is their support for multiple independent subscriptions multiple independent subscriptions can be attached to a topic and work in the same way as cues from the receiver's side a subscriber to a topic can receive a copy of each message sent to that topic it's also worth noting that subscriptions or named entities providing an organizational structure you can Define rules on a subscription a subscription rule has a filter that specifies a condition for a message to be copied into the subscription as well as an optional action that modifies message metadata when creating a topic consider the following configuration settings Max topic size you can specify a size ranging from 1 to 5 GB time to live the setting determines the duration after which a message is removed from the topic duplicate message avoidance this ensures that duplicate messages are processed partitioning useful for efficiently managing a large influx of events a subscription in Azure service bus is a named entity associated with a topic that allows subscribers to receive copies of messages sent to that topic in a publish subscribe model of azure service bus topics can be thought of as the channels to send messages subscriptions are like the virtual cues to receive those messages there are also additional configuration settings for subscriptions Max delivery count this setting can be adjusted between 1 to 2,000 defining the number of delivery attempts for a particular message message sessions when sessions are activated a subscription can ensure that messages are delivered in a first in first out sequence so that's an overview of topics and Azure so that's an overview of topics and Azure the next topic we'll be covering is the different pricing tiers in Azure service bus Azure service bus has three different pricing tiers basic standard and premium the more expensive the tiers the more functionality it provides let break down the features and their availability across these tiers Hughes this is the foundational messaging structure in service bus allowing messages to be sent and received all three tiers offer support for cues schedule messages this feature allows users to set messages to be dispatched at a future specified time it's available across all three tiers topics topics support the publish subscribe messaging pattern this feature isn't available in the basic tier but is offered in both the standard and premium tiers transactions and uring a set of operations are completed successfully and in order transactions are not available in the basic tier however both the standard and premium tiers support this feature to duplication this ensures that duplicate messages are processed more than once only the standard and premium tiers offer this feature with the basic tier lacking it sessions ensuring ordered and related sets of messages are processed in the correct sequence this feature is excluded in the basic tier but is available in the standard and premium tiers forward to send via this facility Ates forwarding a message or routing it via a specific path it's not supported in the basic tier but is available in the standard and premium tiers message size dictates the maximum allowable size for an individual message the basic and standard tier support messages up to 256 kilobytes while the premium tier significantly extends this limit to 100 megabytes resource isolation this provides isolated computational resources to ensure better performance and reliability it's a feature exclusive to the premium tier G Geo Disaster Recovery in the event of significant geographical or infrastructure disruptions this recovery feature helps maintain service Integrity it's denoted with an asterisk suggesting there might be additional details or conditions this feature is reserved for the premium tear Java messaging service supporting JMS a standard messaging protocol for Java this feature is only available in the premium tier availability Zone support this ensures High resilience and availability by Distributing Services across multiple isolated data centers or zones it's exclusive to the premium tier the next topic we'll be going over is a dead letter q and Azure service bus the dead letter q and Azure service bus is a specialized queue that stores messages that couldn't be delivered or processed successfully these messages might fail due to various reasons message that is sent to a queue that does not exist Hue length limit exceeded message length limit exceeded message is rejected by another q Exchange message reaches a threshold read counter number because it is not consumed sometimes this is called a backout q the message expires due to per message TTL or the message is not processed successfully dead letter cues provide several benefits including monitoring failed message deliveries to understand and address the underlying issues ruing messages for another attempt at processing especially after resolving the reason for the initial failure initiating followup actions such as alerting remediation or alternative processing paths when specific failure patterns are detected in essence dead letter cues are vital for ensuring the reliability and resilience of a messaging system the next topic will be covering are the Azure CLI commands for Azure service bus Azure CLI offers various subcommands specifically tailored for Azure service bus these subcommands enable efficient management and configuration as serviceus geore recovery Alias as service bu migration as service bu Nam space as service bu Q as serviceus topic notably unlike Azure storage Q Azure service bus does not have direct CLI commands to send messages to cues or topics for sending messages to a que You' need to use the Azure SDK for example using the node.js SDK you can install the necessary package with npm install at Azure servicebus this will allow you to integrate Azure service bus functionalities directly into your application and send messages programmatically so that's an overview of the key Azure CLI commands for Azure service bus hey this is Andrew Brown from exampro and we are going to take a look at service bus so what I want you to do is type in servers bus at the top here what's interesting is that this is the old icon they have a new icon so just realize that there's some inconsistencies there and that's not my fault that's azure's fault um but the first thing we need to do is create a name space because a service bus is kind of like you you know storage account where you can have uh a variety of different kinds of storages well you can have uh more than one type of messaging system and so we have our traditional one uh like event messaging similar to storage CU but with first in first out functionality and we have Pub sub via topic so what you'll do is create a new service bus namespace and I'm going to create a new Resource Group I'm can call this the a204 um service bus and we'll say message or sorry Q because we're going to do a que and then we're going to do a topic separately and for this I'm going to call the Nam space um service bus Q to keep it simple we'll let it launch wherever it wants to launch and notice there are multiple pricing tiers depending on the tier affects the functionality so if we do basic we're only going to have access to Q we're not going to have access to um uh topics and so this is totally safe and fine to do um like even if we did premium it's fine because it's based on your consumption it's not based on um you know you just having holding around so we'll get the basic one here and we'll go to networking I don't think there's anything interesting there we'll go ahead and review and create and we'll let that create click create again and it's deploying as that is deploying which will not take too long what I've done is set up a private repository here you'll probably see me use this throughout the course it's literally an empty repository because I already have the code done I've been doing the follow alongs and document them here in the free Azure developer associate but when you're doing follow alongs with me you should do them with uh do them from scratch and then if you need to you can reference the stuff here so um I have this uh separate repository I have a g pod account which has a free tier you can totally do this in your own Visual Studio code on your little local machine the reason I'm doing a g pod is because I always want to show you how to set up the CLI and those other tools um and when you launch G pod gives you a blank environment so I'm just going to launch that up there as that's going we'll go back here and take a look and see if this is ready just hit refresh here it is still going but we already have our environment and while that is going in the background I want to go install the Azure CLI so we don't even have a single file here I'm just going to say readme.md so I can see what is going on here maybe we'll just dump things in here as we go I'm going to go get the Azure CLI uh Linux because this is running Linux Ubuntu uh here so something you should always check is like uh what Linux version am I running if you're on Windows of course this is going to be different but even Windows using the windows subsystem Linux is using Ubuntu as well so what I'll do here I'll go to the first link nope that's not the one I want maybe the second one there's usually like a command here I can run uh which Linux version am I running let's try this one here it's usually like so maybe it's this here cat proc version it really does vary based on what you're using and so here I'm going to go file uh or sorry terminal new terminal we'll paste that in there hit enter and so here it says Linux 513 Ubuntu 11 uh 18 so I know that this is auntu I don't really like that one there um let's try this instead because it just doesn't read very well there we go we're running Ubuntu 204 I already knew that um but I just wanted to double check and the reason that matters is that when you're installing the CLI it might matter what version you're using so we're going to go here and I'm going to go to Linux and the instructions might vary this one says 16 18 20 so they're all the same here and we have this one liner here that we'll install and what I'll do here is drop drop it in here and hit enter I'm not sure if this font is too small so while that's going I'm going to see if I can bump up our font here I'm looking for the terminal font size here terminal let's just say 20 here there we go and so the Azure CLI should be installed so I'll just type in clear so we say Azure or a to run it looks good to me so we'll type in Azure login um I don't want to log in with um that way I want to log with the device so we'll do ay login device because if you're on your regular computer you can just click a button and go to the browser but I'm not going to be able to do that so I going to have to do device login device well I'll have to do it the wrong way first to do it the right way soit enter Because the problem is if I go here it's going to go to Local Host because it's trying to launch in my local machine so it does that and that's no good and so here it says do the a login use device code okay so that's the one I really wanted to use so hyphen hyphen use device code enter and that will give us a code so what we do is we will need uh this link here so I'll have to expand that to here and then I'm going to go ahead and grab that code continue and so now go back this will authenticate it'll just take a second here close the tab here there we go may had to close the tab uh and so now I'm authenticated so I should should be able to uh do whatever I want um what I need to do next is create ourselves a message Q so we'll go to the resource here and notice here entities it only says Q now if we had uh other than the basic the standard plan then we would see topics here we'll go click into que click create a new queue I'm going to call it my qu we have some options here the Q size can go up to 5 gigabytes the max delivery count so this is the maximum deliveries time to live that is how long they live in the queue before they are dropped out or they are dropped into a dead letter a system there we have lock duration so the set the amount of time a message is locked for other receivers you can enable partitioning uh that's pretty complicated but we'll go ahead and create our que and so this should be pretty darn quick there is our queue we'll click into it and you'll notice that there isn't really a way to view messages there's not a way to add messages uh here we have the service bus Explorer um which I guess technically you can send and receive here I had not noticed this before at least it was not working for me so I suppose we could send a message here saying like Hello World this literally wasn't here last time I checked here um and we can go ahead and just hit send okay and notice here it says there's one active message and we can receive it say yes and so it says it received the message it's not showing us the answer so I guess they're kind of is something here I guess they're still working on it but uh mostly what we're going to have to do is uh do things programmatically so that is why we have this account so what I want you to do is open a new tab here and we're going to type in aure service bus documentation cuz we're going to grab some code there modify it make it our own so it's a bit easier to to work with so here I'm in the service bus we'll go to tutorials um I'm not sure if this one is the right one Azure service bus documentation Q it's the same thing here but this is doesn't look right what is service bus I mean it is the right page but it had a couple tutorials here uh that I had here so we'll type in like Azure service bus uh tutorial topics sometimes things aren't where you think they're supposed to be okay we'll type in service bus seems like the same page again ah it was quick starts sorry so we have tutorials here and then we have quick starts so under the quick starts this is where I was Finding uh the example code that I thought was okay notice that we could do everything via the CLI um that is not that fun but I mean this only does the creation of it doesn't necessarily do sending and receiving messages notice so that we only can use code so we'll use JavaScript because I think that will be the easiest to use uh so I already have node node comes preinstalled on git pod you'll have to figure that out for yourself on your own machine or you can just use get pod as well because it does have a generous free tier what I'll do is go ahead and paste on in this command it doesn't seem to want to paste today so I'll hit copy and then we'll go back here and go right click enter paste hit enter and so what that will do is install that Library if you're not very familiar with nodejs package Json is the package manager and this is showing that this requirement is there I want to install one other thing uh called EnV this will make our lives a lot easier um for node it comes for different things but I just want it for um JavaScript here so then we'll do mpm install. emvy save that's just a safe way for us to pass along our environment variables and so now both of these are installed so what we'll do is we'll go back over to this code and we'll scroll on down and they have one called send and they have one called receive so what I'll do is create myself a couple files here so we'll have send. Js and we'll have receive. Js and then what we'll do is go ahead and copy this is the send code so we'll put this in the sjs file and then down below we have the receive code and we will paste that on in there so I'll just make this bigger and we'll take a quick look look here at what it's doing so what this does is it Imports the SDK for service bus we need to set a connection string we need to set the Q name here is a bunch of messages that we are going to be passing along here we establish a service bus client very common in all sdks a set of a client first then we are creating a um sender and then here we are doing uh create batch messages so it's a way of sending messages in batch very efficiently so we have a for Loop here and uh so it says there's a batch and then it says try to add the message to the batch if it's not wait until it's ready then send the message okay so pretty straightforward for that code receive is going to be similar so connection string Q name create that client create a receiver and then from there we will set up a Handler for the receiver an error and so then we'll subscribe and we will listen for the message and handlers so even though we are doing cu's it's called a subscription still so just don't get too mixed up with that what I want to do is just make sure that we're passing our environment variables in safely or configuration so this is pretty standard or uh good uh best practices when working with any language the idea is you don't want to hardcode your values so I'm going to do process EnV uh and we'll do connection string here and then we'll do process EnV Q name this is the way you grab environment variables in JavaScript for every language it's a little bit different okay and I believe these are the same so I'm just going to go ahead and grab that there like this and I'll paste that on in here and I want to load environment variable so I'll make a new file here called EnV this is all part of that EnV dot thing they we're looking at EMV dot I'm just pulling that up again here or EnV you going to get the the right one because we need this line here requir Dov config that will load the environment variables uh it'll load it from that EnV file so we will go above here and hit paste and then we'll go to the receive here and do this as well and in here we need to Define these so I'm just going to copy this so I don't have to type it out by hand we will paste that on in here and so I just need need the Q Q name and connection string we'll just say equals and then equals so our Q was called my q and then we need to go grab the connection string so I'm just thinking here this is probably yeah it's at the names Space level and we'll go to share access policies notice it's called shared access policies remember when we were doing the storage key it was like called key access so it's totally different interface this is what I'm talking about where um Azure is inconsistent we're clicking on the root manage shared access key probably could create your own so it doesn't have full privileges but for this purpose we're just going to use this one on the Le hand side we have a primary and secondary we're going to use the primary one and we will go back here and we will paste on that value in so I'll paste that in there notice we don't have to do double quotations here it should uh already Escape in double quotations but when we're doing the CLI when we did the storage accounts that wasn't something that we could do um so we have these two values here so they should get loaded when we use them and this should all be good so we'll type in node sjs and hopefully it just works fingers crossed and so it sent a batch of messages to the queue so we'll go back over to our Q here and see if we can see anything and we'll click into here I'm just trying to see so there's 10 active messages that are here right now and so what we'll do is we'll receive all those messages so we'll go up and hit node receive JS so this code is now receiving those messages from the service bus q and we're just going to wait here because it takes time for whatever reason to uh finish here but we'll give it a little bit of time here to figure out that it's done still waiting there we go and so that's all there really is to it so that is q and we will do this again but next time with topics so what I want you to do is make your way over back to your resource groups we'll find the one that we just created which was uh this one here AZ 204 service bus Q we'll delete this Service Group hit delete delete and there you go and as always double check to make sure that you've uh for sure deleted that stuff and that's it for service bus Q we'll do topics next okay hey this is Andrew Brown from exam Pro and we are looking at service bus and this time we're looking at topics so what I want you to do just like before is go to the top we'll type in service bus and we'll go to the service bus uh service you can still see the old one is there it should be deleting that's how slow this thing is but we'll create a new one and we will create a new Resource Group and we will call it a204 uh service bus topic we'll say okay we'll name this service bus topic and this time it already exists and if it does just dump a bunch of numbers here on the end because it's Unique based on that so having a domain name if somebody has it you're going to have a problem and so here I'm going to go to standard because in order for us to use those additional features we will need to be on the standard plan so we'll go ahead and hit review and create and that'll take a little bit of time to create but while that's going I'm going to uh launch my environment here so this was the one I was just using a moment ago with uh GitHub and so what I'm going to do is I'm just going to go back to our repo here I'm just doing it off screen because I don't want to expose all my stuff here here and again if you want to you can do this in your local V Visual Studio code I just want to show everything from scratch every time so here's my M3 repo with get pod and so I'll just close this one and it will vanish all that code is now gone I'll H get pod this will launch a new environment it's trying to tell me to open the last one nope I'm going to make a new one for this workspace here and we'll get going here in a moment so this namespace has been created so we'll go ahead and hit create and uh I guess it's still making I I thought I already deployed it there but I guess I didn't okay while that's going we'll go ahead and install the Azure CLI we'll type in Azure CLI Linux because that is what we're using here today we'll go to Linux here we'll scroll on down grab this oneliner here to install it I'm going to open up my terminal your terminal might be somewhere else allow I'm going to go ahead and paste that on in there that's going to install our Azure CLI as that's installing it will be done here in a moment let's see if this is done this is still creating the uh namespace I think and this is still installing shouldn't take too long while this is going we can start grabbing the code for this so for this we'll type in Azure um Azure service bus documentation and this I found it under the quick start so we will go to Quick Start Ser topics and services because there's some code here that I want under the JavaScript and we have one for send to topic so we'll just grab that name there make a new file send to topic JS and we will go down here to this other one this is receive from subscription you're going to notice this is very similar to doing a q the difference is that you can have multiple subscriptions consume the same stuff uh our CLI is done so we'll type in a login um I can't remember what it is so we'll hit enter here I know this is the wrong way but I just can't remember what it is I want used device code that's what it is so I'll go ahead and grab this and I'll just hit contrl C to exit out of that and then paste that in there and then we'll do it the way that we actually want to do it so then we will go ahead and grab grab this for device login we will provide the code AS suggested here we will hit continue we will close this and it will say that we are authenticated so we are now authenticated we can use a or the Azure c um I don't know if we need the CLI well we have it anyway at least we had to authenticate so at least that was out of the way I'm not sure if we're actually going to use the CLI in this one um but what we'll do is go to the resource here and we want to create a topic this time around and I'm going to call this my topic we have a topic size between 1 to five you have a TTL you can do U make sure there aren't duplicates we're going to go ahead and create this topic okay so just like last time we need to install install a couple things so if we go to the top here there should be like an npm install here it is npm install service bus so we'll paste that on in there we'll need ourv so look upv again and we will just install npm install. EnV for environment variables hit enter we'll create ourselves a newv file and we will need to look at what environment variables we are going to need so uh looks like we didn't copy the send to topic content at least I didn't so we will go back over to here and go up here and get sent to topic paste that on in there we'll go all the way to the top and I'm looking for what we need here so here we have one which is connection string whoops did not mean to delete all that so we'll hit process EnV connection string and we have to spell it right or it's going to have a problem and then we have process EnV topic name and I think in this one it actually also has subscription so we'll have to have a third one so I'm just going to copy these two over so I'm to type it 100 times and this one will be process EnV subscription name you're noticing I'm not having the um semicolon that is optional in JavaScript so it won't break anything if I don't have them there subscription name and so these are the three that we'll need I'm going to just split this to make my life a little bit easier and I'm just going to copy this here paste not exactly how I wanted to paste it but that's totally fine I'm going to copy this I don't think we made a subscription yet but I know we're going to call it my subscription to make our lives easy so we'll say my topic my subscription and we'll grab this in a moment so we'll go back over to Azure we got too many tabs open here while we're here we might as well go grab this code um before we go back to Azure so I'm going grab grab this EMV config that is going to load our configur uh configuration environments so we go to the top here paste that on in there for both files again best practice never to hardcode your values always pass it in like that with environment variables we'll go back to our service bus the first thing we need well we made a topic we're going to need a subcription so we go to topic here and click into it we can create ourselves a subscription so we'll go here we'll say my subscription notice we have Max delivery count we have to set this between a value of 1 and 2,000 I'm going to say 12 for fun we can set the idle notice here if we want to um have first in first out we would checkbox enable sessions we're going to leave that alone does not matter too much for our demo so we'll go ahead and create that subscription it is created now what I need you to do is go back to the the service bus topic namespace and from there on the left hand side we're going to Shared access policies we're going to click into the root manage shared access we're going to grab the primary connection string key the secondary would work too it's just a second optional one because they always give you a two we'll paste that on in there and if this is all correct these should just work so now what we'll do is type in node um actually we did want the CI installed because I wanted to show you uh that there was stuff in the queue whereas with um storage queue when we're doing Azure storage que I couldn't show you because um I just didn't know of a command I believe at least I think that was the case but any we'll take a look and see what we can see okay um so actually we didn't do it at any time I wonder which one I did that for let's just double check here I have um off screen here somewhere my instructions because I wrote it for one of these maybe it was for the que that we did it yeah I didn't do the last one but I did an Azure service bus Q show and the idea was to show you that there was a message count 10 so you could see the que but I think that since we saw it in the UI just wasn't too worried about it now did I do it for this one I'm not sure yeah we do a topic show so we'll do that for fun but first we need to insert our messages which are part of the topic send I don't think we read through these so let's just quickly read they look very very similar to the last one so you have your messages that you want to send you create your yourself a client uh you create yourself a sender you create a batch message send message like it's basically identical like I can't even tell the difference here except here we're supplying a topic name so um I mean I see it here ah here create Center the topic name is specified there so I just imagine that instead of providing a topic name you provide the Q name and that's how it knows the difference but anyway what we'll do is go ahead and execute this code we'll say node sent to topic uh JS okay and so it sent the stuff now we didn't do this last time so let's do it this time around so we'll type in Azure service bus topic show um and here we'll need to set the resource Group so this was called I don't know let's go take a look here what is our service group called it is called AZ 204 bring that down a bit here a 20 for service bus topic we need to specify the namespace name so that's just called service bus topic hyphen 7238 49 because we couldn't get the number we want and then we need to specify the name so I assume it's the name of the topic so the top is my topic and I'll enter and it says service bus name is misspelled or not recognized by the service you mean service bus yeah I got to spell that right enter and that looks fine but I just want to specify it as output yaml just hit up on your keyboard if you want to go back to those previous commands with enter so this is a little bit easier to read and so what we're looking for here is just kind of like the message count does it show us here we subscription count I don't see it so I guess it's not visible in the same way as uh the que is like if we did this and we didn't do in the last one it would we just saw like that message count there but let's take a look at what we can see in the CLI to just see what information or a UI that or portal so we can see some information here so we have one subscription here we'll click into here um we have Max size incoming request 12 um yeah I don't really see it message count 10 Max delivery count 12 okay so I guess there was 10 and that's the 12 so I guess that's where it's being counted let's go run the other one to receive I'm going to just double check to make sure that we set those we did that's all good so we'll type in node receive if you're wondering how I'm Auto completing without typing that I just hit tab on my keyboard so it's receiving those good and that finish I'm going to go hit refresh see if there's any difference here notice that the message counted zero so when the topics were there it was held in the subscription saying 10 is here and they've yet to be delivered when we ran it they were received and so that number cleared out that's all we really need to learn for um uh topics so we are done with topics so let's make our way over to Resource groups and we'll go over to our service bus topic go ahead and delete this here delete there it is deleting we are all good to go um and you know just as always don't ever trust Azure to delete these things come back and check in 3 four minutes make sure it's deleted so you just don't have things lingering around um but uh yeah there you go hey this is Andrew Brown from exam Pro in this section we're diving into Azure event grid Azure event grid is a service that allows you to manage event routing from any source to any destination event grid provides a simple and strong customizable event delivery process that allows you to manage at a minimum level which types of events will be received as well as which subscribers will receive those events one of event grid strengths is its event fan out capability combined with 24hour retry reliability ensuring your events are consistently delivered it's a costeffective serverless solution with dynamic scalability making it an excellent choice for businesses of all sizes asure event grid is ideal in eventdriven architectures you can subscribe to events from Azure resources and then route them to an event handler or webbook Furthermore with custom topics you have the flexibility to craft and publish your own events within your event grid so that's an overview of azure event grid the next topic will be covering our event sources and handlers in Azure event grid Azure event grid is divided into two categories event sources and event handlers azure's event grid operates as a central Hub coordinating between various event sources and event handlers on the side of event sources which are services that emit data we have blob storage signal R resource groups Azure app configuration subscription Azure machine learning event how Azure communication Services media service Azure cash for Ries iot Hub Cloud events service bus Azure policy Azure Maps custom events and Azure container registry these sources send their data to the event grid on the other end we have event handlers which are the services that receive and act upon this data the serverless code category involves functions for workflow and integration the services user service bus and logic apps buffering and competing consumers handle the influx of data through event Hub and storage queue additionally there are other services and applications which incompass hybrid connections web sockets web hooks Automation and essentially any service or application these handlers effectively process or route the data sent by the event sources through event grid enabling a seamless flow of information across Azure Services the next topic will be going over are the key concepts of azure event grid domains are used to group event grid topics that are related to the same application for easier management topics these serve as the destinations to which events are dispatched system topics Azure services offer these builtin topics in contrast custom topics pertain to individual applications and thirdparty topics additionally partner events enable thirdparty software as a service providers to broadcast events events these encapsulate specific occurrences within a service Publishers is the service that published The Event Event sources are where the event took place event subscriptions are the mechanism that routes the events event subscription expiration this allows users to designate a lifespan for their event subscriptions after which they become valid event handlers these are the applications or Services tailored to process or act upon the received events event delivery is the delivery of events and batches or a single events batching is the sending of a group of events in a single request overall these are the key concepts of azure event grid hey this is angre Brown from exam Pro and we're going to take a look at event grid Basics so let's get to it so the first thing we're going to want to do is we're going to search up subscriptions because we're going to need to make sure that in uh like in order for us to use aent grid that we have it turned on and so we'll go into our subscription and we're going to go under um resource providers resource providers ah there it is and this is all the stuff that is registered all the providers that are registered so what we're doing is just make making sure that event grid is turned on because that one's not always turned on by default and just make sure that it's registered okay so you'll know that it's registered because it'll have a green uh check mark here and it'll say registered and so once that is done we can proceed to create a storage account because we're going to integrate a storage account into our event grid so we we'll do is create a new storage account and we're going to create a new Resource Group I'm going to call this Resource Group event grid Basics and we're going to name our storage account event event grid Basics all right and uh yes you can have hyphens you can't have anything else um and we'll just make sure we spelled that right Event Event grid Basics I can't remember if these are fully qualified domains if they are you might have to add some numbers on the end there but it is what it is we're doing uh USC I mean it just randomizes every time but this is where you should probably set it to we have standard premium we'll leave it as standard and everything else seems fine so we'll go ahead and create review and create and then it's going to allow us to review we'll go ahead and hit create and it's going to create the resource Group here we'll just wait for it finish uh deploying and then we'll go into the resource all right looks like it is finished deploying so we're going to go ahead and go to that resource we're going to go to containers and we're going to have to create a couple containers the first the first one is going to be called basic we're going to leave it as uh private just make sure it's basic not Basics and and we'll create another container called basic alt the idea is that we're going to use an event Grid in order to move one file from one storage account to another uh and that's going to be facilitated by log uh via a logic app because that's going to be the easiest way to use event grid um so what I want you to do is search for logic apps up here and we're going to go ahead and add a new logic app and what we want to do is choose our Resource Group and we'll just go event grid Basics here uh we are going to name uh this event grid Basics um maybe we'll do LG to indicate that it is a logic app or LG maybe uh L yeah LG is fine like short for logic and from here we have a workflow or a Docker container we're going to stick with a workflow um just to knock on wood we're going to just put in the same region as our um storage account so East US we have standard consumption I'd rather do consumption for this um so you pay only as much as you use because we don't need Enterprise level service application here we just need a consumption model we're going to leave this uh to disable so that is totally fine there's nothing else to do here so we'll go review plus create and we'll go ahead and create this logic app and we just have to wait for this to finish deploying all right so that should have been very very quick like under uh 10 seconds there so we've gone into the resource I just click go to the resource and so we have this uh very fun interface and so what we need to do is start with a common trigger there's a few different ways to get to it but there should be something on the front here uh I don't know if they redesigned this recently so I'm just going to search start with a comment oh yeah it's up here okay I'm being silly um and so what we want to do is because this is an event grid follow along we want to click on when an event grid occurs and so this is the designer where we can make things a lot easier for ourself and so we're going to have to first sign in to authenticate so I'm this is my tenant exam Pro Training Inc so we'll go ahead and get connected there just give it a moment we'll select Andrew Brown which is totally fine and now that is connected so that is great so once we are signed in we can click continue and we're going to go ahead and add select our subscription here um and we need to choose a resource here so I guess in this case it's going to be event grid event Grid or could have swore yeah yeah I think that's what we want to do let me just double check here oh you know what it's just not for some reason I'm I'm searching it's not Auto completing properly okay I just wasn't sure there and as far as I can remember this would be probably an event grid topic and then we need to give this a resource name so um let me just think about this for a moment okay all right so I think I understand where my confusion was it was because we click continue and I never I didn't see event crit anymore so I thought we had to configure it when it was already configured right so this is where we were so we're not we at this stage like event Grid's already hooked up so it's ready to be triggered so this is the step that follows into it which is where we want to do our storage account so that's where I was getting confused so we'll choose our subscription here it's okay you know if you ever get confused just step uh step back a couple steps and just double check what you're doing happens to me all the time so um what we want to do is actually um connect storage uh storage accounts so we type in storage accounts here great uh we'll have that selected and then we need to select our storage account so this one's called event grid Basics and then we're going to have to enter in um some additional information event type so we want to have it happen when we add something to The Container so the basic container so we'll do blob created um and then from there we need to actually filter out the information so we need to add a new parameter and I think we'll have to do it on the prefix filter so a filter like whatever yeah so that's probably a good idea CU then we could place it into a particular place and I believe that um there are very specific uh filters that you can do for this because if I recall there's like standardized ones yeah see here like it's always going to be SL blob Services default containers Etc and you'll know that because you know if you read the documentation and you have to do that stuff you'll figure that out so I'm just just going to type it by hand here blob Services default containers and then we can put our container name so basic uh and I believe we have it without the S there so uh Services see I don't trust my writing here I'm just going to copy paste it in okay and that looks good to me um so I think that is what it needs to be so we'll go ahead and hit the next step and so the idea is anything in that folder uh like when something's added to that folder then follow up with this operation right um and maybe before we do that we should probably um you know observe that this stuff works that's probably be a good idea so what I'm going to do here is I'm going to make my way back over to our storage account so we'll just close this tab here I'm going to open a new tab and we're going to make our way over to storage accounts and we'll go event grid Basics and we will go to containers and we'll click into our basic container because I want to just see that this is working um and I'm going to need a file upload so let me just go grab an image really quick all right so I just grabbed an image off the internet so I just have data here but before we upload we probably should save what we have because if we don't save it we're not going to be able to observe it so I went back to logic app and we just hit save in the top left corner so we'll give it a moment to save and it looks like it's saved now I'm just going to go back over to here if we can um look at some of the code that gets executed I'm just trying to remember uh where it is because once it executes we want to um see what happened right so what I'll do is I'm going to go all the way back over here and I'm just going to go and drag or actually I'll have the upload button so I don't trust that that there and I'll drag it onto here nope I still don't trust it so what I'm going to do is just click the files and I'm just going to grab it this way and say open and we'll do upload and so that is now uploaded and so there is somewhere where we can observe um where stuff has happened so I'm just trying to remember where it is um I mean we could run the trigger we yeah we probably should run the trigger right run I think it's running so we'll just give it a moment okay you know I was thinking about it it doesn't make sense we shouldn't have to run it because it should just happen automatically I think it's on the overview page ah okay so if we look here we can see the Run history and so and there's also trigger history of when the things are triggered so we could we could manually but it doesn't make sense so I think this is the run that we just did if we click into here yes this is what it is so here we can see what what has happened so if we expand it we can see the inputs right so it's we have a blob created um it might show some information so here we can see data yep and it's a webp file and so it's gotten this far through and so that's a great way to kind of like debug so you can uh logic run app you can do it each step but right now we are using um event grid to do that integration right we're just doing through logic uh logic app because it's a lot easier so now that we have that what we should do is go back to our designer and we're going to have to add the followup step um so we have this oh yeah okay so that's the first step event grid doesn't show up there which is weird but um so we have this step here from our storage account and so the next step what we want to do is put it into another uh container so that will be the tricky part um so I'm just trying to remember what we do so we'll hit next step and um I think what we need to do is initialize a variable first because we're going to have to get some way to grab the name of the string because if we go back to our run over here just give it a moment here and we go enter a run again here we need to extract some data to pass along because there are some limitations in terms of how Json gets passed along or data gets passed along and so um what we want is we just want this part of the name we want to say take this name as the identifier so that when we're copying stuff over it will work and so what we'll have to do is store that into an intermediate variable so um we'll just type in variables here and I'm just seeing ah yeah so they look like this CU I can remember they might be in the builtin yeah that looks a lot better um and so we need a variable and it's an initialized variable and we're going to name this file name and this is a string of course and now we need to insert the value so in here what we need to do is write an expression in order to extract that information out um so what we'll do is go to the expression Tab and over here you can see we have all sorts of Expressions that we can use so going to type in last parentheses and then in there we'll do split parenthesis and then what we're looking for is trigger body and then we'll do question mark Square braces single quotations subject how did I know how to do that I I looked it up I looked it up somewhere and you know I just don't feel like there's much uh reason to to teach this part because uh you I mean if you really need to know you can go here um and learn all about it but a lot of times like if you need something you can just say I need this kind of function somebody's already done it right because there's so many common use cases so I probably search something like how do I get the name out of the the thing you know like for the blob and somebody had that there but it makes sense to me so let's hit okay here and it should turn purple because it is dime expression if you type it in here it probably won't work correctly you have to type it in here and then hit okay so it shows up like that uh but you notice we typed in like trigger body so if we go back over to our run here um this is the body here so when they say trigger body they're talking about here and then it was just grabbing that subject line there all right um so that would be the second step and that gets it into a variable but the next part is we need to actually um get the blob content and then insert it and then create a new blob so what we'll do is hit next step and we'll type in Blob and see if we can find anything here and from here we need to get the get Blob content using path version two so I'm just going to scroll down here and take a look for it there it is based on the path and we'll go down here and um um I guess it would be access key oh because we're setting up a connection for the first time so enter name for connection um I know what the storage account is but what is the connection connection name I do not remember give me two seconds okay there wasn't much to help me here cuz what I remember before was that you click it and you'd authenticate it like the event grid but it's not doing that SoBe maybe we just have to name it something so I don't know we'll just say Azure storage account maybe it just wants a name maybe it doesn't really matter oh yeah like they sign in that's what I want so connection name yeah so we'll say uh you know storage account event grid okay because if we can just single sign on let's do that that's super easy and we'll click that there okay so this is starting to look how I expect it to look and so we need the storage account name I don't know why it's not showing me any names here but that's okay we'll just go over back to our storage account here and it's called event grid Basics so we'll type in event grid Basics event grid basics uh that's custom value sure I mean that's what its name is I'm not sure why it's not autocom completing um but here what we need to do is we need to provide the path so it's going to be for SL basic ah and so now there's our environment V or that variable so we'll just click that there so that will make it super super easy now notice that it is showing basic now so I just clicked here the folder we typed it in manually but we could have clicked uh here and then put the environment variable in or the uh this this initialized variable in here but I did type that manually and it still did work correctly so we are okay here infer the content type sure why not um it doesn't matter if they do um so this gets the content so now this gets the path and so the next thing is actually to create the blob you can't like do an easy clone you have to do it this um intermediate step that's just how it works um and so what we'll need to do is go to our builtin once here we'll type in Blob again maybe standard and this time we want to create a blob so there it is uh block blob no we just want a blob and so what we'll do is um I guess we have to connect again I'm surprised it's not showing the name yeah it's just the name that's fine so we'll go back over here I just don't want to type it wrong so we'll just copy paste in event grid Basics event grid Basics enter custom value because it's giving us so much trouble for no particular reason make sure there's no space on the end there there now it works fine um and in this case what we want is basic alt and the blob name can be the file name which is totally fine and the blob content will be the file content and I don't think we need anything else so what we'll do is go ahead and click off and we will save all right so that's just the way we're going to have to do it um so what we can do is go back to our overview and we'll go back to our basic folder and we'll delete data say okay and we'll go upload we'll select our file again we'll grab it we will upload and then we will make our way back over to our logic app close this tab here so we don't get too mixed up refresh the page and it failed so it failed for some reason so something has not been configured correctly it failed on the initialized variable so something's wrong there so unable to process the template language expression in the actions initialized variable uh inputs at line Z column zero template function split is not defined is not valid so it's possible I just spelled it wrong so what we'll do is go back to our event grid we'll go back to our logic app designer here initialize variable we will click it and we probably just spiled it wrong spilt spilt what if we do lit spilt split um so if that's wrong we'll just scroll on down and we'll just take a look I could have swore that it autoc completed for us oh you know what it is spelled wrong it should be SP i l t spoit all right and I'm just double checking to see if there's any other problems here no looks fine to me so go ahead and say update we will save it in the top left corner we'll go back we'll delete our file here we'll say okay and we'll have to select a new file we'll click open just double check make sure that's been saved it looks like it's been saved we'll hit upload we'll go back to our overview page it's already running super fast by the way and we'll click into it and we'll see if we get any other failure so there's another failure that's totally okay so we'll just expand it this request is not authorized to perform this operation using the permissions um so it does not like the permissions I gave it totally fine so we will go back to our app designer we will go to this second step here even though it did select this properly so we'll change the connection uh I guess we'll add a new one so we did ad integrated oh let's do managed identity you you must enable managed identities in the logic app to use manage identities authentication you must Grant required access to the identity in the Target room resource okay uh there is an identity tab so we can go over there and take a look there quickly I don't remember it being that hard to do uh system assign manage identities restricted to one person one person resource is tied to life cycle you can grant permissions etc etc um can we just turn that on and hit save well it can be granted access resources protected by a sure let's give it a go all right so um it seems like we have to assign some Ro stuff so we can try and assign a role um can we do the subscription level contributor okay there's few different ways you can authenticate so hopefully this will be the EAS way to do it uh we'll refresh here did assign it and I don't think it said it all right so give me a moment and let me see what I can figure out okay you know what just to make this easier I think what we should do is just do the access key cuz that seems like the easiest way to do it I was just hoping that we could have you know just did a simple sign in here but it's not a big deal so we'll hit change connection we're going to add a new connection just say storage account um Event Event grid um key and so this is going to want the aure storage account name so this one will be the name of the storage account if we can find it it's called event grid Basics and then we need the Azure storage account access key so there's probably a tab called keys yep and we will show the key and we will copy the key if I don't have to pass along keys I like to not do that please check your account info again uh storage account access key should be a correct base 64 encoded string come on give me a break here I am doing what you asked me to do so we will try this again this thing just hates me today give me a second okay you know what it was really short so I really don't trust it so let's just do I just cleared it out there I didn't do anything else what we're going to do is go back here click the copy and then rightclick and paste that's so much longer okay that has to be the right key we'll hit create and we'll give it a moment okay great so that's for that one um but this has to have the right connection as well so what we'll do is just change the connection you have a few here e um and the one we want is the one that's valid so we'll go to this one down below as you can see few attempts here and we'll save it and we'll go back to our overview here I'm just going to close that Tab out we're going to close this out we're going to go back into here we're going to go into our containers we're going to go into our basic we're going to go ahead and delete this we'll say okay and we will upload a new file we will choose the new file we'll choose data upload it we'll go back over here here and I want to see the latest run here we'll give this a refresh is it running it looks like it's running it's hard because this one looks like it just failed and and now the the messaging is getting really muddy here what is it doing so we'll click off here sometimes the portal is a bit funny is it just triggering over and over again did we make an infinite Loop uhoh okay think we have a problem here what if we go here is it basic or it's basic alt this one's basic so what's the problem we'll refresh failed why did it fail conflict another active upload session exists please retry after some time okay uh well let's just go take a look here go back it's here so it's it's here uh so it clearly has worked why it's triggering multiple times I don't know um don't particularly like that there we go I'm just go ahead and delete this one here and it's just it's just going over and over and over again so there's something wrong with my workflow so this looks fine to me that looks fine to me maybe it's triggering oh you know what the the parameters out of here so this is supposed to have a prefix here so what's happening is that it's triggering on anytime a basic one is set up or basic alt one and it's just stuck in an infinite Loop which is really really bad um so we did do this earlier but for whatever reason um the changes still are not here so what we'll need to do is set up that prefix so what we'll do is type in Blob Services default containers basic because we really don't want to trigger it on any but that that uh container there and uh did it save it it doesn't look yeah I mean it should be there so what we'll do is go ahead like why is it not filtering oh um I guess it wants to filter based on name but we gave it its name so I'm not sure what else we would have to type there okay um I'm just going to put um dot it's the prefix filter so data I guess I don't know like it's not letting me save okay there we go we'll save that I just want to stop the infinite Loop there for a moment so we'll go back over to the overview and we'll just make sure we're not uh running up our bill here and I'm just refreshing I just want to see it's not triggering anymore so it stopped triggering which is good and we'll go back over to here and we'll look at this prefix filter because I I don't remember having to do this so a filter like sample etc etc so we'll type it in again I guess blob Services default containers basic it's very oddd because like we typed it oh you know what I probably did I typed in the the filter parameter here we're supposed to add it then put it in there so it's just me getting confused by the UI silly me okay so what we'll do is go back to the overview and this time we just want to see it trigger once so we go back to basic alt we'll go ahead and delete this we'll say okay and we'll go back to our vent grid we go in or our event grid our basic our basic container we're going to go ahead and delete delete uh data here again and we're going to go upload one more time it's actually good that we had that problem because I got to show you uh why filters are so important um when we're dealing with uh the app logic there um or logic apps so we'll go ahead and hit upload we'll go make our way back over here we're going to give this a refresh and now we have a new one and it only happened once and that's what we wanted to happen so we'll go back over to here and we go to basic alt there it is so that's a means to which we can use event grid to integrate stuff you can see logic app is extremely useful for developers uh building uh all sorts of tools uh but we are all done here and what we'll do is make our way over to our Resource Group and we are going to just go ahead and clean up so we'll go into event grid Basics and we'll go ahead and delete this Resource Group there we go and it's going to go ahead and delete there uh yeah and there you go hey this is Andrew Brown from exam Pro and in this section we'll be covering Azure event Hub Azure event Hub serves as a critical component for event ingestion capable of consuming millions of events from mirriad sources and processing them in real time or via micro batching its potent Auto inflate feature automatic scales throughput units to accommodate varying demands ensuring seamless processing even during traffic spikes notably Azure event Hub offers seamless integration with Apache cafka applications and clients Bridging the capabilities of both platforms Azure event Hub helps you build your big data pipeline to analyze logging anomalies user and device Telemetry where you only pay for what you use breaking down the key concepts of azure event Hub Nam space is an endpoint for receiving and distributing events to event hubs event Hub is where your events will be delivered event Hub cluster is a dedicated event Hub with a 99.99% SLA event Hub capture allows you to automatically capture and save streaming events events hubs for Apache Kafka event Hub and points are compatible with Apache cafka event Publishers or applications or services that publish events to an event Hub publisher policy is a unique IDE used to identify Publishers partitions are used to organize the sequence of events in an event Hub event consumers or applications or services that read events from an event Hub consumer group enable consuming applications to each have a separate view of the event stream stream offset holds the position of an event inside a partition checkpointing is the process of distinguishing between read and unread events diving into scaling with Azure event Hub Auto inflate a dynamic feature that enables automatic scaling up to the Pinnacle of tus predicated on traffic exigencies however it's worth noting that this feature is not accessible within the basic pricing tier the image showcases various configuration options related to Azure event Hub pricing tier the standard tier is selected priced at approximately $22 USD per throughput unit monthly other plans can be browsed for more features throughput units the selected number is one in Azure event Hub throughput units dictate the events process per second and related data volume Auto inflate maximum throughput units the maximum number of throughput units that can be scaled to using the auto inflate feature is set to 12 the next topic will be covering are the pricing tiers for Azure event Hub plans and their key features basic Plan cost 1 cent divided by hour for every throughput unit data input charges 2 cents for every million events storage holds up to 84 GB of data data hold time keeps data for only one day does not offer capture Apache Kafka schem of registry or extended retention standard Plan cost 3 cents divided hour per two data input same cost as Basic 2 cents per million events capture available at an additional $73 divided by month for each two storage same as basic 84 GB data hold time keeps data for 7 Days offers Apache Kafka and schem of registry however it lacks extended retention Premium plan cost a bit pricier at $123 divided hour but now it's for every processing unit data input included in the plan no extra cost capture included as well storage a lot more space with 1 terab for each poo data whole time retains data for a longer 90 days offers Apache CFA schema registry and also has extended retention at 12 cents GB month with 1 terabyte already included dedicated Plan cost the highest at $684 divided our for each capacity unit data input included in the plan no extra cost capture also included storage huge space with 10 terab for each CU data hold time holds data for 90 days same as premium offers all the features of premium and its extended retention is also at 12 cents GB month but generously includes 10 terabytes terms capacity Unit A measure of capacity for the dedicated plan Processing Unit A measure of capacity for the Premium plan serit Unit A measure of capacity for both basic and standard plans the next thing we'll be going over is the producer in Azure event Hub also known as the publisher the producer is responsible for sending data to the stream Publishers can publish events using the following protocols https the majority of azure sdks prefer https amqp 1.0 Advanced message queuing protocol taffa protocol compatible with the Apache CFA ecosystem generally developers use the Azure SDK for publishing events you can publish an event either one at a time events can be published individually or batches multiple events can be grouped and published together however there's a size limit of 1 Megabyte for both individual and batched events any event or batch exceeding this limit will be rejected for authorization Publishers use either Azure ad with oo2 issued JWT tokens or shared access signature comparison https versus amqp for publishing events amqp requires the establishment of a persistent by directional socket in addition to transport level security or SSL TLS amqp is higher Network costs when initializing the session amqp is higher performance for frequent Publishers and can achieve much lower latencies when used with a synchronous publishing code https requires additional TLS overhead for every request publisher policies event hubs enables granular control over event Publishers through publisher policies publisher policies are runtime features designed to facilitate large numbers of independent event Publishers with Publisher policies each publisher uses its own unique identifier when publishing events to an event Hub ensuring that events are properly segregated authenticated and managed for each individual Source enhancing security and traceability within the system the next topic we'll be covering is a consumer in Azure event Hub the Azure event Hub consumer also commonly referred to as a reader is responsible for receiving and processing data from the the stream connection protocol all event hubs consumers Connect using the amqp 1.0 protocol as events become available they are delivered through this session this eliminates the need for the client to continuously check or pull for the availability of new data a consumer group represents a particular view like a state position or offset of an entire event Hub consumer groups enable multiple consuming applications to each have a separate view of the event stream and to read the stream independently at their own pace and with their own offsets typically in a stream processing architecture each Downstream application equates to a consumer group there's always a default consumer group in an event Hub and you can create up to the maximum number of consumer groups for the corresponding pricing tier there can be at most five concurrent readers on a partition per consumer group however it's recommended that there's only one active receiver on a partition per consumer group some clients offered by the Azure sdks are intelligent consumer agents that automatically manage the details of ensuring that each partition has a single reader and that all partitions for an event Hub are being read from this allows your code to focus on processing the events being read from the event Hub so it can ignore many of the details of the partitions so that's an overview of a consumer in Azure event Hub next let's talk about offsets in Azure event Hub in the realm of azure event Hub the term offset refers to the position of an event within a specific partition offsets enables a consumer to specify a point in the event stream for from which they want to begin reading events you have the option to specify the offset either as a distinct timestamp or as a numerical offset value consumers are responsible for storing their own offset values outside of the event Hub service every event situated within a partition comes equipped with an offset Azure event Hub checkpointing checkpointing is a process by which readers Mark or commit their position within a partition event sequence checkpointing is the responsibility of the consumer and occurs on a per partition basis within a cons consumer group the consumer is fully responsible for checkpointing this means that for each consumer group every individual partition reader must monitor its ongoing position within the event stream and notify the event Hub service once it recognizes the data stream to be complete or processed so that's an overview of offsets in Azure event Hub the next topic we'll be covering is the event retention for Azure event Hub published events are removed from an vent Hub based on a configurable timebase retention policy the default value in shortest possible retention period is 1 hour for event Hub standard the maximum retention period is 7 days for event hubs premium and dedicated the maximum retention period is 90 days if you change the retention period it applies to all messages including messages that are already in the event Hub it's important to note that individual events cannot be explicitly deleted the reason for event hubs limit on data retention based on time is to prevent large volumes of historic customer data getting trapped in a deep store that is only indexed by a timestamp and only allows for sequential access if you need to Archive events beyond the allowed retention period you can have them automatically stored in Azure storage or Azure data Lake by turning on the event hub's capture feature if you need to search or analyze such deep archives you can easily import them into Azure synaps or other similar stores and analytics platforms so that's an overview of vent retention for Azure event hub let's break down a comparison between event grid event Hub and service bus event grid event Hub and service bus all event driven services for application integration and use an event bus as means to work with event data Azure event grid provides the backbone for eventdriven architectures without the need for infrastructure management Azure service to service communication primarily designed for communication between various Azure Services dynamically scalable cost efficient and guarantees at least once delivery of an event Azure event Hub streaming data ideal for ingesting massive amounts of streaming data low latency processes events with minimal delay High throughput capable of receiving and processing millions of events every second and guarantees at least once delivery of an event Azure service bus supports both cues and publish subscribe patterns making it suitable for a range of web applications reliable as synchronous message delivery that requires in advanced messaging features like first in and first out batching sessions transactions dead lettering temporal control routing and filtering and duplicate detection guarantees at least once delivery of a message and offers an optional feature to ensure messages are delivered in sequence the breakdown highlights the differences between Azure event grid event Hub and service bus showcasing their unique strengths tailored to different scenarios hey this is angrew Brown from exam Pro and this fall along we are going to learn all about Azure event Hub uh so what I want you to do is go to the top here and type in event Hub and we are going to create ourselves a new event Hub uh namespace so we'll go here and hit create and we'll create a new Resource Group as we always do and call it my event Hub and then for the namespace name we'll say um my event Hub if it doesn't let you do that you'll have to put some numbers on the end I'm just putting some numbers here because often these are taken doesn't matter what location is just choose whichever one we're going go with basic because there's not a huge difference between the pricing terms in terms of feature sets that we want to use today so we'll go ahead and create this namespace so we'll give it a moment and we'll go ahead and hit create and then we'll just wait for this namespace to provision all right so after waiting about a couple minutes there our namespace is deployed we're going to go ahead and create our CES and event Hub I'm going to call this one My Hub we'll go ahead and hit review and create and we'll create and these create very very quickly so we're not going to have to wait too long we need to set some shared access policies here so I'm going to go into the Hub here we going to hit manage I call this my SAS SAS Shar access policy and there we will now have the ability to have a primary key and connection key so we can actually connect uh to it so what we're going to do is go to GitHub and I want to create a new Repository and we'll go ahead here and we'll go down below go to exam Pro we'll say my event Hub that's already taken because I've done this before so for M I'm going to call it new and we'll go down to private we're going to use nodejs so we'll just type in nodejs so that it ignores the node modules you're going to want to have git pod or a visual studio code installed on your computer the easiest way is honestly get pod because these environments are temporary and it's free to uh utilize it so if you can go get the Chrome extension or uh if you don't want to install the Chrome extension all you got to do is attach this to the end of the repo to launch a GitHub or git pod environment so I'll give that a moment to launch and there we go so I do have some code uh for this so I'm just looking for it off screen here and uh we're going to need to have a couple files here we're going to need a new file called send JS and we're going to need a new file here called receive Js I'm not typing the full word receive because I'm always really bad at spelling it so I'm just trying to save myself some trouble and uh we're also going to have to initialize a new package Json file so there we go and we're going to have to get um a couple things installed yeah so we'll need mpm install Azure Event Event hubs Azure storage blob Azure event hubs checkpoint store blob save Dev and it make our lives a bit easier seems like I typed something wrong here I'll just hit up I forgot the Ford slash here and so we'll just go event Hub um Azure JavaScript because I believe yeah I kind of use this one but I modified it to make it a little bit easier I think this is the one I was doing no I don't think so tutorial might be this one yeah it looks like this um so this is just the JavaScript there so for send we will grab this code here we're not going to do exactly the way they do it but pretty close and then there is a receive code so we'll go down below and we'll grab this as you can see there's a lot going on here that will be our receive couple things we need to properly set so these will be all environment variables so what we'll do is go to the top here and we'll just do constant process requires process that's going to allow us to import our environment variables so we just say copy let's just save that we'll paste that in there as well and then this is where we need to replace all environment variables as you can see I always have things pcking up on me here so just close my teams out and just Lin to make this a little bit faster so default will be what we'll keep here um this will be our storage connection string say process EnV uh storage connection string then this one will be process EnV container name then we need process EnV event Hub name and up here we'll have process EnV um event Hub connection string and then we'll go over to our send here we'll do something very similar so process EnV event Hub connection string and then we'll have process EnV event Hub name so we need to set all of these I'm just going to copy this for a moment here I'm going to make a new file file new file sure it doesn't really matter we're just using this as a quick scratch pad and so uh what I want to do here is just delete out this part and we're going to do export on the end here we'll take out this one here and the idea is that we'll just set them all here and then we will make our lives a little bit easier when we have to mass set Mass set these so we do have the connection string because we saw it over here so we we'll grab the primary one doesn't matter which one primary secondary and that is for the uh vent Hub we called the the vent Hub was called My Hub I believe we'll just double check what the Hub was called yeah it's called My Hub up there we'll need a storage account so what I'm going to do wish this thing we get out of here get out of here I'm not I'm not trying to save a freaking file there we go hit escape a bunch of times and we'll go back here and we'll create ourselves a new storage account I'm actually make this in a new tab so we can see what we're doing so we go over to storage accounts and we'll create ourselveses a new storage account we'll create this uh storage account in the same name space so we'll go down to my event Hub we'll just say my um or we'll just say my event Hub 8888 again you might have to change it based on your standards but um or like what is available to you we'll go ahead and go review create and for the container name uh we'll probably just call container maybe container one we just have to wait for this to create to grab that connection string so this usually doesn't take too long just a couple seconds okay so there we are um we'll go over to access keys and we'll grab the connection string from here I believe this one should work U let me just double check yeah I think this will work so what we'll do is go back over here if it doesn't we'll find out pretty soon and we'll just generate out a a shared policy uh we'll go ahead and paste that on in here and just double checking that this is the correct one this looks identical to that one that can't be correct so we'll go back here this is the storage account so we'll go oh I have to actually hit the copy button that's what it didn't do we'll go ahead and paste that in and so theoretically this should work so we'll go ahead and copy these we'll drag our terminal up a bit we'll paste these in here and uh what I'll do is just double check that they're here so we'll say EnV GP event Hub so those are both set and then we'll do storage that one is set and we'll do container that one is set so these are all in good shape for our storage account we still have to create the container so go here and create a new container say container one we'll go ahead and create that we'll make our way back over here and then instead of just having export we'll do GP EnV this is just in case we have to restart the environment for for any reason so that these environment variables get exported twice so we paste that in there I believe those are all set go ahead and hit enter on the last one there and let's see if our code works so we'll do Ascend um actually we have to set up two scripts here so that we can actually call them so I one call here is called send this will be node send JS and we'll have receive so re just because I always spell receive wrong and I just don't want to have to type it a thousand times times we'll have that there so now what we'll do is do mpm run send see if that works it says a batch of three events have been sent we'll go and confirm that over in event Hub if it worked if we go to the overview it should show us some messages were received sometimes there's a bit of delay so we'll just give it a teeny tiny amount of time we'll hit refresh here because we know we sent them but while we're waiting for those to kind of propagate what we'll go back here and just kind of look at the code because we did didn't really look at it so the way it works is you are defining a client and it will be the producer client um and then down below so we say Okay a producer is someone that produces events it's very common in the messaging system to have a producer and consumer we're going to create a batch job and we're going to add them all to the batch job then we're going to send them all at once and it's going to close and it'll and it will complete and if there are any errors it will alert us about it so we'll go back over here and we'll do a refresh so I want to see messages messages would normally show up here so since I don't trust it I'm just going to run it again I mean clearly worked because there was no errors and we'll go back over here and um not here but we'll go back here we'll refresh and I'm just waiting to see something here processing data this is like something that's really powerful with um vent Hub here so still don't see the messages just give me a second to debug this I've done this lab like four times so it should work but uh you know sometimes sometimes it's trouble so just a moment all right so I literally did nothing and now it's actually showing up in the uh messaging queue so you know or The Hub the funnel uh so that's just something you have to consider is that sometimes you just have to be a little bit patient let's see if we can go receive those messages now by running the other uh script so what I'm going to do here is do mpm run receive and it should receive the messages as long as something isn't typed incorrectly so we'll go back over here we've seem to introduce a little mistake so I'll go ahead and save that we'll hit up and it should receive the events so it should print the three out so there we go and so we are streaming that through the consumer is technically um Storage storage accounts but if we go to the storage account there's nothing really uh um intelligible in terms of what's in here so like there are stuff in here checkpoints I guess it's saved to checkpoint I personally don't know what I'm looking at so I'm not exactly sure what the point of doing that I guess it's just saying the checkpoint is like the last point it it wrote but if we just take a look at the code here quickly you can see this called consumer so we hit consumer client there's a blob check Point store then we have the consumer client we are subscribing so it's saying hey are there any events let's consume them if there are no events throw a console log so tell us about it iterate through them then update the checkpoint so move it to the next Point um just say hey this is where it is now and that's pretty much all it wanted you to do we can go ahead and save this code we so we'll say um event Hub code doesn't matter what you name it there we'll sync the changes and we'll go ahead and clean up so we'll go back to our resour resource groups we'll go to vent Hub and we will then go and delete this Resource Group and we'll go here and there you go hey this is Andrew Brown from exam Pro and in this section we'll be covering the Azure Cloud adoption framework Cloud adoption framework is a white paper that is a stepbystep process to help organizations plan and migrate their workloads to Azure the image outlines the Microsoft cloud adoption framework for Azure a systematic approach to transitioning to the Azure Cloud the process is categorized into stages defined strategy here the focus is on understanding motivations grasping the reasons for cloud adoption business outcomes identifying the desired results business justification validating the moves reasons first adoption project kickstarting the cloud Journey plan this stage includes rationalizing digital estate evaluating current digital assets initial organization alignment ensuring everyone is aligned with the migration goals skills Readiness Plan equipping teams with necessary Cloud skills Cloud adoption plan laying out a road map for the cloud transition ready this phase ensures preparedness Azure Readiness guide preparing the environment for Azure First Landing Zone setting up an initial secure Azure environment expanding the blueprint broadening the Azure setup as per requirements best practice validation ensuring adherence to azure best practices adopt the actionable phase where migrate existing workloads or moved to Azure this entails the first workload migration understanding expanded scenarios validating best practices and making process improvements innovate transform services in the Azure environment using Innovation guides exploring new scenarios validating best practices and furthering process improvements govern this is about oversight and management establish a methodology and Benchmark for governance Implement initial best practices standards for Azure use measure governance maturity how well governance rules are followed manage the deals with ongoing operations ensure business commitments are met during the transition set and assess the operations Baseline determine operations maturity gauge the efficiency of cloud operations now let's take a look at the security roles and responsibilities of the Azure Cloud adoption framework Business and Technology outcomes go and results expected from security functions security outcomes results an organization aims for including governance prevention and response role types security leadership provides security Direction and strategy security architect designs and implements security blueprints platform app security Engineers ensures security of platforms and applications security operations manages realtime security threats responsibilities security leadership sets security strategy security architecture design secure systems security compliance ensures adherence to regulations policy and standards set security policies posture management manages overall security stance phases of security implementation plan identifies security needs build Implement strategies including access control and asset protection run manages ongoing operations including prevention and response feedback loop continuous Improvement cycle and security operations in summary the framework offers a structured way to transition to Azure ensuring strategy alignment preparation adoption governance and effective management let's talk about the Azure well architected framework Azure well architected framework provides best practices for Designing and implementing Solutions on Azure it is structured around five key pillars cost optimization this pillar focuses on maximizing the value delivered by managing and controlling costs proper cost management can lead to significant savings without compromising function or performance operational excellence this emphasizes implementing and maintaining system processes to ensure smooth and efficient operations in a production environment it involves routine operations deployment practices monitoring and iterative Improvement performance efficiency refers to ensuring that systems can scale appropriately and adapt to varying loads both expect and unexpected it's not just about speed but ensuring resources are used efficiently to meet performance requirements reliability concerns the system's ability to recover from interruptions such as failures or outages and continue to operate without significant degradation this pillar stresses designing for high availability and disaster recovery security Central to the Azure well architected framework this pillar emphasizes the protection of data and applications from potential threats it covers a broad spectrum from access controls and encryption to threat detection and response strategies so that's an overview of the Azure well architected framework the next topic we'll be covering is azure migrate Azure migrate offers a streamlined service for migration modernization and optimization on Azure it simplifies the premigration processes like discovering assessing and appropriately sizing on premises resources for infrastructure data and applications with an extensible framework as migrate easily integrates with thirdparty tools broadening its range of supported scenarios here's what it offers unified migration platform a centralized portal to initiate execute and monitor your Azure migration Journey diverse tool set Azure migrate provides a suite of tools for both assessment and migration it features tools such as Azure migrate Discovery and assessment in migration and modernization furthermore it seamlessly integrates with other Azure Services tools and thirdparty offerings from independent software vendors comprehensive migration and modernization capabilities in the Azure migrate Hub you can assess migrate and modernize servers databases and web apps assess and migrate on premises servers web apps and SQL Server instances to Azure databases analyze on premises SQL Server instances and databases and migrate them to Azure SQL on a VM Azure SQL managed instance or Azure SQL database web applications evaluate on premises web applications and transition them to the azzure app service or Azure kubernets service virtual desktops review your onsite virtual desktop infrastructure and move it to Azure virtual desktop data transfer efficiently and affordably transfer vast data volumes to Azure using Azure data box products by using Azure migrate organizations can streamline and simplify their migration process reduce downtime and improve the overall efficiency and cost effectiveness of their Cloud migration the next thing we'll be covering are the integrated Tools in Azure migrate the Azure migrate Hub includes these tools Azure migrate Discovery and assessment discover and assess servers including SQL and web apps discover and assess on premises servers running on VMware hyperv and physical servers in preparation for migration to Azure migration and modernization migrate servers migrate VMware VMS hyperv VMS physical servers other virtualized servers and public cloud VMS to Azure data migration assistant assess SQL Server databases for migration to Azure SQL database Azure SQL managed instance or Azure vm's running SQL Server data migration assistant assesses SQL servers identifies potential migration problems unsupported features and suggest the best path for database migration Azure database migration service migrate on premises databases to Azure vm's running SQL Server Azure SQL database or SQL managed instances is azure database migration service is a managed service for seamless migrations to Azure data platforms with minimal downtime maneuver assess servers mover is a SAS platform that enhances business intelligence by accurately depicting it environments within a day web app migration assistant assess on premises web apps and migrate them to Azure Azure app service migration assistant is a standalone tool to assess on premises websites for migration to Azure app service Azure data box migrate offline data use Azure datab box products to move large amounts of offline data to Azure so that's an overview of the integrated Tools in Azure migrate the next type of migration solution we'll be going over is azure database migration service Azure database migration service is a fully managed service that enables seamless migrations from various database sources to Azure data platforms with minimal downtime it simplifies the process of moving databases to the cloud and reduces the risk associated with migration the service supports various Source database engines such as SQL Server MySQL Oracle and postgressql and targets Azure database platforms such as Azure SQL database Azure SQL managed instance and Azure database for posters ql and nql Azure database migration service currently offers two versions database migration service via Azure SQL migration extension for Azure data Studio Azure portal Powershell and Azure CLI database migration service via Azure portal Powershell and Azure CLI use cases migrations of on premises databases to Azure SQL Services database consolidation and migration to a single platform in Azure Cloud bursting and Disaster Recovery Solutions features support for a wide range of source and Target database platforms automated schema and data migration with minimal downtime migration assessment to identify potential issues and guidance for successful migration realtime migration monitoring and error notifications pre and post migration validation and cleanup tools integration with Azure services such as Azure security Center and Azure monitor for enhanced security and monitoring capabilities how it works set up the migration project create a new migration project in Azure DMS Define the source and Target environments and specify the database objects to migrate configure the source and Target configure the network connectivity and security settings for the source and Target environments and install the Azure DMS extension on the source database server start the migration start the migration and monitor the progress in the Azure DMS portal the service automatically replicates The Source database to the Target environment and tracks the changes that occur during the migration perform ceed over and post migration tasks after replication initiate the ceed over to transition the application to the Target environment Azure DMS offers validation and cleanup tools post migration the next migration tool we'll be covering is storage migration service the storage migration service simplifies the process of migrating storage to Windows server or Azure it comes with a userfriendly graphical tool that takes stock of data on Windows litex and net appy ifs servers and facilitates its transfer to newer servers or Azure virtual machines the service also enables the migration of a server's identity to the destination server ensuring that applications and users can access access their data without any changes to Links or paths why use storage migration service use storage migration service because you've got a server or multiple servers that you want to migrate to newer Hardware or virtual machines storage migration service is designed to help by doing the following tasks inventory multiple servers and their data rapidly transfer files file shares and security configuration from The Source servers optionally take over the identity of the source servers also known as cutting over so that users and apps don't have to change CH anything to access existing data manage one or multiple migrations from the windows admin center user interface migration is a threestep process wi inventory servers to gather info about their files and configuration shown in the following figure to transfer data from The Source servers to the destination servers three cut over to the new servers the destination servers assume the source servers former identities so that apps and users don't have to change anything although the source servers retain their files they enter a maintenance mode and become inaccessible at a suitable time these servers can be decommissioned so that's an overview of storage migration service the next migration solution we'll be covering is azure data box the Microsoft Azure data box Cloud solution lets you send terabytes of data into and out of azure in a quick inexpensive and reliable way each storage device has a maximum usable storage capacity of 80 terab and is transported to your data Center through a regional carrier it is designed to help customers with slow or limited internet connectivity to move large volumes of data to the cloud let's take a look at some of azure data Box's use cases data box is used to import data to Azure for onetime migrations moving large on premises data transitioning offline tapes relocating BMS SQL servers applications and transferring historical data for Azure based analysis initial bulk transfers large scale transfers using data box followed by incremental Network transfers for example moving vast backups with Partners like convolt periodic uploads transferring large volumes of data generated periodically like video content from oil rigs or windmill farms for exporting from Azure data boxes used for Disaster Recovery restoring Azure data on premises quickly security requirements meeting mandates that require data extraction from meure Storage tiers Like Us Secret migration moving data back to on premises or to a different cloud provider here's how Azure data box works customers order a data box from the Azure portal when the data box arrives customers connect it to their Network and configure it using the Azure portal customers copy data to the data box using standard file transfer protocols such as SMB or NFS once the data transfer is complete customers ship the data box back to Azure Azure copies the data from the data box to the customer's Azure storage account so that's an overview of azure data box it's cases and workflow let's talk about networking for Azure and so everything kind of revolves around the virtual Network also known as the vnet and this is a logically isolated section of your Azure Network where you launch your Azure resources and here's a very simple uh diagram of using vnet but there's a lot of networking components uh that you're going to be utilizing and we're not going to go through the exhaustive list here but let's just go through some of them just to give you an idea of like all the things you can do within Azure networking so you have Azure DNS this manages your uh your DNS domain then you have the vnet itself and so underneath that it'll have like address spaces route table subnets then you have network security groups this acts as a virtual firewall at the subnet or Nick level you have express route this helps you create a very fast connection between your onprem to your vnet then you have virtual Wan this is a centralized Network to rote different network connections then you have virtual Network Gateway this is a sight tosite vpm connection between vets and local networks then you have your NYX or your network interfaces and these are virtual network devices to allow VMS to communicate using IP protocols and then you have like all your load balancers and other things like that so again not an exhaustive list uh but just shows you that there's a lot you can do uh within networking on Azure so as your private links allow you to establish secure connections between Azure resources so traffic remains within your Azure Network and so I got this big old graphic here and so let's just uh Define a few things I'm going to pull out my pen here and so imagine you have workloads on your on Prem or you have your own virtual Network on Azure uh and so you have some VMS doing some stuff and the idea is that you want to connect them to some other services over here uh but the thing is is that if you wanted to connect them they might try to transverse the internet so if you're on Prem it's going out here and it's going over here and so the issue with this is that um it's not necessarily secure uh another issue could be um you know it's just faster if it would to stay within the actual Network because if you're already connected you you know your stuff over here then why wouldn't it just go through uh the actual Network another thing is like data transfer cost so it's always going out the internet and coming back in and that's not a great scenario uh so I'm just going to erase all that stuff there uh the idea with private link is that it's just keeping everything within the Azure Network um and so that's what it's doing so if you want to uh you know um connect your workloads in your vnet or your on Prem that connects to a specific vnet the idea is that you can launch a private link endpoint which is a network interface and this is what's establishing that connection and you're going to have to give that private endpoint a private IP address from your vnet and so then the idea is that on the right hand side that's what you want to connect to there's a lot of azure Services by default works with private link so there's no additional configuration you'll just be able to uh say use private link and they will uh same thing with some thirdparty providers um in the marketplace uh they might be powered by private link and then there's a third case where let's say you have like a private subnet and you have some VMS there uh so what you could do is um if you have a load Bouncer and you need to have an internal load balcer there you can uh launch the private link service and this is going to basically make your uh your workload in that vnet become compatible or powered by private link so there you go let's take a look at subnets and a subnet is a logical division of an address space so we just looked at address spaces so we're cutting it up one step further and subnets help you define different kinds of workloads and allows you to apply virtual is isolation within your network so when you launch an aure resource you choose the subnet you want to launch with within and an IP from that subnet is assigned to your resource so uh the thing is is that uh when you create route tables that's how you are associating uh the subnet so it can access the internet or access anything uh and then there's public and private subnets but this thing is interesting because like again if you if you're using AWS uh those are very clearly defined things but in Azure uh they don't have this concept of public and private you have to Define it yourself so really a public or private subnet is just a subnet that doesn't have access to the internet and so we saw that when we had the route table and we over over uh Road um the access to the internet to none that M essentially made it a private subnet uh another thing you can do with subnets is you can associate a network security group and this is going to help protect uh protect your traffic entering and leaving the subnet so you're making rules based on the IP address port and protocol uh and then there's a special thing called a Gateway subnet so uh it's a specialized uh type of subnet and it's just for Azure virtual Network gateways and so um it's interesting you'll go in and there'll be a separate section just for it and so you'll create it it's just for that service so uh virtual private networks also know as vpns what they do is they extend a private Network across a public network and enables your users to send and receive data across shared or public networks as if their Computing devices were directly connected to the private Network all right uh and so now we have to talk about what is a virtual Network Gateway and so virtual Network Gateway is the software a VPN device uh for your Azure virtual Network so that's how these uh devices are going to connect to your network so when you deploy a virtual Network Gateway it will deploy to or uh two or more specialized uh VMS in specific subnets you need to create a Gateway subnet and these deployed VMS contain routing tables and run specific Gateway Services you can uh choose the type of Gateway you want it to be and this is going to determine whether You' use something like VPN Gateway or express route Gateway so it's as simple as an option like that but to really understand uh these virtual Network gateways we should just look at some VP and Gateway design so you understand why you're creating these things so to understand VPN Gateway we need to understand the utility of the service and so we'll look at some different topologies and the first one is site to site this is when you connect Azure to on premise data center or vice versa and the idea is you create a VPN Gateway that's going to establish connection to your on premise environment creating IPC tunnel which is a secure connection IP uh connection there but you might be asking well isn't that what Express rout is for well Express rout goes through a part um a edge partner uh so there's a lot more work involved in that setup this one just goes over the internet so it's not going to be as fast um and there's some other uh downsides but the idea is it's the easiest way to get connected from your uh your on premise to uh Azure the next one is multi sight this is when you connect Azure to multiple on premise data centers um so it's just like the same model except there's more than one uh tunnel here to more than one uh uh site then you have point to site this is when you connect Azure to multiple individual computers so imagine you have a bunch of employees around the world and they have laptops and they just need to connect uh securely to the um to the the private vnet or just the vnet in general and so that's where we're going to need uh VPN Gateway and the last one is vnet to vnet this is where you're going to connect two vets in two different regions and subscriptions uh or have different deployment models uh and so uh that's again those are transversing the internet uh in all these cases here but yeah that's pretty much it and I guess I think that if you were to set up Express Ro I think you'd have to set up VPN gateways anyway uh but anyway that's what that is there so hopefully that now makes sense so Azure Express Roes creates private connections between Azure data centers and infrastructure on your premises or in collocation environments and so uh connections don't go over the public internet and as a result offer more reliability faster speeds consistent latencies and higher security so here's a big old graphic here kind of representing what's going on here and the idea is that you're going to have connectivity from different things like from any to any so that's ipvpn Network a pointtopoint ethernet Network a virtual cross connection and this is all going to be going through a connectivity provider at a co location facility so this example would be a uh an edge partner and so you would be the customer Network that' be your on premise or whatever you want to connect so that you can make it all the way to your Azure services and the way you would establish a connection is you create these express route circuits uh it's pretty common to have uh more than one because because uh you want to have high availability just in case the other one goes out uh the idea is that you can route them uh route this traffic to a couple different places so the first one would be things like peering to office uh 365 Dynamics 365 or even your Azure public services so when we say Public Services imagine you just launch a um a virtual machine that is in a public subnet that has a an IP that's reachable from the internet uh that could just be to that or maybe you're using like cosmod DB or like you have fully qualified domains for other services uh so that's just a way that you can get there and then let's talk about the other side which is uh um doing private peering for uh vets so the idea is that you have a private vnet so the subnets are private and so the only way is going to be access it uh you want a direct way to access it that way so uh that's the two ways there uh and just to note there is express route direct and so this is like express route with an additional benefit that has greater bandwidth connections from 50 megabytes per second up to 10 GB per second and this is really ideal if you have hybrid Solutions where you uh you're removing massive amounts of uh data or where latency matters because you you want uh to feel like these uh cloud services are right there with your on premise environment you're going to need a lot of speed for that aure firewall is a managed cloudbased network security service that protects Azure vnet's resources and it's a fully stateful firewall as a service so you're going to get builtin High availability and unrestricted cloud scalability and what you can do is uh is essentially create enforce and log application network connectivity policies across subscription and virtual networks so Azure firewall uses a static public IP address for your vnet resources allowing outside firewalls to identify originating from your virtual Network and the service is fully integrated with Azure monitor for logging and analytics so here is a representation of azure firewall and so the idea is that you're going to launch an Azure firewall into its own vnet and then other vets and your on premise or other things are going to pass through that Central vnet onto wherever they want to go and the idea is that uh we're going to be able to uh do things like utilize Microsoft threat intelligence this is going to block known malicious IPS in fqdns that stands for fully qualified domain names and by default the traffic is uh set to deny uh but you can set connectivity policies to filter out traffic in a variety of ways to make sure that you are protected uh there so yeah that's all there is to it so network security groups filter Network traffic to and from the Azure resource naet uh and so an NSG is composed of many security rules and so here's an example of setting an inbound security Rule and each security rule has the following properties so you can give it a unique name which is all the way at the bottom there uh you can set the source or destination so you're going to set an IP address cider block service tag application group things like that uh then you set the port range so it could just be Port 80 could be all ports where it shows an aster it could be a range of ports you're going to set the protocol so you got TCP UDP icmp you set the action whe whether it's allowed or denied and then you set the priority which is a number between 149 96 and we have two types of security rules we got inbound rules which applies to traffic entering the NSG and outbound rules that apply to traffic leaving the NSG let's take a look at some of the default security rules that NSG sets for you so when you create an NSG you're going to have some created for you by default uh and that's going to be for outbound rules and inbound rules so uh for inbound rules uh you're going to be able to uh accept any virtual networks into your um through the uh NS uh it's going to allow Azure load balancer to make its way in and it's going to deny uh everything else all right then for your outbound rules it's allowed to uh go to any other virtual Network it's allowed to make its way out to the internet and then everything else is denied so there you go let's take a look here at the logic for security rules and there's a lot of logic here so we'll work our way through it unfortunately there's no fun way to visualize this so we just got work through the textt so you may not create two security rules with the same prior priority and Direction you can have 5,000 nsgs per subscription and 1,000 nsgs rules per NSG for priority rules are processed in priority order with lowest number uh process before the higher number and network security groups uh rules are valuated By Priority using a five Tuple information to allow or deny traffic based on source source ports destination destination ports and protocol honestly I don't know what they mean by that uh and I couldn't make any sense it's in the documentation so I'm just showing it to you here then there's flow records They don't exactly explain what they are but I assume the idea is that uh when a request throws uh flows through the NSG they're attaching additional information to it uh so the flow record allows a network security group to be stateful a flow record is created for existing connections uh communication is a Lowder denied based on the connection state of the flow record and so let's talk about statefulness so if you if you specify an outbound Security Port you don't need to set the inbound Port since it will be set for you you only need to specify an inbound uh security rule if the communication is init initially externally uh initiated externally and the opposite is also true if inbound traffic is allowed over a port it's not necessary to specify an outbound security rule to respond to traffic over the port uh and the reason I know this when a when another Port is set with another one that's when it's stateful if it didn't do that then it would be considered stateless but it's state F uh let's talk about Interruption so existing connections may not be interrupted when you remove a security rule that uh enabled the flow and traffic flows are interrupted when connections are stopped and no traffic is flowing in either direction for at least a few minutes so there's the rules uh there a lot of stuff there honestly I won't even remember uh 90% of the stuff but I just wanted to go uh through it with you so there you go hey this is Andrew Brown from exam Pro and in this section we'll be covering Azure DDOS protection what is a DDOS attack a malicious attempt to disrupt normal traffic by flooding a website with large amounts of fake traffic DDOS attacks are big worries for people moving their apps to the cloud these attacks try to use up all of an app's resources so real users can access it any online Point can be a target for these attacks Azure DDOS protection offers Advanced features to counteract DDOS attacks when integrated with recommended application design practices this service is specifically designed for Azure resources within a virtual Network enabling this protection on both new and established virtual networks is simple and doesn't require any changes to apps or resources most frequent types of DDOS attack volumetric attacks these are volume driven attacks that deluse the network with seemingly legitimate traffic by doing so they exhaust available bandwidth leaving legitimate users unable to access the website these are typically measured in bits per second protocol attacks these tax exhaust server Resources by sending fake protocol requests that exploit vulnerabilities examples include UDP and TCP Flooding at layers 3 and four these are measured in packets per second application layer attacks these attacks Target the application layer examples include HTTP floods SQL injections cross site scripting parameter tampering and slis attacks to defend against these web application firewalls are often employed Azure offers two tiers of DDOS protection DDOS Network protection when combined with best practices in application design Azure DDOS Network protection offers Advanced DDOS mitigation tools it's automatically configured to safeguard specific Azure resources in a virtual Network DDOS IP protection this is a paper protected IP model while it shares core features with DDOS Network protection it provides additional services like rapid DDOS response support cost protection and discounts on Waf so that's an overview of azure DDOS protection so Azure has uh a few different kinds of load balancers and one in particular is the application Gateway and this is for application Level routing and load bouncing service so application Gateway operates at the OS I layer 7 uh which is also known as the application layer and the idea here is that when you're working about applications you're working with HTTP requests that's what it is uh and so the idea is that it can look at the contents of the HTP request and do some interesting things so maybe it's looking at the path and saying okay you're uh if you have a path um and it's payments go to the VM that has our payment system and then if it's for admin go to the VM that does that uh maybe it has to do something with cookies or um maybe you want to apply a Waf policy to to it and so you can look at those HB requests and determine if it is malicious traffic and filter it out uh so yeah it's all about the application which has to do with HP requests to configure this thing you need to set up a front ends uh ring rules and backend pools and there are two configurations for the front end you can either have it with a private IP which makes it now an internal load balcer or a public IP which makes it either we would call a public or external load balancer uh and there's a both I've never used that option before um but I mean you just really need to know the public and private for the backends you create backend pools and a backend pool is just a collection of resources to which your application Gateway sends traffic uh and so a backend pool can contain virtual machines virtual machine scale sets IP addresses domain names app Service uh and I think you can also send it to like on Prem uh so let's just talk about routing rules which connects the front end and the back end together and so uh here is a more complex example gives you a better idea of this Service uh uh at full uh and so the idea is that you have your application Gateway it's going to send uh traffic or sorry like a DNS is going to send to application Gateway and then you have these listeners and they they listen for incoming traffic uh and then what it will do it will pass on that to a rule and a rule just really says who should we pass the data to uh and then if you are defining a backend pool then you need to have um an HTTP setting that says how do we handle the HTP requests okay so now we'll just look at more granular detail about request or routing rules let's take a closer look at routing rules this is where all the magic happens so a listener listens on a specified Port an IP address for traffic that uses a specified protocol if the listener criteria are met the application Gateway will apply the routing Rule and these come in two flavors we got basic which forwards all requests for any domain to backend pools and multisite forward requests to different backend pools based on host header and host name so the thing is is that uh you uh you can have multiple listeners and you can have an order of them and the idea is that you really want the basic to be on the uh be in last in priority because if it's a first it's going to capture everything because that's its job it's like a catchall uh so just make sure that um if you're using multiple listeners you put basic last uh so that's just something you need to know then for those backend Targets this is where we either Define a back and pool or redirection redirection is just an HTP redirection like uh 403 or whatever you have their temporary or permanent very simple uh but for backend pools you have to create HTP settings uh and this tells us a little bit more about how we want to handle HTP you know cookies cook uh connection draining Port requests Etc let's talk about a little bit more so here are the actual options that we can configure for HTP settings so you have your backend Port uh so generally it's either Port 80 or 443 it just depends on where you're doing your SSL termination uh but generally um if you need end end encryption it'll be 443 or uh and then if you are doing connect or if you're doing SSL termination at the blow balancer then it's 80 um then you have cookie base Affinity this allows you to keep a user session on the same server so if you need to persist cookies and use them for authentication you'll want to enable that you got connection draining this gracefully uh removes backend pool members during plan service updates so you is that you know when there is an update it's not just going to abruptly cut over it's going to wait until a connection disconnects from a server before uh not no longer sending more requests there because you don't want a a connection dropping in the middle of an update it's not great for a user then you have request timeouts this is the number of seconds the application Gateway will will wait to receive a response from the backend pool before it returns a connection timeout error message and last our um oh sorry we still have override back in path these allow you to override the path in the URL so the request for spe a specific path can be red to another path the idea is imagine you want to send it to um but bananas and you want bananas actually internally route to oranges I don't know why you want to do that or maybe to plantains that' make a lot more sense so that's something you can do uh and the last one is the override the host name so application Gateway normally leaves the host name alone uh but you know if you're using multitenant services like apps uh like app service or API management it needs very specific host header set so this is where you can override them and change them for those services so hopefully that really gives you a good picture of application Gateway but I feel that that's all you need to know so let's take a look here at Azure scale sets and these are used to automatically increase or decrease your virtual machine capacity so imagine uh you have a web application behind an application load balancer and you have an increase in traffic you want to be able to quickly add more capacity by adding another identical virtual machine uh or when uh there's a decrease in traffic remove it to save cost and that is what Azure skill sets do um so you're creating uh scale policies to automatically add remove based on host metrics host metrics could be like CPU utilization or uh Network in uh you're going to create health checks and you can also uh set a repair policy to replace unhealthy instances you can associate a load balancer to distribute virtual machines across uh availability zones and you can scale to 100 or even a th000 VMS using scale sets so one thing you're going to probably want to do with your scale set is associate with a load Bouncer and in honest I don't run any kind of application workload uh unless it is in a scale set and behind a load balancer because it's just good practice to do that and the reason why you're going to want a load balancer is that it's going to help you evenly distribute your VMS across multiple availability zones and that's going to give you high availability uh because you definitely want to try to run uh 3 VMS across three azs to get that a high Veil ability uh you can also use a load balancer probe checks and these are more robust health checks than what skill set provides you uh so that's just an added benefit there uh and when we're talking about um load balancers we have two different choices here we have the application Gateway and so this is for HTTP htps web traffic load balancer uh and then you also have Azure load balancer and this is going to support TCP and UDP Network traffic and things like that so depending on what oi layer you need to operate on is going to determine what kind of load balcer you need to use so let's take a look at scaling policies and these determine when a virtual machine should be added or removed to meet the current requirements and you have this uh Little Wizard here this is what the What wizard you see or form you see when you're creating a scale set initially with very simple features but let's just focus on two things scaling out and scaling in so scaling out is when an instance should be added to a skill set to increase capacity so you choose your metric uh which is hardcode in this case to CPU threshold so you choose your metric the duration how many VMS you want to add and then the scale in is the opposite that's when you're removing uh uh uh something from the scale set to decrease your capacity and again that interface is very simple uh when you actually go ahead and create a scale set after you go and update it you're going to end up with a lot more options and here are the options you can see they're extremely robust and so uh you have the option to choose one of the builtin host metrics that are already be being collected on your virtual machine so uh the most popular here and there's more than this but uh the most popular here is CPU Network in network out dis read and write uh then you have your Aggregates so you can decide how you want to group or uh collect that data uh before you apply your final operations on it uh then you have your operator so you can say greater than or greater than or equal to then you have your actions you can say increase the count of so add uh x amount of of servers or you can say increase by percentage so imagine you had um uh 10 servers and you want to increase the load by 30% that would add three additional servers if you want more metrics than just the builtin host metrics you can absolutely get more uh but you're going to have to install a couple things uh for app uh if you want more app specific metrics like page performance or page load performance and sessions count you would uh install a small instrumentation package for app insights and uh if you want to have more detailed host metric so you might have host metrics but you want them in more detail or more available host metrics you'd install the Azure diagnostic extension within your VM um there are a few other uh scaling policy options that are well they're not exactly in that form but they are around or are associated with them one is the scale in policy not to be confused with the scale in option um and this determines what virtual machine should be removed when you decrease capacity so it's just a simple drop down and you have a few different options so there's the default option this is where it deletes the VM with the highest instance ID and it's going to do that b uh uh take in consideration uh the balancing across A's and ads or it can delete the newest virtual machine so delete the newest VM again uh this one's only across uh virt or availability zones or delete the oldest VM this is going to delete the oldest VM and balance across availability zones then you have an update policy and this determines how uh how VM instances are brought up to date with the latest scale that model so again it's another drop down and you have a a few options here so you have automatic so increasing the start upgrade immediately in random over uh order or or manual so existent or existing instances must be manually upgraded or rolling and so this update upgrades roll out uh in batches with optional pause uh and just one other note here is that if you want to do automatic OS upgrades you can enable uh this to help ease update management by safely and automatically upgrading the OS dis for all instances so there you go Health monitoring is a feature with scill sets you're absolutely going to want to turn on uh and what it does is it determines whether your instance your virtual machine instance is healthy or unhealthy so you have that option to disable or enable it and it comes in two different modes we're going to have the application Health extension which is what we're seeing on the right hand side here and this is where you ping an HTTP or htps request with a specific path and expect a back a specific status so the status here would be 200 so the idea is that you could say ping the homepage and if the homepage appears then therefore the server must be healthy or you can make it your own custom uh page that's what I like to do I have like a health check page the other mode is load balancer probe uh this is only going to work if you have an Associated load balancer and uh here you can check based on TCP UDP HTTP request so this is a little bit more robust um um so I generally would recommend using this mode over um the the first mode because generally when you have a scale set you're going to also want to have a load balancer uh now if you want to replace unhealthy instances they have an automatic repair policy so this is an automatic I mean like it's not turned on by default so you have to uh explicitly say you want to turn it on and what that will do is that if it finds an instance that is unhealthy it's just going to uh terminate it and then launch a new instance all right and so there you go all right so now that we have our scale set we're going to continue on learning more about availability by creating an application Gateway so this is actually a type of load balancer even though it has Gateway in the name then there's Azure load balancers and the difference between these two is this one's layer seven for the applications and this is layer four uh so TCP UDP like at a lower layer so uh you know since we have a web application which is a simple apoi page we're going to want to serve it up via the application Gateway all right so make your way over to the top here and we'll click on application Gateway or just type that in and we'll go ahead and add ourselves a new application Gateway we'll choose wolf here I'm going to name this wolf um uh application agw and then we do not yeah we don't need any Autos scaling right now so we'll just leave that alone I'm going to set that to one we're going to choose two one and three because the rule of three counts here we always want three and then down below make sure you choose the correct virtual network if you're not conf make sure you go over to your scale set and just double check to make sure where it resides so if I go over here into my scale set and check it's in wolfnet uh wolf vet 499 how I ended up with more than one I don't know I just the tutorial I've done this a few revisions so I've ended up with an additional one there notice it's complaining because it's trying to uh put this in the default but it needs its own uh subnet so here under subnets we're going to create one just for the vgw so vgw and we're going to need a range first so go over to address uh space ranges and we'll pick out the next number so 10.0 uh. 2.02 24 we'll hit save and once that has created we'll make our way over to the subnet and when we hit subnet it will automatically select the next one for us I'm going to put vgw in here we'll hit save and then once that's saved we'll make our way back to the wizard notice up here the bread comes if we click one back we can make our way uh back to where we are and then choose that vgw subnet we'll go to the front to the front end uh section here we got public private or both we want public today and we will need a new IP address so I'm going to call this uh wolf vgw we'll go ahead and hit okay we'll go to backends here add a backend pool I'm just going to call this backend and we'll drop down and choose a virtual machine scale set if you do not see it here it's because they're not in the same vnet make sure that's the case and then we'll go to configuration so we got a front the back end now we got to glue them together with routing rules we'll just say my rules we have listeners and backend targets so we'll name our listener name my listener if you can spell it that's a hard one for me so I'm just going to copy paste that in take out the space there uh the front end IP is it's going to be public we're going to have HTTP on Port 80 we'll have basic if we had multi sight so if you have more than one destination but we do not we'll go to backend targets and we will choose the backend pool that we created and then we have to choose or add um HTTP settings this is all kind of crammed together it doesn't really line up very well so I get a bit confused here so I think what we want to do is hit add new here and then we'll say my HTTP settings uh Port 80 is good we do not have cookies we do not care about connection draining all these options are not important to us uh for our very simple application um but if you were creating a real web application that uh that has a session you probably would want to turn these both on okay we'll go ahead and hit add and we do have pathbased routing but this is not something we need to worry about right now this is where you could say you could have multiple targets so you could say Okay anytime it is uh cookie then go to this virtual machine and then if it's another one you could say go somewhere else or even do a redirect that's a great way of attaching things to your um uh virtual machine there we'll go ahead and hit add and so now we have all of our information we'll hit tags and we will go hit create and I'll see you back here uh when this is done deploying okay all right so I've been waiting a while here for uh this to complete but uh it looks like that we're still having a bit of trouble uh so what I'm going to do is make my way over to the actual uh load balancer here because I just remembered and this happens because I don't normally make a scale set separately I usually make it with the load balcer but if we make our way over to the backend pool and we're to click into here it actually has a a thing that says down here virtual machine skill set was added to this backend pool upgrade all in es of wolf scale set for this change to work so uh those the scale set needs to get updated is the is the problem here so what I'm going to do is make my way over to the scale set and we'll see if we can do an upgrade here so if we just checkbox them all on there I'm going to go hit upgrade and then this will perform an upgrade on the instances and we'll just give that some time and I'll see you back here in a moment okay so that didn't take too long for the upgrade and if you just hit the refresh here you're going to wait for the status was running and now they're all healthy so if we make our way back to um our load balancer tab here if you still have something open just go to backend pools you should see three targets if we go up to our overview over here we'll go grab whoops our IP address now why the uh they copy the text there I have no idea but if we go paste that up into here and take that out we get our page so there you go that's all it took to get the availability um or the actual server Runing if we go on the left hand side and we go over to health probes this is interesting to look at we're not going to set it up but I just want to show you that you'd write in your uh health check here and then you could specify um a different path so if you had like a page called and this is pretty common for a lot of places they'll have a page called health check and so that's a just kind of like a way to configure a custom health check uh for your instances but this is all I really wanted to accomplish here today um I don't think there's really much of anything else that I'd like to do here um so what we'll do is we'll go and make make our way back to all of our resources and we'll try to find our original Resource Group here if we can uh find it's probably easier if we just go to Resource groups great and I'll just go ahead and delete this one here there we go it's going to delete a whole lot of stuff uh but yeah there we go
