With timestamps:

00:06 - all right i think you can get started
00:08 - now
00:09 - please take a seat
00:12 - so uh good morning everybody my name is
00:14 - julian i'm a tech evangelist with aws
00:18 - i've been working with them for almost
00:20 - three years now
00:21 - and these days
00:23 - i focus on ai and machine learning and
00:25 - it's really nice to be back in krakow i
00:27 - was here for a code europe in december
00:31 - it was cold and dark
00:33 - very cold and very dark it's very nice
00:35 - to enjoy the city with sunlight and
00:38 - getting a chance to visit it's really
00:39 - beautiful
00:41 - so good to be back
00:43 - so my topic today is deep learning for
00:46 - developers
00:47 - and well
00:49 - the title is really important because it
00:51 - means
00:52 - if you can read and write
00:54 - let's say 50 lines of python you can do
00:57 - this okay so we're gonna start with an
01:00 - introduction to the
01:01 - underlying concept some of the theory
01:04 - that actually holds deep learning
01:06 - together but i'm gonna try to make it as
01:08 - pragmatic and and concrete as possible
01:11 - with minimal theory and then we'll look
01:13 - at some code run some examples and try
01:15 - to put everything together okay so
01:18 - you've heard of
01:19 - deep learning as a subset of machine
01:22 - learning that uses a technology called
01:24 - neural networks right
01:26 - so the first thing we should uh we
01:28 - should look at and i realized this is
01:30 - really hard to start with this slide
01:33 - but hey we got to understand what this
01:35 - thing is all about right so stay with me
01:37 - here
01:39 - we got to understand what a neuron is
01:41 - and what we're trying to do really is
01:43 - trying to build uh some kind of
01:46 - approximation of a biological neuron so
01:48 - what we know about biological neurons is
01:51 - they have inputs they have connections
01:53 - and if those inputs are stimulated
01:55 - enough then the neuron will actually
01:57 - fire it will emit an electrical current
02:00 - if it's not stimulated enough it won't
02:02 - fire okay so we're trying to build
02:04 - something that looks a bit like that
02:06 - so obviously for a deep learning neuron
02:09 - we will have inputs okay and these will
02:12 - be floating point values
02:14 - um
02:15 - we'll see where those come from
02:16 - and to each of these inputs is
02:19 - associated a weight which is another
02:22 - floating point value and this weight
02:25 - in a way
02:27 - makes this input more or less important
02:30 - than others okay
02:32 - and the first thing that we do when we
02:33 - actually uh
02:35 - run
02:36 - something on that neuron is to compute
02:38 - this multiply and accumulate operations
02:40 - so we take each input we multiply it by
02:43 - the corresponding weight and we add
02:45 - everything together okay it's called
02:47 - multiply and accumulate
02:49 - so there you go right
02:51 - deep learning is really about adding and
02:53 - multiplying stuff and doing this at very
02:56 - large scale so if you can add and
02:57 - multiply you already know 50 of what
03:01 - deep learning really is okay
03:03 - but the problem with this function is is
03:05 - that it's a linear function if we change
03:07 - the inputs right if the inputs vary
03:10 - linearly then the result that u value
03:12 - also varies linearly and that's not what
03:15 - we want because remember we want
03:16 - something that fires or not okay so
03:19 - there needs to be some kind of limit
03:21 - some kind of threshold
03:22 - that says when does the neuron fire or
03:25 - not so this is why we add
03:27 - the activation function
03:29 - at the output of each neuron and over
03:32 - time a number of functions have been
03:33 - used these days a relu is the popular
03:36 - one
03:37 - and as you can see uh here it's it's
03:40 - non-linear right if if the input value
03:42 - to relu okay so if that u value here is
03:45 - negative
03:46 - then the neuron will output zero
03:49 - and if the input to the activation
03:52 - function is positive
03:54 - then the neuron will output that same
03:56 - value and it could be a very very large
03:58 - value okay so this is really what we
04:00 - want right
04:02 - nothing happens until a certain point
04:05 - and then
04:06 - we output a value and this could be a
04:08 - really large value okay so
04:10 - if the neuron is stimulated enough it
04:14 - can actually output very large values
04:16 - and this is what we want and this is how
04:17 - we introduce that non-linear behavior in
04:19 - the neuron okay
04:22 - so obviously a neuron by itself doesn't
04:23 - do much they become interesting when we
04:25 - combine them into layers and networks
04:27 - and this is probably the simplest one
04:29 - you could build
04:30 - okay so we have an input layer where we
04:33 - will actually put our data data samples
04:35 - to predict
04:36 - we have an output layer where we will
04:38 - read results right we will read the
04:40 - predictions and in the middle we have a
04:42 - hidden layer
04:45 - at least one so in this case just one
04:48 - and as you can see it's fully connected
04:50 - right so all neurons are
04:53 - fully connected to the inputs fully
04:54 - connected to the outputs okay that's why
04:56 - they're called fully connected networks
04:58 - okay
04:59 - so
05:00 - let's say we're already trained listing
05:03 - okay we'll we'll talk about training in
05:04 - a few minutes but let's say it's already
05:06 - trained how would we use it okay so of
05:09 - course we need data samples and for this
05:12 - discussion let's say we're trying to
05:13 - classify pictures okay so we have a
05:16 - number of pictures let's say animals
05:18 - right dogs cats elephants tigers
05:20 - anything that you want
05:21 - and we know what they are okay so
05:24 - we're going to take each image we're
05:26 - going to flatten it into a vector and
05:28 - we're going to store the pixel values in
05:31 - a matrix like that okay so this is the
05:33 - first picture this is the second picture
05:35 - and so on and so on okay so each picture
05:38 - has has been transformed into a vector
05:40 - of pixel values okay
05:43 - and we know what those pictures are okay
05:46 - so uh
05:48 - um
05:49 - we know for example that the first
05:50 - picture here is a category two and maybe
05:54 - category two are dogs right and we know
05:56 - the second picture
05:58 - is category zero
06:00 - maybe that's cats and so on and so on
06:02 - okay and let's say we have ten
06:04 - categories in total so that's the data
06:06 - set okay a number of pictures that we
06:08 - flatten into vectors and that we label
06:11 - with category numbers from zero to nine
06:14 - let's say okay
06:16 - actually we're not
06:17 - we don't like using this if you already
06:19 - work with machine learning you know we
06:21 - don't like
06:22 - categorical values
06:24 - to be handled like this
06:26 - we use another technique called one hot
06:28 - encoding
06:29 - okay and that's a complicated word for a
06:30 - simple thing
06:32 - one hot encoding means instead of using
06:35 - category numbers you're going to build a
06:37 - vector of bits okay and again let's say
06:40 - we have 10 categories here
06:42 - so we will have 10 bits and we'll flip
06:44 - to one the bit that corresponds to the
06:46 - right category okay so if this one is
06:48 - category two then bit zero one two is
06:52 - flipped okay this one is category zero
06:55 - so we flip bit zero and so on okay
06:58 - and why is this important because
07:00 - well this form right is actually much
07:03 - more expressive than this because here
07:06 - we can see how many classes we have
07:08 - first
07:09 - and second we could look at those
07:11 - numbers as probabilities for each of the
07:14 - classes
07:14 - okay and what this really says here is
07:17 - uh this sample
07:20 - has a hundred percent chance of being
07:22 - category two and zero percent chance of
07:25 - being any other category right we know
07:27 - for sure because we know what the data
07:28 - set looks like
07:29 - okay
07:31 - same thing here we know that this last
07:33 - sample here is category four so it has
07:36 - 100 chance of being category four and
07:38 - zero percent chance of being anything
07:40 - else
07:41 - okay and this is this is actually what
07:44 - we will get on the output layer when we
07:46 - when we put those samples on the input
07:48 - layer here right and we run
07:52 - those multiply and accumulate and those
07:54 - activation functions etc
07:56 - what we will read here
07:58 - in those neurons are the probabilities
08:01 - for the samples okay so hopefully if we
08:04 - train our network right
08:06 - okay we would get to something like this
08:08 - we take a data sample
08:10 - and as you guessed we need add many we
08:12 - need as many neurons in the input layer
08:14 - as we have features okay so if we have
08:17 - a thousand pixels here we'll need a
08:18 - thousand neurons here okay
08:20 - then we run what is called forward
08:22 - propagation so running that sample
08:24 - through the model multiply accumulate
08:27 - activate etc
08:28 - and you read some results on the output
08:31 - layer and again
08:32 - if you have 10 classes you will need 10
08:35 - neurons on the output layer okay and you
08:38 - will read the probabilities for each
08:39 - class so in a perfect world this is what
08:41 - we would see right we would really see
08:43 - zeros every everywhere except a one
08:46 - in the new round that corresponds to the
08:48 - right class to the right category
08:51 - but nothing is perfect and it's not
08:53 - going to happen like that
08:55 - anyway
08:57 - what we care about is accuracy so being
08:59 - able to successfully predict
09:01 - the maximum number of samples okay this
09:04 - is what we're going to measure and track
09:07 - okay so this is where we want to get
09:10 - but of course initially
09:12 - the network is not trained okay so all
09:14 - the weights corresponding to all those
09:17 - connections here
09:18 - right
09:19 - all those parameters
09:21 - they have random values so
09:23 - if you take a data sample and forward
09:26 - propagate you get something completely
09:28 - wrong on the output layer and it would
09:30 - it would not be something like this
09:31 - actually it would not be a one in the
09:33 - wrong place and zero zeros it would just
09:35 - be random probabilities for the ten
09:37 - classes okay because again the network
09:39 - has not been trained
09:41 - so
09:42 - if we see this neural network as a as a
09:44 - function and we take a sample and
09:46 - compute the output we're not going to
09:48 - get the right
09:50 - output we're not going to get the right
09:52 - label
09:53 - the right probability vector we're going
09:55 - to get something different okay let's
09:57 - call it y prime one
09:59 - so obviously you need to measure the
10:01 - difference between what you expected and
10:04 - what you really got okay and we do this
10:06 - using a math function called a loss
10:08 - function and remember that y one and y
10:11 - prime one are really vectors okay
10:13 - probability vectors so it's not as easy
10:16 - as uh subtracting one from the other but
10:19 - you don't have to worry about those
10:20 - because all the deep running libraries
10:22 - provide those loss functions already so
10:23 - you can just use one that matches you
10:25 - the problem you're trying to solve okay
10:27 - so this loss function will give give us
10:30 - the error okay just a numerical value
10:32 - measuring the error the distance so to
10:34 - speak between the two vectors
10:37 - so
10:39 - actually we're not going to do this
10:40 - sample by sample okay imagine you have
10:42 - 10 million samples in the data set
10:45 - it takes too much time to process every
10:47 - single one and and train on every single
10:49 - one so we actually train on batches of
10:51 - samples okay so we're going to take 32
10:54 - samples for 64 samples at a time
10:57 - run them one by one through the model
10:59 - and compute the total error for the
11:01 - batch okay and then we're going to make
11:03 - decisions on how to reduce the error
11:06 - okay more on this in a minute okay so
11:09 - keep in keep in your mind we train on
11:11 - batches of samples not individual
11:13 - samples
11:14 - okay
11:16 - so in a nutshell
11:18 - the purpose of the training process
11:20 - is really only one thing
11:22 - it's to
11:24 - minimize loss okay minimize prediction
11:27 - error for the data set
11:30 - by training over and over right
11:32 - iteratively
11:34 - and by by adjusting the weights
11:38 - gradually during the training process
11:40 - okay so what we're really trying to do
11:42 - here is
11:43 - um in these examples we i don't know how
11:46 - many weights we have maybe let's say 25
11:48 - or something like that okay
11:50 - we're trying to find the set of weights
11:52 - the set of parameters
11:54 - that give us the highest accuracy for
11:57 - that data set and that means the lowest
11:59 - possible error for that data set okay
12:02 - and we start from random weights and we
12:04 - have to try to go
12:07 - as close as possible to that optimal set
12:09 - of weights
12:10 - okay and that's a difficult problem
12:12 - because you cannot compute them right
12:14 - it's not something it's not an equation
12:16 - you can solve so you have to gradually
12:19 - discover what those weights should be
12:21 - okay
12:22 - more on this in a second so the training
12:24 - process really looks like this we have a
12:26 - data set like i said we slice it in
12:29 - batches and again we'll see in the code
12:31 - that deep learning libraries do this
12:32 - automatically so no no work needed and
12:35 - we take one batch
12:37 - okay let's say it's 32 samples and we
12:40 - run each one of those samples into the
12:42 - network okay
12:44 - and we get to prediction
12:46 - and we compare the prediction to the
12:49 - actual result we were expecting
12:51 - and we compute loss okay and we do this
12:53 - for about all the batch samples and we
12:56 - add up
12:57 - all the errors into the batch error okay
12:59 - so we now we have a
13:01 - batch arrow that tells us
13:03 - you know what's the what's the mistake
13:05 - how big is the mistake
13:07 - we made for this batch okay and now we
13:09 - can take decisions on how to reduce it
13:12 - and this is done with an algorithm
13:14 - called back propagation and it's a it's
13:17 - a scary one it usually
13:19 - that's usually where when people stop
13:21 - looking
13:22 - and uh and stop studying deep learning
13:24 - because they want to understand back
13:25 - propagation in detail and it's a bit
13:27 - scary okay so back propagation
13:30 - as the name implies
13:33 - will go back from the output layer to
13:35 - the front
13:36 - okay
13:37 - and
13:38 - layer by layer and neuron by neuron it's
13:42 - going to adjust the weights
13:44 - in a direction that we know reduces
13:46 - error
13:47 - okay so for example here
13:49 - okay this neuron the the error value
13:52 - that you get for this neuron
13:55 - depends on three parameters okay so for
13:58 - these three parameters you have to
14:00 - figure out if each of them should be
14:03 - increased or decreased
14:05 - to
14:05 - to lower error right or remember these
14:07 - are floating point values so these are
14:09 - the only two choices that you have right
14:11 - you can increase them a bit or decrease
14:13 - them a bit
14:14 - so how do you know okay i will answer
14:17 - that in a second
14:18 - so once you've done that here okay
14:20 - you've updated those you've updated
14:22 - those weights so you could compute the
14:24 - new error for the next layer
14:27 - and then you do it again so this the
14:29 - error that you get here for this
14:30 - individual neuron
14:32 - depends on one two three four five
14:34 - parameters okay so once again here you
14:37 - need to make five individual decisions
14:40 - on increasing or decreasing those
14:42 - weights and you do this for every single
14:44 - neuron and then you move back to the
14:46 - previous layer until you get to the
14:48 - input okay that's why it's called back
14:50 - propagation
14:51 - okay
14:53 - so now obviously uh
14:56 - you want to do this all over again for
14:58 - the next batch right
15:00 - compute the batch error back propagate
15:03 - and you do this again and again and
15:04 - again and again okay until you get to
15:06 - the end of the data set
15:08 - okay and this is called an epoch
15:10 - and typically you're going to train for
15:13 - 50 100 200 epochs okay so it's really an
15:16 - iterative process and you train and you
15:19 - predict again and again and again and
15:22 - again right computing the batch error
15:25 - and then adjusting the weights
15:27 - in a direction that you know will reduce
15:30 - error and and you do this until you
15:32 - hopefully get to the right level of
15:34 - accuracy that you were expecting okay so
15:36 - that's the big picture okay
15:38 - and at the end of that process
15:42 - you get to a trained neural network
15:44 - okay
15:45 - and as you can see um we have a number
15:47 - of parameters that are really important
15:49 - the batch size is important if you have
15:51 - very tiny batches
15:52 - you know you back propagate a lot you
15:54 - take takes a lot of time
15:56 - so yes you will probably get to the
15:59 - right
15:59 - spot but it takes too long
16:02 - if you have a very large batch size
16:05 - then
16:06 - you get less opportunities to run back
16:09 - propagation per epoch so maybe that's a
16:12 - problem too
16:13 - the learning rate will actually decide
16:15 - on the
16:16 - the the size of the updates that you
16:19 - make to the weight
16:20 - okay we'll we'll talk about that in a
16:21 - minute so it's important as well small
16:24 - learning rate small updates to the
16:25 - weight large learning rate
16:27 - large update to weights
16:29 - okay and again too small will be a
16:31 - problem too large would be a problem
16:33 - and the number of epochs is how many
16:35 - times you go through the data set
16:37 - these are called hyper parameters and if
16:39 - you don't get them right
16:41 - you will have lots of problems training
16:43 - okay
16:44 - so that's the big picture okay it's not
16:47 - really complicated the only weird thing
16:50 - is
16:52 - hey i get the batch size the batch thing
16:55 - and i get the back propagation but then
16:57 - you said we need to adjust the weight
16:59 - the weights each weight actually in the
17:02 - direction that we know reduces error how
17:04 - do we know
17:05 - right how do we know if a given weight
17:07 - should be increased or decreased
17:10 - okay let's look at a real example so
17:13 - imagine you have a function here
17:16 - let's call it f and it has two
17:17 - parameters
17:19 - okay
17:20 - and
17:21 - the output is let's call it z okay and
17:24 - let's let's say that x and y are
17:27 - parameters and z is the error okay
17:30 - and which we start with random values of
17:32 - x and y and we want to get we want to
17:34 - figure out
17:36 - what x and y should be to get the
17:38 - smallest value of z
17:40 - okay that's really what we're trying to
17:42 - do find the set of parameters that give
17:44 - us the smallest possible output
17:47 - okay
17:48 - and if we plot that function let's say
17:49 - it looks like it looks like this okay
17:51 - and remember x and y are initially
17:53 - random so we're going to start anywhere
17:56 - here
17:57 - okay so let's say we start here okay
18:01 - and let's say this is x and this is y
18:04 - all right
18:05 - so
18:06 - we want to get to the smallest possible
18:09 - value of z so probably that's here right
18:12 - so in a way we want to walk down that
18:14 - slope
18:15 - until the lowest point in the valley
18:17 - right we're trying to get down to the
18:19 - valley
18:20 - and remember we cannot compute x and y
18:23 - okay we have to discover them
18:26 - so how would you do that right if you
18:27 - were in the mountain how would you do
18:29 - that imagine you're in the fog you see
18:31 - nothing
18:33 - um and you have to decide if you uh you
18:35 - know if you should go forward or
18:36 - backward and if you should go left or
18:38 - right okay so what you would do is maybe
18:40 - you know you would with your foot
18:42 - you would try in this direction and say
18:44 - ah yeah okay this is down all right fine
18:47 - and then you would go right and say oh
18:48 - no this is up
18:50 - oh this is down okay so this is down
18:52 - this is down
18:53 - so i'm going to take a small step here
18:56 - and i'm a little lower than i was before
18:58 - okay and you do this again and again and
19:01 - again and again
19:03 - right and if there's not a big
19:05 - crack
19:07 - if it's a smooth surface like this if
19:09 - it's friendly you will actually get to
19:11 - the lowest point iteratively by taking
19:14 - small steps in the right direction
19:16 - okay that's the intuition
19:18 - now obviously
19:21 - deep learning doesn't have intuition
19:22 - it's it's math so how do you know for
19:24 - sure that you need to maybe increase x a
19:27 - bit okay or actually here it would be if
19:30 - the origin is here okay x and y
19:33 - actually we have to decrease x and y a
19:36 - little bit to go down right initially
19:39 - so how would we know
19:40 - well remember high school that's the
19:42 - part you're gonna hate by the way so get
19:44 - ready to hate it uh remember high school
19:47 - right
19:48 - i remember slopes
19:50 - and remember derivatives
19:53 - right no all right
19:55 - are you too kind right
19:57 - well that's exactly what we do here
19:59 - okay
20:01 - this is a function okay yes it has two
20:04 - parameters but we can compute the
20:06 - partial derivative for x and we can
20:08 - compute the partial derivative for y at
20:10 - this specific point okay
20:13 - and so that gives me the slope in the x
20:15 - direction and that gives me the slope in
20:17 - the y direction and then if i have the
20:20 - slope i know which way is up and which
20:22 - way is down
20:23 - okay
20:24 - simple
20:26 - so once i know that
20:28 - well i should decrease x a bit to go
20:31 - down in the x direction and i should
20:33 - decrease y a bit in the y direction then
20:36 - fine i'm just going to do that i'm just
20:38 - going to going to modify x
20:41 - and y just a bit in the right direction
20:43 - and i get let me say here
20:45 - okay and i do it again and again and
20:48 - again and again
20:50 - okay and this algorithm is called
20:52 - stochastic gradient descent or sgd it's
20:54 - the granddaddy of
20:55 - optimizing functions uh it it dates back
20:58 - to 1951 okay so it's nothing new it has
21:02 - actually nothing to do with machine
21:03 - learning it's a it's a math optimization
21:06 - function
21:07 - and that's the one that is still heavily
21:09 - used in deep learning
21:11 - okay so coming back to my example here
21:14 - this is what actually
21:15 - [Music]
21:17 - will happen during the training process
21:19 - when you run back propagation
21:21 - okay
21:22 - the library that you're using
21:24 - will will compute the derivative for
21:27 - each of those three weights
21:29 - okay
21:30 - with respect to the error and then
21:32 - decided they should be increased or
21:34 - decreased
21:35 - do that and then move on to the previous
21:37 - layer and do it again and again and
21:38 - again okay and this is how you know in
21:41 - which direction all those weights should
21:42 - be updated and since we're taking small
21:45 - steps we need to do it again and again
21:47 - and again right and this is why we do
21:49 - batch training and this is why we need
21:50 - many epochs because we take very tiny
21:53 - steps but we if we take tiny steps
21:56 - always in the right direction right
21:57 - eventually we get to the right place
22:00 - okay
22:01 - that's all there is to it right
22:03 - high school math
22:05 - nothing to worry about
22:07 - um
22:09 - so obviously surf the surface is not
22:11 - going to look like this right
22:13 - would be too easy
22:15 - it could look like this right or you
22:17 - could be seeing things like this
22:19 - where yes you seem to have a nice global
22:22 - or you know a lower minimum here and you
22:25 - have you know
22:27 - not as good ones here
22:29 - so maybe you could start you know on the
22:31 - mountain here and and walk down and
22:33 - maybe you could fall in this hole here
22:35 - or this one
22:36 - right and let's say this one gives you a
22:38 - higher error than this one does
22:41 - so it's probably more desirable to be
22:43 - here than to be here right
22:46 - and they're called local minima
22:49 - and another problem could be saddle
22:51 - points and this one is actually even
22:53 - worse so it's look it's like a horse
22:55 - saddle
22:56 - and uh
22:58 - in this direction right
22:59 - we can see
23:01 - yes this point here the green point here
23:03 - is actually the minimum so fine
23:06 - but in the other direction right like
23:08 - this
23:10 - this point here is actually a maximum
23:12 - right and if you remember high school
23:14 - again you know that the derivative is is
23:16 - equal to zero
23:18 - when a point is
23:20 - a minimum or a maximum
23:23 - okay so if we actually started here and
23:25 - went down exactly to that green point
23:28 - uh we'd be in trouble because
23:29 - derivatives would be zero we could not
23:31 - update the weights
23:33 - okay so saddle points are actually a
23:35 - problem too
23:36 - so it's a it's a long-lasting debate in
23:38 - the deep learning community on whether
23:41 - these things are really exist whether
23:43 - they really are a problem do they do we
23:46 - really meet them
23:47 - in our daily experiments etc
23:50 - and this reference article from 2015
23:53 - says pretty much yes okay these things
23:56 - exist okay and keep in mind here we have
23:59 - only two parameters
24:01 - two dimensions
24:03 - but state-of-the-art models they can
24:05 - have millions tens of millions of
24:07 - parameters okay so you have to try to
24:09 - visualize this in 10 million dimensions
24:12 - okay if you can actually do that welcome
24:15 - to earth right please solve our problems
24:18 - okay so chances are you cannot okay and
24:21 - i can't either so that's why intuition
24:23 - is really important so yes it's quite
24:26 - likely that we have weird things like
24:27 - local minima and saddle points but what
24:30 - uh ian goodfellow says is
24:32 - yes they're here
24:34 - but they don't really
24:35 - impede the training process right we
24:38 - by some magic we don't quite understand
24:42 - we actually managed to go around them or
24:45 - escape them
24:46 - so you know don't lose too much sleep
24:48 - about those anyway right that's the
24:50 - number one question i guess ah yeah but
24:52 - deep learning is just uh it's all crap
24:54 - because of local minima it's like yeah
24:56 - okay in theory yes there is a problem in
24:59 - practice
25:00 - um it it doesn't really bother us okay
25:03 - if you want to read that one you'll get
25:04 - more information
25:06 - okay
25:07 - so
25:08 - that's for the training process right so
25:10 - we do that again and again and again um
25:12 - and
25:14 - how do we know that we're actually
25:16 - making progress well we keep part of the
25:18 - data set we call this the validation
25:20 - data set okay
25:22 - and it's very typical to do this in
25:24 - machine learning you split your data set
25:25 - between training validation and and
25:28 - periodically so at the end of each epoch
25:31 - we run the validation data set
25:35 - through the model uh in training right
25:38 - which this should really say in training
25:40 - neural network not trained because it's
25:42 - not finished we're still training okay
25:45 - and we measure the prediction accuracy
25:47 - for this data set and this gives us a
25:50 - sense of how well or how bad
25:53 - the network does on sample on samples
25:56 - that it hasn't seen so far okay because
25:58 - remember these samples are different
26:00 - from the training set okay
26:03 - so we're going to do that we're going to
26:04 - see this
26:06 - at the end of each epoch and hopefully
26:08 - it goes up showing that not only is the
26:11 - network learning it's also learning to
26:13 - predict samples it has never seen
26:16 - and at the very end once we're done
26:18 - right once we're completely done with
26:20 - training we're done tweaking and we want
26:22 - to compare this model
26:24 - to the previous models that we trained
26:26 - last month last year etc we want some
26:28 - kind of benchmark we're going to use a
26:30 - test data set okay so a third data set
26:33 - that gives us just
26:35 - um again a benchmark accuracy for that
26:37 - model okay so it's a very typical way of
26:39 - doing things three data sets
26:42 - okay
26:43 - so if we plot all those things it should
26:45 - look like this so we should see training
26:47 - accuracy going up really quick and then
26:50 - plateauing
26:51 - if that's a word
26:53 - and if we train long enough we get to
26:55 - 100 accuracy okay guaranteed
26:58 - it's if the network is deep enough large
27:01 - enough if you train long enough if you
27:02 - have enough data you always get 200
27:06 - okay
27:07 - so the loss function the the error right
27:09 - the prediction error
27:11 - uh obviously goes down almost to zero
27:14 - if you plot the validation accuracy it's
27:16 - possible you see something like this
27:18 - okay so again it goes up it tends to
27:21 - follow the training accuracy and
27:22 - plateaus
27:24 - and and you might get you know a high
27:26 - value here like a small bump here and
27:28 - then plateaus again and then it drops
27:31 - and it never never recovers okay it's
27:34 - quite possible this is really more you
27:35 - know
27:37 - more jittery than this it could be like
27:38 - this right very very jittery
27:41 - but then at some point it drops and
27:42 - never recovers
27:44 - and that's
27:45 - extremely bad okay
27:47 - this is called overfitting
27:49 - it's a
27:50 - the number one problem in machine
27:52 - learning and what overfitting really
27:54 - says
27:55 - is you train so hard on the training set
27:58 - that it's the only thing you can predict
28:00 - okay so the model is specialized
28:04 - in predicting the training data set it
28:07 - cannot do anything else okay so that's
28:10 - really bad because obviously in real
28:12 - life you don't want to predict the
28:14 - training set you want to predict
28:16 - real life samples
28:18 - right and these
28:20 - will uh come
28:21 - from you know the validation accuracy
28:23 - anyway
28:24 - so this means that when you're training
28:27 - you should really uh plot this you
28:30 - should keep an eye on the training
28:32 - accuracy of course but you should also
28:34 - keep an eye on validation accuracy
28:36 - and and you should plot this thing at
28:38 - the end of training and decide which one
28:40 - is the best epoch right which which
28:43 - version of the network
28:46 - should you actually use for prediction
28:48 - okay and you cannot know in advance so
28:51 - in order to do this what you really do
28:54 - is save at the end of each epoch you
28:57 - save the weights right you save
29:00 - the version of the model at the end of
29:03 - this epoch and then you plot and you say
29:05 - okay i trained for maybe 200 epochs
29:08 - but the best one is actually 100 129 and
29:12 - that's the one i'm using okay so you
29:14 - have to save the weights
29:15 - um
29:16 - all the weight at the end of each epoch
29:18 - to know which one
29:20 - you're going to use in the end okay
29:22 - so to sum things up
29:24 - deep learning is really about finding a
29:27 - minimum right you you don't care about
29:29 - finding the minimum you'll never know if
29:31 - you found it it's a it's a np hard
29:34 - problem so
29:35 - there you go so all you need to find is
29:38 - a minimum that's good enough right
29:40 - to give you the accuracy that you need
29:42 - for your business problem okay if you if
29:44 - your business problem requires 95
29:46 - accuracy then fine right do that um no
29:49 - point
29:50 - in in chasing uh 97 for weeks and weeks
29:54 - and weeks you maybe you'll never get
29:55 - there so you need to decide what
29:57 - accuracy you need to solve your business
29:59 - problem
30:00 - and once you get there
30:01 - good okay
30:04 - and of course you need to find it fast
30:05 - because if you were able to find it fast
30:07 - if you can actually learn quicker
30:10 - then you train for shorter periods of
30:13 - time you can iterate more you can save
30:15 - money on training et cetera et cetera
30:17 - and if you can't find it reliably if
30:19 - it's not just a lucky accident if you
30:21 - can find it again and again and again
30:23 - it's very good because again you will
30:24 - train many models you will try many
30:26 - combinations you will try different data
30:29 - sets you will try different hyper
30:30 - parameters you will retrain periodically
30:33 - etc so you need to be able to hit
30:36 - this accuracy level again and again and
30:39 - again if you just hit it once and then
30:41 - have trouble hitting it again
30:43 - then you know it's not it's not a viable
30:45 - solution
30:46 - so let's look at a first example
30:56 - yes that's the one
30:58 - okay i guess that's large enough
31:01 - so i'm going to use here i'm going to
31:02 - use a deep learning library called
31:04 - apache mxnet which
31:07 - which is the
31:08 - favorite library at aws
31:11 - and we're going to
31:13 - try to classify a data set called mnist
31:16 - which i'm sure you've seen before
31:19 - uh it's a
31:20 - it's a nice toy data set
31:23 - it's not good for anything
31:26 - except
31:27 - experimenting okay so this is mnist
31:30 - right over there so 70 000 digits
31:33 - handwritten digits so zero to nine and
31:36 - obviously the game is to learn to
31:38 - classify those images in the right
31:40 - category okay and they're black and
31:43 - white images 28 by 28 pixels
31:47 - so the first thing i'm going to do is
31:48 - download the data set
31:51 - that is already split in training and
31:53 - validation i'm going to import mxnet
31:57 - i want to train for 50 epochs
32:00 - i'm gonna use an iterator to load the
32:03 - data set from the files okay and the
32:05 - iterator will slice that data set into
32:08 - batches automatically for me okay so i
32:10 - don't have to
32:11 - slice it into pieces and pass that to
32:14 - the model the iterator does it
32:16 - automatically for me
32:18 - and then we build
32:20 - we build the network and this is
32:23 - almost yeah this is pretty much
32:25 - the one we saw on the slide except it
32:27 - has two hidden layers so i've got a
32:29 - first layer which is the input layer i
32:31 - call it data i don't need to give it
32:33 - size
32:35 - mxnet will figure it out automatically
32:37 - from the from the iterator
32:39 - and
32:40 - if you remember that slide with the
32:42 - fully connected network
32:44 - um my input layer is flat okay
32:47 - it needs that it needs a vector right it
32:49 - needs uh the data samples should be
32:51 - vectors
32:52 - so i have to flatten my images okay so i
32:56 - flatten the image the images into
32:58 - vectors
32:59 - and then i have a fully connected layer
33:02 - with 10 24 neurons active activated by
33:05 - relu and then i have a second fully
33:07 - connected layer with 512 neurons
33:09 - activated by relu and then i have the
33:11 - output layer with 10 neurons because
33:13 - i've got 10 categories right zero to
33:15 - nine
33:16 - okay so those six or seven lines are all
33:19 - i need
33:20 - to actually define
33:22 - my neural network okay and it doesn't
33:24 - matter which library you use it you'll
33:26 - find something similar to this you just
33:28 - stack the layers like this you connect
33:30 - them and
33:31 - obviously you never worry about
33:32 - connecting individual neurons that would
33:34 - be that would be painful okay
33:36 - so now i've got a model okay
33:40 - and i've got a data set in an iterator
33:43 - and i have a model so i need to put
33:45 - the two together this is called binding
33:48 - here
33:49 - okay so binding the model i just created
33:51 - to the training iterator and the
33:53 - validation iterator
33:56 - and then i need to set an optimizer okay
33:59 - so i could actually use a sgd
34:02 - right here
34:03 - with a learning rate a fixed learning
34:05 - rate like this why not
34:07 - um but there is a whole bunch i don't
34:10 - know i said sgd is a
34:13 - host that was invented in 1951 so
34:15 - you know
34:17 - more you know more interesting options
34:18 - have been invented since
34:20 - specifically
34:22 - algos like ada delta and so on that can
34:26 - actually modify the learning rate during
34:28 - the training process right so if this if
34:30 - the slope is very steep they can speed
34:32 - up
34:33 - and if the slope is very flat they will
34:35 - actually slow down and and
34:37 - allow you to explore okay and that's
34:39 - what adam means with adaptative okay so
34:41 - they will change the learning rate um
34:44 - during the training process
34:46 - okay and then i can train so let's
34:48 - actually run this one
34:54 - okay and we can see the uh we can see
34:56 - the training process going on okay
34:58 - whoops
35:00 - so we see the we see the epochs going by
35:03 - and we see the batches going by
35:05 - okay and uh
35:08 - and we see uh you know we we actually
35:10 - have a training accuracy that's uh
35:12 - that gets to one very quickly because
35:14 - it's a large network
35:16 - and it overfits
35:18 - pretty quickly so let you know i've done
35:20 - this before in the interest of time
35:22 - and i saved my model here
35:25 - and i get to a validation accuracy of
35:28 - 98.13 it trends for a few minutes but i
35:31 - don't want to waste a few minutes
35:33 - um okay so this is what i get right i
35:35 - train for 50 epochs that fully connected
35:38 - network on my data set right and i get
35:40 - to the validation accuracy of 98.13
35:44 - so is this good is this bad
35:47 - how do you know right
35:49 - sounds good
35:50 - okay 98
35:52 - but you will not know until you try
35:55 - until you try real-life samples
35:58 - so
36:00 - i did
36:01 - what i had to do i took my paintbrush
36:03 - application
36:05 - and i i drew some digits right and you
36:07 - can see them here and we're going to try
36:09 - and predict them
36:11 - using the model that i just trained so i
36:14 - can load that model again okay which is
36:17 - pretty much as easy as this right
36:20 - load checkpoint with the name of the
36:22 - file storing all the weights
36:24 - okay and then i can predict my digits
36:27 - okay and predicting one of those images
36:29 - is very easy you load the png file
36:32 - uh from disk
36:34 - you turn it into a vector right you
36:36 - flatten the image into a vector
36:39 - and you push it through the train
36:41 - network
36:42 - okay there's nothing to be worried about
36:44 - that's really one line of code okay
36:47 - load the image
36:49 - put it in a vector push it through the
36:51 - trained model and then read the outputs
36:53 - okay so very simple to predict
36:56 - and when we do that and let me run those
36:58 - cells again
36:59 - i just want to make sure
37:01 - i have the right model here
37:04 - oops
37:06 - there we go run all the cells
37:09 - okay
37:10 - and so for each of those pictures we see
37:12 - a vector of 10 probabilities right this
37:15 - is exactly what we read on the output
37:17 - layer for the for the neural neural
37:19 - network okay
37:21 - so we see that zero worked out okay
37:24 - because actually
37:25 - uh probability zero is the highest 99
37:29 - okay
37:30 - and for the one probability one is the
37:32 - highest
37:33 - etc etc so two is very good three is
37:36 - very good
37:37 - and uh you never really see zeros and
37:39 - ones it's because i stopped at four
37:41 - decimals so this really means it's at
37:43 - least
37:44 - 0.9999 something okay
37:47 - you will always see uh non-null values
37:51 - here
37:52 - four is fine five is fine six
37:55 - good seven
37:57 - eight
37:59 - and nine
38:01 - this should be the highest
38:04 - right
38:05 - it's not
38:07 - this one is the highest so zero one two
38:11 - three four okay so my model is confused
38:13 - it thinks this is a four
38:15 - okay
38:16 - so that's what i told you you have to
38:18 - try different
38:19 - samples you know the validation data set
38:22 - is one thing but the test data set which
38:24 - should really be composed of real life
38:27 - samples stuff that people will really
38:30 - predict not just
38:32 - sample data sets this this is really
38:35 - what tells you how well your network
38:36 - performs so this is a disappointing
38:39 - right because i try 10 and one of them
38:40 - is wrong
38:42 - so why is that so you could say well
38:45 - this is really a very ugly nine so yeah
38:48 - you know you can't expect the model to
38:49 - predict ugly stuff like that
38:52 - okay fair enough but some people write
38:54 - really really bad
38:55 - uh and uh digits so you should still be
38:58 - able to figure it out but remember what
39:01 - we did to these images right
39:03 - we flattened them
39:05 - okay so
39:08 - intuitively it doesn't sound like a
39:09 - really great id
39:11 - because images are really 2d objects and
39:14 - we flatten them so we lose the
39:16 - relationship right the proximity between
39:19 - some of the pixels
39:20 - and probably that doesn't help
39:22 - understanding what's in those pictures
39:25 - so to solve this
39:32 - a different type of network was uh was
39:35 - invented 20 years ago already and
39:37 - they're called convolutional neural
39:38 - networks
39:40 - and these networks are the kings of
39:43 - image processing image classification
39:46 - okay
39:47 - um
39:48 - and
39:50 - the the basic idea here is
39:53 - we'll will work with the 2d image
39:56 - or maybe 3d if it's a color image right
39:58 - it's going to be 3d because it's red
40:00 - green blue okay
40:01 - but anyway we don't flatten anything we
40:03 - stick to uh to multi-dimensional objects
40:07 - and we use two operations we use
40:08 - convolution and we use pooling also
40:11 - called sub sampling okay
40:13 - so convolution is actually extracting
40:16 - features okay here's an example
40:19 - if you take this filter three by three
40:22 - and you slide it across the image right
40:25 - so
40:25 - nine pixels by nine pixels going all the
40:28 - way
40:29 - applying the convolution operation which
40:31 - is
40:32 - pretty much
40:33 - taking the underlying pixel here
40:35 - multiplying it by the corresponding
40:39 - value in the kernel
40:41 - okay and you do this nine times for the
40:42 - nine pixels here and you add everything
40:45 - together and that becomes the new top
40:47 - left pixel and then you slide and you do
40:49 - it again and again and again
40:51 - okay
40:53 - if you use this filter the specific
40:55 - values you go from this image to this
40:57 - image okay so this is really an edge
41:00 - detector filter
41:02 - and this what did we achieve here well
41:05 - we we detected
41:07 - the edges in the original picture
41:10 - and we threw away pretty much everything
41:12 - else everything else
41:13 - is black right we only kept the edges so
41:17 - we extracted
41:19 - the edge feature in this image
41:22 - okay and this is what convolution is
41:24 - about so by running many different
41:27 - filters
41:28 - okay
41:29 - it could be
41:30 - you know 20 30 50 filters
41:34 - you you extract different features from
41:36 - the image so it could be edges it could
41:38 - be contrast it could be
41:40 - vertical lines you know horizontal lines
41:43 - whatever you know
41:45 - and you get a combination a new set of
41:49 - uh
41:49 - convoluted images
41:51 - which you then shrink with pooling i'll
41:54 - show you how in a second
41:55 - and then you do that again
41:57 - okay and you get your collection of very
41:59 - tiny images that look nothing like the
42:01 - original image but they kept the good
42:04 - stuff they kept the features and the
42:06 - important information and then you can
42:08 - flatten this and use a fully connected
42:10 - network okay that's the
42:12 - the intuition between behind convolution
42:15 - networks you automatically extract
42:17 - features using a collection of filters
42:20 - and then
42:22 - you throw away you shrink the images you
42:25 - throw away the useless information in
42:27 - these pictures and you do that again and
42:28 - again and again and if you heard that
42:30 - deep learning
42:31 - extract features automatically this is
42:34 - it okay this is what people mean
42:36 - you use kernels or filters to extract
42:40 - important information and throw away the
42:43 - rest
42:45 - and obviously
42:46 - we have no idea what those values should
42:48 - be right here it's just a
42:51 - it's an example but initially
42:53 - the values for all the features
42:55 - generating those pictures are random
42:58 - so they are learned during the training
43:00 - process so the training process of a
43:02 - convolutional neural network is really
43:04 - about
43:05 - finding which filters extract the right
43:08 - features to help you understand those
43:11 - images right and it's the exact same
43:13 - process
43:14 - back propagation sgd et cetera et cetera
43:17 - okay
43:19 - so it gets a little more complicated but
43:20 - the overall picture is the same okay you
43:23 - learn the filters that lets you extract
43:26 - the right features
43:27 - okay
43:28 - and pooling is much simpler pooling is
43:31 - just about
43:32 - if we do a two by two pulling here we
43:34 - just uh take two by two pixel blocks and
43:37 - we keep
43:38 - the brightest right the brightest pixel
43:40 - the the highest value
43:42 - okay and if we do this all over the
43:44 - picture then
43:45 - you know we get a smaller picture
43:47 - and the intuition here is that
43:50 - in this image
43:51 - you don't care about the black stuff
43:53 - okay you want to throw that away it
43:54 - tells you nothing it was filtered out
43:57 - okay what's important is really the
43:59 - white pixels the edge
44:01 - of that
44:02 - weird beast
44:04 - okay
44:04 - so you can shrink the image by keeping
44:07 - the white pixels and it will gradually
44:09 - distort the image obviously but
44:12 - you uh
44:13 - you keep enough information to learn to
44:15 - classify them
44:17 - that's the id
44:18 - so let's really quickly try this
44:21 - so we're going to try to classify
44:24 - the same
44:27 - data set but this time
44:29 - with a convolution network and it's
44:31 - pretty much the one you saw on the slide
44:33 - right input layer
44:35 - convolution with 32 filters
44:39 - pooling
44:40 - 2 by 2 pulling
44:42 - convolution again with 64 filters
44:46 - pulling again 2x2 right so here from the
44:49 - initial image we built 32 and then from
44:52 - the 32 we built 64. okay and we shrink
44:54 - them so we end up having a large
44:56 - collection of tiny images
44:58 - and we flatten everything at the end so
45:01 - all those tiny images become one big
45:03 - vector and we use a fully connected
45:05 - layer to classify them into ten
45:08 - categories because once again zero to
45:10 - nine okay so you see this api is quite
45:13 - simple you can literally look
45:15 - at the at the network and encode it
45:18 - right just stack the layers
45:20 - and uh and there you go and the rest is
45:22 - the same right we bind we train
45:25 - so i train again in interest of time
45:26 - takes again two three minutes
45:29 - and we get to 99.15 accuracy this time
45:33 - okay
45:34 - so it's a bit higher than the previous
45:35 - one
45:37 - but now we're you know we're suspicious
45:40 - so
45:41 - we're gonna
45:45 - we're going to try that new model the
45:47 - one that we saved and we're going to
45:49 - predict again
45:54 - so
45:55 - 0 1 2 3 you know they all look very good
45:59 - six seven eight
46:01 - what about nine
46:04 - so
46:05 - is this the highest now
46:07 - yes it's the highest it's still not
46:09 - great
46:11 - right which is a tribute to my ugly nine
46:13 - here
46:14 - it's still not great but it's the
46:16 - highest
46:17 - okay so if i had to decide which thing
46:19 - is this then i would say okay it's a
46:22 - nine
46:22 - not super super sure about it but okay
46:25 - if i have to make a guess i'll tell you
46:26 - it's a nine okay and
46:29 - obviously you could keep tweaking this
46:30 - to get better results you could add have
46:32 - more filters more layers etc etc but
46:34 - again the intuition is really important
46:37 - okay
46:38 - that's why cnns work really well
46:40 - on on images it's because they keep the
46:44 - 2d or the 3d image
46:46 - and
46:47 - and they don't flatten stuff okay they
46:49 - just extract features from the actual
46:51 - image so it makes sense that they would
46:52 - be able to classify them better
46:55 - so you can do all kinds of crazy stuff
46:57 - with this
46:59 - and
47:00 - there's a an extra api in mxnet called
47:03 - gluon
47:04 - and specifically gluon cv which means
47:06 - computer vision that provides you with a
47:09 - collection of pre-trained models okay
47:11 - and these are state-of-the-art models
47:13 - with hundreds of layers they've been
47:15 - trained on really really big data sets
47:17 - and uh and you can do classification
47:20 - detection
47:21 - segmentation right these are just a few
47:24 - examples
47:25 - and any of those is literally five lines
47:27 - of code so i'm just going to show you
47:29 - one but you'll find everything on github
47:34 - let's look maybe at uh yeah let's look
47:37 - at uh
47:39 - okay classification
47:42 - okay so how complicated is it
47:45 - to do this
47:48 - so uh we're going to call this api to
47:52 - load a pre-trained model
47:54 - okay so it's called model zoo so we
47:56 - fetch a pre-trained model from the web
47:59 - already trained no training needed
48:02 - we load a local image
48:05 - we're going to normalize it okay
48:07 - normalization is important so make sure
48:09 - the you know the red green and blue
48:11 - channels etc are normalized
48:14 - and then
48:16 - we predict it so we take that image push
48:18 - it through the train model
48:20 - and then we can read the results and
48:22 - that's it
48:23 - okay
48:24 - so training your own model uh is fine
48:28 - and we could we could run one of those
48:31 - yeah let's try uh okay let's try this
48:33 - one okay let's try to segment this
48:36 - here we go okay
48:40 - okay so that's the original picture
48:43 - and i'm running it on my mac so no gpu
48:45 - so it takes a takes a few seconds
48:49 - but here you take an existing model
48:51 - pre-trained and and you just get the job
48:54 - done right so of course you could train
48:56 - that model on your own data set right
48:59 - there you go
49:00 - five six seconds no gpu so on a gpu it
49:03 - would be just like this
49:04 - um and this really takes five lines of
49:07 - code
49:07 - okay so before you go and try to build
49:10 - everything from scratch just consider
49:14 - existing models pre-trained models that
49:17 - that might be good enough
49:19 - to do the job right and you would save
49:21 - an insane amount of time by using those
49:23 - pre-trained models
49:29 - okay
49:31 - very last thing i want to talk about is
49:35 - we looked at fully connected networks
49:37 - and we looked at
49:40 - convolution networks okay and if we
49:42 - predicted with those if we took 100
49:44 - samples and predicted them in any order
49:46 - we would still get the same results okay
49:49 - sequence doesn't matter
49:51 - which is a problem because when you want
49:53 - to work with time series when you want
49:55 - to work with sequences of data the order
49:58 - of the samples does matter right when
49:59 - we're translating for example we don't
50:02 - translate each word independently we
50:04 - look at the previous words to uh to
50:07 - actually uh get some context so we need
50:09 - some kind of memory short-term memory
50:12 - on the past few predictions to make the
50:14 - right prediction and this is what lstm
50:17 - networks are all about so they have a
50:19 - different type of neuron which i will
50:20 - not explain i don't have time and it's a
50:22 - it's a little more involved
50:24 - but you just need to remember that a
50:26 - prediction done by an lstm neuron
50:28 - depends on the input
50:30 - and it depends also on the past few
50:33 - predictions so they have some kind of
50:34 - memory right that's why they're called
50:36 - short-term memory networks
50:38 - and this is very good if you want to
50:40 - predict sequences
50:42 - right so time series machine translation
50:46 - bitcoin prices anything you want
50:49 - and uh talking about machine translation
50:51 - uh we have an open source project called
50:54 - sockeye
50:55 - that you'll find on github that uses an
50:58 - lstm architecture to
51:00 - to let you train machine learning models
51:03 - for
51:04 - machine translation okay and uh it's
51:07 - super simple to use
51:10 - and maybe we can do a really quick demo
51:12 - if i still have a
51:14 - come on
51:16 - i still have an ssh connection here
51:19 - so i took a data set for uh german to
51:22 - english
51:25 - and
51:26 - no
51:27 - oh yeah
51:30 - slow connection but we'll find
51:31 - okay so i have a few million sentences
51:34 - in english and in german and english and
51:36 - i trained a model from german to english
51:38 - it took a few hours on a couple of gpus
51:41 - so i could take
51:43 - some german sentences here
51:45 - and i could run them through that
51:48 - and i have just one for you here
51:50 - hopefully that works
51:53 - yeah okay see
51:55 - so if you want to build your own
51:57 - translation model
51:59 - just go grab a sockeye train for a few
52:02 - hours if you have the data set it's not
52:04 - complicated it's a pre-existing
52:05 - architecture and you can just do it like
52:07 - that okay so you can build your own
52:10 - translation service just like that okay
52:13 - all right
52:18 - i'm out of time so
52:19 - okay i'll take 30 seconds for the crazy
52:21 - stuff because i hate not talking about
52:23 - it and then i'm done
52:25 - so the last type of architecture i want
52:27 - to talk about is called gans generative
52:29 - adversarial networks and they
52:33 - they are weird right so
52:35 - who knows who these people are
52:39 - i know you're working hard but you gotta
52:41 - watch movies right
52:43 - come on
52:44 - tv or something no no one no one knows
52:48 - i i had nothing to win but okay if i had
52:50 - something to win i would give it away
52:53 - no no
52:55 - that's your last word
52:57 - okay now i know you're just very
52:59 - suspicious now right of this frenchman
53:01 - showing you weird stuff okay you're
53:03 - right to be suspicious because these
53:04 - people do not exist right they are
53:07 - generated faces
53:09 - and they're not copy paste right so it's
53:10 - not the nose of brad pitt and the ears
53:13 - of johnny depp or something like that
53:15 - it's really generated pixel by pixel by
53:17 - looking at a large number of images in
53:20 - in the celebrity data set that's why
53:22 - they end up being quite good looking
53:26 - the network learns to generate similar
53:29 - samples
53:30 - okay
53:31 - but these are completely fake
53:33 - right
53:34 - here's another example you
53:37 - remember that right when you were
53:38 - five-year-old or if you have kids you
53:40 - have stuff like this on your fridge
53:42 - right
53:42 - i'm sure i do
53:45 - so these are cars right this is the road
53:47 - these are trees etc and this is called a
53:49 - semantic map okay
53:52 - and again you can by training a neural
53:56 - network on images and the corresponding
53:59 - semantic map
54:00 - you can then predict
54:03 - this semantic map okay so you give this
54:06 - to the model and the model will generate
54:08 - the actual picture
54:10 - okay and this is hd right now these are
54:13 - hd pictures so you can zoom in and you
54:15 - can see a lot of detail actually
54:17 - right these are really really precise
54:20 - okay so this and this gun
54:22 - area of research is moving all the time
54:24 - it's it's really crazy and
54:27 - and if you're interested you know i
54:29 - would suggest you read about that you'll
54:30 - see some even crazier applications of
54:33 - guns right generating a new reality
54:38 - right so um how do you get started with
54:40 - all of this
54:42 - um
54:43 - well i would recommend obviously taking
54:45 - a look at uh the the adobe's website
54:49 - where you will find information on all
54:51 - the tools
54:52 - and all the libraries that our customers
54:55 - use to build a pretty cool stuff okay
54:58 - the blog has some technical articles etc
55:01 - if you're curious about mxnet and gluon
55:03 - it's all on github right so just go and
55:05 - grab the sources grab the tutorials
55:08 - there's lots of documentation for it and
55:10 - you can replay
55:12 - a lot of those examples
55:14 - if you uh if you're already doing
55:15 - machine learning in production and you
55:17 - have scaling problems right
55:20 - we have a service called sagemaker that
55:22 - lets you train pretty much anything
55:26 - tensorflow mxnet pi torch scikit-learn
55:29 - we have some built-in algos as well to
55:31 - save you more time so if you just want
55:33 - to focus on machine learning and not on
55:35 - infrastructure
55:36 - i would recommend taking a look at
55:38 - sagemaker we have sdks
55:40 - and we have a very very cool integration
55:42 - with spark if you use spark
55:45 - sagemaker makes a lot of sense as well
55:47 - and finally uh here's my blog on medium
55:49 - where you'll find lots of articles on
55:51 - machine learning deep learning sage
55:53 - maker
55:54 - and more crazy stuff
55:56 - i have a by now i've got quite a
55:57 - collection of talks on youtube as well
55:59 - so if you want to dive deeper on on
56:01 - those topics or or explore what aws has
56:05 - to offer you'll find something here
56:07 - and the code that i use today and much
56:10 - more
56:11 - is available on gitlab so just go and
56:13 - grab that and you can run these examples
56:15 - and and more all right
56:17 - okay i'm done thank you very much if you
56:20 - want to stay in touch thank you
56:23 - [Applause]

Cleaned transcript:

all right i think you can get started now please take a seat so uh good morning everybody my name is julian i'm a tech evangelist with aws i've been working with them for almost three years now and these days i focus on ai and machine learning and it's really nice to be back in krakow i was here for a code europe in december it was cold and dark very cold and very dark it's very nice to enjoy the city with sunlight and getting a chance to visit it's really beautiful so good to be back so my topic today is deep learning for developers and well the title is really important because it means if you can read and write let's say 50 lines of python you can do this okay so we're gonna start with an introduction to the underlying concept some of the theory that actually holds deep learning together but i'm gonna try to make it as pragmatic and and concrete as possible with minimal theory and then we'll look at some code run some examples and try to put everything together okay so you've heard of deep learning as a subset of machine learning that uses a technology called neural networks right so the first thing we should uh we should look at and i realized this is really hard to start with this slide but hey we got to understand what this thing is all about right so stay with me here we got to understand what a neuron is and what we're trying to do really is trying to build uh some kind of approximation of a biological neuron so what we know about biological neurons is they have inputs they have connections and if those inputs are stimulated enough then the neuron will actually fire it will emit an electrical current if it's not stimulated enough it won't fire okay so we're trying to build something that looks a bit like that so obviously for a deep learning neuron we will have inputs okay and these will be floating point values um we'll see where those come from and to each of these inputs is associated a weight which is another floating point value and this weight in a way makes this input more or less important than others okay and the first thing that we do when we actually uh run something on that neuron is to compute this multiply and accumulate operations so we take each input we multiply it by the corresponding weight and we add everything together okay it's called multiply and accumulate so there you go right deep learning is really about adding and multiplying stuff and doing this at very large scale so if you can add and multiply you already know 50 of what deep learning really is okay but the problem with this function is is that it's a linear function if we change the inputs right if the inputs vary linearly then the result that u value also varies linearly and that's not what we want because remember we want something that fires or not okay so there needs to be some kind of limit some kind of threshold that says when does the neuron fire or not so this is why we add the activation function at the output of each neuron and over time a number of functions have been used these days a relu is the popular one and as you can see uh here it's it's nonlinear right if if the input value to relu okay so if that u value here is negative then the neuron will output zero and if the input to the activation function is positive then the neuron will output that same value and it could be a very very large value okay so this is really what we want right nothing happens until a certain point and then we output a value and this could be a really large value okay so if the neuron is stimulated enough it can actually output very large values and this is what we want and this is how we introduce that nonlinear behavior in the neuron okay so obviously a neuron by itself doesn't do much they become interesting when we combine them into layers and networks and this is probably the simplest one you could build okay so we have an input layer where we will actually put our data data samples to predict we have an output layer where we will read results right we will read the predictions and in the middle we have a hidden layer at least one so in this case just one and as you can see it's fully connected right so all neurons are fully connected to the inputs fully connected to the outputs okay that's why they're called fully connected networks okay so let's say we're already trained listing okay we'll we'll talk about training in a few minutes but let's say it's already trained how would we use it okay so of course we need data samples and for this discussion let's say we're trying to classify pictures okay so we have a number of pictures let's say animals right dogs cats elephants tigers anything that you want and we know what they are okay so we're going to take each image we're going to flatten it into a vector and we're going to store the pixel values in a matrix like that okay so this is the first picture this is the second picture and so on and so on okay so each picture has has been transformed into a vector of pixel values okay and we know what those pictures are okay so uh um we know for example that the first picture here is a category two and maybe category two are dogs right and we know the second picture is category zero maybe that's cats and so on and so on okay and let's say we have ten categories in total so that's the data set okay a number of pictures that we flatten into vectors and that we label with category numbers from zero to nine let's say okay actually we're not we don't like using this if you already work with machine learning you know we don't like categorical values to be handled like this we use another technique called one hot encoding okay and that's a complicated word for a simple thing one hot encoding means instead of using category numbers you're going to build a vector of bits okay and again let's say we have 10 categories here so we will have 10 bits and we'll flip to one the bit that corresponds to the right category okay so if this one is category two then bit zero one two is flipped okay this one is category zero so we flip bit zero and so on okay and why is this important because well this form right is actually much more expressive than this because here we can see how many classes we have first and second we could look at those numbers as probabilities for each of the classes okay and what this really says here is uh this sample has a hundred percent chance of being category two and zero percent chance of being any other category right we know for sure because we know what the data set looks like okay same thing here we know that this last sample here is category four so it has 100 chance of being category four and zero percent chance of being anything else okay and this is this is actually what we will get on the output layer when we when we put those samples on the input layer here right and we run those multiply and accumulate and those activation functions etc what we will read here in those neurons are the probabilities for the samples okay so hopefully if we train our network right okay we would get to something like this we take a data sample and as you guessed we need add many we need as many neurons in the input layer as we have features okay so if we have a thousand pixels here we'll need a thousand neurons here okay then we run what is called forward propagation so running that sample through the model multiply accumulate activate etc and you read some results on the output layer and again if you have 10 classes you will need 10 neurons on the output layer okay and you will read the probabilities for each class so in a perfect world this is what we would see right we would really see zeros every everywhere except a one in the new round that corresponds to the right class to the right category but nothing is perfect and it's not going to happen like that anyway what we care about is accuracy so being able to successfully predict the maximum number of samples okay this is what we're going to measure and track okay so this is where we want to get but of course initially the network is not trained okay so all the weights corresponding to all those connections here right all those parameters they have random values so if you take a data sample and forward propagate you get something completely wrong on the output layer and it would it would not be something like this actually it would not be a one in the wrong place and zero zeros it would just be random probabilities for the ten classes okay because again the network has not been trained so if we see this neural network as a as a function and we take a sample and compute the output we're not going to get the right output we're not going to get the right label the right probability vector we're going to get something different okay let's call it y prime one so obviously you need to measure the difference between what you expected and what you really got okay and we do this using a math function called a loss function and remember that y one and y prime one are really vectors okay probability vectors so it's not as easy as uh subtracting one from the other but you don't have to worry about those because all the deep running libraries provide those loss functions already so you can just use one that matches you the problem you're trying to solve okay so this loss function will give give us the error okay just a numerical value measuring the error the distance so to speak between the two vectors so actually we're not going to do this sample by sample okay imagine you have 10 million samples in the data set it takes too much time to process every single one and and train on every single one so we actually train on batches of samples okay so we're going to take 32 samples for 64 samples at a time run them one by one through the model and compute the total error for the batch okay and then we're going to make decisions on how to reduce the error okay more on this in a minute okay so keep in keep in your mind we train on batches of samples not individual samples okay so in a nutshell the purpose of the training process is really only one thing it's to minimize loss okay minimize prediction error for the data set by training over and over right iteratively and by by adjusting the weights gradually during the training process okay so what we're really trying to do here is um in these examples we i don't know how many weights we have maybe let's say 25 or something like that okay we're trying to find the set of weights the set of parameters that give us the highest accuracy for that data set and that means the lowest possible error for that data set okay and we start from random weights and we have to try to go as close as possible to that optimal set of weights okay and that's a difficult problem because you cannot compute them right it's not something it's not an equation you can solve so you have to gradually discover what those weights should be okay more on this in a second so the training process really looks like this we have a data set like i said we slice it in batches and again we'll see in the code that deep learning libraries do this automatically so no no work needed and we take one batch okay let's say it's 32 samples and we run each one of those samples into the network okay and we get to prediction and we compare the prediction to the actual result we were expecting and we compute loss okay and we do this for about all the batch samples and we add up all the errors into the batch error okay so we now we have a batch arrow that tells us you know what's the what's the mistake how big is the mistake we made for this batch okay and now we can take decisions on how to reduce it and this is done with an algorithm called back propagation and it's a it's a scary one it usually that's usually where when people stop looking and uh and stop studying deep learning because they want to understand back propagation in detail and it's a bit scary okay so back propagation as the name implies will go back from the output layer to the front okay and layer by layer and neuron by neuron it's going to adjust the weights in a direction that we know reduces error okay so for example here okay this neuron the the error value that you get for this neuron depends on three parameters okay so for these three parameters you have to figure out if each of them should be increased or decreased to to lower error right or remember these are floating point values so these are the only two choices that you have right you can increase them a bit or decrease them a bit so how do you know okay i will answer that in a second so once you've done that here okay you've updated those you've updated those weights so you could compute the new error for the next layer and then you do it again so this the error that you get here for this individual neuron depends on one two three four five parameters okay so once again here you need to make five individual decisions on increasing or decreasing those weights and you do this for every single neuron and then you move back to the previous layer until you get to the input okay that's why it's called back propagation okay so now obviously uh you want to do this all over again for the next batch right compute the batch error back propagate and you do this again and again and again and again okay until you get to the end of the data set okay and this is called an epoch and typically you're going to train for 50 100 200 epochs okay so it's really an iterative process and you train and you predict again and again and again and again right computing the batch error and then adjusting the weights in a direction that you know will reduce error and and you do this until you hopefully get to the right level of accuracy that you were expecting okay so that's the big picture okay and at the end of that process you get to a trained neural network okay and as you can see um we have a number of parameters that are really important the batch size is important if you have very tiny batches you know you back propagate a lot you take takes a lot of time so yes you will probably get to the right spot but it takes too long if you have a very large batch size then you get less opportunities to run back propagation per epoch so maybe that's a problem too the learning rate will actually decide on the the the size of the updates that you make to the weight okay we'll we'll talk about that in a minute so it's important as well small learning rate small updates to the weight large learning rate large update to weights okay and again too small will be a problem too large would be a problem and the number of epochs is how many times you go through the data set these are called hyper parameters and if you don't get them right you will have lots of problems training okay so that's the big picture okay it's not really complicated the only weird thing is hey i get the batch size the batch thing and i get the back propagation but then you said we need to adjust the weight the weights each weight actually in the direction that we know reduces error how do we know right how do we know if a given weight should be increased or decreased okay let's look at a real example so imagine you have a function here let's call it f and it has two parameters okay and the output is let's call it z okay and let's let's say that x and y are parameters and z is the error okay and which we start with random values of x and y and we want to get we want to figure out what x and y should be to get the smallest value of z okay that's really what we're trying to do find the set of parameters that give us the smallest possible output okay and if we plot that function let's say it looks like it looks like this okay and remember x and y are initially random so we're going to start anywhere here okay so let's say we start here okay and let's say this is x and this is y all right so we want to get to the smallest possible value of z so probably that's here right so in a way we want to walk down that slope until the lowest point in the valley right we're trying to get down to the valley and remember we cannot compute x and y okay we have to discover them so how would you do that right if you were in the mountain how would you do that imagine you're in the fog you see nothing um and you have to decide if you uh you know if you should go forward or backward and if you should go left or right okay so what you would do is maybe you know you would with your foot you would try in this direction and say ah yeah okay this is down all right fine and then you would go right and say oh no this is up oh this is down okay so this is down this is down so i'm going to take a small step here and i'm a little lower than i was before okay and you do this again and again and again and again right and if there's not a big crack if it's a smooth surface like this if it's friendly you will actually get to the lowest point iteratively by taking small steps in the right direction okay that's the intuition now obviously deep learning doesn't have intuition it's it's math so how do you know for sure that you need to maybe increase x a bit okay or actually here it would be if the origin is here okay x and y actually we have to decrease x and y a little bit to go down right initially so how would we know well remember high school that's the part you're gonna hate by the way so get ready to hate it uh remember high school right i remember slopes and remember derivatives right no all right are you too kind right well that's exactly what we do here okay this is a function okay yes it has two parameters but we can compute the partial derivative for x and we can compute the partial derivative for y at this specific point okay and so that gives me the slope in the x direction and that gives me the slope in the y direction and then if i have the slope i know which way is up and which way is down okay simple so once i know that well i should decrease x a bit to go down in the x direction and i should decrease y a bit in the y direction then fine i'm just going to do that i'm just going to going to modify x and y just a bit in the right direction and i get let me say here okay and i do it again and again and again and again okay and this algorithm is called stochastic gradient descent or sgd it's the granddaddy of optimizing functions uh it it dates back to 1951 okay so it's nothing new it has actually nothing to do with machine learning it's a it's a math optimization function and that's the one that is still heavily used in deep learning okay so coming back to my example here this is what actually will happen during the training process when you run back propagation okay the library that you're using will will compute the derivative for each of those three weights okay with respect to the error and then decided they should be increased or decreased do that and then move on to the previous layer and do it again and again and again okay and this is how you know in which direction all those weights should be updated and since we're taking small steps we need to do it again and again and again right and this is why we do batch training and this is why we need many epochs because we take very tiny steps but we if we take tiny steps always in the right direction right eventually we get to the right place okay that's all there is to it right high school math nothing to worry about um so obviously surf the surface is not going to look like this right would be too easy it could look like this right or you could be seeing things like this where yes you seem to have a nice global or you know a lower minimum here and you have you know not as good ones here so maybe you could start you know on the mountain here and and walk down and maybe you could fall in this hole here or this one right and let's say this one gives you a higher error than this one does so it's probably more desirable to be here than to be here right and they're called local minima and another problem could be saddle points and this one is actually even worse so it's look it's like a horse saddle and uh in this direction right we can see yes this point here the green point here is actually the minimum so fine but in the other direction right like this this point here is actually a maximum right and if you remember high school again you know that the derivative is is equal to zero when a point is a minimum or a maximum okay so if we actually started here and went down exactly to that green point uh we'd be in trouble because derivatives would be zero we could not update the weights okay so saddle points are actually a problem too so it's a it's a longlasting debate in the deep learning community on whether these things are really exist whether they really are a problem do they do we really meet them in our daily experiments etc and this reference article from 2015 says pretty much yes okay these things exist okay and keep in mind here we have only two parameters two dimensions but stateoftheart models they can have millions tens of millions of parameters okay so you have to try to visualize this in 10 million dimensions okay if you can actually do that welcome to earth right please solve our problems okay so chances are you cannot okay and i can't either so that's why intuition is really important so yes it's quite likely that we have weird things like local minima and saddle points but what uh ian goodfellow says is yes they're here but they don't really impede the training process right we by some magic we don't quite understand we actually managed to go around them or escape them so you know don't lose too much sleep about those anyway right that's the number one question i guess ah yeah but deep learning is just uh it's all crap because of local minima it's like yeah okay in theory yes there is a problem in practice um it it doesn't really bother us okay if you want to read that one you'll get more information okay so that's for the training process right so we do that again and again and again um and how do we know that we're actually making progress well we keep part of the data set we call this the validation data set okay and it's very typical to do this in machine learning you split your data set between training validation and and periodically so at the end of each epoch we run the validation data set through the model uh in training right which this should really say in training neural network not trained because it's not finished we're still training okay and we measure the prediction accuracy for this data set and this gives us a sense of how well or how bad the network does on sample on samples that it hasn't seen so far okay because remember these samples are different from the training set okay so we're going to do that we're going to see this at the end of each epoch and hopefully it goes up showing that not only is the network learning it's also learning to predict samples it has never seen and at the very end once we're done right once we're completely done with training we're done tweaking and we want to compare this model to the previous models that we trained last month last year etc we want some kind of benchmark we're going to use a test data set okay so a third data set that gives us just um again a benchmark accuracy for that model okay so it's a very typical way of doing things three data sets okay so if we plot all those things it should look like this so we should see training accuracy going up really quick and then plateauing if that's a word and if we train long enough we get to 100 accuracy okay guaranteed it's if the network is deep enough large enough if you train long enough if you have enough data you always get 200 okay so the loss function the the error right the prediction error uh obviously goes down almost to zero if you plot the validation accuracy it's possible you see something like this okay so again it goes up it tends to follow the training accuracy and plateaus and and you might get you know a high value here like a small bump here and then plateaus again and then it drops and it never never recovers okay it's quite possible this is really more you know more jittery than this it could be like this right very very jittery but then at some point it drops and never recovers and that's extremely bad okay this is called overfitting it's a the number one problem in machine learning and what overfitting really says is you train so hard on the training set that it's the only thing you can predict okay so the model is specialized in predicting the training data set it cannot do anything else okay so that's really bad because obviously in real life you don't want to predict the training set you want to predict real life samples right and these will uh come from you know the validation accuracy anyway so this means that when you're training you should really uh plot this you should keep an eye on the training accuracy of course but you should also keep an eye on validation accuracy and and you should plot this thing at the end of training and decide which one is the best epoch right which which version of the network should you actually use for prediction okay and you cannot know in advance so in order to do this what you really do is save at the end of each epoch you save the weights right you save the version of the model at the end of this epoch and then you plot and you say okay i trained for maybe 200 epochs but the best one is actually 100 129 and that's the one i'm using okay so you have to save the weights um all the weight at the end of each epoch to know which one you're going to use in the end okay so to sum things up deep learning is really about finding a minimum right you you don't care about finding the minimum you'll never know if you found it it's a it's a np hard problem so there you go so all you need to find is a minimum that's good enough right to give you the accuracy that you need for your business problem okay if you if your business problem requires 95 accuracy then fine right do that um no point in in chasing uh 97 for weeks and weeks and weeks you maybe you'll never get there so you need to decide what accuracy you need to solve your business problem and once you get there good okay and of course you need to find it fast because if you were able to find it fast if you can actually learn quicker then you train for shorter periods of time you can iterate more you can save money on training et cetera et cetera and if you can't find it reliably if it's not just a lucky accident if you can find it again and again and again it's very good because again you will train many models you will try many combinations you will try different data sets you will try different hyper parameters you will retrain periodically etc so you need to be able to hit this accuracy level again and again and again if you just hit it once and then have trouble hitting it again then you know it's not it's not a viable solution so let's look at a first example yes that's the one okay i guess that's large enough so i'm going to use here i'm going to use a deep learning library called apache mxnet which which is the favorite library at aws and we're going to try to classify a data set called mnist which i'm sure you've seen before uh it's a it's a nice toy data set it's not good for anything except experimenting okay so this is mnist right over there so 70 000 digits handwritten digits so zero to nine and obviously the game is to learn to classify those images in the right category okay and they're black and white images 28 by 28 pixels so the first thing i'm going to do is download the data set that is already split in training and validation i'm going to import mxnet i want to train for 50 epochs i'm gonna use an iterator to load the data set from the files okay and the iterator will slice that data set into batches automatically for me okay so i don't have to slice it into pieces and pass that to the model the iterator does it automatically for me and then we build we build the network and this is almost yeah this is pretty much the one we saw on the slide except it has two hidden layers so i've got a first layer which is the input layer i call it data i don't need to give it size mxnet will figure it out automatically from the from the iterator and if you remember that slide with the fully connected network um my input layer is flat okay it needs that it needs a vector right it needs uh the data samples should be vectors so i have to flatten my images okay so i flatten the image the images into vectors and then i have a fully connected layer with 10 24 neurons active activated by relu and then i have a second fully connected layer with 512 neurons activated by relu and then i have the output layer with 10 neurons because i've got 10 categories right zero to nine okay so those six or seven lines are all i need to actually define my neural network okay and it doesn't matter which library you use it you'll find something similar to this you just stack the layers like this you connect them and obviously you never worry about connecting individual neurons that would be that would be painful okay so now i've got a model okay and i've got a data set in an iterator and i have a model so i need to put the two together this is called binding here okay so binding the model i just created to the training iterator and the validation iterator and then i need to set an optimizer okay so i could actually use a sgd right here with a learning rate a fixed learning rate like this why not um but there is a whole bunch i don't know i said sgd is a host that was invented in 1951 so you know more you know more interesting options have been invented since specifically algos like ada delta and so on that can actually modify the learning rate during the training process right so if this if the slope is very steep they can speed up and if the slope is very flat they will actually slow down and and allow you to explore okay and that's what adam means with adaptative okay so they will change the learning rate um during the training process okay and then i can train so let's actually run this one okay and we can see the uh we can see the training process going on okay whoops so we see the we see the epochs going by and we see the batches going by okay and uh and we see uh you know we we actually have a training accuracy that's uh that gets to one very quickly because it's a large network and it overfits pretty quickly so let you know i've done this before in the interest of time and i saved my model here and i get to a validation accuracy of 98.13 it trends for a few minutes but i don't want to waste a few minutes um okay so this is what i get right i train for 50 epochs that fully connected network on my data set right and i get to the validation accuracy of 98.13 so is this good is this bad how do you know right sounds good okay 98 but you will not know until you try until you try reallife samples so i did what i had to do i took my paintbrush application and i i drew some digits right and you can see them here and we're going to try and predict them using the model that i just trained so i can load that model again okay which is pretty much as easy as this right load checkpoint with the name of the file storing all the weights okay and then i can predict my digits okay and predicting one of those images is very easy you load the png file uh from disk you turn it into a vector right you flatten the image into a vector and you push it through the train network okay there's nothing to be worried about that's really one line of code okay load the image put it in a vector push it through the trained model and then read the outputs okay so very simple to predict and when we do that and let me run those cells again i just want to make sure i have the right model here oops there we go run all the cells okay and so for each of those pictures we see a vector of 10 probabilities right this is exactly what we read on the output layer for the for the neural neural network okay so we see that zero worked out okay because actually uh probability zero is the highest 99 okay and for the one probability one is the highest etc etc so two is very good three is very good and uh you never really see zeros and ones it's because i stopped at four decimals so this really means it's at least 0.9999 something okay you will always see uh nonnull values here four is fine five is fine six good seven eight and nine this should be the highest right it's not this one is the highest so zero one two three four okay so my model is confused it thinks this is a four okay so that's what i told you you have to try different samples you know the validation data set is one thing but the test data set which should really be composed of real life samples stuff that people will really predict not just sample data sets this this is really what tells you how well your network performs so this is a disappointing right because i try 10 and one of them is wrong so why is that so you could say well this is really a very ugly nine so yeah you know you can't expect the model to predict ugly stuff like that okay fair enough but some people write really really bad uh and uh digits so you should still be able to figure it out but remember what we did to these images right we flattened them okay so intuitively it doesn't sound like a really great id because images are really 2d objects and we flatten them so we lose the relationship right the proximity between some of the pixels and probably that doesn't help understanding what's in those pictures so to solve this a different type of network was uh was invented 20 years ago already and they're called convolutional neural networks and these networks are the kings of image processing image classification okay um and the the basic idea here is we'll will work with the 2d image or maybe 3d if it's a color image right it's going to be 3d because it's red green blue okay but anyway we don't flatten anything we stick to uh to multidimensional objects and we use two operations we use convolution and we use pooling also called sub sampling okay so convolution is actually extracting features okay here's an example if you take this filter three by three and you slide it across the image right so nine pixels by nine pixels going all the way applying the convolution operation which is pretty much taking the underlying pixel here multiplying it by the corresponding value in the kernel okay and you do this nine times for the nine pixels here and you add everything together and that becomes the new top left pixel and then you slide and you do it again and again and again okay if you use this filter the specific values you go from this image to this image okay so this is really an edge detector filter and this what did we achieve here well we we detected the edges in the original picture and we threw away pretty much everything else everything else is black right we only kept the edges so we extracted the edge feature in this image okay and this is what convolution is about so by running many different filters okay it could be you know 20 30 50 filters you you extract different features from the image so it could be edges it could be contrast it could be vertical lines you know horizontal lines whatever you know and you get a combination a new set of uh convoluted images which you then shrink with pooling i'll show you how in a second and then you do that again okay and you get your collection of very tiny images that look nothing like the original image but they kept the good stuff they kept the features and the important information and then you can flatten this and use a fully connected network okay that's the the intuition between behind convolution networks you automatically extract features using a collection of filters and then you throw away you shrink the images you throw away the useless information in these pictures and you do that again and again and again and if you heard that deep learning extract features automatically this is it okay this is what people mean you use kernels or filters to extract important information and throw away the rest and obviously we have no idea what those values should be right here it's just a it's an example but initially the values for all the features generating those pictures are random so they are learned during the training process so the training process of a convolutional neural network is really about finding which filters extract the right features to help you understand those images right and it's the exact same process back propagation sgd et cetera et cetera okay so it gets a little more complicated but the overall picture is the same okay you learn the filters that lets you extract the right features okay and pooling is much simpler pooling is just about if we do a two by two pulling here we just uh take two by two pixel blocks and we keep the brightest right the brightest pixel the the highest value okay and if we do this all over the picture then you know we get a smaller picture and the intuition here is that in this image you don't care about the black stuff okay you want to throw that away it tells you nothing it was filtered out okay what's important is really the white pixels the edge of that weird beast okay so you can shrink the image by keeping the white pixels and it will gradually distort the image obviously but you uh you keep enough information to learn to classify them that's the id so let's really quickly try this so we're going to try to classify the same data set but this time with a convolution network and it's pretty much the one you saw on the slide right input layer convolution with 32 filters pooling 2 by 2 pulling convolution again with 64 filters pulling again 2x2 right so here from the initial image we built 32 and then from the 32 we built 64. okay and we shrink them so we end up having a large collection of tiny images and we flatten everything at the end so all those tiny images become one big vector and we use a fully connected layer to classify them into ten categories because once again zero to nine okay so you see this api is quite simple you can literally look at the at the network and encode it right just stack the layers and uh and there you go and the rest is the same right we bind we train so i train again in interest of time takes again two three minutes and we get to 99.15 accuracy this time okay so it's a bit higher than the previous one but now we're you know we're suspicious so we're gonna we're going to try that new model the one that we saved and we're going to predict again so 0 1 2 3 you know they all look very good six seven eight what about nine so is this the highest now yes it's the highest it's still not great right which is a tribute to my ugly nine here it's still not great but it's the highest okay so if i had to decide which thing is this then i would say okay it's a nine not super super sure about it but okay if i have to make a guess i'll tell you it's a nine okay and obviously you could keep tweaking this to get better results you could add have more filters more layers etc etc but again the intuition is really important okay that's why cnns work really well on on images it's because they keep the 2d or the 3d image and and they don't flatten stuff okay they just extract features from the actual image so it makes sense that they would be able to classify them better so you can do all kinds of crazy stuff with this and there's a an extra api in mxnet called gluon and specifically gluon cv which means computer vision that provides you with a collection of pretrained models okay and these are stateoftheart models with hundreds of layers they've been trained on really really big data sets and uh and you can do classification detection segmentation right these are just a few examples and any of those is literally five lines of code so i'm just going to show you one but you'll find everything on github let's look maybe at uh yeah let's look at uh okay classification okay so how complicated is it to do this so uh we're going to call this api to load a pretrained model okay so it's called model zoo so we fetch a pretrained model from the web already trained no training needed we load a local image we're going to normalize it okay normalization is important so make sure the you know the red green and blue channels etc are normalized and then we predict it so we take that image push it through the train model and then we can read the results and that's it okay so training your own model uh is fine and we could we could run one of those yeah let's try uh okay let's try this one okay let's try to segment this here we go okay okay so that's the original picture and i'm running it on my mac so no gpu so it takes a takes a few seconds but here you take an existing model pretrained and and you just get the job done right so of course you could train that model on your own data set right there you go five six seconds no gpu so on a gpu it would be just like this um and this really takes five lines of code okay so before you go and try to build everything from scratch just consider existing models pretrained models that that might be good enough to do the job right and you would save an insane amount of time by using those pretrained models okay very last thing i want to talk about is we looked at fully connected networks and we looked at convolution networks okay and if we predicted with those if we took 100 samples and predicted them in any order we would still get the same results okay sequence doesn't matter which is a problem because when you want to work with time series when you want to work with sequences of data the order of the samples does matter right when we're translating for example we don't translate each word independently we look at the previous words to uh to actually uh get some context so we need some kind of memory shortterm memory on the past few predictions to make the right prediction and this is what lstm networks are all about so they have a different type of neuron which i will not explain i don't have time and it's a it's a little more involved but you just need to remember that a prediction done by an lstm neuron depends on the input and it depends also on the past few predictions so they have some kind of memory right that's why they're called shortterm memory networks and this is very good if you want to predict sequences right so time series machine translation bitcoin prices anything you want and uh talking about machine translation uh we have an open source project called sockeye that you'll find on github that uses an lstm architecture to to let you train machine learning models for machine translation okay and uh it's super simple to use and maybe we can do a really quick demo if i still have a come on i still have an ssh connection here so i took a data set for uh german to english and no oh yeah slow connection but we'll find okay so i have a few million sentences in english and in german and english and i trained a model from german to english it took a few hours on a couple of gpus so i could take some german sentences here and i could run them through that and i have just one for you here hopefully that works yeah okay see so if you want to build your own translation model just go grab a sockeye train for a few hours if you have the data set it's not complicated it's a preexisting architecture and you can just do it like that okay so you can build your own translation service just like that okay all right i'm out of time so okay i'll take 30 seconds for the crazy stuff because i hate not talking about it and then i'm done so the last type of architecture i want to talk about is called gans generative adversarial networks and they they are weird right so who knows who these people are i know you're working hard but you gotta watch movies right come on tv or something no no one no one knows i i had nothing to win but okay if i had something to win i would give it away no no that's your last word okay now i know you're just very suspicious now right of this frenchman showing you weird stuff okay you're right to be suspicious because these people do not exist right they are generated faces and they're not copy paste right so it's not the nose of brad pitt and the ears of johnny depp or something like that it's really generated pixel by pixel by looking at a large number of images in in the celebrity data set that's why they end up being quite good looking the network learns to generate similar samples okay but these are completely fake right here's another example you remember that right when you were fiveyearold or if you have kids you have stuff like this on your fridge right i'm sure i do so these are cars right this is the road these are trees etc and this is called a semantic map okay and again you can by training a neural network on images and the corresponding semantic map you can then predict this semantic map okay so you give this to the model and the model will generate the actual picture okay and this is hd right now these are hd pictures so you can zoom in and you can see a lot of detail actually right these are really really precise okay so this and this gun area of research is moving all the time it's it's really crazy and and if you're interested you know i would suggest you read about that you'll see some even crazier applications of guns right generating a new reality right so um how do you get started with all of this um well i would recommend obviously taking a look at uh the the adobe's website where you will find information on all the tools and all the libraries that our customers use to build a pretty cool stuff okay the blog has some technical articles etc if you're curious about mxnet and gluon it's all on github right so just go and grab the sources grab the tutorials there's lots of documentation for it and you can replay a lot of those examples if you uh if you're already doing machine learning in production and you have scaling problems right we have a service called sagemaker that lets you train pretty much anything tensorflow mxnet pi torch scikitlearn we have some builtin algos as well to save you more time so if you just want to focus on machine learning and not on infrastructure i would recommend taking a look at sagemaker we have sdks and we have a very very cool integration with spark if you use spark sagemaker makes a lot of sense as well and finally uh here's my blog on medium where you'll find lots of articles on machine learning deep learning sage maker and more crazy stuff i have a by now i've got quite a collection of talks on youtube as well so if you want to dive deeper on on those topics or or explore what aws has to offer you'll find something here and the code that i use today and much more is available on gitlab so just go and grab that and you can run these examples and and more all right okay i'm done thank you very much if you want to stay in touch thank you
