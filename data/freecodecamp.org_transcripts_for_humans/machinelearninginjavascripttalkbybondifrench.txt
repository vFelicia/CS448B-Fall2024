With timestamps:

00:00 - so
00:01 - this
00:02 - talk being just half an hour so i am
00:04 - repeating myself for those who have been
00:06 - here a bit before
00:08 - is obviously uh
00:10 - it's a high level presentation about
00:13 - what
00:14 - you can do with javascript in and
00:16 - machine learning
00:18 - and what libraries are out there and
00:20 - what you know
00:22 - current libraries are trying to overcome
00:26 - so
00:27 - there are a lot of things that you can
00:29 - do
00:30 - in javascript right now
00:32 - with pure javascript
00:34 - first you can do a cat recognition
00:37 - for those who like
00:39 - cats so i'm just showing you
00:42 - up
00:43 - hopefully it will work
00:46 - if it does
00:49 - sorry
00:50 - i need
00:51 - ah here we go and oh it found a cat in
00:54 - the middle of the picture i don't know
00:56 - if you
00:58 - there we go
01:00 - so
01:02 - you can do a digital recognition which
01:04 - is a classic uh
01:06 - machine learning
01:08 - example so here i'm gonna draw a seven
01:11 - very roughly
01:13 - recognize oh it's a
01:15 - nice
01:18 - up
01:19 - i recognize here it's rubbish
01:24 - it's very rough
01:25 - obviously my handwriting is however
01:28 - but
01:29 - yeah
01:31 - other interesting applications um
01:34 - so self-driving car simulations
01:36 - obviously
01:37 - you know tesla being in the media and
01:39 - stuff but there is a
01:41 - the mit did a
01:44 - a course on self-driving gas and
01:49 - and so there was
01:51 - several in tesla and to simulate how
01:54 - you know you need to recognize or adjust
01:56 - the parameters
01:58 - they they've done this simulation which
02:01 - is on top of a famous
02:03 - uh javascript framework called confnet
02:05 - gs and basically you can
02:08 - model your red car and see how it goes
02:11 - and how fast you can go
02:14 - so
02:15 - in my presentation i have all the links
02:17 - that you need if you want to experience
02:19 - these
02:20 - libraries
02:22 - image manipulation
02:24 - [Music]
02:26 - so
02:27 - to file uh here lighthouse
02:32 - and you can see how far how fast it is
02:35 - to actually manipulate the data so
02:38 - grayscale the picture
02:40 - uh draw the edges
02:42 - flip the
02:43 - picture or resize it zoom it
02:46 - actually zoomed and flipped actually
02:49 - a bit reverse but
02:51 - [Music]
02:53 - what as you can do
02:56 - you know for developers that's
02:58 - interesting the automatic logo generator
03:01 - so i'm gonna try to do it
03:05 - live so
03:09 - camgs and if you go down
03:13 - you'll see they it generates different
03:16 - types of logos like you know
03:20 - just to improve your creativity or
03:23 - add you know arrows to your
03:26 - creativity
03:27 - uh the other thing you
03:30 - machine learning you not only work with
03:33 - numbers you can work with words as well
03:36 - so classif
03:38 - example are spam filters
03:40 - um
03:41 - and uh yeah email
03:43 - filtering um or you know when you work
03:48 - when you have a salary on your iphone or
03:51 - or
03:51 - search boxes it's all about work
03:54 - processing so it's called natural
03:56 - language processing
03:59 - and there is also this uh project that i
04:02 - found on and
04:03 - on github recently it's a ai for robots
04:06 - so for those um i think jessica you
04:09 - talked about
04:11 - sensor
04:12 - and
04:13 - anna was talking about arduinos so this
04:16 - combines
04:17 - you know artificial intelligence and
04:19 - arduino and robots so you can have fun
04:23 - with everything and all in node.js and
04:26 - node
04:30 - very high level uh i'll try to expose
04:33 - the key concepts of machine learning so
04:36 - we oppose supervised to unsupervised
04:41 - machine learning so supervisors when you
04:44 - you have a specific set of inputs
04:47 - and you know uh your output you know
04:51 - uh what to expect at the end of the day
04:53 - so you're trying to find a function if
04:56 - you want that will map all the
04:59 - x variables to the targeted output y
05:03 - so
05:04 - typical uh
05:06 - there are two types like classification
05:08 - when it's a discrete set of
05:11 - outputs so
05:13 - if you are
05:15 - you know
05:16 - let's say you have
05:17 - a set of colors so you categorize red or
05:20 - blue or your emails if it's spam or not
05:24 - spam it's
05:26 - classification problems
05:28 - then you have regular regression
05:30 - problems so regression is when the
05:32 - output variable is continuous
05:35 - so it can be in dollars or weights for
05:37 - instance if you think of
05:39 - let's say you wanted to do a study on
05:45 - property prices and you would think of
05:48 - well what's what's the location what's
05:51 - the square footage whole number of rooms
05:54 - uh
05:55 - you know does it have a garage a
05:57 - swimming pool all those features and you
06:00 - try to have at the end of the day how
06:02 - much your property would be worth so
06:04 - that's kind of regression uh problem so
06:08 - examples of algorithms that support that
06:11 - it's a linear regression
06:13 - uh support vector machines and random
06:15 - force um
06:17 - random force is like a tree and
06:20 - basically
06:21 - you do multiple trees randomly you set
06:24 - up the parameters randomly so that's why
06:26 - we call it random forest
06:30 - unsupervised learning is when you don't
06:32 - actually know the output or you want
06:36 - actually to discover
06:38 - the different groupings and the
06:40 - different classifications
06:42 - out of the blue so
06:44 - you have clustering problems and
06:46 - association problems so clustering
06:49 - um
06:51 - given a certain set of data
06:56 - it's used for instance in
06:58 - in the medical world
07:01 - like if you have a
07:03 - a certain set of features uh
07:06 - your sex your age
07:09 - your
07:10 - your health habits uh uh how
07:13 - uh
07:14 - how
07:14 - and
07:16 - uh or
07:18 - in biology in general like if you have a
07:20 - certain set of features uh and you group
07:23 - them what type of
07:24 - disease or what type of plants you come
07:28 - to
07:28 - associations is to discover the rules
07:31 - among this data so really at the high
07:34 - level it's supervised versus
07:36 - unsupervised
07:38 - so that's a classic um
07:41 - description of
07:45 - neural network which is the one of the
07:47 - main
07:48 - algorithm
07:50 - in machine learning so you have a set of
07:52 - inputs
07:53 - so as i said before for instance in the
07:57 - real estate example
07:59 - the
08:00 - square footage the number of rooms sets
08:03 - your input layer so you have a number
08:06 - the number of inputs are the number of
08:08 - features
08:09 - and the output so here you have two
08:11 - different outputs it could be a
08:14 - spam not spam or
08:17 - it could be a just one output but a
08:20 - price uh you have an output layer and in
08:22 - between the output and out
08:25 - sorry in between the input and the
08:27 - output layers you can have hidden layers
08:30 - and there can be uh
08:32 - uh
08:33 - like it can be from one to
08:35 - n number of
08:37 - uh
08:38 - of layers depending on the conclusion
08:40 - complexity and here it is
08:42 - where it's more an art than a science
08:45 - to decide which number of hidden layers
08:49 - are in a network
08:52 - but
08:53 - this might not appeal to everybody in
08:55 - terms of
08:56 - uh understanding of a neural network i
08:59 - find sometimes it's easier to have this
09:01 - kind of representation like a typical
09:04 - tabula or presentation so
09:07 - the features are um you know from
09:11 - you can see in terms of widths of the
09:13 - data set and the
09:15 - number of examples are
09:18 - like the height of x in that example and
09:22 - so it's uh
09:23 - obviously it's it looks like a matrix
09:26 - and you try to arrive
09:29 - at the
09:30 - for each example you know
09:33 - uh which output you're gonna get so it's
09:36 - you try to approximate the y the column
09:40 - y that you have to next to the matrix
09:43 - so you're trying to find out the uh the
09:46 - function or the
09:48 - the weights that you apply to the your
09:51 - data set the the x
09:53 - and
09:54 - this approximation gives you a
09:56 - it's called a y-hat which is
09:59 - um
10:01 - you know what outputs given a current
10:03 - model
10:04 - so
10:05 - uh
10:06 - in in mathematical terms it would be
10:08 - just it's f
10:10 - of x gives you a y and you try to
10:13 - minimize y hat compared to hawaii i
10:17 - don't know if it makes sense to
10:18 - everybody
10:21 - yes
10:24 - this one just looks easier for you yeah
10:27 - i don't know
10:29 - [Laughter]
10:32 - it's uh i say maybe the second
10:34 - representation makes you
10:36 - more i understand that you know you you
10:38 - have all this
10:40 - number of examples the output that you
10:42 - expect the why and we are trying the
10:45 - game is really to try to minimize
10:48 - uh why and why hearts okay and we are
10:51 - and to
10:53 - basically to do that you adjust you you
10:55 - you adjust the w which are the weights
10:58 - that you apply to the number of features
11:02 - y hat it's basically the result of your
11:05 - model
11:06 - of w times the x if you want the
11:09 - different weights that you apply to your
11:11 - data set
11:14 - so
11:16 - basically
11:18 - this
11:20 - thing
11:21 - called perceptron is just one part of
11:26 - let's say
11:27 - what i showed in the picture of the
11:29 - person from just this part so you have a
11:31 - different number of inputs your plan
11:33 - number of whites in the middle and you
11:35 - activate them and you have one neuron in
11:39 - your network
11:41 - so the in terms of calculations if you
11:44 - apply
11:45 - to your inputs uh
11:47 - different weights
11:49 - you sum them
11:50 - like in a multi
11:52 - linear regression and you activate them
11:54 - making them
11:56 - like you know a binary is it zero or one
12:01 - do you accept it in your model or not
12:04 - and so this perceptron is
12:07 - or all the hidden layers and the output
12:09 - layers are calculated in that manner you
12:13 - apply a different set of weights between
12:16 - the input and the hidden layers and a
12:18 - different set of weights between hidden
12:20 - layer and output layer and you at the
12:23 - end of the day you try to compare the
12:25 - output layer to
12:27 - the calculations from your weights and
12:30 - the different layers to the output layer
12:33 - and you try to minimize
12:35 - uh this error
12:37 - between what you've calculated and what
12:39 - is actual
12:42 - does it make sense
12:44 - it makes sense
12:47 - so
12:47 - another key concept
12:49 - and it was discovered relatively
12:52 - recently recently in that case means in
12:54 - the last 15 years
12:56 - is
12:58 - the concept of back propagation so
13:01 - you can do this calculations about
13:03 - weights in one way
13:05 - but really
13:06 - uh for it to improve and so it's that
13:10 - the network learns you have this concept
13:13 - of back propagation so the you you back
13:16 - up the errors that you found out at the
13:18 - way that you calculate at the end of the
13:20 - day between the
13:21 - um the calculated output and the real
13:24 - output you try to
13:26 - integrate it backwards into the weights
13:30 - um into the weights that are between the
13:32 - hidden and output layer and between the
13:34 - input and hidden layer
13:36 - so
13:37 - how
13:38 - um so you re-integrate the error
13:41 - and it's a iterative process until you
13:45 - your error is close to zero or as close
13:48 - to zero as possible
13:50 - and that can be done
13:52 - basically
13:53 - uh thanks to um
13:56 - because we all know the properties
13:58 - of the activation functions
14:01 - so
14:02 - this is possible to to go backwards or
14:05 - to back propagate your error
14:07 - thanks to the derivatives so activation
14:11 - these are examples of
14:13 - activation functions so
14:16 - i
14:17 - really you have to think of activation
14:20 - functions as
14:21 - trying to squeeze a number
14:24 - here in this case for the logistic
14:26 - functions between 0 and 1 you know that
14:29 - everything between infinity minus
14:32 - infinity plus infinity will be squeezed
14:34 - between
14:35 - 0 and 1
14:39 - i don't know how you say it in english
14:42 - sorry just ten times
14:45 - ten h okay it's you squeeze every number
14:48 - from minus infinity to plus infinity
14:50 - between minus one and plus one and we
14:54 - can back propagate uh and do this
14:57 - inaudible process because actually
15:00 - mathematically we know what the
15:02 - derivative of this um
15:05 - of these different functions are
15:07 - and i don't know if you have
15:08 - recollection from
15:12 - you are doing linear algebra and maybe
15:14 - in high school or in college when you
15:17 - want so most cost functions have a
15:20 - convex feature okay and so um
15:24 - you know what convex is like it looks
15:27 - like like a u basically and when you
15:29 - want to minimize so to go to the bottom
15:33 - to find the local minimum on a
15:36 - on a convex function you use the
15:38 - derivative and try to go to the zero
15:41 - basically so you use a slope
15:44 - of the curve in in the convex function
15:47 - to try to reach the minimum as a local
15:50 - minimum
15:51 - so that's that's how we achieve back
15:53 - propagation and that's how we can
15:56 - achieve the whole
15:58 - iterative process of improvement so you
16:01 - calculate a certain output you calculate
16:04 - the cost error and back propagate it
16:06 - into the weights and you
16:09 - and then you there is a learning rate
16:11 - which makes it improve every time
16:14 - so these are the classic uh cost
16:16 - functions the most i guess the most
16:19 - known one in terms of statistics is a
16:21 - mean square error
16:24 - but you have a number of different types
16:26 - of
16:28 - squares
16:29 - okay
16:30 - enough math for today
16:33 - though
16:36 - so obviously
16:37 - all these calculations uh you know at
16:40 - the end of the day
16:42 - you need performance from your computer
16:44 - to um to realize
16:46 - this
16:47 - and
16:48 - the browser obviously is not the
16:51 - uh
16:52 - the lowest level you can do your
16:54 - calculations and most libraries would be
16:57 - in uh c plus plus i don't think they're
16:59 - in any assembly to my knowledge but they
17:02 - would be in
17:03 - in a very in much more low level
17:04 - language however
17:08 - when you handle calculations it's often
17:10 - done in python and python is just a
17:12 - wrapper around these c plus plus
17:14 - libraries
17:16 - [Music]
17:17 - that being said if we are just doing uh
17:21 - python versus javascript performance
17:23 - comparisons uh most of the time
17:25 - javascript with the improvements of the
17:28 - v8 engine and you know the different
17:31 - browser engines it's actually uh faster
17:34 - than python
17:35 - uh it's just that when you do
17:38 - uh deep uh learning
17:40 - networks that uh when you you do like
17:44 - image recognition and not just of cats
17:46 - because here it was a bit cheating the
17:49 - cat was recognized because we could uh
17:51 - find
17:52 - two ears like you know the the sign of
17:55 - so that's it was a shortcut but if you
17:57 - want to do it at a production level like
18:00 - you know at google size you need really
18:03 - like
18:05 - a lot of gpus to do that and why gpu is
18:09 - because
18:11 - coming from the gaming world
18:13 - that's where most calculations for
18:15 - refreshing screens come from and so they
18:18 - are very much used to
18:20 - all the vectors and matrix calculations
18:22 - that are actually
18:24 - used in machine learning so
18:26 - the leader in this in that field is
18:29 - nvidia or when you have nvidia
18:33 - a gpu cat
18:35 - they have actually also created
18:38 - libraries that enable you advanced
18:40 - mathematical operations on it so
18:44 - in the javascript world we are used to
18:47 - the simplest way to do that is array of
18:49 - arrays but it's actually not the most
18:54 - performing a way to do that uh for
18:57 - accessing a particular item you do what
19:01 - you know given a matrix a an array of
19:04 - arrays you you access your
19:07 - elements by you know i mean a i
19:11 - j which you know i'm sure everybody
19:14 - would be able to do but it's not if you
19:17 - want to iterate over that or access or
19:20 - transpose or
19:22 - in this metrics
19:24 - it it won't be very fast one uh faster
19:28 - way uh to do that is uh to use typed
19:31 - arrays
19:32 - which is my second example and
19:37 - it's it's faster but the thing is here
19:39 - the you you can see
19:41 - this so this is just the same matrix as
19:45 - this one but it's just
19:46 - flat it has been flattened and here the
19:50 - index is um so as you can see it's a
19:54 - three
19:54 - rows and three columns matrix and here
19:57 - you can see that the
19:59 - index has been fixed
20:01 - so one
20:03 - so it's very fast using typed arrays but
20:07 - to access
20:08 - the
20:09 - the different elements when you want to
20:11 - do uh
20:12 - higher order operations
20:14 - is is a bit cumbersome so there is a
20:16 - compromise where this index the three in
20:19 - my example here is actually um
20:22 - it's
20:24 - sorry
20:26 - it's a general linear function so here
20:31 - this
20:32 - formula basically becomes to that so
20:36 - so here you can
20:37 - really switch rows and columns very
20:40 - quickly you can transpose numbers in
20:42 - your matrix much quicker
20:44 - and there's been a library that has
20:47 - used this idea of strided arrays
20:51 - and which is ndrs
20:54 - it's used in a number of
20:57 - [Music]
20:58 - low-level and high-level
21:02 - libraries
21:04 - numgs which is a kind of numpy for those
21:08 - who are familiar with python that tries
21:11 - to
21:12 - use the same kind of
21:14 - functions that
21:16 - numpy or matlab have has
21:19 - keras
21:21 - takes the name of
21:22 - of a famous uh
21:24 - library which is a
21:27 - higher
21:29 - [Music]
21:31 - order library a higher level library on
21:34 - top of tensorflow or
21:37 - tiano which are more lower level or
21:40 - matrix calculations so keras it would be
21:42 - easier to do
21:45 - artificial neural network a
21:48 - recurrent neural network
21:51 - it's it's good for prototyping
21:54 - machine learning models the only thing
21:58 - the keras javascript model you can only
22:02 - in a way do
22:03 - a forward propagation you can only do
22:06 - what's called inference and not back
22:08 - propagation so the training of your
22:09 - models won't be done in javascript it
22:12 - would be done
22:13 - in
22:14 - in python and then once you have your
22:17 - model
22:19 - you use javascript to visualize these
22:21 - things so tensorfire is the latest
22:25 - library
22:27 - it uses ndis but it also uses webgl
22:31 - under the hood
22:34 - so
22:35 - other ways to do calculations in on the
22:38 - cpu is using uh web assembly there are
22:41 - fewer examples of that and they are less
22:44 - mature
22:46 - but it's certainly
22:48 - avenues to explore to
22:50 - improve the speed of calculations on the
22:52 - cpu and also to increase the
22:56 - the parallelism of calculations in
22:59 - browsers you
23:00 - you can use service workers
23:03 - that being said there is a cost of
23:04 - transferring your data from
23:08 - your browser thread to the service
23:10 - workers
23:11 - and back
23:12 - so there will be a slight delay compared
23:15 - to pure
23:17 - cpu
23:18 - so as i mentioned
23:21 - you can do calculations leveraging your
23:24 - cpu or you can leverage
23:26 - uh webgl
23:28 - so a number of
23:30 - uh
23:32 - of libraries have emerged
23:34 - and
23:35 - the latest one being tensorfire which
23:38 - was i saw that on twitter like uh two
23:40 - weeks ago so it's really recent but um
23:44 - they haven't
23:45 - for the moment open source their code so
23:47 - as i said i found the
23:49 - npm library but i couldn't find a github
23:52 - basically
23:54 - it
23:54 - leverages
23:56 - jrsl so the the
23:59 - language that is used with webgl
24:02 - you can use also uh leverage uh your gpu
24:06 - differently using opencl or
24:10 - web gpu but then
24:12 - i guess it's not uh it will be a
24:14 - different setup it will be using a node
24:17 - rather than purely the browser and
24:20 - usually it's
24:22 - you can it's restricted to a certain
24:24 - number of browsers it's not cross
24:26 - browser compatible
24:30 - right um so that was my high level uh
24:35 - in terms of concepts from machine
24:38 - learning
24:39 - uh
24:40 - i wanted to export a
24:43 - number of
24:44 - blogs and
24:47 - libraries that you can use
24:51 - in javascript or courses if you are
24:53 - interested in the area
24:56 - so
24:57 - this bright camber did an old blog
25:01 - in machine learning addressing not
25:03 - neural network but a lot of
25:06 - unsupervised learning algorithm and the
25:10 - second one is
25:11 - that i mentioned here andrej capaci is
25:15 - is quite interesting because
25:17 - this guy is
25:19 - relatively famous he developed one of
25:22 - the first deep learning library in
25:23 - javascript called confnet
25:25 - gs to do convolutional
25:28 - neural networks on pictures
25:31 - uh and he's uh he used to work for so
25:34 - he's from stanford and he used to do
25:37 - uh work for open li and recently he has
25:40 - been he has been hired by tesla to
25:43 - improve machine learning for cars
25:47 - so there are a number of
25:50 - javascript specific
25:52 - resources
25:54 - so
25:55 - content us unfortunately is no longer
25:57 - maintained tensorflow
25:59 - you have a
26:00 - nice
26:01 - tensorflow playground to play with which
26:04 - is here you can add a number of hidden
26:08 - layers
26:09 - different features
26:11 - and you can
26:14 - so it's nice to play around in terms of
26:17 - what you can achieve and how long it
26:20 - takes to
26:22 - to improve your model um
26:24 - [Music]
26:26 - so it's tensorflow playground so it's
26:29 - it's a really it's written in typescript
26:31 - and
26:33 - uh
26:34 - it's only a very small set of the
26:36 - tensorflow library
26:38 - do you know actually
26:40 - do do people understand what a tensor is
26:43 - by any chance
26:45 - no okay
26:46 - so
26:48 - you should think of tensor as a
26:50 - container
26:52 - like a a number a scalar would be a
26:55 - tensor of of size zero
26:58 - a vector
26:59 - so i'm gonna go to
27:01 - [Music]
27:04 - so a vector would be a tensor of
27:07 - one dimension
27:10 - a matrix
27:11 - with rows and columns would be a tensor
27:14 - of two dimensions
27:16 - and a cube
27:18 - so three dimensions would be a
27:19 - tensorflow a tensor of three dimensions
27:22 - so it's you have to think of it as a
27:24 - container
27:29 - so libraries uh
27:31 - and
27:32 - if
27:33 - if you are more interested so
27:35 - you can get involved
27:37 - at different levels maybe some like to
27:40 - go into the
27:42 - lower level so the matrix and vector
27:44 - calculations
27:45 - uh or you can
27:47 - go higher level and talk about different
27:49 - types of architecture and
27:53 - convert yes
27:54 - and more recently this one knit optic
27:59 - which tries to do neural evolution so
28:02 - neural evolution for those who don't
28:03 - know it's it's kind of uh
28:07 - it tries to
28:09 - it's a gene kind of genetic algorithm
28:12 - which means that they
28:14 - let's say
28:15 - you can't to randomly generate a number
28:18 - of uh
28:20 - of examples and basically you you try to
28:24 - the you try to eliminate those who
28:27 - don't work as much so it's it's a very
28:29 - darwinian process in a way you generate
28:31 - a number of
28:32 - of examples you you keep the ones who
28:36 - survive the longest then you regenerate
28:39 - a number of set of examples and and and
28:42 - so forth back in so
28:44 - um
28:45 - [Music]
28:49 - so
28:51 - there has been a number of um
28:54 - universities uh involved in uh
28:58 - to do machine learning in javascript so
29:01 - one
29:02 - would i would say is more
29:05 - on the edges
29:07 - from the university of tokyo maldives
29:11 - they do
29:12 - libraries which are
29:14 - relatively
29:15 - very japanese friendly like sushi too
29:18 - tempura
29:19 - sukiyaki
29:20 - but they have like sushi too is for
29:22 - instance for matrix calculation sukiyaki
29:25 - is for on top of sushi 2 to do different
29:29 - machine learning models
29:31 - they do
29:33 - distributed computing so they have
29:35 - created a framework where you have
29:37 - different browsers and basically you
29:39 - leverage the capabilities from you know
29:42 - maybe using web sockets so you can
29:44 - connect different
29:46 - uh computers different browsers and you
29:49 - leverage
29:50 - the cpus of these different browsers
29:54 - you have two one swiss university which
29:58 - the other two universities are
30:00 - maybe less known or
30:02 - less marketed in a way
30:06 - i had really i learned just by chance
30:09 - that they
30:10 - existed and they have actually quite a
30:12 - few uh
30:13 - libraries uh on npm on github um
30:18 - the last one actually doesn't have a
30:23 - npm but it does have a github i asked
30:26 - them if you wanted to
30:28 - leverage npm but the guy didn't see the
30:30 - point of using uh
30:32 - javascript who is not yes i don't know
30:34 - that's his point but they have a nice
30:38 - online uh laboratory where you can
30:41 - actually
30:42 - download
30:45 - some files and you can
30:48 - you know use the most common machine
30:50 - learning uh libraries on it to you know
30:53 - try to um
30:55 - create a model
30:57 - um coursera i personally so i'm not an
31:00 - expert in the field i uh i'm still doing
31:03 - it actually um
31:05 - i recommend the corsair course from uh
31:08 - from underneath who is a
31:10 - uh who used to work at google um
31:14 - and
31:15 - and baidu he was a chief um
31:19 - machine learning or chief artificial
31:21 - intelligence guy from baidu
31:23 - the chinese
31:25 - company
31:27 - jeffrey hinton
31:29 - saw his course on our networks in corsa
31:32 - jeffrey hinton is one of the
31:35 - leaders in machine learning and it's
31:38 - it's quite
31:39 - unique that you can find his you know
31:42 - course from him
31:44 - completely free on conserva udacity
31:47 - proposes a number of
31:51 - of
31:52 - nanodegrees in the field
31:54 - the example that i showed you on deep
31:57 - traffic the
31:59 - mit course a lot of people from the
32:01 - self-driving engineer uh
32:04 - were involved in that so
32:07 - i don't oh i don't have network so
32:10 - unfortunately on behavior but basically
32:12 - they participated in in this
32:16 - in this setup as well
32:18 - there are lots of resources on the
32:20 - internet
32:21 - uh
32:22 - blogs uh which are not
32:25 - most of the time not javascript specific
32:27 - the javascript ones are the
32:29 - these ones that i mentioned
32:33 - yes
32:34 - so
32:35 - you can get involved also to encourage
32:38 - getting involved there is a ai grant
32:41 - where for whatever projects and
32:44 - tensorfire the latest library that i
32:46 - mentioned
32:47 - they participated in the ai grant
32:51 - competition and they won 5 000 from that
32:54 - to to when they created tensorflow or
32:57 - kaggle kaggle actually was co-founded by
33:01 - an australian
33:02 - guy and it's a
33:04 - it's a website where people
33:07 - can do machine learning competitions and
33:09 - and there are open data sets and there
33:12 - is a forum where you share your tips and
33:14 - tricks to improve the accuracy of your
33:16 - models
33:18 - sorry there was very fast and uh
33:22 - you know very involved but if you have
33:23 - any questions happy
33:30 - sorry
33:31 - for all the links yes sure uh yeah i
33:34 - will put this yes
33:38 - graphical forum
33:40 - machine learning
33:41 - activity
33:43 - effect
33:45 - screen rendering is it going to
33:48 - uh if you've got an app that
33:51 - yeah so turn safari i think you can try
33:54 - the uh the demos that they have on your
33:56 - mobile
33:58 - uh i personally don't know
34:00 - uh you know the performance uh
34:03 - comparisons for that
34:05 - i the
34:06 - [Music]
34:09 - one of the thing is
34:10 - i guess the calculations would be
34:15 - i mean i don't i don't think your
34:17 - gpu on the mobile is in terms of power
34:21 - it's it's great that great
34:24 - so i don't think you know um i mean it
34:26 - will improve a bit in a way because it's
34:28 - paralyzed so it's uh you know
34:31 - in parallel to your cpu but in terms of
34:34 - pure raw performance i don't think it's
34:36 - right
34:37 - that being said i know that
34:40 - google with tensorflow has tried to you
34:43 - know
34:45 - i mean
34:47 - democratize the use of its library
34:51 - and so they encourage it and they do
34:53 - android apps on that but i personally
34:56 - haven't tested it so
35:04 - [Laughter]
35:13 - so the whites you have to think it's the
35:15 - same set of weights that you apply to
35:18 - the
35:19 - examples okay so you will try to follow
35:21 - them together very quickly um you can
35:24 - vary
35:26 - yeah so
35:28 - obviously it's different set of inputs
35:31 - but basically it's m and you can see
35:33 - it's
35:34 - a number of features any number of
35:37 - features you multiply each feature but
35:39 - it's white
35:42 - okay
35:47 - so first it's randomly generated
35:50 - and then so
35:52 - so first you randomly generate
35:54 - these weights
35:56 - so then you do the
35:57 - calculations so the
36:00 - like the matrix calculations okay
36:02 - you activate them using one of the
36:04 - functions that i showed you before
36:06 - and
36:07 - um you do that for every neuron every
36:11 - layer and gives you y hat the what comes
36:15 - from your weights okay
36:17 - so then you try to calculate
36:21 - costs you have a cost measure so the
36:23 - most famous one is a mean square arrow
36:26 - of
36:26 - comparing
36:28 - it's basically
36:30 - y y minus y hat square divided by the
36:34 - number of examples
36:36 - okay so once you have that measure of
36:39 - error okay you you back propagate
36:42 - this error into
36:45 - thanks to the derivatives of the active
36:48 - er evaluation functions
36:50 - back to the initial
36:52 - input okay so it changed and
36:55 - and then it's an iterative process
36:57 - thanks to what's called the learning
36:58 - rate where you add that at the end of
37:01 - the day you will
37:03 - me you will diminish the error
37:06 - that you calculated
37:16 - a numerator example or coding example if
37:19 - you want on a
37:35 - like the number of uh
37:37 - i suppose dimensions that you can work
37:39 - on the problem is it just for like a two
37:41 - dimension problem and then anything
37:43 - after that
37:45 - the error rate
37:46 - you know just can't really model it very
37:48 - well
37:49 - uh
37:50 - [Music]
37:53 - so actually the fact that you have
37:55 - hidden layers and
37:57 - and you have
37:59 - so it improves uh
38:01 - if you want the non-linearity of your
38:04 - form so let me show you what i mean so
38:07 - you're thinking of
38:09 - giving us a set like that
38:11 - you can see it's it's not clear if you
38:13 - were just doing linear regression it
38:15 - would be a straight line okay
38:17 - so if you were doing just input outputs
38:20 - that's what you would expect it's a it's
38:22 - a linear regression but the fact that
38:24 - you implement a hidden layer in the
38:27 - middle
38:28 - give
38:29 - improves the non-linearity of your uh
38:33 - of your model if you want so
38:35 - uh i i've done that example in in
38:38 - javascript so
38:40 - just logistic regression and just input
38:43 - output without hidden layers would give
38:45 - you a straight line okay but if you had
38:48 - a
38:49 - hidden layer and and i said i mean here
38:52 - the color is in titan but i've done the
38:54 - thing
38:55 - in javascript
38:56 - you
38:57 - the fact that you
38:59 - a hidden layer they will find
39:02 - you know
39:03 - here so you can see here this invariable
39:06 - for hidden layer size three you can see
39:08 - you you add a non-linearity to your
39:11 - problem so and the more you add
39:14 - basically the more precise in your
39:16 - non-linearity you can go
39:19 - okay so then it's a complex then there
39:22 - is i didn't want to go over that but you
39:25 - can overfit your data so then in terms
39:28 - of
39:29 - uh
39:30 - if you want to predict you know a cert
39:33 - with your model
39:35 - you have to be uh very careful that it
39:38 - doesn't offer that your model doesn't
39:40 - overfit your
39:41 - test data but if you do that what's the
39:44 - cost in terms of processing oh obviously
39:47 - that's
39:48 - numerical complexities
39:51 - i mean
39:55 - i i i don't have the exact answer but
39:57 - you know it's it's adding a role
40:00 - it's basically adding
40:02 - uh a few mattresses to your
40:05 - uh
40:06 - to your calculations so in terms of
40:09 - i know computer scientists calculate in
40:11 - terms of or something
40:14 - it would be at least
40:16 - i mean square
40:18 - added square capacity
40:24 - any
40:26 - other question
40:44 - you

Cleaned transcript:

so this talk being just half an hour so i am repeating myself for those who have been here a bit before is obviously uh it's a high level presentation about what you can do with javascript in and machine learning and what libraries are out there and what you know current libraries are trying to overcome so there are a lot of things that you can do in javascript right now with pure javascript first you can do a cat recognition for those who like cats so i'm just showing you up hopefully it will work if it does sorry i need ah here we go and oh it found a cat in the middle of the picture i don't know if you there we go so you can do a digital recognition which is a classic uh machine learning example so here i'm gonna draw a seven very roughly recognize oh it's a nice up i recognize here it's rubbish it's very rough obviously my handwriting is however but yeah other interesting applications um so selfdriving car simulations obviously you know tesla being in the media and stuff but there is a the mit did a a course on selfdriving gas and and so there was several in tesla and to simulate how you know you need to recognize or adjust the parameters they they've done this simulation which is on top of a famous uh javascript framework called confnet gs and basically you can model your red car and see how it goes and how fast you can go so in my presentation i have all the links that you need if you want to experience these libraries image manipulation so to file uh here lighthouse and you can see how far how fast it is to actually manipulate the data so grayscale the picture uh draw the edges flip the picture or resize it zoom it actually zoomed and flipped actually a bit reverse but what as you can do you know for developers that's interesting the automatic logo generator so i'm gonna try to do it live so camgs and if you go down you'll see they it generates different types of logos like you know just to improve your creativity or add you know arrows to your creativity uh the other thing you machine learning you not only work with numbers you can work with words as well so classif example are spam filters um and uh yeah email filtering um or you know when you work when you have a salary on your iphone or or search boxes it's all about work processing so it's called natural language processing and there is also this uh project that i found on and on github recently it's a ai for robots so for those um i think jessica you talked about sensor and anna was talking about arduinos so this combines you know artificial intelligence and arduino and robots so you can have fun with everything and all in node.js and node very high level uh i'll try to expose the key concepts of machine learning so we oppose supervised to unsupervised machine learning so supervisors when you you have a specific set of inputs and you know uh your output you know uh what to expect at the end of the day so you're trying to find a function if you want that will map all the x variables to the targeted output y so typical uh there are two types like classification when it's a discrete set of outputs so if you are you know let's say you have a set of colors so you categorize red or blue or your emails if it's spam or not spam it's classification problems then you have regular regression problems so regression is when the output variable is continuous so it can be in dollars or weights for instance if you think of let's say you wanted to do a study on property prices and you would think of well what's what's the location what's the square footage whole number of rooms uh you know does it have a garage a swimming pool all those features and you try to have at the end of the day how much your property would be worth so that's kind of regression uh problem so examples of algorithms that support that it's a linear regression uh support vector machines and random force um random force is like a tree and basically you do multiple trees randomly you set up the parameters randomly so that's why we call it random forest unsupervised learning is when you don't actually know the output or you want actually to discover the different groupings and the different classifications out of the blue so you have clustering problems and association problems so clustering um given a certain set of data it's used for instance in in the medical world like if you have a a certain set of features uh your sex your age your your health habits uh uh how uh how and uh or in biology in general like if you have a certain set of features uh and you group them what type of disease or what type of plants you come to associations is to discover the rules among this data so really at the high level it's supervised versus unsupervised so that's a classic um description of neural network which is the one of the main algorithm in machine learning so you have a set of inputs so as i said before for instance in the real estate example the square footage the number of rooms sets your input layer so you have a number the number of inputs are the number of features and the output so here you have two different outputs it could be a spam not spam or it could be a just one output but a price uh you have an output layer and in between the output and out sorry in between the input and the output layers you can have hidden layers and there can be uh uh like it can be from one to n number of uh of layers depending on the conclusion complexity and here it is where it's more an art than a science to decide which number of hidden layers are in a network but this might not appeal to everybody in terms of uh understanding of a neural network i find sometimes it's easier to have this kind of representation like a typical tabula or presentation so the features are um you know from you can see in terms of widths of the data set and the number of examples are like the height of x in that example and so it's uh obviously it's it looks like a matrix and you try to arrive at the for each example you know uh which output you're gonna get so it's you try to approximate the y the column y that you have to next to the matrix so you're trying to find out the uh the function or the the weights that you apply to the your data set the the x and this approximation gives you a it's called a yhat which is um you know what outputs given a current model so uh in in mathematical terms it would be just it's f of x gives you a y and you try to minimize y hat compared to hawaii i don't know if it makes sense to everybody yes this one just looks easier for you yeah i don't know [Laughter] it's uh i say maybe the second representation makes you more i understand that you know you you have all this number of examples the output that you expect the why and we are trying the game is really to try to minimize uh why and why hearts okay and we are and to basically to do that you adjust you you you adjust the w which are the weights that you apply to the number of features y hat it's basically the result of your model of w times the x if you want the different weights that you apply to your data set so basically this thing called perceptron is just one part of let's say what i showed in the picture of the person from just this part so you have a different number of inputs your plan number of whites in the middle and you activate them and you have one neuron in your network so the in terms of calculations if you apply to your inputs uh different weights you sum them like in a multi linear regression and you activate them making them like you know a binary is it zero or one do you accept it in your model or not and so this perceptron is or all the hidden layers and the output layers are calculated in that manner you apply a different set of weights between the input and the hidden layers and a different set of weights between hidden layer and output layer and you at the end of the day you try to compare the output layer to the calculations from your weights and the different layers to the output layer and you try to minimize uh this error between what you've calculated and what is actual does it make sense it makes sense so another key concept and it was discovered relatively recently recently in that case means in the last 15 years is the concept of back propagation so you can do this calculations about weights in one way but really uh for it to improve and so it's that the network learns you have this concept of back propagation so the you you back up the errors that you found out at the way that you calculate at the end of the day between the um the calculated output and the real output you try to integrate it backwards into the weights um into the weights that are between the hidden and output layer and between the input and hidden layer so how um so you reintegrate the error and it's a iterative process until you your error is close to zero or as close to zero as possible and that can be done basically uh thanks to um because we all know the properties of the activation functions so this is possible to to go backwards or to back propagate your error thanks to the derivatives so activation these are examples of activation functions so i really you have to think of activation functions as trying to squeeze a number here in this case for the logistic functions between 0 and 1 you know that everything between infinity minus infinity plus infinity will be squeezed between 0 and 1 i don't know how you say it in english sorry just ten times ten h okay it's you squeeze every number from minus infinity to plus infinity between minus one and plus one and we can back propagate uh and do this inaudible process because actually mathematically we know what the derivative of this um of these different functions are and i don't know if you have recollection from you are doing linear algebra and maybe in high school or in college when you want so most cost functions have a convex feature okay and so um you know what convex is like it looks like like a u basically and when you want to minimize so to go to the bottom to find the local minimum on a on a convex function you use the derivative and try to go to the zero basically so you use a slope of the curve in in the convex function to try to reach the minimum as a local minimum so that's that's how we achieve back propagation and that's how we can achieve the whole iterative process of improvement so you calculate a certain output you calculate the cost error and back propagate it into the weights and you and then you there is a learning rate which makes it improve every time so these are the classic uh cost functions the most i guess the most known one in terms of statistics is a mean square error but you have a number of different types of squares okay enough math for today though so obviously all these calculations uh you know at the end of the day you need performance from your computer to um to realize this and the browser obviously is not the uh the lowest level you can do your calculations and most libraries would be in uh c plus plus i don't think they're in any assembly to my knowledge but they would be in in a very in much more low level language however when you handle calculations it's often done in python and python is just a wrapper around these c plus plus libraries that being said if we are just doing uh python versus javascript performance comparisons uh most of the time javascript with the improvements of the v8 engine and you know the different browser engines it's actually uh faster than python uh it's just that when you do uh deep uh learning networks that uh when you you do like image recognition and not just of cats because here it was a bit cheating the cat was recognized because we could uh find two ears like you know the the sign of so that's it was a shortcut but if you want to do it at a production level like you know at google size you need really like a lot of gpus to do that and why gpu is because coming from the gaming world that's where most calculations for refreshing screens come from and so they are very much used to all the vectors and matrix calculations that are actually used in machine learning so the leader in this in that field is nvidia or when you have nvidia a gpu cat they have actually also created libraries that enable you advanced mathematical operations on it so in the javascript world we are used to the simplest way to do that is array of arrays but it's actually not the most performing a way to do that uh for accessing a particular item you do what you know given a matrix a an array of arrays you you access your elements by you know i mean a i j which you know i'm sure everybody would be able to do but it's not if you want to iterate over that or access or transpose or in this metrics it it won't be very fast one uh faster way uh to do that is uh to use typed arrays which is my second example and it's it's faster but the thing is here the you you can see this so this is just the same matrix as this one but it's just flat it has been flattened and here the index is um so as you can see it's a three rows and three columns matrix and here you can see that the index has been fixed so one so it's very fast using typed arrays but to access the the different elements when you want to do uh higher order operations is is a bit cumbersome so there is a compromise where this index the three in my example here is actually um it's sorry it's a general linear function so here this formula basically becomes to that so so here you can really switch rows and columns very quickly you can transpose numbers in your matrix much quicker and there's been a library that has used this idea of strided arrays and which is ndrs it's used in a number of lowlevel and highlevel libraries numgs which is a kind of numpy for those who are familiar with python that tries to use the same kind of functions that numpy or matlab have has keras takes the name of of a famous uh library which is a higher order library a higher level library on top of tensorflow or tiano which are more lower level or matrix calculations so keras it would be easier to do artificial neural network a recurrent neural network it's it's good for prototyping machine learning models the only thing the keras javascript model you can only in a way do a forward propagation you can only do what's called inference and not back propagation so the training of your models won't be done in javascript it would be done in in python and then once you have your model you use javascript to visualize these things so tensorfire is the latest library it uses ndis but it also uses webgl under the hood so other ways to do calculations in on the cpu is using uh web assembly there are fewer examples of that and they are less mature but it's certainly avenues to explore to improve the speed of calculations on the cpu and also to increase the the parallelism of calculations in browsers you you can use service workers that being said there is a cost of transferring your data from your browser thread to the service workers and back so there will be a slight delay compared to pure cpu so as i mentioned you can do calculations leveraging your cpu or you can leverage uh webgl so a number of uh of libraries have emerged and the latest one being tensorfire which was i saw that on twitter like uh two weeks ago so it's really recent but um they haven't for the moment open source their code so as i said i found the npm library but i couldn't find a github basically it leverages jrsl so the the language that is used with webgl you can use also uh leverage uh your gpu differently using opencl or web gpu but then i guess it's not uh it will be a different setup it will be using a node rather than purely the browser and usually it's you can it's restricted to a certain number of browsers it's not cross browser compatible right um so that was my high level uh in terms of concepts from machine learning uh i wanted to export a number of blogs and libraries that you can use in javascript or courses if you are interested in the area so this bright camber did an old blog in machine learning addressing not neural network but a lot of unsupervised learning algorithm and the second one is that i mentioned here andrej capaci is is quite interesting because this guy is relatively famous he developed one of the first deep learning library in javascript called confnet gs to do convolutional neural networks on pictures uh and he's uh he used to work for so he's from stanford and he used to do uh work for open li and recently he has been he has been hired by tesla to improve machine learning for cars so there are a number of javascript specific resources so content us unfortunately is no longer maintained tensorflow you have a nice tensorflow playground to play with which is here you can add a number of hidden layers different features and you can so it's nice to play around in terms of what you can achieve and how long it takes to to improve your model um so it's tensorflow playground so it's it's a really it's written in typescript and uh it's only a very small set of the tensorflow library do you know actually do do people understand what a tensor is by any chance no okay so you should think of tensor as a container like a a number a scalar would be a tensor of of size zero a vector so i'm gonna go to so a vector would be a tensor of one dimension a matrix with rows and columns would be a tensor of two dimensions and a cube so three dimensions would be a tensorflow a tensor of three dimensions so it's you have to think of it as a container so libraries uh and if if you are more interested so you can get involved at different levels maybe some like to go into the lower level so the matrix and vector calculations uh or you can go higher level and talk about different types of architecture and convert yes and more recently this one knit optic which tries to do neural evolution so neural evolution for those who don't know it's it's kind of uh it tries to it's a gene kind of genetic algorithm which means that they let's say you can't to randomly generate a number of uh of examples and basically you you try to the you try to eliminate those who don't work as much so it's it's a very darwinian process in a way you generate a number of of examples you you keep the ones who survive the longest then you regenerate a number of set of examples and and and so forth back in so um so there has been a number of um universities uh involved in uh to do machine learning in javascript so one would i would say is more on the edges from the university of tokyo maldives they do libraries which are relatively very japanese friendly like sushi too tempura sukiyaki but they have like sushi too is for instance for matrix calculation sukiyaki is for on top of sushi 2 to do different machine learning models they do distributed computing so they have created a framework where you have different browsers and basically you leverage the capabilities from you know maybe using web sockets so you can connect different uh computers different browsers and you leverage the cpus of these different browsers you have two one swiss university which the other two universities are maybe less known or less marketed in a way i had really i learned just by chance that they existed and they have actually quite a few uh libraries uh on npm on github um the last one actually doesn't have a npm but it does have a github i asked them if you wanted to leverage npm but the guy didn't see the point of using uh javascript who is not yes i don't know that's his point but they have a nice online uh laboratory where you can actually download some files and you can you know use the most common machine learning uh libraries on it to you know try to um create a model um coursera i personally so i'm not an expert in the field i uh i'm still doing it actually um i recommend the corsair course from uh from underneath who is a uh who used to work at google um and and baidu he was a chief um machine learning or chief artificial intelligence guy from baidu the chinese company jeffrey hinton saw his course on our networks in corsa jeffrey hinton is one of the leaders in machine learning and it's it's quite unique that you can find his you know course from him completely free on conserva udacity proposes a number of of nanodegrees in the field the example that i showed you on deep traffic the mit course a lot of people from the selfdriving engineer uh were involved in that so i don't oh i don't have network so unfortunately on behavior but basically they participated in in this in this setup as well there are lots of resources on the internet uh blogs uh which are not most of the time not javascript specific the javascript ones are the these ones that i mentioned yes so you can get involved also to encourage getting involved there is a ai grant where for whatever projects and tensorfire the latest library that i mentioned they participated in the ai grant competition and they won 5 000 from that to to when they created tensorflow or kaggle kaggle actually was cofounded by an australian guy and it's a it's a website where people can do machine learning competitions and and there are open data sets and there is a forum where you share your tips and tricks to improve the accuracy of your models sorry there was very fast and uh you know very involved but if you have any questions happy sorry for all the links yes sure uh yeah i will put this yes graphical forum machine learning activity effect screen rendering is it going to uh if you've got an app that yeah so turn safari i think you can try the uh the demos that they have on your mobile uh i personally don't know uh you know the performance uh comparisons for that i the one of the thing is i guess the calculations would be i mean i don't i don't think your gpu on the mobile is in terms of power it's it's great that great so i don't think you know um i mean it will improve a bit in a way because it's paralyzed so it's uh you know in parallel to your cpu but in terms of pure raw performance i don't think it's right that being said i know that google with tensorflow has tried to you know i mean democratize the use of its library and so they encourage it and they do android apps on that but i personally haven't tested it so [Laughter] so the whites you have to think it's the same set of weights that you apply to the examples okay so you will try to follow them together very quickly um you can vary yeah so obviously it's different set of inputs but basically it's m and you can see it's a number of features any number of features you multiply each feature but it's white okay so first it's randomly generated and then so so first you randomly generate these weights so then you do the calculations so the like the matrix calculations okay you activate them using one of the functions that i showed you before and um you do that for every neuron every layer and gives you y hat the what comes from your weights okay so then you try to calculate costs you have a cost measure so the most famous one is a mean square arrow of comparing it's basically y y minus y hat square divided by the number of examples okay so once you have that measure of error okay you you back propagate this error into thanks to the derivatives of the active er evaluation functions back to the initial input okay so it changed and and then it's an iterative process thanks to what's called the learning rate where you add that at the end of the day you will me you will diminish the error that you calculated a numerator example or coding example if you want on a like the number of uh i suppose dimensions that you can work on the problem is it just for like a two dimension problem and then anything after that the error rate you know just can't really model it very well uh so actually the fact that you have hidden layers and and you have so it improves uh if you want the nonlinearity of your form so let me show you what i mean so you're thinking of giving us a set like that you can see it's it's not clear if you were just doing linear regression it would be a straight line okay so if you were doing just input outputs that's what you would expect it's a it's a linear regression but the fact that you implement a hidden layer in the middle give improves the nonlinearity of your uh of your model if you want so uh i i've done that example in in javascript so just logistic regression and just input output without hidden layers would give you a straight line okay but if you had a hidden layer and and i said i mean here the color is in titan but i've done the thing in javascript you the fact that you a hidden layer they will find you know here so you can see here this invariable for hidden layer size three you can see you you add a nonlinearity to your problem so and the more you add basically the more precise in your nonlinearity you can go okay so then it's a complex then there is i didn't want to go over that but you can overfit your data so then in terms of uh if you want to predict you know a cert with your model you have to be uh very careful that it doesn't offer that your model doesn't overfit your test data but if you do that what's the cost in terms of processing oh obviously that's numerical complexities i mean i i i don't have the exact answer but you know it's it's adding a role it's basically adding uh a few mattresses to your uh to your calculations so in terms of i know computer scientists calculate in terms of or something it would be at least i mean square added square capacity any other question you
