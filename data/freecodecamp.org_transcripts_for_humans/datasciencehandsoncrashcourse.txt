With timestamps:

00:00 - hello everyone and welcome to this data
00:02 - science crash course my name is marco
00:04 - and i just recently started a youtube
00:06 - channel where i do videos on
00:07 - data science so if you want to check it
00:09 - out the link is in the description below
00:11 - a bit of an overview of this crash
00:13 - course we will first answer
00:14 - the question what is data science then
00:17 - we will walk through a setup
00:18 - so that you are ready on your computers
00:20 - to code along with me
00:22 - and then we will move on to the
00:24 - algorithms which is in my opinion
00:26 - the fun part so we'll talk about linear
00:28 - regression
00:29 - then we will move on to classification
00:31 - with logistic regression
00:33 - lda and qda we will talk about
00:35 - resampling and regularization methods
00:38 - which are very important in any workflow
00:40 - for a data scientist
00:42 - we will talk about decision trees uh
00:45 - most of the state-of-the-art algorithms
00:47 - are actually
00:47 - tree-based methods so a very exciting
00:50 - subject
00:51 - and then we will move on to support
00:54 - vector machines
00:55 - and conclude this crash course with
00:57 - unsupervised
00:58 - learning now let's answer this question
01:02 - what is data science
01:03 - well data science is a field that uses
01:05 - scientific methods to extract
01:08 - knowledge and insights from data now
01:11 - this definition
01:12 - may look very broad and that's because
01:15 - data science
01:16 - is a very broad field in fact it
01:19 - encompasses three professions so
01:22 - you can be a machine learning scientist
01:25 - or engineer
01:26 - which means that you are the person who
01:28 - develops the algorithms
01:30 - you could also be a data analyst which
01:33 - is the person who answers
01:34 - business questions for example what is
01:37 - the product
01:37 - that we sold the most in the last month
01:40 - that would be
01:41 - the job of the data analyst or finally
01:44 - you could be a data engineer
01:46 - which is the person that builds the
01:48 - software to gather data from different
01:49 - sources
01:50 - because usually the data needed to solve
01:53 - a problem
01:54 - is not in the same place so those data
01:57 - engineers
01:57 - they gather the data from everywhere and
02:00 - put it in a
02:01 - format that can be used after by the
02:03 - data analyst
02:04 - or the machine learning engineer now
02:07 - this crash course will mostly focus
02:10 - on the machine learning part because we
02:12 - will mostly discuss
02:14 - the algorithms now what can you expect
02:16 - from this course well first of all
02:18 - theory yes as in math equations because
02:21 - theory
02:22 - is important in data science you need to
02:25 - know
02:26 - how a model behaves why it behaves in a
02:30 - certain way
02:31 - and how it works because understanding
02:34 - that
02:34 - is actually way harder than coding the
02:36 - algorithm itself
02:37 - as you will see later on also if your
02:40 - goal is to land a job
02:41 - as a data scientist in a company well
02:44 - you will be asked
02:45 - theoretical questions during the
02:47 - interview so a very important part
02:49 - please don't skip it
02:50 - and of course each algorithm section
02:53 - will be accompanied by
02:54 - hands-on examples in python so for each
02:57 - algorithm
02:58 - we will download a data set and we will
03:00 - apply this algorithm on that data set
03:03 - so without further ado let's get started
03:05 - with the setup
03:06 - all right let's get you set up to do
03:08 - some data science
03:09 - head over to google and search for
03:11 - anaconda
03:12 - then click on the first result which
03:14 - should be anaconda the world's most
03:16 - popular
03:17 - data science platform once you click on
03:20 - it
03:20 - head over to products and then
03:22 - individual edition
03:24 - you can read more about what anaconda is
03:27 - if you want
03:27 - otherwise simply click on download and
03:30 - it brings you
03:31 - here where you can select the installer
03:33 - which is appropriate to your
03:35 - operating system now in my case i have
03:38 - windows
03:38 - 64 so it would be the graphical
03:40 - installer if you're on mac or on linux
03:43 - choose the one that suits you best once
03:45 - you click on it you can
03:47 - download and save the file now i will
03:49 - not do this
03:50 - since i already installed it on my
03:52 - machine but once it is downloaded you
03:54 - can simply
03:55 - follow the instructions on the graphical
03:57 - installer
03:58 - and once everything is done you should
04:00 - be able at least on windows
04:02 - if you hit the windows key and start
04:04 - typing jupiter
04:06 - with a y simply type enter
04:11 - and after a moment you should see this
04:13 - page showing up
04:15 - and that means that jupiter is installed
04:18 - correctly and you're ready to do some
04:20 - data science
04:21 - if you're on mac however simply open
04:23 - your terminal and type in
04:25 - jupiter notebook with a space and you
04:27 - should be fine
04:29 - all right now that we are set up let's
04:31 - kick off this crash course with our very
04:33 - first algorithm which is
04:34 - linear regression we start with a simple
04:37 - linear regression
04:39 - where our target y only depends on one
04:41 - variable
04:42 - x and a constant beta 1 is then
04:45 - a parameter which can be positive or
04:47 - negative that characterizes
04:49 - the slope and beta naught here is the
04:52 - constant
04:54 - to find the parameters we need to
04:56 - minimize a certain error function
04:58 - so here the error is simply the
05:00 - difference between the real target
05:02 - y i and the prediction y hat
05:06 - for linear regression we minimize the
05:08 - sum of squared errors
05:10 - so we raise this equation to the power
05:12 - of 2 and add
05:13 - all errors across all data points
05:17 - visually it looks like this the red dots
05:19 - represent our data
05:21 - and the blue line is our fitted straight
05:23 - line each vertical line
05:25 - is the magnitude of the error so we want
05:28 - the
05:28 - to position the blue line such as the
05:31 - sum
05:31 - of the squared length of each vertical
05:33 - line
05:34 - is as small as possible now you might
05:37 - wonder why do we square the errors well
05:40 - as you saw the points can lie
05:42 - above or below the fiddle line so the
05:45 - error can be positive or negative
05:47 - if we did not square the error we could
05:49 - be adding a bunch of negative errors and
05:51 - reduce the sum of errors
05:53 - it would trick us in thinking that we
05:55 - are fitting a good straight line
05:57 - where in fact we're not it also has the
06:00 - added advantage of penalizing large
06:03 - errors
06:03 - so we really get the best fit possible
06:07 - for simple linear regression you can
06:08 - find the parameters analytically with
06:10 - these formulas where
06:12 - x bar is the mean of the independent
06:14 - variable and y bar is the mean of the
06:16 - target
06:18 - now of course in practice we will use
06:20 - python to estimate those parameters for
06:22 - us
06:22 - here you can see we first initialized
06:25 - the model and then we fit
06:26 - on x and y and then we can retrieve both
06:30 - the intercept and the coefficient as you
06:32 - can see here
06:35 - once you have your coefficients we need
06:37 - a way to assess their relevancy
06:39 - to do so we use the p-value this allows
06:42 - us to quantify
06:43 - the statistical significance and
06:45 - determine if we can reject the null
06:47 - hypothesis
06:48 - or not in python we can analyze
06:52 - the p value for each coefficient like
06:55 - this
06:56 - here we use a statistical package from
06:58 - python that allows us to print out a
07:00 - summary
07:01 - of the model here you can see an example
07:05 - of that summary
07:06 - so for each coefficient you get here the
07:08 - p-value
07:09 - which is the p greater than absolute
07:12 - value of t
07:13 - here you see the value is zero but it is
07:16 - really not zero
07:17 - however it is so small that it appears
07:20 - to be zero
07:22 - once we know our parameters are relevant
07:24 - we must assess the model itself
07:27 - we usually use the residual standard
07:29 - error of course
07:30 - the smaller the value the better it is
07:33 - since the difference between predicted
07:35 - and actual
07:36 - is small as you can see with the
07:37 - equation on the screen
07:39 - also we use the r squared value which
07:42 - measures the proportion of variability
07:44 - explained by a feature x as it
07:47 - approaches 1 it means
07:49 - that we are explaining a lot of the
07:50 - variability in our target
07:54 - in python we can use the same process as
07:56 - we did before to find the p-value
07:58 - using the same package so as you can see
08:01 - when we print out the summary this is
08:03 - the full printout that you get from the
08:05 - python package
08:07 - you see highlighted in yellow the
08:08 - r-squared value
08:10 - in this example now from here multiple
08:13 - linear regression is easy to understand
08:15 - as we simply extend the model to
08:17 - accommodate
08:18 - more features now each feature has its
08:20 - own parameter
08:22 - and p is the number of predictors
08:26 - so in python we saw how to access the
08:29 - constant n1 parameter and if we have
08:33 - multiple parameters
08:34 - then we simply increase the
08:37 - index to get the other parameters
08:40 - in multiple linear regression to assist
08:43 - the model
08:44 - of a multiple linear regression we use
08:46 - the f statistic
08:47 - here p is the number of predictors and n
08:50 - is the number of
08:51 - data points now again in python we'll
08:54 - use the
08:55 - same package that outputs for us the f
08:58 - statistic
08:59 - as you can see here in yellow the f
09:01 - statistic for a multiple linear
09:03 - regression
09:04 - model usually if f is much
09:07 - greater than one we say that there is a
09:09 - strong relationship between our
09:11 - predictors
09:11 - and the target for a small data set of a
09:14 - couple hundred data points
09:16 - then we the f statistic has to be way
09:19 - larger than one so that's it for the
09:22 - theory of linear regression
09:23 - let's see how we can apply it with
09:25 - python
09:29 - so let's fire up our jupyter notebooks
09:31 - here i have a folder
09:32 - called data which contains a data set
09:34 - for this exercise
09:37 - so as with any project we start off by
09:40 - importing the libraries we will use so
09:42 - of course we're going to use pandas as
09:44 - pd let's import numpy as
09:47 - np now it's time to import matplotlib
09:52 - and because we will use that of course
09:54 - to do some plots of our data
09:56 - a lot of straight lines and scatter
09:58 - plots
10:00 - then we're going to use cycle learn to
10:01 - actually fit a linear model
10:03 - to our data set
10:07 - your regression and finally we also
10:11 - import
10:12 - a stats library so stats models
10:15 - dot api as sm
10:18 - and that will give us some very cool um
10:22 - statistical tests to uh test for our
10:24 - predictors and model in general
10:26 - and of course we use some python magic
10:28 - to plot
10:30 - so let's start off by importing our data
10:32 - set so data is equal to pde.read.csv
10:35 - i put in the path of my data so data
10:39 - slash
10:41 - advertising.csv and then i will specify
10:44 - the
10:44 - index call equal to zero
10:48 - and that means that the first column
10:51 - will be used as the index column in our
10:53 - data set
10:54 - doing data.head you should see the
10:55 - following first five entries of our data
10:58 - so as you can see we have
11:00 - ad spend on tv radio and newspaper and
11:03 - the impact on
11:04 - sales so we start off with
11:07 - some simple linear regression so we will
11:10 - only consider the effect of
11:11 - tv on sales in this case feel free to
11:15 - take
11:15 - radio or newspaper so i always like to
11:19 - do a
11:20 - quick plot of my data whenever possible
11:22 - in this case because we only have one
11:24 - feature one target
11:25 - it is indeed possible so i first set the
11:29 - size of my figure and then we do a
11:31 - scatter plot
11:32 - so specify the x which is going to be
11:34 - data tv the y is data sales
11:37 - and then i specify the color i want it
11:38 - to be black
11:41 - afterwards i'm just doing making my plot
11:43 - uh a bit
11:44 - nicer so i just specified an x label
11:48 - um so there's gonna be money spent on tv
11:51 - ads and the units is money such as
11:54 - dollar sign
11:55 - and then plt.y label is going to be
11:58 - equal
11:59 - actually no i do not have to put an
12:02 - equal sign here i need to remove
12:05 - that so remove this from this all right
12:07 - perfect
12:09 - and now i do this is going to be sales
12:12 - and it's going to be in thousands of
12:14 - dollars
12:15 - perfect finally i do plt show and you
12:18 - should see the following
12:20 - plot there you go as you can see so this
12:23 - is our data so the sales
12:24 - with respect to the money spent on tp
12:26 - ads now maybe a linear
12:28 - line a linear regression is not the best
12:31 - model here but let's try it
12:32 - anyways so we're going to specify our
12:37 - feature which in this case is only the
12:39 - data
12:40 - tv so the values are reshaped minus 1 1
12:44 - as required by the scikit learn library
12:46 - and then we specify the
12:48 - target which is our sales values dot
12:51 - reshape
12:52 - minus 1 1 perfect
12:55 - now we simply call the regression
12:59 - model so rag is going to be equal to
13:00 - linear regression
13:02 - and then we will fit the model to our
13:05 - data we pass in x
13:07 - and y awesome and with that we can print
13:12 - our parameters so we can print our
13:15 - constant and coefficient for tv so the
13:18 - linear model is
13:20 - y is going to be equal to so first we're
13:23 - gonna
13:24 - access uh actually we're gonna access
13:26 - the intercept
13:27 - first uh so we intersect underscore
13:31 - zero and then we add the
13:34 - coefficient for x
13:38 - so in this case tv and the coefficient
13:40 - is equivalent to beta1
13:41 - if you looked at the theory portion of
13:44 - this video
13:45 - so as you can see we get a constant of 7
13:48 - and a slope of 0.0475
13:53 - approximately so that's great we have a
13:55 - positive slope
13:56 - positive constant it seems to make sense
14:00 - so let's get some predictions and
14:02 - actually plot our straight line
14:04 - so you can get the prediction simply by
14:06 - calling the predict method
14:08 - on x and then we do
14:12 - another figure so i'm always again
14:15 - just setting the fixed size here not
14:17 - curly braces sorry about that
14:19 - it's actually just normal parentheses so
14:23 - put it the same size as before 16 8
14:27 - then plt dot scatter we pass in
14:31 - x y and of course the color is going to
14:33 - be black so this is going to be our data
14:35 - set
14:35 - as shown uh above a bit earlier
14:38 - and then we add in uh the plot of our
14:41 - predations
14:42 - so same x the y in this case is going to
14:44 - be predictions
14:45 - and i want it to be in blue and i'm
14:47 - going to specify the line width
14:49 - to be equal to 2 just to make sure that
14:51 - we can see it
14:53 - now let's just copy the labels because
14:55 - it's going to be exactly the same the
14:58 - same sorry
14:59 - uh the plt does show as well awesome
15:01 - let's run it
15:02 - and boom as you can see we have ours
15:05 - traded
15:06 - our straight line plotted on our graph
15:11 - uh that's awesome perfect so let's move
15:14 - on to the next
15:15 - portion where we will assess the quality
15:19 - of our
15:19 - uh model so to do so we're actually
15:23 - gonna use
15:24 - the stats library so i'm going to
15:26 - re-specify again my
15:27 - x and my y and we're going to fit
15:30 - another linear
15:32 - model but using the stats library
15:35 - so specify the exogenous variable as sm
15:38 - add constant x and then the estimator
15:42 - is simply sm.ols that stands for
15:45 - ordinary least
15:46 - squares that's the method we're using
15:48 - pass in y pass in x and we fit
15:51 - finally you can print a summary of the
15:54 - estimator
15:55 - and you should see the following result
15:57 - so as you can see we have an r squared
15:59 - value of 0.6
16:00 - so that is not very good only 60 of the
16:03 - variability
16:04 - is explained the s statistic is 312
16:08 - which is much larger than one so it
16:10 - seems that our model is
16:12 - kind of good and as you can see here for
16:14 - tv we get the same coefficients as
16:16 - before
16:16 - and the p-value although probably not
16:19 - zero it seems to be
16:20 - less than 0.05 so it means that our
16:24 - um that our feature is indeed
16:28 - relevant in this model so that was
16:31 - simple linear regression let's move on
16:33 - to multiple
16:33 - linear regression so in this case we
16:35 - will consider all the features so tv
16:38 - radio and newspaper and see how that
16:41 - affects the sales
16:42 - so all my x's to define them i'm just
16:45 - going to drop
16:45 - the sales column and make sure i dropped
16:48 - it
16:48 - on axis equals one so i mean i'm
16:50 - dropping it only the column
16:52 - and on the rows and the y is going to be
16:54 - the same so that is sales the values dot
16:57 - reshape
16:57 - minus one and one again i'm going to fit
17:01 - using scikit-learn so the regressor is
17:03 - linear regression
17:05 - and you call fit so x's and
17:08 - y perfect now as before we're going to
17:11 - print our
17:12 - coefficients so the linear model
17:15 - is skip a line and then
17:18 - y is going to be equal to
17:22 - so let's start off by printing uh the
17:25 - constant right so reg.intercept
17:28 - underscore square bracket is zero
17:32 - and then the coefficients will be in the
17:35 - same order as
17:36 - in the data set so the first one if i
17:38 - remember well it's going to be for tv so
17:40 - reg dot co f
17:42 - 0 0 and then
17:45 - um multiply that by uh tv
17:50 - afterwards we're gonna add the
17:52 - coefficient
17:54 - as the second one oh this is
17:57 - not in the brackets sorry about that
18:04 - radio and let's put all of this inside
18:08 - the brackets
18:13 - start the squiggly brackets right
18:17 - and now we're gonna add the last
18:20 - coefficient so reg dot co f
18:23 - zero one two sorry
18:26 - and let's bring this a bit to the right
18:29 - so you can see the code
18:31 - uh we multiply that by news paper
18:35 - awesome let's run this cell
18:39 - hopefully everything is going to work no
18:42 - the regression object has no attribute
18:44 - code f
18:46 - indeed it does not uh we need to add an
18:49 - underscore
18:50 - on the coefficient of tv so coeff
18:53 - underscore
18:54 - awesome there you go there we have it so
18:57 - uh we have two as a constant .04 so the
19:01 - same slope for tv
19:03 - point 18 for radio and negative for
19:06 - newspaper that is very interesting we
19:09 - have a negative
19:10 - effect from newspaper so
19:13 - again let's fi let's use the stats
19:16 - library
19:16 - to assess the quality of our model so
19:18 - i'm just going to specify the x as
19:20 - np not column stack and then we take
19:23 - everything so data tv
19:25 - we're gonna take uh data radio
19:29 - right yes data radio
19:34 - and data newspaper
19:37 - awesome so this is our x our features
19:40 - and the target again is data
19:44 - sales
19:47 - sorry shape minus one one
19:51 - awesome so again the exogenous variable
19:54 - we're going to do
19:54 - sm.add constant we use x
19:58 - we define our estimator which is sm.ols
20:03 - and then passing y passing x
20:07 - in this case exalt right.fit and we
20:10 - print
20:10 - the summary of our estimator
20:15 - and you should get the following awesome
20:16 - now as you can see the r squared value
20:18 - is
20:18 - much larger than before we have 0.897 so
20:23 - we're explaining almost 90 percent of
20:24 - the variability of the variability of
20:26 - the sales here
20:27 - the f statistic 570 again larger than
20:30 - before so it means that our model is
20:33 - pretty good i actually to predict the
20:35 - sales from that and as you can see here
20:37 - all the constants and all the
20:39 - coefficients
20:40 - now as you see for the last one we have
20:42 - a p-value
20:43 - equal to 0.860 so that is
20:46 - larger than 0.05 and recall that
20:49 - this coefficient here is the one for
20:51 - newspaper so that means the newspaper is
20:54 - actually not relevant in our model
20:56 - and we could and actually we should take
20:58 - it out
21:00 - let's move on now to the next topic
21:02 - which is classification
21:05 - first a bit of terminology binary
21:07 - classification is also termed
21:09 - simple classification and this is the
21:12 - case when we only have two classes
21:14 - for example spam or not spam a
21:17 - fraudulent transaction
21:18 - or not of course you can also have more
21:21 - than two classes
21:22 - for example eye color which can be blue
21:24 - green or brown
21:27 - now you see in the context of
21:28 - classification we have a qualitative or
21:31 - categorical response unlike regression
21:34 - problems where we have a
21:36 - quantitative response or numbers
21:39 - so now let's see how we can perform
21:40 - classification
21:42 - with one algorithm which is logistic
21:44 - regression
21:47 - ideally when doing classification we
21:49 - want to determine the probability of an
21:51 - observation to be part of a class
21:53 - or not therefore we ideally want the
21:56 - output to be between
21:57 - 0 and 1 where 1 means very likely
22:01 - well it just turns out that there is a
22:03 - function to do that and it's the sigmoid
22:05 - function that you see on the screen
22:08 - as you can see as x approaches infinity
22:10 - you approach
22:11 - one and if you go towards negative
22:13 - infinity you approach
22:15 - zero the sigmoid function is expressed
22:18 - like this
22:19 - and here we are assuming only one
22:20 - predictor x
22:22 - for now we stick to one predictor to
22:24 - make the explanation simpler
22:26 - with some manipulation you can get to
22:28 - this formula here
22:29 - so we are trying to get a linear
22:31 - equation for x
22:33 - take the log on both sides and you get
22:35 - the log it
22:36 - as you can see now it is linear with
22:39 - respect to x
22:40 - and most importantly the probability is
22:43 - bound
22:43 - between 0 and 1. now of course we can
22:47 - extend the log it formula to accommodate
22:49 - multiple predictors
22:51 - this always gives better results since
22:53 - you are considering
22:54 - more predictors in python we can perform
22:57 - classification using
22:59 - logistic regression like this so again
23:01 - we initialize the model first
23:03 - and then we fit on our training data
23:06 - passing the features
23:07 - x and target y then we can get the
23:11 - probability
23:12 - that an observation is part of a class
23:14 - once we have that probability
23:16 - if it is greater than 0.5 we will say
23:20 - that it is part of that class so we will
23:22 - output one
23:24 - otherwise it will be equal to zero which
23:27 - is what the last line of this script
23:29 - is doing now that's it for logistic
23:32 - regression let's move on to
23:34 - linear discriminant analysis or lda
23:39 - now we want to learn about lda because
23:41 - logistic regression has some caveats
23:44 - when classes are well separated the
23:46 - parameters estimate
23:47 - tend to be unstable they are also
23:50 - unstable when the data set is small
23:53 - and finally logistic regression can only
23:55 - be used for binary classification
23:58 - with lda you can overcome those issues
24:01 - because
24:02 - it models the distribution of predictors
24:04 - for each class
24:06 - so you can have more than two target
24:07 - classes and it does so using bae's
24:10 - theorem now bae's theorem is explained
24:14 - like this
24:15 - suppose that we want to classify an
24:17 - observation into one of
24:19 - capital k classes where capital k
24:22 - is greater than or equal to two then we
24:25 - let
24:26 - pi k be the overall probability that an
24:29 - observation
24:29 - is associated to the kth class
24:33 - then let f k of x denote the density
24:37 - function
24:38 - of x for an observation that comes from
24:41 - the kth class
24:43 - this means that k of x is large
24:46 - if the probability that an observation
24:48 - from the kth class
24:49 - has capital x is equal to
24:52 - small x then the jesus theorem states
24:56 - the equation that you see the
24:58 - probability of the class
25:00 - being capital k given capital
25:03 - x equals to small x is the ratio
25:06 - of pi k fk of x over the sum
25:11 - of pi l f l of x from l
25:14 - to k so for all classes
25:18 - this was a bit challenging so make sure
25:20 - to probably
25:22 - rerun this section of the lesson because
25:24 - it is very important to understand now
25:28 - the challenge here
25:29 - is really approximating the density
25:30 - function so we will assume only one
25:33 - predictor
25:34 - and normal distribution this is
25:36 - expressed by the function that you see
25:38 - here
25:38 - this is the normal distribution function
25:41 - so if we plug this
25:42 - function in the formula that we saw
25:44 - before and take the log
25:46 - we find out that we must maximize this
25:49 - equation
25:50 - now this is called the discriminant and
25:52 - as you can see it is
25:54 - a linear function with respect to x
25:56 - hence the name
25:58 - linear discriminant analysis
26:01 - when applying lda we need to be aware of
26:03 - the assumptions it makes
26:04 - and make sure that it applies to our
26:06 - situation here lda assumes that each
26:09 - class has a normal distribution
26:11 - and each class has its own mean but the
26:13 - variance
26:14 - is common for all classes
26:17 - if you add more than one predictor which
26:19 - should be the case
26:20 - because usually more predictors gives
26:22 - you better results
26:23 - then each class is drawn from a
26:25 - multivariate gaussian distribution
26:28 - and each class has its own mean vector
26:31 - and there is a common covariance matrix
26:33 - so basically we must use vectors and
26:36 - matrices
26:37 - instead of single numbers compared to
26:38 - lda with only one predictor
26:42 - in practice this is how we apply lda
26:44 - once
26:45 - we initialize the logistic the linear
26:48 - discriminant analysis algorithm
26:51 - and then we fit it on our training data
26:53 - passing the features x
26:54 - and the target y then again we get the
26:58 - probability with the method predict
27:00 - prabha
27:01 - and finally if the probability is
27:03 - greater than 0.5
27:04 - we say it is part of a class so we
27:07 - output 1
27:08 - otherwise we output 0.
27:13 - now that we understand lda qda is fairly
27:16 - straightforward
27:17 - the main difference is in the
27:19 - assumptions just like lda we assume
27:22 - each class is from a multivariate normal
27:24 - distribution
27:25 - and that each class has its own mean
27:27 - vector but this time
27:29 - each class also has its own covariance
27:32 - matrix
27:34 - of course because we are talking about
27:36 - quadratic discriminant analysis
27:39 - well the discriminant here is expressed
27:41 - like this and you see that the equation
27:43 - is now
27:44 - quadratic with respect to x since we
27:46 - have two terms of x being multiplied
27:49 - together now whereas lda was better than
27:52 - logistic regression in some situations
27:54 - qda is also better than lda mainly when
27:57 - you have a large data set
27:59 - because it has a lower bias and higher
28:01 - variance
28:03 - if your data set is small then lda
28:05 - should be enough
28:07 - again in python you should see some kind
28:09 - of a pattern here
28:10 - because as you see it is very similar to
28:13 - lda and even logistic regression
28:15 - so again simply initialize your model
28:18 - fit it on x and y
28:19 - predict the probability and if it's
28:21 - greater than 0.5
28:23 - output 1 otherwise the output 0.
28:28 - now before we move on to the coding
28:29 - portion we must understand
28:31 - how to validate our models in the
28:33 - context of classification
28:35 - to do so we use sensitivity and
28:37 - specificity
28:39 - sensitivity is the true positive rate so
28:42 - the proportion of actual positives
28:44 - identified so for example it would be
28:47 - the proportion of fraudulent
28:48 - transactions that are
28:50 - actually fraudulent on the other hand
28:53 - specificity is the true negative rate so
28:57 - the proportion of non-fraudulent
28:59 - transactions
29:00 - that are actually non-fraudulent
29:05 - we can also use the rok curve where rock
29:08 - stands for receiver operating
29:10 - characteristic we usually take the area
29:13 - under the curve or
29:15 - auc that's why you probably hear about
29:17 - the rock
29:18 - auc we want the rock auc
29:21 - to be close to one why well
29:24 - as you can see we plot the false
29:27 - positive rate against the true positive
29:29 - rate
29:30 - ideally we have a false positive rate of
29:32 - 0 and a true positive rate of 1
29:35 - which would give an area under the curve
29:37 - of 1
29:38 - and the curve would hug the upper left
29:40 - corner of the graph
29:42 - as you will see soon in the coding
29:44 - portion we can write a function in
29:45 - python that will plot
29:46 - for us the roc curve and report its area
29:50 - under the curve we will actually walk
29:52 - through
29:53 - the plotting and the writing of this
29:55 - function in the coding portion
29:57 - now in the case of a perfect classifier
29:59 - as i said we get an auc
30:01 - of one and the rock curve should look
30:03 - like this
30:05 - so that's it for the theory about
30:07 - classification logistic regression
30:09 - lda and qda let's move on to a hands-on
30:12 - example
30:17 - alright so let's start off with this
30:19 - project fire up your jupiter
30:21 - notebooks are i have already mine open
30:24 - uh as always i have this folder called
30:27 - data in which i put my
30:28 - data set called mushrooms.csv
30:32 - the link for the dataset is in the
30:34 - description of the video
30:36 - so in this project we are going to
30:37 - classify mushrooms as either
30:40 - being edible or poisonous depending on
30:42 - different features
30:43 - so you have cap shape cap surface cap
30:45 - collar
30:46 - etc and i put all the possible values
30:50 - here at the beginning of the notebook so
30:53 - let's start off by importing the
30:54 - libraries we're going to use open spd
30:57 - known by snp of course
31:00 - matplotlib.pipeplot
31:01 - as plt and also seaborn
31:05 - as sns
31:09 - now we are going to import sklearn
31:12 - the pre-processing and specifically
31:15 - label encoder you will see how that will
31:17 - be used
31:18 - later on also from sklearn.model
31:22 - selection
31:26 - import train test
31:30 - split with underscores and also cross
31:33 - val score awesome
31:38 - from sklearn.metrics we are
31:42 - going to import the raw curve
31:45 - and the auc as well as the confusion
31:49 - matrix shift enter oh
31:52 - sorry first we're gonna put the
31:54 - matplotlib in line so we can see
31:56 - our plots all right so shift enter
32:00 - and now i am just going to um
32:04 - to define a path for my data set
32:08 - so data slash mushrooms with an
32:11 - s dot csv and now i'm just going to
32:15 - display the first five rows
32:17 - of the data set
32:28 - all right so as you can see these are
32:30 - our first five rows we see the class
32:32 - poisonous are edible and then we see the
32:33 - values for each
32:34 - feature so now i'm just going to do a
32:37 - plot to see how many
32:38 - poisonous and edible mushrooms we have
32:40 - in our data set
32:42 - so this will help us see if the data set
32:44 - is balanced or not
32:46 - so for that i'm going to use seaborn and
32:49 - as you can see
32:50 - our data set is fairly balanced we
32:51 - almost have the same amount of poisonous
32:54 - and edible mushrooms
32:56 - so that is very good we're not going to
32:57 - have to do a lot of preprocessing
33:00 - for our analysis
33:04 - now i'm going to define a function that
33:06 - will allow us to see
33:09 - depending on what feature how many
33:12 - uh mushrooms are poisonous or edible so
33:15 - for example
33:16 - if i if i plot for the cap surface
33:20 - so for all possibilities of values for
33:22 - the capped surface i want to know
33:24 - how many of those mushrooms are edible
33:26 - and how many of those
33:27 - are poisonous so that's going to give us
33:30 - a bit of intuition
33:31 - as to which feature helps you to
33:34 - actually classify your mushrooms
33:39 - and that's it so that's the function now
33:42 - i'm going to show you how you can use
33:45 - this function
33:46 - but i am not going to run it because
33:50 - this will actually run for all the
33:52 - features of the data set so it's going
33:54 - to generate a lot
33:55 - of plots so i'm just going to show you
33:58 - how to use it and you can run it on your
34:00 - notebook if you want so you set the hue
34:03 - equal to data
34:04 - class so that means that you're going to
34:06 - have two colors right so
34:07 - one for poisonous one for edible and
34:10 - then you simply want to drop
34:12 - the class column and then plot
34:15 - the rest of the data so
34:18 - to do that you just do plot data and
34:20 - then you pass in the hue
34:22 - and data to plot so fairly simple
34:25 - very straightforward like i said you can
34:26 - run this function on your own notebook
34:28 - if you want
34:29 - to see uh the examples but i will not
34:32 - run it
34:32 - for now so let's move on to
34:34 - preprocessing so you can do this by
34:36 - doing escape 2 by the way
34:38 - and shift enter so now i'm going to
34:41 - check how many
34:42 - null values do we have in our data sets
34:44 - because we do not want
34:46 - any anti-values so if we're calling
34:49 - data.columns i want to print
34:52 - the name of the column and then the sum
34:55 - of null values if they are null so you
34:58 - do that by data
34:59 - call is dot sum
35:02 - if you run it that is amazing as you can
35:05 - see we have
35:06 - zero everywhere so that means that we
35:08 - have no null values in our data set
35:11 - that's perfect
35:14 - now we are going to use the label
35:16 - encoder so what label encoder will do
35:18 - is that it will transform our class
35:21 - column into one and zeros because we
35:24 - cannot work with letters we have to work
35:26 - with numbers
35:26 - right so you do le.fee fit transform
35:30 - data class and now i will show you the
35:33 - result data head
35:36 - and there you go now the you you as you
35:39 - can see the class is now one
35:41 - and zero so either it is poisonous
35:44 - or not poisonous so one being true zero
35:46 - being
35:47 - false then you want to one hot encode
35:50 - the rest of the data set
35:51 - so to do that we do pd.get dummies
35:54 - and then you pass in the data so let's
35:57 - see what the result of that will be
36:02 - and as you can see now we have added a
36:04 - lot of columns so we went from 23
36:06 - columns to 118
36:07 - columns because now for every feature we
36:11 - have either true
36:12 - or false and now that is perfect this
36:15 - data is ready to be worked with because
36:16 - we have only
36:18 - numbers everywhere
36:22 - so let's move on to modeling
36:26 - first i'm gonna determine uh what the
36:29 - target variable
36:30 - is so in this case it is the class
36:34 - uh the values.reshape minus one one
36:37 - perfect and then
36:38 - the features is going to be the encoded
36:42 - data the head dot
36:45 - sorry dot drop the class column
36:48 - axis equals to 1 to make sure that we
36:50 - drop the column
36:52 - and then we're going to define a train
36:54 - and test
36:56 - set so we do x strain x test y train y
37:00 - test
37:00 - is going to be equal to the train test
37:03 - split that we imported earlier
37:06 - so you pass in your features you pass in
37:08 - your target variable
37:09 - and then you define the size in this
37:11 - case i'm going to do 0.2 so 20
37:13 - of the data set will be randomly uh
37:16 - removed
37:17 - to to use as a test set and we're going
37:20 - to use the rest to train
37:23 - you can set a random state by the way to
37:25 - keep the results constant
37:27 - so let's apply logistic regression our
37:30 - first algorithm for
37:31 - classification so from sklearn dot
37:35 - linear model import logistic
37:38 - regression
37:44 - now we're going to initialize the model
37:46 - so logistic regress
37:47 - reg sorry is going to be equal to
37:49 - logistic regression
37:55 - now i will fit the model to our train
37:59 - set so pass in x-strain and y-train dot
38:03 - ravel
38:08 - now we want to get the probabilities so
38:11 - y prob is going to be equal to logistic
38:14 - reg dot predict
38:18 - underscore probably and we're going to
38:21 - use the
38:22 - test set in this in this case because we
38:25 - fit before and now we are
38:26 - doing probabilities on the test set
38:28 - right
38:30 - and now we set our threshold to 0.5 so
38:32 - the actual prediction
38:33 - is going to be np dot where so
38:37 - in this case if the prediction is
38:39 - greater than 0.5
38:41 - we're going to say it's equal to 1 and
38:43 - otherwise it's going to be equal to
38:45 - zero so this is really where we are
38:48 - classifying our mushrooms
38:51 - so we have run that and everything is
38:54 - okay you can safely ignore the warning
38:56 - on the screen
38:58 - and now we're going to see at a
39:00 - confusion matrix so the confusion matrix
39:02 - is actually going to show you
39:04 - uh how many mushrooms were correctly
39:07 - classified so confusion matrix
39:10 - you pass in white test and you pass in
39:13 - the
39:14 - white prediction and hopefully if those
39:17 - are equal
39:18 - you will see that we're going to have a
39:20 - diagonal matrix
39:22 - and that is actually amazing we have
39:25 - a diagonal matrix so all poisonous and
39:29 - all edible mushrooms were correctly
39:32 - identified
39:33 - so let's check that with actually
39:35 - another metric
39:38 - we're going to use here the false
39:40 - positive rate
39:42 - and the true positive
39:46 - rate as well as the thresholds
39:49 - and we're going to set that equal to the
39:51 - rock
39:52 - curve pass in
39:56 - y test and y prob
40:06 - and then the rocket you see simply going
40:07 - to be equal to the auc
40:09 - and then you pass in the false positive
40:11 - rate
40:12 - and the true positive rate
40:15 - so here we are actually uh going to use
40:18 - the rock
40:19 - curve and as you can see when we
40:21 - calculate the rocket you see we get
40:22 - one which is again perfect
40:25 - classification
40:27 - now i'm just going to define a function
40:30 - to plot
40:31 - the rock curve so we can visualize it
40:33 - see how it looks
40:36 - so this function is going to take in the
40:37 - rock auc and here i'm just
40:39 - basically building the plot itself so
40:42 - i'm just
40:43 - setting the fix the figure size to seven
40:45 - by seven
40:47 - i'm setting the title to uh receiver
40:51 - operating characteristic
41:02 - then i am actually going to plot the
41:04 - false positive rate
41:05 - and the true positive rate
41:11 - and i'm going to give it a different
41:13 - color here i'm going to use
41:15 - red we're going to give it a label
41:19 - called auc
41:24 - and i'm only also going to approximate
41:30 - the the auc basically so this is going
41:32 - to give us
41:34 - approximated to the two decimal places
41:46 - and then i'm going to give it a legend
41:52 - i'm going to put it on the lower right
41:55 - side of the plot
41:58 - now i am also going to plot a straight
42:01 - line
42:02 - going through 0 and 1. this just serves
42:05 - us as a
42:06 - general guide to evaluate the rock
42:09 - curve and i'm going to make this line
42:14 - dashed
42:19 - now i am going to define the axis
42:22 - or plt axis
42:28 - tight
42:30 - give some labels so the y label is the
42:33 - true positive rate and the x label will
42:36 - be the
42:37 - false positive rate
42:45 - and that's it for our function so let's
42:47 - actually plot
42:48 - the rock curve that we obtained
42:51 - above with logistic regression
42:54 - and you should get the following so this
42:56 - is actually a perf
42:58 - perfect rock curve so it's hugging the
43:00 - upper left corner
43:02 - and we have an auc of one so that means
43:05 - perfect classification
43:09 - so now let's move on to our second
43:12 - algorithm which is
43:13 - linear discriminant analysis and let's
43:16 - see if the results are going to be
43:18 - different of course it cannot be better
43:21 - right
43:21 - so from sklearn dot discriminant
43:25 - analysis you are going to import
43:29 - linear discriminant analysis
43:35 - now feel free to pause the video and try
43:37 - it on your own because we are going to
43:38 - basically repeat the same steps as above
43:41 - only this time we are using a different
43:43 - algorithm
43:44 - so you can always pause the video and
43:46 - try it on your own as an exercise
43:49 - so lda is going to be equal to linear
43:51 - discriminant analysis so here i'm
43:53 - initializing the model
43:54 - then i'm going to fit the model with our
43:57 - train
43:58 - set so pass in stream and ytrain.gravel
44:02 - then i am going to get the probabilities
44:05 - from the lda model
44:06 - so lea dot predict prabha
44:10 - and you pass in x test
44:19 - and then you get the predictions and we
44:22 - use the same threshold as
44:23 - before 0.5 so
44:26 - it's actually going to be the y pro np
44:29 - dot where
44:30 - um y probe
44:35 - there we go if it's greater than 0.5
44:38 - we're just
44:39 - going to classify it as 1 and otherwise
44:42 - it will be 0.
44:47 - run this cell and awesome everything
44:50 - went well as again
44:51 - again you can ignore the warning on the
44:54 - screen
44:55 - so we're going to build a confusion
44:56 - matrix here
45:00 - so white test and
45:04 - y print lda
45:08 - and let's display the confusion matrix
45:10 - and as you can see perfect
45:12 - classification again so
45:15 - as an exercise we will still build the
45:17 - rock curve
45:18 - and show it uh simply to make sure that
45:21 - we get a rock of
45:22 - rock auc of one
45:26 - so get the false positive rate true
45:28 - positive rate and
45:30 - thresholds to be equal to rock curve
45:34 - passing y tests and bison the y
45:37 - uh probabilities here in this case
45:40 - y prob lda
45:47 - now display the rock auc of
45:50 - lda so first you assign it
45:55 - false positive very true positive rate
45:57 - and now we are
45:58 - ready to display it so rock a you see
46:01 - lda and we should get one and as as
46:04 - expected
46:05 - we indeed get one
46:11 - now we are going to plot the rock curve
46:13 - using the function we
46:14 - defined earlier and as you can see you
46:17 - get the exact same
46:19 - function which is again expected right
46:21 - because our confusion matrix was the
46:23 - same the rocket you see was the same
46:25 - the plot should be the same so again lda
46:28 - is a perfect classifier in this case and
46:31 - finally we are going to
46:32 - implement quadratic discriminant
46:34 - analysis again i strongly suggest that
46:37 - you pause the video
46:39 - at this point and really try to repeat
46:41 - the steps
46:42 - that we have done before using qda
46:46 - so as always we're going to import the
46:48 - model from sklearn so from
46:51 - sklearn.discriminant
46:52 - analysis import
46:55 - quadratic discriminant analysis
47:03 - now you set the model so you initialize
47:06 - it sorry so qda is
47:08 - quadratic discriminate analysis you can
47:11 - always press tab
47:12 - by the way for autocomplete you fit the
47:14 - model
47:15 - on your train set
47:20 - and then you get the probabilities
47:24 - like i said the steps are exactly the
47:26 - same it's just that we are using
47:27 - a different model so white prob
47:31 - qda is qda.predict
47:35 - prabha and you use the test set of
47:38 - course
47:42 - and then we get our classification so
47:45 - np.where
47:46 - use the same threshold so if it is
47:49 - greater than 0.5
47:51 - classify as one otherwise it is zero
48:00 - run the cell and ignore the warning
48:07 - then we're going to take a look at the
48:09 - confusion matrix
48:11 - so let's see if we also get a perfect
48:14 - classifier here with
48:15 - qda so confusion matrix you pass in a y
48:18 - test and
48:20 - the predictions
48:23 - displaying the confusion matrix as you
48:25 - can see we get the exact same as before
48:28 - so again qda is a perfect classifier
48:32 - for our data set
48:35 - now we're gonna plot the rock uh curve
48:38 - and get the a
48:39 - rock auc as well so just like before
48:42 - false positive rate true positive rate
48:44 - thresholds
48:44 - is going to be equal to rock curve pass
48:47 - in y test
48:48 - and the y probabilities y probe
48:51 - qda and then you use the false positive
48:55 - rate and true positive rate
48:56 - to get your rock auc
49:00 - so here i'm calling it rocket you see
49:01 - qda it's going to be equal to the auc
49:04 - pass in false positive rate and the true
49:06 - positive rate
49:07 - and now you can display the rock auc of
49:10 - qda
49:13 - and you get one perfect as expected now
49:16 - we are simply going to plot it
49:18 - to make sure that it looks like the
49:20 - other plots and you know that
49:22 - it will and it should and
49:25 - as expected uh perfect rock curve auc
49:29 - of one now let's talk about resampling
49:32 - and
49:33 - regularization resampling and
49:35 - regularization are two important steps
49:37 - that can significantly improve
49:39 - our model's performance and our
49:41 - confidence in the model
49:43 - specifically resampling helps us
49:45 - validate our model and we usually do so
49:47 - with cross validation regularization
49:51 - is used to prevent overfitting and we
49:52 - will cover both rich regression
49:54 - and lasso so first let's cover
49:58 - resampling methods resampling involves
50:01 - repeatedly drawing samples from a
50:03 - training set
50:04 - and refitting the model on each sample
50:06 - this allows us to gain more information
50:09 - than if we were fitting the model only
50:10 - once
50:13 - we can also test how the model would
50:15 - perform on unseen data
50:17 - without collecting new data that is
50:19 - important
50:20 - because a model in production will have
50:22 - to predict on data it has not been
50:24 - trained on
50:26 - cross-validation is a widely used method
50:29 - for resampling
50:30 - we use it to evaluate a model's
50:33 - performance
50:34 - and to find the best parameters for the
50:35 - model there are three ways
50:37 - we can do validation we can do have a
50:41 - validation set
50:42 - we can do a leave one out cross
50:44 - validation
50:45 - or we can use the method of k-fold
50:47 - cross-validation
50:49 - let's explore each one of them the
50:52 - validation set
50:53 - is the most basic approach we have a
50:56 - data set
50:56 - of endpoints and we randomly split the
50:59 - data set
50:59 - into a training set that you see in blue
51:02 - and a test set
51:03 - that you see in orange we fit the model
51:06 - on the blue set and make predictions
51:08 - using the orange set
51:11 - this has some drawbacks because the test
51:14 - error rate is variable depending on
51:16 - which observations were in each set
51:18 - since we are splitting randomly also
51:21 - only a small subset of data is used for
51:23 - training
51:24 - when ideally we want as much data as
51:27 - possible for training
51:29 - so instead we could use leave one out
51:32 - cross validation
51:33 - or loocv in this method only one data
51:37 - point
51:37 - in orange is used for validation and the
51:40 - rest is used for training
51:42 - you repeat the process for as many times
51:44 - as you have data points
51:46 - so in this case n times
51:50 - the error rate is approximated as the
51:52 - mean of errors for each run
51:55 - now this method has the benefit of
51:57 - having no randomness
51:59 - but it's not a viable option for very
52:01 - large data sets
52:03 - so now we introduce the k-fold cross
52:05 - validation
52:06 - this is by far the most common approach
52:09 - here
52:09 - we randomly split the data set into k
52:12 - groups or
52:13 - folds we use the blue set for training
52:16 - and the orange set for validation and we
52:18 - repeat the process
52:20 - k times now realize that
52:23 - l-o-o-c-v is a special case of k-fold
52:26 - where k is simply equal to n
52:29 - the number of data points usually we set
52:32 - k to 5 or 10
52:34 - and like i said this method is probably
52:35 - the best and most widely used
52:39 - in python we can perform cross
52:41 - validation like this
52:43 - here we see an example with linear
52:45 - regression we first initialize the model
52:47 - and then we can get the mean squared
52:49 - error for each fold
52:51 - notice that we are performing a
52:53 - five-fold cross-validation
52:55 - because the cv parameter is set to five
52:59 - then we can get the mean of all errors
53:02 - now notice that we report the negative
53:04 - of the mean because the algorithm uses
53:07 - the negative mean square error as a
53:10 - scoring system
53:11 - therefore we need to get the negative so
53:14 - that we bring it back
53:15 - to a positive value
53:19 - now let's move on to regularization
53:22 - models can sometimes overfit meaning
53:24 - that they will not generalize well
53:26 - and perform poorly on unseen data
53:29 - this brings me to the subject of bias
53:31 - variance trade-off
53:33 - on the left the model has a high bias
53:36 - and low variance and you see that it's
53:39 - not a very good fit
53:41 - on the right you see a model with high
53:44 - variance
53:44 - and low bias the model is overfitting
53:47 - and varying a lot and we will not give
53:50 - good predictions
53:51 - so we want to find a middle ground and
53:54 - prevent the models from
53:55 - overfitting and that's why we use
53:58 - regularization
54:00 - it will help us decorate our model to
54:03 - prevent
54:04 - overfitting here we will discuss ridge
54:07 - regression
54:07 - and lasso note that these methods are
54:10 - also called
54:11 - shrinkage methods
54:14 - we know that traditional linear fitting
54:17 - minimizes the
54:18 - rss the residual sum of squares
54:21 - with ridge regression we add another
54:23 - parameter to the optimization function
54:27 - here we add the sum of parameters
54:29 - squared with
54:30 - a coefficient lambda
54:34 - lambda is called a tuning parameter to
54:36 - find the best value of lambda
54:38 - we will use cross validation with a
54:41 - range of values for lambda
54:43 - the best value will be the one
54:44 - minimizing the test error
54:47 - with this method all predictors are kept
54:50 - and note that this is also called
54:52 - l2 regularization
54:55 - in python to use it we first initialize
54:57 - the
54:58 - ridge model then the lambda parameter
55:01 - is actually alpha with the library that
55:04 - we use in python
55:06 - in this code snippet we provide an array
55:08 - of different values for alpha
55:10 - going from 10 to the minus 15 to 20.
55:14 - then we will use five-fold
55:16 - cross-validation
55:17 - to use the best value of alpha for our
55:20 - model
55:21 - we'll get to apply this later on during
55:24 - the
55:25 - project example now with lasso we also
55:29 - add a new term to the optimization
55:31 - function
55:31 - and lasso is also called l1
55:34 - regularization
55:36 - here we add the sum of absolute values
55:39 - of all coefficients and we still have
55:41 - our tuning parameter
55:43 - lambda now if lambda is large enough
55:46 - some betas will go to zero meaning that
55:49 - some features will disappear
55:50 - and so feature selection can be done
55:53 - with lasso
55:55 - in python as you can see it is very
55:57 - similar to what we did with ridge
55:58 - only this time we initialize the lasso
56:02 - algorithm so that's it for the theory
56:04 - now
56:05 - let's apply these methods in a project
56:11 - all right let's open our jupiter
56:12 - notebooks in this exercise we will
56:14 - revisit a previous data set we used
56:17 - for linear regression so i always have
56:19 - my data folder
56:21 - and we'll use the advertising.csv
56:23 - dataset
56:25 - so we start off by importing pandas
56:27 - numpy and
56:28 - matplotlib so pandas as pd
56:31 - numpy as np and then
56:36 - matplotlib.pyplot as
56:38 - plt and of course do not forget your
56:41 - jupiter magic
56:42 - so that we can display our plots
56:46 - in the notebook then i will set the path
56:49 - to my data set and read in the data
56:54 - so data slash advertising dot csv
56:58 - and then you read the data set with
57:01 - pandas and we will display the first
57:03 - five rows
57:04 - of the data set using the dataframe.head
57:09 - method index call is equal to zero
57:12 - data ahead and there you go the first
57:15 - five rows of our
57:16 - data set now let's define a function
57:20 - that will allow us to plot the target
57:23 - against
57:23 - each feature so as you see the target
57:26 - will be sales
57:27 - and then we have three features tv radio
57:30 - and newspaper
57:32 - so i'm going to define scatterplot and
57:34 - pass in feature as a parameter
57:37 - now i will specify the size of the plot
57:39 - to make it slightly bigger so we can see
57:41 - it
57:42 - clearly on the screen plt.scatter
57:46 - data feature so this is the x-axis and
57:49 - then on the y-axis it will always be
57:52 - sales because that is our target
57:55 - and i will specify the color of the
57:56 - points i want them to be
57:58 - black i'm going to give it an x
58:01 - label so on the x axis this will be the
58:04 - money spent
58:05 - on whatever feature we are plotting
58:09 - so it will be money spent on tv ads or
58:12 - radio ads or
58:13 - newspaper ads
58:18 - and then i specify a label for the
58:20 - y-axis
58:21 - and in this case it will be our sales in
58:24 - thousands of dollars
58:31 - finally we simply display the plot so
58:34 - now we can
58:35 - run this function and pass in each
58:38 - feature
58:38 - so we can get a sense of how each
58:41 - feature is correlated
58:42 - with the target
58:46 - so tv radio and newspaper and once you
58:49 - run this cell
58:51 - you should see three different plots
58:54 - and as you can see newspaper does not
58:56 - seem to be very
58:58 - well correlated with sales
59:01 - so now let's define our baseline
59:04 - model and see how regularization will
59:07 - improve it
59:08 - so first i will import cross val score
59:13 - so model selection import cross
59:16 - underscore
59:16 - val underscore score
59:19 - and linear regression and these
59:23 - will be used for our baseline so our
59:25 - baseline model will be a very simple
59:28 - multiple linear regression
59:32 - so as a first step we will define our
59:35 - feature vector
59:36 - so in this case we are only going to
59:38 - drop the sales
59:39 - column
59:44 - and then we will define our target which
59:46 - will be sales
59:48 - so data sales dot values dot reshape
59:51 - minus one one perfect
59:55 - now we will fit the model so first
59:58 - initialize it linear regression
60:03 - and then we will calculate the mscs
60:06 - so we are going to use cross validation
60:09 - uh
60:10 - here calculate the mean squared error
60:12 - and then we are going to average
60:14 - those errors so you pass in the model x
60:17 - y the scoring we need to use negative
60:20 - mean
60:20 - squared error as required by uh
60:24 - this library and we do five-fold
60:27 - cross-validation
60:30 - so this will give us five different mean
60:32 - squared errors
60:34 - so then to get the mean we simply do
60:37 - np dot mean of those
60:40 - mses and now we will report
60:44 - the average mean squared error uh bring
60:47 - it back to being positive because
60:49 - remember they were negative
60:50 - and so we get 3.073
60:53 - approximately so let's see now how
60:57 - regularization can help us improve on
61:00 - our
61:01 - baseline
61:04 - so let's try ridge regression first
61:08 - regularization and then ridge regression
61:13 - perfect we will need to import a grid
61:16 - search
61:17 - to find the optimal value for our tuning
61:19 - parameter
61:20 - and of course we will need to import
61:22 - ridge so esky learn the model selection
61:25 - import grid search cv
61:28 - and from sklearn dot
61:31 - linear model we are going to import
61:35 - ridge simply like that
61:38 - awesome so we start off by initializing
61:42 - the model
61:42 - as always so ridge is going to be equal
61:44 - to ridge and now we will define a list
61:47 - of possible values
61:48 - for our tuning parameter so in this case
61:52 - uh the the parameter is called alpha in
61:55 - scikit-learn and now let's pass in a
61:57 - bunch of values
61:58 - to test and we will use cross-validation
62:01 - to find out which one is the best
62:03 - so we pass in one e to the minus 15
62:06 - minus 10 minus 8
62:07 - minus 4 minus 3 minus 2 and then we go
62:11 - to 1
62:11 - 5 10 and 20.
62:15 - feel free to use as many values as you
62:17 - want but as a starting point we'll use
62:19 - these ones
62:22 - so now to run our five fold cross
62:25 - validation
62:26 - we run grid search cv we pass in the
62:29 - model
62:30 - and we pass in the list of parameters
62:33 - the scoring will also be negative mean
62:36 - squared
62:37 - error so we keep this consistent with
62:40 - our baseline model
62:41 - and again fivefold cross validation
62:49 - now we can fit the model pass in your
62:52 - feature and your target that is awesome
62:56 - and it's done so now we can
63:00 - actually print out uh the best value for
63:03 - the parameters
63:04 - and we can print out as well the mean
63:07 - squared error
63:08 - so you can do that by finding the best
63:10 - params
63:11 - and then it's going to be the best um
63:16 - score exactly so when we print this out
63:19 - you see that the best value for alpha is
63:21 - 20. and
63:23 - let's not forget that our scores were
63:24 - negative so let's bring it back to
63:26 - positive
63:26 - it's 3.0722
63:30 - so it is a slightly lower msc
63:34 - so now let's try out lasso and see if we
63:36 - get an even better
63:38 - result so at this point you can try and
63:42 - pause the video to work it out on your
63:43 - own as the method will be very similar
63:45 - to what we have done
63:46 - with range so moving on
63:49 - uh we're gonna initialize lasso and set
63:52 - the tolerance to 0.05 so that it can
63:55 - converge
63:55 - in a reasonable amount of steps
63:59 - let's grab our list of parameters for
64:02 - alpha
64:03 - because this i'm simply going to reuse
64:06 - the exact same values
64:08 - to test during our grid
64:11 - and then we exactly reproduce what we
64:14 - did with ridge
64:15 - regression so lasso regressor
64:18 - is going to be equal to grid search cv
64:21 - you pass in your model
64:22 - lasso you pass in your parameters
64:26 - the scoring method will be neg mean
64:29 - squared error and again we do
64:33 - a cross validation of fivefold
64:37 - finally we can print out the best
64:40 - parameters and the best
64:42 - score so this will give us the value of
64:44 - alpha
64:45 - that gave the lowest mean squared
64:49 - error and of course you do that after
64:52 - you fit the model best params
64:54 - print lasso underscore regressor dot
64:58 - best underscore score underscore
65:01 - once we run this cell don't forget your
65:05 - negative
65:06 - once we run the cell there you have it
65:07 - the best value for alpha is one and we
65:09 - get a
65:11 - mean squared error of 3.036
65:15 - approximately and this is indeed the
65:16 - best
65:17 - score all right let's kick off this
65:19 - portion about decision trees with
65:21 - a bit of theory tree-based methods can
65:24 - be used for both classification
65:26 - and regression they involve dividing the
65:28 - prediction space into a number of
65:31 - regions the set of splitting rules can
65:34 - be summarized in a tree
65:35 - hence the name decision trees now a
65:38 - single decision tree is often not better
65:41 - than a linear regression logistic
65:43 - regression or lea
65:45 - that's why we introduce bagging random
65:47 - forests and
65:48 - boosting to dramatically improve our
65:50 - trees
65:52 - now before we move on we need to get
65:54 - familiar with a bit of terminology
65:57 - trees are drawn upside down the final
65:59 - regions
66:00 - are called leaves the point where split
66:03 - occurs is called
66:04 - a node and finally the segments that
66:06 - connect the nodes
66:07 - are called branches here is an example
66:10 - of
66:11 - basic decision trees with the leaves at
66:13 - the bottom and then
66:14 - the branches and you see the nodes where
66:17 - it splits
66:19 - now let's see how a regression tree
66:22 - works
66:23 - to create a regression tree we divide
66:25 - the predicted space
66:26 - into j distinct and non-overlapping
66:29 - regions
66:30 - then for each observation that falls in
66:32 - a region
66:33 - we predict the mean response value of
66:36 - that region
66:38 - each region is split to minimize the rss
66:41 - it uses a top-down greedy approach also
66:44 - called
66:44 - recursive binary splitting
66:48 - now why top down because all
66:50 - observations are in a single region
66:53 - before the split and why do we call this
66:56 - greedy
66:57 - that is because the best split occurs at
66:59 - a particular step to make the best
67:01 - prediction
67:02 - at that step instead of looking ahead
67:05 - and making a split that will give a
67:06 - better prediction later on
67:09 - now mathematically we define the pair of
67:11 - half planes
67:12 - like this and we seek j and s
67:15 - to minimize the rss of both planes
67:20 - however this may lead to overfitting and
67:23 - as you can see on the right
67:24 - that's why we need to sometimes prune
67:26 - the trees
67:27 - and use cross validation to prevent
67:29 - overfitting
67:32 - in python a regression tree is very
67:34 - simple to implement
67:35 - as you can see we import the model from
67:38 - the sklearn library
67:40 - we initialize it and then we fit on x
67:42 - train
67:43 - and y train
67:46 - now let's see how a classification tree
67:49 - works
67:50 - it is very similar to a regression tree
67:52 - but instead
67:53 - we predict the most commonly occurring
67:56 - class in a region
67:58 - also we cannot use the rss since we are
68:00 - dealing with
68:01 - categories so we must minimize the
68:04 - classification
68:05 - error rate the classification error rate
68:09 - is simply the fraction
68:10 - of training observations in a region
68:12 - that do not belong to the most common
68:14 - class
68:16 - however this is not sensitive enough for
68:18 - tree growing
68:21 - so instead we use the genie index which
68:24 - is a measure of total variance across
68:26 - all classes the gd index will be close
68:29 - to 0
68:30 - if the proportion is close to 0
68:34 - or 1 which makes it a good measure of
68:37 - node purity a similar rationale is
68:41 - applied to cross
68:42 - entropy which can also be used for tree
68:45 - growing
68:47 - now in practice we can simply import the
68:50 - decision tree classifier model
68:52 - initialize it and fit it on our data x
68:56 - and y now that the basics are covered
69:00 - let's move on to more advanced topics on
69:02 - decision trees
69:04 - bagging stands for bootstrap aggregation
69:07 - we know that bootstrap can compute the
69:09 - standard deviation of any quantity
69:12 - we also know that variance is a high in
69:14 - decision trees
69:16 - since they are prone to overfitting so
69:19 - bagging is a method to reduce variance
69:21 - and improve
69:22 - the performance of our algorithm
69:26 - bagging involves repeatedly drawing
69:28 - samples from the data set
69:29 - generating b different bootstrap
69:31 - training sets
69:33 - once all sets are trained we get a
69:34 - prediction for each set and we
69:36 - average those predictions to get a final
69:39 - prediction
69:40 - so mathematically we expect the final
69:43 - prediction like this
69:44 - and so you recognize that this is simply
69:45 - the mean of all
69:47 - b predictions this means that we can
69:51 - construct a high number of trees
69:53 - that overfit but by averaging their
69:55 - predictions
69:56 - we effectively reduce the variance and
69:59 - improve the performance
70:01 - in python as you can see it is very
70:03 - simple to apply bagging
70:05 - we can import the bagging classifier
70:07 - initialize it
70:09 - and then we fit it on our data
70:12 - now let's see how a random forest can
70:14 - also improve the
70:15 - quality of our predictions random
70:18 - forests
70:19 - provide an improvement over bagging by
70:21 - making a small tweak that decorates the
70:24 - trees
70:25 - again multiple trees are grown but at
70:27 - each split
70:28 - only a random sample of m predictor
70:31 - is a lot is chosen from all p predictors
70:35 - and the split is only allowed to use one
70:37 - of the
70:38 - m predictors now typically m
70:41 - is the square root of p
70:45 - now how is that a good thing well in
70:47 - bagging
70:48 - if there is a strong predictor it will
70:50 - likely be the top split
70:52 - and all trees will therefore be similar
70:54 - so variance will not be reduced
70:56 - once we average all predictions with
70:59 - random forest
71:00 - because we force a random sample of
71:02 - predictors for each split
71:04 - we avoid the situation also realize that
71:07 - if
71:07 - m is equal to p then it is just like
71:10 - bagging
71:12 - again you see that applying the random
71:14 - forest algorithm is very simple in
71:16 - python
71:17 - once imported we initialize the model
71:19 - and pass in the number of trees we would
71:21 - like to use
71:22 - in this case we use 100 then we can fit
71:25 - on x and y
71:28 - finally let's cover boosting boosting
71:31 - works in a similar way to bagging but
71:33 - trees are grown sequentially
71:35 - they use the information from previously
71:37 - grown trees
71:39 - this means that the algorithm learns
71:41 - slowly
71:42 - also the trees fit the residuals instead
71:45 - of the target
71:46 - so the trees will be small and will
71:48 - slowly improve
71:49 - the predictions there are three tuning
71:52 - parameters for boosting
71:54 - the number of trees b where if it is too
71:57 - large then it will overfit
71:59 - so use cross validation to find the
72:01 - right number of trees
72:03 - we have the shrinkage parameter alpha
72:05 - which is a small positive number that
72:07 - controls the learning rate
72:09 - typically we set it to 0.01 or 0.001
72:15 - and finally we have the number of splits
72:18 - in each tree
72:19 - which controls the complexity of the
72:21 - boosted ensemble
72:23 - typically a single split works best we
72:26 - also call that the
72:27 - interaction depth now of course i said
72:30 - that those
72:31 - there are three tuning parameters in
72:32 - boosting usually there are more than
72:34 - that
72:35 - but those are the three main ones that
72:37 - we can focus on
72:38 - when using the algorithm to improve its
72:40 - performance
72:43 - in python we apply boosting like this
72:46 - now in this module
72:47 - the learning rate is the shrinkage
72:49 - parameter
72:50 - and estimators is the number of trees in
72:54 - this case set to 100
72:55 - and the max depth is the interaction
72:59 - depth that we looked at earlier again
73:02 - once the model is initialized
73:04 - we fit it on our data so that's it for
73:07 - the theory now
73:08 - let's see how we can apply this in a
73:09 - project
73:14 - all right so let's get some code done i
73:16 - have a notebook open here
73:17 - also feel free to grab the data set in
73:19 - the description and put it
73:21 - in a folder called data and the data set
73:24 - here
73:24 - is about breast cancer so we are trying
73:27 - to
73:28 - identify patients with breast cancer
73:30 - from a
73:32 - simple blood test so we start off by
73:36 - importing our usual
73:37 - libraries so numpy as np we are going to
73:40 - import pandas as pd
73:42 - then matplotlib dot type plot
73:46 - as plt we will also need
73:50 - seaborn
73:54 - and today we are going to use
73:58 - the function plot confusion matrix as
74:01 - well
74:01 - this is going to be useful to evaluate
74:03 - our decision trees
74:05 - later on
74:08 - finally you use some jupiter magic to
74:10 - display your
74:12 - plots in the notebook
74:15 - so now let's read our data set uh i will
74:18 - simply
74:19 - define my data path in this case so it
74:21 - is in the folder data
74:22 - and the data set is called breast cancer
74:24 - dot csv
74:26 - then i will use pandas to read the data
74:29 - so data is going to be equal to
74:31 - pd.read.csv
74:32 - and i pass in my datapath feel free to
74:35 - use tab
74:36 - at any time to autocomplete
74:39 - and we will display the first five rows
74:41 - of the data set and there you go
74:43 - as you can see the first five rows of
74:45 - our data set perfect
74:48 - now we are going to check if our data
74:50 - set is balanced
74:52 - because we have we are in a
74:53 - classification problem so i want to make
74:54 - sure that
74:56 - we don't have too much of healthy
74:58 - patients or
74:59 - patients with breast cancer in the
75:01 - dataset that would make it
75:03 - imbalance so we use the count plot and
75:06 - as you can see the classes are fairly
75:08 - balanced here so we do not need to know
75:10 - to do some crazy manipulations in this
75:12 - case
75:14 - now it will be interesting to uh define
75:17 - a function to make violin plots that
75:20 - will allow us to see
75:21 - the distribution of each feature for
75:24 - both classes
75:25 - so it can give us some intuition about
75:27 - the data so for example maybe we will
75:29 - see that
75:30 - most of the healthy controls are younger
75:34 - so defining the function we will need x
75:36 - y and data as
75:37 - parameters and then i will enumerate
75:40 - each y so we will define
75:43 - a figure
75:47 - then i will set some parameters in this
75:49 - case i will i am simply setting the uh
75:52 - figure size i want it to be
75:55 - fairly large for you guys to see so i
75:58 - will set it to
75:59 - 11.7 and
76:02 - uh 8.27 i know i am
76:05 - very precise and then you will simply do
76:09 - a violin plot for each one so sns.violin
76:12 - plot
76:13 - x is equal to x y is the colon and data
76:16 - is equal to
76:17 - data that is perfect
76:20 - so now in this case uh the y is actually
76:24 - going to be data dot columns uh
76:27 - everything but the last column so i want
76:29 - so in this case the features are
76:30 - actually going to be y and
76:32 - x is going to be the target variable
76:36 - because i want to get the distribution
76:38 - of each feature for each class
76:41 - now we can run the function actually
76:43 - passing in
76:44 - our x y and data
76:47 - and you get the following plots so feel
76:49 - free to
76:52 - study those plots a little bit longer
76:55 - and get an intuition about the data set
76:57 - we are working with
77:02 - now we are going to check for null
77:04 - values to make sure that
77:06 - nothing is missing
77:09 - so for column in data dot
77:13 - columns i will print
77:16 - the name of the column so curly brackets
77:19 - call and then we will print
77:23 - the sum of null values so that is data
77:26 - call dot is null dot sum
77:30 - running this cell and you see that we
77:33 - have no
77:34 - null values in this data set that is
77:37 - amazing
77:38 - now we will start some pre-processing
77:40 - first i would like to do
77:42 - some label encoding on the target
77:45 - variable so that we bring it to
77:46 - one or zero so from scaling up
77:50 - pre-processing import
77:51 - label encoder we will initialize the
77:53 - label encoder
77:55 - and then we will fit transform that
77:59 - on the row classification so data
78:01 - classification
78:02 - is le.fit transform
78:07 - and you pass in data classification
78:14 - now to make sure that everything is
78:15 - right we'll display the first five rows
78:18 - and everything is right now the healthy
78:20 - control is
78:21 - zero and someone with breast cancer will
78:24 - have a label of
78:25 - one now we will split our data set into
78:30 - a training and
78:31 - test set so from scalar dot model
78:34 - selection we are going to import
78:36 - train test split
78:39 - so our target is of course the
78:42 - classification
78:44 - the values.reshape minus one one
78:49 - and our features is gonna be everything
78:51 - but
78:52 - classification so i'm simply going to
78:54 - drop the classification column
78:57 - and i'm going to specify the axis as
78:59 - well axis is equal to
79:00 - 1. awesome
79:06 - now actually splitting our data set uh
79:09 - note that our data set is fairly
79:11 - small in this case so i will use a
79:13 - smaller test size than we are used to in
79:15 - this case i will use only 10
79:17 - of it as a test uh
79:21 - size so you pass in x y test size is
79:24 - equal to 0.1
79:25 - and a random state of 42 so that we make
79:27 - sure that we get the same results
79:30 - so now let's build our baseline model
79:35 - so it's been our baseline model it will
79:37 - be a simple decision tree
79:39 - uh classifier so uh from
79:43 - sklearn dot tree
79:46 - we are going to import um decision tree
79:50 - classifier
79:53 - we will initialize the classifier so
79:58 - then the brackets then we will
80:01 - fit the model so call fit and then you
80:04 - pass in x-train
80:05 - and y-train
80:08 - and finally we will plot the confusion
80:11 - matrix
80:12 - so the confusion matrix will show us how
80:13 - many instances were misclassified
80:16 - so you pass in your classifier x test
80:19 - white test
80:20 - and i will specify that i want uh
80:23 - blue colors in this case so it's going
80:25 - to be a gradient of blue
80:27 - um i do not want the grid and i want to
80:29 - show the plot so as you can see we get
80:33 - this confusion matrix and you see that
80:34 - only three instances were misclassified
80:37 - in this case now i would like to show
80:40 - you a cool trick because you can
80:42 - visualize your decision tree
80:44 - with the function plot tree so from
80:47 - excalibur not tree you can import plot
80:49 - tree and then let
80:50 - let's see what it looks like so you're
80:52 - passing the classifier and i'm going to
80:53 - specify the max depth to five so we'll
80:55 - only see
80:56 - five different splits and there you see
80:59 - it
80:59 - so you can see the top split you can see
81:01 - which feature was used
81:03 - what what's the value of the split and
81:04 - you can also check for the genie index
81:07 - of each region so that is pretty cool
81:10 - feel free to you know not even pass in
81:12 - max that so you can visualize
81:14 - the entire decision tree if you want to
81:18 - so now let's try and improve on our
81:20 - baseline model and
81:21 - we will use bagging first
81:26 - so from sklearn dot ensemble we are
81:29 - going to import
81:31 - um bagging classifier
81:36 - we initialize the model as always so
81:39 - bagging
81:40 - clf is bagging classifier then we
81:43 - fit the model so that fit
81:46 - pass in your x train and y train
81:50 - in this case you need to do dot
81:54 - ravel
81:58 - and now we will plot the confusion
82:01 - matrix
82:04 - you pass in your classifier pass in
82:06 - x-test
82:07 - y-test and again i will specify gradient
82:11 - of
82:11 - blues to keep the plots consistent
82:14 - in the entire notebook
82:19 - i will remove the grid and
82:22 - show the plot
82:28 - and as you can see we only have one
82:30 - misclassified instance
82:32 - in this case so bagging is an
82:34 - improvement over our baseline
82:36 - now let's see how we can implement
82:38 - random forest at this point feel free to
82:40 - pause the video and try it on your own
82:42 - as the process will be very similar
82:44 - uh to bagging so to what we've done uh
82:47 - above
82:48 - so from sklearn.ensemble we're gonna
82:50 - import random forest classifier
82:52 - we initialize uh the model so random clf
82:56 - is going to be equal to random
82:57 - first classifier and in this case i will
82:59 - specify the number of trees i want a
83:01 - hundred trees
83:03 - then we fit the model we pass in our
83:05 - train test our train
83:06 - set sorry extreme and white train
83:09 - and then we will plot the confusion
83:11 - matrix so i'll just grab this code right
83:13 - here
83:14 - copy paste it down and then all i have
83:17 - to do is
83:18 - replace bagging clf with uh random
83:20 - forest
83:21 - cliff and i forgot to
83:25 - to ravel the right train sorry about
83:28 - that
83:28 - so i trained unravel and there you have
83:31 - it we have actually a perfect classifier
83:33 - with
83:34 - no instances that were misclassified
83:37 - that is pretty great however keep in
83:38 - mind this is a small data set
83:40 - it doesn't mean that our model is
83:41 - necessarily very good
83:44 - at this point and finally we're going to
83:47 - implement boosting
83:49 - so uh from sklearn
83:53 - dot ensembl we're going to import
83:56 - gradient boosting classifier
84:00 - gradient boosting classifier
84:06 - so as we have done before we initialize
84:09 - the model
84:11 - then we fit it
84:16 - so boost clf dot fit x train
84:19 - and y train and then we will
84:23 - plot the confusion matrix so grabbing
84:26 - the code again
84:28 - copy paste it below
84:30 - and remove a random clf and
84:34 - paste boost clf instead and again i
84:36 - forgot the ravel
84:37 - sorry about that guys my train dot ravel
84:41 - and then you get this following
84:43 - confusion matrix
84:44 - where only one instance is misclassified
84:46 - which is
84:47 - not better than random forest but better
84:49 - than our baseline
84:50 - alright let's cover some theory about
84:53 - support
84:54 - vector machine for classification we
84:56 - have seen quite a few algorithms
84:58 - such as logistic regression lda qda and
85:01 - decision trees
85:02 - support vector machine is another
85:04 - algorithm used for classification
85:07 - its main advantage is that it can
85:09 - accommodate non-linear boundaries
85:11 - between classes
85:14 - to understand svm we must first
85:16 - understand the
85:17 - maximum margin classifier like i said
85:20 - the maximum margin classifier is the
85:23 - basic algorithm from which
85:24 - svm extends it relies on separating
85:28 - different classes
85:29 - using a hyperplane in a p-dimensional
85:32 - space
85:32 - a hyperplane is defined as a flat athene
85:35 - subspace
85:36 - of dimension p minus 1. therefore in a
85:39 - 2d
85:40 - space the hyperplane will be a line and
85:42 - in a 3d space
85:43 - the hyperplane will be a flat plane
85:47 - the equation for a hyperplane is defined
85:49 - like this where p
85:50 - is the number of dimensions now here is
85:54 - an example of a hyperplane in 2d
85:56 - which is represented by the line
85:59 - if an observation satisfies the equation
86:02 - we just saw earlier
86:03 - then the point is on the line otherwise
86:06 - it is above or below the hyperplane
86:09 - now realize that if data can be
86:10 - separated perfectly
86:12 - then there is an infinite number of
86:13 - hyperplanes but we only just want
86:15 - one so that's why we use the maximum
86:18 - margin hyperplane
86:19 - or the optimal separating hyperplane
86:23 - to find it to do so we must calculate
86:26 - the perpendicular distance between each
86:29 - training point
86:30 - and the hyperplane that distance is
86:33 - called
86:33 - the margin then the optimal separating
86:37 - hyperplane will be the one with the
86:39 - largest margin
86:41 - here you see an example of a maximum
86:43 - margin hyperplane
86:45 - and you see the margin illustrated by
86:46 - the arrows
86:48 - notice that the plane depends only on
86:50 - the closest points
86:52 - known as support vectors and if those
86:55 - points move
86:56 - then the hyperplane will move
87:00 - but what if there is no clear separation
87:02 - between the classes
87:03 - as shown here well that's when svm
87:06 - is required as i mentioned svm is simply
87:09 - an extension of the maximal margin
87:11 - classifier
87:13 - this time it uses kernels to enlarge the
87:16 - feature space and accommodate for
87:18 - non-linear boundaries
87:19 - between classes a kernel is simply a
87:23 - function that quantifies the similarity
87:25 - of two observations
87:27 - the kernel can be a function of any
87:29 - degree but of course
87:30 - if the degree is greater than one then
87:33 - we add more flexibility to the boundary
87:37 - here is an example that we will
87:39 - implement later on during the coding
87:41 - portion of this
87:42 - section here the classes can be linearly
87:45 - separated
87:46 - so it's easy enough however notice the
87:48 - outlier on the left
87:50 - and we can use regularization to account
87:52 - for it or not
87:54 - and we'll see how that impacts the model
87:58 - here is another example that we will
88:00 - code and as you can see here the
88:01 - boundary is definitely
88:03 - not linear but svm does a pretty good
88:05 - job
88:06 - at finding a boundary and separating
88:08 - each class
88:09 - so that's it for the theory now let's
88:11 - move on to the coding project
88:13 - and generate those plots ourselves
88:17 - all right so prepare your notebooks and
88:19 - grab the data from the link in the
88:21 - description
88:22 - in this tutorial we're actually going to
88:24 - import
88:26 - five different data sets so you can see
88:28 - me i'm checking them right now so
88:30 - x6 data one two three and then spam test
88:34 - and spamtrain.mat
88:38 - so as always we start off by importing
88:40 - all the libraries
88:42 - that we will need throughout this
88:44 - project
88:45 - by the way these exercises are taken
88:47 - from the machine learning course by
88:49 - andrew angie
88:51 - i am simply solving them using python
88:54 - here
88:55 - by the way it's an amazing course and i
88:57 - will leave the link in the description
88:59 - if you want to check it out
89:00 - i definitely recommend it so we import
89:03 - numpy pandas matplotlib.pipeplot
89:06 - also matplotlib.cm and from scipy.i
89:10 - will import loadmat and finally
89:13 - mypluslibinline
89:14 - to show our beautiful graphs
89:17 - now i will simply define the path to all
89:20 - my datasets
89:22 - so path 1 is going to be for
89:25 - x 6 data 1 dot mat
89:29 - and then we'll simply do the same data
89:32 - path 2
89:33 - for x6 data 2 dot matt
89:41 - moving on to data path 3 for
89:44 - ex 6 data 3.
89:53 - and i will define the path for the spam
89:57 - train and test as well and we will use
90:00 - those data sets at the very end of this
90:02 - of the tutorial when we will build a
90:05 - classifier
90:06 - for spam using support vector machines
90:14 - training is done now data spam test is
90:16 - data
90:17 - slash spam train
90:20 - test sorry dot matt perfect
90:24 - uh now we will need to write a function
90:28 - to plot our data i decided to write a
90:32 - helper function here because
90:34 - we will be plotting quite often
90:42 - so we will need x y x label y label
90:45 - we'll use pause label and neg label
90:48 - because this is
90:49 - mostly classification right and we'll
90:52 - also pass in
90:53 - x min max y min and y
90:56 - max and set x is equal to
91:00 - none
91:08 - so first we're going to set the
91:10 - parameters of the plot
91:12 - we're actually going to set the figure
91:14 - size to make it nice and big
91:16 - for you guys so you can see it clearly
91:17 - on the screen
91:21 - i'm going to set it to 20 and 14.
91:37 - now i will specify what will be
91:39 - considered as
91:40 - positive so it's when the the label is
91:44 - going to be equal to one
91:45 - and negative is of course
91:48 - when it will be equal to zero
91:57 - then if axis is equal to none
92:01 - which by default it is
92:04 - then we're going to set axis equal to
92:10 - plt.gca
92:16 - then we're going to draw a scatter plot
92:18 - so axis dot
92:20 - scatter
92:25 - pass in the positives
92:33 - then you pass in the y for
92:36 - the positives i will specify the
92:40 - marker so i want dots so you can specify
92:43 - that by putting uh
92:45 - the letter o in strings then i will also
92:48 - specify
92:49 - a color at this point feel free to use
92:53 - any color
92:54 - you want
93:00 - s equal to 50 and the line width will be
93:04 - equal to
93:04 - two and finally the label
93:08 - will be the pause label because those
93:11 - are the
93:12 - positive data points now i will draw
93:15 - another scatter plot
93:17 - but for the negative samples
93:20 - so it will be fairly similar to the line
93:22 - that we wrote above only this time it's
93:23 - going to be for negative
93:26 - so nag column comma
93:29 - zero and then again so you pass in the y
93:32 - now so x snag colon comma 1
93:37 - the marker is still going to be a dot
93:42 - this time we're going to specify another
93:44 - color
93:45 - so that we can differentiate them easily
93:48 - on the plot
93:49 - so ffa 600
93:52 - s will be equal to 50 the line width
93:55 - will also be equal to 2
93:57 - and then you set the label equal to neg
94:00 - underscore label awesome
94:10 - now i will set the limits on the x-axis
94:14 - and then on the y-axis so of course
94:17 - we'll just pass in the
94:18 - x-min and x-max and then you do the same
94:20 - for the y-axis so while
94:23 - and then you pass in the array from
94:24 - y-min to
94:30 - y-max
94:37 - now let's set the label for x
94:43 - so set x label will be x label
94:47 - simply and we'll specify the font size
94:49 - i'm going to put it to 12
94:50 - and you do the same thing for y label so
94:53 - you set y label
94:54 - pass in my label and specify the font
94:56 - size as 12
94:57 - as well
95:00 - then we will specify the position for
95:03 - the legend
95:04 - so acts as a legend is a box
95:08 - sorry is it b box
95:13 - to anchor equal to
95:17 - one comma one and fancy box
95:21 - we'll put equal to true
95:25 - so let's start the coding portion by
95:28 - exploring the
95:28 - effect of regularization on svm
95:32 - and we'll start off this time with a
95:34 - small regularization parameter
95:37 - so first let's see what our data set
95:40 - looks like
95:41 - so data one is load matte the data path
95:45 - number one
95:50 - x will be data x and the y
95:54 - will simply be data y
95:58 - and now we will plot the data set
96:02 - so we use our function plot data pass in
96:04 - x pass in y
96:06 - the label will be x and y
96:11 - then the positive way label we'll simply
96:14 - call it positive
96:15 - the negative will call it negative
96:19 - we want the plot to be from 0 to 4.2
96:22 - and then from 0 to five
96:25 - and i made a mistake
96:29 - because it should be data one not data
96:32 - all right
96:32 - and you see this following plot so as
96:35 - you can see the positives in dark blue
96:37 - the negative is in
96:38 - yellow and you see that this data set is
96:40 - clearly
96:41 - linearly separable and you also notice
96:43 - this outlier
96:44 - on the left
96:49 - so now let's see um with a small
96:52 - regularization parameter what will
96:53 - happen to that
96:54 - outlier if it will be
96:57 - classified correctly or not so we start
97:00 - off by importing
97:01 - svm from sklearn and we'll set the
97:04 - regularization parameter to 1 for now
97:08 - so clf will be equal to svm.svc
97:14 - the kernel will be linear because as you
97:17 - can see we can use a
97:19 - straight line to separate the classes c
97:21 - will be equal to one
97:23 - and then you specify the decision
97:25 - function shape to
97:26 - obr
97:31 - then you fit the model on your data
97:41 - and now we will plot the data as well
97:44 - as the boundary
97:48 - so here actually we can just
97:51 - grab this line from the previous cell
97:55 - because it will be exactly the same
97:56 - we're just plotting
97:58 - the same data again and now we'll plot
98:01 - the boundary on top of this data
98:04 - sorry i accidentally ran this cell
98:08 - but now let's actually plot uh the
98:10 - boundary
98:12 - or the hyperplane
98:16 - so we start off by specifying x1 and x2
98:19 - to be a numpy dot mesh grid
98:24 - and then we pass in np arrange so we
98:27 - want
98:28 - from 0 to five
98:32 - and from with the steps of 0.01
98:38 - and np range oh
98:41 - sorry this should not be here this
98:42 - should be inside
98:44 - the big bracket of mesh grid so np range
98:48 - from 0 to 5 as well with steps of 0.01
98:53 - so i'm taking small steps here to make
98:55 - sure that we plot
98:56 - um a smooth hyperplane as smooth as
99:00 - possible
99:02 - then zed will be the prediction pass
99:06 - in np dot c underscore
99:09 - x underscore one dot ravel
99:14 - and do the same with x2 dot ravel
99:24 - then zed is going to be equal to z dot
99:27 - reshape
99:28 - x1 dot shape
99:32 - and now we are ready to plot the
99:35 - hyperplane
99:36 - so plt.contour you pass in x underscore
99:39 - one
99:40 - x underscore two pass in
99:43 - zed pass in
99:46 - uh zero comma five
99:50 - and the colors will be uh black
99:53 - so b and there is a mistake we have a
99:57 - warning here no contour levels were
99:59 - found within the data
100:00 - uh range and that is
100:04 - because okay here's the mistake
100:07 - shouldn't be a comma
100:08 - it should be a point so 0.5 perfect
100:12 - and you see this spot here with our
100:14 - hyperplane which is a straight line
100:16 - in blue and we have and you can see here
100:20 - that
100:20 - the outlier was not taken into account
100:24 - and is in this case
100:25 - misclassified
100:30 - so now let's see what happens if the
100:32 - parameter is
100:33 - very high so feel free to pause the
100:36 - video
100:37 - and try it on your own as an exercise
100:39 - because the code will be
100:41 - very similar to the previous cell
100:47 - so here we are going to use a large
100:50 - regularization parameter we'll set it to
100:52 - c
100:52 - equal to 100 and actually i simply gonna
100:55 - grab
100:55 - everything from this cell here and just
100:57 - copy paste it below because the code is
100:59 - exactly the same we are simply changing
101:03 - the value of the hyper parameter so we
101:05 - don't need to import
101:07 - svm again and we'll simply use c
101:10 - equal to 100 and see what happens next
101:15 - so as you can see now the hyperplane
101:17 - shifted to account for the outlier
101:20 - but in this case we are likely
101:22 - overfitting which means that the model
101:24 - will not generalize well
101:25 - so ideally we will go with the previous
101:28 - model
101:28 - that we built so now let's try
101:32 - an example with a non-linear boundary
101:37 - so svm with non-linear boundary
101:40 - so for that we'll need our second data
101:43 - set
101:43 - so data2 is load matte data path
101:46 - underscore 2 and like before we'll
101:50 - specify the x and y
101:52 - and plot our data set so x2 will be
101:55 - data 2 x y2 will be data 2
101:59 - y
102:05 - and now we will use our helper function
102:07 - to plot
102:08 - the data so pass in x2 and y2
102:11 - again the label will be simply x and y
102:14 - for the class labels we'll simply use
102:17 - positive
102:17 - and negative and now we will set
102:22 - the limits on the x-axis so zero
102:25 - to one and on the y-axis it's going to
102:27 - go from 0.38
102:29 - to 1. and there you go
102:32 - so as you can see now clearly a
102:35 - non-linear boundary but the classes
102:38 - seem to be separable so let's see
102:42 - how svm will be able to do that
102:46 - for this example we will use a radial
102:48 - basis function for the kernel
102:50 - here we must define gamma which is a
102:52 - parameter that specifies how far the
102:54 - influence of a data point reaches
102:56 - a low value means very far and a high
102:59 - value means
103:00 - close and you can express gamma
103:03 - in function of sigma
103:06 - so let's define sigma as being equal to
103:10 - 0.1
103:13 - and then gamma will simply be 1 over
103:17 - 2 sigma squared
103:23 - so one over
103:26 - open brackets two times sigma
103:30 - star star squared perfect
103:38 - then our classifier will be equal to
103:41 - svm.svc
103:43 - we pass in the kernel and this time is
103:45 - going to be equal to rbf
103:47 - so for radial basis function gamma will
103:50 - be equal to gamma
103:52 - we set the regularization parameter to 1
103:55 - and the decision function shape will
103:57 - again be equal to
104:04 - ovr
104:06 - next we fit the classifier to
104:09 - our data so x underscore 2
104:12 - and y underscore 2 not revel
104:20 - then we will plot the data set so again
104:22 - just grab this line from the previous
104:24 - cell
104:25 - and paste it under because we are simply
104:28 - plotting the same
104:29 - scatter plot as before and now we will
104:32 - plot the uh hyperplane so the actual
104:36 - boundary
104:37 - from uh the svm algorithm
104:41 - so again as before x underscore one x
104:43 - underscore two
104:45 - will be a mesh grid you range from 0 to
104:48 - 1
104:48 - with bounds of width steps sorry of 0.03
104:52 - so here the steps are going to be
104:54 - slightly smaller because we are drawing
104:56 - a non-linear boundary and again it's
104:58 - just to make it as smooth as possible
105:02 - and from 0.38 to 1 for y
105:07 - then z will be equal to the prediction
105:13 - pass in np dot c underscore
105:16 - this is actually just to stack uh the
105:18 - predictions
105:19 - by the way so x underscore one dot ravel
105:23 - x underscore 2. ravel
105:31 - then we will reshape zed so that don't
105:34 - reshape
105:35 - we'll take the shape of x one so x
105:37 - underscore one dot
105:39 - shape
105:42 - and now we are ready to plot the
105:44 - hyperplane
105:45 - so plt.contour pass in x1
105:48 - pass in x2
105:53 - 0.5 and then colors will be equal to
105:56 - blue and now there is a
105:59 - big mistake input z must be 2d
106:03 - and easy enough i forgot to pass in z so
106:06 - x1 x2
106:07 - z and then the array 0.5 color is equal
106:10 - to blue
106:11 - and there you have it we can see in blue
106:14 - the
106:15 - uh boundary that svm uh predicted for us
106:19 - and you see he's doing a pretty decent
106:20 - job at separating everything you see you
106:23 - have a few points misclassified here and
106:25 - there
106:25 - but otherwise a good non-linear boundary
106:31 - but now let's explore a situation where
106:33 - the data is not
106:34 - easily separable so for that we'll
106:37 - explore the
106:38 - third data set so load matte data path
106:41 - underscore three
106:46 - as before x uh sorry about that
106:49 - x underscore three will be equal to data
106:52 - three x y underscore three will be equal
106:56 - to data three
106:58 - y and now we will plot the data
107:03 - using our helper function so plot data
107:06 - we pass in x3 y3
107:10 - the labels will be the same so x and y
107:14 - positive and negative for
107:18 - the labels
107:21 - and we want to plot from minus 0.55 to
107:24 - 0.35
107:25 - and from minus 0.8 to 0.6
107:31 - so that makes the data points nice and
107:33 - centered and as you can see now there is
107:34 - a clear
107:35 - overlap between both classes so there is
107:38 - no clear boundary
107:40 - between each class so this is when we
107:42 - need to use cross validation in order to
107:45 - find the best parameters
107:46 - for the best boundary
107:52 - so i will specify a list of possible
107:54 - values
107:55 - for sigma so we'll go from zero point
107:59 - zero one
108:00 - point zero three point one point three
108:02 - one three 10 and
108:03 - 30 and we'll do the same for the
108:06 - regularization parameter
108:08 - c so the same values 0.01.03
108:12 - 0.1.3
108:15 - and then 1 3 10
108:18 - and 30. perfect
108:23 - now i will initialize an empty list of
108:27 - errors and a limited list
108:30 - for sigma and c
108:34 - so for each value in sigma
108:40 - and then for each value for c
108:45 - we will define a classifier and we will
108:48 - fit it so clf
108:49 - is going to be equal to svm.svc the
108:52 - kernel
108:52 - will be rbf again because
108:58 - the boundary is likely non-linear
109:01 - gamma will be equal to uh
109:05 - 1 over 2 times each
109:09 - squared
109:13 - and then after two brackets we pass in
109:16 - the value for c
109:17 - which is going to be each underscore c
109:20 - and the decision function
109:23 - shape will be equal to ovr
109:28 - perfect then we will fit it to our data
109:33 - so x underscore three y underscore three
109:40 - unravel
109:42 - and we will append uh the errors
109:46 - to our list we defined earlier so we're
109:49 - going to span clf.score
109:52 - on data 3 x
109:56 - val and data three
110:00 - y vowel
110:05 - dot ravel
110:15 - and then we're going to append to sigma
110:18 - c
110:20 - the value for sigma and for
110:23 - c as a tuple now running this
110:26 - i have a mistake that's because
110:30 - singa should actually be sigma
110:35 - and i still wrote simga at the very end
110:39 - yeah right here simga c is actually
110:42 - sigma c
110:43 - perfect so the loop red it is finished
110:45 - so now we can see
110:47 - uh what value for uh c and for sigma
110:51 - is the best so index will be np.r
110:54 - max pass in errors
110:59 - sigma max and c max will be equal to
111:05 - sigma underscore c
111:09 - at that index
111:15 - and now we can print out the values
111:20 - so the optimal value
111:23 - of sigma is
111:27 - colon squiggly brackets sigma
111:31 - underscore max and we'll print the
111:34 - optimal value for c
111:36 - as well
111:51 - and as you can see we get an optimal
111:52 - value of 0.1 for sigma
111:55 - and 1 for c so now we can fit an svm
111:58 - algorithm
112:01 - so we set sigma equal to 0.1 gamma
112:04 - is going to be equal to 1 over 2 times
112:07 - sigma squared
112:08 - then we will pass in those parameters to
112:10 - our classifier
112:12 - so the optimal classifier is
112:15 - svn.svc the kernel will be
112:19 - rbf
112:23 - gamma is equal to gamma
112:26 - c is equal to 1 and the decision
112:29 - function shape
112:30 - is ovr
112:40 - now we will fit our classifier to our
112:42 - data
112:43 - so x underscore three minus score three
112:46 - unravel
112:51 - then we will plot the data so again just
112:53 - go back up let's grab
112:55 - this line here and paste it
112:59 - back in our current cell
113:03 - and as before we will now
113:07 - find the points for the boundary and we
113:09 - will plot it so x1 and x2
113:11 - is np.mesh grid pass in
113:14 - np range from
113:18 - negative 0.6 to 0.4 and we'll take steps
113:21 - of 0.004
113:23 - to make it smooth and np range
113:27 - from negative 0.8
113:31 - to 0.6 in steps of
113:36 - 0.004 that is then going to be the
113:39 - optimal
113:41 - clf dot predict
113:44 - and we will stack the prediction so np
113:47 - dot c underscore
113:48 - you pass in x underscore one dot ravel
113:52 - and x underscore two the unravel
113:57 - reshape z z dot reshape
114:01 - is x underscore one
114:04 - dot shape and now we will plot
114:08 - the boundary so plt on contour
114:13 - pass in x1 pass in x2
114:19 - pass in z 0.5 in brackets and the color
114:23 - will be blue and the mistake again here
114:25 - is i wrote
114:26 - symga instead of sigma very sorry about
114:28 - that guys hopefully
114:29 - we're not making the same mistakes as i
114:31 - am and as you can see here we have
114:34 - our best boundary found from cross
114:37 - validation
114:38 - with support vector machine so it's
114:40 - actually not that bad
114:45 - and finally let's use svm for spam
114:48 - classification for our
114:50 - emails right so spam train we're simply
114:54 - going to load
114:54 - the data set called data path
114:58 - and i called it spam train
115:04 - then spam test will be equal to load mat
115:06 - and you pass in
115:07 - data spam test
115:14 - we will set the regularization parameter
115:16 - to 0.1
115:18 - and then x train will be
115:21 - spam train and we pass in x
115:25 - y train will be spam train
115:29 - and we want the column y
115:36 - then x test will be spam test
115:41 - pass in x y test will be
115:44 - spam test passing y
115:51 - our classifier so clf underscore spam
115:54 - will be again svm.svc
115:58 - we will start with a linear kernel see
116:00 - how that performs
116:02 - so kernel is going to be linear the c
116:04 - parameter
116:05 - will be equal to c we set it above
116:10 - and the decision function shape will be
116:12 - equal to
116:13 - ovr
116:22 - then we fit our classifier to the
116:26 - train set so we pass in x-strain and
116:28 - y-train
116:29 - unravel
116:37 - and then we will calculate the accuracy
116:41 - so clf underscore spam dot score
116:47 - and we will score against spam train
116:50 - x and spam train
116:55 - y that ravel
117:03 - and then we will test for the test
117:05 - accuracy
117:06 - so again cliff underscore spam dot
117:10 - score and you pass in uh
117:13 - x test and y test
117:17 - dot revel
117:22 - finally we will print out those scores
117:27 - so the training accuracy
117:32 - squiggly brackets pass in train
117:35 - underscore
117:36 - acc times 100
117:40 - so we have it as a percentage already
117:43 - and we do the same for the test accuracy
117:57 - running everything uh we have a key
117:59 - error for
118:00 - x yes that's because here it should be x
118:03 - test
118:04 - and y test sorry about this little
118:07 - mistake
118:08 - so if we run this cell now our model
118:12 - is training and fitting
118:15 - and you get a training accuracy of 99.8
118:18 - percent
118:19 - and a test accuracy of 98.9
118:22 - which is very good moving on now to
118:25 - unsupervised learning let's cover some
118:27 - theory unsupervised learning is a set
118:31 - of statistical tools for scenarios in
118:33 - which we have
118:34 - features but no targets
118:37 - this means that we cannot make
118:38 - predictions instead we are interested in
118:41 - finding a way to visualize data
118:43 - or discovering a subgroup of similar
118:46 - observations
118:48 - unsupervised tends to be a bit more
118:50 - challenging because the analysis is
118:52 - subjective
118:53 - also it's hard to assess if the results
118:55 - are good or bad since there is no true
118:57 - answer
118:59 - in this section we will mainly focus on
119:01 - two techniques
119:02 - which are principal component analysis
119:04 - or pca
119:05 - and we will take a look at clustering
119:07 - algorithms
119:09 - let's cover pca first pca is a process
119:13 - by which
119:14 - principal components are computed and
119:16 - used to better understand data
119:18 - they can also be used for visualizations
119:22 - now what is a principal component well
119:24 - suppose you want to visualize
119:26 - n observations on a set of p features
119:29 - you could do a 2d plot
119:31 - for each two features at a time but
119:34 - that's not very efficient
119:35 - and unrealistic if p is very large
119:39 - with pca you can find a low dimensional
119:42 - representation of the data set
119:44 - that contains as much of the variance as
119:46 - possible
119:47 - that means that you will only consider
119:49 - the most interesting features
119:51 - since they account for the majority of
119:53 - the variants
119:55 - and therefore a principal component is
119:57 - simply the normanized
119:58 - linear combination of a feature that has
120:01 - the largest variance
120:03 - you see the equation here and that
120:05 - should remind you a bit of linear
120:06 - regression
120:08 - also this equation is for the first
120:10 - component
120:11 - the next one will be in a direction
120:13 - perpendicular to the first one
120:16 - and the third component would be
120:17 - perpendicular to the first two
120:19 - principal components in this equation
120:22 - here
120:22 - phi is referred to as the loadings
120:27 - here's an example of how you can apply
120:29 - pca in
120:31 - python in this case actually we are
120:33 - trying to visualize the iris
120:35 - data set in 2d this is something that we
120:38 - will
120:39 - apply later on during the coding portion
120:42 - so this data set contains more than two
120:44 - features
120:45 - for each species of iris so using this
120:49 - snippet we can initialize pca
120:52 - and then specify that we only want the
120:54 - first two principal
120:55 - components and then you can see here
120:58 - that we can
120:59 - uh find the explained variance ratio
121:02 - and then use that to plot a
121:06 - 2d figure of the features of this data
121:09 - set
121:09 - which is what you see here so as you can
121:11 - see we can plot the transformed data
121:13 - and see how each species of iris are
121:16 - different or
121:17 - separable from one another so that's it
121:20 - for pca now let's take a look at
121:22 - clustering
121:22 - methods clustering is a set of
121:26 - techniques for finding
121:27 - subgroups or clusters in a data set
121:30 - this helps us to partition the data into
121:32 - observations that are similar to one
121:34 - another
121:35 - an application of that is for example
121:37 - for market segmentation
121:38 - in the context of marketing we will
121:41 - first explore key means clustering
121:43 - which partitions data in a specified
121:45 - number of k
121:46 - clusters and we will also look at
121:48 - hierarchical clustering
121:50 - which does not need a specific number of
121:52 - clusters instead we can generate a
121:54 - dendrogram
121:55 - and see the clusters for all possible
121:58 - number of clusters
122:00 - but first let's focus on k-means
122:04 - this method simply separates the
122:05 - observations into k
122:07 - clusters and we must provide that number
122:10 - it assumes that each observation belongs
122:12 - to at least
122:13 - one of the k clusters and that the
122:15 - clusters do not overlap
122:17 - it is important to note that the
122:18 - variation within each cluster
122:20 - is minimized here you can see an example
122:23 - of how the number of clusters will
122:25 - affect
122:25 - how the data is partitioned feel free to
122:28 - pause the video if you want to study
122:30 - this
122:30 - a little bit longer now clustering is
122:33 - achieved by minimizing the sum
122:35 - of the squared euclidean distance
122:36 - between each observation in a cluster
122:39 - you can see the equation here of the
122:41 - euclidean distance
122:43 - and we wish to minimize it to do so the
122:46 - algorithm first starts by
122:48 - randomly assigning each observation to a
122:51 - cluster
122:52 - then for each cluster a centroid is
122:54 - computed which is a vector representing
122:56 - the mean
122:57 - of the features in the cluster then each
123:00 - observation is assigned to the cluster
123:02 - whose centroid is the closest the two
123:05 - steps above are repeated
123:06 - until the cluster assignment stops
123:08 - changing
123:10 - now note that k-means will find a local
123:12 - minimum
123:13 - therefore it highly depends on the
123:15 - initial cluster assignment
123:17 - so make sure to run the algorithm
123:19 - multiple times to see if you always get
123:21 - the same results in the coding section
123:24 - of the tutorial we will use k-means
123:26 - clustering to perform
123:28 - color quantization more on that later on
123:31 - this process allows us to take a picture
123:33 - and reduce the number of colors
123:35 - so in this code snippet here we specify
123:38 - that we want only 64 colors
123:40 - in the picture then we can apply the
123:42 - algorithm on the image
123:43 - and output a modified image with only 64
123:47 - colors
123:49 - the output will look like this of course
123:52 - because we use k-means we can specify
123:53 - any number of clusters
123:55 - to group the most similar colors
123:56 - together so in this case we use 64
123:59 - but later on in the coding portion we
124:00 - can use 10 28
124:02 - whatever number we want
124:05 - now let's take a look at hierarchical
124:08 - clustering
124:10 - as i mentioned the potential
124:12 - disadvantage of k-means is that you must
124:14 - specify the number of clusters
124:15 - and sometimes you simply don't know how
124:17 - many clusters you need
124:19 - this is when hierarchical clustering
124:20 - comes in because you do not need to
124:22 - specify the number of clusters
124:24 - the most common type of hierarchical
124:26 - clustering is called
124:27 - agglomerative clustering it generates a
124:30 - dendrogram from the leaves
124:32 - and the clusters are combined into
124:33 - larger clusters
124:35 - up to the trunk here is an example
124:39 - of gender grams we see the individual
124:41 - observations at the bottom
124:43 - and they are combined into larger
124:44 - clusters as you move up
124:47 - in the y-axis
124:50 - the algorithm is fairly easy to
124:52 - understand it starts by defining a
124:55 - dissimilarity measure between each pair
124:57 - of observations
124:58 - and it assumes that each observation
125:00 - pertains to its own cluster
125:02 - note that in this case the dissimilarity
125:04 - measure is usually
125:06 - the euclidean distance then
125:09 - the two most similar clusters are
125:11 - combined so that there are n
125:12 - minus one clusters the next two are
125:15 - combined resulting in
125:16 - n minus two clusters and that is
125:19 - repeated until
125:20 - all observations fall in one big cluster
125:24 - now although simple how do we define the
125:27 - similarity measure
125:28 - well that depends on the type of linkage
125:30 - and there are four
125:31 - types complete single average
125:34 - and centroid complete is
125:38 - also called maximal inter-cluster
125:40 - dissimilarity
125:41 - so it computes the pairwise the
125:43 - similarities in clusters
125:45 - a and b and it records the largest one
125:49 - with single it's the opposite and we
125:51 - talk about the minimal
125:52 - inter-cluster dissimilarity so here the
125:55 - smallest
125:56 - of the dissimilarities is recorded and
125:59 - this can mean that single observations
126:01 - are fused one at a time
126:04 - then we have average as the name
126:06 - suggests the average of the pairwise
126:07 - dissimilarities
126:08 - is recorded and finally we have
126:12 - centroid which computes the
126:13 - dissimilarity between the centroids
126:15 - of cluster a and b this is sometimes a
126:18 - problem
126:19 - as smaller clusters can be more similar
126:21 - to a larger one
126:22 - than to their individual clusters which
126:24 - can lead to inversions
126:26 - in your dendrogram complete average and
126:29 - centroid are definitely the most popular
126:31 - types of linkage
126:32 - note that the final dendrogram highly
126:34 - depends on the type of linkage you
126:36 - select
126:38 - as you can see here average and complete
126:41 - are
126:42 - quite similar to one another but with
126:44 - single leakage the dendrogram is quite
126:46 - unbalanced and that's why this method
126:48 - is not used often so that's it for the
126:51 - theory
126:51 - now let's get coding
126:54 - let's apply what we learned in python
126:56 - now these exercises
126:58 - are available as examples on the sklearn
127:00 - website i am simply reworking them a bit
127:02 - or explaining them here
127:03 - the links are in the description and the
127:05 - complete notebook on github is also in
127:08 - the description down below
127:10 - so we'll start off by importing some
127:12 - libraries we will need numpy
127:14 - we will also need matplotlib.pyplot
127:17 - as plt and finally
127:20 - from sklearn.utils we will import
127:24 - shuffle so let's kick off this tutorial
127:28 - with
127:28 - a clustering we will do color
127:30 - quantization with k-means
127:32 - which is a technique to reduce the
127:34 - number of colors of an image
127:36 - while keeping the integrity of the image
127:41 - so to do that we will learn we will need
127:44 - from sklearn.datasets
127:46 - import load underscore sample underscore
127:49 - image
127:50 - and from sklearn dot cluster we will
127:53 - import k-means
127:57 - now after importing our libraries we
127:59 - will load the image of a flower
128:03 - so the flower will be equal to load
128:05 - sample image
128:07 - and we will pass in the name of the
128:09 - image in this case it is
128:10 - flower dot jpeg
128:15 - now we need to convert to floats and
128:18 - divide by 255
128:21 - because colors are expressed as rgb
128:24 - right red green and blue
128:25 - with values from 0 to 255
128:28 - so we need to normalize that so that the
128:30 - image displays correctly with matplotlib
128:33 - so this is what we are doing here so we
128:36 - convert two floats and we divide by 255
128:38 - to normalize everything
128:40 - finally we can show the image with
128:42 - plt.mshow
128:43 - and we pass in flower
128:48 - now i have made a mistake here uh the
128:51 - name
128:51 - np is not defined that's because i did
128:54 - not import numpy as np
128:56 - sorry about that so after re-running
128:58 - this cell
129:00 - and re-running this cell here we finally
129:03 - get
129:04 - the picture of our flower and this is
129:06 - what you should get
129:11 - now we will change the image to a 2d
129:13 - matrix
129:16 - so width height and depth will be equal
129:19 - to
129:20 - original shape which is tuple of flower
129:23 - dot shape here uh d is the depth
129:26 - will be three uh because as i
129:30 - as i explained the earlier each layer
129:32 - will correspond to
129:33 - either red green or blue so
129:37 - three values in this case and now we
129:39 - reshape it
129:40 - so image array is equal to np dot
129:42 - reshape
129:44 - flower and then we'll reshape with the
129:47 - width times the height
129:49 - and the other dimension will be the
129:51 - depth awesome
129:52 - now we will reduce the number of colors
129:54 - to 64
129:56 - by running the k-means algorithm where k
129:59 - will be set to
130:00 - 64.
130:04 - so our image sample will be equal to
130:09 - shuffle the image array
130:13 - we'll give it a random state equal to 42
130:15 - so that the
130:16 - uh results are constant whenever we
130:20 - rerun the cell and we'll take the first
130:22 - 1000
130:26 - samples
130:28 - now we will fit the k-means algorithm
130:35 - and we set here the number of colors as
130:37 - i said this will be equal to
130:38 - 64. then k means
130:42 - will be equal to k means we initialize
130:46 - the model we pass in the number of
130:48 - clusters
130:49 - which is the same as the number of
130:50 - colors in this case
130:52 - and again the random state equal to 42
130:55 - because as you know from
130:56 - the theory part um k-means starts by
131:00 - randomly assigning
131:01 - uh each observation to a cluster so we
131:04 - keep the random state
131:05 - equal to 42 to give the same results
131:08 - every time and then we simply fit
131:11 - the algorithm
131:21 - then we get the indices for each color
131:24 - for the full image
131:25 - that will be useful when we need to
131:27 - reconstruct the image right
131:29 - so each pixel in the 2d array will be
131:32 - assigned to a certain cluster
131:33 - and that will help us to bring back the
131:35 - color and rebuild the image
131:37 - so it's simply the labels which is the
131:39 - prediction from the k-means
131:44 - now we need to write a function to
131:45 - rebuild the image
131:47 - so like i said each pixel is assigned to
131:49 - a cluster which corresponds to
131:51 - a specific color
132:01 - so we define reconstruct underscore
132:04 - image and we will need as parameters
132:07 - the cluster centers we'll need the
132:10 - labels
132:11 - and we pass in the width and the height
132:13 - of the picture
132:18 - so d will be equal to the cluster
132:22 - centers
132:23 - dot shape and we take d
132:28 - at index one
132:31 - then the image will be simply an array
132:33 - of zeros
132:35 - in this case and the shape will of
132:37 - course be the width
132:38 - the height and the depth
132:45 - the label index will start at zero
132:51 - and then for i in the range of the width
132:55 - and for j in the range the height
133:05 - we write that image
133:08 - at index i j so this is the coordinates
133:11 - in the 2d matrix will be equal to
133:16 - the cluster centers
133:21 - at labels and that itself will be
133:25 - at the label index
133:31 - and then we increment the label index
133:34 - so plus equal one
133:38 - and finally we return the reconstructed
133:42 - image
133:45 - so that's it for this function now we
133:46 - are ready to display both the original
133:48 - image
133:49 - and the reconstructed one with only 64
133:52 - colors
133:53 - so the first plot will be the original
133:56 - image
133:58 - so we'll turn off the axes and then
134:01 - plt.title will be the original
134:05 - image with
134:09 - 96 615 colors
134:16 - and then we will show the original image
134:20 - which
134:20 - in this case is simply flower and now
134:23 - our second plot
134:24 - so plt.figure 2.
134:30 - here we will display the reconstructed
134:32 - image
134:33 - so again turning off the axes
134:37 - the title will be
134:44 - here we will write a string actually
134:45 - while passing a parameter in this string
134:47 - so reconstructed
134:49 - image with
134:54 - n colors because you can change the
134:55 - number of colors we will do that
134:57 - after
135:00 - and then we show the reconstructed image
135:03 - so in here
135:04 - in there we will pass in our function we
135:06 - construct image
135:08 - and you pass in k-means dot
135:11 - cluster underscore
135:15 - centers underscore
135:19 - pass in also the labels and you pass in
135:21 - the width and the height
135:22 - that we defined earlier
135:25 - and you get the following result so as
135:27 - you can see the integrity of the image
135:29 - is
135:29 - kept actually the flower itself is very
135:31 - similar i would say that only the
135:33 - background is very different
135:34 - so let's go above and change the number
135:36 - of colors just for fun so let's say we
135:38 - want only four colors
135:40 - so we're running these cells um
135:43 - as you can see now with four colors the
135:45 - image is very different but you can see
135:47 - it's almost like a
135:48 - an artistic effect that you can play
135:51 - around with so feel free to play around
135:53 - with this number of colors with yourself
135:55 - now let's work with pca for
135:58 - dimensionality
135:59 - reduction
136:03 - here we will work with the iris data set
136:05 - this data set has four features
136:07 - about three different kinds of iris
136:09 - flowers
136:10 - and our goal is to visualize the data
136:12 - set in two dimensions
136:15 - so from sqlearn.datasets we'll import
136:17 - load iris
136:18 - and from masculine decomposition import
136:20 - pca
136:24 - now let's load the iris dataset so iris
136:27 - will be equal to load
136:28 - iris the features is iris.data
136:31 - the target is iris dot target
136:37 - and then the labels or target names
136:40 - here is iris dot target
136:44 - underscore names awesome
136:51 - now let's initialize the pca algorithm
136:54 - and we will specify that we want only
136:56 - the first two principal components
136:58 - since we want a 2d plot
137:02 - then x underscore r is pca dot
137:05 - fit x dot transform
137:12 - x
137:17 - now let's actually print out the amount
137:20 - of variance that is explained
137:22 - by each principal component
137:25 - so the explained variance ratio
137:28 - from pca
137:31 - and you can extract this information
137:33 - from
137:34 - the pca object itself so it's pca dot
137:38 - explained underscore
137:41 - variance underscore ratio underscore
137:49 - running this cell as you can see the
137:50 - first principal component explains
137:52 - 92 percent of the variance and the
137:54 - second one 5
137:56 - so that means that a total of 97 of the
137:58 - variance
137:59 - is explained with only two components
138:03 - so now we are ready to plot our data set
138:05 - in 2d
138:06 - and that data that newly transformed
138:09 - data
138:10 - contains about 97 of the variance of the
138:14 - original
138:14 - data set so here we'll just specify
138:17 - three different colors
138:18 - to distinguish between the three
138:20 - different kind of iris flowers
138:25 - so the final one will be ffa 600
138:31 - and we'll set the line width equal to 2.
138:40 - then plt.figure
138:43 - and then for color in
138:48 - oh sorry so for color i
138:51 - target name in zip
138:54 - and we pass in colors we will
138:57 - pass in uh zero
139:01 - one and two and we pass in the target
139:05 - names so 0 1 and 2 here
139:08 - are simply the the classes right
139:11 - so we'll draw a scatter plot so
139:14 - plt.scatter
139:15 - xr
139:19 - when y is equal to i
139:24 - and zero and then x r
139:28 - when y is equal to i
139:35 - and one so this is basically the x-axis
139:38 - and then the y-axis
139:41 - and the color will be equal to uh
139:45 - the color at this point in the loop
139:47 - alpha will be equal to 0.8 and then lw
139:50 - we set it equal to lw that we specified
139:52 - above
139:53 - finally the label will be equal to the
139:55 - target name
139:57 - at that specific step in the loop
140:08 - now we will simply put a legend on our
140:11 - plot
140:12 - the location sorry the location equal
140:15 - will be equal to
140:16 - best
140:20 - and we don't want any shadow
140:26 - finally let's set a title to our plot
140:30 - so pca of iris dataset
140:35 - running this cell as you can see now we
140:37 - get this
140:38 - plot right here and so you can visualize
140:41 - in two dimension a dataset that
140:43 - contained
140:44 - four features and three classes so
140:47 - now you could follow up with some
140:49 - classifier maybe decision trees on this
140:51 - transform data set
140:52 - to classify each kind of flower
140:56 - alright so that's it for this data
140:58 - science crash course
140:59 - i hope that you enjoyed it and that you
141:00 - learned something interesting
141:02 - if you want more videos on this topic or
141:04 - videos on end-to-end data science
141:06 - projects
141:07 - please check out my youtube channel
141:09 - until next time take care

Cleaned transcript:

hello everyone and welcome to this data science crash course my name is marco and i just recently started a youtube channel where i do videos on data science so if you want to check it out the link is in the description below a bit of an overview of this crash course we will first answer the question what is data science then we will walk through a setup so that you are ready on your computers to code along with me and then we will move on to the algorithms which is in my opinion the fun part so we'll talk about linear regression then we will move on to classification with logistic regression lda and qda we will talk about resampling and regularization methods which are very important in any workflow for a data scientist we will talk about decision trees uh most of the stateoftheart algorithms are actually treebased methods so a very exciting subject and then we will move on to support vector machines and conclude this crash course with unsupervised learning now let's answer this question what is data science well data science is a field that uses scientific methods to extract knowledge and insights from data now this definition may look very broad and that's because data science is a very broad field in fact it encompasses three professions so you can be a machine learning scientist or engineer which means that you are the person who develops the algorithms you could also be a data analyst which is the person who answers business questions for example what is the product that we sold the most in the last month that would be the job of the data analyst or finally you could be a data engineer which is the person that builds the software to gather data from different sources because usually the data needed to solve a problem is not in the same place so those data engineers they gather the data from everywhere and put it in a format that can be used after by the data analyst or the machine learning engineer now this crash course will mostly focus on the machine learning part because we will mostly discuss the algorithms now what can you expect from this course well first of all theory yes as in math equations because theory is important in data science you need to know how a model behaves why it behaves in a certain way and how it works because understanding that is actually way harder than coding the algorithm itself as you will see later on also if your goal is to land a job as a data scientist in a company well you will be asked theoretical questions during the interview so a very important part please don't skip it and of course each algorithm section will be accompanied by handson examples in python so for each algorithm we will download a data set and we will apply this algorithm on that data set so without further ado let's get started with the setup all right let's get you set up to do some data science head over to google and search for anaconda then click on the first result which should be anaconda the world's most popular data science platform once you click on it head over to products and then individual edition you can read more about what anaconda is if you want otherwise simply click on download and it brings you here where you can select the installer which is appropriate to your operating system now in my case i have windows 64 so it would be the graphical installer if you're on mac or on linux choose the one that suits you best once you click on it you can download and save the file now i will not do this since i already installed it on my machine but once it is downloaded you can simply follow the instructions on the graphical installer and once everything is done you should be able at least on windows if you hit the windows key and start typing jupiter with a y simply type enter and after a moment you should see this page showing up and that means that jupiter is installed correctly and you're ready to do some data science if you're on mac however simply open your terminal and type in jupiter notebook with a space and you should be fine all right now that we are set up let's kick off this crash course with our very first algorithm which is linear regression we start with a simple linear regression where our target y only depends on one variable x and a constant beta 1 is then a parameter which can be positive or negative that characterizes the slope and beta naught here is the constant to find the parameters we need to minimize a certain error function so here the error is simply the difference between the real target y i and the prediction y hat for linear regression we minimize the sum of squared errors so we raise this equation to the power of 2 and add all errors across all data points visually it looks like this the red dots represent our data and the blue line is our fitted straight line each vertical line is the magnitude of the error so we want the to position the blue line such as the sum of the squared length of each vertical line is as small as possible now you might wonder why do we square the errors well as you saw the points can lie above or below the fiddle line so the error can be positive or negative if we did not square the error we could be adding a bunch of negative errors and reduce the sum of errors it would trick us in thinking that we are fitting a good straight line where in fact we're not it also has the added advantage of penalizing large errors so we really get the best fit possible for simple linear regression you can find the parameters analytically with these formulas where x bar is the mean of the independent variable and y bar is the mean of the target now of course in practice we will use python to estimate those parameters for us here you can see we first initialized the model and then we fit on x and y and then we can retrieve both the intercept and the coefficient as you can see here once you have your coefficients we need a way to assess their relevancy to do so we use the pvalue this allows us to quantify the statistical significance and determine if we can reject the null hypothesis or not in python we can analyze the p value for each coefficient like this here we use a statistical package from python that allows us to print out a summary of the model here you can see an example of that summary so for each coefficient you get here the pvalue which is the p greater than absolute value of t here you see the value is zero but it is really not zero however it is so small that it appears to be zero once we know our parameters are relevant we must assess the model itself we usually use the residual standard error of course the smaller the value the better it is since the difference between predicted and actual is small as you can see with the equation on the screen also we use the r squared value which measures the proportion of variability explained by a feature x as it approaches 1 it means that we are explaining a lot of the variability in our target in python we can use the same process as we did before to find the pvalue using the same package so as you can see when we print out the summary this is the full printout that you get from the python package you see highlighted in yellow the rsquared value in this example now from here multiple linear regression is easy to understand as we simply extend the model to accommodate more features now each feature has its own parameter and p is the number of predictors so in python we saw how to access the constant n1 parameter and if we have multiple parameters then we simply increase the index to get the other parameters in multiple linear regression to assist the model of a multiple linear regression we use the f statistic here p is the number of predictors and n is the number of data points now again in python we'll use the same package that outputs for us the f statistic as you can see here in yellow the f statistic for a multiple linear regression model usually if f is much greater than one we say that there is a strong relationship between our predictors and the target for a small data set of a couple hundred data points then we the f statistic has to be way larger than one so that's it for the theory of linear regression let's see how we can apply it with python so let's fire up our jupyter notebooks here i have a folder called data which contains a data set for this exercise so as with any project we start off by importing the libraries we will use so of course we're going to use pandas as pd let's import numpy as np now it's time to import matplotlib and because we will use that of course to do some plots of our data a lot of straight lines and scatter plots then we're going to use cycle learn to actually fit a linear model to our data set your regression and finally we also import a stats library so stats models dot api as sm and that will give us some very cool um statistical tests to uh test for our predictors and model in general and of course we use some python magic to plot so let's start off by importing our data set so data is equal to pde.read.csv i put in the path of my data so data slash advertising.csv and then i will specify the index call equal to zero and that means that the first column will be used as the index column in our data set doing data.head you should see the following first five entries of our data so as you can see we have ad spend on tv radio and newspaper and the impact on sales so we start off with some simple linear regression so we will only consider the effect of tv on sales in this case feel free to take radio or newspaper so i always like to do a quick plot of my data whenever possible in this case because we only have one feature one target it is indeed possible so i first set the size of my figure and then we do a scatter plot so specify the x which is going to be data tv the y is data sales and then i specify the color i want it to be black afterwards i'm just doing making my plot uh a bit nicer so i just specified an x label um so there's gonna be money spent on tv ads and the units is money such as dollar sign and then plt.y label is going to be equal actually no i do not have to put an equal sign here i need to remove that so remove this from this all right perfect and now i do this is going to be sales and it's going to be in thousands of dollars perfect finally i do plt show and you should see the following plot there you go as you can see so this is our data so the sales with respect to the money spent on tp ads now maybe a linear line a linear regression is not the best model here but let's try it anyways so we're going to specify our feature which in this case is only the data tv so the values are reshaped minus 1 1 as required by the scikit learn library and then we specify the target which is our sales values dot reshape minus 1 1 perfect now we simply call the regression model so rag is going to be equal to linear regression and then we will fit the model to our data we pass in x and y awesome and with that we can print our parameters so we can print our constant and coefficient for tv so the linear model is y is going to be equal to so first we're gonna access uh actually we're gonna access the intercept first uh so we intersect underscore zero and then we add the coefficient for x so in this case tv and the coefficient is equivalent to beta1 if you looked at the theory portion of this video so as you can see we get a constant of 7 and a slope of 0.0475 approximately so that's great we have a positive slope positive constant it seems to make sense so let's get some predictions and actually plot our straight line so you can get the prediction simply by calling the predict method on x and then we do another figure so i'm always again just setting the fixed size here not curly braces sorry about that it's actually just normal parentheses so put it the same size as before 16 8 then plt dot scatter we pass in x y and of course the color is going to be black so this is going to be our data set as shown uh above a bit earlier and then we add in uh the plot of our predations so same x the y in this case is going to be predictions and i want it to be in blue and i'm going to specify the line width to be equal to 2 just to make sure that we can see it now let's just copy the labels because it's going to be exactly the same the same sorry uh the plt does show as well awesome let's run it and boom as you can see we have ours traded our straight line plotted on our graph uh that's awesome perfect so let's move on to the next portion where we will assess the quality of our uh model so to do so we're actually gonna use the stats library so i'm going to respecify again my x and my y and we're going to fit another linear model but using the stats library so specify the exogenous variable as sm add constant x and then the estimator is simply sm.ols that stands for ordinary least squares that's the method we're using pass in y pass in x and we fit finally you can print a summary of the estimator and you should see the following result so as you can see we have an r squared value of 0.6 so that is not very good only 60 of the variability is explained the s statistic is 312 which is much larger than one so it seems that our model is kind of good and as you can see here for tv we get the same coefficients as before and the pvalue although probably not zero it seems to be less than 0.05 so it means that our um that our feature is indeed relevant in this model so that was simple linear regression let's move on to multiple linear regression so in this case we will consider all the features so tv radio and newspaper and see how that affects the sales so all my x's to define them i'm just going to drop the sales column and make sure i dropped it on axis equals one so i mean i'm dropping it only the column and on the rows and the y is going to be the same so that is sales the values dot reshape minus one and one again i'm going to fit using scikitlearn so the regressor is linear regression and you call fit so x's and y perfect now as before we're going to print our coefficients so the linear model is skip a line and then y is going to be equal to so let's start off by printing uh the constant right so reg.intercept underscore square bracket is zero and then the coefficients will be in the same order as in the data set so the first one if i remember well it's going to be for tv so reg dot co f 0 0 and then um multiply that by uh tv afterwards we're gonna add the coefficient as the second one oh this is not in the brackets sorry about that radio and let's put all of this inside the brackets start the squiggly brackets right and now we're gonna add the last coefficient so reg dot co f zero one two sorry and let's bring this a bit to the right so you can see the code uh we multiply that by news paper awesome let's run this cell hopefully everything is going to work no the regression object has no attribute code f indeed it does not uh we need to add an underscore on the coefficient of tv so coeff underscore awesome there you go there we have it so uh we have two as a constant .04 so the same slope for tv point 18 for radio and negative for newspaper that is very interesting we have a negative effect from newspaper so again let's fi let's use the stats library to assess the quality of our model so i'm just going to specify the x as np not column stack and then we take everything so data tv we're gonna take uh data radio right yes data radio and data newspaper awesome so this is our x our features and the target again is data sales sorry shape minus one one awesome so again the exogenous variable we're going to do sm.add constant we use x we define our estimator which is sm.ols and then passing y passing x in this case exalt right.fit and we print the summary of our estimator and you should get the following awesome now as you can see the r squared value is much larger than before we have 0.897 so we're explaining almost 90 percent of the variability of the variability of the sales here the f statistic 570 again larger than before so it means that our model is pretty good i actually to predict the sales from that and as you can see here all the constants and all the coefficients now as you see for the last one we have a pvalue equal to 0.860 so that is larger than 0.05 and recall that this coefficient here is the one for newspaper so that means the newspaper is actually not relevant in our model and we could and actually we should take it out let's move on now to the next topic which is classification first a bit of terminology binary classification is also termed simple classification and this is the case when we only have two classes for example spam or not spam a fraudulent transaction or not of course you can also have more than two classes for example eye color which can be blue green or brown now you see in the context of classification we have a qualitative or categorical response unlike regression problems where we have a quantitative response or numbers so now let's see how we can perform classification with one algorithm which is logistic regression ideally when doing classification we want to determine the probability of an observation to be part of a class or not therefore we ideally want the output to be between 0 and 1 where 1 means very likely well it just turns out that there is a function to do that and it's the sigmoid function that you see on the screen as you can see as x approaches infinity you approach one and if you go towards negative infinity you approach zero the sigmoid function is expressed like this and here we are assuming only one predictor x for now we stick to one predictor to make the explanation simpler with some manipulation you can get to this formula here so we are trying to get a linear equation for x take the log on both sides and you get the log it as you can see now it is linear with respect to x and most importantly the probability is bound between 0 and 1. now of course we can extend the log it formula to accommodate multiple predictors this always gives better results since you are considering more predictors in python we can perform classification using logistic regression like this so again we initialize the model first and then we fit on our training data passing the features x and target y then we can get the probability that an observation is part of a class once we have that probability if it is greater than 0.5 we will say that it is part of that class so we will output one otherwise it will be equal to zero which is what the last line of this script is doing now that's it for logistic regression let's move on to linear discriminant analysis or lda now we want to learn about lda because logistic regression has some caveats when classes are well separated the parameters estimate tend to be unstable they are also unstable when the data set is small and finally logistic regression can only be used for binary classification with lda you can overcome those issues because it models the distribution of predictors for each class so you can have more than two target classes and it does so using bae's theorem now bae's theorem is explained like this suppose that we want to classify an observation into one of capital k classes where capital k is greater than or equal to two then we let pi k be the overall probability that an observation is associated to the kth class then let f k of x denote the density function of x for an observation that comes from the kth class this means that k of x is large if the probability that an observation from the kth class has capital x is equal to small x then the jesus theorem states the equation that you see the probability of the class being capital k given capital x equals to small x is the ratio of pi k fk of x over the sum of pi l f l of x from l to k so for all classes this was a bit challenging so make sure to probably rerun this section of the lesson because it is very important to understand now the challenge here is really approximating the density function so we will assume only one predictor and normal distribution this is expressed by the function that you see here this is the normal distribution function so if we plug this function in the formula that we saw before and take the log we find out that we must maximize this equation now this is called the discriminant and as you can see it is a linear function with respect to x hence the name linear discriminant analysis when applying lda we need to be aware of the assumptions it makes and make sure that it applies to our situation here lda assumes that each class has a normal distribution and each class has its own mean but the variance is common for all classes if you add more than one predictor which should be the case because usually more predictors gives you better results then each class is drawn from a multivariate gaussian distribution and each class has its own mean vector and there is a common covariance matrix so basically we must use vectors and matrices instead of single numbers compared to lda with only one predictor in practice this is how we apply lda once we initialize the logistic the linear discriminant analysis algorithm and then we fit it on our training data passing the features x and the target y then again we get the probability with the method predict prabha and finally if the probability is greater than 0.5 we say it is part of a class so we output 1 otherwise we output 0. now that we understand lda qda is fairly straightforward the main difference is in the assumptions just like lda we assume each class is from a multivariate normal distribution and that each class has its own mean vector but this time each class also has its own covariance matrix of course because we are talking about quadratic discriminant analysis well the discriminant here is expressed like this and you see that the equation is now quadratic with respect to x since we have two terms of x being multiplied together now whereas lda was better than logistic regression in some situations qda is also better than lda mainly when you have a large data set because it has a lower bias and higher variance if your data set is small then lda should be enough again in python you should see some kind of a pattern here because as you see it is very similar to lda and even logistic regression so again simply initialize your model fit it on x and y predict the probability and if it's greater than 0.5 output 1 otherwise the output 0. now before we move on to the coding portion we must understand how to validate our models in the context of classification to do so we use sensitivity and specificity sensitivity is the true positive rate so the proportion of actual positives identified so for example it would be the proportion of fraudulent transactions that are actually fraudulent on the other hand specificity is the true negative rate so the proportion of nonfraudulent transactions that are actually nonfraudulent we can also use the rok curve where rock stands for receiver operating characteristic we usually take the area under the curve or auc that's why you probably hear about the rock auc we want the rock auc to be close to one why well as you can see we plot the false positive rate against the true positive rate ideally we have a false positive rate of 0 and a true positive rate of 1 which would give an area under the curve of 1 and the curve would hug the upper left corner of the graph as you will see soon in the coding portion we can write a function in python that will plot for us the roc curve and report its area under the curve we will actually walk through the plotting and the writing of this function in the coding portion now in the case of a perfect classifier as i said we get an auc of one and the rock curve should look like this so that's it for the theory about classification logistic regression lda and qda let's move on to a handson example alright so let's start off with this project fire up your jupiter notebooks are i have already mine open uh as always i have this folder called data in which i put my data set called mushrooms.csv the link for the dataset is in the description of the video so in this project we are going to classify mushrooms as either being edible or poisonous depending on different features so you have cap shape cap surface cap collar etc and i put all the possible values here at the beginning of the notebook so let's start off by importing the libraries we're going to use open spd known by snp of course matplotlib.pipeplot as plt and also seaborn as sns now we are going to import sklearn the preprocessing and specifically label encoder you will see how that will be used later on also from sklearn.model selection import train test split with underscores and also cross val score awesome from sklearn.metrics we are going to import the raw curve and the auc as well as the confusion matrix shift enter oh sorry first we're gonna put the matplotlib in line so we can see our plots all right so shift enter and now i am just going to um to define a path for my data set so data slash mushrooms with an s dot csv and now i'm just going to display the first five rows of the data set all right so as you can see these are our first five rows we see the class poisonous are edible and then we see the values for each feature so now i'm just going to do a plot to see how many poisonous and edible mushrooms we have in our data set so this will help us see if the data set is balanced or not so for that i'm going to use seaborn and as you can see our data set is fairly balanced we almost have the same amount of poisonous and edible mushrooms so that is very good we're not going to have to do a lot of preprocessing for our analysis now i'm going to define a function that will allow us to see depending on what feature how many uh mushrooms are poisonous or edible so for example if i if i plot for the cap surface so for all possibilities of values for the capped surface i want to know how many of those mushrooms are edible and how many of those are poisonous so that's going to give us a bit of intuition as to which feature helps you to actually classify your mushrooms and that's it so that's the function now i'm going to show you how you can use this function but i am not going to run it because this will actually run for all the features of the data set so it's going to generate a lot of plots so i'm just going to show you how to use it and you can run it on your notebook if you want so you set the hue equal to data class so that means that you're going to have two colors right so one for poisonous one for edible and then you simply want to drop the class column and then plot the rest of the data so to do that you just do plot data and then you pass in the hue and data to plot so fairly simple very straightforward like i said you can run this function on your own notebook if you want to see uh the examples but i will not run it for now so let's move on to preprocessing so you can do this by doing escape 2 by the way and shift enter so now i'm going to check how many null values do we have in our data sets because we do not want any antivalues so if we're calling data.columns i want to print the name of the column and then the sum of null values if they are null so you do that by data call is dot sum if you run it that is amazing as you can see we have zero everywhere so that means that we have no null values in our data set that's perfect now we are going to use the label encoder so what label encoder will do is that it will transform our class column into one and zeros because we cannot work with letters we have to work with numbers right so you do le.fee fit transform data class and now i will show you the result data head and there you go now the you you as you can see the class is now one and zero so either it is poisonous or not poisonous so one being true zero being false then you want to one hot encode the rest of the data set so to do that we do pd.get dummies and then you pass in the data so let's see what the result of that will be and as you can see now we have added a lot of columns so we went from 23 columns to 118 columns because now for every feature we have either true or false and now that is perfect this data is ready to be worked with because we have only numbers everywhere so let's move on to modeling first i'm gonna determine uh what the target variable is so in this case it is the class uh the values.reshape minus one one perfect and then the features is going to be the encoded data the head dot sorry dot drop the class column axis equals to 1 to make sure that we drop the column and then we're going to define a train and test set so we do x strain x test y train y test is going to be equal to the train test split that we imported earlier so you pass in your features you pass in your target variable and then you define the size in this case i'm going to do 0.2 so 20 of the data set will be randomly uh removed to to use as a test set and we're going to use the rest to train you can set a random state by the way to keep the results constant so let's apply logistic regression our first algorithm for classification so from sklearn dot linear model import logistic regression now we're going to initialize the model so logistic regress reg sorry is going to be equal to logistic regression now i will fit the model to our train set so pass in xstrain and ytrain dot ravel now we want to get the probabilities so y prob is going to be equal to logistic reg dot predict underscore probably and we're going to use the test set in this in this case because we fit before and now we are doing probabilities on the test set right and now we set our threshold to 0.5 so the actual prediction is going to be np dot where so in this case if the prediction is greater than 0.5 we're going to say it's equal to 1 and otherwise it's going to be equal to zero so this is really where we are classifying our mushrooms so we have run that and everything is okay you can safely ignore the warning on the screen and now we're going to see at a confusion matrix so the confusion matrix is actually going to show you uh how many mushrooms were correctly classified so confusion matrix you pass in white test and you pass in the white prediction and hopefully if those are equal you will see that we're going to have a diagonal matrix and that is actually amazing we have a diagonal matrix so all poisonous and all edible mushrooms were correctly identified so let's check that with actually another metric we're going to use here the false positive rate and the true positive rate as well as the thresholds and we're going to set that equal to the rock curve pass in y test and y prob and then the rocket you see simply going to be equal to the auc and then you pass in the false positive rate and the true positive rate so here we are actually uh going to use the rock curve and as you can see when we calculate the rocket you see we get one which is again perfect classification now i'm just going to define a function to plot the rock curve so we can visualize it see how it looks so this function is going to take in the rock auc and here i'm just basically building the plot itself so i'm just setting the fix the figure size to seven by seven i'm setting the title to uh receiver operating characteristic then i am actually going to plot the false positive rate and the true positive rate and i'm going to give it a different color here i'm going to use red we're going to give it a label called auc and i'm only also going to approximate the the auc basically so this is going to give us approximated to the two decimal places and then i'm going to give it a legend i'm going to put it on the lower right side of the plot now i am also going to plot a straight line going through 0 and 1. this just serves us as a general guide to evaluate the rock curve and i'm going to make this line dashed now i am going to define the axis or plt axis tight give some labels so the y label is the true positive rate and the x label will be the false positive rate and that's it for our function so let's actually plot the rock curve that we obtained above with logistic regression and you should get the following so this is actually a perf perfect rock curve so it's hugging the upper left corner and we have an auc of one so that means perfect classification so now let's move on to our second algorithm which is linear discriminant analysis and let's see if the results are going to be different of course it cannot be better right so from sklearn dot discriminant analysis you are going to import linear discriminant analysis now feel free to pause the video and try it on your own because we are going to basically repeat the same steps as above only this time we are using a different algorithm so you can always pause the video and try it on your own as an exercise so lda is going to be equal to linear discriminant analysis so here i'm initializing the model then i'm going to fit the model with our train set so pass in stream and ytrain.gravel then i am going to get the probabilities from the lda model so lea dot predict prabha and you pass in x test and then you get the predictions and we use the same threshold as before 0.5 so it's actually going to be the y pro np dot where um y probe there we go if it's greater than 0.5 we're just going to classify it as 1 and otherwise it will be 0. run this cell and awesome everything went well as again again you can ignore the warning on the screen so we're going to build a confusion matrix here so white test and y print lda and let's display the confusion matrix and as you can see perfect classification again so as an exercise we will still build the rock curve and show it uh simply to make sure that we get a rock of rock auc of one so get the false positive rate true positive rate and thresholds to be equal to rock curve passing y tests and bison the y uh probabilities here in this case y prob lda now display the rock auc of lda so first you assign it false positive very true positive rate and now we are ready to display it so rock a you see lda and we should get one and as as expected we indeed get one now we are going to plot the rock curve using the function we defined earlier and as you can see you get the exact same function which is again expected right because our confusion matrix was the same the rocket you see was the same the plot should be the same so again lda is a perfect classifier in this case and finally we are going to implement quadratic discriminant analysis again i strongly suggest that you pause the video at this point and really try to repeat the steps that we have done before using qda so as always we're going to import the model from sklearn so from sklearn.discriminant analysis import quadratic discriminant analysis now you set the model so you initialize it sorry so qda is quadratic discriminate analysis you can always press tab by the way for autocomplete you fit the model on your train set and then you get the probabilities like i said the steps are exactly the same it's just that we are using a different model so white prob qda is qda.predict prabha and you use the test set of course and then we get our classification so np.where use the same threshold so if it is greater than 0.5 classify as one otherwise it is zero run the cell and ignore the warning then we're going to take a look at the confusion matrix so let's see if we also get a perfect classifier here with qda so confusion matrix you pass in a y test and the predictions displaying the confusion matrix as you can see we get the exact same as before so again qda is a perfect classifier for our data set now we're gonna plot the rock uh curve and get the a rock auc as well so just like before false positive rate true positive rate thresholds is going to be equal to rock curve pass in y test and the y probabilities y probe qda and then you use the false positive rate and true positive rate to get your rock auc so here i'm calling it rocket you see qda it's going to be equal to the auc pass in false positive rate and the true positive rate and now you can display the rock auc of qda and you get one perfect as expected now we are simply going to plot it to make sure that it looks like the other plots and you know that it will and it should and as expected uh perfect rock curve auc of one now let's talk about resampling and regularization resampling and regularization are two important steps that can significantly improve our model's performance and our confidence in the model specifically resampling helps us validate our model and we usually do so with cross validation regularization is used to prevent overfitting and we will cover both rich regression and lasso so first let's cover resampling methods resampling involves repeatedly drawing samples from a training set and refitting the model on each sample this allows us to gain more information than if we were fitting the model only once we can also test how the model would perform on unseen data without collecting new data that is important because a model in production will have to predict on data it has not been trained on crossvalidation is a widely used method for resampling we use it to evaluate a model's performance and to find the best parameters for the model there are three ways we can do validation we can do have a validation set we can do a leave one out cross validation or we can use the method of kfold crossvalidation let's explore each one of them the validation set is the most basic approach we have a data set of endpoints and we randomly split the data set into a training set that you see in blue and a test set that you see in orange we fit the model on the blue set and make predictions using the orange set this has some drawbacks because the test error rate is variable depending on which observations were in each set since we are splitting randomly also only a small subset of data is used for training when ideally we want as much data as possible for training so instead we could use leave one out cross validation or loocv in this method only one data point in orange is used for validation and the rest is used for training you repeat the process for as many times as you have data points so in this case n times the error rate is approximated as the mean of errors for each run now this method has the benefit of having no randomness but it's not a viable option for very large data sets so now we introduce the kfold cross validation this is by far the most common approach here we randomly split the data set into k groups or folds we use the blue set for training and the orange set for validation and we repeat the process k times now realize that loocv is a special case of kfold where k is simply equal to n the number of data points usually we set k to 5 or 10 and like i said this method is probably the best and most widely used in python we can perform cross validation like this here we see an example with linear regression we first initialize the model and then we can get the mean squared error for each fold notice that we are performing a fivefold crossvalidation because the cv parameter is set to five then we can get the mean of all errors now notice that we report the negative of the mean because the algorithm uses the negative mean square error as a scoring system therefore we need to get the negative so that we bring it back to a positive value now let's move on to regularization models can sometimes overfit meaning that they will not generalize well and perform poorly on unseen data this brings me to the subject of bias variance tradeoff on the left the model has a high bias and low variance and you see that it's not a very good fit on the right you see a model with high variance and low bias the model is overfitting and varying a lot and we will not give good predictions so we want to find a middle ground and prevent the models from overfitting and that's why we use regularization it will help us decorate our model to prevent overfitting here we will discuss ridge regression and lasso note that these methods are also called shrinkage methods we know that traditional linear fitting minimizes the rss the residual sum of squares with ridge regression we add another parameter to the optimization function here we add the sum of parameters squared with a coefficient lambda lambda is called a tuning parameter to find the best value of lambda we will use cross validation with a range of values for lambda the best value will be the one minimizing the test error with this method all predictors are kept and note that this is also called l2 regularization in python to use it we first initialize the ridge model then the lambda parameter is actually alpha with the library that we use in python in this code snippet we provide an array of different values for alpha going from 10 to the minus 15 to 20. then we will use fivefold crossvalidation to use the best value of alpha for our model we'll get to apply this later on during the project example now with lasso we also add a new term to the optimization function and lasso is also called l1 regularization here we add the sum of absolute values of all coefficients and we still have our tuning parameter lambda now if lambda is large enough some betas will go to zero meaning that some features will disappear and so feature selection can be done with lasso in python as you can see it is very similar to what we did with ridge only this time we initialize the lasso algorithm so that's it for the theory now let's apply these methods in a project all right let's open our jupiter notebooks in this exercise we will revisit a previous data set we used for linear regression so i always have my data folder and we'll use the advertising.csv dataset so we start off by importing pandas numpy and matplotlib so pandas as pd numpy as np and then matplotlib.pyplot as plt and of course do not forget your jupiter magic so that we can display our plots in the notebook then i will set the path to my data set and read in the data so data slash advertising dot csv and then you read the data set with pandas and we will display the first five rows of the data set using the dataframe.head method index call is equal to zero data ahead and there you go the first five rows of our data set now let's define a function that will allow us to plot the target against each feature so as you see the target will be sales and then we have three features tv radio and newspaper so i'm going to define scatterplot and pass in feature as a parameter now i will specify the size of the plot to make it slightly bigger so we can see it clearly on the screen plt.scatter data feature so this is the xaxis and then on the yaxis it will always be sales because that is our target and i will specify the color of the points i want them to be black i'm going to give it an x label so on the x axis this will be the money spent on whatever feature we are plotting so it will be money spent on tv ads or radio ads or newspaper ads and then i specify a label for the yaxis and in this case it will be our sales in thousands of dollars finally we simply display the plot so now we can run this function and pass in each feature so we can get a sense of how each feature is correlated with the target so tv radio and newspaper and once you run this cell you should see three different plots and as you can see newspaper does not seem to be very well correlated with sales so now let's define our baseline model and see how regularization will improve it so first i will import cross val score so model selection import cross underscore val underscore score and linear regression and these will be used for our baseline so our baseline model will be a very simple multiple linear regression so as a first step we will define our feature vector so in this case we are only going to drop the sales column and then we will define our target which will be sales so data sales dot values dot reshape minus one one perfect now we will fit the model so first initialize it linear regression and then we will calculate the mscs so we are going to use cross validation uh here calculate the mean squared error and then we are going to average those errors so you pass in the model x y the scoring we need to use negative mean squared error as required by uh this library and we do fivefold crossvalidation so this will give us five different mean squared errors so then to get the mean we simply do np dot mean of those mses and now we will report the average mean squared error uh bring it back to being positive because remember they were negative and so we get 3.073 approximately so let's see now how regularization can help us improve on our baseline so let's try ridge regression first regularization and then ridge regression perfect we will need to import a grid search to find the optimal value for our tuning parameter and of course we will need to import ridge so esky learn the model selection import grid search cv and from sklearn dot linear model we are going to import ridge simply like that awesome so we start off by initializing the model as always so ridge is going to be equal to ridge and now we will define a list of possible values for our tuning parameter so in this case uh the the parameter is called alpha in scikitlearn and now let's pass in a bunch of values to test and we will use crossvalidation to find out which one is the best so we pass in one e to the minus 15 minus 10 minus 8 minus 4 minus 3 minus 2 and then we go to 1 5 10 and 20. feel free to use as many values as you want but as a starting point we'll use these ones so now to run our five fold cross validation we run grid search cv we pass in the model and we pass in the list of parameters the scoring will also be negative mean squared error so we keep this consistent with our baseline model and again fivefold cross validation now we can fit the model pass in your feature and your target that is awesome and it's done so now we can actually print out uh the best value for the parameters and we can print out as well the mean squared error so you can do that by finding the best params and then it's going to be the best um score exactly so when we print this out you see that the best value for alpha is 20. and let's not forget that our scores were negative so let's bring it back to positive it's 3.0722 so it is a slightly lower msc so now let's try out lasso and see if we get an even better result so at this point you can try and pause the video to work it out on your own as the method will be very similar to what we have done with range so moving on uh we're gonna initialize lasso and set the tolerance to 0.05 so that it can converge in a reasonable amount of steps let's grab our list of parameters for alpha because this i'm simply going to reuse the exact same values to test during our grid and then we exactly reproduce what we did with ridge regression so lasso regressor is going to be equal to grid search cv you pass in your model lasso you pass in your parameters the scoring method will be neg mean squared error and again we do a cross validation of fivefold finally we can print out the best parameters and the best score so this will give us the value of alpha that gave the lowest mean squared error and of course you do that after you fit the model best params print lasso underscore regressor dot best underscore score underscore once we run this cell don't forget your negative once we run the cell there you have it the best value for alpha is one and we get a mean squared error of 3.036 approximately and this is indeed the best score all right let's kick off this portion about decision trees with a bit of theory treebased methods can be used for both classification and regression they involve dividing the prediction space into a number of regions the set of splitting rules can be summarized in a tree hence the name decision trees now a single decision tree is often not better than a linear regression logistic regression or lea that's why we introduce bagging random forests and boosting to dramatically improve our trees now before we move on we need to get familiar with a bit of terminology trees are drawn upside down the final regions are called leaves the point where split occurs is called a node and finally the segments that connect the nodes are called branches here is an example of basic decision trees with the leaves at the bottom and then the branches and you see the nodes where it splits now let's see how a regression tree works to create a regression tree we divide the predicted space into j distinct and nonoverlapping regions then for each observation that falls in a region we predict the mean response value of that region each region is split to minimize the rss it uses a topdown greedy approach also called recursive binary splitting now why top down because all observations are in a single region before the split and why do we call this greedy that is because the best split occurs at a particular step to make the best prediction at that step instead of looking ahead and making a split that will give a better prediction later on now mathematically we define the pair of half planes like this and we seek j and s to minimize the rss of both planes however this may lead to overfitting and as you can see on the right that's why we need to sometimes prune the trees and use cross validation to prevent overfitting in python a regression tree is very simple to implement as you can see we import the model from the sklearn library we initialize it and then we fit on x train and y train now let's see how a classification tree works it is very similar to a regression tree but instead we predict the most commonly occurring class in a region also we cannot use the rss since we are dealing with categories so we must minimize the classification error rate the classification error rate is simply the fraction of training observations in a region that do not belong to the most common class however this is not sensitive enough for tree growing so instead we use the genie index which is a measure of total variance across all classes the gd index will be close to 0 if the proportion is close to 0 or 1 which makes it a good measure of node purity a similar rationale is applied to cross entropy which can also be used for tree growing now in practice we can simply import the decision tree classifier model initialize it and fit it on our data x and y now that the basics are covered let's move on to more advanced topics on decision trees bagging stands for bootstrap aggregation we know that bootstrap can compute the standard deviation of any quantity we also know that variance is a high in decision trees since they are prone to overfitting so bagging is a method to reduce variance and improve the performance of our algorithm bagging involves repeatedly drawing samples from the data set generating b different bootstrap training sets once all sets are trained we get a prediction for each set and we average those predictions to get a final prediction so mathematically we expect the final prediction like this and so you recognize that this is simply the mean of all b predictions this means that we can construct a high number of trees that overfit but by averaging their predictions we effectively reduce the variance and improve the performance in python as you can see it is very simple to apply bagging we can import the bagging classifier initialize it and then we fit it on our data now let's see how a random forest can also improve the quality of our predictions random forests provide an improvement over bagging by making a small tweak that decorates the trees again multiple trees are grown but at each split only a random sample of m predictor is a lot is chosen from all p predictors and the split is only allowed to use one of the m predictors now typically m is the square root of p now how is that a good thing well in bagging if there is a strong predictor it will likely be the top split and all trees will therefore be similar so variance will not be reduced once we average all predictions with random forest because we force a random sample of predictors for each split we avoid the situation also realize that if m is equal to p then it is just like bagging again you see that applying the random forest algorithm is very simple in python once imported we initialize the model and pass in the number of trees we would like to use in this case we use 100 then we can fit on x and y finally let's cover boosting boosting works in a similar way to bagging but trees are grown sequentially they use the information from previously grown trees this means that the algorithm learns slowly also the trees fit the residuals instead of the target so the trees will be small and will slowly improve the predictions there are three tuning parameters for boosting the number of trees b where if it is too large then it will overfit so use cross validation to find the right number of trees we have the shrinkage parameter alpha which is a small positive number that controls the learning rate typically we set it to 0.01 or 0.001 and finally we have the number of splits in each tree which controls the complexity of the boosted ensemble typically a single split works best we also call that the interaction depth now of course i said that those there are three tuning parameters in boosting usually there are more than that but those are the three main ones that we can focus on when using the algorithm to improve its performance in python we apply boosting like this now in this module the learning rate is the shrinkage parameter and estimators is the number of trees in this case set to 100 and the max depth is the interaction depth that we looked at earlier again once the model is initialized we fit it on our data so that's it for the theory now let's see how we can apply this in a project all right so let's get some code done i have a notebook open here also feel free to grab the data set in the description and put it in a folder called data and the data set here is about breast cancer so we are trying to identify patients with breast cancer from a simple blood test so we start off by importing our usual libraries so numpy as np we are going to import pandas as pd then matplotlib dot type plot as plt we will also need seaborn and today we are going to use the function plot confusion matrix as well this is going to be useful to evaluate our decision trees later on finally you use some jupiter magic to display your plots in the notebook so now let's read our data set uh i will simply define my data path in this case so it is in the folder data and the data set is called breast cancer dot csv then i will use pandas to read the data so data is going to be equal to pd.read.csv and i pass in my datapath feel free to use tab at any time to autocomplete and we will display the first five rows of the data set and there you go as you can see the first five rows of our data set perfect now we are going to check if our data set is balanced because we have we are in a classification problem so i want to make sure that we don't have too much of healthy patients or patients with breast cancer in the dataset that would make it imbalance so we use the count plot and as you can see the classes are fairly balanced here so we do not need to know to do some crazy manipulations in this case now it will be interesting to uh define a function to make violin plots that will allow us to see the distribution of each feature for both classes so it can give us some intuition about the data so for example maybe we will see that most of the healthy controls are younger so defining the function we will need x y and data as parameters and then i will enumerate each y so we will define a figure then i will set some parameters in this case i will i am simply setting the uh figure size i want it to be fairly large for you guys to see so i will set it to 11.7 and uh 8.27 i know i am very precise and then you will simply do a violin plot for each one so sns.violin plot x is equal to x y is the colon and data is equal to data that is perfect so now in this case uh the y is actually going to be data dot columns uh everything but the last column so i want so in this case the features are actually going to be y and x is going to be the target variable because i want to get the distribution of each feature for each class now we can run the function actually passing in our x y and data and you get the following plots so feel free to study those plots a little bit longer and get an intuition about the data set we are working with now we are going to check for null values to make sure that nothing is missing so for column in data dot columns i will print the name of the column so curly brackets call and then we will print the sum of null values so that is data call dot is null dot sum running this cell and you see that we have no null values in this data set that is amazing now we will start some preprocessing first i would like to do some label encoding on the target variable so that we bring it to one or zero so from scaling up preprocessing import label encoder we will initialize the label encoder and then we will fit transform that on the row classification so data classification is le.fit transform and you pass in data classification now to make sure that everything is right we'll display the first five rows and everything is right now the healthy control is zero and someone with breast cancer will have a label of one now we will split our data set into a training and test set so from scalar dot model selection we are going to import train test split so our target is of course the classification the values.reshape minus one one and our features is gonna be everything but classification so i'm simply going to drop the classification column and i'm going to specify the axis as well axis is equal to 1. awesome now actually splitting our data set uh note that our data set is fairly small in this case so i will use a smaller test size than we are used to in this case i will use only 10 of it as a test uh size so you pass in x y test size is equal to 0.1 and a random state of 42 so that we make sure that we get the same results so now let's build our baseline model so it's been our baseline model it will be a simple decision tree uh classifier so uh from sklearn dot tree we are going to import um decision tree classifier we will initialize the classifier so then the brackets then we will fit the model so call fit and then you pass in xtrain and ytrain and finally we will plot the confusion matrix so the confusion matrix will show us how many instances were misclassified so you pass in your classifier x test white test and i will specify that i want uh blue colors in this case so it's going to be a gradient of blue um i do not want the grid and i want to show the plot so as you can see we get this confusion matrix and you see that only three instances were misclassified in this case now i would like to show you a cool trick because you can visualize your decision tree with the function plot tree so from excalibur not tree you can import plot tree and then let let's see what it looks like so you're passing the classifier and i'm going to specify the max depth to five so we'll only see five different splits and there you see it so you can see the top split you can see which feature was used what what's the value of the split and you can also check for the genie index of each region so that is pretty cool feel free to you know not even pass in max that so you can visualize the entire decision tree if you want to so now let's try and improve on our baseline model and we will use bagging first so from sklearn dot ensemble we are going to import um bagging classifier we initialize the model as always so bagging clf is bagging classifier then we fit the model so that fit pass in your x train and y train in this case you need to do dot ravel and now we will plot the confusion matrix you pass in your classifier pass in xtest ytest and again i will specify gradient of blues to keep the plots consistent in the entire notebook i will remove the grid and show the plot and as you can see we only have one misclassified instance in this case so bagging is an improvement over our baseline now let's see how we can implement random forest at this point feel free to pause the video and try it on your own as the process will be very similar uh to bagging so to what we've done uh above so from sklearn.ensemble we're gonna import random forest classifier we initialize uh the model so random clf is going to be equal to random first classifier and in this case i will specify the number of trees i want a hundred trees then we fit the model we pass in our train test our train set sorry extreme and white train and then we will plot the confusion matrix so i'll just grab this code right here copy paste it down and then all i have to do is replace bagging clf with uh random forest cliff and i forgot to to ravel the right train sorry about that so i trained unravel and there you have it we have actually a perfect classifier with no instances that were misclassified that is pretty great however keep in mind this is a small data set it doesn't mean that our model is necessarily very good at this point and finally we're going to implement boosting so uh from sklearn dot ensembl we're going to import gradient boosting classifier gradient boosting classifier so as we have done before we initialize the model then we fit it so boost clf dot fit x train and y train and then we will plot the confusion matrix so grabbing the code again copy paste it below and remove a random clf and paste boost clf instead and again i forgot the ravel sorry about that guys my train dot ravel and then you get this following confusion matrix where only one instance is misclassified which is not better than random forest but better than our baseline alright let's cover some theory about support vector machine for classification we have seen quite a few algorithms such as logistic regression lda qda and decision trees support vector machine is another algorithm used for classification its main advantage is that it can accommodate nonlinear boundaries between classes to understand svm we must first understand the maximum margin classifier like i said the maximum margin classifier is the basic algorithm from which svm extends it relies on separating different classes using a hyperplane in a pdimensional space a hyperplane is defined as a flat athene subspace of dimension p minus 1. therefore in a 2d space the hyperplane will be a line and in a 3d space the hyperplane will be a flat plane the equation for a hyperplane is defined like this where p is the number of dimensions now here is an example of a hyperplane in 2d which is represented by the line if an observation satisfies the equation we just saw earlier then the point is on the line otherwise it is above or below the hyperplane now realize that if data can be separated perfectly then there is an infinite number of hyperplanes but we only just want one so that's why we use the maximum margin hyperplane or the optimal separating hyperplane to find it to do so we must calculate the perpendicular distance between each training point and the hyperplane that distance is called the margin then the optimal separating hyperplane will be the one with the largest margin here you see an example of a maximum margin hyperplane and you see the margin illustrated by the arrows notice that the plane depends only on the closest points known as support vectors and if those points move then the hyperplane will move but what if there is no clear separation between the classes as shown here well that's when svm is required as i mentioned svm is simply an extension of the maximal margin classifier this time it uses kernels to enlarge the feature space and accommodate for nonlinear boundaries between classes a kernel is simply a function that quantifies the similarity of two observations the kernel can be a function of any degree but of course if the degree is greater than one then we add more flexibility to the boundary here is an example that we will implement later on during the coding portion of this section here the classes can be linearly separated so it's easy enough however notice the outlier on the left and we can use regularization to account for it or not and we'll see how that impacts the model here is another example that we will code and as you can see here the boundary is definitely not linear but svm does a pretty good job at finding a boundary and separating each class so that's it for the theory now let's move on to the coding project and generate those plots ourselves all right so prepare your notebooks and grab the data from the link in the description in this tutorial we're actually going to import five different data sets so you can see me i'm checking them right now so x6 data one two three and then spam test and spamtrain.mat so as always we start off by importing all the libraries that we will need throughout this project by the way these exercises are taken from the machine learning course by andrew angie i am simply solving them using python here by the way it's an amazing course and i will leave the link in the description if you want to check it out i definitely recommend it so we import numpy pandas matplotlib.pipeplot also matplotlib.cm and from scipy.i will import loadmat and finally mypluslibinline to show our beautiful graphs now i will simply define the path to all my datasets so path 1 is going to be for x 6 data 1 dot mat and then we'll simply do the same data path 2 for x6 data 2 dot matt moving on to data path 3 for ex 6 data 3. and i will define the path for the spam train and test as well and we will use those data sets at the very end of this of the tutorial when we will build a classifier for spam using support vector machines training is done now data spam test is data slash spam train test sorry dot matt perfect uh now we will need to write a function to plot our data i decided to write a helper function here because we will be plotting quite often so we will need x y x label y label we'll use pause label and neg label because this is mostly classification right and we'll also pass in x min max y min and y max and set x is equal to none so first we're going to set the parameters of the plot we're actually going to set the figure size to make it nice and big for you guys so you can see it clearly on the screen i'm going to set it to 20 and 14. now i will specify what will be considered as positive so it's when the the label is going to be equal to one and negative is of course when it will be equal to zero then if axis is equal to none which by default it is then we're going to set axis equal to plt.gca then we're going to draw a scatter plot so axis dot scatter pass in the positives then you pass in the y for the positives i will specify the marker so i want dots so you can specify that by putting uh the letter o in strings then i will also specify a color at this point feel free to use any color you want s equal to 50 and the line width will be equal to two and finally the label will be the pause label because those are the positive data points now i will draw another scatter plot but for the negative samples so it will be fairly similar to the line that we wrote above only this time it's going to be for negative so nag column comma zero and then again so you pass in the y now so x snag colon comma 1 the marker is still going to be a dot this time we're going to specify another color so that we can differentiate them easily on the plot so ffa 600 s will be equal to 50 the line width will also be equal to 2 and then you set the label equal to neg underscore label awesome now i will set the limits on the xaxis and then on the yaxis so of course we'll just pass in the xmin and xmax and then you do the same for the yaxis so while and then you pass in the array from ymin to ymax now let's set the label for x so set x label will be x label simply and we'll specify the font size i'm going to put it to 12 and you do the same thing for y label so you set y label pass in my label and specify the font size as 12 as well then we will specify the position for the legend so acts as a legend is a box sorry is it b box to anchor equal to one comma one and fancy box we'll put equal to true so let's start the coding portion by exploring the effect of regularization on svm and we'll start off this time with a small regularization parameter so first let's see what our data set looks like so data one is load matte the data path number one x will be data x and the y will simply be data y and now we will plot the data set so we use our function plot data pass in x pass in y the label will be x and y then the positive way label we'll simply call it positive the negative will call it negative we want the plot to be from 0 to 4.2 and then from 0 to five and i made a mistake because it should be data one not data all right and you see this following plot so as you can see the positives in dark blue the negative is in yellow and you see that this data set is clearly linearly separable and you also notice this outlier on the left so now let's see um with a small regularization parameter what will happen to that outlier if it will be classified correctly or not so we start off by importing svm from sklearn and we'll set the regularization parameter to 1 for now so clf will be equal to svm.svc the kernel will be linear because as you can see we can use a straight line to separate the classes c will be equal to one and then you specify the decision function shape to obr then you fit the model on your data and now we will plot the data as well as the boundary so here actually we can just grab this line from the previous cell because it will be exactly the same we're just plotting the same data again and now we'll plot the boundary on top of this data sorry i accidentally ran this cell but now let's actually plot uh the boundary or the hyperplane so we start off by specifying x1 and x2 to be a numpy dot mesh grid and then we pass in np arrange so we want from 0 to five and from with the steps of 0.01 and np range oh sorry this should not be here this should be inside the big bracket of mesh grid so np range from 0 to 5 as well with steps of 0.01 so i'm taking small steps here to make sure that we plot um a smooth hyperplane as smooth as possible then zed will be the prediction pass in np dot c underscore x underscore one dot ravel and do the same with x2 dot ravel then zed is going to be equal to z dot reshape x1 dot shape and now we are ready to plot the hyperplane so plt.contour you pass in x underscore one x underscore two pass in zed pass in uh zero comma five and the colors will be uh black so b and there is a mistake we have a warning here no contour levels were found within the data uh range and that is because okay here's the mistake shouldn't be a comma it should be a point so 0.5 perfect and you see this spot here with our hyperplane which is a straight line in blue and we have and you can see here that the outlier was not taken into account and is in this case misclassified so now let's see what happens if the parameter is very high so feel free to pause the video and try it on your own as an exercise because the code will be very similar to the previous cell so here we are going to use a large regularization parameter we'll set it to c equal to 100 and actually i simply gonna grab everything from this cell here and just copy paste it below because the code is exactly the same we are simply changing the value of the hyper parameter so we don't need to import svm again and we'll simply use c equal to 100 and see what happens next so as you can see now the hyperplane shifted to account for the outlier but in this case we are likely overfitting which means that the model will not generalize well so ideally we will go with the previous model that we built so now let's try an example with a nonlinear boundary so svm with nonlinear boundary so for that we'll need our second data set so data2 is load matte data path underscore 2 and like before we'll specify the x and y and plot our data set so x2 will be data 2 x y2 will be data 2 y and now we will use our helper function to plot the data so pass in x2 and y2 again the label will be simply x and y for the class labels we'll simply use positive and negative and now we will set the limits on the xaxis so zero to one and on the yaxis it's going to go from 0.38 to 1. and there you go so as you can see now clearly a nonlinear boundary but the classes seem to be separable so let's see how svm will be able to do that for this example we will use a radial basis function for the kernel here we must define gamma which is a parameter that specifies how far the influence of a data point reaches a low value means very far and a high value means close and you can express gamma in function of sigma so let's define sigma as being equal to 0.1 and then gamma will simply be 1 over 2 sigma squared so one over open brackets two times sigma star star squared perfect then our classifier will be equal to svm.svc we pass in the kernel and this time is going to be equal to rbf so for radial basis function gamma will be equal to gamma we set the regularization parameter to 1 and the decision function shape will again be equal to ovr next we fit the classifier to our data so x underscore 2 and y underscore 2 not revel then we will plot the data set so again just grab this line from the previous cell and paste it under because we are simply plotting the same scatter plot as before and now we will plot the uh hyperplane so the actual boundary from uh the svm algorithm so again as before x underscore one x underscore two will be a mesh grid you range from 0 to 1 with bounds of width steps sorry of 0.03 so here the steps are going to be slightly smaller because we are drawing a nonlinear boundary and again it's just to make it as smooth as possible and from 0.38 to 1 for y then z will be equal to the prediction pass in np dot c underscore this is actually just to stack uh the predictions by the way so x underscore one dot ravel x underscore 2. ravel then we will reshape zed so that don't reshape we'll take the shape of x one so x underscore one dot shape and now we are ready to plot the hyperplane so plt.contour pass in x1 pass in x2 0.5 and then colors will be equal to blue and now there is a big mistake input z must be 2d and easy enough i forgot to pass in z so x1 x2 z and then the array 0.5 color is equal to blue and there you have it we can see in blue the uh boundary that svm uh predicted for us and you see he's doing a pretty decent job at separating everything you see you have a few points misclassified here and there but otherwise a good nonlinear boundary but now let's explore a situation where the data is not easily separable so for that we'll explore the third data set so load matte data path underscore three as before x uh sorry about that x underscore three will be equal to data three x y underscore three will be equal to data three y and now we will plot the data using our helper function so plot data we pass in x3 y3 the labels will be the same so x and y positive and negative for the labels and we want to plot from minus 0.55 to 0.35 and from minus 0.8 to 0.6 so that makes the data points nice and centered and as you can see now there is a clear overlap between both classes so there is no clear boundary between each class so this is when we need to use cross validation in order to find the best parameters for the best boundary so i will specify a list of possible values for sigma so we'll go from zero point zero one point zero three point one point three one three 10 and 30 and we'll do the same for the regularization parameter c so the same values 0.01.03 0.1.3 and then 1 3 10 and 30. perfect now i will initialize an empty list of errors and a limited list for sigma and c so for each value in sigma and then for each value for c we will define a classifier and we will fit it so clf is going to be equal to svm.svc the kernel will be rbf again because the boundary is likely nonlinear gamma will be equal to uh 1 over 2 times each squared and then after two brackets we pass in the value for c which is going to be each underscore c and the decision function shape will be equal to ovr perfect then we will fit it to our data so x underscore three y underscore three unravel and we will append uh the errors to our list we defined earlier so we're going to span clf.score on data 3 x val and data three y vowel dot ravel and then we're going to append to sigma c the value for sigma and for c as a tuple now running this i have a mistake that's because singa should actually be sigma and i still wrote simga at the very end yeah right here simga c is actually sigma c perfect so the loop red it is finished so now we can see uh what value for uh c and for sigma is the best so index will be np.r max pass in errors sigma max and c max will be equal to sigma underscore c at that index and now we can print out the values so the optimal value of sigma is colon squiggly brackets sigma underscore max and we'll print the optimal value for c as well and as you can see we get an optimal value of 0.1 for sigma and 1 for c so now we can fit an svm algorithm so we set sigma equal to 0.1 gamma is going to be equal to 1 over 2 times sigma squared then we will pass in those parameters to our classifier so the optimal classifier is svn.svc the kernel will be rbf gamma is equal to gamma c is equal to 1 and the decision function shape is ovr now we will fit our classifier to our data so x underscore three minus score three unravel then we will plot the data so again just go back up let's grab this line here and paste it back in our current cell and as before we will now find the points for the boundary and we will plot it so x1 and x2 is np.mesh grid pass in np range from negative 0.6 to 0.4 and we'll take steps of 0.004 to make it smooth and np range from negative 0.8 to 0.6 in steps of 0.004 that is then going to be the optimal clf dot predict and we will stack the prediction so np dot c underscore you pass in x underscore one dot ravel and x underscore two the unravel reshape z z dot reshape is x underscore one dot shape and now we will plot the boundary so plt on contour pass in x1 pass in x2 pass in z 0.5 in brackets and the color will be blue and the mistake again here is i wrote symga instead of sigma very sorry about that guys hopefully we're not making the same mistakes as i am and as you can see here we have our best boundary found from cross validation with support vector machine so it's actually not that bad and finally let's use svm for spam classification for our emails right so spam train we're simply going to load the data set called data path and i called it spam train then spam test will be equal to load mat and you pass in data spam test we will set the regularization parameter to 0.1 and then x train will be spam train and we pass in x y train will be spam train and we want the column y then x test will be spam test pass in x y test will be spam test passing y our classifier so clf underscore spam will be again svm.svc we will start with a linear kernel see how that performs so kernel is going to be linear the c parameter will be equal to c we set it above and the decision function shape will be equal to ovr then we fit our classifier to the train set so we pass in xstrain and ytrain unravel and then we will calculate the accuracy so clf underscore spam dot score and we will score against spam train x and spam train y that ravel and then we will test for the test accuracy so again cliff underscore spam dot score and you pass in uh x test and y test dot revel finally we will print out those scores so the training accuracy squiggly brackets pass in train underscore acc times 100 so we have it as a percentage already and we do the same for the test accuracy running everything uh we have a key error for x yes that's because here it should be x test and y test sorry about this little mistake so if we run this cell now our model is training and fitting and you get a training accuracy of 99.8 percent and a test accuracy of 98.9 which is very good moving on now to unsupervised learning let's cover some theory unsupervised learning is a set of statistical tools for scenarios in which we have features but no targets this means that we cannot make predictions instead we are interested in finding a way to visualize data or discovering a subgroup of similar observations unsupervised tends to be a bit more challenging because the analysis is subjective also it's hard to assess if the results are good or bad since there is no true answer in this section we will mainly focus on two techniques which are principal component analysis or pca and we will take a look at clustering algorithms let's cover pca first pca is a process by which principal components are computed and used to better understand data they can also be used for visualizations now what is a principal component well suppose you want to visualize n observations on a set of p features you could do a 2d plot for each two features at a time but that's not very efficient and unrealistic if p is very large with pca you can find a low dimensional representation of the data set that contains as much of the variance as possible that means that you will only consider the most interesting features since they account for the majority of the variants and therefore a principal component is simply the normanized linear combination of a feature that has the largest variance you see the equation here and that should remind you a bit of linear regression also this equation is for the first component the next one will be in a direction perpendicular to the first one and the third component would be perpendicular to the first two principal components in this equation here phi is referred to as the loadings here's an example of how you can apply pca in python in this case actually we are trying to visualize the iris data set in 2d this is something that we will apply later on during the coding portion so this data set contains more than two features for each species of iris so using this snippet we can initialize pca and then specify that we only want the first two principal components and then you can see here that we can uh find the explained variance ratio and then use that to plot a 2d figure of the features of this data set which is what you see here so as you can see we can plot the transformed data and see how each species of iris are different or separable from one another so that's it for pca now let's take a look at clustering methods clustering is a set of techniques for finding subgroups or clusters in a data set this helps us to partition the data into observations that are similar to one another an application of that is for example for market segmentation in the context of marketing we will first explore key means clustering which partitions data in a specified number of k clusters and we will also look at hierarchical clustering which does not need a specific number of clusters instead we can generate a dendrogram and see the clusters for all possible number of clusters but first let's focus on kmeans this method simply separates the observations into k clusters and we must provide that number it assumes that each observation belongs to at least one of the k clusters and that the clusters do not overlap it is important to note that the variation within each cluster is minimized here you can see an example of how the number of clusters will affect how the data is partitioned feel free to pause the video if you want to study this a little bit longer now clustering is achieved by minimizing the sum of the squared euclidean distance between each observation in a cluster you can see the equation here of the euclidean distance and we wish to minimize it to do so the algorithm first starts by randomly assigning each observation to a cluster then for each cluster a centroid is computed which is a vector representing the mean of the features in the cluster then each observation is assigned to the cluster whose centroid is the closest the two steps above are repeated until the cluster assignment stops changing now note that kmeans will find a local minimum therefore it highly depends on the initial cluster assignment so make sure to run the algorithm multiple times to see if you always get the same results in the coding section of the tutorial we will use kmeans clustering to perform color quantization more on that later on this process allows us to take a picture and reduce the number of colors so in this code snippet here we specify that we want only 64 colors in the picture then we can apply the algorithm on the image and output a modified image with only 64 colors the output will look like this of course because we use kmeans we can specify any number of clusters to group the most similar colors together so in this case we use 64 but later on in the coding portion we can use 10 28 whatever number we want now let's take a look at hierarchical clustering as i mentioned the potential disadvantage of kmeans is that you must specify the number of clusters and sometimes you simply don't know how many clusters you need this is when hierarchical clustering comes in because you do not need to specify the number of clusters the most common type of hierarchical clustering is called agglomerative clustering it generates a dendrogram from the leaves and the clusters are combined into larger clusters up to the trunk here is an example of gender grams we see the individual observations at the bottom and they are combined into larger clusters as you move up in the yaxis the algorithm is fairly easy to understand it starts by defining a dissimilarity measure between each pair of observations and it assumes that each observation pertains to its own cluster note that in this case the dissimilarity measure is usually the euclidean distance then the two most similar clusters are combined so that there are n minus one clusters the next two are combined resulting in n minus two clusters and that is repeated until all observations fall in one big cluster now although simple how do we define the similarity measure well that depends on the type of linkage and there are four types complete single average and centroid complete is also called maximal intercluster dissimilarity so it computes the pairwise the similarities in clusters a and b and it records the largest one with single it's the opposite and we talk about the minimal intercluster dissimilarity so here the smallest of the dissimilarities is recorded and this can mean that single observations are fused one at a time then we have average as the name suggests the average of the pairwise dissimilarities is recorded and finally we have centroid which computes the dissimilarity between the centroids of cluster a and b this is sometimes a problem as smaller clusters can be more similar to a larger one than to their individual clusters which can lead to inversions in your dendrogram complete average and centroid are definitely the most popular types of linkage note that the final dendrogram highly depends on the type of linkage you select as you can see here average and complete are quite similar to one another but with single leakage the dendrogram is quite unbalanced and that's why this method is not used often so that's it for the theory now let's get coding let's apply what we learned in python now these exercises are available as examples on the sklearn website i am simply reworking them a bit or explaining them here the links are in the description and the complete notebook on github is also in the description down below so we'll start off by importing some libraries we will need numpy we will also need matplotlib.pyplot as plt and finally from sklearn.utils we will import shuffle so let's kick off this tutorial with a clustering we will do color quantization with kmeans which is a technique to reduce the number of colors of an image while keeping the integrity of the image so to do that we will learn we will need from sklearn.datasets import load underscore sample underscore image and from sklearn dot cluster we will import kmeans now after importing our libraries we will load the image of a flower so the flower will be equal to load sample image and we will pass in the name of the image in this case it is flower dot jpeg now we need to convert to floats and divide by 255 because colors are expressed as rgb right red green and blue with values from 0 to 255 so we need to normalize that so that the image displays correctly with matplotlib so this is what we are doing here so we convert two floats and we divide by 255 to normalize everything finally we can show the image with plt.mshow and we pass in flower now i have made a mistake here uh the name np is not defined that's because i did not import numpy as np sorry about that so after rerunning this cell and rerunning this cell here we finally get the picture of our flower and this is what you should get now we will change the image to a 2d matrix so width height and depth will be equal to original shape which is tuple of flower dot shape here uh d is the depth will be three uh because as i as i explained the earlier each layer will correspond to either red green or blue so three values in this case and now we reshape it so image array is equal to np dot reshape flower and then we'll reshape with the width times the height and the other dimension will be the depth awesome now we will reduce the number of colors to 64 by running the kmeans algorithm where k will be set to 64. so our image sample will be equal to shuffle the image array we'll give it a random state equal to 42 so that the uh results are constant whenever we rerun the cell and we'll take the first 1000 samples now we will fit the kmeans algorithm and we set here the number of colors as i said this will be equal to 64. then k means will be equal to k means we initialize the model we pass in the number of clusters which is the same as the number of colors in this case and again the random state equal to 42 because as you know from the theory part um kmeans starts by randomly assigning uh each observation to a cluster so we keep the random state equal to 42 to give the same results every time and then we simply fit the algorithm then we get the indices for each color for the full image that will be useful when we need to reconstruct the image right so each pixel in the 2d array will be assigned to a certain cluster and that will help us to bring back the color and rebuild the image so it's simply the labels which is the prediction from the kmeans now we need to write a function to rebuild the image so like i said each pixel is assigned to a cluster which corresponds to a specific color so we define reconstruct underscore image and we will need as parameters the cluster centers we'll need the labels and we pass in the width and the height of the picture so d will be equal to the cluster centers dot shape and we take d at index one then the image will be simply an array of zeros in this case and the shape will of course be the width the height and the depth the label index will start at zero and then for i in the range of the width and for j in the range the height we write that image at index i j so this is the coordinates in the 2d matrix will be equal to the cluster centers at labels and that itself will be at the label index and then we increment the label index so plus equal one and finally we return the reconstructed image so that's it for this function now we are ready to display both the original image and the reconstructed one with only 64 colors so the first plot will be the original image so we'll turn off the axes and then plt.title will be the original image with 96 615 colors and then we will show the original image which in this case is simply flower and now our second plot so plt.figure 2. here we will display the reconstructed image so again turning off the axes the title will be here we will write a string actually while passing a parameter in this string so reconstructed image with n colors because you can change the number of colors we will do that after and then we show the reconstructed image so in here in there we will pass in our function we construct image and you pass in kmeans dot cluster underscore centers underscore pass in also the labels and you pass in the width and the height that we defined earlier and you get the following result so as you can see the integrity of the image is kept actually the flower itself is very similar i would say that only the background is very different so let's go above and change the number of colors just for fun so let's say we want only four colors so we're running these cells um as you can see now with four colors the image is very different but you can see it's almost like a an artistic effect that you can play around with so feel free to play around with this number of colors with yourself now let's work with pca for dimensionality reduction here we will work with the iris data set this data set has four features about three different kinds of iris flowers and our goal is to visualize the data set in two dimensions so from sqlearn.datasets we'll import load iris and from masculine decomposition import pca now let's load the iris dataset so iris will be equal to load iris the features is iris.data the target is iris dot target and then the labels or target names here is iris dot target underscore names awesome now let's initialize the pca algorithm and we will specify that we want only the first two principal components since we want a 2d plot then x underscore r is pca dot fit x dot transform x now let's actually print out the amount of variance that is explained by each principal component so the explained variance ratio from pca and you can extract this information from the pca object itself so it's pca dot explained underscore variance underscore ratio underscore running this cell as you can see the first principal component explains 92 percent of the variance and the second one 5 so that means that a total of 97 of the variance is explained with only two components so now we are ready to plot our data set in 2d and that data that newly transformed data contains about 97 of the variance of the original data set so here we'll just specify three different colors to distinguish between the three different kind of iris flowers so the final one will be ffa 600 and we'll set the line width equal to 2. then plt.figure and then for color in oh sorry so for color i target name in zip and we pass in colors we will pass in uh zero one and two and we pass in the target names so 0 1 and 2 here are simply the the classes right so we'll draw a scatter plot so plt.scatter xr when y is equal to i and zero and then x r when y is equal to i and one so this is basically the xaxis and then the yaxis and the color will be equal to uh the color at this point in the loop alpha will be equal to 0.8 and then lw we set it equal to lw that we specified above finally the label will be equal to the target name at that specific step in the loop now we will simply put a legend on our plot the location sorry the location equal will be equal to best and we don't want any shadow finally let's set a title to our plot so pca of iris dataset running this cell as you can see now we get this plot right here and so you can visualize in two dimension a dataset that contained four features and three classes so now you could follow up with some classifier maybe decision trees on this transform data set to classify each kind of flower alright so that's it for this data science crash course i hope that you enjoyed it and that you learned something interesting if you want more videos on this topic or videos on endtoend data science projects please check out my youtube channel until next time take care
