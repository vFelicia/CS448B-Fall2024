With timestamps:

00:00 - all right well it wouldn't you know so
00:01 - we covered uh we covered uh kubernetes
00:04 - cyber security it wouldn't be a good
00:06 - morning if we didn't include artificial
00:08 - intelligence uh in the mix here uh this
00:11 - is uh we're very fortunate to have a set
00:13 - of experts here this morning to discuss
00:16 - the ai landscape how it intersects with
00:18 - open source so i want to welcome all of
00:20 - them to the stage why don't you all come
00:22 - on up and uh
00:24 - sit here and
00:25 - i'll introduce each of you so uh deepak
00:28 - agarwal uh he is the vp of engineering
00:31 - uh and artificial intelligence uh at
00:34 - linkedin how many people here use
00:36 - linkedin
00:38 - a few of you uh so he's responsible for
00:40 - ai efforts across the entire company
00:43 - please have a seat um
00:45 - mazin gilbert is uh vice president of
00:47 - advanced technology at at labs he's
00:50 - heading up ai initiatives uh there as
00:53 - well
00:54 - uh let me make sure i've got my right
00:56 - list here uh terry singh is an author ai
01:01 - machine learning expert uh in uh both
01:04 - deep learning uh and ai at coursera uh
01:08 - and finally rachel thomas who's the
01:10 - co-founder of fast ai
01:13 - please give them a warm welcome
01:18 - so i want to kick off by just having
01:20 - each of you quickly
01:22 - talk about what you're doing in ai what
01:24 - your organization uh is doing um and uh
01:27 - uh what kind of business imperative it
01:29 - is for each of you so deepak i'm gonna
01:31 - start off with you
01:33 - so i'll probably start with
01:35 - the mission and vision of linkedin so
01:37 - linkedin
01:38 - for those of you are on it and for those
01:40 - of you who are not i'll encourage you to
01:42 - sign up uh so our mission is to connect
01:45 - uh talent with opportunity at scale and
01:48 - again opportunity i don't mean in a
01:50 - narrow sense of just finding a job
01:52 - opportunity could be any professional
01:54 - opportunity
01:55 - and so
01:57 - ai and machine learning is something
01:58 - that is embedded in all our product in
02:00 - fact we
02:02 - many times refer to be the oxygen of
02:04 - color product right so it kind of is a
02:07 - horizontal layer that permeates all our
02:09 - product so if you're on linkedin and
02:11 - you're looking for a job all the job
02:12 - recommendations you get is all powered
02:14 - through machine learning in ai if you're
02:16 - a recruiter trying to source a candidate
02:18 - then the search results which you're
02:19 - getting is powered through ai if you're
02:21 - on the news feed consuming content
02:23 - that's all powered through you if you're
02:24 - trying to connect with someone which you
02:26 - should
02:27 - for getting professional opportunities
02:29 - in the future those recommendations are
02:30 - powered here so it's kind of ubiquitous
02:32 - and we have been doing it for a long
02:34 - time now it's very mature and
02:37 - we are no longer in a state where we
02:39 - think about it it is kind of become an
02:41 - integral part of everything we do it's
02:44 - embedded in everything we do i think we
02:46 - are now at a state where we are thinking
02:47 - of
02:48 - what can we do with ai
02:50 - to power the next generation of user
02:52 - experience on the platform
02:56 - mazdan you guys are running some huge
02:58 - networks out there with hundreds of
03:00 - millions of users and 5gs just around
03:03 - the corner yeah here i see you're two in
03:04 - two cities
03:05 - yeah it's um so a tnt which i'm hoping
03:08 - that 95 of you guys are att customers
03:12 - today so thank you very much
03:14 - um so atnt's mission is is to really
03:17 - connect people with their world um where
03:20 - they live where they work and and really
03:22 - do it better than anybody else and when
03:24 - you think about a company who's who's
03:26 - their sole business is communication and
03:29 - entertainment
03:30 - ai is a foundation
03:32 - not just to drive one application but
03:34 - really to drive how we live and how we
03:36 - work as a society and basically globally
03:39 - so if you're thinking about well how
03:41 - would we drive 4g and go into 5g and how
03:44 - do we really scale cloud technologies
03:48 - all the way to the edge or really how do
03:50 - we try to try to drive um
03:53 - our network how do we operationalize our
03:55 - network we talked about security earlier
03:57 - on how do we make sure we get you know
04:00 - uh tens of millions of attacks every day
04:02 - on our network how do we really address
04:04 - those really ai is the foundation of
04:07 - pretty much our business from the get-go
04:11 - so uh i'm gonna start with the mission
04:13 - and the vision no i'm just kidding
04:17 - so my name is terry um i am a founder
04:19 - ceo uh neuroscience researcher uh
04:22 - studying brain and that kind of stuff
04:24 - uh deep kapha.ai i'm also a mentor at
04:27 - coursera uh
04:29 - working with a couple smart people
04:30 - andrew and a bunch of other guys who are
04:32 - developing uh deep learning
04:34 - specialization
04:35 - so uh help you know we're this year we
04:37 - have a plan to get about a couple
04:39 - hundred thousand to about a million
04:40 - people are trained in deep learning and
04:42 - ai
04:44 - and an amazing amazing training uh set
04:47 - up you know from stanford and andrew
04:49 - setting it up and i guess i know you do
04:51 - that a lot of stuff as well with fast ai
04:53 - but so so you know very short what i do
04:57 - is i i work with enterprises and and
05:00 - helping specifically they could be
05:02 - engineers software developers
05:04 - or they could be phd post docs as well
05:06 - training them to to work with uh deep
05:08 - learning techniques right
05:10 - composition neural networks or vision or
05:13 - or text kind of stuff you know for the
05:15 - guys
05:16 - who don't understand but uh so the goal
05:19 - is for for me the goal is to convert a
05:22 - whole lot of people
05:24 - to to adopt artificial intelligence or
05:27 - deep learning
05:28 - and and the other part of the things
05:29 - that i do if i'm not working with with
05:31 - the enterprise i work in the most
05:34 - difficult parts of the world i go to
05:35 - tunisia i
05:37 - even go to syria
05:39 - and and we work with women and these are
05:42 - young girls you know having all kinds of
05:44 - problems in the world and and for
05:46 - instance after this i'm going to tunisia
05:48 - after i'm done with this conference and
05:50 - then we go to turkey but we have you
05:52 - know
05:53 - people who were totally distracted with
05:56 - everything that life can take out of you
05:58 - and put those kids and say let's go and
06:00 - work and let's work with code let's work
06:03 - with tensorflow and this is this is what
06:05 - what i do so i guess i this is a bit of
06:07 - an introduction that's amazing that's uh
06:09 - that is incredible
06:10 - uh thank you rachel i your i checked out
06:12 - your book uh this is the practical deep
06:15 - learning for coders so
06:18 - lots of coders in the audience here
06:20 - yeah so um i'm rachel thomas i'm
06:22 - co-founder of fastai which is a
06:24 - non-profit research lab and i'm also a
06:26 - professor at the university of san
06:28 - francisco and with fastai we're trying
06:30 - to make deep learning easier to use and
06:32 - so we do this both through kind of
06:34 - building the tools we have an open
06:35 - source library
06:37 - it's kind of very high level and encodes
06:39 - best practices called fastai and we also
06:42 - have a free course practical deep
06:43 - learning for coders over a hundred
06:45 - thousand students have taken it we've
06:47 - had students get jobs at google brain
06:50 - have their work featured on hbo
06:52 - but we're particularly trying to reach
06:54 - coders who don't have to have any
06:56 - advanced math background
06:59 - and we're interested in people that are
07:01 - i think particularly that are kind of
07:02 - working on
07:04 - projects outside the mainstream things
07:06 - they very care about very much care
07:08 - about and don't have access to a lot of
07:10 - resources and we've had students
07:13 - improve agricultural loans in india try
07:16 - to stop illegal deforestation of
07:19 - endangered rainforests
07:21 - help patients with parkinson's diseases
07:23 - um so a lot of interesting applications
07:26 - amazing very very cool well i want you
07:29 - know for for a lot of i get these
07:30 - questions all the time a lot of people
07:31 - in the audience find sort of the ai
07:34 - landscape totally confusing right that
07:38 - there's all these different tools and
07:39 - there's different ways to deploy and how
07:41 - you do it i think you're all kind of the
07:42 - exception in that you're in roles where
07:44 - you're just far down that path
07:47 - but like you know if you're talking to
07:48 - someone who is maybe confused about it
07:50 - what are some of the exciting areas what
07:52 - are some of the things that people
07:53 - should be looking at right now in ai i
07:55 - think i'll just kind of go back around
07:57 - robin you're starting with you deepak
08:00 - yeah so i think first of all uh ai
08:03 - before 2012 was very different than what
08:06 - it is today and in 2012 something great
08:08 - happened so
08:10 - you know
08:11 - a bunch of profess researchers they were
08:13 - able to
08:15 - figure out a new way of computing things
08:18 - at scale by using gpu cards and this is
08:20 - what kind of ushered in the era we are
08:22 - in right so i would definitely want you
08:24 - to pay attention to
08:26 - everything that's happening in deep
08:27 - learning so
08:28 - what what has happened is that you you
08:30 - have with the with the availability of
08:33 - cloud computing data management has
08:35 - become commoditized right and various
08:38 - and with the availability of deep
08:40 - learning tools and you know things like
08:42 - past ai and what andrew is doing slowly
08:44 - that's also becoming commoditized so if
08:46 - you have a problem where you know
08:49 - you want to predict something by using a
08:51 - lot of input signal this is slowly
08:53 - getting commoditized and this alone can
08:56 - be transformative look at what has
08:57 - happened in computer vision look at what
08:59 - has happened in natural language
09:00 - processing and speech i mean these
09:02 - things have become way more accurate
09:04 - than what they were like 10 years ago
09:06 - and
09:07 - the impact of that is pervasive right
09:10 - you know in every area now we are kind
09:12 - of we are able to use ai technology to
09:14 - do things that you're not not able to do
09:16 - before like for instance in the old days
09:18 - if you're a radiologist you will
09:20 - actually send the mri image to india i
09:22 - know and then someone there is going to
09:23 - read the image and so in the morning you
09:25 - have it on your desk you don't have to
09:26 - do that anymore right i mean you know
09:28 - you can have ai software kind of do that
09:31 - for you and again i can go on and on
09:33 - with with example so definitely
09:34 - supervised this is this class of problem
09:37 - where you're predicting some output
09:38 - based on input that's called supervised
09:40 - learning and this is slowly getting
09:41 - commoditized and this alone can actually
09:43 - have a very big impact in many different
09:45 - things that you do right so you have
09:47 - this paradigm where you can have data
09:50 - learn itself
09:51 - learn learn patterns through
09:53 - through algorithms right you don't have
09:55 - to write rules to kind of program a
09:57 - computer you can have a computer learn
09:59 - there are other areas that are not very
10:01 - well
10:02 - that are not very well researched and we
10:04 - have a long way to go like if you look
10:05 - at unsupervised learning or human level
10:07 - intelligence
10:09 - uh that machine can learn that's still
10:11 - an open research area and i think in the
10:13 - next 10 years or so maybe we will be
10:16 - we will be there and it may become as
10:18 - commoditized as as the supervised
10:20 - learning techniques are
10:22 - called how about you mazda like you know
10:24 - you've been involved in some pretty
10:25 - cutting-edge stuff in terms of trying to
10:27 - get
10:28 - this technology out there and deployed
10:31 - you know i'll plug accumulates a little
10:33 - bit uh for you you know in terms of
10:35 - building a marketplace where these
10:36 - things can be reused what's exciting to
10:38 - you so um just for those who who who
10:40 - don't know me that i i did my phd in the
10:43 - 80s in neural nets
10:45 - for speech i was excited about ai
10:47 - because
10:48 - the the concept of getting a computer
10:51 - to use neural nets which at the time
10:53 - there is there was this resemblance of
10:55 - artificial neural nets with the
10:57 - biological neuron that still today
10:59 - there's that sort of association and
11:01 - confusion to get them to have a machine
11:04 - actually articulate sounds and speak
11:07 - just like humans do was was just amazing
11:09 - and completely fascinating and that's
11:11 - how our house started and we were at the
11:14 - time like few hundred people we will go
11:15 - to workshop we will go to conferences um
11:18 - and even in the past 20 years 30 years
11:20 - is there there's been phases of interest
11:23 - of ai it went for a buzz to a hype it
11:26 - went down again if you look at the
11:27 - literature
11:29 - uh deep learning and neural nets and ai
11:31 - never actually stopped in the community
11:33 - in the research communities being worked
11:36 - on for for at least three decades
11:38 - it is different now and i agree with
11:40 - deepak here that the distributed
11:43 - computing the gpus the the the flux of
11:46 - of larger data um is is really made a
11:50 - big change what i actually think that a
11:52 - bigger driver to that uh we started with
11:55 - a few hundred people today thanks to you
11:57 - guys that we have hundreds of thousands
11:59 - going to the millions really the big
12:02 - driver the big revolution is open source
12:05 - if you think before is that you know 20
12:08 - years ago when i was working on this is
12:10 - there there are a few of us who can
12:11 - write some code we would join very large
12:13 - companies who have deep pocket to have
12:15 - big computers great uh
12:18 - craig computers and others to be able to
12:20 - run these type of jobs that can require
12:23 - a lot of data
12:24 - today if you've gone to ces every
12:26 - company is now an ai company because to
12:29 - really be in that business now it's
12:31 - nothing more than just download a
12:32 - software and you can just get going and
12:35 - you can take a course at coursera or
12:37 - somewhere and you can pretty much get
12:39 - going in a very very short period of
12:40 - time and by doing that that's really
12:43 - trying to have created a huge revolution
12:45 - in the industry so what we did however
12:48 - even with that excitement and revolution
12:50 - from a company like at t and probably
12:52 - most of you are the same
12:53 - is that we are hitting big bottlenecks
12:56 - and our bottlenecks we believe those
12:58 - bottlenecks are
13:00 - tremendously large bottlenecks that ai
13:02 - cannot move to that next step
13:05 - of scale without addressing those the
13:07 - first one
13:09 - is that it's still the case that there
13:11 - are
13:12 - uh lack of understanding and and and
13:15 - training and learning about what ai is
13:18 - and where it can be used there are a lot
13:19 - of tools out there and the question is
13:22 - that which tool do you use and are they
13:24 - interconnected with each other we needed
13:26 - to figure out a way to harmonize those
13:28 - number two when you ask your team i need
13:30 - to build an ai solution they would go
13:32 - and start from pretty much from scratch
13:35 - there is no reusability of ai these are
13:37 - very expensive things to build it takes
13:40 - months it may take more years or so so
13:43 - what accumus is trying to do which we
13:44 - have announced on the linux foundation
13:47 - is that you create a marketplace
13:49 - a marketplace a distributed marketplace
13:51 - for ai so think of the app store
13:54 - with one difference
13:56 - is that with this app store is that the
13:58 - applications are built by many different
14:00 - tools it's agnostic to the tool that's
14:02 - being used that's number one number two
14:05 - is that the applications that you build
14:07 - in this marketplace
14:08 - they interconnect and interoperate think
14:11 - of them as microservices that
14:13 - interconnect so you could be using
14:14 - tensorflow and she could be using
14:17 - scikit-learn
14:18 - and you can actually connect the outputs
14:20 - of those to create new solutions
14:22 - the third thing is
14:24 - when we talk about machine learning a
14:26 - lot of people talk about data scientists
14:28 - and machine learners and so forth and
14:30 - then when you start thinking about what
14:31 - does it take to move that into
14:34 - production
14:35 - from an att
14:37 - it's sometimes months a year two years
14:39 - you have to figure out funding you have
14:40 - to get prioritization you have to put a
14:42 - team together the developers don't think
14:44 - the same way as the data scientists in
14:46 - fact these are completely different
14:47 - organizations okay that maybe they
14:49 - report to the same you know senior vp at
14:51 - some point what we try to do with acumas
14:54 - is really to streamline the process from
14:56 - a data scientist building a model to
14:59 - that model being completely production
15:00 - and do that in a matter of minutes as
15:03 - opposed to what it takes today and
15:04 - really have that as part of a
15:05 - marketplace that you can just download
15:07 - and run on any cloud and be agnostic to
15:09 - the cloud so
15:11 - this is a very big revolution it's a
15:13 - community we're all trying to really get
15:15 - together to change that i actually
15:17 - believe that without doing that it's
15:19 - going to be very hard for us to really
15:21 - move to where we are today where we are
15:23 - deploying ai for some applications to
15:26 - really making ai mainstream where
15:28 - practically a 12 year old kid who can
15:30 - design and build and deploy
15:33 - a website can basically do the same
15:35 - thing with ai well terry it sounds like
15:37 - you to some degree are doing that right
15:39 - you're working with young kids and
15:41 - they're able to take this open source
15:43 - tooling and do real things with it like
15:45 - what are your experiences there so
15:48 - yeah i think uh the the point that
15:50 - you've raised on open source is
15:52 - something we
15:53 - we just kind of assume you know it's
15:54 - like like fresh water you pick it up and
15:57 - you start building stuff so i want to
15:58 - say first thing is
16:00 - i i wouldn't know how to take these
16:03 - technologies uh either to the enterprise
16:07 - customers
16:09 - or into syria uh which i am going to be
16:11 - there on the 19th amazing you know we're
16:14 - going to be getting coverage from cnn
16:15 - and bbc and a couple other guys that's
16:17 - amazing
16:18 - but i almost forget
16:21 - that
16:22 - if it was not for
16:24 - you know
16:25 - you know i contributed tensorflow as
16:26 - well you know you can just go contribute
16:28 - to developer it's all open source it's
16:29 - like free you know it's like free as in
16:32 - super free that's that's great
16:35 - really i mean
16:36 - it's something i can just get on my hard
16:38 - disk and i can go and i can implement
16:41 - and and you can set up for servers you
16:42 - can set up virtual machines also free
16:44 - you can split up on virtualbox you know
16:46 - free up okay i think oracle has bought
16:48 - it but it's still free you can download
16:51 - it
16:52 - so uh you know i think
16:54 - that is something which i realized when
16:56 - i had a question so angela she she
16:59 - sent me a list of questions to all of us
17:01 - and i said hey you know this is the
17:03 - revolution it's like a silent revolution
17:06 - and you know guys like richard stallman
17:07 - all these guys everybody's been working
17:09 - on it it's it's really super we should
17:11 - be like super thankful for everybody all
17:14 - these millions of developers who make
17:16 - this happen so i can just pick up my
17:18 - laptop and and i can go to tunis where
17:21 - i'm going to be in tunis and there are
17:23 - like people pulling me all over the
17:24 - place so the political parties saying
17:26 - why don't you talk about ai because we
17:28 - need to really clear the air said okay
17:30 - fine so i'm going gonna make a businessy
17:32 - kind of presentation as long as you
17:34 - don't talk about that stupid robot
17:35 - called sofia
17:37 - but uh
17:38 - everything else is free you can take it
17:41 - and you can just implement it guys and
17:43 - and the practical example is this so i
17:45 - said okay so i have free stuff so what
17:47 - do i do
17:49 - and then you know so we said okay there
17:51 - is you know i can go into detail so i'll
17:53 - keep a little bit high level for example
17:55 - skin cancer detection is is something
17:58 - which you know you have you know human
18:00 - bias and then you have technology and
18:03 - and i said okay so i you know that stuff
18:05 - is free as well you can download it i
18:07 - have all those data sets in fact you
18:09 - know like i don't know it's like 40 50
18:11 - gigabytes i can put it on my flash disk
18:13 - and just go anywhere
18:14 - and we can train on those data sets
18:16 - provided by is i see this international
18:18 - skin image classification society here
18:20 - in the u.s
18:22 - there are a whole lot of data sets from
18:24 - skin cancer images you know just
18:26 - identifying your moles or you know those
18:28 - kind of you know if it's some uh
18:31 - malignant or if it's benign or if it's
18:33 - never you know people have been teaching
18:34 - me different things i'm not a surgeon
18:36 - but uh it's it's like okay so we have
18:38 - free software
18:40 - and then we have a problem which we can
18:42 - solve and then what that led to i gave a
18:45 - bunch of lectures on different you know
18:46 - i we're researching capsules just like
18:48 - 10 people in the world who are
18:49 - researching on capsules building stuff
18:52 - and and capsule networks is like the
18:54 - next uh let's say convolution neural
18:56 - networks thing
18:58 - or the evolution of that
19:00 - and and so we take that and then i said
19:02 - okay so we're going to take a step
19:03 - further who wants to develop an app in
19:05 - android free stuff again and okay core
19:09 - ml with apple is also free you can
19:10 - download it and can build an ios app as
19:12 - well and people are building apps right
19:14 - now so i'm just back from finland where
19:16 - uh i know if martin's still in the room
19:18 - uh
19:19 - you know being at espo and they're like
19:22 - really smart researchers and we're
19:23 - building apps so
19:24 - it's all possible i didn't have to go
19:26 - and anybody to to ask money or
19:28 - permission from a manager who would say
19:30 - i need to talk to an account manager
19:31 - because this big corporate company needs
19:32 - to give you software and when you have
19:34 - software you need licenses you need this
19:36 - you need that so
19:38 - it's it's just available it's it's super
19:41 - amazing rachel i want to ask you though
19:43 - a question that i i get a lot you know i
19:45 - i agree i think that we're standing on
19:48 - the shoulders of giants in terms of
19:50 - folks like richard stallman who came up
19:53 - with this concept of sharing and
19:55 - critical open source licenses and all
19:57 - the folks who followed that you know
19:58 - whether it you know folks from apache or
20:01 - other organizations
20:03 - but one question that we
20:04 - keep hearing is all right that's code
20:08 - what about data
20:09 - right how you know is is data the new
20:13 - proprietary right is this where
20:15 - one thing that um so i think there's
20:17 - some misconceptions and that a lot of
20:19 - people think that
20:21 - yeah you need google size data sets and
20:23 - you need you know like millions of
20:25 - dollars worth of gpu power to do deep
20:27 - learning
20:28 - and those aren't the case and so kind of
20:30 - getting to your question about this like
20:31 - what if you don't have the data um a lot
20:34 - of people are
20:35 - releasing
20:37 - pre-trained models so you know if
20:39 - someone has a large data set
20:41 - train a model they release that model
20:43 - and then you can do something called
20:44 - transfer learning where you are
20:46 - fine-tuning that to a much smaller data
20:48 - set so we had a student
20:50 - download i think it was just 20 pictures
20:52 - of people playing baseball 20 pictures
20:55 - of people playing cricket and trying to
20:57 - classify her to tell cricket from
20:59 - baseball just using 40 images
21:03 - and that's something because they were
21:04 - using this pre-trained net you know and
21:07 - just fine-tuning the last layers and so
21:09 - there's really amazing potential there i
21:11 - think in terms of being able to
21:13 - get deep learning to work on smaller
21:15 - data sets and it's part of why it's so
21:18 - important that people do um
21:20 - share their weights and their models
21:22 - openly through open source
21:25 - yeah and you know we worked on a
21:27 - data sharing license agreement that
21:29 - would both a copy left one which would
21:31 - be a share back license then a
21:32 - permissive license which you just didn't
21:34 - didn't need to do that um but we're
21:36 - trying to get ahead of this ability to
21:39 - kind of share yeah and there are issues
21:41 - because it's like i mean data you know
21:43 - is an important part of you know of how
21:45 - models are trained um but there's also
21:47 - so much around privacy and anonymous you
21:50 - know true anonymization is
21:52 - almost impossible and there have been
21:54 - several kind of high profile cases of
21:55 - people thinking they've anonymized data
21:57 - and then
21:59 - it's been de-anonymized
22:01 - and so i think there is kind of a bit of
22:02 - a tension sometimes between yeah wanting
22:04 - to protect privacy
22:06 - um
22:07 - and yeah like it is really important
22:09 - though to be sharing models in your
22:10 - training process yeah i wanted to
22:13 - imagine here because you know i mean
22:15 - you're the telecommunications market's
22:17 - very very competitive but you know to
22:19 - her point what what are data sets that
22:22 - you would want to share i don't know
22:23 - sell time with power maintenance or
22:25 - whatever that just isn't uh
22:27 - competitive per se but kind of follows
22:30 - that open source philosophy of like hey
22:31 - this is just data we want models that we
22:33 - just want to share with you do you see
22:34 - patterns there and so i i um i think the
22:38 - idea of of having companies share data
22:42 - is not new it's been people been talking
22:43 - about this for for several decades we've
22:46 - never cracked it we've never created
22:49 - an open
22:50 - shareable infrastructure community where
22:53 - people can easily with security can
22:56 - share data
22:57 - with hipaa requirements and their
22:59 - privacy rules i think we're starting to
23:01 - get almost there i think with you guys
23:03 - and and a lot of the policies you guys
23:05 - putting in place and that's what i'm
23:07 - hoping we're going to do with
23:08 - achievements achievements cannot really
23:10 - succeed without having a a really clear
23:12 - understanding of the data behind it from
23:15 - an att there are obviously data we can't
23:16 - share um there's no doubt about that you
23:19 - know we we we carry data and we track
23:21 - data about cell uh coverage so we those
23:24 - kind of data we cannot share but there
23:26 - are other data that we actually we want
23:28 - to do as part of the acumen to consider
23:30 - whether we can open a community looking
23:32 - at that just like you mentioned with
23:34 - transfer learning one of the we're a
23:36 - very capital intensive company so you
23:38 - can imagine that one of the things we do
23:39 - we send people out
23:41 - we own millions of polls we own
23:43 - thousands of cell sites macro cells
23:45 - small cells etc we send people literally
23:47 - up a poll to go and check a cell to see
23:50 - if there's something wrong with it if a
23:52 - wire is disconnected if it's rusty if
23:53 - there is dirt etc what we're trying to
23:56 - do now with transfer learning and ai is
23:58 - that
23:58 - we send a drone
24:00 - and that drone has visual capabilities
24:03 - that drone detect whether there is what
24:06 - the object it's looking at and we can't
24:08 - do that we don't have enough data to do
24:10 - that so we've just collected few
24:11 - hundreds of those data points
24:14 - and then that drones looks at to see if
24:16 - that object has a rust and if that's the
24:18 - issue that we can perhaps do something
24:20 - about or send somebody there or not 95
24:23 - of the time we don't need to send
24:24 - somebody so there's a safety aspect
24:26 - we're using this for we could never do
24:28 - that by collecting significant amount of
24:31 - data we can only do it with small set of
24:33 - data but thanks to the open source
24:35 - community by having models it's not just
24:37 - data people think of data as raw data
24:40 - and i think raw data is very important
24:42 - and there are situations we cannot
24:45 - share raw data for for many reasons
24:47 - but there are there are derivatives of
24:50 - data there are probably an area we need
24:52 - to talk more about which is that when
24:54 - you build these models these models now
24:57 - reflect weights
24:59 - and and capabilities that that resemble
25:02 - the data you cannot probably do a
25:03 - reverse engineering they hide the
25:05 - privacy aspect but those models can be
25:08 - shared and those models with some
25:10 - additional new data can do something
25:12 - really remarkable and that's exactly the
25:14 - kind of things you're talking about
25:16 - deepak how about you like at linkedin
25:18 - are there similar patterns where you're
25:20 - finding these sort of commodity
25:21 - components like you know what how do you
25:24 - make those decisions how are you doing
25:26 - yeah i think so
25:27 - i agree with everything so one thing i
25:29 - would like to mention even
25:31 - if you if you're a developer we provide
25:33 - developer apis where you can actually
25:35 - get information about linkedin public
25:37 - profile information right so for
25:38 - instance you can
25:40 - you can you can use the apis to get
25:42 - a person's
25:44 - job title or you know if you want more
25:46 - information about a company like how
25:47 - many employees work there and how many
25:49 - people have changed up so these are
25:50 - things that are available through the
25:52 - developer apis there are some other
25:53 - information that we provide to companies
25:55 - those are not available for free so you
25:58 - have to talk to us and based on the use
26:00 - case we can still provide that to you so
26:01 - that's already happening
26:03 - now i think one example for instance i
26:06 - can give you right away for instance you
26:08 - know one of the things challenges we
26:10 - face on our news feed is there is a lot
26:11 - of
26:12 - uh
26:14 - non-professional content
26:16 - that gets like things like hate speech
26:18 - and porn and things like that so
26:21 - we don't build those models from the
26:22 - scratch like for instance there have
26:24 - been a lot of nice models that have been
26:26 - built by using imagenet data using
26:28 - resnet and again we use the same
26:29 - technique like we chop off the last two
26:32 - layers of the neural net and then
26:33 - customize it for our use cases
26:36 - we have a lot of different teams that
26:38 - are working with different problems so
26:39 - we have a notion of what we call
26:41 - internally a feature marketplace right
26:44 - so features or
26:46 - signals that go into your machine
26:47 - learning models and we don't want every
26:49 - team to be building the same signals
26:51 - over and over again so there is this
26:53 - there is there is a framework which
26:56 - where you know if you create a certain
26:57 - user interest vector for instance or
27:00 - signal that kind of captures a user
27:02 - interest you can actually share it with
27:03 - every other team right so it goes into
27:04 - the feature marketplace and anyone can
27:06 - then grab it and start using it in their
27:07 - model so that's how we are scaling it
27:10 - and you know we are in a world where we
27:12 - are we are not just doing machine
27:13 - learning
27:16 - we don't have only experts doing machine
27:18 - learning anymore we have opened up the
27:19 - machine learning
27:20 - to every single software engineer in the
27:23 - company so we have training programs
27:24 - where every software engineer can get
27:26 - trained in machine learning we have a
27:27 - feature marketplace these think of these
27:29 - as cookie cutter prefabricated features
27:31 - if you will right and so if you are a
27:33 - developer you take a course you have
27:35 - prefabricated features available in the
27:36 - marketplace and you have a problem that
27:38 - you want to solve in the product you can
27:40 - go take these prefabricated features
27:42 - build a model and deploy it in your
27:43 - product you don't even have to talk to
27:44 - an expert in many cases okay
27:46 - i had a question this morning that i
27:48 - want to i'll start with you tyrion but i
27:49 - want to go to rachel and as and and come
27:51 - back to you deepak that uh it's not
27:53 - necessarily on our list of questions so
27:55 - i'm going to surprise you on this one
27:57 - there were a list of questions this yeah
27:59 - there was a list of questions
28:02 - um so you have a lot of developers and
28:04 - people who care about coding out here
28:06 - and um
28:08 - uh direct you'll remember this on the
28:09 - 25th anniversary of linux i made a toast
28:11 - to the colonel community and folks
28:14 - saying you know congratulations on 25
28:16 - years of linux
28:17 - and i want to announce our next big
28:19 - project after linux which is an
28:20 - artificial intelligence uh and uh that
28:23 - is actually self a self-coding platform
28:25 - so drink up because tonight's your last
28:28 - night of employment
28:30 - but this is i get this question all the
28:32 - time and there there's startup there's a
28:33 - startup in spain called source which
28:35 - sort of cached all the code that's on
28:37 - github and never been written in a lot
28:39 - of other repositories
28:41 - just
28:42 - as experts i'll start with you terry
28:45 - when are we going to get to self co to
28:47 - to aware coding like to the ability to
28:49 - either like azure coding to use ai to
28:51 - improve the quality or to actually have
28:53 - self-coding systems
28:56 - crazily i get this question all yeah
28:59 - yeah
29:00 - you know um
29:02 - what you
29:04 - probably are never gonna get and i hope
29:06 - you're not gonna ask about these general
29:08 - ai kind of things because i kind of shut
29:10 - down i i get it i told you yeah yeah the
29:12 - the evil robots take it yeah yeah yeah
29:14 - yeah yeah yeah yeah no no this is
29:15 - specific so i think it the most
29:17 - beautiful thing is that there's a whole
29:20 - lot of code out there
29:21 - uh the intuitions behind the way the
29:25 - code has been written
29:27 - is not something you can encapsulate in
29:29 - software and create a kind of an
29:32 - automated software development kind of
29:34 - library that says you know these guys
29:36 - are great in in convolution neural
29:39 - networks image classification kind of
29:41 - technology
29:42 - that these guys are great in in
29:44 - recurrent neural networks it's advanced
29:46 - kind of you know text analysis and
29:48 - making kind of predictions
29:50 - what you will not get is the intuition
29:52 - of what is coming next so as long as you
29:55 - are building things for instance just an
29:58 - example
29:59 - uh we know you know there's been delay
30:02 - and i've studied astronomy as well so i
30:04 - really really follow a whole lot of
30:05 - things and i code also on the side to to
30:08 - understand how we're kind of you know
30:10 - learning about gravitational waves and
30:11 - all that so we have web going into the
30:14 - space next year sometime in june
30:16 - and it keeps getting delayed it's really
30:18 - sad but anyways so we're going to get
30:20 - huge humongous amount of data coming out
30:22 - of the universe at us
30:23 - and and to do that you that's you know
30:26 - that software which we are super excited
30:28 - and self-congratulating that you know we
30:30 - have great software it's not going to
30:31 - help to a certain extent it will
30:33 - definitely help in best practices and
30:36 - and you know doing unit testing and all
30:38 - those things definitely huge scope in
30:40 - making those things work and automating
30:42 - it i say really i think we should
30:43 - automate that so from that perspective i
30:46 - think shouldn't worry too much about
30:47 - jobs getting out of you know the door
30:50 - because there's a whole lot of beautiful
30:51 - things we need to do if you need to you
30:52 - know colonize mars or you know something
30:54 - i wanted to do as well as a kid so i
30:56 - said like when you know this gentleman
30:57 - from south africa who comes to the u.s
30:59 - and sets up a bunch of companies says
31:01 - says hey okay makes sense but so there's
31:03 - a whole lot of beautiful things we need
31:04 - to solve in the universe so from that
31:06 - perspective there's definite like 60 70
31:08 - percent of encapsulation of doing things
31:10 - which we don't need to do anymore
31:13 - can be
31:14 - grabbed literally
31:16 - from github and and from even you know
31:19 - best practices from stack overflow
31:21 - whatever and you can put these and kind
31:23 - of provide guidance so people don't lose
31:25 - time we spend a lot of time doing a
31:27 - whole lot of stuff which we should not
31:28 - be doing so from that perspective i
31:30 - totally agree but i don't think my
31:33 - cognition as a human being a single
31:35 - human being i can already envision a
31:37 - universe i don't need like 100 people to
31:39 - do that i can do that already you know
31:40 - that power has been given to me
31:42 - the other thing is my intuitions of
31:44 - trying to solve a problem in it could be
31:46 - any problem here right now physical
31:48 - problem
31:49 - object detection problem is something is
31:51 - has doesn't exist at all um in in in
31:54 - software library what does definitely
31:56 - exist is best practices which are
31:58 - definitely search and seek out to
32:01 - and so yeah i think so i guess you know
32:04 - 60 70 percent of that work definitely
32:06 - can be automated so self encoding well
32:09 - we can call it self coding but basically
32:11 - just grab information understands how
32:13 - you want to follow that logic and
32:15 - eventually you know throw it into your
32:17 - uh into your algorithms and or in your
32:20 - software library and do that
32:22 - and the other 30 percent just keep you
32:23 - know
32:24 - hold on to it you know your intelligence
32:26 - and and your beautiful cognition
32:28 - and your powers to you know uh
32:30 - grab the universe and make it your own
32:32 - or yours they're not going anywhere
32:34 - they're not going away all right quick
32:36 - quick last word from rachel mazen and
32:38 - deepak on you know advice to coders out
32:40 - there and how can they use mlai to
32:43 - improve their projects to yeah i mean i
32:45 - would say with um i mean one just to
32:47 - know that it's possible if you know how
32:49 - to code you can learn to use deep
32:51 - learning um
32:54 - that
32:56 - domain expertise is
32:59 - still incredibly valuable um and so i
33:01 - think um something i hear a lot from
33:03 - companies is you know like oh it's
33:05 - so hard to hire like a stanford phd and
33:08 - it's just that's not what you need at
33:10 - all like the people that are already
33:13 - working with and for you are kind of the
33:15 - right people like they understand your
33:16 - problem and your domain um and so
33:20 - i guess again this is something that i
33:21 - think is
33:22 - like whatever um and i know here
33:25 - we're a lot of coders but um like having
33:28 - specialized knowledge um around a domain
33:31 - is still super crucial um so this showed
33:33 - up recently
33:35 - mit released a deep learning course and
33:37 - i don't know anything about the course
33:38 - but the image that they
33:40 - led with was like you know see why the
33:42 - algorithm predicted pneumothor thorax
33:44 - you know it's a picture of the lungs and
33:46 - a radiologist who's also a machine
33:49 - learning specialist responded and was
33:50 - like oh that doesn't make sense you know
33:52 - like uh that model must have been
33:54 - overfitted you know and so that like
33:56 - that kind of um domain expertise is
33:58 - going to remain very valuable for a long
34:00 - time and like
34:01 - one of our goals at fastai is to kind of
34:03 - be taking the domain experts and
34:06 - teaching them deep learning as opposed
34:08 - to trying to engage a
34:11 - deep learning specialist with your
34:13 - particular domain
34:14 - yeah
34:15 - mazdan deepak your last word on that
34:18 - debug yes so
34:21 - so i think in order to machine learning
34:22 - or ai you need three things one is you
34:24 - need to know what your objectives are
34:26 - right you need to know what you're
34:28 - trying to build the algorithm to do
34:30 - you need data label data and then you
34:32 - need algorithms that can learn from data
34:34 - so if you have a simple objective like
34:36 - identifying a cat in an image
34:38 - that's easy i think that is something
34:40 - that you can put together very quickly
34:41 - like with few lines of code because all
34:43 - the other materials are available right
34:45 - as long as you have label data but let's
34:47 - say if i have to solve a more complex
34:48 - objective like i want more users to come
34:51 - to linkedin every day
34:53 - this is a very complex objective right
34:55 - like users can come to linkedin because
34:57 - they like the news feed because they
34:58 - want to connect to people because they
35:00 - want to search for someone
35:03 - how do you formulate a series of machine
35:05 - learning problems that can actually
35:07 - solve this objective is
35:09 - very difficult like this
35:11 - you cannot in cos encapsulate at least
35:13 - today in a software
35:15 - right so that's where you have to go and
35:17 - understand the domain very well do some
35:19 - data analysis and
35:20 - once you're able to formulate those
35:22 - series of objectives then again the
35:24 - coding is very easy right so
35:26 - i would say i think in a few years
35:28 - everything else will get commoditized
35:31 - pretty much not completely but pretty
35:33 - much and
35:34 - we have to spend a lot more time
35:36 - understanding what you're really trying
35:37 - to solve like if you want to do job
35:39 - recommendations without caring about
35:40 - diversity well you know if you if your
35:42 - algorithm does not take care of that
35:44 - it's not going to optimize for that it's
35:46 - just going to optimize for number of
35:48 - applications but if you tell the
35:49 - algorithm now i care about that as well
35:51 - and
35:52 - put that as part of the object then it's
35:54 - going to do something about that so
35:56 - that that's that's probably what's going
35:58 - to happen in the next 10 years as we
36:00 - start commoditizing and we will be able
36:02 - to solve more complex problem than what
36:03 - we are able to do today
36:06 - um coders you know
36:08 - doesn't matter what your background is
36:10 - absolutely get involved do some training
36:13 - and learn this field and my organization
36:14 - every code every programmer has to
36:17 - doesn't matter what their they have a
36:18 - phd or a master's whatever it is 100
36:21 - compliance this year they all have to
36:22 - learn it they all have to be able to
36:24 - code machine learning and ai
36:25 - capabilities at t in general we put out
36:28 - six months ago a plan a program that
36:31 - everybody in the company we have 300 000
36:33 - employees to be able to go through mlai
36:36 - training even if you're a marketing or
36:38 - legal or whatever we all need to know
36:40 - this is not about just coders it's about
36:42 - everybody we the people in the legal
36:44 - team they need to understand and the
36:45 - marketing team needs to understand it uh
36:47 - financial people everyone really need to
36:49 - come to the same page so we go away from
36:52 - every problem ai is the solution that
36:55 - hype too really what are the key
36:57 - problems we need to solve for from an
36:58 - atn t and from even the coders is there
37:01 - look where the problems are where the
37:03 - challenges are as i have always told my
37:05 - team three things look for where
37:06 - spending a lot of money
37:08 - to look where there are opportunities of
37:10 - revenue we can have and we could do with
37:12 - these technologies and three is safety
37:14 - there are places where we can apply
37:16 - these technologies and i mentioned the
37:18 - thing about polls and the 5g for safety
37:20 - and i think that's really if you can
37:22 - start with a real problem just like what
37:24 - d-pad just mentioned real problem that
37:26 - really wants a solution that's probably
37:29 - a long way to go
37:30 - so here here if you're not training
37:32 - everybody in the organization you're
37:34 - going to be caught up in a never-ending
37:35 - hype cycle uh and that's a pretty
37:39 - interesting practical way that att is
37:40 - handling that problem so thank you
37:42 - everyone i really appreciate you uh
37:44 - coming here today

Cleaned transcript:

all right well it wouldn't you know so we covered uh we covered uh kubernetes cyber security it wouldn't be a good morning if we didn't include artificial intelligence uh in the mix here uh this is uh we're very fortunate to have a set of experts here this morning to discuss the ai landscape how it intersects with open source so i want to welcome all of them to the stage why don't you all come on up and uh sit here and i'll introduce each of you so uh deepak agarwal uh he is the vp of engineering uh and artificial intelligence uh at linkedin how many people here use linkedin a few of you uh so he's responsible for ai efforts across the entire company please have a seat um mazin gilbert is uh vice president of advanced technology at at labs he's heading up ai initiatives uh there as well uh let me make sure i've got my right list here uh terry singh is an author ai machine learning expert uh in uh both deep learning uh and ai at coursera uh and finally rachel thomas who's the cofounder of fast ai please give them a warm welcome so i want to kick off by just having each of you quickly talk about what you're doing in ai what your organization uh is doing um and uh uh what kind of business imperative it is for each of you so deepak i'm gonna start off with you so i'll probably start with the mission and vision of linkedin so linkedin for those of you are on it and for those of you who are not i'll encourage you to sign up uh so our mission is to connect uh talent with opportunity at scale and again opportunity i don't mean in a narrow sense of just finding a job opportunity could be any professional opportunity and so ai and machine learning is something that is embedded in all our product in fact we many times refer to be the oxygen of color product right so it kind of is a horizontal layer that permeates all our product so if you're on linkedin and you're looking for a job all the job recommendations you get is all powered through machine learning in ai if you're a recruiter trying to source a candidate then the search results which you're getting is powered through ai if you're on the news feed consuming content that's all powered through you if you're trying to connect with someone which you should for getting professional opportunities in the future those recommendations are powered here so it's kind of ubiquitous and we have been doing it for a long time now it's very mature and we are no longer in a state where we think about it it is kind of become an integral part of everything we do it's embedded in everything we do i think we are now at a state where we are thinking of what can we do with ai to power the next generation of user experience on the platform mazdan you guys are running some huge networks out there with hundreds of millions of users and 5gs just around the corner yeah here i see you're two in two cities yeah it's um so a tnt which i'm hoping that 95 of you guys are att customers today so thank you very much um so atnt's mission is is to really connect people with their world um where they live where they work and and really do it better than anybody else and when you think about a company who's who's their sole business is communication and entertainment ai is a foundation not just to drive one application but really to drive how we live and how we work as a society and basically globally so if you're thinking about well how would we drive 4g and go into 5g and how do we really scale cloud technologies all the way to the edge or really how do we try to try to drive um our network how do we operationalize our network we talked about security earlier on how do we make sure we get you know uh tens of millions of attacks every day on our network how do we really address those really ai is the foundation of pretty much our business from the getgo so uh i'm gonna start with the mission and the vision no i'm just kidding so my name is terry um i am a founder ceo uh neuroscience researcher uh studying brain and that kind of stuff uh deep kapha.ai i'm also a mentor at coursera uh working with a couple smart people andrew and a bunch of other guys who are developing uh deep learning specialization so uh help you know we're this year we have a plan to get about a couple hundred thousand to about a million people are trained in deep learning and ai and an amazing amazing training uh set up you know from stanford and andrew setting it up and i guess i know you do that a lot of stuff as well with fast ai but so so you know very short what i do is i i work with enterprises and and helping specifically they could be engineers software developers or they could be phd post docs as well training them to to work with uh deep learning techniques right composition neural networks or vision or or text kind of stuff you know for the guys who don't understand but uh so the goal is for for me the goal is to convert a whole lot of people to to adopt artificial intelligence or deep learning and and the other part of the things that i do if i'm not working with with the enterprise i work in the most difficult parts of the world i go to tunisia i even go to syria and and we work with women and these are young girls you know having all kinds of problems in the world and and for instance after this i'm going to tunisia after i'm done with this conference and then we go to turkey but we have you know people who were totally distracted with everything that life can take out of you and put those kids and say let's go and work and let's work with code let's work with tensorflow and this is this is what what i do so i guess i this is a bit of an introduction that's amazing that's uh that is incredible uh thank you rachel i your i checked out your book uh this is the practical deep learning for coders so lots of coders in the audience here yeah so um i'm rachel thomas i'm cofounder of fastai which is a nonprofit research lab and i'm also a professor at the university of san francisco and with fastai we're trying to make deep learning easier to use and so we do this both through kind of building the tools we have an open source library it's kind of very high level and encodes best practices called fastai and we also have a free course practical deep learning for coders over a hundred thousand students have taken it we've had students get jobs at google brain have their work featured on hbo but we're particularly trying to reach coders who don't have to have any advanced math background and we're interested in people that are i think particularly that are kind of working on projects outside the mainstream things they very care about very much care about and don't have access to a lot of resources and we've had students improve agricultural loans in india try to stop illegal deforestation of endangered rainforests help patients with parkinson's diseases um so a lot of interesting applications amazing very very cool well i want you know for for a lot of i get these questions all the time a lot of people in the audience find sort of the ai landscape totally confusing right that there's all these different tools and there's different ways to deploy and how you do it i think you're all kind of the exception in that you're in roles where you're just far down that path but like you know if you're talking to someone who is maybe confused about it what are some of the exciting areas what are some of the things that people should be looking at right now in ai i think i'll just kind of go back around robin you're starting with you deepak yeah so i think first of all uh ai before 2012 was very different than what it is today and in 2012 something great happened so you know a bunch of profess researchers they were able to figure out a new way of computing things at scale by using gpu cards and this is what kind of ushered in the era we are in right so i would definitely want you to pay attention to everything that's happening in deep learning so what what has happened is that you you have with the with the availability of cloud computing data management has become commoditized right and various and with the availability of deep learning tools and you know things like past ai and what andrew is doing slowly that's also becoming commoditized so if you have a problem where you know you want to predict something by using a lot of input signal this is slowly getting commoditized and this alone can be transformative look at what has happened in computer vision look at what has happened in natural language processing and speech i mean these things have become way more accurate than what they were like 10 years ago and the impact of that is pervasive right you know in every area now we are kind of we are able to use ai technology to do things that you're not not able to do before like for instance in the old days if you're a radiologist you will actually send the mri image to india i know and then someone there is going to read the image and so in the morning you have it on your desk you don't have to do that anymore right i mean you know you can have ai software kind of do that for you and again i can go on and on with with example so definitely supervised this is this class of problem where you're predicting some output based on input that's called supervised learning and this is slowly getting commoditized and this alone can actually have a very big impact in many different things that you do right so you have this paradigm where you can have data learn itself learn learn patterns through through algorithms right you don't have to write rules to kind of program a computer you can have a computer learn there are other areas that are not very well that are not very well researched and we have a long way to go like if you look at unsupervised learning or human level intelligence uh that machine can learn that's still an open research area and i think in the next 10 years or so maybe we will be we will be there and it may become as commoditized as as the supervised learning techniques are called how about you mazda like you know you've been involved in some pretty cuttingedge stuff in terms of trying to get this technology out there and deployed you know i'll plug accumulates a little bit uh for you you know in terms of building a marketplace where these things can be reused what's exciting to you so um just for those who who who don't know me that i i did my phd in the 80s in neural nets for speech i was excited about ai because the the concept of getting a computer to use neural nets which at the time there is there was this resemblance of artificial neural nets with the biological neuron that still today there's that sort of association and confusion to get them to have a machine actually articulate sounds and speak just like humans do was was just amazing and completely fascinating and that's how our house started and we were at the time like few hundred people we will go to workshop we will go to conferences um and even in the past 20 years 30 years is there there's been phases of interest of ai it went for a buzz to a hype it went down again if you look at the literature uh deep learning and neural nets and ai never actually stopped in the community in the research communities being worked on for for at least three decades it is different now and i agree with deepak here that the distributed computing the gpus the the the flux of of larger data um is is really made a big change what i actually think that a bigger driver to that uh we started with a few hundred people today thanks to you guys that we have hundreds of thousands going to the millions really the big driver the big revolution is open source if you think before is that you know 20 years ago when i was working on this is there there are a few of us who can write some code we would join very large companies who have deep pocket to have big computers great uh craig computers and others to be able to run these type of jobs that can require a lot of data today if you've gone to ces every company is now an ai company because to really be in that business now it's nothing more than just download a software and you can just get going and you can take a course at coursera or somewhere and you can pretty much get going in a very very short period of time and by doing that that's really trying to have created a huge revolution in the industry so what we did however even with that excitement and revolution from a company like at t and probably most of you are the same is that we are hitting big bottlenecks and our bottlenecks we believe those bottlenecks are tremendously large bottlenecks that ai cannot move to that next step of scale without addressing those the first one is that it's still the case that there are uh lack of understanding and and and training and learning about what ai is and where it can be used there are a lot of tools out there and the question is that which tool do you use and are they interconnected with each other we needed to figure out a way to harmonize those number two when you ask your team i need to build an ai solution they would go and start from pretty much from scratch there is no reusability of ai these are very expensive things to build it takes months it may take more years or so so what accumus is trying to do which we have announced on the linux foundation is that you create a marketplace a marketplace a distributed marketplace for ai so think of the app store with one difference is that with this app store is that the applications are built by many different tools it's agnostic to the tool that's being used that's number one number two is that the applications that you build in this marketplace they interconnect and interoperate think of them as microservices that interconnect so you could be using tensorflow and she could be using scikitlearn and you can actually connect the outputs of those to create new solutions the third thing is when we talk about machine learning a lot of people talk about data scientists and machine learners and so forth and then when you start thinking about what does it take to move that into production from an att it's sometimes months a year two years you have to figure out funding you have to get prioritization you have to put a team together the developers don't think the same way as the data scientists in fact these are completely different organizations okay that maybe they report to the same you know senior vp at some point what we try to do with acumas is really to streamline the process from a data scientist building a model to that model being completely production and do that in a matter of minutes as opposed to what it takes today and really have that as part of a marketplace that you can just download and run on any cloud and be agnostic to the cloud so this is a very big revolution it's a community we're all trying to really get together to change that i actually believe that without doing that it's going to be very hard for us to really move to where we are today where we are deploying ai for some applications to really making ai mainstream where practically a 12 year old kid who can design and build and deploy a website can basically do the same thing with ai well terry it sounds like you to some degree are doing that right you're working with young kids and they're able to take this open source tooling and do real things with it like what are your experiences there so yeah i think uh the the point that you've raised on open source is something we we just kind of assume you know it's like like fresh water you pick it up and you start building stuff so i want to say first thing is i i wouldn't know how to take these technologies uh either to the enterprise customers or into syria uh which i am going to be there on the 19th amazing you know we're going to be getting coverage from cnn and bbc and a couple other guys that's amazing but i almost forget that if it was not for you know you know i contributed tensorflow as well you know you can just go contribute to developer it's all open source it's like free you know it's like free as in super free that's that's great really i mean it's something i can just get on my hard disk and i can go and i can implement and and you can set up for servers you can set up virtual machines also free you can split up on virtualbox you know free up okay i think oracle has bought it but it's still free you can download it so uh you know i think that is something which i realized when i had a question so angela she she sent me a list of questions to all of us and i said hey you know this is the revolution it's like a silent revolution and you know guys like richard stallman all these guys everybody's been working on it it's it's really super we should be like super thankful for everybody all these millions of developers who make this happen so i can just pick up my laptop and and i can go to tunis where i'm going to be in tunis and there are like people pulling me all over the place so the political parties saying why don't you talk about ai because we need to really clear the air said okay fine so i'm going gonna make a businessy kind of presentation as long as you don't talk about that stupid robot called sofia but uh everything else is free you can take it and you can just implement it guys and and the practical example is this so i said okay so i have free stuff so what do i do and then you know so we said okay there is you know i can go into detail so i'll keep a little bit high level for example skin cancer detection is is something which you know you have you know human bias and then you have technology and and i said okay so i you know that stuff is free as well you can download it i have all those data sets in fact you know like i don't know it's like 40 50 gigabytes i can put it on my flash disk and just go anywhere and we can train on those data sets provided by is i see this international skin image classification society here in the u.s there are a whole lot of data sets from skin cancer images you know just identifying your moles or you know those kind of you know if it's some uh malignant or if it's benign or if it's never you know people have been teaching me different things i'm not a surgeon but uh it's it's like okay so we have free software and then we have a problem which we can solve and then what that led to i gave a bunch of lectures on different you know i we're researching capsules just like 10 people in the world who are researching on capsules building stuff and and capsule networks is like the next uh let's say convolution neural networks thing or the evolution of that and and so we take that and then i said okay so we're going to take a step further who wants to develop an app in android free stuff again and okay core ml with apple is also free you can download it and can build an ios app as well and people are building apps right now so i'm just back from finland where uh i know if martin's still in the room uh you know being at espo and they're like really smart researchers and we're building apps so it's all possible i didn't have to go and anybody to to ask money or permission from a manager who would say i need to talk to an account manager because this big corporate company needs to give you software and when you have software you need licenses you need this you need that so it's it's just available it's it's super amazing rachel i want to ask you though a question that i i get a lot you know i i agree i think that we're standing on the shoulders of giants in terms of folks like richard stallman who came up with this concept of sharing and critical open source licenses and all the folks who followed that you know whether it you know folks from apache or other organizations but one question that we keep hearing is all right that's code what about data right how you know is is data the new proprietary right is this where one thing that um so i think there's some misconceptions and that a lot of people think that yeah you need google size data sets and you need you know like millions of dollars worth of gpu power to do deep learning and those aren't the case and so kind of getting to your question about this like what if you don't have the data um a lot of people are releasing pretrained models so you know if someone has a large data set train a model they release that model and then you can do something called transfer learning where you are finetuning that to a much smaller data set so we had a student download i think it was just 20 pictures of people playing baseball 20 pictures of people playing cricket and trying to classify her to tell cricket from baseball just using 40 images and that's something because they were using this pretrained net you know and just finetuning the last layers and so there's really amazing potential there i think in terms of being able to get deep learning to work on smaller data sets and it's part of why it's so important that people do um share their weights and their models openly through open source yeah and you know we worked on a data sharing license agreement that would both a copy left one which would be a share back license then a permissive license which you just didn't didn't need to do that um but we're trying to get ahead of this ability to kind of share yeah and there are issues because it's like i mean data you know is an important part of you know of how models are trained um but there's also so much around privacy and anonymous you know true anonymization is almost impossible and there have been several kind of high profile cases of people thinking they've anonymized data and then it's been deanonymized and so i think there is kind of a bit of a tension sometimes between yeah wanting to protect privacy um and yeah like it is really important though to be sharing models in your training process yeah i wanted to imagine here because you know i mean you're the telecommunications market's very very competitive but you know to her point what what are data sets that you would want to share i don't know sell time with power maintenance or whatever that just isn't uh competitive per se but kind of follows that open source philosophy of like hey this is just data we want models that we just want to share with you do you see patterns there and so i i um i think the idea of of having companies share data is not new it's been people been talking about this for for several decades we've never cracked it we've never created an open shareable infrastructure community where people can easily with security can share data with hipaa requirements and their privacy rules i think we're starting to get almost there i think with you guys and and a lot of the policies you guys putting in place and that's what i'm hoping we're going to do with achievements achievements cannot really succeed without having a a really clear understanding of the data behind it from an att there are obviously data we can't share um there's no doubt about that you know we we we carry data and we track data about cell uh coverage so we those kind of data we cannot share but there are other data that we actually we want to do as part of the acumen to consider whether we can open a community looking at that just like you mentioned with transfer learning one of the we're a very capital intensive company so you can imagine that one of the things we do we send people out we own millions of polls we own thousands of cell sites macro cells small cells etc we send people literally up a poll to go and check a cell to see if there's something wrong with it if a wire is disconnected if it's rusty if there is dirt etc what we're trying to do now with transfer learning and ai is that we send a drone and that drone has visual capabilities that drone detect whether there is what the object it's looking at and we can't do that we don't have enough data to do that so we've just collected few hundreds of those data points and then that drones looks at to see if that object has a rust and if that's the issue that we can perhaps do something about or send somebody there or not 95 of the time we don't need to send somebody so there's a safety aspect we're using this for we could never do that by collecting significant amount of data we can only do it with small set of data but thanks to the open source community by having models it's not just data people think of data as raw data and i think raw data is very important and there are situations we cannot share raw data for for many reasons but there are there are derivatives of data there are probably an area we need to talk more about which is that when you build these models these models now reflect weights and and capabilities that that resemble the data you cannot probably do a reverse engineering they hide the privacy aspect but those models can be shared and those models with some additional new data can do something really remarkable and that's exactly the kind of things you're talking about deepak how about you like at linkedin are there similar patterns where you're finding these sort of commodity components like you know what how do you make those decisions how are you doing yeah i think so i agree with everything so one thing i would like to mention even if you if you're a developer we provide developer apis where you can actually get information about linkedin public profile information right so for instance you can you can you can use the apis to get a person's job title or you know if you want more information about a company like how many employees work there and how many people have changed up so these are things that are available through the developer apis there are some other information that we provide to companies those are not available for free so you have to talk to us and based on the use case we can still provide that to you so that's already happening now i think one example for instance i can give you right away for instance you know one of the things challenges we face on our news feed is there is a lot of uh nonprofessional content that gets like things like hate speech and porn and things like that so we don't build those models from the scratch like for instance there have been a lot of nice models that have been built by using imagenet data using resnet and again we use the same technique like we chop off the last two layers of the neural net and then customize it for our use cases we have a lot of different teams that are working with different problems so we have a notion of what we call internally a feature marketplace right so features or signals that go into your machine learning models and we don't want every team to be building the same signals over and over again so there is this there is there is a framework which where you know if you create a certain user interest vector for instance or signal that kind of captures a user interest you can actually share it with every other team right so it goes into the feature marketplace and anyone can then grab it and start using it in their model so that's how we are scaling it and you know we are in a world where we are we are not just doing machine learning we don't have only experts doing machine learning anymore we have opened up the machine learning to every single software engineer in the company so we have training programs where every software engineer can get trained in machine learning we have a feature marketplace these think of these as cookie cutter prefabricated features if you will right and so if you are a developer you take a course you have prefabricated features available in the marketplace and you have a problem that you want to solve in the product you can go take these prefabricated features build a model and deploy it in your product you don't even have to talk to an expert in many cases okay i had a question this morning that i want to i'll start with you tyrion but i want to go to rachel and as and and come back to you deepak that uh it's not necessarily on our list of questions so i'm going to surprise you on this one there were a list of questions this yeah there was a list of questions um so you have a lot of developers and people who care about coding out here and um uh direct you'll remember this on the 25th anniversary of linux i made a toast to the colonel community and folks saying you know congratulations on 25 years of linux and i want to announce our next big project after linux which is an artificial intelligence uh and uh that is actually self a selfcoding platform so drink up because tonight's your last night of employment but this is i get this question all the time and there there's startup there's a startup in spain called source which sort of cached all the code that's on github and never been written in a lot of other repositories just as experts i'll start with you terry when are we going to get to self co to to aware coding like to the ability to either like azure coding to use ai to improve the quality or to actually have selfcoding systems crazily i get this question all yeah yeah you know um what you probably are never gonna get and i hope you're not gonna ask about these general ai kind of things because i kind of shut down i i get it i told you yeah yeah the the evil robots take it yeah yeah yeah yeah yeah yeah yeah no no this is specific so i think it the most beautiful thing is that there's a whole lot of code out there uh the intuitions behind the way the code has been written is not something you can encapsulate in software and create a kind of an automated software development kind of library that says you know these guys are great in in convolution neural networks image classification kind of technology that these guys are great in in recurrent neural networks it's advanced kind of you know text analysis and making kind of predictions what you will not get is the intuition of what is coming next so as long as you are building things for instance just an example uh we know you know there's been delay and i've studied astronomy as well so i really really follow a whole lot of things and i code also on the side to to understand how we're kind of you know learning about gravitational waves and all that so we have web going into the space next year sometime in june and it keeps getting delayed it's really sad but anyways so we're going to get huge humongous amount of data coming out of the universe at us and and to do that you that's you know that software which we are super excited and selfcongratulating that you know we have great software it's not going to help to a certain extent it will definitely help in best practices and and you know doing unit testing and all those things definitely huge scope in making those things work and automating it i say really i think we should automate that so from that perspective i think shouldn't worry too much about jobs getting out of you know the door because there's a whole lot of beautiful things we need to do if you need to you know colonize mars or you know something i wanted to do as well as a kid so i said like when you know this gentleman from south africa who comes to the u.s and sets up a bunch of companies says says hey okay makes sense but so there's a whole lot of beautiful things we need to solve in the universe so from that perspective there's definite like 60 70 percent of encapsulation of doing things which we don't need to do anymore can be grabbed literally from github and and from even you know best practices from stack overflow whatever and you can put these and kind of provide guidance so people don't lose time we spend a lot of time doing a whole lot of stuff which we should not be doing so from that perspective i totally agree but i don't think my cognition as a human being a single human being i can already envision a universe i don't need like 100 people to do that i can do that already you know that power has been given to me the other thing is my intuitions of trying to solve a problem in it could be any problem here right now physical problem object detection problem is something is has doesn't exist at all um in in in software library what does definitely exist is best practices which are definitely search and seek out to and so yeah i think so i guess you know 60 70 percent of that work definitely can be automated so self encoding well we can call it self coding but basically just grab information understands how you want to follow that logic and eventually you know throw it into your uh into your algorithms and or in your software library and do that and the other 30 percent just keep you know hold on to it you know your intelligence and and your beautiful cognition and your powers to you know uh grab the universe and make it your own or yours they're not going anywhere they're not going away all right quick quick last word from rachel mazen and deepak on you know advice to coders out there and how can they use mlai to improve their projects to yeah i mean i would say with um i mean one just to know that it's possible if you know how to code you can learn to use deep learning um that domain expertise is still incredibly valuable um and so i think um something i hear a lot from companies is you know like oh it's so hard to hire like a stanford phd and it's just that's not what you need at all like the people that are already working with and for you are kind of the right people like they understand your problem and your domain um and so i guess again this is something that i think is like whatever um and i know here we're a lot of coders but um like having specialized knowledge um around a domain is still super crucial um so this showed up recently mit released a deep learning course and i don't know anything about the course but the image that they led with was like you know see why the algorithm predicted pneumothor thorax you know it's a picture of the lungs and a radiologist who's also a machine learning specialist responded and was like oh that doesn't make sense you know like uh that model must have been overfitted you know and so that like that kind of um domain expertise is going to remain very valuable for a long time and like one of our goals at fastai is to kind of be taking the domain experts and teaching them deep learning as opposed to trying to engage a deep learning specialist with your particular domain yeah mazdan deepak your last word on that debug yes so so i think in order to machine learning or ai you need three things one is you need to know what your objectives are right you need to know what you're trying to build the algorithm to do you need data label data and then you need algorithms that can learn from data so if you have a simple objective like identifying a cat in an image that's easy i think that is something that you can put together very quickly like with few lines of code because all the other materials are available right as long as you have label data but let's say if i have to solve a more complex objective like i want more users to come to linkedin every day this is a very complex objective right like users can come to linkedin because they like the news feed because they want to connect to people because they want to search for someone how do you formulate a series of machine learning problems that can actually solve this objective is very difficult like this you cannot in cos encapsulate at least today in a software right so that's where you have to go and understand the domain very well do some data analysis and once you're able to formulate those series of objectives then again the coding is very easy right so i would say i think in a few years everything else will get commoditized pretty much not completely but pretty much and we have to spend a lot more time understanding what you're really trying to solve like if you want to do job recommendations without caring about diversity well you know if you if your algorithm does not take care of that it's not going to optimize for that it's just going to optimize for number of applications but if you tell the algorithm now i care about that as well and put that as part of the object then it's going to do something about that so that that's that's probably what's going to happen in the next 10 years as we start commoditizing and we will be able to solve more complex problem than what we are able to do today um coders you know doesn't matter what your background is absolutely get involved do some training and learn this field and my organization every code every programmer has to doesn't matter what their they have a phd or a master's whatever it is 100 compliance this year they all have to learn it they all have to be able to code machine learning and ai capabilities at t in general we put out six months ago a plan a program that everybody in the company we have 300 000 employees to be able to go through mlai training even if you're a marketing or legal or whatever we all need to know this is not about just coders it's about everybody we the people in the legal team they need to understand and the marketing team needs to understand it uh financial people everyone really need to come to the same page so we go away from every problem ai is the solution that hype too really what are the key problems we need to solve for from an atn t and from even the coders is there look where the problems are where the challenges are as i have always told my team three things look for where spending a lot of money to look where there are opportunities of revenue we can have and we could do with these technologies and three is safety there are places where we can apply these technologies and i mentioned the thing about polls and the 5g for safety and i think that's really if you can start with a real problem just like what dpad just mentioned real problem that really wants a solution that's probably a long way to go so here here if you're not training everybody in the organization you're going to be caught up in a neverending hype cycle uh and that's a pretty interesting practical way that att is handling that problem so thank you everyone i really appreciate you uh coming here today
