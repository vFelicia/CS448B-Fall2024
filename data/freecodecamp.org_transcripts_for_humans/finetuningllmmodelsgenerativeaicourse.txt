With timestamps:

00:00 - learn all about fine-tuning llm models
00:02 - in this course Chris will teach you
00:04 - fine-tuning using Cura and Laura as well
00:07 - as quantization using llama 2 gradient
00:10 - and the Google Gemma model this crash
00:12 - course includes both theoretical and
00:15 - practical instruction to help you
00:17 - understand how to perform fine-tuning so
00:20 - guys uh here is an amazing crash course
00:22 - to help you understand that how actually
00:24 - you can perform fine tuning uh
00:27 - specifically with respect to llm models
00:29 - uh and again uh uh in this crash course
00:31 - we will be discussing the theoretical
00:33 - intuition what exactly fine tuning is
00:35 - how you can actually perform fine tuning
00:37 - we'll be learning about Concepts such as
00:38 - quantization Laura CLA PFT right and
00:42 - along with that we will be implementing
00:43 - multiple projects with the help of
00:45 - hugging face open source uh open source
00:48 - llm models like Lama 2 along with that
00:51 - we'll also be seeing how you can perform
00:52 - fine tuning with your own custom data
00:54 - set with the help of Google Gamma model
00:56 - now uh in most of the companies that are
00:59 - probably looking for ative AI roles uh
01:01 - usually want this specific skill set of
01:03 - fine-tuning and uh you should definitely
01:06 - know how you can actually fine tune with
01:07 - your own custom data set and how you can
01:09 - solve different different use cases so
01:12 - this entire crash course will be having
01:13 - a stepbystep process of understanding it
01:16 - includes both theoretical intuition
01:18 - practical intuition and we will be also
01:20 - developing completely end to-end
01:22 - Solutions with respect to this so I hope
01:24 - uh you will enjoy this series please
01:26 - make sure that you watch this series
01:27 - till the end and definitely you will be
01:29 - able to learn a lot of things out of it
01:31 - so yes best of luck let's go ahead and
01:33 - watch this series in one of our previous
01:36 - video i' had already shown you how you
01:37 - can actually fine tune Lama 2 model with
01:40 - the with your own custom data set and uh
01:43 - over there we learned about or we saw
01:45 - code that were related to something
01:46 - called as quantization Laura CLA
01:49 - techniques and all right and all these
01:51 - techniques are super important if you
01:53 - also want to train or fine tune your own
01:57 - llm models with your own custom data set
01:59 - now when I showed the code right when I
02:01 - executed that particular code many
02:03 - people had actually requested about to
02:06 - explain the theoretical indepth
02:08 - intuition about it and that is what I'm
02:10 - actually going to do uh the best thing
02:13 - is that when I learned about this
02:14 - theoretical intuition and I'm doing it
02:16 - from past 2 to 3 months it's quite
02:18 - amazing guys you now this is where that
02:20 - machine learning era is probably coming
02:22 - where I used to upload a lot of
02:24 - theoretical in-depth geometrical
02:26 - intuitions regarding various machine
02:28 - learning algorithms similarly here also
02:30 - in the series of videos in this video we
02:32 - are going to discuss about quantization
02:34 - now what exactly is quantization we
02:36 - going to discuss about that in the
02:38 - upcoming video we are going to see
02:40 - techniques like Laura CLA every maths
02:43 - intuition that is probably involved uh
02:45 - and these all are important for the
02:47 - fine-tuning technique right if I
02:49 - probably talk about generative AI one of
02:51 - the most important interview questions
02:54 - will be something related to fine tuning
02:56 - and what is the techniques that is
02:58 - usually used behind it right so what all
03:01 - things we are going to cover in this
03:02 - video in this video we going to talk
03:04 - about
03:05 - quantization um specifically when I say
03:08 - quantization it is all about model
03:10 - quantization because if you remember in
03:12 - our Lama 2 code right when we doing the
03:14 - fine tuning here you could see that we
03:18 - had put some parameters right regarding
03:21 - Precision base modeling we had spoken
03:24 - about quanis you know when what when we
03:27 - are downloading the models you know from
03:29 - a higher bit to a lower bit why we are
03:32 - specifically doing this I will be
03:33 - explaining about that right so with
03:36 - respect to each and every parameters
03:37 - definitely I will explain you the
03:39 - theoretical intuition and later on you
03:41 - just go ahead and see my previous video
03:44 - with respect to all the coding now you
03:45 - everything will make sense okay so what
03:49 - exactly is quation we're going to
03:50 - discuss you know uh we're going to
03:52 - discuss about full Precision half
03:54 - precision and this is something related
03:56 - to data types like how the data is
03:58 - stored in the memory when I specifically
04:01 - say data in llm models I will talk about
04:04 - weights and parameters right because at
04:08 - the end of the day llms are also deep
04:10 - learning neural networks in the form of
04:11 - Transformers or Bird right then we going
04:14 - to discuss about what exactly is
04:16 - calibration uh this is also called as
04:19 - like calibration in model quantization
04:21 - right we are going to also make sure
04:23 - that we are going to see some problems
04:25 - right how we can actually do calibration
04:28 - then there is different different modes
04:30 - of quation right uh first of all I will
04:33 - explain you the definition then only
04:35 - you'll be able to understand in modes of
04:36 - quation we're going to discuss about two
04:38 - types one is post trining Quant and
04:40 - Quant aware training right so these all
04:43 - are very important in terms of fin
04:45 - tuning techniques now let's go ahead and
04:48 - talk about quantization and we will try
04:51 - to see the definition right quantization
04:55 - okay now if you want to really
04:59 - understand
05:00 - the meaning of quation it is better to
05:03 - write a simple definition for it okay so
05:06 - quation basically means
05:09 - conversion
05:12 - from
05:14 - higher memory
05:18 - format to a
05:23 - lower memory
05:25 - format
05:28 - right now now I've written a very
05:31 - generic definition what exactly
05:33 - quantization mean it is nothing but
05:36 - conversion from a higher memory format
05:38 - to a lower memory format now when I say
05:40 - higher memory format let's let's
05:42 - consider um any data right and if I
05:45 - probably consider any neural network
05:47 - okay so let's say if I have neural
05:49 - network right and when we train this
05:51 - neural network right all this neural
05:53 - network are interconnected at the end of
05:56 - the day what are the parameters that is
05:57 - probably involved over here it is
05:59 - nothing but we weights right we
06:01 - specifically have weights now weights
06:04 - are usually in the form of metrics right
06:07 - let's say that I have a 3 cross3 weight
06:09 - okay I'm just taking as an example in
06:11 - one of the layer I have 3 cross three
06:13 - weights and over here every value right
06:18 - is probably
06:20 - stored in the memory in the form of 32
06:25 - bits right 32 bits we also say this bits
06:29 - as we also we also denote it as
06:31 - something like something like fp32 what
06:34 - exactly is fp32 so fp32 basically means
06:38 - I can also consider it as see FP full
06:41 - form is not floating Point okay but we
06:43 - are I'm just writing floating Point 32
06:46 - bits right when I say FP FP basically
06:50 - means it is nothing but full
06:52 - Precision right full Precision or single
06:56 - Precision okay so this is the definition
06:58 - that is probably given right but in
07:00 - short this is like a floating Point
07:02 - number okay so over here let's say my
07:04 - number is somewhere around
07:06 - 7.23 now this number is stored based on
07:10 - 32 bits in the memory
07:13 - right now understand when you have a
07:16 - very big neural network or you have llm
07:19 - models right as you see different
07:21 - different llm models right parameters
07:24 - keeps on increasing right some may have
07:27 - 70 billion parameters if I probably
07:28 - consider llama 2 with 70 billion
07:32 - parameters that basically means it has
07:34 - 70 billion parameters in terms of
07:35 - weights and bias okay now it is not
07:39 - possible for me to let's say I want to
07:41 - use this particular model and I want to
07:44 - probably do a some fine tuning with
07:46 - respect to the normal GPU that I have
07:48 - right and let's say I have a very
07:50 - limited Ram in my system let's say the
07:52 - ram I have is somewhere around 32 GB
07:54 - right I cannot directly download the
07:57 - specific model and put it in my me in my
08:01 - Ram itself right let's say or load it in
08:03 - my vram that is available in the GPU
08:05 - because GPU also has some limited Ram
08:08 - right and it is not possible you cannot
08:10 - directly download it obviously it will
08:12 - require h space the other way is that
08:14 - yes I can probably take a cloud space
08:16 - somewhere let's say in AWS I can create
08:19 - my instance I can say hey give me this
08:21 - this much RAM 64 GB RAM and I probably
08:24 - want this much GPU and then I will try
08:26 - to load the model over there that you
08:28 - can do it but over there what is
08:30 - basically happening lot of cost is
08:33 - involved right based on the
08:36 - resources the cost is involved
08:39 - right so in this scenario and why the 70
08:43 - billion parameter is happening why this
08:45 - model has a 7 parameter because every
08:48 - weights or every bias that is available
08:50 - in this you know this may be getting
08:53 - stored in 32 bits so what we can
08:56 - specifically do is that we can con
08:59 - convert this 32 bits into as I said over
09:03 - see conversion from a higher memory
09:05 - format to a lower memory format let's
09:07 - say I I can probably convert this 32 bit
09:09 - into int 8 and then download the model
09:13 - or then use the model right after doing
09:17 - this what will happen within my system I
09:19 - will be able to inference it right
09:22 - obviously for fine tuning if I want to F
09:24 - tune with the new data set I will
09:26 - obviously require GPU but if I consider
09:28 - with respect to inferencing it becomes
09:31 - quite easy because now all my values
09:33 - that are stored in the form of 32 bits
09:35 - it will be stored in the form of 8
09:38 - Bits so what we are specifically doing
09:40 - over here we are converting from a high
09:42 - memory format to a low memory format and
09:45 - this is what is called as Quant
09:48 - quantisation a very important thing now
09:52 - why quantisation is important because
09:55 - you will be able to inference it quickly
09:57 - see inferencing basically means what if
09:59 - I have an llm model if I give any input
10:02 - to that I should be able to get any
10:03 - output right I should be able to get
10:05 - response right now when I give any input
10:08 - all the calculation with respect to
10:10 - different different weights will happen
10:11 - right and obviously if I have a bigger
10:14 - GPU this inferencing will quickly happen
10:17 - right but if I have a GPU with less less
10:20 - scores let's say then what will happen
10:22 - this calculation will take time but if I
10:26 - convert my 32 bit to 8 Bits right every
10:30 - weights are basically converted into Ed
10:32 - bits now just imagine the calculation
10:34 - will there be a difference yes it will
10:36 - happen little bit much more quicker so
10:39 - quantization is very much important for
10:42 - inferencing and some of the example that
10:44 - I can probably talk about is that and
10:46 - obviously you may have heard about this
10:48 - it's not like only in llm model we
10:50 - specifically do right in different
10:52 - computer vision models in NLP models
10:54 - also where you think that there is a lot
10:56 - of Weights that is involved and all
10:58 - these weights if I I want to quantize it
11:01 - right I can actually do it right now
11:03 - this inferencing let's say I want to use
11:05 - a specific deep learning model in my
11:07 - mobile
11:09 - phone so in my mobile phone if I want to
11:12 - use it right in in a specific app then
11:15 - what I will do I will try to quantize
11:17 - right whatever deep learning model I
11:19 - have created from 32 bits to 8 bit and
11:21 - then I will try to deploy it in my
11:23 - mobile phone or any Edge
11:25 - device any Edge device right it is not
11:29 - not possible that you can probably
11:30 - deploy this big model over there right
11:33 - it is not possible so and so many
11:35 - parameters so what we do we basically
11:37 - perform quantisation so I hope you're
11:39 - able to understand over here
11:41 - quantisation is nothing but we are
11:43 - trying to lower down the memory right
11:47 - with respect to any weights that we have
11:49 - like from 32 to probably 6 uh inate or
11:54 - let's say fp16 we can also say fp16
11:57 - right let's say if I have an FP
11:59 - p32 bit that is specifically required to
12:02 - store any information in my memory I can
12:05 - also convert this into FP 16bit this is
12:09 - also quation only
12:12 - right usually all these values are
12:14 - stored in floating point right we
12:17 - specifically say this fp32 bit we say
12:19 - single Precision or full Precision we uh
12:22 - for FP 16 bit if I'm trying to convert
12:24 - like this it is basically called as half
12:26 - Precision right so you should be able to
12:29 - understand all these technical terms
12:31 - right and in short these are nothing but
12:33 - these are floating Point numbers right
12:35 - now similarly in tensor flow also you'll
12:37 - be able to see when we probably work
12:40 - with tensor flow you'll find tf32 bit
12:43 - right the data types the the numbers are
12:45 - stored in this particular format right
12:48 - and it is important okay this all are
12:51 - terminologies that are super important
12:53 - but I hope you got an idea what what is
12:55 - the main aim what is the main motivation
12:58 - out of quation is that if I have a
13:00 - bigger model I should be able to
13:02 - quantise it and make it as a smaller
13:03 - model so that I can use it for my faster
13:07 - inferencing purpose both in Mobile
13:09 - phones in Edge devices let's say in even
13:12 - watches smart watches I want to use it
13:15 - over here I can actually do that right
13:19 - now if I talk with respect to llm model
13:21 - also with the help of quation see once
13:24 - we compress this particular model right
13:26 - later on we can also perform find tuning
13:30 - right fine tuning but here there is one
13:33 - disadvantage when we quantize right when
13:37 - we perform this
13:38 - quation since we are converting from 32
13:41 - bits to intake let's say as an example
13:43 - there is some loss of information also
13:45 - and because of this there will be some
13:47 - loss of accuracy now how to overcome
13:50 - this we will talk about it there are
13:52 - different different techniques how we
13:54 - can specifically overcome it but I hope
13:57 - you got an example what EXA quation what
14:00 - is full Precision half Precision half
14:02 - Precision example is something like this
14:04 - now let's talk about what exactly is
14:07 - calibration now calibration basically
14:09 - means how we will be able to convert
14:12 - this 32bit into int 8 like what is the
14:16 - formula what is the mathematical
14:18 - intuition that is specifically required
14:20 - let's go ahead and discuss that so guys
14:23 - now let's go ahead and try to understand
14:26 - how to perform quantization and this is
14:29 - super important in terms of mathematical
14:32 - concept that I'm probably going to talk
14:34 - about because with with the help of
14:37 - tensor FL just by writing four lines of
14:38 - code you know I will be able to perform
14:40 - quation but it is important you should
14:43 - know that how you can actually do it
14:44 - manually whenever I talk in terms of
14:48 - what are the types of quation that we
14:50 - have so we have two different types of
14:53 - conation one is symmetric contag and one
14:56 - is called as asymmetric contag now just
14:59 - by showing you an example you will be
15:01 - able to understand what is the exact
15:02 - difference between them okay let's say I
15:05 - have a task and this first task that I
15:07 - am probably going to talk is with
15:09 - respect to symmetric and understand I
15:12 - hope in deep learning you have heard of
15:13 - something called as batch
15:16 - normalization so if you have heard about
15:18 - this batch normalization so batch
15:20 - normalization is a technique of
15:23 - symmetric quantisation right so every
15:25 - time you'll be able to see that whenever
15:27 - we do forward propag and backward
15:29 - propagation in between all the layers we
15:31 - apply batch normalization so that all
15:33 - our all our weights are zero centered
15:37 - that is near the zero and the entire
15:39 - distribution of the weights will be
15:40 - centered near zero okay so batch
15:43 - normalization is one technique uh of
15:46 - symmetric quantization so let's go ahead
15:47 - and see one example so this will be my
15:49 - first example over here I will go ahead
15:51 - and write it down and now you'll be able
15:53 - to understand it how symmetric
15:55 - quantisation is basically performed what
15:57 - is quation you have understood from
16:00 - higher memory format to lower memory
16:01 - format we'll try to convert okay so here
16:04 - we are going to understand the
16:05 - mathematical intution so let's go ahead
16:07 - and talk about one technique which is
16:09 - called as
16:11 - symmetric
16:13 - unsigned unsigned in it unside in8 okay
16:18 - quantization so we will see this
16:20 - technique
16:21 - first now here what is our main aim
16:25 - let's say I have a floating Point number
16:28 - number okay so let's go ahead and write
16:31 - it down let's say I have a floating
16:33 - Point number between 0 to th000 now just
16:37 - imagine that these are my
16:38 - weights right whatever Matrix Ms I have
16:42 - my values ranges between 0 to th000
16:45 - Let's consider in this particular way
16:47 - right and this is let's say these are
16:50 - the weights for my larger model OKAY
16:53 - larger
16:54 - model okay now one very important thing
16:58 - that you need to understand right when I
17:01 - talk about any larger model let's
17:03 - consider any llm model okay so in llm
17:06 - model you have lot of parameters let's
17:08 - consider this is one kind of llm model
17:11 - now when I say all these weights are
17:13 - there this may be getting stored in 32
17:16 - bits okay usually uh what will happen
17:19 - guys the weights will not be in this
17:21 - range also okay it will be in the
17:22 - minimalistic range so I will just
17:25 - consider this as some numbers okay so
17:27 - that you don't get confused with respect
17:30 - to these are some numbers I will also
17:32 - not consider llm model over here and
17:34 - let's say this this this numbers are
17:36 - stored in the form of 32 bits okay now
17:40 - my main aim is to convert this
17:44 - into unsigned in 8 that basically means
17:49 - 8bit right so 8bit basically means what
17:52 - 2 ra to 8 is how much 2 to 8 but when we
17:55 - say U unsigned that basically means my
17:58 - value will be ranging between 0 to 255
18:00 - so I want to quantise from this values
18:03 - to this value okay what is my aim I want
18:06 - to quantize my range of values between 0
18:08 - to th000 to 0 to
18:11 - 255 okay this is what in my aim with
18:14 - respect to this okay so this is what is
18:18 - my target now let's see okay so if I
18:21 - probably draw just a real points and the
18:24 - same thing we will do with the weights
18:25 - over there whatever quantization process
18:27 - we are specifically doing let's say I
18:29 - have values between zero and this is
18:31 - basically th000 okay then what I will do
18:35 - over
18:36 - here I want to convert this into again 0
18:40 - to 255 okay now let me talk about one
18:44 - very important thing guys whenever we
18:46 - have any let's let's consider this one
18:50 - if we have this single
18:53 - Precision single
18:55 - Precision floating
18:57 - Point 30
18:59 - 2 right if you have this number how this
19:02 - number is stored do you know that the
19:05 - one bit will specifically be used for
19:09 - sign or unsign values right let's say
19:11 - positive or negative if positive is
19:12 - there then this will be plus one if
19:13 - negative is there it will be minus one
19:15 - so all the values are basically saved
19:17 - between 0 to 1 right uh it can be Z or 1
19:20 - okay then the next eight bits are
19:25 - stored for exponent okay 1 2 3 4 5 6 7 8
19:33 - okay so this is for
19:35 - sign these all numbers that you
19:38 - see it is basically stored for
19:44 - exponent this is how it is stored inside
19:46 - the memory and remaining 23
19:50 - numbers all these 23 bits will basically
19:54 - be saved for
19:56 - manesa this is specifically for the
19:58 - fraction
19:59 - so if I have a number which looks like
20:02 - 7.32 now 7.32 is a number it is a
20:04 - positive number so for my sign bit there
20:08 - will be a positive value let's say one
20:09 - over here then this seven will be
20:13 - probably put up in this 8 Bit And
20:16 - remaining 32 will be put up in this
20:20 - mantisa right so this is how the numbers
20:23 - are basically stored in the memory right
20:26 - if I consider an example with respect to
20:28 - to FP 16 right half Precision floating
20:32 - Point 16 bit then you'll be able to see
20:35 - there will be one bit for the sign
20:37 - number there'll be five bits for the
20:40 - exponent we basically say exponent 1 2 3
20:43 - 4 5 let's consider this
20:47 - five let's let's draw till here okay so
20:50 - this let's let's let's say that this is
20:52 - five 1 2 3 4 5 Okay so this will be five
20:57 - and remaining remaining 10 bits will be
21:00 - saved with respect to
21:02 - mantisa mantisa so we basically say this
21:05 - as a fraction right anything that comes
21:07 - after the decimal right and this is how
21:10 - you will be able to see it will take
21:11 - this will take less memory this will
21:12 - take High memory right now what is our
21:14 - main aim over here I already have a
21:17 - 32-bit number I need to probably convert
21:19 - this into a range of unsigned intake
21:22 - unsigned in end basically means I will
21:25 - not take any negative numbers it will be
21:26 - between 0 to 255 right this is what I
21:29 - really want to do okay now for this what
21:33 - will be the equation and I hope you have
21:35 - heard about something called as minmax
21:38 - scalar I've I've repeated so many times
21:41 - this in my machine learning sections
21:42 - also so any number that is over here how
21:45 - will I be able to convert from this
21:47 - floating point to this unit8 unit inate
21:51 - right unsigned intent right how I will
21:54 - be able to do it now for this let's go
21:56 - ahead and calculate it and what equation
21:58 - is specifically required okay so that
22:01 - basically means over here what we are
22:02 - going to do 0.0 will be converted into a
22:05 - quantied value of Zer okay not 0 Z it
22:09 - will be zero and similarly thousand
22:11 - should be converted to a quantize value
22:13 - of 255 at the end of the day the bits
22:16 - are decreasing so quantization is
22:18 - basically happening but we have to
22:19 - probably come up with a scale factor now
22:22 - what exactly is a scale factor so let me
22:23 - Define a scale over here so here the
22:27 - scale formula will be x max / x mean and
22:32 - then they will be divided by Q Max minus
22:35 - Q
22:36 - mean this Q is nothing but quantization
22:39 - now what is the x max over here 1,000
22:42 - right so this I will consider as my X
22:45 - this I will consider as my Q right I'm
22:48 - showing you how quation happens in a
22:50 - symmetric distribution symmetric
22:52 - basically means all the data is evenly
22:55 - distributed okay and we really need to
22:57 - convert this based on this itself these
23:01 - are evenly distributed now what is xmax
23:03 - xmax is nothing but 1,000 minus
23:07 - 0 then qmax qmax basically means
23:11 - 255 - 0 right so if I probably go ahead
23:16 - with this specific division right then
23:19 - what will be the value that I will be
23:21 - having right 1,000 divid by 255 it is
23:23 - nothing but
23:25 - 3.92 so this is nothing but this is
23:28 - called as a scale factor right scale
23:31 - factor so any number that I have over
23:34 - here if I want to convert from this fp32
23:38 - bit to U in 8 I just need to use the
23:41 - scale along with one formula which is
23:44 - called as round okay so I will apply
23:47 - round with respect to any number let's
23:49 - say if I consider 500 /
23:53 - 3.92 or let's let's just consider 250
23:58 - divide by 3.92 so if I want to see what
24:01 - will happen to
24:03 - 250 right what will be the number in
24:06 - this U inate so I can probably go ahead
24:08 - and divide it it'll be nothing but 250
24:11 - ID
24:13 - 3.92 so if I go and calculate it it is
24:15 - nothing but 63.7 7 so if I do the
24:20 - rounding that basically means this will
24:22 - be 64 so in short any number that is
24:26 - over here let's say it is 250 over here
24:28 - this will get converted to a quanti
24:31 - value to 64 right so the same thing the
24:34 - code will also be doing and this is for
24:37 - symmetric unsigned inate okay quation if
24:42 - I want a quation some for some other
24:44 - Factor let's let's talk about this okay
24:46 - so let's say um I have another kind of
24:49 - distribution and this is time that it is
24:53 - asymmetric and I want this as u in 8 so
24:58 - if I want to perform this contration so
25:00 - what will happen now in this particular
25:02 - case let's say if I have a values
25:04 - between -
25:06 - 20.0 to 1,000 okay so these are my
25:10 - floating point now I want to perform
25:12 - contag and convert this into 0 to
25:17 - 255 right now in case of asymmetric what
25:20 - will happen is that in my real number
25:22 - section right this numbers are not
25:24 - symmetrically distributed it may be
25:26 - right skewed it may be left skewed okay
25:30 - so in this scenario you'll be able to
25:32 - see that my values are ranging between -
25:33 - 20 to 1,000 I want to convert this into
25:36 - this now in this scenario if I apply the
25:39 - same formula x max minus x mean so how
25:42 - it will be 1,000 - 20 so minus of - 20
25:47 - is nothing plus + 20 divid by 255 so if
25:51 - I probably do the calculation then you
25:54 - will be able to see that I will be
25:55 - getting somewhere around
25:57 - 4.0 okay
25:59 - now very important thing that basically
26:01 - means this
26:04 - 20.0 if I quantize
26:06 - it right if I quantize it it will get
26:09 - converted to something like 4.0 oh sorry
26:13 - this this is the my scale factor okay
26:16 - scale factor now if I take any number
26:18 - and try to convert it let's consider
26:20 - minus 20 if I try to convert it by
26:23 - dividing by 4.0 right and if I do the
26:26 - round so what it will become - 5 right -
26:31 - 5 of round this much I will probably get
26:34 - - 5 right now you can understand that
26:37 - this - 20.0 is getting converted to -
26:40 - 5.0 but you can see over here my
26:42 - distribution starts from 0 to
26:44 - 255 so how can I forcefully make this -
26:49 - 20.0 to zero all you have to do is that
26:52 - go ahead and add the same number in a
26:54 - positive way so in this case the number
26:57 - that you see this 5 right this is
27:00 - basically called as 0 point
27:04 - right so there are two important
27:07 - parameters that we specifically talk
27:09 - with respect to quantization one is 0o
27:12 - point for for the above one since we
27:15 - have a symmetrical distribution here the
27:18 - 0 point was Zero only and the scale was
27:22 - 3.92 in this particular case since it
27:25 - asymmetrical distribution here we have a
27:27 - z 0 point as nothing but five but scale
27:32 - is 4.0 so this two parameters we usually
27:37 - require to perform
27:39 - quantization okay and these are some of
27:42 - the examples that I have shown you to
27:43 - just give you an idea like how
27:45 - quantization basically happens and super
27:48 - important in terms of understanding is
27:50 - the simple equations you'll be able to
27:54 - understand how things are basically
27:55 - working right at the end of the day
27:59 - understand quantization is a simple
28:01 - process of converting that high uh full
28:05 - single Precision of full Precision
28:06 - floating Point 32 bits into small bits
28:10 - you know it can be uh unsigned integer 8
28:13 - it can be signed integer eight if we say
28:15 - signed integer eight then what will
28:17 - happen it is that it will be ranging
28:19 - between - 128 to 127 and based on that
28:23 - you can specifically apply the formula
28:25 - right now let's go ahead see we had
28:28 - already discussed about these two topics
28:31 - one is this and second one we wanted to
28:35 - discuss about calibration now this
28:37 - squeezing that you could see right from
28:39 - here
28:40 - to here to here here to here we SK
28:44 - squeezing it right this squeezing
28:46 - process is basically called as
28:48 - calibration whatever process we are
28:50 - basically applying in this quantization
28:52 - process it is nothing but it is called
28:53 - as calibration because we are squeezing
28:55 - those values from a higher format to
28:58 - lower format okay so that is nothing but
29:01 - calibration so we have completed both
29:03 - this thing okay now let's see what are
29:06 - the different modes of quantisation one
29:08 - is called as post training quantisation
29:11 - and quantisation aware training right I
29:13 - will talk about this why it is super
29:15 - important both this technique okay so
29:17 - you'll get an idea about it over here so
29:19 - first of all we will go ahead and say
29:21 - post
29:23 - training quantization
29:28 - so what exactly is posttraining
29:31 - conation here we already have a
29:34 - pre-trained
29:37 - model so we already have a pre-trained
29:40 - model now if I want to use this
29:43 - pre-trained model obviously the weights
29:44 - are very high here we
29:49 - apply
29:52 - calibration right when I say calibration
29:55 - that basically means squeezing the value
29:57 - from high form format to a lower format
29:59 - right and then after performing this
30:01 - particular calibration we take what kind
30:04 - of data we take that weights data
30:07 - whatever weight data is basically there
30:09 - in this particular pin model and then we
30:13 - convert this into a quantized
30:18 - model okay so once we apply this process
30:22 - then only we'll be able to get the
30:23 - quantise model and then we can use this
30:26 - entire model for any use cases okay for
30:30 - any use cases right this is a simple
30:32 - mechanism with respect to post training
30:34 - quantization see understand post trining
30:36 - basically means I already have a
30:38 - pre-train model where my weights are
30:40 - fixed I don't need to change those
30:43 - weights I will just take or download
30:45 - those weights I will take this weights
30:47 - data apply the calibration and then
30:50 - convert this into a quantise model right
30:53 - the second technique that we have
30:55 - written over is quantisation aware
30:57 - training
30:59 - okay quantization aware training so
31:01 - let's talk about
31:04 - this
31:06 - quantization aware
31:09 - training this is also called as
31:13 - CU so this is also called as
31:16 - Q8 okay so we can write it as
31:20 - Q8 okay quantization aware technique
31:22 - this is basically called as ptq ptq
31:26 - Okay so over here what is the exact
31:29 - difference we'll try to see between this
31:31 - two okay now in quantisation aare
31:34 - training what happens see over here what
31:36 - is the problem if I probably perform
31:39 - calibration and if I create a quanti
31:41 - model there is a loss of data and
31:45 - because of this what will happen is that
31:47 - the accuracy will also decrease okay for
31:50 - any use
31:52 - cases but in the case of quantisation
31:55 - aware training okay you'll be able to
31:58 - see that we will be taking our train
32:00 - model whatever train model is
32:03 - there train model is there okay let me
32:07 - just go ahead and write it
32:10 - down train model is there then we
32:13 - perform
32:14 - quantisation again quation is what same
32:18 - the calibration process will apply over
32:20 - there we will probably do all these
32:22 - things okay and then once we do this the
32:26 - next step is that we will will go ahead
32:28 - and
32:30 - perform
32:33 - finetuning see we know
32:36 - that we know that with the help of ptq
32:40 - you'll be seeing over here on the top
32:42 - some loss of data and accuracy is there
32:44 - but here with respect to finetuning we
32:47 - will take new training
32:53 - data new training data now once we
32:57 - specifically take new training data we
32:59 - will be fine-tuning this model and then
33:02 - we create a quantied
33:04 - model quantied model so with respect to
33:08 - any fine-tuning technique that you will
33:10 - be seeing we don't use post trining
33:12 - quantisation we specifically use
33:14 - quantization aare training technique so
33:17 - that basically means we are just not
33:19 - losing accuracy or data over here
33:21 - because we are in turn adding more data
33:23 - for the training purpose and through
33:25 - this we will be fine-tuning our data and
33:27 - then we create our quantized models
33:30 - right so this is the basic difference so
33:32 - all the fine tuning technique that I
33:34 - will probably show you in the future
33:36 - will be of this type that is
33:37 - quantization aware training so that we
33:39 - do not lose much data accuracy so I hope
33:43 - you got an idea with respect to all
33:44 - these three techniques guys going ahead
33:46 - there are two important techniques that
33:48 - we really need to understand one is
33:50 - clora and one is
33:52 - Laura so this techniques specifically
33:55 - will be also understanding with respect
33:57 - to to fine tuning uh we are into the
33:59 - part two of fine tuning series and in
34:02 - this particular video we're going to
34:03 - discuss about Laura and CLA indepth
34:07 - intuition already in the playlist of
34:10 - fine tuning I've discussed about quation
34:12 - and I hope you have seen the video early
34:14 - many people were requesting for Laura
34:16 - and CLA also so let's uh discuss and uh
34:20 - we will try to discuss the complete
34:22 - in-depth maths
34:23 - intuition uh the best thing will be that
34:26 - guys I'll try to explain this math
34:28 - in-depth intuition uh I've already seen
34:31 - the research paper and there are lot of
34:33 - complicated things that is probably
34:35 - there in the research paper but I will
34:38 - try to teach you in such a way that at
34:40 - least you should be able to understand
34:43 - you know what exactly is Laura what
34:45 - exactly is Clara and then we'll also see
34:47 - one example you know how with the help
34:50 - of code you will be able to do it and
34:53 - trust me these are some of the very
34:54 - important things in fine tuning because
34:56 - tomorrow you go in any interviews you
34:59 - are going to get
35:00 - asked respect to this kind of questions
35:03 - that may be coming because at the end of
35:05 - the day any generative AI projects
35:08 - specifically with llm right llm models
35:11 - if you are working in the company they
35:13 - will be giving you fine-tuning task so
35:16 - let me quickly share my screen and show
35:18 - it to you so here already I have
35:20 - uploaded a video uh part one where we
35:23 - have discussed about
35:25 - quation uh a good 32 minutes video
35:28 - Everything you'll be able to understand
35:29 - it because here I've also used all the
35:31 - mathematical intuitions everything as
35:34 - possible as as much as possible uh to
35:37 - talk about the research paper so what
35:38 - does Laura means Laura basically means
35:40 - low rack adaptation of large language
35:43 - models uh this is this amazing research
35:46 - paper and uh probably you'll be seeing
35:49 - lot of this kind of equations as you go
35:51 - ahead there'll be different different
35:53 - performance metrics but as usual I will
35:56 - do what I'm good at I will try to break
35:58 - down all these things and probably
36:00 - explain you with respect to examples
36:03 - with respect to code and many more
36:05 - things so quickly let's go
36:08 - ahead so why Laura and CLA is basically
36:12 - used Laura basically means low low rank
36:15 - adaptation lower order rank adaptation
36:19 - it is specifically used in F tuning of
36:22 - llm models okay so let's go ahead and
36:25 - discuss this and I've created this
36:26 - amazing diagram over here here initially
36:30 - whenever whenever you have a pre-trained
36:33 - nlm model that basically means uh let's
36:35 - say that there's a model something like
36:37 - GPT 4 or gp4 Turbo right and this model
36:42 - has been created by open AI so we
36:45 - basically see this model as the base
36:47 - model OKAY gp4 turbo and this model is
36:52 - strained with huge amount of data right
36:55 - so the data sources will be in internet
36:58 - it can be books it can be multiple
37:00 - sources right at the end of the day all
37:04 - these models how you can measure them
37:07 - they may be saying that hey uh it
37:09 - supports 1.5 million tokens you know it
37:12 - has been trained with this many number
37:13 - of words right so many tokens of words
37:17 - now what will happen is that all these
37:18 - models you know probably to predict the
37:20 - next word it will have the context of
37:23 - all those tokens and then it will be
37:25 - able to give you the response so these
37:26 - all models are basically
37:28 - base model and we also say this as a
37:30 - pre-train model OKAY some of the
37:32 - examples again I'll be telling you GPT 4
37:35 - gp4 Turbo right gpt3 GPT 3.5 something
37:40 - so all these models are specifically
37:42 - pre-train models now further we can take
37:46 - this model and there are various ways of
37:47 - fine tuning it please make sure that you
37:49 - watch this video till the end if you
37:52 - watch this video you will understand
37:54 - everything that is actually required
37:56 - with respect to fine tuning
37:58 - and there are multiple ways of fine
38:00 - tuning also now let's say that I take
38:01 - this model I do some amount of fine
38:03 - tuning okay and this fine tuning is done
38:06 - on all the weights of the specific model
38:08 - right so some of the application that we
38:10 - may generate is like chat GPT you know
38:13 - we may generate like Cloud uh cloudy
38:15 - chat GPT right cloudy GPT itself like
38:18 - the chatbot that we specifically use
38:20 - some of the examples okay so this one
38:22 - way of fine tuning is there where we
38:24 - train all our parameters so here we
38:27 - specifically say
38:29 - full full full parameter training okay
38:32 - so here you can see full parameter fine
38:34 - tuning so here I'm going to write full
38:37 - parameter fine tuning now this is one
38:40 - way of fine tuning where we train our
38:42 - entire parameter based on our data that
38:45 - we have okay and after training it we
38:48 - can develop applications like chat GPT
38:50 - or any custom GPD that you specifically
38:53 - create as we go ahead you can also take
38:57 - these models and
39:00 - perform domain specific fine tuning okay
39:03 - so one type of fine tuning is this the
39:05 - other tune other fine tuning technique
39:07 - that you can specifically use is called
39:09 - as domain specific fine tuning some of
39:11 - the example let's say that I'm fine
39:13 - tuning a a a a chatbot model you know
39:17 - which will be for finance it can be for
39:19 - sales it can be for different different
39:22 - domains itself right so here the main
39:24 - important word is domain so this fine
39:27 - tune also we can perform okay again why
39:29 - I'm saying all these things because
39:30 - there are various ways of fine tuning
39:32 - things right one more fine tuning we can
39:36 - basically divide by is something called
39:38 - as specific task fine tuning in case of
39:41 - specific task fine tuning these are my
39:43 - different different task let's say this
39:45 - is Task a b c d this task can be
39:49 - something related to Q&A chb this task
39:52 - can be something related to document Q&A
39:54 - chat but different different
39:56 - applications so that is the reason why
39:58 - we are specifically saying over here as
40:00 - specific task okay specific task fine
40:03 - tuning now perfect now this is good you
40:06 - have seen all the different ways of fine
40:08 - tuning okay now let's talk about full
40:11 - parameter fine tuning again I'll repeat
40:13 - it this is my base model right now if
40:15 - you use it as an example as I said GPT 4
40:18 - Turbo GPT 3.5 you know Gemini Gemini 1.4
40:23 - different different models can be there
40:25 - we can take this base model we can find
40:27 - F tune and create applications like chat
40:29 - GPT we can create other other
40:30 - application like stable diffusions you
40:32 - know not specifically to llm but LM we
40:35 - can actually do it then we can also
40:37 - further fine-tune this based on domain
40:40 - specific fine tuning right based on
40:42 - different different domains like Finance
40:44 - sales retail we can also take this model
40:47 - and do more specific task finetuning
40:50 - like task a task B task D let's say I
40:53 - want to convert this into text to SQL I
40:55 - want to have this as document Q so I can
40:58 - further fine tune it based on specific
41:00 - task now let's talk about this full
41:03 - parameter fine tuning okay and what are
41:05 - the challenges with full parameter fine
41:07 - tuning and that is where see I'm
41:08 - building up the story later on I'll be
41:11 - explaining you where Laura will be used
41:14 - now in full parameter fine tuning the
41:16 - major challenge is that we really need
41:17 - to update all the model weights let's
41:20 - say that I have I have a model which has
41:23 - somewhere around 175 billion parameters
41:27 - that basically means 175 billion weights
41:30 - now in this particular case whenever I
41:32 - finetune this model I need to update all
41:35 - the weights great now when I'm saying
41:38 - updating all the model weights and when
41:41 - we talk about this many number of
41:43 - parameters why can be a challenge
41:45 - because there will be Hardware resource
41:48 - constraint right so with respect to
41:51 - different different task if I really
41:53 - want to use this particular model that
41:55 - much RAM I really require for
41:57 - inferencing purpose that much GPU I
42:00 - really require right so for Downstream
42:04 - task it becomes very difficult now what
42:07 - is Downstream task Downstream task some
42:10 - of the example is like model
42:12 - monitoring right model
42:15 - monitoring the other task can be like uh
42:19 - model
42:20 - inferencing right model inferencing
42:25 - similarly the GPU constru that we may
42:28 - have the ram constraint that we may have
42:30 - so we may face multiple challenges when
42:33 - we have this full parameter
42:35 - tuning full parameter fine tuning now in
42:38 - order to overcome this challenge we will
42:41 - specifically use Laur and CLA okay what
42:46 - exactly is Laura as I said low order
42:49 - rank adaptation and CLA is something it
42:52 - is also called as Laura 2.0 so we'll
42:54 - discuss about both of them with respect
42:57 - to mathematical intuition and you'll get
42:58 - a complete idea what I'm actually trying
43:00 - to say now what does Laura do okay now
43:04 - let's read the first point and this is
43:06 - very much important because in the
43:08 - research paper you will find this
43:10 - equation okay this equation now what
43:12 - exactly Laura will do Laura says that
43:15 - instead of updating all the weights in
43:18 - full parameter fine tuning right instead
43:21 - of updating all the weights it will not
43:24 - update them instead it will track the
43:26 - chain
43:27 - changes it will track the changes now
43:31 - what changes this is basically tracking
43:34 - it will track the changes of the new
43:36 - weights based on finetuning okay this is
43:38 - very much important to understand so uh
43:42 - based on the new weights how we are
43:44 - going to combine this weights with the
43:46 - pre-trained weights okay so here you can
43:48 - see these are my pre-trained weights
43:50 - from the base model like let's say that
43:53 - this model is llama 2 now if you're
43:55 - performing fine tuning using laa then
43:58 - Laura will track the new weights over
44:02 - here which will be of the same size okay
44:04 - so let's say if this uh weight is 3
44:07 - cross 3 then the new weights when it is
44:10 - probably doing the forward and the
44:11 - backward propagation those new weights
44:14 - will be tracked in a separate Matrix and
44:16 - then these two weights will be combined
44:19 - wherein we will get the fine
44:21 - weights now this way what will happen is
44:24 - that this tracking will happen in a
44:25 - separate way but still you may be
44:27 - thinking Kish here also we are updating
44:30 - all the weights itself right so here
44:34 - also the resource constraint will
44:35 - definitely happen yeah fine I'm talking
44:37 - about 3 cross 3 but what about uh
44:39 - weights and parameters where there are
44:41 - 175 billion right 175 billion parameters
44:44 - or 7 billion parameters that time I'll
44:46 - be having a huge metrix right so at that
44:50 - scenario you need to Now understand how
44:52 - Laura works because this
44:55 - weights how is getting tracked it will
44:58 - just not get tracked in this 3 cross3
45:00 - Matrix instead all these weights that is
45:03 - getting
45:04 - tracked there a simple mathematical
45:07 - equation will happen or I'll not say
45:10 - mathematical equation will happen but
45:12 - there will be a technique that will
45:14 - happen which is called as Matrix
45:16 - decomposition that basically means the
45:18 - same 3 cross3 Matrix is saved it in a
45:22 - two smaller Matrix now in this two
45:25 - smaller Matrix you can see this is is
45:27 - nothing but 1 cross 3 and this is
45:28 - nothing but 3 CR 1 right sorry this is 3
45:32 - CR 1 and this is 1 CR 3 right so this is
45:36 - 3 cross 1 and this is 1 cross 3 when we
45:39 - multiply both these weights then I will
45:41 - be getting this weight only right so
45:43 - over here if I consider I have some
45:46 - around nine weights 4 5 6 7 8 9 right
45:52 - you'll be able to see that I will be
45:54 - able to get all these nine weights from
45:57 - how many number of parameters just six
46:01 - parameters right because when we
46:04 - multiply this then you'll be able to see
46:06 - that I'll get all these nine parameters
46:08 - or nine weights that I have right so in
46:11 - short what Laura is doing is that it is
46:13 - performing this Matrix decomposition
46:15 - where a big Matrix and this Matrix can
46:17 - be of any size is decomposed into two
46:21 - smaller metrics based on a parameter
46:24 - which is called as rank
46:27 - how to calculate a rank of a metrix you
46:28 - can definitely check out any YouTube
46:30 - channel it is a simple algebraic
46:33 - equations based on transpose of a matrix
46:36 - how we calculate the rank but let's say
46:38 - that this Matrix that I have which is a
46:40 - 3 cross one over here the rank of this
46:43 - particular Matrix is one right and if I
46:48 - use this two Matrix you can obviously
46:50 - see that the number of parameters that
46:51 - I'm storing over here is less when
46:53 - compared to this right yes there will be
46:56 - a loss in Precision but it is making
46:58 - sure that when we combine both this
47:00 - metrics we'll be able to get the entire
47:03 - updated weights and just imagine start
47:05 - thinking guys let's say that if I have 7
47:08 - billion parameters now I'm trying to
47:10 - perform finetuning on those parameters
47:13 - so whenever I track those weights this
47:15 - huge Matrix will be decomposed into two
47:18 - smaller Matrix and when we are
47:22 - decomposing this this Matrix this
47:24 - updated track weights Matrix into two
47:26 - smaller metrix obviously will be
47:28 - requiring less parameter to store all
47:30 - these values right and this way your
47:34 - fine tuning becomes very much efficient
47:37 - and this really solves the resource
47:39 - constraint right this is the most
47:42 - important thing right and so in any
47:45 - research paper that you go ahead you'll
47:47 - be seeing this equation w0 this is my
47:51 - pre-train weights plus the track changed
47:54 - weights is nothing but my pre train
47:57 - weights plus b multiplied a what is b b
48:01 - is this a is this so when we multiply
48:04 - this you'll be able to see that we are
48:06 - able to get the all the track change
48:08 - weights right and obviously this
48:11 - requires less parameter if you
48:12 - decomposing our bigger Matrix into two
48:15 - smaller Matrix less parameters is
48:17 - required now what will happen if we keep
48:19 - on increasing the ranks if we keep on
48:22 - increasing the ranks this parameters
48:24 - will also keep on increasing but but it
48:27 - will always be less than this right if I
48:31 - have 7 billion parameters if I try to
48:33 - decompose that into two two matrices two
48:36 - small R matrices with increasing rank
48:39 - then also the parameters that will be
48:41 - required will be less how I am seeing
48:43 - this because in the research paper also
48:45 - they have tried with multiple trainable
48:47 - parameters now let's see over here there
48:49 - are multiple techniques of fine tuning
48:51 - some of the techniques that are there is
48:53 - something called as prefix embed prefix
48:55 - layer adap adapter is one very famous
48:58 - thing that is probably used before Laura
49:01 - right you can see as the rank is
49:02 - increasing the parameters also increases
49:06 - right initially the trainable parameters
49:08 - are 175 billion but when I use
49:11 - techniques like adapter right so
49:13 - initially it will have 7.5 7.1 million
49:16 - parameters with rank is equal to 1 but
49:18 - as I keep on increasing the rank as I
49:20 - keep on increasing the rank you'll be
49:22 - able to see that this parameters also
49:24 - get increased but you can see from 175
49:26 - billion parameter if I compare 7.1
49:28 - million weights the percentage is very
49:30 - less now similarly in Laura because of
49:34 - that uh metrix decomposition you'll be
49:36 - able to see that as I keep on increasing
49:39 - my ranks so these are my ranks with
49:41 - respect to Q KV because in transfer you
49:44 - have this three parameter Q KV there
49:46 - only all the metrix multiplication will
49:48 - happen with respect to these three
49:49 - parameters and then we as we keep on
49:51 - increasing the rank here you can see
49:53 - four here you can see 8 here you can see
49:56 - 64
49:57 - then you'll be able to see initially we
49:58 - got 4.7 million parameters compare from
50:02 - 175 billion to 4.7 million how this was
50:05 - possible because of the because of the
50:08 - metric
50:10 - decomposition because of the metrix
50:14 - decomposition right and as we keep on
50:17 - increasing the rank you'll be seeing
50:18 - that the parameters are increasing right
50:21 - the parameters are obviously increasing
50:22 - but if I compare it with 175 billion
50:25 - parameter this is very very less 9.4
50:27 - million if you just see the percentage
50:30 - right so here also when rank is equal to
50:33 - 8 37.7 million then rank is equal to
50:35 - 6430 1.9 million right parameters are
50:38 - there so still it is making sure that
50:41 - the parameter is not that much like like
50:43 - not not like 175 billion or not near to
50:46 - this if I talk with respect to
50:48 - percentage it is very very less I've
50:50 - also made another table right just to
50:52 - show you if I have different different
50:54 - models number of
50:58 - trainable
51:02 - parameter number of trainable parameter
51:05 - here you can see I have one llm model
51:07 - with 7 billion if I use rank is equal to
51:09 - 1 then I will be having 167k parameters
51:12 - to fine tune fine tune weights based on
51:14 - fine tune weights then and this 17 67k
51:18 - parameter basically means what this
51:20 - decomposed Matrix that I have right two
51:22 - matric that many number of parameters so
51:24 - in the first case it is just nothing but
51:26 - 7 167k parameters that is the available
51:29 - in this decomposed metrix okay when we
51:32 - combine them then we will be able to get
51:35 - how many 7 billion parameters if we
51:37 - combine both this metrics then we'll be
51:39 - getting uh 7 billion parameters okay
51:42 - then similarly you can see in 13 billion
51:44 - then you have 2 to 8K parameters in 70
51:46 - billion you have 5 to 9k parameters in
51:49 - 180 billion you have 849k parameters so
51:52 - as you keep on increasing the weight
51:54 - this parameters will keep on increasing
51:55 - but it is not increasing with that huge
51:57 - amount right even you can see when we
51:59 - keep the rank as 512 right so here you
52:02 - can see 86 million parameters is there
52:04 - when compared to billions right uh
52:07 - Microsoft uh you know it came up with
52:09 - this Laura technique uh in one of the
52:11 - research paper and it has used rank is
52:13 - equal to 8 okay to probably do the
52:15 - finding and it has performed absolutely
52:17 - well so most of the time we select this
52:20 - particular value but at the end of the
52:23 - day how to select this right it want
52:26 - matter you know because the parameters
52:28 - are increasing by very less number over
52:30 - here as we go ahead so usually you can
52:32 - select rank 1 2 8 while you're
52:34 - performing fine tuning now there may be
52:37 - also scenario that when should we use
52:39 - very high rank when to use high
52:42 - rank when to use high rank this answer
52:48 - because in the interview they may ask
52:49 - you if the model wants to learn complex
52:53 - things
52:56 - complex things then you can specifically
52:58 - use high rank right let's say some of
53:01 - the model is not trained to probably uh
53:04 - interact or probably uh perform some of
53:07 - the behavior at that point of time those
53:08 - complex things can be handled when you
53:10 - are probably increasing the number of
53:12 - ranks okay so this can be a very simple
53:14 - question that may be asked in the
53:16 - interview but I hope you got a complete
53:18 - idea at the end of the day this is the
53:20 - equation that you'll be able to see in
53:22 - most of the research paper uh what Laura
53:25 - is doing is that nothing very is simple
53:27 - all the track weights is decomposed into
53:28 - two smaller matrixes with different
53:30 - different ranks it can be different
53:32 - different ranks when you're fine tuning
53:34 - the first thing is that you really need
53:35 - to set that rank okay if you set that
53:38 - rank like in this particular case if I
53:40 - probably see if I go ahead and calculate
53:41 - with all the mathematical stuff I will
53:43 - be able to get the rank is equal to one
53:45 - okay uh for this also rank is equal to
53:47 - one right so similarly if you have rank
53:49 - two so one of the metrix can be
53:51 - something like this so decomposed Matrix
53:54 - so this is based on rank two okay so if
53:57 - I probably combine this right so how
53:59 - many 1 2 3 4 5 6 7 8 9 10 11 12 right if
54:03 - I multiply this I'll be getting a metrix
54:06 - uh of much more parameters right but at
54:08 - the end of the day for in this
54:09 - particular case it is less number of
54:10 - parameters right so this is what Laura
54:13 - is all about and because of this
54:15 - technique the fine tuning is done less
54:17 - the the weights the parameters becomes
54:19 - less so this is how the main resource
54:21 - constraint is done and uh with respect
54:23 - to all the downstream task it becomes
54:25 - very much easy
54:27 - now one more thing that I really want to
54:28 - talk about is chora okay
54:32 - chora chora basically means
54:35 - quantized quantized Laura okay now you
54:40 - have already learned from the first
54:41 - video what is quantied quantied
54:43 - basically means now in CLA case what
54:46 - will happen is that all these parameters
54:48 - that is probably stored in float float
54:51 - 16bit okay we will try to convert this
54:54 - into 4bit that's it
54:56 - okay once we do this you'll be able to
54:59 - see that we reduce the Precision and
55:02 - then we try to reduce this values also
55:05 - by this you won't require much more
55:06 - memory so that is the reason we say
55:08 - quantized Laura technique okay and the
55:11 - best thing about is this is that U clora
55:14 - also has one amazing algorithm which
55:17 - will be able to take care of both this
55:19 - part let's say if there is a float 16bit
55:21 - I quantize it to 4bit I can also convert
55:23 - this back into 16bit okay so with
55:27 - respect to this explanation guys I've
55:29 - already spoken about many things over
55:30 - here Laura and CLA uh just to show you
55:33 - one example so here is one example uh
55:36 - I've already shown you this finetuning
55:38 - using Google Gamma model let me talk
55:40 - about quation quation over here is
55:42 - basically done by bits and bytes config
55:44 - so here it says load in 4bit true that
55:47 - basically means we going to convert that
55:49 - entire model of 16 bit to 4bit
55:51 - quantisation technique that we are going
55:52 - to use is something called as nh4 and uh
55:55 - all the further fine tuning is basically
55:57 - done in V float 16 okay um now there is
56:02 - also one more thing right see La
56:04 - configuration here we have selecting the
56:06 - first rank value eight and then Target
56:08 - modules where we need to apply this
56:10 - particular decomposition and the task
56:12 - type casual LM once you do this and just
56:14 - execute everything you'll be able to see
56:16 - it right and that is how the entire
56:18 - quation and the Laura happens guys in
56:21 - this particular video we are going to
56:23 - see the stepbystep way of probably fine
56:27 - tuning your llm models in this case I'm
56:30 - going to specifically take open source
56:32 - Lama 2 model and with the help of a
56:35 - custom data set we are going to
56:37 - fine-tune this specific model right over
56:40 - here we are going to learn about various
56:42 - techniques practically not theoretically
56:45 - because if you really want theoretically
56:46 - you can let me know in the comment
56:48 - section so we will be discussing about
56:50 - something called as parameter efficient
56:53 - transfer learning for NLP which is an
56:56 - amazing technique to basically fine-tune
56:59 - all these llm models which will
57:01 - definitely be of use size like 70
57:03 - billion parameters and all so how this
57:06 - parameter efficient transfer learning
57:08 - actually happens we'll try to see in the
57:09 - code and we are also going to see a
57:11 - technique which is called as Laura right
57:14 - so Laura paper if I go ahead and search
57:17 - right it is basically called as low rank
57:20 - adapt adaptation of large language
57:23 - models right so these are some of the
57:25 - mathematical concept don't worry in the
57:27 - upcoming videos I will talk about all
57:29 - every theoretical intuition about PFT
57:32 - about Laura right now a simple way of
57:36 - fine-tuning I'm just going to show you
57:38 - because many people were requesting for
57:39 - this right so initially what we will do
57:42 - is that we will go ahead and install
57:44 - some of the important libraries like
57:46 - accelerate PFT as I said PFT is nothing
57:49 - but parameter efficient transfer
57:51 - learning inside this only you'll find
57:53 - this Laura technique which is called as
57:55 - low Bank adaptation of large language
57:58 - models uh then we have bits and bytes
58:00 - bits and bytes are specifically used for
58:02 - doing quantization now what does
58:04 - quantisation basically mean all these
58:06 - llm models you know when they are
58:07 - trained with 70 billion parameters or 13
58:10 - billion parameters by default the
58:12 - weights data types are in the form of
58:14 - floating values right when we say
58:16 - floating values that they are basically
58:18 - 32 bit values what we can actually do
58:21 - and obviously since I'm actually going
58:22 - to do this in Google collab we get a
58:25 - very less Ram so it is a better way that
58:27 - you quantize those weights you know from
58:30 - float 32 probably convert that into int
58:32 - 8 and then probably based on the Ram
58:36 - size you'll be able to quickly fine tune
58:38 - it along with that I will be also we'll
58:40 - also be using Transformers and then you
58:42 - have TRL so all this libraries will go
58:45 - ahead and execute it and once we
58:48 - specifically execute it you'll be able
58:50 - to see that all these libraries will get
58:52 - installed now in the Second Step the
58:56 - major thing is that we will specifically
58:58 - be using the library called as
59:00 - Transformers which is specifically used
59:02 - for this particular purpose and
59:04 - internally we'll also be using PFT which
59:07 - is having some Laura configuration and
59:09 - we'll use this PFT model I know you'll
59:11 - not be able to understand what exactly
59:13 - PFT is but I'll just tell you in some
59:15 - time just let me go ahead with but at
59:18 - the end of the day PFT actually uh you
59:21 - know uses techniques which will try to
59:23 - freeze you know when it applies transfer
59:25 - learning learning on these llm models it
59:27 - is freezing most of the weights of that
59:29 - llm model and only some of the weights
59:32 - will be retrained and based on that they
59:35 - will be able to provide you accurate
59:37 - results based on your custom data set
59:39 - okay uh how it is done don't worry I'll
59:42 - create a amazing dedicated video to make
59:45 - you understand this mathematical
59:46 - intuitions okay now over here you'll be
59:49 - able to see that I'm going to import OS
59:51 - import torch I'm going to use a data set
59:53 - I will talk about what data set we are
59:55 - going to specify spefically do the
59:57 - finetuning but here we are specifically
59:58 - using open source llm models and then
60:01 - from Transformer I'm going to use Auto
60:03 - model for casual LM a tokenizer bits and
60:05 - bytes I will talk about all these
60:07 - libraries as we go ahead so let me
60:09 - quickly go ahead and execute it okay now
60:12 - till this is getting executed this
60:14 - import statement is getting executed
60:16 - let's talk about some of the important
60:18 - properties over here with respect to
60:19 - Lama 2 in the case of Lama 2 the
60:21 - following prom template is used for chat
60:23 - model so this is the specific prompt
60:26 - template uh here we be give an
60:30 - instruction in this s symbol and then we
60:33 - have our system prompt which will be
60:34 - closed with the CIS brackets and then
60:37 - you will also be able to give your user
60:40 - prompt over here and the model answer
60:42 - will be coming after this after this
60:43 - entire instruction okay so this is how
60:46 - the entire Lama 2 models llm models
60:50 - specifically require the system prompt
60:52 - and the user prompt and the model answer
60:54 - format right now
60:57 - any data set that you specifically get
60:59 - right we really need to convert that
61:01 - data set into this format okay and that
61:03 - is how I will show you how to probably
61:05 - do this there's a technique uh you can
61:08 - also write your own custom code and all
61:09 - there are many ways okay now what we'll
61:12 - do we will reformat our instruction data
61:14 - set to follow Lama 2 template so right
61:16 - now we are going to use this data set
61:19 - which is basically called as open open
61:23 - Assistant Guan guano I hope I'm
61:25 - pronouncing it right now here you will
61:28 - be able to see this is my data set right
61:30 - human can you write a short introduction
61:31 - about the relevance of term uh monopsony
61:34 - in economics please use example related
61:36 - to this and then Mon Mon monopsony
61:39 - refers to the market so here you can see
61:41 - assistant answer so here the data set is
61:44 - basically in the form of human and
61:46 - assistant like human has a question over
61:48 - there and assistant is probably
61:50 - providing uh you a specific answer so in
61:53 - this format you'll be able to find out
61:55 - each and and every rows each and every
61:57 - rows in different different languages so
62:00 - we are going to take this entire data
62:02 - set and then considering this entire
62:05 - data set what we are going to do we are
62:07 - going to reform the data set following
62:08 - the Lama 2 template and out of all this
62:11 - samples all this data set there are
62:13 - around uh how many data sets are there I
62:16 - guess there are around 10 10K records we
62:18 - just going to take, uh th000 Records or
62:21 - 1K records the reason is that I really
62:23 - need to show you how the fineing is
62:25 - basically done so if I go ahead and
62:27 - click on this and if you see this format
62:30 - right this format you'll be able to see
62:33 - that this entire data set is converted
62:35 - in this format only right instruction is
62:37 - basically there the answer is over here
62:40 - and this entire s is getting closed
62:42 - right so all the data set is basically
62:44 - converted into that specific format now
62:46 - how do you convert it right so for that
62:49 - already what we have basically done is
62:51 - that over here to know how this data set
62:53 - was created you can check this notebook
62:55 - so this not notebook is there already
62:57 - you can see that we are loading the data
62:59 - set we are applying this we are taking
63:00 - the Thousand records and then we are
63:02 - transforming right so in transforming
63:04 - basically a simple python code like I
63:06 - have to probably keep in that specific
63:08 - format right so that is the reason I'm
63:10 - showing you this specific code over here
63:12 - just by one click you will be able to do
63:14 - that okay so all the links are actually
63:16 - given now you need to follow Now
63:19 - understand guys see understanding how
63:22 - the specific techniques are definitely
63:24 - I'll create a dedicated theoretic video
63:26 - understanding all the math equations
63:28 - that is required right over here we are
63:30 - trying to see that how you can also run
63:32 - your own fine tun model right so note
63:34 - you don't need to follow a specific prom
63:36 - template if you're using the base llama
63:37 - 2 model but right now we'll not use
63:39 - we'll use we'll not use this base Lama 2
63:41 - model okay how to find tune Lama 2 so
63:43 - these are some of the steps not only
63:45 - with LMA 2 with other models also this
63:47 - will work but again there the format may
63:50 - change you know the the format of the
63:52 - instruction the format of your prompts
63:54 - may change so free go gole collabs
63:56 - offers a 15gb graphic card right so
63:59 - limited resources barely enough to store
64:01 - Lama to 7 billion weights now here we
64:03 - are going to use 7 billion weights but
64:04 - it is also very difficult to store 15 GB
64:07 - right whatever free model that we
64:08 - specifically have we also need to
64:10 - consider the over head due to Optimizer
64:13 - State gradient and forward activation
64:15 - okay so usually in in any llm models
64:18 - you'll be having gradients you'll be
64:20 - having forward activations you'll be
64:21 - having optimizers so there also you
64:23 - require some amount of memory find
64:25 - tuning is not possible here right
64:27 - obviously this will not be possible
64:29 - because 7 billion weights you cannot
64:31 - store it in 15 GB that is the reason we
64:35 - require this parameter efficient
64:37 - fine-tuning technique now what does PFT
64:39 - basically do it is going to freeze most
64:42 - of the weights that is present in that
64:44 - llm model like Lama 2 and only with some
64:47 - of the weights after applying quation it
64:50 - is going to probably perform the fight
64:52 - fine tuning now parameter efficient fine
64:54 - tuning I will in my next video I will
64:56 - talk about this research paper if you
64:58 - quickly want this video please make sure
64:59 - that you make the video likes 2,000 okay
65:02 - now what we are going to do over here we
65:03 - are going to use techniques like Laura
65:05 - and clora as I said Laura or clora Laura
65:07 - is nothing but low rank adaptation of r
65:10 - large language model again I'm apologies
65:12 - guys if you don't know the mathematical
65:13 - Concepts I will explain in the upcoming
65:15 - video okay so first of all we will load
65:18 - a Lama 27 B J GPT model this chart HF
65:21 - model then train it on this 1K sample
65:24 - which will produce a finetune model with
65:27 - which in the name of chat fine tune
65:29 - we'll try to create in this clora will
65:32 - use a rank of 64 with a scaling
65:34 - parameter of 16 we will load the Lama 2
65:36 - model directly in 4bit Precision we are
65:38 - trying to convert that 32bit into 4bit
65:41 - so that is how we are going to do the
65:42 - training and with respect to Chlor in
65:45 - order to find the low rank index we are
65:47 - going to use the rank of 64 right this
65:51 - is an hyper tuning parameter you can
65:53 - just consider right now this is a kind
65:55 - of hyper tuning parameter with a scaling
65:58 - parameter Alpha this is also called as
65:59 - Alpha it will be having a scaling
66:01 - parameter of 16 as I said everything
66:04 - will be explained detailly when I
66:06 - probably go with the mathematical
66:08 - equation but right now our main aim is
66:10 - to probably learn how to find tuning it
66:13 - now what model we are going to use we
66:14 - are going to use Lama 2 7bh uh 7B chart
66:18 - HF then the instruction data set to use
66:21 - is this particular data set we will be
66:22 - downloading it from the hugging face the
66:24 - model name also will be downloading it
66:26 - and after fine-tuning it this will be my
66:28 - new model name okay now these are some
66:31 - of the clor parameters that is required
66:34 - okay so one is laurore R 64 what is this
66:37 - R this R is a rank of 64 a kind of
66:40 - hyperparameter Laura Alpha as I said
66:43 - Alpha right I told you Alpha why because
66:45 - I know the entire mathematics stuffs in
66:48 - this okay just to increase the Curiosity
66:51 - I'm coming up with this first video and
66:53 - later on I will come up with that then
66:55 - here also Dropout is basically required
66:57 - now in order to do the quantization we
67:00 - will be using bits and bytes parameter
67:01 - so here you can see activate 4 bit
67:04 - precision based model so there is a
67:05 - parameter which is called as _ 4bit
67:08 - which is equal to true then compute data
67:11 - type for 4bit base model so here it is
67:13 - basically float 16 then quantisation we
67:17 - using fp4 on np4 so B andb 4bit Quant
67:20 - type you have to keep this particular
67:21 - value to np4 since it is 4bit activate
67:24 - Ned quation for 4bit based model so here
67:27 - we are keeping it as false Now
67:29 - understand Guys these are some of the
67:30 - basic parameters that we specifically
67:32 - use in Laura technique specifically in
67:34 - PFT then training argument parameters
67:37 - our output directory will be present in
67:39 - this results I'm going to run one Epoch
67:42 - then we are going to enable this fp6 and
67:44 - B bf6 training okay uh it is set to True
67:49 - with an A1 100 right so a100 uh you can
67:52 - set it if you're using a100 you can set
67:54 - it to true right now I'm using T4 if you
67:57 - have the paid version of Google collab
67:58 - then you can set it to
67:59 - True bass size for uh Pur GPU for
68:03 - training I hope you know what is bass
68:04 - size then you have GPU for evaluation
68:06 - bass size then gradient accumulation
68:09 - step checkpoints Max gr uh Max grad Norm
68:12 - learning rate weight DK right Optimizer
68:15 - page adamw we will be using which is of
68:17 - a variety of Adam itself then learning
68:20 - sh learn uh LR schedular type cosign
68:23 - because it works on similarity right
68:25 - whatever question and answers we
68:26 - specifically write then maximum steps is
68:28 - minus one number of training steps
68:30 - override number of training epochs and
68:33 - after this you are also putting logging
68:35 - steps is equal to 25 now with respect to
68:37 - any fine tuning technique you use
68:39 - something called as supervised tuning
68:41 - right in supervised tuning that is you
68:43 - require some parameters right Max
68:45 - sequence length then packing then device
68:47 - map so this is load the entire model on
68:49 - the gpu0 right so this is what are the
68:51 - some of the parameters don't worry uh
68:54 - these are some of the parameters that
68:55 - you don't need to learn each and every
68:57 - parameter because already all these
68:59 - things are provided by the official page
69:01 - itself I've just copied and pasted it
69:03 - over here right so we will go ahead and
69:05 - execute it so let's go ahead and execute
69:07 - it so all these parameters are set now
69:10 - the step four right there are multiple
69:12 - four steps right uh one more step is
69:14 - there later on load everything and start
69:17 - the fine-tuning process right first of
69:19 - all we want to load the data set we
69:21 - defined here our data set is already
69:23 - pre-processed but usually this is where
69:24 - you should ref format The Prompt right
69:26 - filter out bad text combine multiple
69:28 - data some amount of pre-processing is
69:30 - required but already we have done that
69:31 - so we are not going to do it then we are
69:34 - Recon we are configuring bits and bytes
69:36 - for 4 bit quantization as I said right
69:38 - from 16 from 32 or 16 bit we are
69:40 - converting that into 4 bit so that it
69:42 - requires less space with respect to GPU
69:44 - for the fineing purpose next we are
69:46 - loading the Lama 2 model in 4bit
69:48 - Precision GPU with the corrent
69:50 - corresponding tokenizer right with that
69:52 - tokenizer we'll try to load that and
69:54 - obviously we'll also be loading it with
69:56 - the 4bit Precision finally we are
69:58 - loading the configuration of clora so uh
70:01 - and passing everything to the sft
70:03 - trainer so here is what self fine tuning
70:05 - uh s uh uh this sft will basically
70:08 - happen right now let's go ahead and
70:10 - let's do this so first of all we are
70:12 - loading the data set we are loading the
70:14 - tokenizer model with clor configuration
70:17 - so here I've return this B&B compute D
70:19 - type and we are using torch so along
70:22 - with that you also require bits and
70:23 - bytes config again load we enabling this
70:25 - 4 bit then all the necessary comp
70:28 - parameters like compute D type will be
70:30 - using honet Nest Quant okay again I'm
70:33 - telling you guys there is nothing new to
70:35 - learn in this because all these formats
70:37 - will be available in the official
70:38 - documentation then we are going to check
70:40 - the GPU compatibility with float 16 if
70:43 - compute dipe is equal to torch. float 16
70:45 - use 4bit otherwise these all things are
70:48 - there right then we are going to load
70:50 - the base model see whenever we want to
70:53 - load the base model from hugging face we
70:54 - can use this Auto model for casual LM
70:56 - right that is the reason we have
70:58 - imported on top Dot from pre-trained
71:01 - model name what is my model name I've
71:02 - given that quantisation config so here
71:05 - you'll be able to see in quantisation
71:06 - config we are also given something
71:08 - called as BNB config right so here
71:11 - you'll be able to see this is the
71:12 - compute
71:14 - type let me just search for it somewhere
71:16 - here only it will be available
71:22 - so so BNB config
71:26 - so here you can see this entire bytes
71:28 - config is basically there so uh based on
71:31 - that you'll be okay yeah computer okay
71:34 - yeah perfect so BNB config is basically
71:35 - given over here then device map is
71:38 - nothing but with respect to the GPU we
71:39 - are mapping then model. config use
71:42 - caches false you can also make it true
71:44 - if you want model. config pre-training _
71:46 - TP is equal to 1 then we are loading the
71:49 - Lama tokenizer see for any LM model we
71:51 - also need a tokenizer so that it will be
71:54 - able to convert any any llm model the
71:56 - input data that we are specifically
71:58 - using into word embeddings and all so
72:00 - that is the reason order organizer from
72:02 - pre-trained again model name we are
72:03 - going to use this trust remote code is
72:05 - one additional parameter that is used
72:07 - then we going to put a pad token with
72:10 - respect to the end of statement token
72:12 - right so do this eore token specifically
72:15 - applies the token for the Llama itself
72:18 - right and here we are giving the padding
72:20 - side as right fixed weird overflow issue
72:22 - with fp16 training all these parameters
72:25 - will be almost fixed guys only thing
72:26 - that you will probably be changing is
72:28 - with respect to the configuration then
72:30 - load Laura configuration here you'll be
72:32 - able to see PFT config Lura config all
72:35 - the values that you're putting with
72:37 - respect to this Lowa configs and yes
72:39 - here you have your PFT
72:41 - configuration now this is the most
72:43 - important thing because in this training
72:45 - arguments we set all the parameters
72:47 - output directory number of epo this this
72:49 - this learning rate PP p uh FP 16 BS 16
72:55 - you can probably see over here and then
72:57 - finally we are reporting it to the tal
72:58 - flow right tensor board then you can
73:01 - also see that supervised fine tuning
73:03 - parameters right I'm giving my model
73:05 - name I'm giving my data set my PFT
73:07 - config my data set text field this PFT
73:10 - config has a Lowa config right then you
73:12 - have a tokenizer you have the arguments
73:14 - you you have packing then you have
73:16 - finally trainer. TR okay now this is
73:19 - what is the main thing and that is where
73:22 - your supervised fine tuning will happen
73:24 - step by step we have done it okay let me
73:26 - repeat it quickly we have loaded the
73:27 - data set we have set our D type right we
73:31 - are setting up all our conation process
73:34 - over here here we are checking whether
73:36 - GPU is compatible or not here we are
73:38 - loading our llm model that is Lama 2
73:41 - here we are specifically loading our
73:43 - tokenizer which will be used in Lama 2
73:45 - along with this we are putting padding
73:47 - techniques then my Laura configuration
73:49 - which will specifically be in terms of
73:51 - PETA PFT config and then all my training
73:54 - arguments will go inside this right um
73:58 - the this training arguments is with
73:59 - respect to where my output directory is
74:01 - and all learning rate and all okay
74:04 - finally set supervised tuning parameters
74:07 - here we have seted model data set PFT
74:10 - config text Max equal length tokenizer
74:12 - everything is put up over here and
74:14 - finally we go ahead and train this now
74:17 - once we train it it is going to run for
74:19 - 250 epox uh I think 250 step size I have
74:22 - actually given over here sorry 20 five
74:25 - steps uh logging steps let's see what is
74:28 - the bass size bass size is
74:31 - four um yeah till that much it will
74:34 - probably go so let this start so it has
74:36 - already started I guess so here you can
74:39 - see it is downloading here you'll also
74:41 - be able to see the data set
74:44 - okay sample data right now you cannot
74:47 - see it because the data set will get
74:48 - loaded okay so table of contents
74:52 - installed all the required packages
74:54 - we'll reformat all the steps are given
74:56 - side by side you can also read it out I
74:58 - know this looks like a little bit tough
75:00 - guys but at the end of the day uh I'll
75:02 - not say that it is easy and just the
75:05 - reason why I'm sharing you this
75:06 - fine-tuning technique because you should
75:08 - just get in your mind later on you know
75:11 - this is the pattern that I'm following
75:13 - first execute this don't worry about
75:16 - anything as such just try to get an high
75:18 - level overview how things work later on
75:21 - I will try to break down each and
75:23 - everything in my next video by breaking
75:25 - this entire code why this specific
75:27 - parameters used because the main thing
75:30 - is to understand what is PFT what is
75:32 - quantisation what is precision and uh
75:35 - how how do you specifically use this PFT
75:38 - technique what is qora everything what
75:41 - is low order rank index uh how to
75:44 - basically calculate that everything I
75:46 - will talk about it okay so we'll wait
75:48 - for some time till then uh just let let
75:52 - us wait and uh we will I'll just just uh
75:55 - come again I I think it'll take 15 to 20
75:58 - minutes to complete this entire fine
75:59 - tuning with thousand records and then
76:01 - again I'll come back and we'll start
76:03 - doing and seeing whether we are able to
76:05 - get the good results or not so yes uh
76:07 - let's wait for some time thank you so
76:10 - guys uh finally you can see the 250 EPO
76:13 - or 250 steps have completed it took 25
76:15 - minutes and again this is in Google
76:17 - collab if you have paid version of
76:20 - Google collab it will probably take
76:21 - hardly 5 to 10 minutes to complete Okay
76:24 - so over here you can see the global step
76:26 - was 250 training loss it went went till
76:29 - 1.36 metrics runtime everything met
76:33 - training samples per second all this
76:35 - information is basically done okay and
76:37 - please remember this particular word
76:39 - which is called as floss okay total
76:40 - floss because I'm going to discuss about
76:42 - this in my next video also now once we
76:46 - do this we are going to save this train
76:47 - model right and understand the new model
76:50 - name what it will be right so here you
76:52 - can probably see Lama 27 be chat fine
76:55 - tune so this is my results with respect
76:58 - to run all the results you'll be able to
77:00 - see over here also okay so here uh in
77:03 - this fine tuning technique it is also
77:05 - creating some something called as
77:06 - adapter adapter model okay please
77:09 - remember these words because in the next
77:11 - theoretical intuition we are going to
77:12 - discuss each and everything as we go
77:14 - ahead okay so please make sure that you
77:16 - remember it so we are going to save this
77:18 - model so we have written trainer. model.
77:20 - save. pre-rain model right now you can
77:22 - also check out in the tensor board but I
77:25 - will just go ahead and show you quickly
77:27 - that how it is probably going to
77:28 - generate it right so here we have
77:30 - created a prompt which is called as what
77:32 - is large language model I've used
77:34 - pipeline right so this pipeline we have
77:36 - already imported it the task will be
77:38 - task generation whatever model we have
77:40 - actually created that model will be
77:41 - there tokenizer will be used over here
77:44 - and max length we can keep to 2 200 to
77:46 - 250 the result uh and always understand
77:49 - as I always suggested with respect to
77:52 - Lama 2 this will be my format there will
77:54 - be an s s then there will be an
77:55 - instruction and here I will be having my
77:58 - prompt and with respect to this
78:00 - particular prompt we are going to get
78:02 - some kind of response so whatever
78:03 - response we are going to get inside this
78:05 - result variable it will be in the form
78:07 - of list and inside that there will be
78:09 - one field which is called as generated
78:10 - text so if I go ahead and search what is
78:13 - large language model you'll be able to
78:15 - see that how we going to get the result
78:18 - okay because we are running the same
78:20 - model over here so here is my prompt
78:22 - here we are using pipeline pipeline
78:23 - basically helps you to come combine
78:25 - multiple things like task model
78:27 - tokenizers you know multiple things it
78:29 - will be able to give you right now since
78:31 - this is already running in this
78:33 - particular collab uh and obviously
78:36 - you'll be able to see RAM and all are
78:38 - almost it is used the dis bace of
78:40 - somewhere around 39 GB right so just
78:43 - wait for some time and here you will be
78:45 - able to get the response if you quickly
78:47 - want to get the response obviously you
78:48 - need to have a good GPU right based on
78:51 - that it'll be able to give you a quick
78:53 - result right so after that you'll be
78:55 - also able to see that we'll be able to
78:56 - delete all these V Rams and all okay so
79:00 - let's see and let's see whether we'll be
79:02 - able to get our result in the next step
79:04 - we can also push our model to the
79:06 - hugging phas which I will keep it right
79:08 - now I will not explain it because this I
79:11 - will show you as an complete project as
79:13 - we go ahead so here you can see what is
79:15 - large language Model A large language
79:17 - model is a type of artificial
79:18 - intelligence large language model often
79:20 - seen then here you can also see all the
79:22 - information are there some example of
79:24 - large language model models are uh
79:26 - include this okay now what we are going
79:28 - to do let's go ahead and take any one
79:30 - example over here from this particular
79:32 - data set okay so I will just write how
79:35 - to own a plane in United States okay so
79:40 - this will be
79:41 - my over here and I'll paste it over here
79:45 - let's see so this will also run and I
79:49 - will finally get my result also so same
79:51 - same question I have taken right so from
79:53 - this 1K result so to own a plane this is
79:55 - the answer that we will probably be
79:58 - getting let's see how much time it'll
80:00 - take to probably showcase but always
80:03 - remember please keep on looking at this
80:05 - particular Ram like how much uh time it
80:07 - is probably taking and uh how much space
80:09 - it is taking okay so so guys here you
80:13 - can probably see the response how to own
80:15 - a plane in united state in United State
80:16 - and owning a plane is this determine
80:18 - your budget so this is completely based
80:20 - on this information that is present over
80:22 - here but here I've have written only 200
80:24 - max length so I can only see 200
80:26 - characters that is given right so you
80:28 - can probably try with each and
80:29 - everything as you go ahead one of the
80:32 - most interesting thing in the field of
80:34 - data science or generative AI is that
80:36 - the kind of research that is currently
80:38 - happening right every day you'll be
80:40 - seeing some new things that are actually
80:41 - happening which is very much beneficial
80:43 - for the entire Community who are working
80:45 - with llm models uh specifically today I
80:48 - saw this amazing research paper where it
80:50 - is written as era of 1 bit llm so I'll
80:53 - be going to talk about this particular
80:54 - research paper and what exactly 1 bit
80:57 - llm is and how it is far more
80:59 - advantageous when compared to those
81:01 - 32bit or 16bit llms models okay so
81:04 - everything I'll be discussing about one
81:06 - important thing that I also want to make
81:08 - sure that you learn from this particular
81:09 - video is that how do you read a research
81:11 - paper what are the important points that
81:14 - you should definitely highlight while
81:16 - reading a research paper and how you
81:18 - should definite and one thing is that
81:20 - you cannot directly understand just by
81:21 - reading it you really need to have some
81:23 - basic knowledge and without that
81:25 - particular basic knowledge it will be
81:26 - very difficult to understand so if
81:28 - you're following my tutorials I always
81:30 - make sure that whenever I make my videos
81:32 - right I definitely watch or see all the
81:34 - research papers and then with respect to
81:37 - that I simplify the those Concepts and
81:38 - try to explain it to you so let's go
81:40 - ahead and understand about this onebit
81:42 - llm now guys uh if you remember in my
81:45 - previous video we have already discussed
81:47 - about quation right so quation was
81:51 - covered now with respect to quation what
81:53 - we were doing is that let's say I have a
81:55 - model which is called as Lama 2 which is
81:57 - an open source model let's say this
81:59 - model is 7 billion having 7 billion
82:01 - parameters when we say 7 billion
82:04 - parameters I'm talking about weights
82:06 - okay now obviously if I have a system
82:09 - where I don't have very high
82:10 - configuration not I have resource
82:12 - constraint I have limited amount of Ram
82:13 - or gpus what we specifically do we
82:16 - perform quantisation and we convert this
82:19 - Lama 2 model which is probably in FP
82:22 - 32bit and we try to convert this into to
82:24 - int 8 bit okay int 8 which is nothing
82:28 - but 8 Bits right now when we are once we
82:31 - are doing this specific process what is
82:33 - basically happening is that the model
82:34 - size is getting decreased right and
82:36 - because of that we will be able to load
82:38 - it and we'll be able to perform any task
82:40 - along with this we can also perform fine
82:42 - tuning with the help of Laura and CLA
82:45 - right so I hope you know this Laura and
82:46 - CLA I've already discussed in my
82:49 - previous video please just go click on
82:51 - my uh click on my channel otherwise just
82:53 - go ahead and see in the description I've
82:55 - been providing that particular links
82:57 - with respect to fine tuning now with the
82:59 - help of LA and cl we can perform the
83:00 - fine tuning okay now the question is
83:03 - that what is this one bit llm right as I
83:07 - said that with the help of quantisation
83:08 - we will try to convert this into 32 to
83:10 - 16 bit or it can be 8 bit right but
83:13 - converting this into a one bit that can
83:15 - be again uh if you're trying if you now
83:18 - just by seeing this right if we are able
83:20 - to convert this into one bit that
83:21 - basically means we will never be having
83:23 - any resour constraint right resource
83:27 - constraint yes with limited Ram with
83:29 - limited GPU with limited storage we can
83:32 - probably perform everything from fine
83:34 - tuning to inferencing right so
83:36 - inferencing can also be performed right
83:39 - and this is what is so amazing about
83:41 - this and this is I I don't know like
83:43 - what is going to happen just in some
83:45 - days because once this is probably gone
83:47 - right now we just have the research
83:49 - paper once this implementation gets
83:50 - started trust me it will be quite
83:53 - amazing for the entire Community who are
83:54 - working with llm models okay so this was
83:57 - just a brief idea about this one now
83:59 - let's go ahead and discuss what is
84:02 - onebit llm okay and when we say to be
84:06 - precise when we say that all large
84:07 - language models it is basically in 1.58
84:10 - bits okay white is 1.58 we'll discuss
84:13 - about it and there are many points that
84:14 - needs to be discussed uh along with me
84:17 - please make sure that you watch this
84:18 - video till the end because I'm going to
84:20 - read over here because this will also
84:21 - give you an idea that how you should
84:22 - probably go ahead and read the research
84:24 - paper
84:24 - so let me quickly uh go ahead and clear
84:28 - this let's see whether it will getting
84:30 - cleared or not okay so over here okay
84:35 - clear is basically
84:37 - happening um okay I will just rub it
84:40 - okay now let's go ahead and discuss
84:42 - about this and let's read some of the
84:45 - important information that is present
84:47 - over here okay and trust me guys read
84:49 - along with me then only you'll be able
84:51 - to understand how you can read the
84:52 - research paper okay
84:55 - now what exactly this 1bit llm model is
84:58 - u in this work we introduce a 1bit llm
85:01 - variant namely bit net okay so bit net
85:06 - is the llm model name one bit llm model
85:08 - name and then where every single
85:11 - parameter or weight of the llm is Turner
85:14 - right now it is not floating 62 bit uh
85:16 - sorry 32bit or 16 bit it is Turner
85:19 - Turner basically means it has only three
85:21 - values it can have only three values
85:23 - weights it can be minus one 0o or 1 okay
85:27 - it matches the full Precision
85:29 - Transformer M with the same model size
85:31 - and training tokens in terms of
85:33 - perplexity perplexity basically means so
85:35 - with respect to any query that I ask an
85:37 - end to end task performance right while
85:39 - being significantly more cost effective
85:41 - in performance of latence memory
85:43 - throughput and energy consumption so
85:45 - obviously at the end of the day all the
85:47 - llm model will specifically have this
85:49 - kind of constraint right which are
85:51 - specifically with huge uh number of
85:54 - parameters let's say 7 billion 170
85:56 - billion right and if you're just using
85:58 - this three numbers minus 1 0 1 you'll be
86:01 - able to understand why I'm saying that
86:03 - because of the stary uh values right
86:06 - you'll be seeing how abundance the
86:08 - performance improves okay so furthermore
86:11 - uh so here you can probably see all this
86:13 - points uh latence memory throughput and
86:15 - energy consum uh consumption uh energy
86:18 - consumption can be with respect to
86:19 - inferencing with respect to fine tuning
86:21 - and all okay now let's understand how
86:26 - this operators how this values will be
86:28 - basically used okay this is also
86:30 - important so with respect to this what I
86:32 - am actually going to do I am going to
86:35 - make sure that to explain you I take the
86:39 - right thing okay so let's understand
86:42 - this okay understand guys whenever we
86:44 - talk about parameters these are my
86:47 - weights okay these are my
86:50 - weights let's see so these are my
86:53 - weights
86:55 - okay and these are my weights so let's
86:58 - consider that my initial Transformer llm
87:00 - weights is this one okay now by when we
87:05 - say 1 bit
87:06 - llm we are going to convert all this
87:10 - values and replace them with either of
87:13 - these three values Min - one 0 comma 1
87:16 - okay so that is the reason that you see
87:17 - over here all these weights is being
87:20 - getting converted into something like
87:22 - this okay minus1 0 or 1 only that three
87:25 - parameters is there okay and this is
87:28 - what we basically say as bitnet B
87:31 - 1.58 okay and this is also called as
87:35 - parito Improvement how this is basically
87:37 - happening I will talk about it okay just
87:39 - give me some time there will be some
87:41 - kind of quantization getting applied
87:44 - here also okay quantization getting
87:46 - applied over here okay to convert this
87:49 - values to this okay now let's understand
87:53 - one very important thing okay and this
87:55 - is the most important thing what will
87:57 - happen if you convert this values to
88:00 - this see with respect to any fine-tuning
88:03 - of forward propagation backward
88:04 - propagation what exactly happens the
88:06 - model weights the model weights over
88:10 - here is basically getting multiplied by
88:12 - the inputs and then we get the output
88:15 - right yes additionally we add a bias so
88:17 - it's okay we don't include a bias right
88:19 - now over here just to show it to you so
88:21 - over here this let's consider that this
88:23 - is my float flating 16 number so every
88:26 - number will get multiplied by the input
88:29 - right and then what will happen is
88:31 - that after that all the it's it's just
88:34 - like this right summation of I equal to
88:36 - 1 to n w of x + B right so this is what
88:41 - is the operation that is basically
88:42 - happening whenever we do the forward
88:44 - propagation Whenever there is an
88:46 - updation of weight that basically means
88:48 - we are doing the summation of weights
88:50 - and the input right so once we are doing
88:53 - this and then we are doing the sumission
88:55 - okay but if we have all these weights in
88:57 - the form of -1 1 0 then what will happen
89:01 - is that over here you'll be seeing that
89:03 - multiplication operation will not be you
89:05 - know that much valuable right so over
89:08 - here first of all we are doing
89:09 - multiplication then addition but over
89:11 - here we are just doing addition no
89:13 - multiplication because any number it
89:15 - will be multiplied by 0 is z only any
89:17 - number that is multiplied by one is one
89:19 - only any number that is multiplied by
89:21 - minus1 is min-1 only so over here the
89:24 - main thing is that your addition
89:26 - operation is only
89:27 - happening addition operation is only
89:30 - Happening Now obviously if you only need
89:32 - to do addition operation then what will
89:34 - happen your GPU will not be requiring
89:36 - that much GPU also so your GPU will also
89:38 - get
89:39 - reduced why why this operation takes
89:42 - more GPU because multiple multiplication
89:44 - needs to happen right with respect to
89:46 - different different weights right then
89:48 - addition of all those values needs to
89:49 - happen because in the forward
89:51 - propagation this is what is the equation
89:53 - that specific Ally happens right we
89:55 - multiply by the weights with the inputs
89:58 - and then we do the summation and then
89:59 - finally we add the bias right so this is
90:02 - the most important thing so here you'll
90:05 - be able to understand with floating 16
90:08 - right all the numbers is first of all
90:10 - multiplied by the inputs and then the
90:11 - sumission is done but here your values
90:13 - are with respect to Turner that is min-1
90:16 - 0 1 so here multiplication is already
90:19 - skipped because 1 into x0 is x0 only
90:22 - right it is a simple multipli ation
90:24 - right and that much resources will not
90:26 - be required for simplistic
90:27 - multiplication so here maximum to
90:30 - maximum only addition will be required
90:32 - right so I hope you're able to
90:34 - understand because of this technique of
90:36 - parito improvement because of this
90:39 - technique of Paro Improvement you'll be
90:41 - able to see that what we are able to
90:43 - achieve right and obviously when we are
90:45 - able to achieve this the GP will be
90:49 - required less when we are doing the fine
90:51 - tuning or training right so I hope you
90:53 - have got this as an complete idea and
90:57 - you have understood right why we
90:59 - specifically do this how it is done how
91:02 - this transformation is done so here you
91:04 - can probably see that it provides a Paro
91:06 - solutions to reduce inferencing cost
91:08 - latency throughput and energy of llm
91:11 - while maintaining the model performance
91:12 - the new computation Paradigm of Paradigm
91:15 - of bitnet 1.58 calls for Action to
91:18 - design new hardware optimization for
91:21 - 1bit llm right I know guys this is more
91:24 - of a research paper so I'm reading and
91:26 - I'm telling you each and everything and
91:28 - also explaining you the concept I know
91:30 - this can be a little bit of boring but
91:32 - trust me you need to understand in this
91:34 - specific way okay now let's talk more
91:38 - about this and we will I've highlighted
91:41 - main main things in this green color
91:42 - okay these models have demonstrated
91:44 - remarkable performance in a wide range
91:46 - of natural language processing task like
91:48 - llm models but their increasing size has
91:51 - posed challenges for deployment and
91:53 - raised conc concern about the
91:54 - environmental and economic impact due to
91:57 - high energy consumption obviously this
91:58 - is the problem with llms that are
92:00 - already available one approach to
92:03 - address the challenges to use post
92:05 - training quantization to create low bit
92:07 - models for inferencing I've already
92:09 - discussed about this quantisation Laura
92:12 - CLA everything this technique reduces
92:14 - the Precision of weights and activation
92:16 - significantly reducing the memory and
92:18 - computational requirement of llm the
92:20 - trend has been to move from 16 bit to
92:22 - lower bit such as 4 bid variant this is
92:25 - what is basically happening with respect
92:27 - to llm models right this is with llm
92:31 - okay this is with
92:34 - llm so here I'll write llm models now
92:37 - let's see with the help of one bit
92:39 - architecture one bit model architecture
92:41 - what we can solve so recent work on one
92:44 - bit model architecture such as bitnet
92:46 - presents a promising direction from
92:48 - reducing the cost of llm while
92:50 - maintaining the performance walina llms
92:54 - vanila LMS are in 16bit floating values
92:57 - and the bulk of L LM is matrix
92:59 - multiplication therefore the major
93:02 - computation cost comes from floating
93:03 - Point addition and multiplication
93:05 - operation I said you just now on top of
93:07 - it right in contract the matrix
93:09 - multiplication of bit net only involves
93:11 - integer addition because anything
93:14 - multiplied by one is one uh anything
93:16 - multiplied by one is that same number
93:18 - anything multiplied by minus one is that
93:20 - same number with a negative sign
93:21 - anything multiplied by 0 is obviously 0
93:24 - right so as the fundamental limit to
93:26 - compute performance in many chips is
93:27 - power this energy saving can be
93:29 - translated into faster computation now
93:31 - this is the most important thing right
93:35 - and here you can clearly see the things
93:37 - that I've highlighted right I hope you
93:40 - get an idea how good this one bit llm
93:43 - can be okay then you can still read
93:47 - about it here we are going to just use
93:48 - turn values like Min -1 0 1 and
93:52 - obviously because of this zero your 58
93:55 - bit is basically increasing there are
93:57 - two major advantages of using this also
94:00 - it is written over here
94:02 - see furthermore bitnet oh my God why
94:06 - this is getting highlighted like okay
94:08 - furthermore bitnet offers two additional
94:11 - Advantage first its modeling capacity is
94:14 - stronger due to explicit support for
94:16 - feature filtering how feature filtering
94:18 - happen because anything multiplied by
94:19 - zero will be zero on now right made
94:23 - possible by the inclusion of zero in the
94:25 - model weight which can significantly
94:26 - improve the performance of 1 bit LM
94:28 - secondly our experiment shows can match
94:30 - full Precision Baseline in terms of this
94:32 - end to end task performence starting
94:34 - from a 3B size okay now most of the
94:36 - things that you are able to see right
94:39 - now let's discuss about one more
94:40 - important thing uh that is how this
94:43 - transformation is happening how this
94:45 - numberers are getting converted to this
94:47 - it is just by using this simple
94:50 - mathematical equation or this
94:52 - quantization function
94:54 - okay quantization
94:57 - function quantization function okay and
95:00 - this quantization function is called as
95:03 - absolute mean quation and this is the
95:05 - formula that is basically used by which
95:08 - all the numbers are basically getting
95:10 - converted to only this three values okay
95:13 - 0 1
95:16 - okay 1 0 1 okay just by applying this
95:20 - particular formula okay so in
95:24 - uh and there is also one more change
95:25 - with respect to the Transformer it
95:27 - replaces nn. linear with bit linear okay
95:31 - so this bit linear I think uh you'll be
95:34 - able to see that it is stained from
95:35 - scratch with 1.58 bit weights and 8 bit
95:38 - Activation so this is what is basically
95:40 - done with respect to the initial
95:41 - training okay so most of the thing I
95:44 - have actually discussed over here uh
95:47 - let's talk about the performance so here
95:49 - you'll be able to see that uh the Llama
95:52 - model of 700 million p parameters bitnet
95:54 - will also have 7 million parameters but
95:56 - here you see the memory is in decreasing
96:00 - right over here 2.08 1.18 12.33 is
96:03 - getting reduced to
96:05 - 8.96 and then this PPL is basically
96:08 - 12.87 so over here you can see that how
96:10 - it is getting reduced now similarly when
96:12 - the billion of parameters are basically
96:14 - increasing right let's say with Lama is
96:17 - 1.3 billion right the parameter will be
96:19 - same but memory again 1.14 is required
96:21 - .97 11.29 right and similarly over here
96:25 - also you'll be able to see the same
96:26 - thing is basically happening so memory
96:28 - is decreasing latency is also decreasing
96:30 - for the inferencing purpose perfect and
96:34 - uh one more parameter that you'll be
96:35 - able to see with respect to model size
96:37 - and latency right model size so the the
96:41 - blue color is basically the Llama model
96:43 - OKAY the orange color is basically one
96:46 - bit llm models you'll be able to see how
96:49 - much huge latency difference is there
96:51 - similarly with respect to this how much
96:55 - huge memory difference is there right to
96:57 - save this kind of models so uh this is
97:00 - just the research paper that has come up
97:01 - recently but uh I'm really really happy
97:04 - to see this because in the future many
97:06 - things is going to happen so again I
97:09 - would like to welcome you all to the era
97:11 - of 1bit llm models and now you'll also
97:14 - be able to use this one bit llm model
97:16 - soon I think first of all hugging pH
97:18 - will only come and try to implement all
97:20 - these things where you can also easily
97:22 - create your application using gen so
97:24 - guys Google is again on bang and it has
97:28 - created its own open-source llm model
97:31 - that is called as gamma now again it is
97:33 - on that specific race of Open Source llm
97:36 - models also till now the most accurate
97:38 - model that we specifically had was from
97:40 - meta that is called as Lama 2 uh now
97:43 - gamma is there uh in this specific video
97:46 - we'll try to see a practical application
97:49 - along with that we'll try to fine tune
97:51 - with the help of this specific gamma
97:52 - model okay
97:54 - uh so what exactly is gamma model uh
97:56 - I'll just show you one block first of
97:57 - all why they have actually created it at
98:00 - the end of the day all the companies you
98:02 - know is saying this thing right so gamma
98:05 - is built for responsible AI development
98:07 - from the same research and technology
98:08 - used to create Gman models okay uh at
98:11 - Google we believe in making AI helpful
98:13 - for everyone we have a long history of
98:15 - contributing to open source communi such
98:16 - as Transformer tens oflow B T5 Jacks
98:19 - Alpha fold and Alpha code obviously
98:21 - Google is doing I think it is it is from
98:23 - past so many years it is doing so much
98:26 - of research specifically in open source
98:28 - contribution it has given tensor flow it
98:29 - has given many things as such uh along
98:32 - with that meta is also in the same race
98:34 - uh both are doing fabulous job so let's
98:37 - go ahead and let's know more about Gamma
98:39 - uh GMA GMA we'll say GMA okay GMA also
98:43 - we can say I don't know how to pronounce
98:45 - it but it's fine so if I probably talk
98:48 - about this the main thing that you
98:49 - really need to know is about the
98:51 - performance metrics uh over here when I
98:54 - compare see two models are there with
98:55 - respect to this particular model one is
98:57 - 7 billion parameter one is 2 billion
98:59 - parameter if I consider with respect to
99:01 - Lama 2 uh uh we have 13 billion and 7
99:03 - billion with respect to all these things
99:06 - I think gamma is performing absolutely
99:09 - fine here you can see that it is having
99:11 - 64.3 with respect to various Benchmark
99:13 - accuracy if you want to know more
99:15 - details about it you can actually see
99:17 - this particular blog uh over here if I
99:19 - if we are just comparing open source
99:21 - models like gamma and Lama 2 uh gamma is
99:24 - Way Forward when we compare with respect
99:26 - to the 7 billion parameter it has right
99:28 - 64.3 the general accuracy reasoning is 5
99:31 - 55.1 81.2 math 46.4 it is far far better
99:36 - than the Lama 2 models also right so
99:38 - this looks absolutely great uh here the
99:41 - accuracy is also very very good now
99:43 - let's uh talk let's see more about it it
99:46 - is specifically used for you know
99:49 - research purpose you can also use it
99:51 - over here you can build your own models
99:53 - and the other thing is that right now uh
99:56 - the gamma 2 models right 2 billion
99:58 - parameters 7 billion parameters already
100:00 - available over here see in hugging phase
100:02 - so once you probably go to hugging phase
100:03 - and search for gamma 7B or gamma 2B
100:06 - you'll be able to see this kind of page
100:08 - here uh you need to get granted to
100:11 - access this model so here they'll tell
100:12 - you to probably uh check the terms and
100:15 - conditions to get the license of this
100:17 - particular model to use it okay so it'll
100:19 - be like a check box you just need to
100:21 - check it I understand that that and give
100:23 - the confirmation right once you probably
100:25 - do that you'll be able to get the excess
100:27 - of this specific model um what is the
100:29 - main a of this particular video I'll try
100:31 - to show you with the help of practical
100:32 - implementation how you can access this
100:34 - particular model and how you can
100:36 - actually use it so now let's go ahead
100:38 - and let's see this fine-tuning technique
100:41 - with the help of Google Gamma model and
100:43 - uh again guys uh this is just a simple
100:46 - use case that I have taken over here but
100:48 - you can do some amazing use cases with
100:50 - the help of this and that is what I'll
100:52 - plan in the future some amazing use
100:54 - cases that I've already developed with
100:55 - paid opening eye models I'll try to also
100:57 - develop that with this along with fine
100:59 - tuning okay so initially uh what we
101:02 - going to do we going to see this Google
101:04 - Gamma model how it performs and what
101:06 - kind of task all that kind of NLP task
101:08 - you'll be able to do it okay so
101:10 - initially to go ahead with I will go
101:11 - ahead and install all these libraries
101:13 - that is bits and bytes PFT TRL aate data
101:16 - sets and Transformers if you don't know
101:18 - guys I've already created a video with
101:20 - respect to finetuning okay now in tuning
101:23 - what we have actually done we have done
101:25 - the fine tuning with the help of Lama 2
101:27 - model okay so let's see a UTF local is
101:29 - required let me restart my
101:31 - [Music]
101:33 - kernel change runtime disconnect and
101:35 - delete runtime okay so let me go ahead
101:38 - and reconnect it and then we will try to
101:40 - do the installation again but I've
101:42 - already created a fine-tuning technique
101:44 - with the help of Lama 2 model the same
101:46 - steps and process will basically happen
101:48 - over here also so let's Okay got
101:51 - connected now let's go ahead andall uh
101:53 - do the PIP install so here you'll be
101:55 - able to see the PIP installation will
101:57 - specifically happen with all these
101:59 - libraries to just tell you what we are
102:01 - going to do this bits and bytes
102:03 - techniques bits and bytes libraries used
102:05 - for quantization purpose I've already
102:07 - created a video with respect to
102:09 - quantisation what is the main name of
102:10 - quantisation quantisation actually help
102:14 - uh allows you let's say that you have a
102:16 - huge model right right now we have this
102:18 - Google Gamma model of 7 billion
102:19 - parameters or 2 billion parameters if I
102:22 - want to load it let's say in Google
102:24 - collab so in Google collab right now I
102:26 - have somewhere around 50 GB Ram or 2011
102:29 - GB hard disk right and right now I'm
102:32 - using this Google collab premium version
102:34 - right let's say if you're using it
102:36 - you'll hardly get 15 GB Ram it is not
102:38 - possible to load that entire model over
102:40 - here so what quation techniques helps
102:42 - you do is that it helps you to uh
102:45 - convert that float 32 bit right usually
102:48 - all the numbers all the weights right
102:50 - all the bi are saved in 32 bits it will
102:53 - help you to convert that into 8 bit 16
102:55 - bit so when you convert that you require
102:58 - less memory at that point of time right
103:00 - so that process is specifically called
103:02 - as quantisation and I have actually
103:04 - explained it I've shown you how
103:05 - mathematically we specifically do
103:07 - quation so everything is explained in
103:09 - this particular video I'll provide you
103:11 - this link in the description of this
103:12 - particular video now the next thing is
103:14 - that we will be importing some important
103:17 - libraries one is OS Transformers then
103:20 - you can probably see torch Google collab
103:23 - user data data sets I'm going to load it
103:25 - sft trainer PFT right PFT PFT Laura
103:30 - configuration as I said that we going to
103:32 - use Laura technique okay sft trainer is
103:35 - specifically used for fine tuning
103:36 - purpose supervised fine tuning we
103:38 - basically say right now from
103:40 - Transformers We also going to use Auto
103:42 - tokenizer because tokenization if you
103:44 - want to really perform tokenization
103:45 - while fine tuning we'll be using this
103:47 - Auto tokenizer so that it can load based
103:50 - on the model that we have then you have
103:52 - this Auto model for casual llm this is
103:54 - specifically used so that we'll be able
103:56 - to fine tune with respect to Casual
103:58 - large language model okay then from
104:01 - Transformers We also going to import
104:02 - bits and bytes config here we are going
104:04 - to give the configuration with respect
104:06 - to quantisation and then we have this
104:08 - gamma tokenizer so let's go ahead and
104:10 - import all these things so gamma
104:12 - tokenizer is something for the gamma
104:14 - model itself right a tokenization
104:16 - technique otherwise you can also use
104:17 - other tokenization it is not necessary
104:19 - that you always need to use gamma
104:21 - tokenizer now let let me tell you one
104:23 - more step the next thing is that after
104:26 - importing all this you know that this
104:28 - gamma 7B model is specifically present
104:30 - in hugging phase right now in order to
104:32 - access this model in order to download
104:34 - this model we really need to have
104:37 - something called as access token exess
104:39 - token of what excess token of the
104:41 - hugging phas so how do you get an excess
104:43 - token go over here click on settings and
104:46 - here you'll be having something called
104:47 - as excess token just copy this okay copy
104:50 - this excess token and once we have this
104:53 - exess token access right then only we'll
104:55 - be able to download the model from here
104:57 - so here what we will do go ahead and set
104:59 - it in the Google collab how do you set
105:01 - it click on this key button so everybody
105:04 - will be having this key button and just
105:06 - click on add a new secret and I have
105:08 - written it over here as HF token with
105:11 - that same value that I've have copied
105:12 - from there so once I probably write it
105:14 - down over here so this is basically
105:16 - saved right so this is saved over here
105:19 - now once I've have saved it any number
105:20 - of notebook that you specifically work
105:22 - in you will be able to access this
105:23 - hugging fish token how do you access it
105:25 - it is in simple os. environment HF token
105:29 - is equal to user data. getet HF token
105:32 - you just need to write user data. get HF
105:34 - token so once you do that it will ask
105:36 - you for the grant you just need to Grant
105:38 - it and then it'll be able to access and
105:40 - it'll be able to also identify which
105:42 - hugging face uh from which username or
105:45 - password for or which usern name you are
105:47 - specifically pulling that particular
105:49 - information okay so this is done right
105:51 - now through this you'll be able to
105:53 - access any models from the hugging phase
105:54 - right Let It Be llama 2 any model that
105:56 - you specifically want to use now what
105:59 - we'll do we will go ahead and call this
106:01 - particular model Google Gamma 2B
106:04 - initially I told showed you 7B right I'm
106:06 - going to load it 2B right 2 billion
106:08 - parameters not 7B 7B also you can do it
106:10 - it still take time for the fine tuning
106:12 - purpose now here you can probably see
106:15 - I've taken this and then I have written
106:17 - bytes BNB configuration that is bits and
106:20 - bytes configuration the first parameter
106:22 - is load in 4 bit is equal to true that
106:24 - basically means what my model the gamma
106:27 - model in 2 billion parameters is
106:29 - specifically in the 32bit right
106:31 - initially we are trying to convert that
106:33 - into 4bit and this process is
106:35 - specifically called as quation right so
106:37 - what we are doing this once we set it in
106:39 - load in 4bit all the activation function
106:42 - sorry all the weights that are stored in
106:44 - 32 bits will get converted into 4bit
106:46 - okay now the second parameter that
106:48 - you'll be able to see BNB 4bit Quant
106:51 - type is equal to n F4 what is nf4 4bit
106:55 - normal float so the quation technique
106:58 - how do you convert this into a 4bit is
107:00 - basically taken from this particular
107:02 - parameter if you really want to know
107:03 - about it I've given a link over here
107:05 - what is 4bit quation and how does it
107:07 - help Lama 2 the reason why I've written
107:09 - Lama 2 because we are talking about open
107:11 - source model then you have BNB 4bit
107:13 - compute type which is nothing but torch
107:15 - do B float 16bit now the reason we have
107:18 - kept this as 15 bit 16bit see this we
107:23 - are performing quation that basically
107:24 - means we are taking a big model we are
107:26 - making it to a small model how by
107:27 - performing quantisation by converting
107:29 - the 32bit into 4bit now along with that
107:32 - whatever fine tuning takes place for
107:34 - those fine tuning all the weights that
107:37 - gets updated will get updated with
107:39 - respect to this particular 16 bit see
107:41 - there is some loss of information with
107:43 - respect to quation to balance that we
107:45 - are keeping the new fin tune parameters
107:47 - in 16bit so that is the reason we have
107:49 - written it over here so once I execute
107:51 - this so this is my model ID this is this
107:52 - is my configuration now based on this
107:54 - model ID and configuration I'm going to
107:56 - use the tokenizer and from this
107:58 - particular pre-train model I'm going to
108:00 - call the tokenizer and for here also
108:02 - I'll be using the HF token okay then you
108:04 - have model over here Order model for
108:06 - casual LM from pre-train model ID we'll
108:09 - use this model ID that see auto
108:11 - tokenizer is to call the tokenizer that
108:14 - is required for this particular model
108:16 - right and here also I've given the HF
108:18 - tokon automatically that tokenizer will
108:20 - got called right then we are using Auto
108:23 - model for casual LM and here also you'll
108:25 - be able to see that we using model ID
108:28 - quantization config with respect to the
108:29 - BNB configuration the device map will be
108:32 - equal to zero which is nothing for the
108:33 - GPU and here you can see that I'm going
108:35 - to use the HF undor token okay so once I
108:39 - execute this you'll be able to see that
108:42 - I am going to download all those
108:44 - information see auto model specifically
108:46 - to call those particular model the model
108:48 - ID that I've given over here by applying
108:51 - this quation technique and this entire
108:53 - process is going to happen in the GPU
108:55 - the GPU that my Google collab is
108:57 - connected to and over here I've given
108:58 - the token so quickly you'll be able to
109:00 - see that the entire model will get
109:02 - downloaded and now all the information
109:05 - will be specifically present in this
109:07 - particular model now with the help of
109:08 - this particular model I will go ahead
109:10 - and test it now now see this how I'm
109:12 - going to test it I've given a quote over
109:14 - here imagination is more I've used a
109:17 - device Cuda is equal to Z that basically
109:18 - means whatever is the GPU that is
109:20 - available and then I'm going to use the
109:22 - same tokenizer see this tokenizer is
109:24 - basically getting used I give the text
109:27 - and I get this particular information so
109:29 - this input will be a vectors right and
109:32 - then what we do we use this SP same
109:34 - model and we say do generate with
109:36 - respect to this particular input and
109:38 - let's say that I'm going to take a token
109:39 - of 20 okay initially then once I get
109:43 - this particular output the output will
109:45 - have in the form of list like what all
109:47 - information is basically coming so once
109:49 - we write tokenizer do decode of output
109:51 - zero see at the end of the day we are
109:53 - doing sentence generation so when we
109:54 - write model. generated is again going to
109:57 - create some embeding vectors right and
109:59 - that output which is in the form of
110:00 - vector will get decoded with the help of
110:03 - tokenizer and then we'll get the real
110:05 - text so if I execute this you'll be able
110:09 - to and it is saying skip special token
110:11 - if any token is there it'll get removed
110:13 - okay so once I execute it you can see
110:15 - imagination is more than knowledge I'm a
110:17 - self-taught artist born in 1985 right so
110:20 - this information is basically coming
110:22 - based on the based on the model that it
110:24 - has let me go ahead and execute once
110:26 - more okay so here also you can able to
110:28 - see that I'm trying to execute one more
110:31 - time let's see whether I get a different
110:32 - output so here you can see imagination
110:35 - is a more than knowledge knowledge is
110:36 - limited imagination in circles award so
110:38 - this information is given by Albert
110:40 - Einstein okay so two information and if
110:43 - you probably search in the internet
110:44 - you'll be able to find this quote also
110:46 - okay so one over here one output we got
110:50 - where with the code completion here you
110:52 - can you you could probably see that I'm
110:54 - also getting the author name okay now
110:57 - this is what it is able to do it okay so
110:59 - whatever information you probably put it
111:00 - over here it'll be able to get the
111:02 - output based on the information that you
111:04 - require now what we going to do we I'm
111:06 - going to show you a fine tuning
111:07 - technique now see this. environment van
111:10 - disabled so one parameter we need to
111:12 - keep it as false that was given in the
111:14 - documentation also I do not still
111:15 - understand why this parameter was used
111:17 - but once I know it I'll I'll let you
111:19 - know because I'm going to create a lot
111:20 - of projects as we go ahead now we are
111:22 - going to fine tuning with the help of
111:24 - Laura configuration so for Laura
111:26 - configuration what are information I
111:27 - require one is rank okay so Laura
111:32 - technique is something called as rank
111:33 - decomposition we'll get to know I've not
111:35 - still uploaded the theory video so here
111:38 - we can select the rank value okay so
111:40 - rank over here is selected as eight why8
111:43 - why cannot select 16 or 64 you can
111:45 - select any number but understand what
111:48 - exactly is rank decomposition for that I
111:49 - need to create a dedicated video and
111:51 - soon I'll be coming up with this the
111:53 - target modules uh in this you
111:55 - specifically require this KV parameters
111:58 - gate up project down project so right
112:01 - these are some of the target models that
112:02 - is required as I said guys once I
112:04 - probably upload a video dedicated to
112:06 - Laura configuration you'll be able to
112:08 - understand the task type we always need
112:10 - to keep it as calore LM that basically
112:13 - means it is specifically used for this
112:16 - language modeling task so this is one of
112:18 - the parameter that we need to set it up
112:20 - so once I execute this now let's go
112:23 - ahead and do the fine tuning okay so
112:25 - this is my Laura configuration now the
112:27 - data set that I'm going to specifically
112:29 - use is this okay so let's see this data
112:33 - set okay so I'll see this arbitrate
112:37 - English quotes
112:41 - Okay okay let me see whether we are able
112:45 - to find it out or not arbitrate hugging
112:50 - face okay I'll I'll just search for
112:52 - hugging face now this is the data set if
112:54 - you see this data set you have two
112:56 - information one is quote one is authors
112:59 - right based on this
113:01 - information this is the quote and this
113:03 - is the author and we'll try to F tune
113:05 - with this let's say that my gamma models
113:07 - knows some of the quotes in the internet
113:10 - along with the author okay now my main
113:12 - thing is that I'll have some more
113:13 - additional data set and we'll train with
113:15 - this specific data set and we'll then
113:17 - identify the author for this particular
113:19 - code okay we'll do something like that
113:21 - we'll Implement something like that so
113:23 - here you can see that I am loading this
113:25 - particular data set from arbitrate onore
113:28 - English quotes and then we are taking a
113:30 - sample from the sample of quotes Okay so
113:33 - here we are going to execute this and if
113:36 - I just execute data train of quote right
113:39 - so I will be able to see all the quotes
113:41 - over here okay quotes you can also see
113:43 - the quotes you can see the uh authors
113:46 - and all so this entire uh is getting
113:48 - generated and here you can see train
113:50 - split is also happening and once this is
113:53 - executed here I have my data set so if
113:54 - you probably go ahead and execute it
113:56 - you'll be able to see all the codes okay
113:58 - so the codes the sample of codes that we
114:00 - have there is nothing Noble in this so
114:02 - so so so information as you can see okay
114:04 - so this is my data set now whenever we
114:08 - perform sft that is supervised
114:10 - fine-tuning right we require some
114:12 - information okay one function we
114:15 - specifically require to indicate what is
114:18 - my input and what is my output okay so
114:21 - here we have used something like this
114:22 - quote example quote of zero so this is
114:25 - my first parameter like zero okay and
114:28 - then whatever information is that this
114:29 - is my input and this will be my output
114:32 - so quote of zero author of zero so that
114:34 - basically means quote of zero indicates
114:36 - I will show you wait this quote of zero
114:38 - basically indicates each and every
114:40 - sentence over here author of zero
114:42 - basically indicates this one okay so
114:44 - this information will be my output and
114:46 - this will be my input something like
114:47 - that okay so once I format this you'll
114:50 - be able to see that I'll be able to
114:51 - return my text okay so let's let's
114:53 - consider over here if I give any
114:56 - examples over here right so here you'll
114:58 - be able to see that uh any kind of
115:00 - example from that example quote will be
115:02 - separated and author information will be
115:04 - separated and then we return that
115:06 - particular text in the form of key value
115:08 - pairs in this list okay now is the time
115:11 - for SE trainer so here we use the model
115:14 - we use the data of train data of train
115:16 - is what so let's see this okay so you'll
115:19 - be able to see what is exactly data of
115:21 - train
115:23 - the same data of train you'll be able to
115:25 - see where I'll have all those
115:27 - information this is my features author
115:29 - tag input ID and attention mask okay
115:32 - then we use some training arguments per
115:35 - device batch one tra gradient
115:37 - accumulation steps four warm-up steps
115:39 - thing I'm going to run it for maximum
115:41 - steps of 100 learning rate is so much
115:43 - fp16 is true then output directory will
115:47 - be this optimization will be this right
115:49 - PFT config will be nothing but my Lowa
115:51 - configuration and whatever formatting
115:53 - function I'm using I'll be using it over
115:54 - here so am formatting function okay so
115:57 - that I get the text in this particular
115:59 - key value pairs so once I execute it
116:02 - you'll be able to see
116:05 - it so here you have uh your training
116:09 - will probably start now okay once I
116:11 - write trainer.
116:12 - train and we are going to run this for
116:15 - 100 steps now see how fast it is going
116:16 - to happen the reason is that 2 billion
116:20 - and we are trying to convert that into 4
116:22 - bill so I think the training will
116:24 - quickly happen and we taking just a
116:25 - sample of data okay sample of data so
116:28 - here you can see 26 steps is done 100
116:31 - steps and here you can also see that
116:32 - loss is decreasing right
116:36 - 3939 21802 080 it's good 131 so losses
116:41 - keep on decreasing over here
116:44 - okay now this code you'll be able to see
116:47 - I've taken from one of the over here
116:51 - let's see so so here so a human is like
116:54 - a te bag and then my author should be
116:56 - Yen roselt right so now once we execute
116:59 - this I think we should be able to get
117:01 - that specific response okay from that
117:04 - particular model so let's say 89 91 92
117:07 - 100 it is reducing but it is increasing
117:10 - a bit but again it is in that step so
117:12 - training output Global step if you still
117:15 - want to reduce the loss what you can
117:16 - basically do you can increase the number
117:17 - of steps now here I have a quote a human
117:20 - is like a teabag Cuda is Z tokenizer
117:23 - this model. generate this same thing now
117:25 - let's go ahead and execute it I should
117:27 - be able to get the author name in
117:29 - forward see illar is coming when is like
117:32 - a teabag you can't tell how strong she
117:34 - is until you put her in hot water okay
117:37 - so that information you can see you can
117:38 - never know how strong it is until you
117:40 - put it in it's hot water so see all this
117:42 - information is basically coming and the
117:44 - author is also coming as elard right so
117:47 - let's see this uh I will also take one
117:50 - more example something like this let's
117:52 - see whether we will be able to generate
117:55 - the same thing or not based on the data
117:58 - set that we have because we have already
117:59 - doing the fine tuning
118:02 - okay so I will something write like this
118:05 - okay now let's execute and see the
118:07 - output and again now every fine tuning
118:10 - you can specifically do this specific
118:12 - way right I've just shown you in one
118:13 - example the opposite of love is not hate
118:16 - it's fear and the opposite of fear is
118:18 - freedom and the most wasted of all days
118:20 - let's see whether it is there or not
118:24 - see the opposite of art is opposite of
118:27 - love is not opposite of love is not it's
118:29 - indifference the opposite of art faith
118:31 - is not somewhat it's almost equivalent
118:35 - right fear opposite of Freedom most of
118:37 - the wanted is okay let's try some more
118:43 - okay this looks unique unless and until
118:46 - this person is not a
118:48 - famous book is man's so I will just go
118:52 - go ahead and execute
118:54 - this let's see whether we will be and
118:57 - again understand guys we have just max
118:58 - new tokens is 20 because it needs to
119:00 - also complete this entire statement so
119:02 - outside of a box Dog a book is a man's
119:04 - best friend see Man's Best Friend author
119:07 - is Nicholas Comfort the quote is again
119:09 - the most wasted days of all days in one
119:12 - so you can probably see over here right
119:15 - inside of a dog something all the
119:17 - information is great right so you can
119:19 - you can check it out with different
119:21 - different examples but again at the end
119:23 - of the day yes uh fine tuning is good in
119:26 - this again as we keep on doing more and
119:28 - more we'll be able to do it now all the
119:30 - application that we have specifically
119:32 - created like text to SQL invoice we'll
119:35 - try to do it with the help of this we'll
119:37 - also try to do the fine tuning now one
119:40 - challenge that I probably see in while
119:42 - you're building an LM application there
119:44 - is a dependency of lot many different
119:48 - different tools okay let's say that I'm
119:51 - specifically using a model let's say I'm
119:53 - using open Ai and I also want to
119:56 - integrate my llm app with external
120:00 - sources it can be documents it can be
120:04 - Google search API it can be Wikipedia
120:07 - search API so for all this kind of work
120:11 - I really need to integrate with
120:12 - different different apis Al together I
120:14 - need to have an environment key created
120:16 - everywhere let's say with respect to
120:18 - Google search API let's say if I want to
120:20 - use Vector embedding I need to have a
120:22 - separate key stored for the vector
120:24 - databases uh like chroma DB pine cone if
120:27 - I want to probably integrate with
120:29 - cassendra DB and data Stacks I need to
120:31 - have different different connection
120:32 - points now this is always a challenge
120:35 - when you are creating a project from
120:37 - scratch because here to create this
120:40 - entire llm pipeline it will definitely
120:42 - take a lot of time along with that you
120:44 - really need to manage all the
120:46 - configuration now here in this video I'm
120:49 - going to talk about this amazing
120:50 - platform which is called as V
120:52 - vext and here we will be simplifying
120:56 - this entire llm Ops along with that
120:58 - we'll be building llm pipelines without
121:00 - writing any code so here you have almost
121:03 - every feature if you want to probably
121:05 - implement application L document Q&A
121:08 - Vector embedding is integrated over here
121:10 - if you're dependent on any external
121:11 - sources like Wikipedia or CF uh if
121:14 - you're dependent on Google Search
121:16 - Everything is available over here so you
121:18 - don't have to probably save all the
121:20 - configuration or create all the apis
121:22 - just with one API you can actually build
121:25 - this entire llm Pipeline and use that
121:27 - specific API in your chat uh chat Bo in
121:30 - your llm application anywhere that you
121:32 - specifically want to use now let us go
121:34 - ahead and let us see that how things
121:36 - will probably go ahead in this you can
121:38 - also start completely for free so I will
121:41 - just go ahead and log in over here so I
121:43 - have already logged in so here you can
121:45 - see with my email ID I've already logged
121:47 - in I'll show you step by step how you
121:48 - can probably perform all these things
121:50 - right please make sure that watch this
121:52 - video till the end an amazing platform
121:54 - alog together at the end of the day I
121:56 - should be able to explain you and teach
121:59 - you many more things as we go ahead
122:01 - right so quickly let's say I will go
122:04 - ahead and log
122:06 - in so once it is logged in and then we
122:09 - will go ahead and see to it let's see I
122:12 - will go ahead and log in in this
122:19 - way so once we login over here so here
122:23 - you can see that you'll be able to see a
122:25 - dashboard okay once you probably log in
122:28 - now after you log in all you have to do
122:31 - is that go and towards this AI project
122:34 - section okay if you have not logged in
122:36 - you have to sign up in the specific
122:38 - platform now what I will do is that I
122:40 - will go ahead and create a AI project so
122:43 - let me go ahead and create an AI project
122:46 - and I will name this AI project to
122:48 - something else let's say I will write uh
122:52 - document Q&A something I can write up to
122:55 - you okay so whatever application let's
122:57 - say I will say rag system right so I I'm
123:00 - actually trying to create this I will
123:02 - enable this also make sure that you
123:03 - enable this then only your application
123:05 - will probably get started now in this
123:07 - entire pipeline initially you'll be able
123:09 - to see two important section one is
123:12 - query and one is output okay so query
123:15 - basically means an event that starts the
123:18 - flow now this will be super important
123:20 - guys because a person who does not know
123:22 - much coding can also probably use this
123:24 - and create its own application right
123:27 - entire llm application with llm Ops
123:30 - right and you can also build the entire
123:32 - pipeline very simple you don't even
123:33 - require a developer over here now here
123:36 - you have query here you have output
123:38 - output basically means whatever is the
123:40 - final response that you will probably
123:42 - get now in between this there is a plus
123:44 - sign over here if I click on the plus
123:46 - sign I have all these options right so
123:49 - let me zoom in so that you you will be
123:52 - able to see to it okay so over here
123:54 - you'll be able to see once I click on
123:55 - Plus in here you have an option to add
123:58 - an action so first you can add a
124:01 - generate response generate response
124:03 - basically means a response from a
124:05 - specific LM model you can also add data
124:09 - sets let's say you want to implement a
124:11 - document Q&A or a system over here then
124:14 - you can add your own data set along with
124:16 - that you also have an option where you
124:19 - can execute a function like this or
124:22 - smart function now what is this exactly
124:24 - smart function I will go ahead and
124:25 - explain you okay now let me do one thing
124:28 - let me first of all add this search data
124:31 - set let's say that I have some multiple
124:32 - PDF file I want to probably create a
124:34 - document Q&A or it can be a retrieval
124:36 - augmentation generation right that kind
124:38 - of rag system we can we want to create
124:41 - so for this I will go ahead and add a
124:43 - data set over here let's go ahead and
124:45 - create a data set let's say this data
124:47 - set I will say these are nothing but
124:48 - research papers okay so I will go ahead
124:51 - and create this research paper now
124:53 - inside this research paper what I will
124:55 - do I will go ahead and add the data set
124:57 - itself right so over here you'll be
124:59 - seeing you'll be having multiple option
125:01 - you can upload a Plaine text you can
125:03 - upload a file you can upload a p caller
125:05 - you can even uh use Google Drive any
125:08 - file that is available in the Google
125:09 - Drive that also you can add in notion in
125:11 - Confluence so still many more options
125:13 - are probably coming up uh as I said for
125:16 - people who is not much familiar with
125:18 - respect to coding in LM they can
125:19 - specifically use this now let me go
125:21 - ahead let me upload a file now over here
125:23 - I'll click on upload a file and then I
125:26 - will go ahead and upload one file over
125:28 - here let's say this is one of the
125:29 - research paper um uh we'll try to see
125:33 - what this research paper is all about
125:34 - and I'll click on upload okay so this
125:37 - one PDF has got uploaded this is one of
125:39 - the research paper on PFT PFT basically
125:41 - means that Laura and CLA configuration
125:44 - the fine-tuning specifically that we
125:46 - used in llm models right so that
125:48 - research paper we have added so I will
125:49 - go ahead and click on ADD Source okay
125:52 - now as soon as I click on ADD Source
125:54 - you'll be able to see that this PDF file
125:56 - will get uploaded over here let me see
125:58 - that okay let me say that I have more
126:00 - files to be added over here so I will go
126:02 - ahead and click on ADD source and let me
126:03 - go ahead and upload one more file so now
126:06 - uh there is one more research paper that
126:07 - is available over here so I'll write
126:09 - attention attention is all you need okay
126:11 - so I will go ahead and click it over
126:13 - here so this is another research paper
126:16 - that was available uh attention is all
126:18 - you need uh that is something related to
126:20 - Transformers so I will also go ahead and
126:22 - upload this particular file okay so as
126:24 - soon as I upload it you will be seeing
126:26 - that I have added the source so inside
126:29 - this particular data set that is
126:30 - research paper I have this two U two PDF
126:34 - files okay the research papers itself
126:37 - and I can have any number of research
126:38 - papers because people will again say hey
126:40 - Krish can we just only add one or we can
126:42 - add any number you can add any number
126:44 - but the maximum PDF size should be 5 MB
126:47 - okay so this is there I've added it now
126:49 - let me go back to my project now inside
126:52 - my project the rack system that I had
126:53 - actually created right now I just have
126:55 - this two flows right my l in my llm
126:58 - pipeline now I'll go ahead and add
127:00 - something called as searge data set so
127:03 - as soon as I add this searge data set
127:05 - you will be able to see that it'll give
127:07 - me an option to select the data set
127:09 - right whatever I have uploaded so over
127:11 - there you'll be able to see that I have
127:13 - my data set called as research paper so
127:14 - I'll add it over here okay now you can
127:17 - see both this specific file has got
127:20 - added right now once this is getting
127:22 - added there is a in the right corner
127:25 - right you'll be able to see there there
127:26 - is a button in the right corner which is
127:28 - called as save I will go ahead and click
127:30 - on Save okay so once I click on save
127:33 - that basically means this inje the data
127:36 - injection that is specifically required
127:38 - is now available inside the search data
127:40 - set okay so over here you'll be able to
127:43 - see this search data set because I want
127:44 - to implement a RG system or it can be a
127:47 - document Q&A I can probably ask any
127:49 - question it should be able to give me
127:50 - the response so inside this this
127:52 - particular data has got added but
127:55 - internally what the system is doing it
127:56 - is already creating that entire
127:58 - embeddings that is specifically required
128:00 - internally if it is requiring any Vector
128:02 - store it is creating all those things
128:04 - okay now this is done so this in is my
128:08 - data injection step now coming to the
128:10 - next step here let me go ahead and
128:14 - generate a response because here what I
128:16 - will do I'll create an I'll add an llm
128:18 - model okay now any question question
128:21 - that I specifically asked to my data set
128:23 - it should be able to give me the answer
128:25 - and summarize that answer using this llm
128:27 - whatever llm I'm selecting over here so
128:29 - generate a response you basically select
128:31 - over here and you have multiple options
128:33 - of selecting different different models
128:35 - now see it is also providing all these
128:37 - models uh Azure open GPD 3.5 Azure open
128:41 - GPD 4 anthropic Cloud instant anthropic
128:44 - Cloud Google J Pro Beta And in the
128:46 - future so many different different
128:48 - models are coming like Lama 2 and all
128:49 - are also coming okay and the the best
128:51 - thing about this model is that they have
128:53 - they have hosted this particular model
128:54 - in their cloud and they are providing it
128:56 - as a service to
128:57 - you now let's go ahead and select anyone
129:00 - so let's say I will go ahead and select
129:02 - Azure open AI gp4 okay now here you can
129:07 - see it is it is showing the next field
129:10 - as purpose so in short here you'll write
129:12 - your prompt like how you want this
129:14 - particular model to prompt to behave so
129:16 - I'll say you are a helpful assistant
129:22 - who please
129:24 - answer please
129:26 - answer the questions based on the
129:30 - context okay so this is what is my
129:33 - prompt I've written it over here now
129:35 - there is also one more option which is
129:37 - called as Behavior any additional thing
129:39 - that you really need to give on top of
129:41 - that particular prompt you can give it
129:43 - over here let's say over here one
129:44 - example is the behavior of this AI
129:46 - application be as specific as possible
129:49 - be very helpful and assist user
129:50 - something any additional prompt that you
129:52 - really want to write right now I'm not
129:54 - giving anything so let me quickly um or
129:57 - let me just give one okay so that uh
130:00 - I'll say be
130:02 - helpful as much
130:06 - as much as
130:08 - possible okay so this is done so now
130:11 - what I will do I will go ahead and save
130:13 - this okay so on the bottom right corner
130:15 - you'll be able to see a save button so
130:17 - I'm going to save this now this is my
130:19 - entire llm application right right so
130:22 - any query that it will go it'll go and
130:24 - probably search in this particular data
130:26 - set and then it'll give me a response
130:28 - Now quickly to check this whether it is
130:30 - working absolutely fine or not here
130:32 - there is an option of
130:33 - playground okay now in this playground I
130:36 - will go ahead and ask all my questions
130:38 - that I want see two research paper that
130:40 - I had added I will talk about both those
130:42 - research paper one is PFT for uh one
130:46 - research paper was something for this
130:48 - parameter efficient transfer learning
130:50 - and one research paper was attention is
130:51 - all you need so here I will go ahead and
130:54 - ask the question what
130:56 - is parameter efficient transfer learning
130:59 - okay so I will go ahead and click on
131:01 - send now let's see whether we'll be able
131:03 - to see the response or not so in short
131:06 - what is happening now any query that I'm
131:07 - giving first of all it will go and
131:08 - search from this particular data set
131:10 - which is already in the vector embeding
131:11 - format and then whatever information is
131:14 - going to come based on the prom it is
131:15 - going to get summarized by this llm
131:17 - model and finally we going to see the
131:19 - output result and under understand one
131:21 - thing is that based on the context
131:23 - whatever context it is available or
131:26 - whatever context we are able to retrieve
131:28 - from this particular data set that
131:30 - response we will specifically get it
131:32 - okay till we get the response I will
131:34 - just go ahead and search for some more
131:38 - questions over here okay let's see um
131:41 - experiments uh let's say I will go ahead
131:44 - and ask instantiation for the
131:46 - Transformer Network now here you can see
131:48 - I've got the response parameter
131:49 - efficient transer refers to the method
131:51 - used in machine learning and
131:53 - specifically natural language processing
131:54 - that allows for the transfer of
131:56 - knowledge and here you are specifically
131:58 - getting your entire answer okay let me
131:59 - just go ahead and let me talk about what
132:02 - is attention is all you need okay so
132:04 - because that research paper also I have
132:06 - added
132:09 - okay please summarize okay so I've given
132:13 - this and here I will go ahead and click
132:15 - on send now the best thing is that the
132:17 - same functionality I can integrate with
132:20 - any chat BS that I specifically want
132:22 - let's say in telegram channel in
132:24 - WhatsApp channel in any channel as such
132:26 - that option also I'll show now again
132:28 - I've got it so attention is all you need
132:30 - is a semi research paper by Ashish
132:32 - bashwani and colleagues so perfectly we
132:34 - are able to get it okay so any Q&A with
132:37 - respect to the data set you will be able
132:38 - to get it now here you see that you do
132:41 - not have to write much line of code
132:43 - right and it was very simple just by
132:44 - drag and drop adding the feature adding
132:47 - the data set everything you are able to
132:48 - do it okay now let let me go ahead and
132:52 - add one more
132:53 - function let's say that I'm going to add
132:56 - a smart function okay see execute a
132:59 - function basically means here you will
133:01 - get some options to write to probably do
133:03 - basic maths uh Wikipedia search Google
133:06 - search RV like let's say you want to ask
133:09 - queries from this particular um you know
133:12 - RV where all the research paper are
133:14 - uploaded you can probably use this okay
133:17 - so you can specifically use any of this
133:19 - activation function one more if you want
133:22 - to go ahead with is something called as
133:25 - smart function now inside the smart
133:28 - function you can add multiple functions
133:30 - Al together so let's say that I want to
133:33 - do a Google search so I will say over
133:35 - here you can see in the description it
133:37 - allows users to perform searches on the
133:39 - Google let's say I want to also go ahead
133:41 - and use RF if I want to also go ahead
133:43 - and use Wikipedia I can actually use
133:46 - over here so let's say that I'm adding
133:48 - all the smart function over here and for
133:51 - right now what I will do is that from my
133:52 - first flow I will remove this search
133:54 - data set because I don't want to use
133:56 - data set right so let's say uh I'm using
133:58 - in this particular way now let me go
134:01 - ahead and quickly search in this
134:03 - playground so if I go ahead and write
134:05 - what is machine learning okay so if I
134:09 - press enter you'll be able to see that
134:12 - now it is going to do the Google search
134:14 - and it is going to give me the specific
134:16 - answer okay so and for the first time
134:19 - once you set it for the first request it
134:20 - will take take some time but from the
134:22 - upcoming request it will become very
134:23 - smooth so here you can see machine
134:25 - learning is a branch of artificial
134:26 - intelligence and all so let's say I will
134:28 - go ahead and write who is krishak so
134:32 - here if I go ahead and click on send
134:34 - here also you can probably see that if
134:36 - it is using the right API Google search
134:38 - API it should be able to give me the
134:40 - answer uh about myself okay so krishn is
134:44 - a YouTuber and data scientist known for
134:46 - his educational content on machine
134:47 - learning so all the answers are
134:48 - perfectly coming up right now this is
134:51 - what is the most amazing thing about
134:53 - this but again that question will come
134:56 - Krish what is the use of all this things
134:59 - you know you are saying that we can do
135:01 - all these things in a specific platform
135:03 - but how to probably do this in the
135:06 - coding because I want to use this entire
135:08 - functionality Implement in my chatbot
135:10 - okay so let me talk about that okay so
135:13 - if you go ahead and click on the output
135:15 - so here is what is the most important
135:18 - thing here you'll be able to see that
135:20 - you get get the HTTP request and you can
135:23 - also get the entire post request right
135:26 - so once you probably get this post
135:27 - request inside this post request you
135:29 - just need to set the API key and then by
135:32 - using whatever payload like whatever is
135:34 - the question you need to include it over
135:36 - here and then by hitting this particular
135:38 - URL and this channel token you can
135:40 - probably see this should be unique some
135:42 - unique name okay so here I will show you
135:44 - okay I'll give you the code also how you
135:46 - can actually hit the post request with
135:48 - the help of Python programming language
135:50 - now what I will do first of all I
135:53 - require an API key right so if in order
135:55 - to create an API key what I will do I
135:57 - will go ahead and click over here and
135:59 - here you can see I will go ahead and
136:00 - click create an API key so let's say I
136:02 - go ahead and write test uh this is my AI
136:06 - project or the rack system and this will
136:09 - be my API key okay please make sure that
136:11 - you save the API key so I'll copy this
136:13 - API key I'll paste it over here okay so
136:16 - this is my API key now what I will do I
136:19 - will go back to my AI project okay I'll
136:24 - go back to my AI project I will take the
136:25 - code the post request okay now see this
136:29 - this entire pipeline I just require one
136:32 - API and with that I can do a post I can
136:34 - do a get and all the functionality I'll
136:37 - do I don't have to even worry about the
136:38 - vector embeddings I don't have to worry
136:40 - about the apis of search Google search
136:42 - or RC or Wikipedia I don't have to worry
136:44 - about it all I'll be worried about this
136:46 - specific API itself right so what I will
136:48 - do the same call P post request that
136:51 - you'll be seeing I have written this in
136:53 - a normal python code okay so here is the
136:56 - entire python code here you can see uh
136:59 - this is my API key so API key I'll just
137:01 - update it this is my older API key that
137:04 - I had actually tried it out but don't
137:06 - use this API key it will be of no use
137:07 - because I'll be deleting it and then
137:10 - here I will write the query what is
137:12 - machine learning then I'll set the
137:14 - headers of content type of application
137:16 - Json the same thing see over here
137:18 - content type is application Json if API
137:21 - key I need to set it up and then the
137:22 - payload which will have my query okay so
137:25 - I will go over here so API key I have
137:27 - set it to API key whatever API key is
137:29 - there my query is what is machine
137:30 - learning and this is my entire data
137:32 - payload all I did I took that curl post
137:35 - I just searched in the chat GPT and it
137:37 - gave me this entire code okay then this
137:39 - is My URL and instead of that catch
137:41 - token I'm writing Krishna 06 a unique
137:44 - value you can put it anything over here
137:46 - and then I'm doing request. poost with
137:49 - this URL with this Json data and headers
137:51 - is equal to headers okay and I'm
137:53 - printing the response. text again see
137:56 - the code it is very simple I'm setting
137:58 - the API key this is my query this is my
138:00 - headers this is my data I'm just setting
138:03 - it up whatever is the post request and
138:05 - now let me go ahead and execute it now
138:08 - if I go ahead and execute it the best
138:10 - thing will be that see I will write
138:12 - python test. py I've asked the question
138:16 - what is machine learning okay now here
138:19 - you'll be able to see that I will be get
138:20 - getting that entire okay text it shows
138:23 - unauthorized okay let's see what is the
138:26 - error um okay I have not saved this one
138:29 - it's okay no worries I not saved it so
138:32 - that is the reason now I will be able to
138:33 - see the response cannot match app with
138:37 - endpoint perfect so still this is not
138:39 - matching because I had already tried it
138:41 - out previously so let me do one thing
138:44 - let me copy this again and then you'll
138:46 - be able to see it okay so I will copy
138:49 - this entire URL
138:51 - and the channel token will be almost
138:53 - same okay so from catch I'm pasting it
138:57 - over here so I'll remove this I think it
139:00 - is dollar catch uh this catch should get
139:03 - removed and this will be my dollar
139:06 - Channel token perfect now let's see
139:10 - okay now it should
139:13 - run what is machine learning and now I
139:16 - think I should be able to get the answer
139:19 - okay so let's wait for the first time it
139:23 - will take time but after that I think it
139:25 - will work absolutely fine so this is
139:27 - done but at the end of the day you can
139:29 - see that it's a simple python code you
139:32 - know a post python code and how you
139:34 - really want to use it from the front end
139:36 - point of view it is up to you you can
139:38 - specifically use it but this is my end
139:41 - point this this is called as an endpoint
139:43 - this is my API key the API key is
139:45 - basically getting appended over
139:48 - here so now here you can see that
139:52 - I'm getting the response machine
139:53 - learning is a sub field of artificial
139:55 - intelligence that involves the creation
139:56 - of algorithms so and so everything is
139:58 - them now this simple post I can include
140:02 - it in any API that I specifically want
140:04 - okay that is the most amazing thing so
140:07 - coding wise you can now see you don't
140:08 - have much dependency now one more thing
140:11 - that I really want to show there is
140:12 - something called as app directory you
140:13 - can also connect with Google Drive
140:15 - Confluence japar slack teams so all
140:17 - these things are coming soon later on
140:20 - you can also work directly with llms and
140:23 - bring your own llm to the platform here
140:25 - you have a AWS Hemer Bedrock hugging
140:27 - face it is also having this entire
140:29 - support So guys in this video we are
140:31 - going to discuss about how we can f tune
140:33 - our own llm models with our own custom
140:35 - data for this purpose we will be
140:38 - specifically using this platform which
140:39 - is called as
140:40 - gradient. now gradient actually provides
140:43 - you a lot of llm models on top of that
140:44 - you can probably do a lot of fine-tuning
140:47 - with your own custom data set custom
140:49 - data sets can just be formatted in a
140:51 - specific way to give it and later on the
140:53 - fine tuning can really happen in a
140:55 - quicker way so probably when I will be
140:57 - doing the fine tuning it hardly take 7
140:59 - to 8 minutes this is the demo that I
141:00 - really want to show you in this specific
141:02 - video by doing this you will get an idea
141:05 - if you have a custom data set and you
141:07 - want to also create your own custom llm
141:09 - model you can definitely do it with the
141:11 - help of gradient. a now let's understand
141:14 - what does gradient. a provides you it is
141:16 - a one platform unlimited AI it not only
141:19 - does the fine tuning it helps you to
141:20 - develop it deploy it and even for the
141:23 - inferencing part so over here you can
141:25 - create your own private llms your own
141:27 - models fully unlocked model right and
141:29 - you can build with any languages it
141:31 - provides you three SDK I think
141:33 - JavaScript Python and Java so that is
141:35 - also provided we will be specifically
141:37 - using python hdk we will be using Google
141:39 - collab and we will be finetuning our own
141:41 - model so it is the only AI platform that
141:44 - allows you to combine industry expert AI
141:46 - with your private data and here is all
141:48 - the other information that you can see
141:50 - that it also provides accelate AI
141:51 - transformation deploy AI to solve
141:54 - Mission critical problem 10 into faster
141:56 - with gradients AI Cloud platform we
141:58 - handle the infrastructure for you so it
142:00 - is an amazing platform alog together
142:02 - where you can not only uh build your
142:04 - model even you can inference it you can
142:06 - even find un it so in order to go ahead
142:08 - with first of all you need to go ahead
142:10 - and sign up so for sign up you can
142:12 - probably put your entire details I have
142:14 - already sign out uh signed up so I'm
142:16 - going to probably do the login over here
142:18 - as soon as I do this login it it will
142:20 - ask you to create a new workspace so
142:22 - this is the first important step that
142:24 - you have to do so for creating a new
142:26 - workspace you just need to create like
142:28 - just click on this particular button
142:30 - create a new worksspace you can give
142:31 - your own workspace name and just submit
142:33 - it once you submit it like in this case
142:36 - I have created a test workspace you need
142:38 - to know this specific ID because this ID
142:41 - is nothing but it is a workspace ID so I
142:43 - will copy this workspace ID so that I
142:46 - will be using it in my coding when I
142:47 - probably do it in the Google collab the
142:49 - next thing that is specifically required
142:51 - you need to click on the access tokens
142:53 - right one is the workspace ID and the
142:55 - other one is the access token again to
142:57 - generate the access token I have already
142:59 - generated it I have kept it uh in my
143:01 - coding environment all you need to do is
143:03 - that just click on generate new exess
143:05 - token and you have to just put your
143:06 - password submit it and it will give you
143:08 - the exess token there you just need to
143:10 - do the copy over there so what I will do
143:12 - I will go ahead and uh uh give my
143:14 - password as soon as I do this so here
143:16 - you can probably see that you get your
143:18 - secret access token you can copy it over
143:20 - here one amazing thing about gradient AI
143:22 - is that it provides you all the
143:24 - necessary documentation over here you
143:25 - can just go and refer the documentation
143:28 - it provides you the entire code that is
143:29 - actually required with respect to python
143:31 - SDK and trust me guys just with a
143:33 - minimal lines of code you can probably
143:35 - find tun your entire model in an easier
143:37 - way right so if you probably go to the
143:39 - guides here you have something called as
143:41 - python SDK example so with respect to
143:44 - python hdk you can probably see this is
143:46 - the code for fine tuning I will copy the
143:48 - same code and I'll probably run it but
143:50 - here I'm going to make some changes with
143:51 - respect to my data set along with that
143:53 - you can also have you can also go ahead
143:56 - and see the entire documentation
143:57 - whatever things you specifically require
143:59 - the integration it specifically provides
144:01 - for Lang chain Lama index mongodb H tack
144:05 - so whatever things is specifically
144:07 - required mongodb and all you can
144:08 - probably do the vector dat you can
144:10 - create it as a vector database and
144:12 - probably store all the vectors over
144:13 - there now let me quickly log in again so
144:16 - once I log in if you probably go to the
144:18 - workspace okay so this is the work work
144:20 - space that I actually created okay now
144:23 - inside this work workspace you can see
144:25 - all this are the models that I've have
144:27 - already created for the F tuning purpose
144:29 - if you probably go ahead and click on
144:31 - fine tuning and if you see if I just go
144:34 - ahead and select uh or create fine
144:37 - tuning over here it provides you three
144:39 - base models okay one is the bloom 560
144:42 - one is Lama 27b chat one is NOS hermas 2
144:46 - okay so these are different different
144:47 - models uh llm models you can use any one
144:49 - of them for the fine tuning purpose now
144:52 - in our example what I'm actually going
144:53 - to do is that I'm going to use this
144:55 - specific thing and then what will happen
144:57 - is that with the help of python hdk I'm
144:59 - going to do the entire fine tuning
145:01 - process here then I will be able to see
145:03 - my fine tuning job also as I go ahead
145:05 - okay so here I have opened my Google
145:08 - collab let's go ahead step by step and
145:11 - let's see that how you can probably
145:12 - fine-tune your llm model so let me first
145:14 - of all make this spelling mistake
145:16 - correctly okay now the first step is
145:19 - that we will go ahead and install
145:20 - install gradient AI so for this I will
145:23 - be using pip install gradient AI upgrade
145:25 - so once I execute this over here you'll
145:27 - be able to see that the installation
145:28 - will start taking place now it shows
145:30 - that requirement is already
145:32 - satisfied now the next thing is that the
145:35 - two important information as I said one
145:37 - is the gradient workspace ID I've
145:39 - already shown you from where you can
145:41 - probably get your workspace ID right uh
145:44 - initially in the workspace when we are
145:46 - probably checking it out that particular
145:48 - ID I copied it over here and pasted it
145:50 - over here and the next ID that I really
145:53 - need to set it is my access token
145:55 - gradient access token so I will set up
145:57 - the environment by using import OS and
146:00 - then I'm going to probably use os.
146:02 - environment gradient workspace ID with
146:05 - my workspace ID itself so this is the ID
146:07 - which you will again get it from the
146:08 - gradient itself once you log in and the
146:11 - second thing is about the gradient
146:12 - access token so here also I have pasted
146:14 - the exess token so let me just go ahead
146:16 - and execute it and I have set up this
146:17 - environment variable one is the gradient
146:19 - workspace ID and gradient access token
146:21 - now is the most important thing this is
146:23 - the code of the entire fine tuning so
146:26 - let me go ahead step by step and see
146:29 - like how the fine tuning is basically
146:31 - done so first of all we will go ahead
146:33 - and import from gradient AI import
146:35 - gradient right so this gradient will be
146:37 - responsible in fine tuning in even
146:39 - calling the base model now I've created
146:41 - a function definition main now this is
146:44 - the function initially I have
146:45 - initialized gradient okay inside this
146:48 - gradient you have a method which is
146:50 - called as get base model and here the
146:53 - first parameter is something called as
146:56 - base model slug that basically means
146:58 - what model you are specifically calling
147:01 - and I have already shown you in the fine
147:02 - tuning right if you probably go ahead
147:04 - and create fine tuning through the UI
147:06 - you have this three models that is there
147:08 - Bloom 560 Lama 2 7B chat No hmus 2 so in
147:13 - this case I'm going to specifically use
147:15 - NOS harmas 2 okay now NOS harmas 2 I'm
147:17 - going to do it see you can also do it
147:19 - with the help of UI but I really want to
147:21 - show you with the help of coding by
147:22 - using python SDK so here is the same
147:25 - model I have pasted it over here right
147:27 - so this specific model over here is my
147:30 - llm model that I'm going to f on top of
147:32 - it I'm going to fine tune it with my own
147:34 - custom data now once we created this
147:37 - base model so this line basically gets
147:39 - you the base model over here now this
147:41 - model will be responsible and we will
147:44 - try to create a model adapter right the
147:47 - reason we are creating a model adapter
147:49 - because because this will be
147:51 - specifically my model let's say this is
147:53 - like Kish model over here and this model
147:56 - the reason we are creating as a model
147:58 - adapter because in further we will be
148:00 - doing fine tuning for this particular
148:01 - model itself so the first step is
148:03 - basically to create an adapter on front
148:05 - of it right and the name of that
148:07 - particular model will be uh Kish model
148:09 - itself so this is the second line of the
148:11 - code that we have executed now here we
148:13 - have created the model adapter with the
148:15 - ID new model adapter. ID so we will be
148:17 - getting the ID itself now is the most
148:20 - important step I will try to go ahead
148:22 - and create a query so this is my sample
148:25 - query the instruction see I've given
148:27 - some kind of format the instruction is
148:30 - who is Krishna so this is the question
148:32 - and usually what happens is that in
148:34 - gradient you really need to use a
148:36 - specific format in creating your custom
148:39 - data and for that you can refer this
148:41 - python SDK here you have some tips and
148:43 - tricks it will talk about like how you
148:46 - can probably create your entire uh data
148:48 - itself right with custom data so you can
148:50 - check it out this specific thing and
148:52 - with respect to that you'll get an idea
148:54 - like how your data set should be created
148:56 - in a form right so some format should be
148:58 - there so let's say if I ask any query
149:01 - right for a base model something like
149:03 - this I've given this three hash uh uh
149:06 - and then the instruction this is the
149:08 - question who is Krishna and then we are
149:11 - closing it with two end line okay and
149:13 - then we will be seeing the response so
149:15 - response over here is kept as blank
149:17 - because this response is going to come
149:18 - from this specific new model adapter
149:21 - okay which has the base model NOS Hermos
149:24 - okay now we will be printing this sample
149:27 - query so already it will be just going
149:29 - and printing this who with all all this
149:31 - information like instruction and
149:33 - response now this model Raptor when I
149:36 - say dot complete okay so do complete is
149:39 - a method over here the first parameter
149:41 - I'll be giving my query that is my
149:43 - sample query the max generated token I
149:46 - will be giving 100 so it is going to
149:48 - probably um take 100 Max token in the
149:50 - form of output and then we are going to
149:53 - use do generated output so this is one
149:55 - of the attribute that is present inside
149:58 - this R complete method okay so after
150:00 - this complete method we will be getting
150:01 - the generated output as soon as we get
150:04 - the generated output that basically
150:06 - means whatever questions I have asked
150:08 - over here who is Krishna I'm going to
150:10 - get the output over here itself right so
150:12 - generated before F tuning now here is my
150:16 - output that I'll be getting and
150:18 - understand I have not started my fine
150:19 - tuning so that is the reason I've
150:20 - written a comment over here before fine
150:23 - tuning now before fine tuning if I ask
150:27 - who is Krishna obviously this model will
150:28 - not know unless and until I'm very much
150:31 - popular okay I'm not that much popular
150:33 - so it'll just say that if if it has
150:36 - heard somewhere Krishna it'll give some
150:37 - information now what I will do I will go
150:40 - ahead and create my own sample data okay
150:42 - regarding Krishna because obviously over
150:45 - here in before fine tuning if I probably
150:47 - execute it I will not be able to get the
150:49 - output the right output now this will
150:51 - basically be my custom data so here you
150:53 - can see that the first question that
150:54 - I've asked the first uh sample that
150:57 - first input data that I have created
150:59 - which will be used for training is that
151:01 - who is
151:02 - Krishna then over here the response I've
151:06 - written as krishn is a popular mentor
151:08 - and YouTuber who uploads video on data
151:10 - science and uh llm and his channel
151:13 - channel and in in his channel krishn so
151:16 - this is one of the input and the output
151:18 - that I have given the similar second
151:20 - input I will again create who is this
151:22 - person named Krishna now I'm giving in a
151:24 - different format of text like question
151:26 - and answer right and again the response
151:28 - have written Krishna likes data science
151:30 - and AI make videos in YouTube and he's
151:32 - also a mentor right something like this
151:35 - then instruction again what do you know
151:36 - about Krishna okay so again I've created
151:38 - one more response Krishna is a popular
151:40 - Creator who specializes in the field of
151:42 - data science and his channel name is
151:43 - krishn okay and again one more input
151:46 - like this I can create lot of inputs
151:48 - related to a single Contex now suppose
151:50 - if I have 100 of context I may probably
151:52 - create this kind of inputs and at the
151:53 - end of the day if in a real world
151:55 - scenario in an end to end project if you
151:56 - see this all information will be coming
151:58 - from some kind of databases it can be a
152:01 - mongodb database it can be a SQL
152:03 - database and all right so this will be
152:04 - my sample of data it's just like more
152:07 - like a key value pair you know a list of
152:10 - key value pairs now this is done now
152:12 - what I'm going to Define after this my
152:14 - input is done my model before fine
152:16 - tuning is done now I need to do the fine
152:18 - tuning so for performing the fine tuning
152:20 - I need to have number of epo one
152:22 - parameter I will initialize count is
152:25 - equal to Z now I'm saying while count is
152:28 - less than number of EPO and then I will
152:31 - start my fine tuning over here so fine
152:33 - tuning count + one then I will say new
152:36 - model adapter. finetune is one method
152:39 - where I am giving my samples okay so
152:42 - samples over here this all samples I'll
152:44 - be giving it over here for the fine
152:45 - tuning
152:46 - purpose so in short I'm using a
152:50 - method which is called as DOT fine tune
152:52 - and we are specifically giving this pamp
152:54 - over here and then we are incrementing
152:56 - the count so that the EPO keeps on
152:58 - increasing so first iteration one
153:00 - iteration 2 iteration three and then
153:03 - this entire fine tuning will happen okay
153:06 - now after this I am again writing this
153:08 - comment after fine tuning and I will
153:11 - execute the same method right over here
153:14 - by giving the same sample query who is
153:16 - Krishna right and over here the same
153:18 - thing is then and get the generator
153:19 - output I am going to display the
153:21 - generated output and after doing this I
153:23 - am deleting my adapter because I will
153:25 - not require it afterwards uh if I wanted
153:29 - to use it further I can keep it like
153:31 - that otherwise I can just go ahead and
153:33 - delete it and here also I'm closing the
153:35 - gradient and this is where I'm starting
153:37 - my main so this is in short what we are
153:39 - doing is that before F tuning I really
153:41 - want to see the response I created my
153:43 - sample data over here and then again I
153:46 - did the fine tuning with respect to
153:47 - number of AO and all and now let's go
153:49 - ahead and execute it so once I probably
153:52 - start executing it you can probably see
153:55 - uh it will it will start uh you know it
153:58 - may take hardly a couple of minutes
154:00 - since I've just done three iteration
154:02 - okay so here created model adapter with
154:04 - this particular ID uh the instruction is
154:07 - who is Krishna so it is waiting for the
154:10 - response let's see over here whether it
154:12 - has been created or not uh model testing
154:16 - let's see finetuning so here also it
154:18 - will as soon as the fine tuning will
154:20 - happen here the model will get created
154:22 - and it will got deleted okay it will get
154:24 - deleted so here the response it it is
154:26 - getting that Krishna is a well-known
154:28 - Indian actor who has prepared in various
154:30 - films uh and television shows he's best
154:32 - known for his uh Raj in the popular I
154:35 - have I've never worked in any industry
154:37 - guys okay so it is saying I'm an Indian
154:39 - actor I'm not an actor
154:42 - altogether okay so this is a very famous
154:44 - serial in India okay I have never acted
154:47 - over there I don't know whom it is
154:49 - considered
154:50 - okay that's also said that I've worked
154:54 - inch H nice so now I am doing the fine
154:58 - tuning so this is the response that I've
155:00 - got generated before fine tuning and now
155:03 - after fine tuning I have given the right
155:05 - kind of data with respect to input and
155:07 - the response uh the third iteration is
155:10 - getting overseas how fast it is
155:12 - happening in the cloud platform gradient
155:14 - Cloud platform that is quite amazing
155:15 - right and this is super super nice at
155:18 - the end of the day you'll be able to see
155:20 - this right beautiful right we already in
155:23 - the third iteration and now here I'm
155:25 - getting my output generated after fine
155:28 - tuning Krish is a popular YouTuber and
155:30 - data scientist is known for his data
155:31 - science python tutorials on his YouTube
155:32 - channel Krish n now see just with this
155:35 - three sentence or four sentence how well
155:38 - it has done the fine tuning now just
155:40 - imagine the power guys right the entire
155:42 - fine tuning is happening in the gradient
155:44 - Cloud itself right if I probably give
155:46 - this kind of data and just increase the
155:48 - number of EPO and train it how
155:50 - beautifully within 5 to 7 minutes you
155:52 - can actually train it and hardly you
155:55 - know just to put my input data it hardly
155:57 - took very less time right so this is one
155:59 - amazing application this is how you can
156:01 - probably do the fine tuning one task I
156:04 - really want to give it to you I okay
156:06 - let's let's reload this I think you'll
156:08 - be able to see my fine tuned model also
156:11 - uh let's see the model uh okay I've
156:14 - already deleted it I
156:16 - think yeah I've deleted it so that is
156:19 - the reason you're not able to see it
156:20 - over here if I had not deleted that you
156:22 - could also see the models over here
156:24 - right so model testing see model is also
156:27 - not here right otherwise you could also
156:29 - see it uh put it over here and you can
156:31 - ask any question you want right this is
156:33 - quite amazing so at the end of the day
156:35 - if you don't want to delete it just
156:36 - don't delete it and keep it like this so
156:39 - I hope you like this particular video I
156:41 - hope you have understood how you can
156:42 - fine tune our amazing llm models uh in
156:46 - with the help of gradient AI Cloud uh
156:48 - which is quite amazing you should
156:50 - definitely use it try to use it and try
156:51 - to see whether you're able to do this or
156:54 - not but uh just by seeing the code and
156:56 - all I think it is quite easy quite
156:58 - amazing again yes this was it for my
157:00 - side I hope you like this particular
157:01 - video I'll see you all in the next video
157:02 - have a great day thank you on all take
157:04 - care bye-bye

Cleaned transcript:

learn all about finetuning llm models in this course Chris will teach you finetuning using Cura and Laura as well as quantization using llama 2 gradient and the Google Gemma model this crash course includes both theoretical and practical instruction to help you understand how to perform finetuning so guys uh here is an amazing crash course to help you understand that how actually you can perform fine tuning uh specifically with respect to llm models uh and again uh uh in this crash course we will be discussing the theoretical intuition what exactly fine tuning is how you can actually perform fine tuning we'll be learning about Concepts such as quantization Laura CLA PFT right and along with that we will be implementing multiple projects with the help of hugging face open source uh open source llm models like Lama 2 along with that we'll also be seeing how you can perform fine tuning with your own custom data set with the help of Google Gamma model now uh in most of the companies that are probably looking for ative AI roles uh usually want this specific skill set of finetuning and uh you should definitely know how you can actually fine tune with your own custom data set and how you can solve different different use cases so this entire crash course will be having a stepbystep process of understanding it includes both theoretical intuition practical intuition and we will be also developing completely end toend Solutions with respect to this so I hope uh you will enjoy this series please make sure that you watch this series till the end and definitely you will be able to learn a lot of things out of it so yes best of luck let's go ahead and watch this series in one of our previous video i' had already shown you how you can actually fine tune Lama 2 model with the with your own custom data set and uh over there we learned about or we saw code that were related to something called as quantization Laura CLA techniques and all right and all these techniques are super important if you also want to train or fine tune your own llm models with your own custom data set now when I showed the code right when I executed that particular code many people had actually requested about to explain the theoretical indepth intuition about it and that is what I'm actually going to do uh the best thing is that when I learned about this theoretical intuition and I'm doing it from past 2 to 3 months it's quite amazing guys you now this is where that machine learning era is probably coming where I used to upload a lot of theoretical indepth geometrical intuitions regarding various machine learning algorithms similarly here also in the series of videos in this video we are going to discuss about quantization now what exactly is quantization we going to discuss about that in the upcoming video we are going to see techniques like Laura CLA every maths intuition that is probably involved uh and these all are important for the finetuning technique right if I probably talk about generative AI one of the most important interview questions will be something related to fine tuning and what is the techniques that is usually used behind it right so what all things we are going to cover in this video in this video we going to talk about quantization um specifically when I say quantization it is all about model quantization because if you remember in our Lama 2 code right when we doing the fine tuning here you could see that we had put some parameters right regarding Precision base modeling we had spoken about quanis you know when what when we are downloading the models you know from a higher bit to a lower bit why we are specifically doing this I will be explaining about that right so with respect to each and every parameters definitely I will explain you the theoretical intuition and later on you just go ahead and see my previous video with respect to all the coding now you everything will make sense okay so what exactly is quation we're going to discuss you know uh we're going to discuss about full Precision half precision and this is something related to data types like how the data is stored in the memory when I specifically say data in llm models I will talk about weights and parameters right because at the end of the day llms are also deep learning neural networks in the form of Transformers or Bird right then we going to discuss about what exactly is calibration uh this is also called as like calibration in model quantization right we are going to also make sure that we are going to see some problems right how we can actually do calibration then there is different different modes of quation right uh first of all I will explain you the definition then only you'll be able to understand in modes of quation we're going to discuss about two types one is post trining Quant and Quant aware training right so these all are very important in terms of fin tuning techniques now let's go ahead and talk about quantization and we will try to see the definition right quantization okay now if you want to really understand the meaning of quation it is better to write a simple definition for it okay so quation basically means conversion from higher memory format to a lower memory format right now now I've written a very generic definition what exactly quantization mean it is nothing but conversion from a higher memory format to a lower memory format now when I say higher memory format let's let's consider um any data right and if I probably consider any neural network okay so let's say if I have neural network right and when we train this neural network right all this neural network are interconnected at the end of the day what are the parameters that is probably involved over here it is nothing but we weights right we specifically have weights now weights are usually in the form of metrics right let's say that I have a 3 cross3 weight okay I'm just taking as an example in one of the layer I have 3 cross three weights and over here every value right is probably stored in the memory in the form of 32 bits right 32 bits we also say this bits as we also we also denote it as something like something like fp32 what exactly is fp32 so fp32 basically means I can also consider it as see FP full form is not floating Point okay but we are I'm just writing floating Point 32 bits right when I say FP FP basically means it is nothing but full Precision right full Precision or single Precision okay so this is the definition that is probably given right but in short this is like a floating Point number okay so over here let's say my number is somewhere around 7.23 now this number is stored based on 32 bits in the memory right now understand when you have a very big neural network or you have llm models right as you see different different llm models right parameters keeps on increasing right some may have 70 billion parameters if I probably consider llama 2 with 70 billion parameters that basically means it has 70 billion parameters in terms of weights and bias okay now it is not possible for me to let's say I want to use this particular model and I want to probably do a some fine tuning with respect to the normal GPU that I have right and let's say I have a very limited Ram in my system let's say the ram I have is somewhere around 32 GB right I cannot directly download the specific model and put it in my me in my Ram itself right let's say or load it in my vram that is available in the GPU because GPU also has some limited Ram right and it is not possible you cannot directly download it obviously it will require h space the other way is that yes I can probably take a cloud space somewhere let's say in AWS I can create my instance I can say hey give me this this much RAM 64 GB RAM and I probably want this much GPU and then I will try to load the model over there that you can do it but over there what is basically happening lot of cost is involved right based on the resources the cost is involved right so in this scenario and why the 70 billion parameter is happening why this model has a 7 parameter because every weights or every bias that is available in this you know this may be getting stored in 32 bits so what we can specifically do is that we can con convert this 32 bits into as I said over see conversion from a higher memory format to a lower memory format let's say I I can probably convert this 32 bit into int 8 and then download the model or then use the model right after doing this what will happen within my system I will be able to inference it right obviously for fine tuning if I want to F tune with the new data set I will obviously require GPU but if I consider with respect to inferencing it becomes quite easy because now all my values that are stored in the form of 32 bits it will be stored in the form of 8 Bits so what we are specifically doing over here we are converting from a high memory format to a low memory format and this is what is called as Quant quantisation a very important thing now why quantisation is important because you will be able to inference it quickly see inferencing basically means what if I have an llm model if I give any input to that I should be able to get any output right I should be able to get response right now when I give any input all the calculation with respect to different different weights will happen right and obviously if I have a bigger GPU this inferencing will quickly happen right but if I have a GPU with less less scores let's say then what will happen this calculation will take time but if I convert my 32 bit to 8 Bits right every weights are basically converted into Ed bits now just imagine the calculation will there be a difference yes it will happen little bit much more quicker so quantization is very much important for inferencing and some of the example that I can probably talk about is that and obviously you may have heard about this it's not like only in llm model we specifically do right in different computer vision models in NLP models also where you think that there is a lot of Weights that is involved and all these weights if I I want to quantize it right I can actually do it right now this inferencing let's say I want to use a specific deep learning model in my mobile phone so in my mobile phone if I want to use it right in in a specific app then what I will do I will try to quantize right whatever deep learning model I have created from 32 bits to 8 bit and then I will try to deploy it in my mobile phone or any Edge device any Edge device right it is not not possible that you can probably deploy this big model over there right it is not possible so and so many parameters so what we do we basically perform quantisation so I hope you're able to understand over here quantisation is nothing but we are trying to lower down the memory right with respect to any weights that we have like from 32 to probably 6 uh inate or let's say fp16 we can also say fp16 right let's say if I have an FP p32 bit that is specifically required to store any information in my memory I can also convert this into FP 16bit this is also quation only right usually all these values are stored in floating point right we specifically say this fp32 bit we say single Precision or full Precision we uh for FP 16 bit if I'm trying to convert like this it is basically called as half Precision right so you should be able to understand all these technical terms right and in short these are nothing but these are floating Point numbers right now similarly in tensor flow also you'll be able to see when we probably work with tensor flow you'll find tf32 bit right the data types the the numbers are stored in this particular format right and it is important okay this all are terminologies that are super important but I hope you got an idea what what is the main aim what is the main motivation out of quation is that if I have a bigger model I should be able to quantise it and make it as a smaller model so that I can use it for my faster inferencing purpose both in Mobile phones in Edge devices let's say in even watches smart watches I want to use it over here I can actually do that right now if I talk with respect to llm model also with the help of quation see once we compress this particular model right later on we can also perform find tuning right fine tuning but here there is one disadvantage when we quantize right when we perform this quation since we are converting from 32 bits to intake let's say as an example there is some loss of information also and because of this there will be some loss of accuracy now how to overcome this we will talk about it there are different different techniques how we can specifically overcome it but I hope you got an example what EXA quation what is full Precision half Precision half Precision example is something like this now let's talk about what exactly is calibration now calibration basically means how we will be able to convert this 32bit into int 8 like what is the formula what is the mathematical intuition that is specifically required let's go ahead and discuss that so guys now let's go ahead and try to understand how to perform quantization and this is super important in terms of mathematical concept that I'm probably going to talk about because with with the help of tensor FL just by writing four lines of code you know I will be able to perform quation but it is important you should know that how you can actually do it manually whenever I talk in terms of what are the types of quation that we have so we have two different types of conation one is symmetric contag and one is called as asymmetric contag now just by showing you an example you will be able to understand what is the exact difference between them okay let's say I have a task and this first task that I am probably going to talk is with respect to symmetric and understand I hope in deep learning you have heard of something called as batch normalization so if you have heard about this batch normalization so batch normalization is a technique of symmetric quantisation right so every time you'll be able to see that whenever we do forward propag and backward propagation in between all the layers we apply batch normalization so that all our all our weights are zero centered that is near the zero and the entire distribution of the weights will be centered near zero okay so batch normalization is one technique uh of symmetric quantization so let's go ahead and see one example so this will be my first example over here I will go ahead and write it down and now you'll be able to understand it how symmetric quantisation is basically performed what is quation you have understood from higher memory format to lower memory format we'll try to convert okay so here we are going to understand the mathematical intution so let's go ahead and talk about one technique which is called as symmetric unsigned unsigned in it unside in8 okay quantization so we will see this technique first now here what is our main aim let's say I have a floating Point number number okay so let's go ahead and write it down let's say I have a floating Point number between 0 to th000 now just imagine that these are my weights right whatever Matrix Ms I have my values ranges between 0 to th000 Let's consider in this particular way right and this is let's say these are the weights for my larger model OKAY larger model okay now one very important thing that you need to understand right when I talk about any larger model let's consider any llm model okay so in llm model you have lot of parameters let's consider this is one kind of llm model now when I say all these weights are there this may be getting stored in 32 bits okay usually uh what will happen guys the weights will not be in this range also okay it will be in the minimalistic range so I will just consider this as some numbers okay so that you don't get confused with respect to these are some numbers I will also not consider llm model over here and let's say this this this numbers are stored in the form of 32 bits okay now my main aim is to convert this into unsigned in 8 that basically means 8bit right so 8bit basically means what 2 ra to 8 is how much 2 to 8 but when we say U unsigned that basically means my value will be ranging between 0 to 255 so I want to quantise from this values to this value okay what is my aim I want to quantize my range of values between 0 to th000 to 0 to 255 okay this is what in my aim with respect to this okay so this is what is my target now let's see okay so if I probably draw just a real points and the same thing we will do with the weights over there whatever quantization process we are specifically doing let's say I have values between zero and this is basically th000 okay then what I will do over here I want to convert this into again 0 to 255 okay now let me talk about one very important thing guys whenever we have any let's let's consider this one if we have this single Precision single Precision floating Point 30 2 right if you have this number how this number is stored do you know that the one bit will specifically be used for sign or unsign values right let's say positive or negative if positive is there then this will be plus one if negative is there it will be minus one so all the values are basically saved between 0 to 1 right uh it can be Z or 1 okay then the next eight bits are stored for exponent okay 1 2 3 4 5 6 7 8 okay so this is for sign these all numbers that you see it is basically stored for exponent this is how it is stored inside the memory and remaining 23 numbers all these 23 bits will basically be saved for manesa this is specifically for the fraction so if I have a number which looks like 7.32 now 7.32 is a number it is a positive number so for my sign bit there will be a positive value let's say one over here then this seven will be probably put up in this 8 Bit And remaining 32 will be put up in this mantisa right so this is how the numbers are basically stored in the memory right if I consider an example with respect to to FP 16 right half Precision floating Point 16 bit then you'll be able to see there will be one bit for the sign number there'll be five bits for the exponent we basically say exponent 1 2 3 4 5 let's consider this five let's let's draw till here okay so this let's let's let's say that this is five 1 2 3 4 5 Okay so this will be five and remaining remaining 10 bits will be saved with respect to mantisa mantisa so we basically say this as a fraction right anything that comes after the decimal right and this is how you will be able to see it will take this will take less memory this will take High memory right now what is our main aim over here I already have a 32bit number I need to probably convert this into a range of unsigned intake unsigned in end basically means I will not take any negative numbers it will be between 0 to 255 right this is what I really want to do okay now for this what will be the equation and I hope you have heard about something called as minmax scalar I've I've repeated so many times this in my machine learning sections also so any number that is over here how will I be able to convert from this floating point to this unit8 unit inate right unsigned intent right how I will be able to do it now for this let's go ahead and calculate it and what equation is specifically required okay so that basically means over here what we are going to do 0.0 will be converted into a quantied value of Zer okay not 0 Z it will be zero and similarly thousand should be converted to a quantize value of 255 at the end of the day the bits are decreasing so quantization is basically happening but we have to probably come up with a scale factor now what exactly is a scale factor so let me Define a scale over here so here the scale formula will be x max / x mean and then they will be divided by Q Max minus Q mean this Q is nothing but quantization now what is the x max over here 1,000 right so this I will consider as my X this I will consider as my Q right I'm showing you how quation happens in a symmetric distribution symmetric basically means all the data is evenly distributed okay and we really need to convert this based on this itself these are evenly distributed now what is xmax xmax is nothing but 1,000 minus 0 then qmax qmax basically means 255 0 right so if I probably go ahead with this specific division right then what will be the value that I will be having right 1,000 divid by 255 it is nothing but 3.92 so this is nothing but this is called as a scale factor right scale factor so any number that I have over here if I want to convert from this fp32 bit to U in 8 I just need to use the scale along with one formula which is called as round okay so I will apply round with respect to any number let's say if I consider 500 / 3.92 or let's let's just consider 250 divide by 3.92 so if I want to see what will happen to 250 right what will be the number in this U inate so I can probably go ahead and divide it it'll be nothing but 250 ID 3.92 so if I go and calculate it it is nothing but 63.7 7 so if I do the rounding that basically means this will be 64 so in short any number that is over here let's say it is 250 over here this will get converted to a quanti value to 64 right so the same thing the code will also be doing and this is for symmetric unsigned inate okay quation if I want a quation some for some other Factor let's let's talk about this okay so let's say um I have another kind of distribution and this is time that it is asymmetric and I want this as u in 8 so if I want to perform this contration so what will happen now in this particular case let's say if I have a values between 20.0 to 1,000 okay so these are my floating point now I want to perform contag and convert this into 0 to 255 right now in case of asymmetric what will happen is that in my real number section right this numbers are not symmetrically distributed it may be right skewed it may be left skewed okay so in this scenario you'll be able to see that my values are ranging between 20 to 1,000 I want to convert this into this now in this scenario if I apply the same formula x max minus x mean so how it will be 1,000 20 so minus of 20 is nothing plus + 20 divid by 255 so if I probably do the calculation then you will be able to see that I will be getting somewhere around 4.0 okay now very important thing that basically means this 20.0 if I quantize it right if I quantize it it will get converted to something like 4.0 oh sorry this this is the my scale factor okay scale factor now if I take any number and try to convert it let's consider minus 20 if I try to convert it by dividing by 4.0 right and if I do the round so what it will become 5 right 5 of round this much I will probably get 5 right now you can understand that this 20.0 is getting converted to 5.0 but you can see over here my distribution starts from 0 to 255 so how can I forcefully make this 20.0 to zero all you have to do is that go ahead and add the same number in a positive way so in this case the number that you see this 5 right this is basically called as 0 point right so there are two important parameters that we specifically talk with respect to quantization one is 0o point for for the above one since we have a symmetrical distribution here the 0 point was Zero only and the scale was 3.92 in this particular case since it asymmetrical distribution here we have a z 0 point as nothing but five but scale is 4.0 so this two parameters we usually require to perform quantization okay and these are some of the examples that I have shown you to just give you an idea like how quantization basically happens and super important in terms of understanding is the simple equations you'll be able to understand how things are basically working right at the end of the day understand quantization is a simple process of converting that high uh full single Precision of full Precision floating Point 32 bits into small bits you know it can be uh unsigned integer 8 it can be signed integer eight if we say signed integer eight then what will happen it is that it will be ranging between 128 to 127 and based on that you can specifically apply the formula right now let's go ahead see we had already discussed about these two topics one is this and second one we wanted to discuss about calibration now this squeezing that you could see right from here to here to here here to here we SK squeezing it right this squeezing process is basically called as calibration whatever process we are basically applying in this quantization process it is nothing but it is called as calibration because we are squeezing those values from a higher format to lower format okay so that is nothing but calibration so we have completed both this thing okay now let's see what are the different modes of quantisation one is called as post training quantisation and quantisation aware training right I will talk about this why it is super important both this technique okay so you'll get an idea about it over here so first of all we will go ahead and say post training quantization so what exactly is posttraining conation here we already have a pretrained model so we already have a pretrained model now if I want to use this pretrained model obviously the weights are very high here we apply calibration right when I say calibration that basically means squeezing the value from high form format to a lower format right and then after performing this particular calibration we take what kind of data we take that weights data whatever weight data is basically there in this particular pin model and then we convert this into a quantized model okay so once we apply this process then only we'll be able to get the quantise model and then we can use this entire model for any use cases okay for any use cases right this is a simple mechanism with respect to post training quantization see understand post trining basically means I already have a pretrain model where my weights are fixed I don't need to change those weights I will just take or download those weights I will take this weights data apply the calibration and then convert this into a quantise model right the second technique that we have written over is quantisation aware training okay quantization aware training so let's talk about this quantization aware training this is also called as CU so this is also called as Q8 okay so we can write it as Q8 okay quantization aware technique this is basically called as ptq ptq Okay so over here what is the exact difference we'll try to see between this two okay now in quantisation aare training what happens see over here what is the problem if I probably perform calibration and if I create a quanti model there is a loss of data and because of this what will happen is that the accuracy will also decrease okay for any use cases but in the case of quantisation aware training okay you'll be able to see that we will be taking our train model whatever train model is there train model is there okay let me just go ahead and write it down train model is there then we perform quantisation again quation is what same the calibration process will apply over there we will probably do all these things okay and then once we do this the next step is that we will will go ahead and perform finetuning see we know that we know that with the help of ptq you'll be seeing over here on the top some loss of data and accuracy is there but here with respect to finetuning we will take new training data new training data now once we specifically take new training data we will be finetuning this model and then we create a quantied model quantied model so with respect to any finetuning technique that you will be seeing we don't use post trining quantisation we specifically use quantization aare training technique so that basically means we are just not losing accuracy or data over here because we are in turn adding more data for the training purpose and through this we will be finetuning our data and then we create our quantized models right so this is the basic difference so all the fine tuning technique that I will probably show you in the future will be of this type that is quantization aware training so that we do not lose much data accuracy so I hope you got an idea with respect to all these three techniques guys going ahead there are two important techniques that we really need to understand one is clora and one is Laura so this techniques specifically will be also understanding with respect to to fine tuning uh we are into the part two of fine tuning series and in this particular video we're going to discuss about Laura and CLA indepth intuition already in the playlist of fine tuning I've discussed about quation and I hope you have seen the video early many people were requesting for Laura and CLA also so let's uh discuss and uh we will try to discuss the complete indepth maths intuition uh the best thing will be that guys I'll try to explain this math indepth intuition uh I've already seen the research paper and there are lot of complicated things that is probably there in the research paper but I will try to teach you in such a way that at least you should be able to understand you know what exactly is Laura what exactly is Clara and then we'll also see one example you know how with the help of code you will be able to do it and trust me these are some of the very important things in fine tuning because tomorrow you go in any interviews you are going to get asked respect to this kind of questions that may be coming because at the end of the day any generative AI projects specifically with llm right llm models if you are working in the company they will be giving you finetuning task so let me quickly share my screen and show it to you so here already I have uploaded a video uh part one where we have discussed about quation uh a good 32 minutes video Everything you'll be able to understand it because here I've also used all the mathematical intuitions everything as possible as as much as possible uh to talk about the research paper so what does Laura means Laura basically means low rack adaptation of large language models uh this is this amazing research paper and uh probably you'll be seeing lot of this kind of equations as you go ahead there'll be different different performance metrics but as usual I will do what I'm good at I will try to break down all these things and probably explain you with respect to examples with respect to code and many more things so quickly let's go ahead so why Laura and CLA is basically used Laura basically means low low rank adaptation lower order rank adaptation it is specifically used in F tuning of llm models okay so let's go ahead and discuss this and I've created this amazing diagram over here here initially whenever whenever you have a pretrained nlm model that basically means uh let's say that there's a model something like GPT 4 or gp4 Turbo right and this model has been created by open AI so we basically see this model as the base model OKAY gp4 turbo and this model is strained with huge amount of data right so the data sources will be in internet it can be books it can be multiple sources right at the end of the day all these models how you can measure them they may be saying that hey uh it supports 1.5 million tokens you know it has been trained with this many number of words right so many tokens of words now what will happen is that all these models you know probably to predict the next word it will have the context of all those tokens and then it will be able to give you the response so these all models are basically base model and we also say this as a pretrain model OKAY some of the examples again I'll be telling you GPT 4 gp4 Turbo right gpt3 GPT 3.5 something so all these models are specifically pretrain models now further we can take this model and there are various ways of fine tuning it please make sure that you watch this video till the end if you watch this video you will understand everything that is actually required with respect to fine tuning and there are multiple ways of fine tuning also now let's say that I take this model I do some amount of fine tuning okay and this fine tuning is done on all the weights of the specific model right so some of the application that we may generate is like chat GPT you know we may generate like Cloud uh cloudy chat GPT right cloudy GPT itself like the chatbot that we specifically use some of the examples okay so this one way of fine tuning is there where we train all our parameters so here we specifically say full full full parameter training okay so here you can see full parameter fine tuning so here I'm going to write full parameter fine tuning now this is one way of fine tuning where we train our entire parameter based on our data that we have okay and after training it we can develop applications like chat GPT or any custom GPD that you specifically create as we go ahead you can also take these models and perform domain specific fine tuning okay so one type of fine tuning is this the other tune other fine tuning technique that you can specifically use is called as domain specific fine tuning some of the example let's say that I'm fine tuning a a a a chatbot model you know which will be for finance it can be for sales it can be for different different domains itself right so here the main important word is domain so this fine tune also we can perform okay again why I'm saying all these things because there are various ways of fine tuning things right one more fine tuning we can basically divide by is something called as specific task fine tuning in case of specific task fine tuning these are my different different task let's say this is Task a b c d this task can be something related to Q&A chb this task can be something related to document Q&A chat but different different applications so that is the reason why we are specifically saying over here as specific task okay specific task fine tuning now perfect now this is good you have seen all the different ways of fine tuning okay now let's talk about full parameter fine tuning again I'll repeat it this is my base model right now if you use it as an example as I said GPT 4 Turbo GPT 3.5 you know Gemini Gemini 1.4 different different models can be there we can take this base model we can find F tune and create applications like chat GPT we can create other other application like stable diffusions you know not specifically to llm but LM we can actually do it then we can also further finetune this based on domain specific fine tuning right based on different different domains like Finance sales retail we can also take this model and do more specific task finetuning like task a task B task D let's say I want to convert this into text to SQL I want to have this as document Q so I can further fine tune it based on specific task now let's talk about this full parameter fine tuning okay and what are the challenges with full parameter fine tuning and that is where see I'm building up the story later on I'll be explaining you where Laura will be used now in full parameter fine tuning the major challenge is that we really need to update all the model weights let's say that I have I have a model which has somewhere around 175 billion parameters that basically means 175 billion weights now in this particular case whenever I finetune this model I need to update all the weights great now when I'm saying updating all the model weights and when we talk about this many number of parameters why can be a challenge because there will be Hardware resource constraint right so with respect to different different task if I really want to use this particular model that much RAM I really require for inferencing purpose that much GPU I really require right so for Downstream task it becomes very difficult now what is Downstream task Downstream task some of the example is like model monitoring right model monitoring the other task can be like uh model inferencing right model inferencing similarly the GPU constru that we may have the ram constraint that we may have so we may face multiple challenges when we have this full parameter tuning full parameter fine tuning now in order to overcome this challenge we will specifically use Laur and CLA okay what exactly is Laura as I said low order rank adaptation and CLA is something it is also called as Laura 2.0 so we'll discuss about both of them with respect to mathematical intuition and you'll get a complete idea what I'm actually trying to say now what does Laura do okay now let's read the first point and this is very much important because in the research paper you will find this equation okay this equation now what exactly Laura will do Laura says that instead of updating all the weights in full parameter fine tuning right instead of updating all the weights it will not update them instead it will track the chain changes it will track the changes now what changes this is basically tracking it will track the changes of the new weights based on finetuning okay this is very much important to understand so uh based on the new weights how we are going to combine this weights with the pretrained weights okay so here you can see these are my pretrained weights from the base model like let's say that this model is llama 2 now if you're performing fine tuning using laa then Laura will track the new weights over here which will be of the same size okay so let's say if this uh weight is 3 cross 3 then the new weights when it is probably doing the forward and the backward propagation those new weights will be tracked in a separate Matrix and then these two weights will be combined wherein we will get the fine weights now this way what will happen is that this tracking will happen in a separate way but still you may be thinking Kish here also we are updating all the weights itself right so here also the resource constraint will definitely happen yeah fine I'm talking about 3 cross 3 but what about uh weights and parameters where there are 175 billion right 175 billion parameters or 7 billion parameters that time I'll be having a huge metrix right so at that scenario you need to Now understand how Laura works because this weights how is getting tracked it will just not get tracked in this 3 cross3 Matrix instead all these weights that is getting tracked there a simple mathematical equation will happen or I'll not say mathematical equation will happen but there will be a technique that will happen which is called as Matrix decomposition that basically means the same 3 cross3 Matrix is saved it in a two smaller Matrix now in this two smaller Matrix you can see this is is nothing but 1 cross 3 and this is nothing but 3 CR 1 right sorry this is 3 CR 1 and this is 1 CR 3 right so this is 3 cross 1 and this is 1 cross 3 when we multiply both these weights then I will be getting this weight only right so over here if I consider I have some around nine weights 4 5 6 7 8 9 right you'll be able to see that I will be able to get all these nine weights from how many number of parameters just six parameters right because when we multiply this then you'll be able to see that I'll get all these nine parameters or nine weights that I have right so in short what Laura is doing is that it is performing this Matrix decomposition where a big Matrix and this Matrix can be of any size is decomposed into two smaller metrics based on a parameter which is called as rank how to calculate a rank of a metrix you can definitely check out any YouTube channel it is a simple algebraic equations based on transpose of a matrix how we calculate the rank but let's say that this Matrix that I have which is a 3 cross one over here the rank of this particular Matrix is one right and if I use this two Matrix you can obviously see that the number of parameters that I'm storing over here is less when compared to this right yes there will be a loss in Precision but it is making sure that when we combine both this metrics we'll be able to get the entire updated weights and just imagine start thinking guys let's say that if I have 7 billion parameters now I'm trying to perform finetuning on those parameters so whenever I track those weights this huge Matrix will be decomposed into two smaller Matrix and when we are decomposing this this Matrix this updated track weights Matrix into two smaller metrix obviously will be requiring less parameter to store all these values right and this way your fine tuning becomes very much efficient and this really solves the resource constraint right this is the most important thing right and so in any research paper that you go ahead you'll be seeing this equation w0 this is my pretrain weights plus the track changed weights is nothing but my pre train weights plus b multiplied a what is b b is this a is this so when we multiply this you'll be able to see that we are able to get the all the track change weights right and obviously this requires less parameter if you decomposing our bigger Matrix into two smaller Matrix less parameters is required now what will happen if we keep on increasing the ranks if we keep on increasing the ranks this parameters will also keep on increasing but but it will always be less than this right if I have 7 billion parameters if I try to decompose that into two two matrices two small R matrices with increasing rank then also the parameters that will be required will be less how I am seeing this because in the research paper also they have tried with multiple trainable parameters now let's see over here there are multiple techniques of fine tuning some of the techniques that are there is something called as prefix embed prefix layer adap adapter is one very famous thing that is probably used before Laura right you can see as the rank is increasing the parameters also increases right initially the trainable parameters are 175 billion but when I use techniques like adapter right so initially it will have 7.5 7.1 million parameters with rank is equal to 1 but as I keep on increasing the rank as I keep on increasing the rank you'll be able to see that this parameters also get increased but you can see from 175 billion parameter if I compare 7.1 million weights the percentage is very less now similarly in Laura because of that uh metrix decomposition you'll be able to see that as I keep on increasing my ranks so these are my ranks with respect to Q KV because in transfer you have this three parameter Q KV there only all the metrix multiplication will happen with respect to these three parameters and then we as we keep on increasing the rank here you can see four here you can see 8 here you can see 64 then you'll be able to see initially we got 4.7 million parameters compare from 175 billion to 4.7 million how this was possible because of the because of the metric decomposition because of the metrix decomposition right and as we keep on increasing the rank you'll be seeing that the parameters are increasing right the parameters are obviously increasing but if I compare it with 175 billion parameter this is very very less 9.4 million if you just see the percentage right so here also when rank is equal to 8 37.7 million then rank is equal to 6430 1.9 million right parameters are there so still it is making sure that the parameter is not that much like like not not like 175 billion or not near to this if I talk with respect to percentage it is very very less I've also made another table right just to show you if I have different different models number of trainable parameter number of trainable parameter here you can see I have one llm model with 7 billion if I use rank is equal to 1 then I will be having 167k parameters to fine tune fine tune weights based on fine tune weights then and this 17 67k parameter basically means what this decomposed Matrix that I have right two matric that many number of parameters so in the first case it is just nothing but 7 167k parameters that is the available in this decomposed metrix okay when we combine them then we will be able to get how many 7 billion parameters if we combine both this metrics then we'll be getting uh 7 billion parameters okay then similarly you can see in 13 billion then you have 2 to 8K parameters in 70 billion you have 5 to 9k parameters in 180 billion you have 849k parameters so as you keep on increasing the weight this parameters will keep on increasing but it is not increasing with that huge amount right even you can see when we keep the rank as 512 right so here you can see 86 million parameters is there when compared to billions right uh Microsoft uh you know it came up with this Laura technique uh in one of the research paper and it has used rank is equal to 8 okay to probably do the finding and it has performed absolutely well so most of the time we select this particular value but at the end of the day how to select this right it want matter you know because the parameters are increasing by very less number over here as we go ahead so usually you can select rank 1 2 8 while you're performing fine tuning now there may be also scenario that when should we use very high rank when to use high rank when to use high rank this answer because in the interview they may ask you if the model wants to learn complex things complex things then you can specifically use high rank right let's say some of the model is not trained to probably uh interact or probably uh perform some of the behavior at that point of time those complex things can be handled when you are probably increasing the number of ranks okay so this can be a very simple question that may be asked in the interview but I hope you got a complete idea at the end of the day this is the equation that you'll be able to see in most of the research paper uh what Laura is doing is that nothing very is simple all the track weights is decomposed into two smaller matrixes with different different ranks it can be different different ranks when you're fine tuning the first thing is that you really need to set that rank okay if you set that rank like in this particular case if I probably see if I go ahead and calculate with all the mathematical stuff I will be able to get the rank is equal to one okay uh for this also rank is equal to one right so similarly if you have rank two so one of the metrix can be something like this so decomposed Matrix so this is based on rank two okay so if I probably combine this right so how many 1 2 3 4 5 6 7 8 9 10 11 12 right if I multiply this I'll be getting a metrix uh of much more parameters right but at the end of the day for in this particular case it is less number of parameters right so this is what Laura is all about and because of this technique the fine tuning is done less the the weights the parameters becomes less so this is how the main resource constraint is done and uh with respect to all the downstream task it becomes very much easy now one more thing that I really want to talk about is chora okay chora chora basically means quantized quantized Laura okay now you have already learned from the first video what is quantied quantied basically means now in CLA case what will happen is that all these parameters that is probably stored in float float 16bit okay we will try to convert this into 4bit that's it okay once we do this you'll be able to see that we reduce the Precision and then we try to reduce this values also by this you won't require much more memory so that is the reason we say quantized Laura technique okay and the best thing about is this is that U clora also has one amazing algorithm which will be able to take care of both this part let's say if there is a float 16bit I quantize it to 4bit I can also convert this back into 16bit okay so with respect to this explanation guys I've already spoken about many things over here Laura and CLA uh just to show you one example so here is one example uh I've already shown you this finetuning using Google Gamma model let me talk about quation quation over here is basically done by bits and bytes config so here it says load in 4bit true that basically means we going to convert that entire model of 16 bit to 4bit quantisation technique that we are going to use is something called as nh4 and uh all the further fine tuning is basically done in V float 16 okay um now there is also one more thing right see La configuration here we have selecting the first rank value eight and then Target modules where we need to apply this particular decomposition and the task type casual LM once you do this and just execute everything you'll be able to see it right and that is how the entire quation and the Laura happens guys in this particular video we are going to see the stepbystep way of probably fine tuning your llm models in this case I'm going to specifically take open source Lama 2 model and with the help of a custom data set we are going to finetune this specific model right over here we are going to learn about various techniques practically not theoretically because if you really want theoretically you can let me know in the comment section so we will be discussing about something called as parameter efficient transfer learning for NLP which is an amazing technique to basically finetune all these llm models which will definitely be of use size like 70 billion parameters and all so how this parameter efficient transfer learning actually happens we'll try to see in the code and we are also going to see a technique which is called as Laura right so Laura paper if I go ahead and search right it is basically called as low rank adapt adaptation of large language models right so these are some of the mathematical concept don't worry in the upcoming videos I will talk about all every theoretical intuition about PFT about Laura right now a simple way of finetuning I'm just going to show you because many people were requesting for this right so initially what we will do is that we will go ahead and install some of the important libraries like accelerate PFT as I said PFT is nothing but parameter efficient transfer learning inside this only you'll find this Laura technique which is called as low Bank adaptation of large language models uh then we have bits and bytes bits and bytes are specifically used for doing quantization now what does quantisation basically mean all these llm models you know when they are trained with 70 billion parameters or 13 billion parameters by default the weights data types are in the form of floating values right when we say floating values that they are basically 32 bit values what we can actually do and obviously since I'm actually going to do this in Google collab we get a very less Ram so it is a better way that you quantize those weights you know from float 32 probably convert that into int 8 and then probably based on the Ram size you'll be able to quickly fine tune it along with that I will be also we'll also be using Transformers and then you have TRL so all this libraries will go ahead and execute it and once we specifically execute it you'll be able to see that all these libraries will get installed now in the Second Step the major thing is that we will specifically be using the library called as Transformers which is specifically used for this particular purpose and internally we'll also be using PFT which is having some Laura configuration and we'll use this PFT model I know you'll not be able to understand what exactly PFT is but I'll just tell you in some time just let me go ahead with but at the end of the day PFT actually uh you know uses techniques which will try to freeze you know when it applies transfer learning learning on these llm models it is freezing most of the weights of that llm model and only some of the weights will be retrained and based on that they will be able to provide you accurate results based on your custom data set okay uh how it is done don't worry I'll create a amazing dedicated video to make you understand this mathematical intuitions okay now over here you'll be able to see that I'm going to import OS import torch I'm going to use a data set I will talk about what data set we are going to specify spefically do the finetuning but here we are specifically using open source llm models and then from Transformer I'm going to use Auto model for casual LM a tokenizer bits and bytes I will talk about all these libraries as we go ahead so let me quickly go ahead and execute it okay now till this is getting executed this import statement is getting executed let's talk about some of the important properties over here with respect to Lama 2 in the case of Lama 2 the following prom template is used for chat model so this is the specific prompt template uh here we be give an instruction in this s symbol and then we have our system prompt which will be closed with the CIS brackets and then you will also be able to give your user prompt over here and the model answer will be coming after this after this entire instruction okay so this is how the entire Lama 2 models llm models specifically require the system prompt and the user prompt and the model answer format right now any data set that you specifically get right we really need to convert that data set into this format okay and that is how I will show you how to probably do this there's a technique uh you can also write your own custom code and all there are many ways okay now what we'll do we will reformat our instruction data set to follow Lama 2 template so right now we are going to use this data set which is basically called as open open Assistant Guan guano I hope I'm pronouncing it right now here you will be able to see this is my data set right human can you write a short introduction about the relevance of term uh monopsony in economics please use example related to this and then Mon Mon monopsony refers to the market so here you can see assistant answer so here the data set is basically in the form of human and assistant like human has a question over there and assistant is probably providing uh you a specific answer so in this format you'll be able to find out each and and every rows each and every rows in different different languages so we are going to take this entire data set and then considering this entire data set what we are going to do we are going to reform the data set following the Lama 2 template and out of all this samples all this data set there are around uh how many data sets are there I guess there are around 10 10K records we just going to take, uh th000 Records or 1K records the reason is that I really need to show you how the fineing is basically done so if I go ahead and click on this and if you see this format right this format you'll be able to see that this entire data set is converted in this format only right instruction is basically there the answer is over here and this entire s is getting closed right so all the data set is basically converted into that specific format now how do you convert it right so for that already what we have basically done is that over here to know how this data set was created you can check this notebook so this not notebook is there already you can see that we are loading the data set we are applying this we are taking the Thousand records and then we are transforming right so in transforming basically a simple python code like I have to probably keep in that specific format right so that is the reason I'm showing you this specific code over here just by one click you will be able to do that okay so all the links are actually given now you need to follow Now understand guys see understanding how the specific techniques are definitely I'll create a dedicated theoretic video understanding all the math equations that is required right over here we are trying to see that how you can also run your own fine tun model right so note you don't need to follow a specific prom template if you're using the base llama 2 model but right now we'll not use we'll use we'll not use this base Lama 2 model okay how to find tune Lama 2 so these are some of the steps not only with LMA 2 with other models also this will work but again there the format may change you know the the format of the instruction the format of your prompts may change so free go gole collabs offers a 15gb graphic card right so limited resources barely enough to store Lama to 7 billion weights now here we are going to use 7 billion weights but it is also very difficult to store 15 GB right whatever free model that we specifically have we also need to consider the over head due to Optimizer State gradient and forward activation okay so usually in in any llm models you'll be having gradients you'll be having forward activations you'll be having optimizers so there also you require some amount of memory find tuning is not possible here right obviously this will not be possible because 7 billion weights you cannot store it in 15 GB that is the reason we require this parameter efficient finetuning technique now what does PFT basically do it is going to freeze most of the weights that is present in that llm model like Lama 2 and only with some of the weights after applying quation it is going to probably perform the fight fine tuning now parameter efficient fine tuning I will in my next video I will talk about this research paper if you quickly want this video please make sure that you make the video likes 2,000 okay now what we are going to do over here we are going to use techniques like Laura and clora as I said Laura or clora Laura is nothing but low rank adaptation of r large language model again I'm apologies guys if you don't know the mathematical Concepts I will explain in the upcoming video okay so first of all we will load a Lama 27 B J GPT model this chart HF model then train it on this 1K sample which will produce a finetune model with which in the name of chat fine tune we'll try to create in this clora will use a rank of 64 with a scaling parameter of 16 we will load the Lama 2 model directly in 4bit Precision we are trying to convert that 32bit into 4bit so that is how we are going to do the training and with respect to Chlor in order to find the low rank index we are going to use the rank of 64 right this is an hyper tuning parameter you can just consider right now this is a kind of hyper tuning parameter with a scaling parameter Alpha this is also called as Alpha it will be having a scaling parameter of 16 as I said everything will be explained detailly when I probably go with the mathematical equation but right now our main aim is to probably learn how to find tuning it now what model we are going to use we are going to use Lama 2 7bh uh 7B chart HF then the instruction data set to use is this particular data set we will be downloading it from the hugging face the model name also will be downloading it and after finetuning it this will be my new model name okay now these are some of the clor parameters that is required okay so one is laurore R 64 what is this R this R is a rank of 64 a kind of hyperparameter Laura Alpha as I said Alpha right I told you Alpha why because I know the entire mathematics stuffs in this okay just to increase the Curiosity I'm coming up with this first video and later on I will come up with that then here also Dropout is basically required now in order to do the quantization we will be using bits and bytes parameter so here you can see activate 4 bit precision based model so there is a parameter which is called as _ 4bit which is equal to true then compute data type for 4bit base model so here it is basically float 16 then quantisation we using fp4 on np4 so B andb 4bit Quant type you have to keep this particular value to np4 since it is 4bit activate Ned quation for 4bit based model so here we are keeping it as false Now understand Guys these are some of the basic parameters that we specifically use in Laura technique specifically in PFT then training argument parameters our output directory will be present in this results I'm going to run one Epoch then we are going to enable this fp6 and B bf6 training okay uh it is set to True with an A1 100 right so a100 uh you can set it if you're using a100 you can set it to true right now I'm using T4 if you have the paid version of Google collab then you can set it to True bass size for uh Pur GPU for training I hope you know what is bass size then you have GPU for evaluation bass size then gradient accumulation step checkpoints Max gr uh Max grad Norm learning rate weight DK right Optimizer page adamw we will be using which is of a variety of Adam itself then learning sh learn uh LR schedular type cosign because it works on similarity right whatever question and answers we specifically write then maximum steps is minus one number of training steps override number of training epochs and after this you are also putting logging steps is equal to 25 now with respect to any fine tuning technique you use something called as supervised tuning right in supervised tuning that is you require some parameters right Max sequence length then packing then device map so this is load the entire model on the gpu0 right so this is what are the some of the parameters don't worry uh these are some of the parameters that you don't need to learn each and every parameter because already all these things are provided by the official page itself I've just copied and pasted it over here right so we will go ahead and execute it so let's go ahead and execute it so all these parameters are set now the step four right there are multiple four steps right uh one more step is there later on load everything and start the finetuning process right first of all we want to load the data set we defined here our data set is already preprocessed but usually this is where you should ref format The Prompt right filter out bad text combine multiple data some amount of preprocessing is required but already we have done that so we are not going to do it then we are Recon we are configuring bits and bytes for 4 bit quantization as I said right from 16 from 32 or 16 bit we are converting that into 4 bit so that it requires less space with respect to GPU for the fineing purpose next we are loading the Lama 2 model in 4bit Precision GPU with the corrent corresponding tokenizer right with that tokenizer we'll try to load that and obviously we'll also be loading it with the 4bit Precision finally we are loading the configuration of clora so uh and passing everything to the sft trainer so here is what self fine tuning uh s uh uh this sft will basically happen right now let's go ahead and let's do this so first of all we are loading the data set we are loading the tokenizer model with clor configuration so here I've return this B&B compute D type and we are using torch so along with that you also require bits and bytes config again load we enabling this 4 bit then all the necessary comp parameters like compute D type will be using honet Nest Quant okay again I'm telling you guys there is nothing new to learn in this because all these formats will be available in the official documentation then we are going to check the GPU compatibility with float 16 if compute dipe is equal to torch. float 16 use 4bit otherwise these all things are there right then we are going to load the base model see whenever we want to load the base model from hugging face we can use this Auto model for casual LM right that is the reason we have imported on top Dot from pretrained model name what is my model name I've given that quantisation config so here you'll be able to see in quantisation config we are also given something called as BNB config right so here you'll be able to see this is the compute type let me just search for it somewhere here only it will be available so so BNB config so here you can see this entire bytes config is basically there so uh based on that you'll be okay yeah computer okay yeah perfect so BNB config is basically given over here then device map is nothing but with respect to the GPU we are mapping then model. config use caches false you can also make it true if you want model. config pretraining _ TP is equal to 1 then we are loading the Lama tokenizer see for any LM model we also need a tokenizer so that it will be able to convert any any llm model the input data that we are specifically using into word embeddings and all so that is the reason order organizer from pretrained again model name we are going to use this trust remote code is one additional parameter that is used then we going to put a pad token with respect to the end of statement token right so do this eore token specifically applies the token for the Llama itself right and here we are giving the padding side as right fixed weird overflow issue with fp16 training all these parameters will be almost fixed guys only thing that you will probably be changing is with respect to the configuration then load Laura configuration here you'll be able to see PFT config Lura config all the values that you're putting with respect to this Lowa configs and yes here you have your PFT configuration now this is the most important thing because in this training arguments we set all the parameters output directory number of epo this this this learning rate PP p uh FP 16 BS 16 you can probably see over here and then finally we are reporting it to the tal flow right tensor board then you can also see that supervised fine tuning parameters right I'm giving my model name I'm giving my data set my PFT config my data set text field this PFT config has a Lowa config right then you have a tokenizer you have the arguments you you have packing then you have finally trainer. TR okay now this is what is the main thing and that is where your supervised fine tuning will happen step by step we have done it okay let me repeat it quickly we have loaded the data set we have set our D type right we are setting up all our conation process over here here we are checking whether GPU is compatible or not here we are loading our llm model that is Lama 2 here we are specifically loading our tokenizer which will be used in Lama 2 along with this we are putting padding techniques then my Laura configuration which will specifically be in terms of PETA PFT config and then all my training arguments will go inside this right um the this training arguments is with respect to where my output directory is and all learning rate and all okay finally set supervised tuning parameters here we have seted model data set PFT config text Max equal length tokenizer everything is put up over here and finally we go ahead and train this now once we train it it is going to run for 250 epox uh I think 250 step size I have actually given over here sorry 20 five steps uh logging steps let's see what is the bass size bass size is four um yeah till that much it will probably go so let this start so it has already started I guess so here you can see it is downloading here you'll also be able to see the data set okay sample data right now you cannot see it because the data set will get loaded okay so table of contents installed all the required packages we'll reformat all the steps are given side by side you can also read it out I know this looks like a little bit tough guys but at the end of the day uh I'll not say that it is easy and just the reason why I'm sharing you this finetuning technique because you should just get in your mind later on you know this is the pattern that I'm following first execute this don't worry about anything as such just try to get an high level overview how things work later on I will try to break down each and everything in my next video by breaking this entire code why this specific parameters used because the main thing is to understand what is PFT what is quantisation what is precision and uh how how do you specifically use this PFT technique what is qora everything what is low order rank index uh how to basically calculate that everything I will talk about it okay so we'll wait for some time till then uh just let let us wait and uh we will I'll just just uh come again I I think it'll take 15 to 20 minutes to complete this entire fine tuning with thousand records and then again I'll come back and we'll start doing and seeing whether we are able to get the good results or not so yes uh let's wait for some time thank you so guys uh finally you can see the 250 EPO or 250 steps have completed it took 25 minutes and again this is in Google collab if you have paid version of Google collab it will probably take hardly 5 to 10 minutes to complete Okay so over here you can see the global step was 250 training loss it went went till 1.36 metrics runtime everything met training samples per second all this information is basically done okay and please remember this particular word which is called as floss okay total floss because I'm going to discuss about this in my next video also now once we do this we are going to save this train model right and understand the new model name what it will be right so here you can probably see Lama 27 be chat fine tune so this is my results with respect to run all the results you'll be able to see over here also okay so here uh in this fine tuning technique it is also creating some something called as adapter adapter model okay please remember these words because in the next theoretical intuition we are going to discuss each and everything as we go ahead okay so please make sure that you remember it so we are going to save this model so we have written trainer. model. save. prerain model right now you can also check out in the tensor board but I will just go ahead and show you quickly that how it is probably going to generate it right so here we have created a prompt which is called as what is large language model I've used pipeline right so this pipeline we have already imported it the task will be task generation whatever model we have actually created that model will be there tokenizer will be used over here and max length we can keep to 2 200 to 250 the result uh and always understand as I always suggested with respect to Lama 2 this will be my format there will be an s s then there will be an instruction and here I will be having my prompt and with respect to this particular prompt we are going to get some kind of response so whatever response we are going to get inside this result variable it will be in the form of list and inside that there will be one field which is called as generated text so if I go ahead and search what is large language model you'll be able to see that how we going to get the result okay because we are running the same model over here so here is my prompt here we are using pipeline pipeline basically helps you to come combine multiple things like task model tokenizers you know multiple things it will be able to give you right now since this is already running in this particular collab uh and obviously you'll be able to see RAM and all are almost it is used the dis bace of somewhere around 39 GB right so just wait for some time and here you will be able to get the response if you quickly want to get the response obviously you need to have a good GPU right based on that it'll be able to give you a quick result right so after that you'll be also able to see that we'll be able to delete all these V Rams and all okay so let's see and let's see whether we'll be able to get our result in the next step we can also push our model to the hugging phas which I will keep it right now I will not explain it because this I will show you as an complete project as we go ahead so here you can see what is large language Model A large language model is a type of artificial intelligence large language model often seen then here you can also see all the information are there some example of large language model models are uh include this okay now what we are going to do let's go ahead and take any one example over here from this particular data set okay so I will just write how to own a plane in United States okay so this will be my over here and I'll paste it over here let's see so this will also run and I will finally get my result also so same same question I have taken right so from this 1K result so to own a plane this is the answer that we will probably be getting let's see how much time it'll take to probably showcase but always remember please keep on looking at this particular Ram like how much uh time it is probably taking and uh how much space it is taking okay so so guys here you can probably see the response how to own a plane in united state in United State and owning a plane is this determine your budget so this is completely based on this information that is present over here but here I've have written only 200 max length so I can only see 200 characters that is given right so you can probably try with each and everything as you go ahead one of the most interesting thing in the field of data science or generative AI is that the kind of research that is currently happening right every day you'll be seeing some new things that are actually happening which is very much beneficial for the entire Community who are working with llm models uh specifically today I saw this amazing research paper where it is written as era of 1 bit llm so I'll be going to talk about this particular research paper and what exactly 1 bit llm is and how it is far more advantageous when compared to those 32bit or 16bit llms models okay so everything I'll be discussing about one important thing that I also want to make sure that you learn from this particular video is that how do you read a research paper what are the important points that you should definitely highlight while reading a research paper and how you should definite and one thing is that you cannot directly understand just by reading it you really need to have some basic knowledge and without that particular basic knowledge it will be very difficult to understand so if you're following my tutorials I always make sure that whenever I make my videos right I definitely watch or see all the research papers and then with respect to that I simplify the those Concepts and try to explain it to you so let's go ahead and understand about this onebit llm now guys uh if you remember in my previous video we have already discussed about quation right so quation was covered now with respect to quation what we were doing is that let's say I have a model which is called as Lama 2 which is an open source model let's say this model is 7 billion having 7 billion parameters when we say 7 billion parameters I'm talking about weights okay now obviously if I have a system where I don't have very high configuration not I have resource constraint I have limited amount of Ram or gpus what we specifically do we perform quantisation and we convert this Lama 2 model which is probably in FP 32bit and we try to convert this into to int 8 bit okay int 8 which is nothing but 8 Bits right now when we are once we are doing this specific process what is basically happening is that the model size is getting decreased right and because of that we will be able to load it and we'll be able to perform any task along with this we can also perform fine tuning with the help of Laura and CLA right so I hope you know this Laura and CLA I've already discussed in my previous video please just go click on my uh click on my channel otherwise just go ahead and see in the description I've been providing that particular links with respect to fine tuning now with the help of LA and cl we can perform the fine tuning okay now the question is that what is this one bit llm right as I said that with the help of quantisation we will try to convert this into 32 to 16 bit or it can be 8 bit right but converting this into a one bit that can be again uh if you're trying if you now just by seeing this right if we are able to convert this into one bit that basically means we will never be having any resour constraint right resource constraint yes with limited Ram with limited GPU with limited storage we can probably perform everything from fine tuning to inferencing right so inferencing can also be performed right and this is what is so amazing about this and this is I I don't know like what is going to happen just in some days because once this is probably gone right now we just have the research paper once this implementation gets started trust me it will be quite amazing for the entire Community who are working with llm models okay so this was just a brief idea about this one now let's go ahead and discuss what is onebit llm okay and when we say to be precise when we say that all large language models it is basically in 1.58 bits okay white is 1.58 we'll discuss about it and there are many points that needs to be discussed uh along with me please make sure that you watch this video till the end because I'm going to read over here because this will also give you an idea that how you should probably go ahead and read the research paper so let me quickly uh go ahead and clear this let's see whether it will getting cleared or not okay so over here okay clear is basically happening um okay I will just rub it okay now let's go ahead and discuss about this and let's read some of the important information that is present over here okay and trust me guys read along with me then only you'll be able to understand how you can read the research paper okay now what exactly this 1bit llm model is u in this work we introduce a 1bit llm variant namely bit net okay so bit net is the llm model name one bit llm model name and then where every single parameter or weight of the llm is Turner right now it is not floating 62 bit uh sorry 32bit or 16 bit it is Turner Turner basically means it has only three values it can have only three values weights it can be minus one 0o or 1 okay it matches the full Precision Transformer M with the same model size and training tokens in terms of perplexity perplexity basically means so with respect to any query that I ask an end to end task performance right while being significantly more cost effective in performance of latence memory throughput and energy consumption so obviously at the end of the day all the llm model will specifically have this kind of constraint right which are specifically with huge uh number of parameters let's say 7 billion 170 billion right and if you're just using this three numbers minus 1 0 1 you'll be able to understand why I'm saying that because of the stary uh values right you'll be seeing how abundance the performance improves okay so furthermore uh so here you can probably see all this points uh latence memory throughput and energy consum uh consumption uh energy consumption can be with respect to inferencing with respect to fine tuning and all okay now let's understand how this operators how this values will be basically used okay this is also important so with respect to this what I am actually going to do I am going to make sure that to explain you I take the right thing okay so let's understand this okay understand guys whenever we talk about parameters these are my weights okay these are my weights let's see so these are my weights okay and these are my weights so let's consider that my initial Transformer llm weights is this one okay now by when we say 1 bit llm we are going to convert all this values and replace them with either of these three values Min one 0 comma 1 okay so that is the reason that you see over here all these weights is being getting converted into something like this okay minus1 0 or 1 only that three parameters is there okay and this is what we basically say as bitnet B 1.58 okay and this is also called as parito Improvement how this is basically happening I will talk about it okay just give me some time there will be some kind of quantization getting applied here also okay quantization getting applied over here okay to convert this values to this okay now let's understand one very important thing okay and this is the most important thing what will happen if you convert this values to this see with respect to any finetuning of forward propagation backward propagation what exactly happens the model weights the model weights over here is basically getting multiplied by the inputs and then we get the output right yes additionally we add a bias so it's okay we don't include a bias right now over here just to show it to you so over here this let's consider that this is my float flating 16 number so every number will get multiplied by the input right and then what will happen is that after that all the it's it's just like this right summation of I equal to 1 to n w of x + B right so this is what is the operation that is basically happening whenever we do the forward propagation Whenever there is an updation of weight that basically means we are doing the summation of weights and the input right so once we are doing this and then we are doing the sumission okay but if we have all these weights in the form of 1 1 0 then what will happen is that over here you'll be seeing that multiplication operation will not be you know that much valuable right so over here first of all we are doing multiplication then addition but over here we are just doing addition no multiplication because any number it will be multiplied by 0 is z only any number that is multiplied by one is one only any number that is multiplied by minus1 is min1 only so over here the main thing is that your addition operation is only happening addition operation is only Happening Now obviously if you only need to do addition operation then what will happen your GPU will not be requiring that much GPU also so your GPU will also get reduced why why this operation takes more GPU because multiple multiplication needs to happen right with respect to different different weights right then addition of all those values needs to happen because in the forward propagation this is what is the equation that specific Ally happens right we multiply by the weights with the inputs and then we do the summation and then finally we add the bias right so this is the most important thing so here you'll be able to understand with floating 16 right all the numbers is first of all multiplied by the inputs and then the sumission is done but here your values are with respect to Turner that is min1 0 1 so here multiplication is already skipped because 1 into x0 is x0 only right it is a simple multipli ation right and that much resources will not be required for simplistic multiplication so here maximum to maximum only addition will be required right so I hope you're able to understand because of this technique of parito improvement because of this technique of Paro Improvement you'll be able to see that what we are able to achieve right and obviously when we are able to achieve this the GP will be required less when we are doing the fine tuning or training right so I hope you have got this as an complete idea and you have understood right why we specifically do this how it is done how this transformation is done so here you can probably see that it provides a Paro solutions to reduce inferencing cost latency throughput and energy of llm while maintaining the model performance the new computation Paradigm of Paradigm of bitnet 1.58 calls for Action to design new hardware optimization for 1bit llm right I know guys this is more of a research paper so I'm reading and I'm telling you each and everything and also explaining you the concept I know this can be a little bit of boring but trust me you need to understand in this specific way okay now let's talk more about this and we will I've highlighted main main things in this green color okay these models have demonstrated remarkable performance in a wide range of natural language processing task like llm models but their increasing size has posed challenges for deployment and raised conc concern about the environmental and economic impact due to high energy consumption obviously this is the problem with llms that are already available one approach to address the challenges to use post training quantization to create low bit models for inferencing I've already discussed about this quantisation Laura CLA everything this technique reduces the Precision of weights and activation significantly reducing the memory and computational requirement of llm the trend has been to move from 16 bit to lower bit such as 4 bid variant this is what is basically happening with respect to llm models right this is with llm okay this is with llm so here I'll write llm models now let's see with the help of one bit architecture one bit model architecture what we can solve so recent work on one bit model architecture such as bitnet presents a promising direction from reducing the cost of llm while maintaining the performance walina llms vanila LMS are in 16bit floating values and the bulk of L LM is matrix multiplication therefore the major computation cost comes from floating Point addition and multiplication operation I said you just now on top of it right in contract the matrix multiplication of bit net only involves integer addition because anything multiplied by one is one uh anything multiplied by one is that same number anything multiplied by minus one is that same number with a negative sign anything multiplied by 0 is obviously 0 right so as the fundamental limit to compute performance in many chips is power this energy saving can be translated into faster computation now this is the most important thing right and here you can clearly see the things that I've highlighted right I hope you get an idea how good this one bit llm can be okay then you can still read about it here we are going to just use turn values like Min 1 0 1 and obviously because of this zero your 58 bit is basically increasing there are two major advantages of using this also it is written over here see furthermore bitnet oh my God why this is getting highlighted like okay furthermore bitnet offers two additional Advantage first its modeling capacity is stronger due to explicit support for feature filtering how feature filtering happen because anything multiplied by zero will be zero on now right made possible by the inclusion of zero in the model weight which can significantly improve the performance of 1 bit LM secondly our experiment shows can match full Precision Baseline in terms of this end to end task performence starting from a 3B size okay now most of the things that you are able to see right now let's discuss about one more important thing uh that is how this transformation is happening how this numberers are getting converted to this it is just by using this simple mathematical equation or this quantization function okay quantization function quantization function okay and this quantization function is called as absolute mean quation and this is the formula that is basically used by which all the numbers are basically getting converted to only this three values okay 0 1 okay 1 0 1 okay just by applying this particular formula okay so in uh and there is also one more change with respect to the Transformer it replaces nn. linear with bit linear okay so this bit linear I think uh you'll be able to see that it is stained from scratch with 1.58 bit weights and 8 bit Activation so this is what is basically done with respect to the initial training okay so most of the thing I have actually discussed over here uh let's talk about the performance so here you'll be able to see that uh the Llama model of 700 million p parameters bitnet will also have 7 million parameters but here you see the memory is in decreasing right over here 2.08 1.18 12.33 is getting reduced to 8.96 and then this PPL is basically 12.87 so over here you can see that how it is getting reduced now similarly when the billion of parameters are basically increasing right let's say with Lama is 1.3 billion right the parameter will be same but memory again 1.14 is required .97 11.29 right and similarly over here also you'll be able to see the same thing is basically happening so memory is decreasing latency is also decreasing for the inferencing purpose perfect and uh one more parameter that you'll be able to see with respect to model size and latency right model size so the the blue color is basically the Llama model OKAY the orange color is basically one bit llm models you'll be able to see how much huge latency difference is there similarly with respect to this how much huge memory difference is there right to save this kind of models so uh this is just the research paper that has come up recently but uh I'm really really happy to see this because in the future many things is going to happen so again I would like to welcome you all to the era of 1bit llm models and now you'll also be able to use this one bit llm model soon I think first of all hugging pH will only come and try to implement all these things where you can also easily create your application using gen so guys Google is again on bang and it has created its own opensource llm model that is called as gamma now again it is on that specific race of Open Source llm models also till now the most accurate model that we specifically had was from meta that is called as Lama 2 uh now gamma is there uh in this specific video we'll try to see a practical application along with that we'll try to fine tune with the help of this specific gamma model okay uh so what exactly is gamma model uh I'll just show you one block first of all why they have actually created it at the end of the day all the companies you know is saying this thing right so gamma is built for responsible AI development from the same research and technology used to create Gman models okay uh at Google we believe in making AI helpful for everyone we have a long history of contributing to open source communi such as Transformer tens oflow B T5 Jacks Alpha fold and Alpha code obviously Google is doing I think it is it is from past so many years it is doing so much of research specifically in open source contribution it has given tensor flow it has given many things as such uh along with that meta is also in the same race uh both are doing fabulous job so let's go ahead and let's know more about Gamma uh GMA GMA we'll say GMA okay GMA also we can say I don't know how to pronounce it but it's fine so if I probably talk about this the main thing that you really need to know is about the performance metrics uh over here when I compare see two models are there with respect to this particular model one is 7 billion parameter one is 2 billion parameter if I consider with respect to Lama 2 uh uh we have 13 billion and 7 billion with respect to all these things I think gamma is performing absolutely fine here you can see that it is having 64.3 with respect to various Benchmark accuracy if you want to know more details about it you can actually see this particular blog uh over here if I if we are just comparing open source models like gamma and Lama 2 uh gamma is Way Forward when we compare with respect to the 7 billion parameter it has right 64.3 the general accuracy reasoning is 5 55.1 81.2 math 46.4 it is far far better than the Lama 2 models also right so this looks absolutely great uh here the accuracy is also very very good now let's uh talk let's see more about it it is specifically used for you know research purpose you can also use it over here you can build your own models and the other thing is that right now uh the gamma 2 models right 2 billion parameters 7 billion parameters already available over here see in hugging phase so once you probably go to hugging phase and search for gamma 7B or gamma 2B you'll be able to see this kind of page here uh you need to get granted to access this model so here they'll tell you to probably uh check the terms and conditions to get the license of this particular model to use it okay so it'll be like a check box you just need to check it I understand that that and give the confirmation right once you probably do that you'll be able to get the excess of this specific model um what is the main a of this particular video I'll try to show you with the help of practical implementation how you can access this particular model and how you can actually use it so now let's go ahead and let's see this finetuning technique with the help of Google Gamma model and uh again guys uh this is just a simple use case that I have taken over here but you can do some amazing use cases with the help of this and that is what I'll plan in the future some amazing use cases that I've already developed with paid opening eye models I'll try to also develop that with this along with fine tuning okay so initially uh what we going to do we going to see this Google Gamma model how it performs and what kind of task all that kind of NLP task you'll be able to do it okay so initially to go ahead with I will go ahead and install all these libraries that is bits and bytes PFT TRL aate data sets and Transformers if you don't know guys I've already created a video with respect to finetuning okay now in tuning what we have actually done we have done the fine tuning with the help of Lama 2 model okay so let's see a UTF local is required let me restart my kernel change runtime disconnect and delete runtime okay so let me go ahead and reconnect it and then we will try to do the installation again but I've already created a finetuning technique with the help of Lama 2 model the same steps and process will basically happen over here also so let's Okay got connected now let's go ahead andall uh do the PIP install so here you'll be able to see the PIP installation will specifically happen with all these libraries to just tell you what we are going to do this bits and bytes techniques bits and bytes libraries used for quantization purpose I've already created a video with respect to quantisation what is the main name of quantisation quantisation actually help uh allows you let's say that you have a huge model right right now we have this Google Gamma model of 7 billion parameters or 2 billion parameters if I want to load it let's say in Google collab so in Google collab right now I have somewhere around 50 GB Ram or 2011 GB hard disk right and right now I'm using this Google collab premium version right let's say if you're using it you'll hardly get 15 GB Ram it is not possible to load that entire model over here so what quation techniques helps you do is that it helps you to uh convert that float 32 bit right usually all the numbers all the weights right all the bi are saved in 32 bits it will help you to convert that into 8 bit 16 bit so when you convert that you require less memory at that point of time right so that process is specifically called as quantisation and I have actually explained it I've shown you how mathematically we specifically do quation so everything is explained in this particular video I'll provide you this link in the description of this particular video now the next thing is that we will be importing some important libraries one is OS Transformers then you can probably see torch Google collab user data data sets I'm going to load it sft trainer PFT right PFT PFT Laura configuration as I said that we going to use Laura technique okay sft trainer is specifically used for fine tuning purpose supervised fine tuning we basically say right now from Transformers We also going to use Auto tokenizer because tokenization if you want to really perform tokenization while fine tuning we'll be using this Auto tokenizer so that it can load based on the model that we have then you have this Auto model for casual llm this is specifically used so that we'll be able to fine tune with respect to Casual large language model okay then from Transformers We also going to import bits and bytes config here we are going to give the configuration with respect to quantisation and then we have this gamma tokenizer so let's go ahead and import all these things so gamma tokenizer is something for the gamma model itself right a tokenization technique otherwise you can also use other tokenization it is not necessary that you always need to use gamma tokenizer now let let me tell you one more step the next thing is that after importing all this you know that this gamma 7B model is specifically present in hugging phase right now in order to access this model in order to download this model we really need to have something called as access token exess token of what excess token of the hugging phas so how do you get an excess token go over here click on settings and here you'll be having something called as excess token just copy this okay copy this excess token and once we have this exess token access right then only we'll be able to download the model from here so here what we will do go ahead and set it in the Google collab how do you set it click on this key button so everybody will be having this key button and just click on add a new secret and I have written it over here as HF token with that same value that I've have copied from there so once I probably write it down over here so this is basically saved right so this is saved over here now once I've have saved it any number of notebook that you specifically work in you will be able to access this hugging fish token how do you access it it is in simple os. environment HF token is equal to user data. getet HF token you just need to write user data. get HF token so once you do that it will ask you for the grant you just need to Grant it and then it'll be able to access and it'll be able to also identify which hugging face uh from which username or password for or which usern name you are specifically pulling that particular information okay so this is done right now through this you'll be able to access any models from the hugging phase right Let It Be llama 2 any model that you specifically want to use now what we'll do we will go ahead and call this particular model Google Gamma 2B initially I told showed you 7B right I'm going to load it 2B right 2 billion parameters not 7B 7B also you can do it it still take time for the fine tuning purpose now here you can probably see I've taken this and then I have written bytes BNB configuration that is bits and bytes configuration the first parameter is load in 4 bit is equal to true that basically means what my model the gamma model in 2 billion parameters is specifically in the 32bit right initially we are trying to convert that into 4bit and this process is specifically called as quation right so what we are doing this once we set it in load in 4bit all the activation function sorry all the weights that are stored in 32 bits will get converted into 4bit okay now the second parameter that you'll be able to see BNB 4bit Quant type is equal to n F4 what is nf4 4bit normal float so the quation technique how do you convert this into a 4bit is basically taken from this particular parameter if you really want to know about it I've given a link over here what is 4bit quation and how does it help Lama 2 the reason why I've written Lama 2 because we are talking about open source model then you have BNB 4bit compute type which is nothing but torch do B float 16bit now the reason we have kept this as 15 bit 16bit see this we are performing quation that basically means we are taking a big model we are making it to a small model how by performing quantisation by converting the 32bit into 4bit now along with that whatever fine tuning takes place for those fine tuning all the weights that gets updated will get updated with respect to this particular 16 bit see there is some loss of information with respect to quation to balance that we are keeping the new fin tune parameters in 16bit so that is the reason we have written it over here so once I execute this so this is my model ID this is this is my configuration now based on this model ID and configuration I'm going to use the tokenizer and from this particular pretrain model I'm going to call the tokenizer and for here also I'll be using the HF token okay then you have model over here Order model for casual LM from pretrain model ID we'll use this model ID that see auto tokenizer is to call the tokenizer that is required for this particular model right and here also I've given the HF tokon automatically that tokenizer will got called right then we are using Auto model for casual LM and here also you'll be able to see that we using model ID quantization config with respect to the BNB configuration the device map will be equal to zero which is nothing for the GPU and here you can see that I'm going to use the HF undor token okay so once I execute this you'll be able to see that I am going to download all those information see auto model specifically to call those particular model the model ID that I've given over here by applying this quation technique and this entire process is going to happen in the GPU the GPU that my Google collab is connected to and over here I've given the token so quickly you'll be able to see that the entire model will get downloaded and now all the information will be specifically present in this particular model now with the help of this particular model I will go ahead and test it now now see this how I'm going to test it I've given a quote over here imagination is more I've used a device Cuda is equal to Z that basically means whatever is the GPU that is available and then I'm going to use the same tokenizer see this tokenizer is basically getting used I give the text and I get this particular information so this input will be a vectors right and then what we do we use this SP same model and we say do generate with respect to this particular input and let's say that I'm going to take a token of 20 okay initially then once I get this particular output the output will have in the form of list like what all information is basically coming so once we write tokenizer do decode of output zero see at the end of the day we are doing sentence generation so when we write model. generated is again going to create some embeding vectors right and that output which is in the form of vector will get decoded with the help of tokenizer and then we'll get the real text so if I execute this you'll be able to and it is saying skip special token if any token is there it'll get removed okay so once I execute it you can see imagination is more than knowledge I'm a selftaught artist born in 1985 right so this information is basically coming based on the based on the model that it has let me go ahead and execute once more okay so here also you can able to see that I'm trying to execute one more time let's see whether I get a different output so here you can see imagination is a more than knowledge knowledge is limited imagination in circles award so this information is given by Albert Einstein okay so two information and if you probably search in the internet you'll be able to find this quote also okay so one over here one output we got where with the code completion here you can you you could probably see that I'm also getting the author name okay now this is what it is able to do it okay so whatever information you probably put it over here it'll be able to get the output based on the information that you require now what we going to do we I'm going to show you a fine tuning technique now see this. environment van disabled so one parameter we need to keep it as false that was given in the documentation also I do not still understand why this parameter was used but once I know it I'll I'll let you know because I'm going to create a lot of projects as we go ahead now we are going to fine tuning with the help of Laura configuration so for Laura configuration what are information I require one is rank okay so Laura technique is something called as rank decomposition we'll get to know I've not still uploaded the theory video so here we can select the rank value okay so rank over here is selected as eight why8 why cannot select 16 or 64 you can select any number but understand what exactly is rank decomposition for that I need to create a dedicated video and soon I'll be coming up with this the target modules uh in this you specifically require this KV parameters gate up project down project so right these are some of the target models that is required as I said guys once I probably upload a video dedicated to Laura configuration you'll be able to understand the task type we always need to keep it as calore LM that basically means it is specifically used for this language modeling task so this is one of the parameter that we need to set it up so once I execute this now let's go ahead and do the fine tuning okay so this is my Laura configuration now the data set that I'm going to specifically use is this okay so let's see this data set okay so I'll see this arbitrate English quotes Okay okay let me see whether we are able to find it out or not arbitrate hugging face okay I'll I'll just search for hugging face now this is the data set if you see this data set you have two information one is quote one is authors right based on this information this is the quote and this is the author and we'll try to F tune with this let's say that my gamma models knows some of the quotes in the internet along with the author okay now my main thing is that I'll have some more additional data set and we'll train with this specific data set and we'll then identify the author for this particular code okay we'll do something like that we'll Implement something like that so here you can see that I am loading this particular data set from arbitrate onore English quotes and then we are taking a sample from the sample of quotes Okay so here we are going to execute this and if I just execute data train of quote right so I will be able to see all the quotes over here okay quotes you can also see the quotes you can see the uh authors and all so this entire uh is getting generated and here you can see train split is also happening and once this is executed here I have my data set so if you probably go ahead and execute it you'll be able to see all the codes okay so the codes the sample of codes that we have there is nothing Noble in this so so so so information as you can see okay so this is my data set now whenever we perform sft that is supervised finetuning right we require some information okay one function we specifically require to indicate what is my input and what is my output okay so here we have used something like this quote example quote of zero so this is my first parameter like zero okay and then whatever information is that this is my input and this will be my output so quote of zero author of zero so that basically means quote of zero indicates I will show you wait this quote of zero basically indicates each and every sentence over here author of zero basically indicates this one okay so this information will be my output and this will be my input something like that okay so once I format this you'll be able to see that I'll be able to return my text okay so let's let's consider over here if I give any examples over here right so here you'll be able to see that uh any kind of example from that example quote will be separated and author information will be separated and then we return that particular text in the form of key value pairs in this list okay now is the time for SE trainer so here we use the model we use the data of train data of train is what so let's see this okay so you'll be able to see what is exactly data of train the same data of train you'll be able to see where I'll have all those information this is my features author tag input ID and attention mask okay then we use some training arguments per device batch one tra gradient accumulation steps four warmup steps thing I'm going to run it for maximum steps of 100 learning rate is so much fp16 is true then output directory will be this optimization will be this right PFT config will be nothing but my Lowa configuration and whatever formatting function I'm using I'll be using it over here so am formatting function okay so that I get the text in this particular key value pairs so once I execute it you'll be able to see it so here you have uh your training will probably start now okay once I write trainer. train and we are going to run this for 100 steps now see how fast it is going to happen the reason is that 2 billion and we are trying to convert that into 4 bill so I think the training will quickly happen and we taking just a sample of data okay sample of data so here you can see 26 steps is done 100 steps and here you can also see that loss is decreasing right 3939 21802 080 it's good 131 so losses keep on decreasing over here okay now this code you'll be able to see I've taken from one of the over here let's see so so here so a human is like a te bag and then my author should be Yen roselt right so now once we execute this I think we should be able to get that specific response okay from that particular model so let's say 89 91 92 100 it is reducing but it is increasing a bit but again it is in that step so training output Global step if you still want to reduce the loss what you can basically do you can increase the number of steps now here I have a quote a human is like a teabag Cuda is Z tokenizer this model. generate this same thing now let's go ahead and execute it I should be able to get the author name in forward see illar is coming when is like a teabag you can't tell how strong she is until you put her in hot water okay so that information you can see you can never know how strong it is until you put it in it's hot water so see all this information is basically coming and the author is also coming as elard right so let's see this uh I will also take one more example something like this let's see whether we will be able to generate the same thing or not based on the data set that we have because we have already doing the fine tuning okay so I will something write like this okay now let's execute and see the output and again now every fine tuning you can specifically do this specific way right I've just shown you in one example the opposite of love is not hate it's fear and the opposite of fear is freedom and the most wasted of all days let's see whether it is there or not see the opposite of art is opposite of love is not opposite of love is not it's indifference the opposite of art faith is not somewhat it's almost equivalent right fear opposite of Freedom most of the wanted is okay let's try some more okay this looks unique unless and until this person is not a famous book is man's so I will just go go ahead and execute this let's see whether we will be and again understand guys we have just max new tokens is 20 because it needs to also complete this entire statement so outside of a box Dog a book is a man's best friend see Man's Best Friend author is Nicholas Comfort the quote is again the most wasted days of all days in one so you can probably see over here right inside of a dog something all the information is great right so you can you can check it out with different different examples but again at the end of the day yes uh fine tuning is good in this again as we keep on doing more and more we'll be able to do it now all the application that we have specifically created like text to SQL invoice we'll try to do it with the help of this we'll also try to do the fine tuning now one challenge that I probably see in while you're building an LM application there is a dependency of lot many different different tools okay let's say that I'm specifically using a model let's say I'm using open Ai and I also want to integrate my llm app with external sources it can be documents it can be Google search API it can be Wikipedia search API so for all this kind of work I really need to integrate with different different apis Al together I need to have an environment key created everywhere let's say with respect to Google search API let's say if I want to use Vector embedding I need to have a separate key stored for the vector databases uh like chroma DB pine cone if I want to probably integrate with cassendra DB and data Stacks I need to have different different connection points now this is always a challenge when you are creating a project from scratch because here to create this entire llm pipeline it will definitely take a lot of time along with that you really need to manage all the configuration now here in this video I'm going to talk about this amazing platform which is called as V vext and here we will be simplifying this entire llm Ops along with that we'll be building llm pipelines without writing any code so here you have almost every feature if you want to probably implement application L document Q&A Vector embedding is integrated over here if you're dependent on any external sources like Wikipedia or CF uh if you're dependent on Google Search Everything is available over here so you don't have to probably save all the configuration or create all the apis just with one API you can actually build this entire llm Pipeline and use that specific API in your chat uh chat Bo in your llm application anywhere that you specifically want to use now let us go ahead and let us see that how things will probably go ahead in this you can also start completely for free so I will just go ahead and log in over here so I have already logged in so here you can see with my email ID I've already logged in I'll show you step by step how you can probably perform all these things right please make sure that watch this video till the end an amazing platform alog together at the end of the day I should be able to explain you and teach you many more things as we go ahead right so quickly let's say I will go ahead and log in so once it is logged in and then we will go ahead and see to it let's see I will go ahead and log in in this way so once we login over here so here you can see that you'll be able to see a dashboard okay once you probably log in now after you log in all you have to do is that go and towards this AI project section okay if you have not logged in you have to sign up in the specific platform now what I will do is that I will go ahead and create a AI project so let me go ahead and create an AI project and I will name this AI project to something else let's say I will write uh document Q&A something I can write up to you okay so whatever application let's say I will say rag system right so I I'm actually trying to create this I will enable this also make sure that you enable this then only your application will probably get started now in this entire pipeline initially you'll be able to see two important section one is query and one is output okay so query basically means an event that starts the flow now this will be super important guys because a person who does not know much coding can also probably use this and create its own application right entire llm application with llm Ops right and you can also build the entire pipeline very simple you don't even require a developer over here now here you have query here you have output output basically means whatever is the final response that you will probably get now in between this there is a plus sign over here if I click on the plus sign I have all these options right so let me zoom in so that you you will be able to see to it okay so over here you'll be able to see once I click on Plus in here you have an option to add an action so first you can add a generate response generate response basically means a response from a specific LM model you can also add data sets let's say you want to implement a document Q&A or a system over here then you can add your own data set along with that you also have an option where you can execute a function like this or smart function now what is this exactly smart function I will go ahead and explain you okay now let me do one thing let me first of all add this search data set let's say that I have some multiple PDF file I want to probably create a document Q&A or it can be a retrieval augmentation generation right that kind of rag system we can we want to create so for this I will go ahead and add a data set over here let's go ahead and create a data set let's say this data set I will say these are nothing but research papers okay so I will go ahead and create this research paper now inside this research paper what I will do I will go ahead and add the data set itself right so over here you'll be seeing you'll be having multiple option you can upload a Plaine text you can upload a file you can upload a p caller you can even uh use Google Drive any file that is available in the Google Drive that also you can add in notion in Confluence so still many more options are probably coming up uh as I said for people who is not much familiar with respect to coding in LM they can specifically use this now let me go ahead let me upload a file now over here I'll click on upload a file and then I will go ahead and upload one file over here let's say this is one of the research paper um uh we'll try to see what this research paper is all about and I'll click on upload okay so this one PDF has got uploaded this is one of the research paper on PFT PFT basically means that Laura and CLA configuration the finetuning specifically that we used in llm models right so that research paper we have added so I will go ahead and click on ADD Source okay now as soon as I click on ADD Source you'll be able to see that this PDF file will get uploaded over here let me see that okay let me say that I have more files to be added over here so I will go ahead and click on ADD source and let me go ahead and upload one more file so now uh there is one more research paper that is available over here so I'll write attention attention is all you need okay so I will go ahead and click it over here so this is another research paper that was available uh attention is all you need uh that is something related to Transformers so I will also go ahead and upload this particular file okay so as soon as I upload it you will be seeing that I have added the source so inside this particular data set that is research paper I have this two U two PDF files okay the research papers itself and I can have any number of research papers because people will again say hey Krish can we just only add one or we can add any number you can add any number but the maximum PDF size should be 5 MB okay so this is there I've added it now let me go back to my project now inside my project the rack system that I had actually created right now I just have this two flows right my l in my llm pipeline now I'll go ahead and add something called as searge data set so as soon as I add this searge data set you will be able to see that it'll give me an option to select the data set right whatever I have uploaded so over there you'll be able to see that I have my data set called as research paper so I'll add it over here okay now you can see both this specific file has got added right now once this is getting added there is a in the right corner right you'll be able to see there there is a button in the right corner which is called as save I will go ahead and click on Save okay so once I click on save that basically means this inje the data injection that is specifically required is now available inside the search data set okay so over here you'll be able to see this search data set because I want to implement a RG system or it can be a document Q&A I can probably ask any question it should be able to give me the response so inside this this particular data has got added but internally what the system is doing it is already creating that entire embeddings that is specifically required internally if it is requiring any Vector store it is creating all those things okay now this is done so this in is my data injection step now coming to the next step here let me go ahead and generate a response because here what I will do I'll create an I'll add an llm model okay now any question question that I specifically asked to my data set it should be able to give me the answer and summarize that answer using this llm whatever llm I'm selecting over here so generate a response you basically select over here and you have multiple options of selecting different different models now see it is also providing all these models uh Azure open GPD 3.5 Azure open GPD 4 anthropic Cloud instant anthropic Cloud Google J Pro Beta And in the future so many different different models are coming like Lama 2 and all are also coming okay and the the best thing about this model is that they have they have hosted this particular model in their cloud and they are providing it as a service to you now let's go ahead and select anyone so let's say I will go ahead and select Azure open AI gp4 okay now here you can see it is it is showing the next field as purpose so in short here you'll write your prompt like how you want this particular model to prompt to behave so I'll say you are a helpful assistant who please answer please answer the questions based on the context okay so this is what is my prompt I've written it over here now there is also one more option which is called as Behavior any additional thing that you really need to give on top of that particular prompt you can give it over here let's say over here one example is the behavior of this AI application be as specific as possible be very helpful and assist user something any additional prompt that you really want to write right now I'm not giving anything so let me quickly um or let me just give one okay so that uh I'll say be helpful as much as much as possible okay so this is done so now what I will do I will go ahead and save this okay so on the bottom right corner you'll be able to see a save button so I'm going to save this now this is my entire llm application right right so any query that it will go it'll go and probably search in this particular data set and then it'll give me a response Now quickly to check this whether it is working absolutely fine or not here there is an option of playground okay now in this playground I will go ahead and ask all my questions that I want see two research paper that I had added I will talk about both those research paper one is PFT for uh one research paper was something for this parameter efficient transfer learning and one research paper was attention is all you need so here I will go ahead and ask the question what is parameter efficient transfer learning okay so I will go ahead and click on send now let's see whether we'll be able to see the response or not so in short what is happening now any query that I'm giving first of all it will go and search from this particular data set which is already in the vector embeding format and then whatever information is going to come based on the prom it is going to get summarized by this llm model and finally we going to see the output result and under understand one thing is that based on the context whatever context it is available or whatever context we are able to retrieve from this particular data set that response we will specifically get it okay till we get the response I will just go ahead and search for some more questions over here okay let's see um experiments uh let's say I will go ahead and ask instantiation for the Transformer Network now here you can see I've got the response parameter efficient transer refers to the method used in machine learning and specifically natural language processing that allows for the transfer of knowledge and here you are specifically getting your entire answer okay let me just go ahead and let me talk about what is attention is all you need okay so because that research paper also I have added okay please summarize okay so I've given this and here I will go ahead and click on send now the best thing is that the same functionality I can integrate with any chat BS that I specifically want let's say in telegram channel in WhatsApp channel in any channel as such that option also I'll show now again I've got it so attention is all you need is a semi research paper by Ashish bashwani and colleagues so perfectly we are able to get it okay so any Q&A with respect to the data set you will be able to get it now here you see that you do not have to write much line of code right and it was very simple just by drag and drop adding the feature adding the data set everything you are able to do it okay now let let me go ahead and add one more function let's say that I'm going to add a smart function okay see execute a function basically means here you will get some options to write to probably do basic maths uh Wikipedia search Google search RV like let's say you want to ask queries from this particular um you know RV where all the research paper are uploaded you can probably use this okay so you can specifically use any of this activation function one more if you want to go ahead with is something called as smart function now inside the smart function you can add multiple functions Al together so let's say that I want to do a Google search so I will say over here you can see in the description it allows users to perform searches on the Google let's say I want to also go ahead and use RF if I want to also go ahead and use Wikipedia I can actually use over here so let's say that I'm adding all the smart function over here and for right now what I will do is that from my first flow I will remove this search data set because I don't want to use data set right so let's say uh I'm using in this particular way now let me go ahead and quickly search in this playground so if I go ahead and write what is machine learning okay so if I press enter you'll be able to see that now it is going to do the Google search and it is going to give me the specific answer okay so and for the first time once you set it for the first request it will take take some time but from the upcoming request it will become very smooth so here you can see machine learning is a branch of artificial intelligence and all so let's say I will go ahead and write who is krishak so here if I go ahead and click on send here also you can probably see that if it is using the right API Google search API it should be able to give me the answer uh about myself okay so krishn is a YouTuber and data scientist known for his educational content on machine learning so all the answers are perfectly coming up right now this is what is the most amazing thing about this but again that question will come Krish what is the use of all this things you know you are saying that we can do all these things in a specific platform but how to probably do this in the coding because I want to use this entire functionality Implement in my chatbot okay so let me talk about that okay so if you go ahead and click on the output so here is what is the most important thing here you'll be able to see that you get get the HTTP request and you can also get the entire post request right so once you probably get this post request inside this post request you just need to set the API key and then by using whatever payload like whatever is the question you need to include it over here and then by hitting this particular URL and this channel token you can probably see this should be unique some unique name okay so here I will show you okay I'll give you the code also how you can actually hit the post request with the help of Python programming language now what I will do first of all I require an API key right so if in order to create an API key what I will do I will go ahead and click over here and here you can see I will go ahead and click create an API key so let's say I go ahead and write test uh this is my AI project or the rack system and this will be my API key okay please make sure that you save the API key so I'll copy this API key I'll paste it over here okay so this is my API key now what I will do I will go back to my AI project okay I'll go back to my AI project I will take the code the post request okay now see this this entire pipeline I just require one API and with that I can do a post I can do a get and all the functionality I'll do I don't have to even worry about the vector embeddings I don't have to worry about the apis of search Google search or RC or Wikipedia I don't have to worry about it all I'll be worried about this specific API itself right so what I will do the same call P post request that you'll be seeing I have written this in a normal python code okay so here is the entire python code here you can see uh this is my API key so API key I'll just update it this is my older API key that I had actually tried it out but don't use this API key it will be of no use because I'll be deleting it and then here I will write the query what is machine learning then I'll set the headers of content type of application Json the same thing see over here content type is application Json if API key I need to set it up and then the payload which will have my query okay so I will go over here so API key I have set it to API key whatever API key is there my query is what is machine learning and this is my entire data payload all I did I took that curl post I just searched in the chat GPT and it gave me this entire code okay then this is My URL and instead of that catch token I'm writing Krishna 06 a unique value you can put it anything over here and then I'm doing request. poost with this URL with this Json data and headers is equal to headers okay and I'm printing the response. text again see the code it is very simple I'm setting the API key this is my query this is my headers this is my data I'm just setting it up whatever is the post request and now let me go ahead and execute it now if I go ahead and execute it the best thing will be that see I will write python test. py I've asked the question what is machine learning okay now here you'll be able to see that I will be get getting that entire okay text it shows unauthorized okay let's see what is the error um okay I have not saved this one it's okay no worries I not saved it so that is the reason now I will be able to see the response cannot match app with endpoint perfect so still this is not matching because I had already tried it out previously so let me do one thing let me copy this again and then you'll be able to see it okay so I will copy this entire URL and the channel token will be almost same okay so from catch I'm pasting it over here so I'll remove this I think it is dollar catch uh this catch should get removed and this will be my dollar Channel token perfect now let's see okay now it should run what is machine learning and now I think I should be able to get the answer okay so let's wait for the first time it will take time but after that I think it will work absolutely fine so this is done but at the end of the day you can see that it's a simple python code you know a post python code and how you really want to use it from the front end point of view it is up to you you can specifically use it but this is my end point this this is called as an endpoint this is my API key the API key is basically getting appended over here so now here you can see that I'm getting the response machine learning is a sub field of artificial intelligence that involves the creation of algorithms so and so everything is them now this simple post I can include it in any API that I specifically want okay that is the most amazing thing so coding wise you can now see you don't have much dependency now one more thing that I really want to show there is something called as app directory you can also connect with Google Drive Confluence japar slack teams so all these things are coming soon later on you can also work directly with llms and bring your own llm to the platform here you have a AWS Hemer Bedrock hugging face it is also having this entire support So guys in this video we are going to discuss about how we can f tune our own llm models with our own custom data for this purpose we will be specifically using this platform which is called as gradient. now gradient actually provides you a lot of llm models on top of that you can probably do a lot of finetuning with your own custom data set custom data sets can just be formatted in a specific way to give it and later on the fine tuning can really happen in a quicker way so probably when I will be doing the fine tuning it hardly take 7 to 8 minutes this is the demo that I really want to show you in this specific video by doing this you will get an idea if you have a custom data set and you want to also create your own custom llm model you can definitely do it with the help of gradient. a now let's understand what does gradient. a provides you it is a one platform unlimited AI it not only does the fine tuning it helps you to develop it deploy it and even for the inferencing part so over here you can create your own private llms your own models fully unlocked model right and you can build with any languages it provides you three SDK I think JavaScript Python and Java so that is also provided we will be specifically using python hdk we will be using Google collab and we will be finetuning our own model so it is the only AI platform that allows you to combine industry expert AI with your private data and here is all the other information that you can see that it also provides accelate AI transformation deploy AI to solve Mission critical problem 10 into faster with gradients AI Cloud platform we handle the infrastructure for you so it is an amazing platform alog together where you can not only uh build your model even you can inference it you can even find un it so in order to go ahead with first of all you need to go ahead and sign up so for sign up you can probably put your entire details I have already sign out uh signed up so I'm going to probably do the login over here as soon as I do this login it it will ask you to create a new workspace so this is the first important step that you have to do so for creating a new workspace you just need to create like just click on this particular button create a new worksspace you can give your own workspace name and just submit it once you submit it like in this case I have created a test workspace you need to know this specific ID because this ID is nothing but it is a workspace ID so I will copy this workspace ID so that I will be using it in my coding when I probably do it in the Google collab the next thing that is specifically required you need to click on the access tokens right one is the workspace ID and the other one is the access token again to generate the access token I have already generated it I have kept it uh in my coding environment all you need to do is that just click on generate new exess token and you have to just put your password submit it and it will give you the exess token there you just need to do the copy over there so what I will do I will go ahead and uh uh give my password as soon as I do this so here you can probably see that you get your secret access token you can copy it over here one amazing thing about gradient AI is that it provides you all the necessary documentation over here you can just go and refer the documentation it provides you the entire code that is actually required with respect to python SDK and trust me guys just with a minimal lines of code you can probably find tun your entire model in an easier way right so if you probably go to the guides here you have something called as python SDK example so with respect to python hdk you can probably see this is the code for fine tuning I will copy the same code and I'll probably run it but here I'm going to make some changes with respect to my data set along with that you can also have you can also go ahead and see the entire documentation whatever things you specifically require the integration it specifically provides for Lang chain Lama index mongodb H tack so whatever things is specifically required mongodb and all you can probably do the vector dat you can create it as a vector database and probably store all the vectors over there now let me quickly log in again so once I log in if you probably go to the workspace okay so this is the work work space that I actually created okay now inside this work workspace you can see all this are the models that I've have already created for the F tuning purpose if you probably go ahead and click on fine tuning and if you see if I just go ahead and select uh or create fine tuning over here it provides you three base models okay one is the bloom 560 one is Lama 27b chat one is NOS hermas 2 okay so these are different different models uh llm models you can use any one of them for the fine tuning purpose now in our example what I'm actually going to do is that I'm going to use this specific thing and then what will happen is that with the help of python hdk I'm going to do the entire fine tuning process here then I will be able to see my fine tuning job also as I go ahead okay so here I have opened my Google collab let's go ahead step by step and let's see that how you can probably finetune your llm model so let me first of all make this spelling mistake correctly okay now the first step is that we will go ahead and install install gradient AI so for this I will be using pip install gradient AI upgrade so once I execute this over here you'll be able to see that the installation will start taking place now it shows that requirement is already satisfied now the next thing is that the two important information as I said one is the gradient workspace ID I've already shown you from where you can probably get your workspace ID right uh initially in the workspace when we are probably checking it out that particular ID I copied it over here and pasted it over here and the next ID that I really need to set it is my access token gradient access token so I will set up the environment by using import OS and then I'm going to probably use os. environment gradient workspace ID with my workspace ID itself so this is the ID which you will again get it from the gradient itself once you log in and the second thing is about the gradient access token so here also I have pasted the exess token so let me just go ahead and execute it and I have set up this environment variable one is the gradient workspace ID and gradient access token now is the most important thing this is the code of the entire fine tuning so let me go ahead step by step and see like how the fine tuning is basically done so first of all we will go ahead and import from gradient AI import gradient right so this gradient will be responsible in fine tuning in even calling the base model now I've created a function definition main now this is the function initially I have initialized gradient okay inside this gradient you have a method which is called as get base model and here the first parameter is something called as base model slug that basically means what model you are specifically calling and I have already shown you in the fine tuning right if you probably go ahead and create fine tuning through the UI you have this three models that is there Bloom 560 Lama 2 7B chat No hmus 2 so in this case I'm going to specifically use NOS harmas 2 okay now NOS harmas 2 I'm going to do it see you can also do it with the help of UI but I really want to show you with the help of coding by using python SDK so here is the same model I have pasted it over here right so this specific model over here is my llm model that I'm going to f on top of it I'm going to fine tune it with my own custom data now once we created this base model so this line basically gets you the base model over here now this model will be responsible and we will try to create a model adapter right the reason we are creating a model adapter because because this will be specifically my model let's say this is like Kish model over here and this model the reason we are creating as a model adapter because in further we will be doing fine tuning for this particular model itself so the first step is basically to create an adapter on front of it right and the name of that particular model will be uh Kish model itself so this is the second line of the code that we have executed now here we have created the model adapter with the ID new model adapter. ID so we will be getting the ID itself now is the most important step I will try to go ahead and create a query so this is my sample query the instruction see I've given some kind of format the instruction is who is Krishna so this is the question and usually what happens is that in gradient you really need to use a specific format in creating your custom data and for that you can refer this python SDK here you have some tips and tricks it will talk about like how you can probably create your entire uh data itself right with custom data so you can check it out this specific thing and with respect to that you'll get an idea like how your data set should be created in a form right so some format should be there so let's say if I ask any query right for a base model something like this I've given this three hash uh uh and then the instruction this is the question who is Krishna and then we are closing it with two end line okay and then we will be seeing the response so response over here is kept as blank because this response is going to come from this specific new model adapter okay which has the base model NOS Hermos okay now we will be printing this sample query so already it will be just going and printing this who with all all this information like instruction and response now this model Raptor when I say dot complete okay so do complete is a method over here the first parameter I'll be giving my query that is my sample query the max generated token I will be giving 100 so it is going to probably um take 100 Max token in the form of output and then we are going to use do generated output so this is one of the attribute that is present inside this R complete method okay so after this complete method we will be getting the generated output as soon as we get the generated output that basically means whatever questions I have asked over here who is Krishna I'm going to get the output over here itself right so generated before F tuning now here is my output that I'll be getting and understand I have not started my fine tuning so that is the reason I've written a comment over here before fine tuning now before fine tuning if I ask who is Krishna obviously this model will not know unless and until I'm very much popular okay I'm not that much popular so it'll just say that if if it has heard somewhere Krishna it'll give some information now what I will do I will go ahead and create my own sample data okay regarding Krishna because obviously over here in before fine tuning if I probably execute it I will not be able to get the output the right output now this will basically be my custom data so here you can see that the first question that I've asked the first uh sample that first input data that I have created which will be used for training is that who is Krishna then over here the response I've written as krishn is a popular mentor and YouTuber who uploads video on data science and uh llm and his channel channel and in in his channel krishn so this is one of the input and the output that I have given the similar second input I will again create who is this person named Krishna now I'm giving in a different format of text like question and answer right and again the response have written Krishna likes data science and AI make videos in YouTube and he's also a mentor right something like this then instruction again what do you know about Krishna okay so again I've created one more response Krishna is a popular Creator who specializes in the field of data science and his channel name is krishn okay and again one more input like this I can create lot of inputs related to a single Contex now suppose if I have 100 of context I may probably create this kind of inputs and at the end of the day if in a real world scenario in an end to end project if you see this all information will be coming from some kind of databases it can be a mongodb database it can be a SQL database and all right so this will be my sample of data it's just like more like a key value pair you know a list of key value pairs now this is done now what I'm going to Define after this my input is done my model before fine tuning is done now I need to do the fine tuning so for performing the fine tuning I need to have number of epo one parameter I will initialize count is equal to Z now I'm saying while count is less than number of EPO and then I will start my fine tuning over here so fine tuning count + one then I will say new model adapter. finetune is one method where I am giving my samples okay so samples over here this all samples I'll be giving it over here for the fine tuning purpose so in short I'm using a method which is called as DOT fine tune and we are specifically giving this pamp over here and then we are incrementing the count so that the EPO keeps on increasing so first iteration one iteration 2 iteration three and then this entire fine tuning will happen okay now after this I am again writing this comment after fine tuning and I will execute the same method right over here by giving the same sample query who is Krishna right and over here the same thing is then and get the generator output I am going to display the generated output and after doing this I am deleting my adapter because I will not require it afterwards uh if I wanted to use it further I can keep it like that otherwise I can just go ahead and delete it and here also I'm closing the gradient and this is where I'm starting my main so this is in short what we are doing is that before F tuning I really want to see the response I created my sample data over here and then again I did the fine tuning with respect to number of AO and all and now let's go ahead and execute it so once I probably start executing it you can probably see uh it will it will start uh you know it may take hardly a couple of minutes since I've just done three iteration okay so here created model adapter with this particular ID uh the instruction is who is Krishna so it is waiting for the response let's see over here whether it has been created or not uh model testing let's see finetuning so here also it will as soon as the fine tuning will happen here the model will get created and it will got deleted okay it will get deleted so here the response it it is getting that Krishna is a wellknown Indian actor who has prepared in various films uh and television shows he's best known for his uh Raj in the popular I have I've never worked in any industry guys okay so it is saying I'm an Indian actor I'm not an actor altogether okay so this is a very famous serial in India okay I have never acted over there I don't know whom it is considered okay that's also said that I've worked inch H nice so now I am doing the fine tuning so this is the response that I've got generated before fine tuning and now after fine tuning I have given the right kind of data with respect to input and the response uh the third iteration is getting overseas how fast it is happening in the cloud platform gradient Cloud platform that is quite amazing right and this is super super nice at the end of the day you'll be able to see this right beautiful right we already in the third iteration and now here I'm getting my output generated after fine tuning Krish is a popular YouTuber and data scientist is known for his data science python tutorials on his YouTube channel Krish n now see just with this three sentence or four sentence how well it has done the fine tuning now just imagine the power guys right the entire fine tuning is happening in the gradient Cloud itself right if I probably give this kind of data and just increase the number of EPO and train it how beautifully within 5 to 7 minutes you can actually train it and hardly you know just to put my input data it hardly took very less time right so this is one amazing application this is how you can probably do the fine tuning one task I really want to give it to you I okay let's let's reload this I think you'll be able to see my fine tuned model also uh let's see the model uh okay I've already deleted it I think yeah I've deleted it so that is the reason you're not able to see it over here if I had not deleted that you could also see the models over here right so model testing see model is also not here right otherwise you could also see it uh put it over here and you can ask any question you want right this is quite amazing so at the end of the day if you don't want to delete it just don't delete it and keep it like this so I hope you like this particular video I hope you have understood how you can fine tune our amazing llm models uh in with the help of gradient AI Cloud uh which is quite amazing you should definitely use it try to use it and try to see whether you're able to do this or not but uh just by seeing the code and all I think it is quite easy quite amazing again yes this was it for my side I hope you like this particular video I'll see you all in the next video have a great day thank you on all take care byebye
