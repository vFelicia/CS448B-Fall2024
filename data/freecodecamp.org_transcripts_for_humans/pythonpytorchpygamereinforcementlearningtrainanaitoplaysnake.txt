With timestamps:

00:00 - patrick lober is a popular python
00:02 - instructor and in this course he will
00:04 - teach you how to train an artificial
00:06 - intelligence to play a snake game using
00:09 - reinforcement learning hey guys today i
00:12 - have a very exciting project for you we
00:14 - are going to build an ai that teaches
00:16 - itself how to play snake and we will
00:19 - build everything from scratch so we
00:21 - start by creating the game with pygame
00:23 - and then we build an agent and a deep
00:26 - learning algorithm with pie torch i will
00:29 - also teach you the basics of
00:30 - reinforcement learning that we need to
00:32 - understand how all of this works so i
00:35 - think this is going to be pretty cool
00:37 - and now before we start let me show you
00:40 - the final project
00:42 - so i can start the script by saying
00:45 - python agents dot pi now this will start
00:49 - training our agent
00:52 - and here we see our game and then here i
00:55 - also plot the scores and then the
00:58 - average score
01:00 - and now let me also start a stopwatch so
01:04 - that you can see that all of this is
01:05 - happening live
01:07 - and now at this point our snake knows
01:10 - absolutely nothing about the game it
01:12 - only is aware of the environment and
01:14 - tries to make some more or less random
01:17 - moves but with each move and especially
01:20 - with each game it learns more and more
01:23 - and then knows how to play the game and
01:26 - it should get better and better
01:28 - so the first few games you won't see a
01:31 - lot of improvements but don't worry
01:33 - that's absolutely normal i can tell you
01:36 - that it takes around 80 to 100 games
01:39 - until our ai has a good game strategy
01:43 - and this will take around 10 minutes
01:46 - also you don't need a gpu for this so
01:49 - all of this training can happen on this
01:52 - cpu that's totally fine
01:54 - okay so let me speed this up a little
01:56 - bit
01:59 - [Music]
02:07 - [Music]
02:16 - all right so now about 10 minutes have
02:18 - passed and we are at about game 90 i
02:22 - guess and now we can clearly see that
02:25 - our snake knows what it should do so
02:27 - it's more or less going straight for the
02:30 - food and tries not to hit the boundaries
02:33 - so it's not perfect at this point but we
02:36 - can see that it's getting better and
02:38 - better
02:39 - so we also see that the average score
02:43 - here is increasing and now the per the
02:46 - best score so far is
02:49 - and to be honest for me this is super
02:52 - exciting so if you imagine that at the
02:54 - beginning our snake didn't know anything
02:56 - about the game and now with a little bit
02:59 - of math behind the scenes it's clearly
03:02 - following a strategy
03:04 - so this is just super cool don't you
03:06 - think all right so let me speed this up
03:08 - a little bit more
03:13 - [Music]
03:21 - all right so after 12 minutes our snake
03:25 - is getting better and better so i think
03:28 - you can clearly see that our algorithm
03:30 - works
03:31 - so now let me stop this and then let's
03:34 - start with the theory
03:36 - so i will split the series into four
03:39 - parts in this first video we learn a
03:41 - little bit about the theory of
03:43 - reinforcement learning in the second
03:46 - part we implement the actual game or
03:48 - also called the environment here with
03:51 - pygame
03:52 - then we implement the agent so i will
03:54 - tell you what this means in a second and
03:57 - in the last part we implement the actual
03:59 - model with pytorch
04:01 - so let's start with a little bit of
04:04 - theory about reinforcement learning
04:07 - so this is the definition from wikipedia
04:10 - so reinforcement learning is an area of
04:13 - machine learning concerned with how
04:15 - software agents ought to take actions in
04:18 - an environment in order to maximize the
04:21 - notion of cumulative reward so this
04:24 - might sound a little bit complicated so
04:27 - in other words we can also say that
04:29 - reinforcement learning is teaching a
04:31 - software agent how to behave in an
04:34 - environment by telling it how good it's
04:36 - doing
04:37 - so what we should remember here is that
04:39 - we have an a chance so that's basically
04:42 - our computer player then we have an
04:45 - environment so this is our game in this
04:48 - case
04:49 - and then we give the agent a reward so
04:51 - with this we tell it how good it's doing
04:55 - and then based on their reward it should
04:58 - try to find the best next action
05:02 - so yeah that's reinforcement learning
05:05 - and to train the agent there are a lot
05:08 - of different approaches and not all of
05:11 - them involve deep learning but in our
05:14 - case we use deep learning and this is
05:16 - also called steep q learning so this
05:20 - approach extends reinforcement learning
05:22 - by using a deep neural network to
05:25 - predict the actions and that's we're
05:27 - going to use in this tutorial
05:30 - all right so let me show you the rough
05:32 - overview of how i organized the code so
05:35 - as i said we're having four parts so in
05:38 - the next part we implement the game with
05:40 - pie game then we implement the agent and
05:44 - then we implement the model with pie
05:46 - torch
05:48 - so
05:49 - our game has to be assigned such that we
05:52 - have a game loop and then with each
05:55 - game loop we do a play step
05:58 - that gets an action
06:01 - and then it does a step so it moves the
06:04 - snake and then after the move it returns
06:08 - the current reward and if we are game
06:10 - over or not and then also the current
06:13 - score
06:14 - then we have the agent and the agent
06:19 - basically puts everything together so
06:21 - that's why it must know about the game
06:24 - and it also knows about the model so we
06:27 - store both both of them in our agent and
06:30 - then we implement the training loop so
06:33 - this is roughly what we have to do
06:36 - so
06:37 - based on the game we have to calculate a
06:40 - state
06:42 - and then based on the state we um
06:45 - calculate the next action
06:48 - and this involves calling model predict
06:52 - and then with this new action
06:55 - we do a next play step and then as i
06:59 - said we get a reward the game overstate
07:02 - and the score
07:04 - and now with this information we
07:06 - calculate a new state
07:09 - and then we remember all of this so we
07:12 - store the new state and the old state
07:15 - and the game over state and the score
07:18 - and with this we then train our model
07:22 - so for the model i call this linear q
07:25 - net so this is not too complicated this
07:27 - is just a feed forward neural net
07:30 - with a few linear layers and it needs to
07:35 - have the these information so the new
07:38 - state and the old state and then we can
07:41 - train the model and we can call model
07:44 - predict
07:45 - and then this gets us the next action so
07:48 - yeah this is
07:49 - a rough overview how the code should
07:52 - look like and now let's talk about some
07:54 - of those variables in more detail for
07:57 - example the action or the state or the
08:01 - reward
08:02 - so let's start with the reward so that's
08:04 - pretty easy
08:05 - so whenever our snake eats a food we
08:09 - give it a plus 10 reward when we are
08:13 - game over so when we die then we get -10
08:16 - and for everything else we just stay at
08:19 - zero so that's pretty simple
08:21 - then we have the action so the action
08:24 - determines our next move
08:27 - so we could think that we have four
08:30 - different actions so left right
08:33 - up and down
08:35 - but if we design it like this then for
08:38 - example what can happen is if we go
08:41 - right then we might take the action left
08:44 - and then we immediately die so this is
08:47 - basically a 180 degree turn
08:51 - so we don't allow that so a better
08:53 - approach to design the action is to only
08:56 - use
08:58 - three different numbers
09:00 - and now this is dependent on the current
09:04 - direction
09:05 - so
09:06 - um 1 zero zero means we stay in the
09:10 - current direction so we go straight so
09:13 - this means if we go right then we stay
09:15 - right if we go left then we go left and
09:19 - so on
09:20 - then if we have 0 1 0 this means we do a
09:24 - right turn and again this depends on the
09:27 - current direction so if we go right and
09:30 - do a right turn then we go
09:33 - down next if we go down and do a right
09:36 - turn again then we go left and then
09:38 - again we would go up
09:40 - so this is the right turn and the left
09:43 - turn is the other way around so if we go
09:45 - left and do a left turn then we go down
09:48 - and so on so with this approach we
09:51 - cannot do a 180 degree turn and we also
09:55 - we only have to predict three different
09:57 - states so this will make it a little bit
10:00 - easier for our model
10:02 - so now we have the reward and the action
10:05 - then we also need to calculate the state
10:09 - and the state means that we have to tell
10:12 - our snake some information about the
10:14 - game that it knows about so it needs to
10:17 - know about the environment
10:19 - and in this case our state has 11 values
10:24 - so it has the information if the danger
10:26 - is straight or if it's ahead if the
10:29 - danger is right or if the danger is left
10:33 - then it has um the current direction so
10:37 - direction left right up and down
10:41 - and then it has the information if the
10:44 - food is left or right or up or down
10:48 - and all of these are boolean values so
10:51 - let me show you an actual example so in
10:54 - this case
10:55 - if we are going right and our food is
10:59 - here
11:00 - then we see um danger straight right and
11:04 - left none of this is true
11:06 - so
11:07 - for example if our snake is over here at
11:12 - this end and it's still going right
11:16 - then danger straight would be a one
11:21 - so this again also depends on the
11:23 - current direction for example if we move
11:27 - up at this corner here
11:30 - then danger right would be a1
11:33 - then for these directions only one of
11:36 - them is one and the rest is always zero
11:39 - so in this case we have danger right set
11:43 - to one
11:44 - and then for this in our case our food
11:47 - is right of the snake and also down of
11:51 - the snake so food right is one and food
11:54 - down as one all right so now with the
11:57 - state and the action we can design our
12:00 - model so this is just a feed forward
12:03 - neural net with an input layer a hidden
12:07 - layer and an output layer and for the
12:11 - input it gets the state so as i said we
12:14 - have 11 different numbers in our state
12:18 - 11 different boolean values zero or one
12:22 - so we need this size 11 at the beginning
12:25 - then we can choose a hidden um size
12:29 - and for the output we need three outputs
12:33 - because then we predict the action so
12:36 - this can be some numbers
12:39 - and these don't need to be probabilities
12:42 - so here we can have raw numbers and then
12:44 - we simply choose the maximum so for
12:47 - example if we
12:49 - take 1 0 zero and if we go back then we
12:53 - see this would be the action straight so
12:56 - keep the current direction
12:58 - so yeah that's how our model looks like
13:02 - and of course now we have to train the
13:05 - model
13:06 - so for this let's talk a little bit
13:09 - about this deep q learning
13:12 - so q
13:13 - stands for this is the q value and this
13:17 - stands for the quality of the action
13:20 - so this is what we want to improve so
13:23 - each actions should improve the quality
13:26 - of the snake
13:28 - so we start by initializing the q value
13:32 - so in this case we initialize our model
13:35 - with some random parameters
13:37 - then we choose an action by calling
13:41 - model predict state and we also
13:44 - sometimes choose a random move so we do
13:47 - this especially at the beginning when we
13:49 - don't know a lot about the game yet
13:52 - so
13:53 - and then later we have to do a trade-off
13:56 - when we don't want to do a random move
13:59 - anymore and only call model predict
14:03 - and this is also called a trade-off
14:06 - between exploration and exploitation
14:10 - so this will get clearer later when we
14:12 - do the actual coding
14:15 - so then with this new action we perform
14:18 - this action so we perform the next move
14:21 - and then we measure the reward and with
14:24 - this information we can update our q
14:27 - value and then train the model and then
14:30 - we repeat this step so this is an
14:32 - iterative
14:34 - training loop so now to train the model
14:37 - as always we need to have some kind of
14:41 - loss function that we want to optimize
14:44 - or minimize so for the loss function we
14:47 - have to look at a little bit of math and
14:51 - for this i want to present you the
14:53 - so-called belmont equation
14:55 - so this might look scary so don't be
14:59 - scared here i will explain everything
15:01 - and actually it's not that difficult
15:03 - when we um understand this and then code
15:06 - this later
15:07 - so
15:08 - what we want to do here we need to
15:10 - update the q value as i said here so
15:14 - according to the belmont equation the
15:16 - new q value is calculated like this so
15:19 - we have the current q value
15:22 - plus the
15:24 - learning rate and then we have the
15:26 - reward for taking that action at that
15:29 - state
15:30 - plus a gamma parameter which is called
15:33 - this count rate so don't worry about
15:36 - this i will also show this later in the
15:38 - code again
15:39 - and then we take the maximum expected
15:42 - future reward
15:44 - given the new state and all possible
15:47 - actions at that new state
15:50 - so
15:51 - yeah this looks scary but i will
15:53 - simplify that for you and then it's
15:55 - actually not that difficult
15:57 - so the old q value is model predict with
16:01 - state 0
16:04 - so if we go back at
16:06 - this overview so the first time we say
16:09 - get state from the game this is our
16:12 - state 0
16:14 - and then after we took this place step
16:17 - we again measure or calculate the next
16:20 - state so this is then our state one
16:24 - so with this information again our first
16:28 - queue is
16:29 - just model predict with the old state
16:33 - and then the new queue is the reward
16:35 - plus our gamma value
16:38 - times the maximum value of the
16:41 - q state so again this is model predict
16:45 - but this time we take state one
16:48 - and then with these two information our
16:51 - loss is simply
16:53 - the q new minus q squared
16:56 - and yeah this is nothing else than the
16:59 - mean squared error so that's a very
17:02 - simple error that we should already know
17:04 - about and then this is what we must use
17:07 - in our optimization
17:09 - so yeah that's what we are going to use
17:11 - so we have to
17:13 - implement all of these three classes
17:17 - and in the next video we start by
17:20 - implementing the game
17:26 - in the last part i showed you all the
17:28 - necessary theory that we need to know to
17:31 - get started with deep q learning
17:34 - and now we start implementing all of the
17:36 - parts so as i said we need to have a
17:39 - game so the environment then we need an
17:42 - agent and we need a model
17:45 - so in this part we start by implementing
17:48 - the game and we use pytorch for this
17:52 - so
17:52 - let me actually start by creating a
17:56 - environment and we install all the
17:58 - necessary dependencies that we need
18:02 - so in this case i use conda to manage
18:06 - the environments and if you don't know
18:08 - how to use conda then i have a tutorial
18:11 - for you that i will link here
18:13 - but yeah if you don't want to use connor
18:16 - you can also just use a normal virtual
18:18 - and but i recommend to use a virtual and
18:22 - and now let's create a virtual and with
18:26 - conda create minus n and then give it a
18:29 - name for example pi game n and i also
18:33 - say i want python equals 3.7
18:38 - all right so now this was created so now
18:41 - we want to actuate it with conda
18:44 - activate and then pie game n and hit
18:47 - enter and then we see the name of the
18:49 - environment in the front so this means
18:52 - that we activated it successfully and
18:54 - now we can start installing all what we
18:57 - need so the first thing we want to
18:59 - install is pie game for our game so pip
19:04 - install pie game and hit enter
19:08 - so this is done the next thing we need
19:11 - is pytorch for our model later so for
19:15 - this we can go to the official home page
19:18 - and on install
19:20 - and then here you can select your
19:22 - operating system so i use mac
19:25 - and i actually i want to say pip install
19:29 - and we don't need cuda support so only a
19:32 - cpu is fine
19:34 - and we don't need torch audio because we
19:37 - don't work with audio files so we can
19:39 - only grab this pip install torch torch
19:43 - vision
19:44 - and then paste it in here and hit enter
19:47 - and now this installs pytorch and all
19:50 - the dependencies
19:52 - all right so this is done and then we
19:55 - need two more things for plotting later
19:58 - so for this i say pip install
20:01 - much plot lip and we also want i python
20:05 - and then hit enter
20:08 - all right so this was successful as well
20:10 - and now we have everything we need so
20:13 - now we can start implementing all the
20:15 - codes and as a starting point i want to
20:19 - grab the code from another tutorial that
20:22 - i did so you can find this on github and
20:24 - then on my um account and then in the
20:28 - repo python fun
20:30 - and here i actually have two snake games
20:33 - so and then we need this one snake pie
20:36 - game
20:37 - and download this so you can do this and
20:41 - i already did this and have this here so
20:44 - if we open up the
20:47 - editor here i'm visuals using visual
20:50 - studio code
20:51 - then we can see we have exactly those
20:54 - two files
20:56 - and
20:57 - then um the first thing i want to do is
21:00 - i want to run this file and test if this
21:04 - is actually working
21:05 - so right now this is just a normal
21:09 - snake game that you have to control
21:11 - yourself so you have to
21:13 - use the arrow keys so let's say python
21:17 - snake game dot pi and then let's hope
21:21 - that this is working so yeah so now i
21:24 - can control the snake
21:26 - and i hope that i can eat the food yes
21:29 - and now if i hit the boundary then we
21:32 - are game over
21:34 - so this is working our environment is
21:36 - set up and now we can start implementing
21:40 - our code so we can change this so that
21:43 - we can use this as a
21:46 - ai controlled game
21:48 - so let me show you the overview from
21:51 - last time so last time i told you that
21:54 - we need a play step in our game and this
21:58 - gets an action and based on this action
22:02 - we then take a move and then we must
22:05 - return a reward the game over state and
22:08 - the current score
22:09 - so
22:10 - first let's
22:12 - write down all the things that we need
22:14 - to change here
22:16 - so first we want to have a reset
22:20 - function so after each
22:22 - um game our agent should be able to
22:26 - reset the game and start with a new game
22:29 - then we need to
22:32 - implement the reward that our agent gets
22:36 - then we need to change the play function
22:40 - so that it takes takes an action and
22:43 - then um
22:45 - returns a or
22:47 - computes the direction
22:49 - then we also want to keep track of the
22:53 - current frame or let's call this
22:56 - game
22:57 - iteration
22:59 - and for later we also need to have a
23:02 - change in the if is collision function
23:06 - to check if this is a collision
23:09 - so
23:10 - first let's
23:11 - let me go over this code quickly so what
23:15 - we do here is we use pi game
23:18 - then for the direction we use an enum
23:22 - then for the point we use a named tuple
23:26 - and then here i created a class snake
23:30 - game and here we initialize the things
23:33 - we need for the game
23:35 - so here we initialize the game state for
23:38 - example for the snake we use a
23:41 - list with
23:43 - three initial values
23:45 - and the head is always the front of this
23:48 - list
23:49 - then we keep track of the score and here
23:52 - we have a helper function to place the
23:55 - food
23:56 - and yeah and we already
23:59 - have a function that is called play step
24:03 - and then if we go down to the very end
24:06 - so here we have our
24:08 - game loop so while this is true we take
24:11 - a game or a play step
24:14 - and we get the game over state and the
24:16 - score so this place the function is the
24:20 - most important one so here first we
24:23 - right now we grab the user input so the
24:27 - key we press
24:28 - then we calculate a move based on this
24:32 - key
24:33 - and then we update our snake and check
24:37 - if we are game over and
24:39 - if we can continue we place the new food
24:42 - or check if we eat the food
24:45 - and we update our ui with this
24:49 - helper function update ui then here we
24:52 - have this helper function is collision
24:56 - where we check if we either hit the
24:58 - boundary or we run into ourself
25:02 - and then we also have this helper
25:05 - function move where we get the current
25:08 - direction and then based on this
25:10 - direction we simply um calculate
25:14 - calculate the new position of the new
25:16 - hat
25:18 - so yeah that's all um
25:21 - what is done here and now let's change a
25:24 - few things though so the first one i
25:26 - want to change the class name to say
25:29 - snake game ai to make it clear that this
25:32 - is a agent controlled game
25:36 - and now so the first thing we want is
25:39 - the reset functionality so in here um i
25:44 - already have this comment where we in it
25:48 - the game state so now we want to
25:50 - refactor all of this into a
25:54 - reset function so we create a new
25:56 - function define
25:58 - and then let's call this reset and it
26:02 - only gets self and no other arguments
26:07 - and here we can grab all of this code
26:11 - and then simply paste it in here
26:14 - and in our
26:16 - initializer we then call self dot reset
26:21 - so this is the first thing we need
26:24 - additionally we want to keep track of
26:27 - the um
26:29 - game iteration or frame iteration so
26:32 - let's call this self dot frame
26:36 - iteration
26:39 - and in the beginning this is just zero
26:42 - then this define place food can stay as
26:46 - it is and now we need to change the play
26:49 - step function so first of all
26:52 - if we have a look at the overview
26:56 - here i already told you that now we need
26:58 - to give this the action from the agent
27:02 - and we need to return a reward so let's
27:06 - start by
27:08 - um using this action parameter
27:12 - and here we grab the user input so
27:16 - actually right now we can get rid of
27:18 - this so the only thing we still check if
27:22 - we want to quit the game and now here um
27:26 - we already
27:29 - have this helper function where we move
27:32 - in the current direction
27:34 - so actually what we change here now this
27:37 - move function doesn't get the direction
27:40 - from the user input so now here it gets
27:43 - the action and then we have to determine
27:46 - the new direction
27:48 - so we do this in a second but first
27:51 - let's only change this and then here we
27:54 - call the self.move
27:56 - with the action
27:59 - and then we update the head then we
28:02 - check if we are game over or not and we
28:06 - actually now we also need the reward so
28:10 - we simply say reward equals zero
28:13 - and let's go back to the slides from
28:16 - last time so the reward is really simple
28:19 - whenever we eat a food we say plus 10
28:22 - when we lose or when we die then we say
28:24 - our reward is -10 and for everything
28:27 - else we just stay at zero
28:30 - so we initialize the reward with zero
28:33 - then if we have a collision and game
28:36 - over then we say our
28:39 - reward equals to -10 and we want to
28:43 - return this as well so return the reward
28:47 - game over and self.score
28:51 - and here we check only if we have a
28:54 - collision so here i actually want to do
28:57 - another
28:59 - check so if
29:01 - nothing happens for a long time so if
29:04 - our snake doesn't improve and doesn't
29:07 - eat the food but also doesn't die then
29:10 - we also want to check this
29:13 - and if this happens for a too long time
29:16 - then we also break here
29:18 - so we can say or and then here we say if
29:22 - self dot frame iteration
29:25 - and if that this gets too large without
29:28 - anything happening then we um stop here
29:33 - so here i use this little formula if
29:36 - this is greater than 100 times the
29:40 - length of our snake so remember this is
29:43 - a list
29:45 - then we break so this is also like this
29:48 - then it's dependent on the
29:50 - length of the snake
29:52 - so the longer our snake is the more time
29:56 - it has
29:58 - so but then if it gets larger than this
30:00 - value then we break
30:03 - and of course we have to
30:05 - update the self.frame iteration and we
30:08 - can simply do this here at the beginning
30:11 - so for each
30:13 - play step we say self dot frame
30:16 - iteration plus
30:18 - equals
30:20 - 1
30:21 - and when we reset it then we reset it
30:25 - back to zero
30:26 - so this is here and then yeah if we stop
30:30 - we have the reward -10
30:33 - then here if our hat
30:36 - hits the food then we eat the food so
30:38 - our score increases and our reward is
30:42 - set to plus 10
30:44 - then we place a new food and say
30:47 - otherwise we remove the last part so we
30:50 - simply move here
30:52 - then this can stay as it is the update
30:55 - function and at the very end we also
30:58 - want to return the reward then
31:01 - for the is collision function we need a
31:05 - slight change so here i only check for
31:08 - self.head but later um to calculate the
31:13 - state or the danger which i told you
31:17 - about so if we
31:19 - have a look at the
31:21 - state so here we calculate the
31:24 - danger so if we are for example if we
31:27 - are here at the corner then we have a
31:30 - danger at the right
31:33 - so for this it might be handy if we
31:36 - don't use self.head inside here
31:39 - but if we give this function a point so
31:43 - this gets the point argument and let's
31:46 - say by default this is none
31:49 - and then here we simply ch check if the
31:51 - point is
31:53 - none then we set the point equals to
31:57 - self dot head
31:59 - so inside this where we call this with
32:02 - no argument it can stay as it is
32:06 - and then here of course we have to
32:09 - change self.head to this is now our
32:12 - point so here if we hit the
32:16 - corner
32:17 - point here and point here and point
32:23 - here
32:24 - then we have a collision and here if our
32:28 - point is in the snake body then we also
32:32 - have a collision
32:33 - and otherwise we don't have a collision
32:36 - all right so the update ui function can
32:39 - stay like this
32:41 - and now for the move function here we
32:43 - need to change something so now we get a
32:47 - action and now based on this action we
32:51 - want to determine the next move
32:54 - so if we go back to the slides so here
32:58 - we designed the action like this so it
33:01 - has three values
33:04 - um one zero zero means we keep the
33:07 - current direction and go straight 0 1 0
33:11 - means we do a right turn and 0 0 1 means
33:15 - we do a left turn
33:17 - so this is dependent on the current
33:20 - direction so if we go right and do a
33:23 - right turn then we go down next if we go
33:27 - down and do a right turn then we go left
33:30 - next and so on and left turn is the
33:32 - other way around
33:34 - so now um we want to determine the
33:38 - direction based on the action so let's
33:42 - write a quick comment here we
33:45 - have straight right turn or left turn so
33:50 - to get the next direction first i want
33:54 - to
33:55 - define all the possible directions in a
33:59 - clockwise order so we say clockwise
34:03 - equals and then a list and here we start
34:07 - with direction dot right so
34:10 - here remember for the direction we use
34:14 - this enum class
34:16 - so it has to be one of those directions
34:20 - so
34:21 - our
34:22 - clockwise directions should start with
34:25 - direction right then from this on the
34:28 - next one is
34:30 - direction dot down
34:33 - then we have direction dot
34:36 - left and as last thing we have direction
34:39 - dot up so right down left up this is
34:43 - clockwise and then to get the current
34:46 - direction or the current index of the
34:50 - current direction we say index equals
34:53 - and then we can say clockwise dot index
34:57 - and then the index of the self dot
35:00 - direction so we are sure that this has
35:04 - to be in this array we because the self
35:06 - direction
35:08 - must be one of those enum values
35:11 - and then we check that different um
35:16 - possible states so these ones
35:19 - so for this we can use numpy and i guess
35:23 - we have to import numpy first as np
35:29 - and then we can use it
35:31 - here we can say if
35:33 - numpy and then we use this function
35:36 - array equal and then here we put in the
35:40 - action and the array that we want to
35:43 - compare
35:44 - so if this is equal to
35:47 - one zero zero
35:49 - then we go straight or we keep the
35:53 - current directions so we simply say
35:56 - our new
35:58 - direction
36:00 - equals and then clockwise
36:03 - of the
36:04 - index and then remember the index is
36:07 - just the index of the current direction
36:10 - so here we basically have no
36:13 - change then we say
36:15 - l if if our
36:18 - array if numpy array equal if the action
36:23 - equals to 0 one zero then we do a
36:28 - right turn
36:30 - so this means we go clockwise so if we
36:34 - go right then the next direction would
36:36 - be down if we go down then the next
36:40 - direction would be left and if we go
36:42 - left then the next
36:44 - direction would be up so here we say
36:48 - index
36:49 - equals or this is our next
36:53 - index actually and here we say
36:56 - this is the current index plus
37:00 - 1 but then modulo 4 so this means if we
37:05 - are at the end up and then do the next
37:08 - one if we have index
37:11 - so this is index 0 1 2 3 and then if we
37:15 - have index
37:17 - 4 modulo 4 is actually again index
37:21 - zero again so from this we do a turn and
37:25 - then come back at the front again
37:28 - so this is our right turn so now this is
37:31 - the
37:32 - next index and now our new direction is
37:35 - clockwise of the
37:38 - next
37:40 - index
37:41 - and then otherwise we can simply use
37:44 - else here and actually change this to an
37:48 - l if so now this is the last case so it
37:52 - has to be here it has to be
37:55 - zero zero one
37:57 - and if this is the case then let's copy
38:01 - and paste this in here then our next
38:05 - index is the current index
38:08 - minus one modulo four
38:10 - so this actually means we go
38:12 - counter clockwise so we do a
38:16 - left turn so if we start with right then
38:20 - the next move would be up and then the
38:22 - next would be
38:24 - left and then the next would be down
38:27 - and then right again and so on so now
38:29 - this is our new direction and then
38:32 - simply we say self direction
38:35 - equals new
38:37 - direction
38:39 - and then we go on so here we extract the
38:43 - head
38:44 - and then here we have to check if self
38:48 - dot's direction now is right then we
38:51 - increase the position of x
38:55 - and so on if we um have the left
38:58 - direction then we decrease x
39:01 - and if we go down then we actually
39:04 - increase y so for
39:08 - so the y starts at the top at zero and
39:11 - then increases if we go down so if we go
39:15 - down then we have to increase y and if
39:18 - we go up then we have to decrease y so
39:21 - if self direction equals up
39:24 - then y minus equals the block size and
39:28 - by the way the block size is just here a
39:30 - constant value of 10 so that's how big
39:33 - our one block of the snake should be in
39:36 - pixels
39:38 - so yeah this is everything we need here
39:41 - in the move function and now here
39:45 - we don't need this anymore so
39:49 - this is actually no longer working with
39:52 - a user input so you can just
39:55 - delete this and then later we control
39:58 - this class from the agent
40:01 - and call this play step function
40:04 - so yeah for now this is all we need to
40:06 - implement the game
40:12 - so i already talked about the theory of
40:15 - deep q learning in the first part
40:18 - in the last part we implemented the pi
40:21 - game so that we can use it in our
40:24 - agent controlled environment and
40:28 - now we need the agent so let's start and
40:32 - so here um if you haven't watched the
40:35 - first two parts then i highly recommend
40:37 - to do so
40:38 - so this is the starting point from last
40:41 - time and i actually want to make one
40:44 - more change that i forgot
40:46 - so
40:47 - here the is collision function should
40:50 - actually be public because then our
40:53 - agent should use it
40:55 - so just remove the underscore here and
40:59 - then also remove it in this class itself
41:02 - when we call this
41:04 - so then we have our snake game and i
41:07 - also want to rename this to just be game
41:11 - and now we create a new file agent dot
41:15 - pi and then start implementing this
41:19 - so first here we import torch from pi
41:22 - torch
41:23 - then we import random because when later
41:27 - we need this then we also need
41:29 - import numpy snp
41:33 - and from our um implemented class we
41:38 - need the snake game so we say from
41:42 - game import snake
41:45 - like snake game a i
41:49 - so i think that's what we call this
41:51 - class snake game a
41:55 - i
41:56 - so yeah that's the right name then we
41:59 - also hear at the beginning we defined
42:02 - this enum for the direction and this
42:06 - named tuple for the point which has an x
42:10 - and a y attribute
42:12 - so we also want to import these two um
42:16 - things so we import direction and we
42:19 - import point and then we also say from
42:24 - collections
42:26 - we want to import deck so this is a data
42:30 - structure where we want to store our
42:33 - memories so
42:35 - um if you don't know what a deck is then
42:38 - i will put a link in the description
42:41 - below
42:42 - so this is really handy in this case and
42:45 - you will see why this is the case later
42:48 - and then here i want to define some
42:51 - parameters as constants so we have a
42:54 - maximum memory of let's say 100
42:59 - 000
43:00 - so we can store 100 000 items in this
43:04 - memory
43:05 - then we also want to to use a batch size
43:10 - that you will see later and here i will
43:12 - set this to 1000 so you can play around
43:15 - with these parameters
43:17 - and i also want a learning rate later
43:21 - and i want to set this to 0 0 1
43:25 - and yeah feel free to change this and
43:27 - then we start creating our class agent
43:32 - and it gets of course an init function
43:35 - with self and no other
43:38 - arguments and then let's have a look at
43:42 - the slides from the first part where i
43:45 - explained the training
43:48 - so we want to create a training function
43:51 - where we do all of this so we need to
43:54 - get the state calculate the state
43:57 - where we are aware of the current
43:59 - environment then we need to calculate
44:03 - the next move from the state
44:06 - and we need to
44:09 - um then we want to update or do the next
44:12 - step and call game.playstep and then
44:14 - calculate the new state again then we
44:17 - want to store everything in memory and
44:20 - then we also want to train our model so
44:23 - we need to store the game and the model
44:26 - in this class
44:28 - so first of all let me create the
44:30 - functions that we need first so we need
44:33 - a function get state which gets self and
44:38 - this this gets the game
44:41 - and then we calculate the state that i
44:44 - showed you with these 11 different
44:46 - variables then we want to have a
44:49 - function that we call remember
44:52 - remember and it has self and here we
44:56 - want to put in the state then the action
45:00 - then we want to remember the reward for
45:03 - this action and we want to calculate or
45:06 - we want to store the next state
45:09 - next state
45:11 - and we also want to store done or bit or
45:15 - you can also call this game over so this
45:18 - is the current game overstate then we
45:22 - need two different functions to train
45:25 - and we call this defined train
45:29 - on the long memory and it only needs
45:32 - self so i will explain this later
45:35 - and we also let's copy and paste this i
45:39 - also have a function define train on
45:44 - short memory so this is only with one
45:47 - step
45:48 - you will see this later
45:50 - then we need a function and we call this
45:54 - get action to get the action based on
45:58 - the state so it gets self and the state
46:02 - and first we only say pass
46:05 - and these are all the functions we need
46:08 - i guess and then i want to have a global
46:12 - function that i call simply train
46:16 - and here we say pass and then when we
46:20 - start this module h and dot pi so we say
46:24 - if name
46:25 - underscore equals equals
46:28 - main then we simply call this train
46:32 - function and then we can start the
46:34 - script by saying python agent dot pi
46:37 - like i did in the very first tutorial so
46:40 - let's start implementing the agent and
46:43 - the training function so let's start
46:46 - with the init function of the agent so
46:49 - here what i want to store is first i
46:52 - want to store some more parameters so
46:56 - self.number of games so i want to keep
47:00 - track of this so this is zero in the
47:02 - beginning
47:03 - then self.epsilon
47:06 - equals um zero in the beginning this is
47:10 - a parameter to control the
47:13 - randomness so you will see this later
47:17 - then we also need self dot gamma equals
47:22 - zero
47:23 - so this is
47:25 - this is the so-called this count rate
47:29 - which i briefly showed in the first
47:31 - tutorial i will explain this a little
47:33 - bit more in the next tutorial where we
47:36 - implement the model and the actual deep
47:38 - q learning algorithm then we want to
47:41 - have a memory so we say
47:43 - self.memory equals and for this we use
47:47 - this stack and this can have a argument
47:51 - max leng equals
47:54 - and here we say max memory
47:57 - and what then happens if we exceed this
48:00 - memory then it will automatically remove
48:04 - elements from the left so then it will
48:07 - call pop left for us and that's why this
48:10 - deck is really handy here
48:12 - and then later here we also want to have
48:15 - our model and the trainer so i will
48:19 - leave this for a or s to do for the last
48:22 - part in the next video and now this is
48:25 - all for the init function and now we can
48:28 - go back and now let's do the training
48:31 - function next so again let's have a look
48:35 - at these slides so we need
48:38 - these
48:39 - functions in this order
48:42 - so
48:43 - let's first let's write some comments
48:46 - of first let's create some lists to keep
48:50 - track of the scores so this is an empty
48:53 - list in the beginning and this is used
48:55 - for plotting later
48:57 - so then we also want to keep track of
49:01 - the mean scores or average scores this
49:04 - is also an empty list in the beginning
49:07 - then our total score equals zero in the
49:11 - beginning
49:12 - our record our best score is zero in the
49:16 - beginning
49:17 - then we set up a agent so agent equals
49:22 - an agent
49:24 - and we also need to create a game so the
49:28 - game is a snake game ai object
49:33 - and then here we create our training
49:37 - loop so we say while true so this should
49:40 - basically run forever until we quit the
49:44 - script
49:45 - and now here let's write some comments
49:48 - so we want to get
49:50 - get the old state or the current state
49:54 - so here let's say state old equals and
49:58 - then we call agent dot
50:01 - get states and this gets the game so we
50:05 - already
50:06 - set this up correctly we only have to
50:09 - implement it then
50:10 - then after this we want to get the move
50:14 - based on this current state
50:17 - so we say the final move equals agent
50:22 - dot
50:23 - get
50:24 - action so we actually called this action
50:27 - and the action is based on the state
50:30 - then with this move we want to perform
50:34 - the move and then and get new state so
50:39 - for this we say
50:41 - reward um
50:43 - done and score
50:46 - equals and here we call game dot play
50:50 - step from last time
50:52 - so
50:53 - i think
50:54 - game dot play step with the action yes
50:58 - game dot play step and this gets the
51:02 - final move
51:04 - and then we get the state
51:06 - old or the new now the new state state
51:10 - new
51:11 - state new equals agent and again gets
51:15 - state now with the new game
51:19 - then after that we want to train the
51:22 - short memory of the agent so only for
51:25 - one step
51:27 - so
51:28 - for this we say agent agent dot train
51:33 - short memory
51:35 - and this gets if we have a look here um
51:40 - actually uh this short memory should get
51:44 - some parameters so exactly the same as
51:47 - we put in the remember function so train
51:50 - short memory gets all of those variables
51:55 - and then here when we call this now we
51:57 - should get some hints strain
52:00 - or let's
52:03 - save this file and then say agent dot
52:06 - train short memory and now we should get
52:10 - the hints no we don't get this but
52:12 - actually we want to have the
52:14 - state action reward next state and done
52:18 - so here let's do this so say let's say
52:21 - state old then the action which was the
52:25 - final move then the reward
52:29 - then the state new and adds last thing
52:33 - the done or game over state variable so
52:37 - now we have this then we want to
52:39 - remember all of these and store this in
52:42 - the memory so we say agent dot remember
52:47 - and then here it gets the same
52:51 - um variables so we want to store all of
52:54 - this in our deck and
52:57 - then this is all we need so now we check
53:00 - if
53:01 - done or if game over then if this is
53:05 - true
53:06 - then what we want to do is um
53:10 - we want to let's write a comment train
53:13 - the long memory and this is also called
53:16 - replay memory or experience replay and
53:21 - this is very important for our agent so
53:24 - now it trains again on all the previous
53:28 - moves and games that it played
53:31 - and this tremendously helps him to
53:34 - improve itself
53:36 - and we also here want to plot the
53:39 - results so first of all we want to reset
53:43 - the game so we can simply do this by
53:46 - saying game dot reset we already have
53:49 - this function
53:50 - here so this initializes the game state
53:54 - and resets everything so the score the
53:57 - snake the frame iteration and places the
54:00 - initial snake and the food
54:02 - so now we have this then we want to
54:06 - increase agents dot number of games so
54:11 - this
54:12 - plus equals one
54:14 - then we want to say agent dot train long
54:19 - memory and this doesn't need any
54:21 - arguments then we want to check if we
54:23 - have a new high score so if score
54:26 - greater than the current record then we
54:31 - say record equals our new score
54:35 - and we will also want to leave this as a
54:39 - to do here so here we want to say agent
54:43 - dot model
54:45 - dot save later when we have the model
54:49 - and so here in the here we want to store
54:52 - this as self.model
54:55 - and now what we also want to do here um
54:59 - let's print some information so print
55:02 - the
55:03 - game and then the current number and
55:06 - then the score and the record
55:09 - so here let's say our game is agent dot
55:14 - n
55:15 - games
55:16 - then we also want to plot the or print
55:19 - the score so this is just the score
55:23 - and we want to print the current record
55:26 - so the record equals record
55:29 - and then here we want to do some
55:32 - plotting so i will implement this in the
55:36 - next tutorial so i will leave this s8 to
55:38 - do
55:39 - so this is all for our training function
55:44 - so what i showed in the slides
55:48 - and now of course we have to implement
55:52 - those functions
55:54 - so for the
55:56 - get
55:56 - state function um let's go back to this
56:01 - overview
56:02 - and here as i said we store 11 values so
56:07 - if the danger is
56:09 - straight right or left then the current
56:13 - direction so only one of these is one
56:16 - and then the position of the food if
56:19 - it's left of the snake right of the
56:21 - snake up or down of the snake
56:24 - so these are the 11 states and now let
56:28 - me actually copy and paste the code in
56:32 - here so that i don't make any
56:35 - mistakes
56:36 - but we will go over this
56:39 - so first let's grab the head from this
56:43 - game so we can do this by calling game
56:46 - dot snake zero so this is a list and the
56:49 - first item is our head
56:52 - then um let's create some points
56:56 - next to this head in all directions that
57:00 - we need to check if this hits the
57:02 - boundary and if this is a danger
57:05 - so for this we can use this named tuple
57:09 - so we can create a point
57:11 - with this location but minus 20 so the
57:15 - 20 is hard coded here so this is the
57:18 - number that i used for the block size
57:22 - so like this we create four points
57:25 - around the head then the current
57:29 - direction is simply a boolean where we
57:32 - check if the current game direction
57:35 - equals to one of those
57:38 - so only one of those is one and the
57:41 - other one is
57:43 - zero or false
57:45 - and then
57:46 - um we create this
57:49 - array or this list with this 11 um
57:53 - states so
57:55 - here we check
57:57 - that if the danger is straight or ahead
58:01 - and this is dependent on the current
58:03 - direction so if we are going right
58:07 - and the point right of us gives us a
58:11 - collision
58:13 - then we have a danger the same or or if
58:18 - we go left and our
58:21 - left point gets a collision then we also
58:24 - have a danger here and so on so this is
58:28 - dangerous straight and then danger right
58:31 - means if we go up
58:34 - and the point right of us would give a
58:37 - collision then we have a danger for a
58:40 - right turn basically
58:42 - and so on and the same for the left so
58:45 - this might be a little bit tricky so i
58:47 - recommend that you pause here and go
58:50 - over this logic for yourself again
58:53 - so yeah these only have give us three
58:57 - values in our state so far
58:59 - then we have the move direction where
59:02 - only one of them is true and the other
59:05 - one is false
59:06 - and for the food location we simply
59:09 - check if food if game food x is
59:14 - smaller than game head x then we have
59:18 - food is left of us
59:20 - and the same way we check for right up
59:23 - and down and then we convert our list to
59:27 - a numpy array and say the data type is
59:31 - in so this is a nice little trick to
59:34 - convert this true or false booleans to a
59:38 - zero or one
59:40 - so yeah now this is the get state method
59:43 - now let's move on to the remember
59:46 - function so here we want to remember all
59:49 - of this in our memory so this is a deck
59:53 - and this is very simple so here we say
59:56 - self dot memory and then the deck has
60:00 - also the append function where we want
60:03 - to append all of this
60:06 - in this order so the state the action
60:09 - the reward the next state and the game
60:12 - over state
60:13 - and as i said if this exceeds the
60:16 - maximum memory
60:18 - then pop left if
60:21 - max
60:23 - mem
60:24 - memory is
60:26 - reached and yeah this is the remember
60:29 - function
60:31 - then let's start implementing the train
60:34 - long and short memory functions so for
60:37 - this so we actually we store a model and
60:42 - a trainer in here so let's actually say
60:45 - self dot
60:47 - model equals let's say this is only none
60:50 - in the beginning and leave a to do
60:53 - and self dot trainer equals none in the
60:57 - beginning and this is a to do
61:00 - so these are objects that we create in
61:03 - the next tutorial
61:05 - and then here
61:06 - we call this trainer to actually do the
61:10 - optimization
61:12 - so let's start here so for only one step
61:16 - we say
61:17 - self.trainer and then this should get a
61:20 - function that we call let's call this
61:23 - train step
61:25 - and then it gets all of these variables
61:28 - so the state the action the reward the
61:31 - next state and the game overstate and
61:34 - this is all that we need to train it for
61:36 - only one game step
61:38 - and we design this function um so that
61:43 - it takes either only one state like this
61:47 - but it can also take a whole tensor or a
61:50 - numpy array and then uses multiple as a
61:55 - so-called batch so let's do this here so
61:58 - for this we take the variables from our
62:02 - memory
62:03 - so here we want to grab a batch and so
62:06 - in the beginning we defined the batch
62:09 - size is 1 000 so we want to grab 1 000
62:14 - samples from our memory
62:16 - but first we check if we um already have
62:21 - a thousand samples in our memory
62:24 - so we say if lang and self dot memory if
62:29 - this is
62:30 - smaller
62:31 - then the batch size then we simply
62:35 - or actually let's say if this is greater
62:39 - so if this is greater than we want to
62:41 - have a random sample and say
62:45 - mini
62:46 - sample equals and then we want to get a
62:49 - random sample so we can use random dot
62:54 - sample so we already imported the random
62:57 - module
62:58 - random dot sample from self dot
63:02 - memory and as a size it should have the
63:06 - batch size
63:08 - so this will return a list of tuples and
63:13 - this is because here i forgot one
63:16 - important thing so when we want to store
63:20 - this and append this we want to append
63:23 - this as only one element so only one
63:27 - tuple so we need extra parenthesis here
63:31 - so this is one tuple that we store
63:34 - and then here we get
63:37 - the batch size number of tuples
63:40 - and otherwise else if we don't have uh
63:45 - a thousand elements yet then we simply
63:47 - take the whole memory so we say mini
63:50 - sample equals self dot
63:54 - memory and
63:56 - then we again want to call this training
64:00 - step and for this so let's call this
64:03 - here again self.trainer.trainstep
64:07 - but here we have
64:08 - multiple states so let's call this
64:12 - states actions rewards next states and
64:16 - done
64:18 - and right now so now we have it in
64:22 - this format that we have one
64:25 - tuple after another
64:28 - and now we want to extract this from the
64:31 - mini sample and then put every states
64:34 - together every action together every
64:37 - reward to it together and so on and this
64:40 - is actually a really simple with python
64:43 - so we can say we want to extract the
64:46 - states the actions
64:49 - the rewards the next
64:52 - states and the
64:54 - duns game overs
64:56 - and here we simply use the built in sip
65:00 - function and have to use one asterisk
65:04 - and then the mini sample argument
65:09 - so yeah check that for yourself if you
65:12 - don't know how the sip function works
65:14 - but again it now it puts every states
65:18 - together every actions and so on if this
65:22 - is too complicated for you then you can
65:24 - also just do a for loop so you can
65:27 - iterate over this mini sample and
65:30 - basically say for
65:32 - action
65:34 - or for state
65:37 - action
65:38 - rewards
65:39 - next state and done in one mini sample
65:45 - and then again you call
65:48 - this here for only one for only one
65:51 - argument so yeah you can do it both ways
65:54 - but actually i recommend to do it this
65:57 - way because then you have this as only
65:59 - one argument and then you can do this
66:02 - faster in pytorch all right so now we
66:05 - have both the training functions now we
66:07 - only need the get action function so
66:10 - here in the beginning we want to do some
66:13 - random moves and this is also called a
66:17 - trade-off between
66:20 - exploration and
66:24 - exploitation in deep learning so at some
66:27 - point or in the beginning one we want to
66:29 - make sure that we also make random moves
66:32 - and explore the environment but then the
66:36 - better our
66:38 - model or our agent gets the less random
66:41 - moves we want to have and the more we
66:44 - want to exploit our agent or our model
66:48 - so
66:49 - yeah this is what we want to do here so
66:52 - for this we use this
66:54 - epsilon parameter that we
66:57 - initialized in the beginning
67:00 - so for this let's implement this first
67:02 - so we say self dot epsilon equals and
67:06 - this is dependent on the number of games
67:10 - so here i hard code this to 80 minus
67:14 - self dot number of games you can play
67:18 - around with this and then let's get the
67:21 - final move so in the beginning
67:24 - we say zero zero zero and then one of
67:28 - those now has to be true
67:30 - so here first let's check if random dot
67:35 - rent int and here between 0 and 200
67:41 - if this is smaller than self dot epsilon
67:46 - then we take a random move so we say
67:50 - move equals
67:52 - random dot rant ins and this must be
67:56 - between 0 and 2 so the 2 is actually
68:01 - included here and this will give us a
68:03 - random
68:05 - value 0 1 or 2 and now this index must
68:10 - be set to one so we say final move of
68:14 - this
68:15 - move index equals one
68:18 - and yeah so so the more games we have
68:22 - the smaller our epsilon will get
68:26 - and the smaller the epsilon will get
68:30 - the less frequent this will be
68:33 - less than the epsilon
68:35 - and when this is even this can even
68:37 - become negative and then we don't longer
68:40 - have a random move so again if this was
68:43 - too fast here then feel free to pause
68:45 - and think about this logic again
68:48 - so now we have that and otherwise else
68:52 - so here we actually here we want to do a
68:56 - move that is based on our model so we
69:00 - want to get a prediction prediction
69:03 - equals self dot model dot predict and it
69:08 - wants to predict the action based on one
69:13 - state so we call the state zero
69:16 - and we get this here but we want to
69:19 - convert this to a tensor so we say state
69:23 - 0
69:24 - equals torch dot tensor and as an input
69:29 - it gets the state
69:31 - and we also give it a data type equals
69:36 - let's use a torch dot float here then we
69:39 - call self.model predict with the state
69:42 - this will give us a prediction and this
69:45 - can be a raw value so if we go
69:50 - back to this slide
69:52 - so this can be a raw value and then we
69:57 - take the maximum of this and set this
70:00 - index to a1
70:02 - so here we say our move equals and we
70:07 - get this by saying torch arc max and the
70:11 - arc max of the prediction and this is a
70:15 - tensor again and to convert this to only
70:19 - one number we can call the item and now
70:23 - this is an integer and now again we can
70:26 - say final move of the smooth index is
70:29 - one
70:31 - and now we have this so now we return
70:34 - the final move here
70:37 - return
70:38 - and yeah this is all we need so now we
70:41 - have this and can save it like this
70:45 - and now we have all that we need for our
70:48 - agent class and now in the next one
70:52 - so what we must do here is implement the
70:55 - model and the trainer and then also the
70:59 - plotting
71:04 - so let's go back to the code and here i
71:08 - left this essay to do so we need a model
71:12 - and a trainer
71:14 - so let's create a new file and let's
71:18 - call this model dot pi
71:21 - and then here let's first import all the
71:25 - things we need so we need import torch
71:28 - then we want to import torch dot n n s n
71:33 - n
71:34 - then we want to import torch dot optim s
71:39 - optim and also import torch
71:43 - dot n n dot functional s capital f
71:49 - and we also want to import o s to save
71:53 - our model
71:54 - and now we want to implement two classes
71:57 - one for the model and one for the
72:00 - trainer so let's create a class and
72:04 - let's call this linear underscore
72:07 - qnet and this has to inherit from nn dot
72:12 - module
72:14 - module
72:15 - and by the way if you are not
72:17 - comfortable with pytorch and want to
72:20 - learn how to use this framework then i
72:22 - have a beginner series here on this
72:24 - tutorial for free and i will put the
72:27 - link in the description so this will
72:29 - teach you everything to need to get
72:32 - started with pytorch
72:34 - so
72:35 - right now let's start implementing this
72:37 - linear qnet function so we need the init
72:42 - function define init and we need to have
72:46 - self
72:47 - and this gets an input size
72:51 - input size a
72:53 - hidden size and an output size
72:58 - and then the first thing we want to do
73:01 - is to call this super initializer so we
73:05 - call super in it
73:07 - and here um this is very simple so if we
73:12 - have a look at the slides
73:14 - then our models should just be a feed
73:17 - forward neural net with a input layer a
73:20 - hidden layer and an output layer
73:23 - um feel free to extend this and improve
73:27 - this but it works fine for this case and
73:30 - it's actually not that bad here
73:32 - so let's create two linear layers so
73:35 - let's call this self.linear1
73:38 - equals nn.linear
73:41 - and this gets the input size as an input
73:46 - and then the hidden size as the output
73:49 - size
73:50 - then we have self.linear2
73:53 - equals
73:54 - nn.linear and now this gets the hidden
73:58 - size as the input and the output size as
74:03 - the output then as always in pi torch we
74:06 - have to implement the forward function
74:10 - with self and it gets x so the tensor
74:15 - and here what we want to do is first we
74:18 - want to apply the linear layer and we
74:21 - also use an actuation function here so
74:24 - again if you don't know what this is
74:26 - then check out my beginner tutorial
74:28 - series there i explain all of this so we
74:31 - say x and then we can call f dot
74:36 - reloose we use this directly from the
74:39 - functional module
74:40 - and here we say self dot linear one with
74:44 - our tensor x as the input so first we do
74:48 - the linear layer then we apply the
74:50 - actuation function
74:52 - and then again we apply the second layer
74:55 - so we call self dot linear 2 with x
74:59 - and we don't need an actuation function
75:02 - here at the end we can simply use the
75:05 - raw numbers and return x
75:08 - so this is our forward function
75:11 - then let's also implement a helper
75:14 - function to save the model later so
75:16 - let's call this self
75:18 - safe and this gets the file name as an
75:22 - input and i use a default here
75:26 - so we say model dot pth is simply the
75:30 - file name and then the last time i think
75:33 - i already
75:35 - called this function um so not yet but
75:38 - now we can comment this out so if we
75:42 - have a new high score then we call agent
75:45 - dot model dot save and here let's create
75:49 - a new
75:50 - folder in here so let's say this is the
75:54 - model folder path equals and let's
75:58 - create a new folder
76:01 - in the current directory and call this
76:05 - model so dot slash model and then we
76:08 - check if this already exists so the file
76:11 - in this folder
76:13 - so we can say if not os dot path dot
76:18 - exists and then we say our
76:22 - model folder path
76:24 - then we create this so we say os dot
76:28 - makers and we want to make this model
76:32 - folder path
76:34 - then we create this final file name so
76:38 - we say file
76:41 - name equals os
76:45 - dot path dot join and we want to join
76:50 - the model folder path and the file name
76:53 - that we use here as the input now this
76:56 - is the file name for saving and then we
76:58 - want to
76:59 - save this and we can do this with torch
77:02 - dot save and we want to save self dot
77:06 - state
77:08 - dict so i also have a tutorial about
77:11 - saving the model we only need to save
77:14 - this state dictionary
77:16 - and then as a path we use this file name
77:20 - so now this is all we need for our
77:22 - linear q net
77:25 - and now to do the actual training and
77:28 - optimization we also do this in a class
77:32 - that i call q
77:34 - trainer q trainer
77:37 - and now here what we want to do we want
77:40 - to implement a init function
77:43 - which gets self then it also gets the
77:47 - model then it should get a learning rate
77:50 - and it should get a gamma parameter
77:54 - and here we simply store everything
77:56 - self.lr equals lr self dot gamma equals
78:02 - gamma
78:03 - and we also store the model so we say
78:06 - self dot model equals model
78:09 - then to do a pie charge optimization
78:13 - step we need a optimizer so we can
78:16 - create this by calling self.optim
78:19 - or let's call this optimizer
78:22 - equals and we get this from the opt-in
78:25 - module and here you can choose one
78:28 - optimizer so i use the atom optimizer
78:32 - and we want to optimize model.parameters
78:36 - and this is a function
78:38 - and then it also needs the learning rate
78:41 - so lr equals self dot l r
78:46 - and now we also need a
78:48 - criterion or a loss function so let's
78:52 - call this self dot criterion
78:55 - equals and now if we go back to these
78:59 - slides at the very end
79:01 - we learned in the first part that this
79:04 - is nothing else than the mean squared
79:06 - error so that's very simple
79:08 - so we can create this here by saying
79:12 - self.criterion equals so this is nn.mse
79:18 - loss and now this is what we need in our
79:22 - initializer and then we also need to
79:25 - define a
79:27 - we call this train step function which
79:30 - gets self
79:32 - and then it needs to have all the stored
79:35 - um parameters from last time
79:38 - so it needs to have or let's have a look
79:41 - at
79:42 - this
79:43 - so here when we call this it needs the
79:46 - state the final move the reward the new
79:50 - states and done
79:52 - so let's copy and paste this in here and
79:56 - rename this slightly so this is just the
79:59 - state
80:00 - this is the action this is the reward
80:04 - so this is the new state this can
80:07 - uh let's call this
80:09 - next state here and then done can stay
80:13 - as it is
80:14 - and for now let's simply do
80:17 - pass here and before we implement this
80:21 - let's go back to the agent and now set
80:25 - this up so here we say from
80:28 - and we call this model and we want to
80:31 - import the linear i think we call this
80:35 - linear q
80:36 - net and q
80:39 - trainer
80:40 - and then here
80:43 - in the initializer we want to create an
80:46 - instance of the model and of the trainer
80:49 - so self.model equals our linear qnet and
80:54 - now this needs the input size the hidden
80:57 - size and the output size
80:59 - so here i use 11 256 and three
81:04 - so remember if we have a look at the
81:07 - slides again
81:09 - um
81:10 - the first one is the size of the state
81:14 - so this is 11 values and the output must
81:17 - be three because and we have three
81:19 - different
81:20 - um
81:22 - three different numbers in our action
81:25 - and you can play around with this hidden
81:27 - size but the other ones have to be
81:30 - eleven and three
81:31 - so this is the model and the trainer
81:34 - equals the q trainer and this gets the
81:38 - model so self.model then it gets the
81:42 - learning rate equals the learning rate
81:44 - which we specified here
81:47 - and we also pass on the gamma value so
81:50 - gamma equals self dot gamma and the
81:55 - gamma is the discount rate so i this has
81:59 - to be a value that is smaller than 1 and
82:04 - usually this is around 0.8 or 0.9 so in
82:09 - this case let's set this to 0.9
82:12 - so you can play around with this as well
82:15 - but keep in mind that it must be smaller
82:18 - than one so now we have this and then i
82:22 - made one error in the last tutorial so
82:25 - this is very important that we fix this
82:27 - right now
82:29 - so here in the get
82:31 - action function i actually
82:33 - i called this self.model
82:36 - predict but actually pythog doesn't have
82:40 - a predict function so this would be the
82:43 - api for tensorflow for example
82:46 - so in pi torch we simply call self.model
82:51 - like this
82:52 - and then this will execute this forward
82:56 - function so this is actually the
82:59 - prediction then
83:01 - so yeah
83:02 - please make sure to fix this
83:05 - okay so now we have everything and if we
83:08 - have a look and go back then we see we
83:12 - call this
83:13 - self.trainer train step
83:15 - with only one parameter but also with
83:19 - multiple ones so we want to make sure
83:22 - that we can handle different sizes
83:25 - so now let's start implementing this
83:29 - function and now the first thing we want
83:31 - to do
83:33 - so right now this can be um either a
83:37 - tuple or a list or just a single value
83:42 - so let's convert this to a pi torch
83:45 - tensor so let me copy and paste this in
83:49 - here so we do this for the states the
83:52 - next state the action and reward
83:54 - and we can do this by calling
83:57 - torch.tensor
83:58 - and then the variable and we specify the
84:01 - data type to torch dot float
84:04 - and we don't have to do this for the
84:07 - done or game over value because we don't
84:10 - need this as a tensor and now we want to
84:14 - handle
84:15 - multiple
84:17 - sizes so we want to check if the length
84:21 - and then we can
84:22 - check
84:23 - state dot shape
84:25 - and if this is one then we only have one
84:30 - dimension and then we want to reshape
84:33 - this so right now we only have
84:35 - if this is the case then we only have
84:38 - one number
84:40 - but actually we want to have it in the
84:42 - form one and then the values so this is
84:46 - the number
84:47 - of um batches so if this is already
84:51 - if this has
84:53 - already multiple values then it's
84:55 - already in the in the size n
84:58 - and x
84:59 - so then it's already correct
85:01 - and now here we want to append one
85:05 - dimension and we can do this with the
85:08 - torch unsqueeze function so we can say
85:12 - state equals states dot or sorry not
85:16 - state but
85:17 - torch dot un squeeze
85:21 - squeeze and then the
85:24 - states and we want to
85:26 - put it in dimension
85:29 - zero
85:30 - or axis zero so this means that it
85:33 - appends one dimension in the beginning
85:36 - and this is then just one then i also
85:39 - wanted to do this for the other um
85:43 - tensors so for the next state and for
85:46 - action and reward
85:48 - and the done value we also want to
85:53 - convert this right now this is only a
85:56 - single value and we want to convert this
85:59 - to a tuple so we can do it like this so
86:02 - now we have a
86:03 - done so this is how you define a tuple
86:06 - with only one value
86:09 - and now um we have it in the correct
86:12 - shape so now what we have to implement
86:16 - is um from last time or from the very
86:19 - first tutorial where i showed this
86:22 - bellman equation
86:24 - and then we simplified this so we have
86:27 - the old queue where we simply call model
86:31 - predict with the old state
86:33 - and the new queue with this formula so
86:37 - let's do this so
86:39 - first let's um write a comment here so
86:43 - as first thing we want to
86:46 - get the predicted
86:48 - predicted q values with the current
86:53 - state
86:55 - and this is simply by doing let's call
86:57 - this prediction equals self dot model
87:02 - and then we want to do this with
87:05 - state 0 or we just call this state here
87:09 - and then for the second part we need
87:12 - this formula the reward plus the gamma
87:16 - value times the maximum of again model
87:20 - predict with state one
87:23 - so first let's write this as a new uh
87:26 - comment so the first thing is we want to
87:29 - apply this formula reward plus
87:32 - gamma times and then
87:35 - the next
87:36 - predicted
87:38 - q value
87:40 - and
87:41 - then we want to have the
87:44 - maximum so the maximum of this so
87:47 - maximum
87:49 - and then um this is a little bit tricky
87:51 - so the maximum of this um
87:54 - sorry let's do it like this maximum of
87:58 - next predicted q value so this is only
88:03 - one value
88:05 - but um if we do it like
88:07 - as first
88:09 - um parameter the predictions this has
88:12 - actually this is the action this is
88:15 - actually
88:16 - three different values
88:18 - so
88:19 - what we do to get the same here is we do
88:23 - a clone of this
88:25 - and then we set the
88:28 - index
88:29 - with this action to the new q value so
88:35 - this is let's call this q new like i
88:38 - showed you in the formula
88:41 - and then we set the
88:43 - let's call this predictions and then the
88:47 - index of
88:49 - the
88:50 - arc max of the action we set this to our
88:56 - q
88:57 - new value
88:58 - so again this might be tricky so again
89:01 - we want to calculate the new q value
89:03 - with this formula that i showed you but
89:06 - then we need to have it in the same
89:08 - format and for this we simply clone this
89:11 - so then we have three values again
89:14 - and two of the values are the same but
89:17 - the value with the
89:19 - action so the action is for example
89:22 - one zero zero
89:24 - so um the index of the one is then set
89:29 - to the new q value
89:31 - so this is what we want to do here
89:34 - so for this let's first let's create a
89:38 - clone target
89:40 - equals prediction dot clone so we can do
89:43 - this with a pi torch tensor and then um
89:48 - we want to iterate over our tensors and
89:53 - apply this formula so for this we say
89:56 - for
89:57 - index in
89:59 - range and then the length of the let's
90:02 - call this done and here everything
90:05 - should have the same size so
90:08 - then this works so now we iterate over
90:11 - this and then one thing that i didn't
90:14 - mention so far is that we only only want
90:18 - to do this only do this if not done
90:23 - um otherwise we simply take the whole
90:25 - reward so we say q new equals reward of
90:31 - the current
90:33 - current index and now we check if we are
90:37 - not done so we say if
90:39 - not
90:40 - done and the done is of the current
90:44 - index
90:45 - then we apply this formula so now we say
90:48 - q
90:49 - new
90:50 - is actually um the reward so the reward
90:55 - of the current
90:57 - index
90:58 - plus
90:59 - self dot
91:01 - gamma
91:02 - and then times torch dot
91:06 - max the maximum value
91:09 - of the next prediction so here's self
91:13 - dot model of next
91:17 - state of this
91:19 - index
91:21 - so this is exactly what we have written
91:23 - here and now we need to set the
91:28 - target of the maximum value of the
91:31 - action to this value
91:34 - so here we get the let's we call this
91:38 - target so the target of the current
91:41 - index and then of the arc max of the
91:45 - action
91:46 - so for this we can again say torch dot
91:50 - arc max of the of the action and we want
91:54 - to have this as a item so as a value and
91:58 - not as a tensor and now this is our q
92:02 - new value so this might be a little bit
92:05 - tricky to understand so i recommend that
92:08 - that you pause here and go over this
92:10 - again
92:11 - and now we have everything that we need
92:14 - so let's have a look at the slides again
92:16 - we have our q and our q
92:19 - new
92:20 - and then we apply the loss function so
92:23 - the mean squared error and in pi torch
92:27 - so what we have to do here we can simply
92:30 - use this optimizer and do a step
92:34 - and first we have to call this zero grad
92:38 - function to empty the gradient so this
92:41 - is just something that we have to
92:42 - remember in pi torch
92:45 - and then we calculate the loss by
92:48 - calling
92:49 - self dot criterion
92:52 - and here we put in the
92:54 - target and the prediction
92:57 - so this is q new and q
93:00 - and then we call loss dot backward and
93:04 - apply back propagation and then update
93:07 - our gradients and then we call
93:09 - self.optimizer.step
93:13 - and this is all that we need in this
93:16 - training step and now this is actually
93:19 - all that we need in this model file
93:23 - so now again let's go back to the agent
93:28 - and i guess we already set up the q
93:31 - trainer
93:32 - and then when we train this we call this
93:37 - train step function
93:39 - either for only one of those parameters
93:42 - or for a whole batch and now this
93:45 - function can handle different sizes
93:48 - and now the only thing left to do here
93:52 - is to actually to plot the results
93:56 - so for this let's create a new file and
94:00 - let's call this hell
94:02 - helper dot pi
94:05 - and then here let me actually copy and
94:08 - paste this in here
94:10 - so this is just a simple function with
94:13 - matplotlib and i python
94:16 - and yeah here we want to plot the scores
94:20 - so this is a list and we want to plot in
94:23 - the plot the mean score
94:26 - so
94:27 - let's create them so here in the agent
94:31 - we say from
94:33 - helper import the plot function
94:38 - and then down here in the training
94:41 - function so we already created an empty
94:44 - list for the scores and for the mean
94:46 - scores
94:48 - and now after each um
94:51 - game we want to append the score
94:55 - so let's remove the to do and implement
94:58 - this
94:59 - so we say plot scores dot
95:02 - appends and then the current score
95:06 - then let's calculate the new mean or
95:09 - average score
95:11 - so for this let's say total score
95:14 - plus equals the score
95:17 - and then let's call this mean score
95:21 - equals the total score divided by the
95:26 - number of games so agent and games
95:30 - and then we append this to plot mean
95:34 - scores dot append
95:36 - the mean score and then we simply call
95:40 - the plot function with the plot scores
95:45 - and then the plot
95:47 - mean scores
95:48 - and now let's save this file and also
95:52 - let's save this file and then let's try
95:54 - it out so in the terminal
95:57 - let's call agent dot pi and let's cross
96:01 - fingers
96:02 - so
96:03 - we have a syntax error in the model.pi
96:07 - file
96:08 - so um
96:10 - here we
96:11 - actually here we have two equal signs
96:15 - so let's fix this and save this and run
96:19 - it again
96:22 - and then we made another mistake name
96:25 - error so here this is actually called
96:30 - prediction.clone so again let's save
96:32 - this and run this
96:35 - and now it starts training without
96:38 - crashing and it also plots
96:42 - so let's
96:43 - let this run and see if this is
96:45 - improving
96:48 - [Music]
97:07 - all right so as we can see the algorithm
97:10 - works and the snake is getting better
97:13 - and better and the scores are getting
97:15 - higher and higher and also the mean or
97:18 - average score is getting higher so i
97:20 - forgot one
97:22 - important thing which i show you in a
97:24 - second
97:26 - but for now um
97:28 - so the snake is not perfect and the main
97:31 - issues are that it traps itself
97:34 - sometimes and also sometimes it gets
97:37 - stuck in an endless loop sequence
97:40 - so this is something that you can
97:42 - improve as a homework so yeah like this
97:44 - it now it trapped itself
97:46 - so yeah let me
97:48 - stop this actually and then show you
97:50 - what i forgot so in the game we can
97:54 - actually um set the speed
97:57 - so for the
97:58 - human controlled game when i want to
98:01 - play this i set this to 20 but now i
98:04 - recommend to set this to a larger number
98:07 - so that the training will be faster
98:10 - so for example you can use 40 here or
98:12 - even higher so i go with 40 and yeah i
98:16 - think that's the whole code you can also
98:18 - find this on github and yeah i hope you
98:21 - really enjoyed this little series about
98:23 - reinforcement learning and if you
98:25 - enjoyed this then please hit the like
98:27 - button and consider subscribing to the
98:29 - channel and then i hope to see you next
98:31 - time bye

Cleaned transcript:

patrick lober is a popular python instructor and in this course he will teach you how to train an artificial intelligence to play a snake game using reinforcement learning hey guys today i have a very exciting project for you we are going to build an ai that teaches itself how to play snake and we will build everything from scratch so we start by creating the game with pygame and then we build an agent and a deep learning algorithm with pie torch i will also teach you the basics of reinforcement learning that we need to understand how all of this works so i think this is going to be pretty cool and now before we start let me show you the final project so i can start the script by saying python agents dot pi now this will start training our agent and here we see our game and then here i also plot the scores and then the average score and now let me also start a stopwatch so that you can see that all of this is happening live and now at this point our snake knows absolutely nothing about the game it only is aware of the environment and tries to make some more or less random moves but with each move and especially with each game it learns more and more and then knows how to play the game and it should get better and better so the first few games you won't see a lot of improvements but don't worry that's absolutely normal i can tell you that it takes around 80 to 100 games until our ai has a good game strategy and this will take around 10 minutes also you don't need a gpu for this so all of this training can happen on this cpu that's totally fine okay so let me speed this up a little bit all right so now about 10 minutes have passed and we are at about game 90 i guess and now we can clearly see that our snake knows what it should do so it's more or less going straight for the food and tries not to hit the boundaries so it's not perfect at this point but we can see that it's getting better and better so we also see that the average score here is increasing and now the per the best score so far is and to be honest for me this is super exciting so if you imagine that at the beginning our snake didn't know anything about the game and now with a little bit of math behind the scenes it's clearly following a strategy so this is just super cool don't you think all right so let me speed this up a little bit more all right so after 12 minutes our snake is getting better and better so i think you can clearly see that our algorithm works so now let me stop this and then let's start with the theory so i will split the series into four parts in this first video we learn a little bit about the theory of reinforcement learning in the second part we implement the actual game or also called the environment here with pygame then we implement the agent so i will tell you what this means in a second and in the last part we implement the actual model with pytorch so let's start with a little bit of theory about reinforcement learning so this is the definition from wikipedia so reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward so this might sound a little bit complicated so in other words we can also say that reinforcement learning is teaching a software agent how to behave in an environment by telling it how good it's doing so what we should remember here is that we have an a chance so that's basically our computer player then we have an environment so this is our game in this case and then we give the agent a reward so with this we tell it how good it's doing and then based on their reward it should try to find the best next action so yeah that's reinforcement learning and to train the agent there are a lot of different approaches and not all of them involve deep learning but in our case we use deep learning and this is also called steep q learning so this approach extends reinforcement learning by using a deep neural network to predict the actions and that's we're going to use in this tutorial all right so let me show you the rough overview of how i organized the code so as i said we're having four parts so in the next part we implement the game with pie game then we implement the agent and then we implement the model with pie torch so our game has to be assigned such that we have a game loop and then with each game loop we do a play step that gets an action and then it does a step so it moves the snake and then after the move it returns the current reward and if we are game over or not and then also the current score then we have the agent and the agent basically puts everything together so that's why it must know about the game and it also knows about the model so we store both both of them in our agent and then we implement the training loop so this is roughly what we have to do so based on the game we have to calculate a state and then based on the state we um calculate the next action and this involves calling model predict and then with this new action we do a next play step and then as i said we get a reward the game overstate and the score and now with this information we calculate a new state and then we remember all of this so we store the new state and the old state and the game over state and the score and with this we then train our model so for the model i call this linear q net so this is not too complicated this is just a feed forward neural net with a few linear layers and it needs to have the these information so the new state and the old state and then we can train the model and we can call model predict and then this gets us the next action so yeah this is a rough overview how the code should look like and now let's talk about some of those variables in more detail for example the action or the state or the reward so let's start with the reward so that's pretty easy so whenever our snake eats a food we give it a plus 10 reward when we are game over so when we die then we get 10 and for everything else we just stay at zero so that's pretty simple then we have the action so the action determines our next move so we could think that we have four different actions so left right up and down but if we design it like this then for example what can happen is if we go right then we might take the action left and then we immediately die so this is basically a 180 degree turn so we don't allow that so a better approach to design the action is to only use three different numbers and now this is dependent on the current direction so um 1 zero zero means we stay in the current direction so we go straight so this means if we go right then we stay right if we go left then we go left and so on then if we have 0 1 0 this means we do a right turn and again this depends on the current direction so if we go right and do a right turn then we go down next if we go down and do a right turn again then we go left and then again we would go up so this is the right turn and the left turn is the other way around so if we go left and do a left turn then we go down and so on so with this approach we cannot do a 180 degree turn and we also we only have to predict three different states so this will make it a little bit easier for our model so now we have the reward and the action then we also need to calculate the state and the state means that we have to tell our snake some information about the game that it knows about so it needs to know about the environment and in this case our state has 11 values so it has the information if the danger is straight or if it's ahead if the danger is right or if the danger is left then it has um the current direction so direction left right up and down and then it has the information if the food is left or right or up or down and all of these are boolean values so let me show you an actual example so in this case if we are going right and our food is here then we see um danger straight right and left none of this is true so for example if our snake is over here at this end and it's still going right then danger straight would be a one so this again also depends on the current direction for example if we move up at this corner here then danger right would be a1 then for these directions only one of them is one and the rest is always zero so in this case we have danger right set to one and then for this in our case our food is right of the snake and also down of the snake so food right is one and food down as one all right so now with the state and the action we can design our model so this is just a feed forward neural net with an input layer a hidden layer and an output layer and for the input it gets the state so as i said we have 11 different numbers in our state 11 different boolean values zero or one so we need this size 11 at the beginning then we can choose a hidden um size and for the output we need three outputs because then we predict the action so this can be some numbers and these don't need to be probabilities so here we can have raw numbers and then we simply choose the maximum so for example if we take 1 0 zero and if we go back then we see this would be the action straight so keep the current direction so yeah that's how our model looks like and of course now we have to train the model so for this let's talk a little bit about this deep q learning so q stands for this is the q value and this stands for the quality of the action so this is what we want to improve so each actions should improve the quality of the snake so we start by initializing the q value so in this case we initialize our model with some random parameters then we choose an action by calling model predict state and we also sometimes choose a random move so we do this especially at the beginning when we don't know a lot about the game yet so and then later we have to do a tradeoff when we don't want to do a random move anymore and only call model predict and this is also called a tradeoff between exploration and exploitation so this will get clearer later when we do the actual coding so then with this new action we perform this action so we perform the next move and then we measure the reward and with this information we can update our q value and then train the model and then we repeat this step so this is an iterative training loop so now to train the model as always we need to have some kind of loss function that we want to optimize or minimize so for the loss function we have to look at a little bit of math and for this i want to present you the socalled belmont equation so this might look scary so don't be scared here i will explain everything and actually it's not that difficult when we um understand this and then code this later so what we want to do here we need to update the q value as i said here so according to the belmont equation the new q value is calculated like this so we have the current q value plus the learning rate and then we have the reward for taking that action at that state plus a gamma parameter which is called this count rate so don't worry about this i will also show this later in the code again and then we take the maximum expected future reward given the new state and all possible actions at that new state so yeah this looks scary but i will simplify that for you and then it's actually not that difficult so the old q value is model predict with state 0 so if we go back at this overview so the first time we say get state from the game this is our state 0 and then after we took this place step we again measure or calculate the next state so this is then our state one so with this information again our first queue is just model predict with the old state and then the new queue is the reward plus our gamma value times the maximum value of the q state so again this is model predict but this time we take state one and then with these two information our loss is simply the q new minus q squared and yeah this is nothing else than the mean squared error so that's a very simple error that we should already know about and then this is what we must use in our optimization so yeah that's what we are going to use so we have to implement all of these three classes and in the next video we start by implementing the game in the last part i showed you all the necessary theory that we need to know to get started with deep q learning and now we start implementing all of the parts so as i said we need to have a game so the environment then we need an agent and we need a model so in this part we start by implementing the game and we use pytorch for this so let me actually start by creating a environment and we install all the necessary dependencies that we need so in this case i use conda to manage the environments and if you don't know how to use conda then i have a tutorial for you that i will link here but yeah if you don't want to use connor you can also just use a normal virtual and but i recommend to use a virtual and and now let's create a virtual and with conda create minus n and then give it a name for example pi game n and i also say i want python equals 3.7 all right so now this was created so now we want to actuate it with conda activate and then pie game n and hit enter and then we see the name of the environment in the front so this means that we activated it successfully and now we can start installing all what we need so the first thing we want to install is pie game for our game so pip install pie game and hit enter so this is done the next thing we need is pytorch for our model later so for this we can go to the official home page and on install and then here you can select your operating system so i use mac and i actually i want to say pip install and we don't need cuda support so only a cpu is fine and we don't need torch audio because we don't work with audio files so we can only grab this pip install torch torch vision and then paste it in here and hit enter and now this installs pytorch and all the dependencies all right so this is done and then we need two more things for plotting later so for this i say pip install much plot lip and we also want i python and then hit enter all right so this was successful as well and now we have everything we need so now we can start implementing all the codes and as a starting point i want to grab the code from another tutorial that i did so you can find this on github and then on my um account and then in the repo python fun and here i actually have two snake games so and then we need this one snake pie game and download this so you can do this and i already did this and have this here so if we open up the editor here i'm visuals using visual studio code then we can see we have exactly those two files and then um the first thing i want to do is i want to run this file and test if this is actually working so right now this is just a normal snake game that you have to control yourself so you have to use the arrow keys so let's say python snake game dot pi and then let's hope that this is working so yeah so now i can control the snake and i hope that i can eat the food yes and now if i hit the boundary then we are game over so this is working our environment is set up and now we can start implementing our code so we can change this so that we can use this as a ai controlled game so let me show you the overview from last time so last time i told you that we need a play step in our game and this gets an action and based on this action we then take a move and then we must return a reward the game over state and the current score so first let's write down all the things that we need to change here so first we want to have a reset function so after each um game our agent should be able to reset the game and start with a new game then we need to implement the reward that our agent gets then we need to change the play function so that it takes takes an action and then um returns a or computes the direction then we also want to keep track of the current frame or let's call this game iteration and for later we also need to have a change in the if is collision function to check if this is a collision so first let's let me go over this code quickly so what we do here is we use pi game then for the direction we use an enum then for the point we use a named tuple and then here i created a class snake game and here we initialize the things we need for the game so here we initialize the game state for example for the snake we use a list with three initial values and the head is always the front of this list then we keep track of the score and here we have a helper function to place the food and yeah and we already have a function that is called play step and then if we go down to the very end so here we have our game loop so while this is true we take a game or a play step and we get the game over state and the score so this place the function is the most important one so here first we right now we grab the user input so the key we press then we calculate a move based on this key and then we update our snake and check if we are game over and if we can continue we place the new food or check if we eat the food and we update our ui with this helper function update ui then here we have this helper function is collision where we check if we either hit the boundary or we run into ourself and then we also have this helper function move where we get the current direction and then based on this direction we simply um calculate calculate the new position of the new hat so yeah that's all um what is done here and now let's change a few things though so the first one i want to change the class name to say snake game ai to make it clear that this is a agent controlled game and now so the first thing we want is the reset functionality so in here um i already have this comment where we in it the game state so now we want to refactor all of this into a reset function so we create a new function define and then let's call this reset and it only gets self and no other arguments and here we can grab all of this code and then simply paste it in here and in our initializer we then call self dot reset so this is the first thing we need additionally we want to keep track of the um game iteration or frame iteration so let's call this self dot frame iteration and in the beginning this is just zero then this define place food can stay as it is and now we need to change the play step function so first of all if we have a look at the overview here i already told you that now we need to give this the action from the agent and we need to return a reward so let's start by um using this action parameter and here we grab the user input so actually right now we can get rid of this so the only thing we still check if we want to quit the game and now here um we already have this helper function where we move in the current direction so actually what we change here now this move function doesn't get the direction from the user input so now here it gets the action and then we have to determine the new direction so we do this in a second but first let's only change this and then here we call the self.move with the action and then we update the head then we check if we are game over or not and we actually now we also need the reward so we simply say reward equals zero and let's go back to the slides from last time so the reward is really simple whenever we eat a food we say plus 10 when we lose or when we die then we say our reward is 10 and for everything else we just stay at zero so we initialize the reward with zero then if we have a collision and game over then we say our reward equals to 10 and we want to return this as well so return the reward game over and self.score and here we check only if we have a collision so here i actually want to do another check so if nothing happens for a long time so if our snake doesn't improve and doesn't eat the food but also doesn't die then we also want to check this and if this happens for a too long time then we also break here so we can say or and then here we say if self dot frame iteration and if that this gets too large without anything happening then we um stop here so here i use this little formula if this is greater than 100 times the length of our snake so remember this is a list then we break so this is also like this then it's dependent on the length of the snake so the longer our snake is the more time it has so but then if it gets larger than this value then we break and of course we have to update the self.frame iteration and we can simply do this here at the beginning so for each play step we say self dot frame iteration plus equals 1 and when we reset it then we reset it back to zero so this is here and then yeah if we stop we have the reward 10 then here if our hat hits the food then we eat the food so our score increases and our reward is set to plus 10 then we place a new food and say otherwise we remove the last part so we simply move here then this can stay as it is the update function and at the very end we also want to return the reward then for the is collision function we need a slight change so here i only check for self.head but later um to calculate the state or the danger which i told you about so if we have a look at the state so here we calculate the danger so if we are for example if we are here at the corner then we have a danger at the right so for this it might be handy if we don't use self.head inside here but if we give this function a point so this gets the point argument and let's say by default this is none and then here we simply ch check if the point is none then we set the point equals to self dot head so inside this where we call this with no argument it can stay as it is and then here of course we have to change self.head to this is now our point so here if we hit the corner point here and point here and point here then we have a collision and here if our point is in the snake body then we also have a collision and otherwise we don't have a collision all right so the update ui function can stay like this and now for the move function here we need to change something so now we get a action and now based on this action we want to determine the next move so if we go back to the slides so here we designed the action like this so it has three values um one zero zero means we keep the current direction and go straight 0 1 0 means we do a right turn and 0 0 1 means we do a left turn so this is dependent on the current direction so if we go right and do a right turn then we go down next if we go down and do a right turn then we go left next and so on and left turn is the other way around so now um we want to determine the direction based on the action so let's write a quick comment here we have straight right turn or left turn so to get the next direction first i want to define all the possible directions in a clockwise order so we say clockwise equals and then a list and here we start with direction dot right so here remember for the direction we use this enum class so it has to be one of those directions so our clockwise directions should start with direction right then from this on the next one is direction dot down then we have direction dot left and as last thing we have direction dot up so right down left up this is clockwise and then to get the current direction or the current index of the current direction we say index equals and then we can say clockwise dot index and then the index of the self dot direction so we are sure that this has to be in this array we because the self direction must be one of those enum values and then we check that different um possible states so these ones so for this we can use numpy and i guess we have to import numpy first as np and then we can use it here we can say if numpy and then we use this function array equal and then here we put in the action and the array that we want to compare so if this is equal to one zero zero then we go straight or we keep the current directions so we simply say our new direction equals and then clockwise of the index and then remember the index is just the index of the current direction so here we basically have no change then we say l if if our array if numpy array equal if the action equals to 0 one zero then we do a right turn so this means we go clockwise so if we go right then the next direction would be down if we go down then the next direction would be left and if we go left then the next direction would be up so here we say index equals or this is our next index actually and here we say this is the current index plus 1 but then modulo 4 so this means if we are at the end up and then do the next one if we have index so this is index 0 1 2 3 and then if we have index 4 modulo 4 is actually again index zero again so from this we do a turn and then come back at the front again so this is our right turn so now this is the next index and now our new direction is clockwise of the next index and then otherwise we can simply use else here and actually change this to an l if so now this is the last case so it has to be here it has to be zero zero one and if this is the case then let's copy and paste this in here then our next index is the current index minus one modulo four so this actually means we go counter clockwise so we do a left turn so if we start with right then the next move would be up and then the next would be left and then the next would be down and then right again and so on so now this is our new direction and then simply we say self direction equals new direction and then we go on so here we extract the head and then here we have to check if self dot's direction now is right then we increase the position of x and so on if we um have the left direction then we decrease x and if we go down then we actually increase y so for so the y starts at the top at zero and then increases if we go down so if we go down then we have to increase y and if we go up then we have to decrease y so if self direction equals up then y minus equals the block size and by the way the block size is just here a constant value of 10 so that's how big our one block of the snake should be in pixels so yeah this is everything we need here in the move function and now here we don't need this anymore so this is actually no longer working with a user input so you can just delete this and then later we control this class from the agent and call this play step function so yeah for now this is all we need to implement the game so i already talked about the theory of deep q learning in the first part in the last part we implemented the pi game so that we can use it in our agent controlled environment and now we need the agent so let's start and so here um if you haven't watched the first two parts then i highly recommend to do so so this is the starting point from last time and i actually want to make one more change that i forgot so here the is collision function should actually be public because then our agent should use it so just remove the underscore here and then also remove it in this class itself when we call this so then we have our snake game and i also want to rename this to just be game and now we create a new file agent dot pi and then start implementing this so first here we import torch from pi torch then we import random because when later we need this then we also need import numpy snp and from our um implemented class we need the snake game so we say from game import snake like snake game a i so i think that's what we call this class snake game a i so yeah that's the right name then we also hear at the beginning we defined this enum for the direction and this named tuple for the point which has an x and a y attribute so we also want to import these two um things so we import direction and we import point and then we also say from collections we want to import deck so this is a data structure where we want to store our memories so um if you don't know what a deck is then i will put a link in the description below so this is really handy in this case and you will see why this is the case later and then here i want to define some parameters as constants so we have a maximum memory of let's say 100 000 so we can store 100 000 items in this memory then we also want to to use a batch size that you will see later and here i will set this to 1000 so you can play around with these parameters and i also want a learning rate later and i want to set this to 0 0 1 and yeah feel free to change this and then we start creating our class agent and it gets of course an init function with self and no other arguments and then let's have a look at the slides from the first part where i explained the training so we want to create a training function where we do all of this so we need to get the state calculate the state where we are aware of the current environment then we need to calculate the next move from the state and we need to um then we want to update or do the next step and call game.playstep and then calculate the new state again then we want to store everything in memory and then we also want to train our model so we need to store the game and the model in this class so first of all let me create the functions that we need first so we need a function get state which gets self and this this gets the game and then we calculate the state that i showed you with these 11 different variables then we want to have a function that we call remember remember and it has self and here we want to put in the state then the action then we want to remember the reward for this action and we want to calculate or we want to store the next state next state and we also want to store done or bit or you can also call this game over so this is the current game overstate then we need two different functions to train and we call this defined train on the long memory and it only needs self so i will explain this later and we also let's copy and paste this i also have a function define train on short memory so this is only with one step you will see this later then we need a function and we call this get action to get the action based on the state so it gets self and the state and first we only say pass and these are all the functions we need i guess and then i want to have a global function that i call simply train and here we say pass and then when we start this module h and dot pi so we say if name underscore equals equals main then we simply call this train function and then we can start the script by saying python agent dot pi like i did in the very first tutorial so let's start implementing the agent and the training function so let's start with the init function of the agent so here what i want to store is first i want to store some more parameters so self.number of games so i want to keep track of this so this is zero in the beginning then self.epsilon equals um zero in the beginning this is a parameter to control the randomness so you will see this later then we also need self dot gamma equals zero so this is this is the socalled this count rate which i briefly showed in the first tutorial i will explain this a little bit more in the next tutorial where we implement the model and the actual deep q learning algorithm then we want to have a memory so we say self.memory equals and for this we use this stack and this can have a argument max leng equals and here we say max memory and what then happens if we exceed this memory then it will automatically remove elements from the left so then it will call pop left for us and that's why this deck is really handy here and then later here we also want to have our model and the trainer so i will leave this for a or s to do for the last part in the next video and now this is all for the init function and now we can go back and now let's do the training function next so again let's have a look at these slides so we need these functions in this order so let's first let's write some comments of first let's create some lists to keep track of the scores so this is an empty list in the beginning and this is used for plotting later so then we also want to keep track of the mean scores or average scores this is also an empty list in the beginning then our total score equals zero in the beginning our record our best score is zero in the beginning then we set up a agent so agent equals an agent and we also need to create a game so the game is a snake game ai object and then here we create our training loop so we say while true so this should basically run forever until we quit the script and now here let's write some comments so we want to get get the old state or the current state so here let's say state old equals and then we call agent dot get states and this gets the game so we already set this up correctly we only have to implement it then then after this we want to get the move based on this current state so we say the final move equals agent dot get action so we actually called this action and the action is based on the state then with this move we want to perform the move and then and get new state so for this we say reward um done and score equals and here we call game dot play step from last time so i think game dot play step with the action yes game dot play step and this gets the final move and then we get the state old or the new now the new state state new state new equals agent and again gets state now with the new game then after that we want to train the short memory of the agent so only for one step so for this we say agent agent dot train short memory and this gets if we have a look here um actually uh this short memory should get some parameters so exactly the same as we put in the remember function so train short memory gets all of those variables and then here when we call this now we should get some hints strain or let's save this file and then say agent dot train short memory and now we should get the hints no we don't get this but actually we want to have the state action reward next state and done so here let's do this so say let's say state old then the action which was the final move then the reward then the state new and adds last thing the done or game over state variable so now we have this then we want to remember all of these and store this in the memory so we say agent dot remember and then here it gets the same um variables so we want to store all of this in our deck and then this is all we need so now we check if done or if game over then if this is true then what we want to do is um we want to let's write a comment train the long memory and this is also called replay memory or experience replay and this is very important for our agent so now it trains again on all the previous moves and games that it played and this tremendously helps him to improve itself and we also here want to plot the results so first of all we want to reset the game so we can simply do this by saying game dot reset we already have this function here so this initializes the game state and resets everything so the score the snake the frame iteration and places the initial snake and the food so now we have this then we want to increase agents dot number of games so this plus equals one then we want to say agent dot train long memory and this doesn't need any arguments then we want to check if we have a new high score so if score greater than the current record then we say record equals our new score and we will also want to leave this as a to do here so here we want to say agent dot model dot save later when we have the model and so here in the here we want to store this as self.model and now what we also want to do here um let's print some information so print the game and then the current number and then the score and the record so here let's say our game is agent dot n games then we also want to plot the or print the score so this is just the score and we want to print the current record so the record equals record and then here we want to do some plotting so i will implement this in the next tutorial so i will leave this s8 to do so this is all for our training function so what i showed in the slides and now of course we have to implement those functions so for the get state function um let's go back to this overview and here as i said we store 11 values so if the danger is straight right or left then the current direction so only one of these is one and then the position of the food if it's left of the snake right of the snake up or down of the snake so these are the 11 states and now let me actually copy and paste the code in here so that i don't make any mistakes but we will go over this so first let's grab the head from this game so we can do this by calling game dot snake zero so this is a list and the first item is our head then um let's create some points next to this head in all directions that we need to check if this hits the boundary and if this is a danger so for this we can use this named tuple so we can create a point with this location but minus 20 so the 20 is hard coded here so this is the number that i used for the block size so like this we create four points around the head then the current direction is simply a boolean where we check if the current game direction equals to one of those so only one of those is one and the other one is zero or false and then um we create this array or this list with this 11 um states so here we check that if the danger is straight or ahead and this is dependent on the current direction so if we are going right and the point right of us gives us a collision then we have a danger the same or or if we go left and our left point gets a collision then we also have a danger here and so on so this is dangerous straight and then danger right means if we go up and the point right of us would give a collision then we have a danger for a right turn basically and so on and the same for the left so this might be a little bit tricky so i recommend that you pause here and go over this logic for yourself again so yeah these only have give us three values in our state so far then we have the move direction where only one of them is true and the other one is false and for the food location we simply check if food if game food x is smaller than game head x then we have food is left of us and the same way we check for right up and down and then we convert our list to a numpy array and say the data type is in so this is a nice little trick to convert this true or false booleans to a zero or one so yeah now this is the get state method now let's move on to the remember function so here we want to remember all of this in our memory so this is a deck and this is very simple so here we say self dot memory and then the deck has also the append function where we want to append all of this in this order so the state the action the reward the next state and the game over state and as i said if this exceeds the maximum memory then pop left if max mem memory is reached and yeah this is the remember function then let's start implementing the train long and short memory functions so for this so we actually we store a model and a trainer in here so let's actually say self dot model equals let's say this is only none in the beginning and leave a to do and self dot trainer equals none in the beginning and this is a to do so these are objects that we create in the next tutorial and then here we call this trainer to actually do the optimization so let's start here so for only one step we say self.trainer and then this should get a function that we call let's call this train step and then it gets all of these variables so the state the action the reward the next state and the game overstate and this is all that we need to train it for only one game step and we design this function um so that it takes either only one state like this but it can also take a whole tensor or a numpy array and then uses multiple as a socalled batch so let's do this here so for this we take the variables from our memory so here we want to grab a batch and so in the beginning we defined the batch size is 1 000 so we want to grab 1 000 samples from our memory but first we check if we um already have a thousand samples in our memory so we say if lang and self dot memory if this is smaller then the batch size then we simply or actually let's say if this is greater so if this is greater than we want to have a random sample and say mini sample equals and then we want to get a random sample so we can use random dot sample so we already imported the random module random dot sample from self dot memory and as a size it should have the batch size so this will return a list of tuples and this is because here i forgot one important thing so when we want to store this and append this we want to append this as only one element so only one tuple so we need extra parenthesis here so this is one tuple that we store and then here we get the batch size number of tuples and otherwise else if we don't have uh a thousand elements yet then we simply take the whole memory so we say mini sample equals self dot memory and then we again want to call this training step and for this so let's call this here again self.trainer.trainstep but here we have multiple states so let's call this states actions rewards next states and done and right now so now we have it in this format that we have one tuple after another and now we want to extract this from the mini sample and then put every states together every action together every reward to it together and so on and this is actually a really simple with python so we can say we want to extract the states the actions the rewards the next states and the duns game overs and here we simply use the built in sip function and have to use one asterisk and then the mini sample argument so yeah check that for yourself if you don't know how the sip function works but again it now it puts every states together every actions and so on if this is too complicated for you then you can also just do a for loop so you can iterate over this mini sample and basically say for action or for state action rewards next state and done in one mini sample and then again you call this here for only one for only one argument so yeah you can do it both ways but actually i recommend to do it this way because then you have this as only one argument and then you can do this faster in pytorch all right so now we have both the training functions now we only need the get action function so here in the beginning we want to do some random moves and this is also called a tradeoff between exploration and exploitation in deep learning so at some point or in the beginning one we want to make sure that we also make random moves and explore the environment but then the better our model or our agent gets the less random moves we want to have and the more we want to exploit our agent or our model so yeah this is what we want to do here so for this we use this epsilon parameter that we initialized in the beginning so for this let's implement this first so we say self dot epsilon equals and this is dependent on the number of games so here i hard code this to 80 minus self dot number of games you can play around with this and then let's get the final move so in the beginning we say zero zero zero and then one of those now has to be true so here first let's check if random dot rent int and here between 0 and 200 if this is smaller than self dot epsilon then we take a random move so we say move equals random dot rant ins and this must be between 0 and 2 so the 2 is actually included here and this will give us a random value 0 1 or 2 and now this index must be set to one so we say final move of this move index equals one and yeah so so the more games we have the smaller our epsilon will get and the smaller the epsilon will get the less frequent this will be less than the epsilon and when this is even this can even become negative and then we don't longer have a random move so again if this was too fast here then feel free to pause and think about this logic again so now we have that and otherwise else so here we actually here we want to do a move that is based on our model so we want to get a prediction prediction equals self dot model dot predict and it wants to predict the action based on one state so we call the state zero and we get this here but we want to convert this to a tensor so we say state 0 equals torch dot tensor and as an input it gets the state and we also give it a data type equals let's use a torch dot float here then we call self.model predict with the state this will give us a prediction and this can be a raw value so if we go back to this slide so this can be a raw value and then we take the maximum of this and set this index to a1 so here we say our move equals and we get this by saying torch arc max and the arc max of the prediction and this is a tensor again and to convert this to only one number we can call the item and now this is an integer and now again we can say final move of the smooth index is one and now we have this so now we return the final move here return and yeah this is all we need so now we have this and can save it like this and now we have all that we need for our agent class and now in the next one so what we must do here is implement the model and the trainer and then also the plotting so let's go back to the code and here i left this essay to do so we need a model and a trainer so let's create a new file and let's call this model dot pi and then here let's first import all the things we need so we need import torch then we want to import torch dot n n s n n then we want to import torch dot optim s optim and also import torch dot n n dot functional s capital f and we also want to import o s to save our model and now we want to implement two classes one for the model and one for the trainer so let's create a class and let's call this linear underscore qnet and this has to inherit from nn dot module module and by the way if you are not comfortable with pytorch and want to learn how to use this framework then i have a beginner series here on this tutorial for free and i will put the link in the description so this will teach you everything to need to get started with pytorch so right now let's start implementing this linear qnet function so we need the init function define init and we need to have self and this gets an input size input size a hidden size and an output size and then the first thing we want to do is to call this super initializer so we call super in it and here um this is very simple so if we have a look at the slides then our models should just be a feed forward neural net with a input layer a hidden layer and an output layer um feel free to extend this and improve this but it works fine for this case and it's actually not that bad here so let's create two linear layers so let's call this self.linear1 equals nn.linear and this gets the input size as an input and then the hidden size as the output size then we have self.linear2 equals nn.linear and now this gets the hidden size as the input and the output size as the output then as always in pi torch we have to implement the forward function with self and it gets x so the tensor and here what we want to do is first we want to apply the linear layer and we also use an actuation function here so again if you don't know what this is then check out my beginner tutorial series there i explain all of this so we say x and then we can call f dot reloose we use this directly from the functional module and here we say self dot linear one with our tensor x as the input so first we do the linear layer then we apply the actuation function and then again we apply the second layer so we call self dot linear 2 with x and we don't need an actuation function here at the end we can simply use the raw numbers and return x so this is our forward function then let's also implement a helper function to save the model later so let's call this self safe and this gets the file name as an input and i use a default here so we say model dot pth is simply the file name and then the last time i think i already called this function um so not yet but now we can comment this out so if we have a new high score then we call agent dot model dot save and here let's create a new folder in here so let's say this is the model folder path equals and let's create a new folder in the current directory and call this model so dot slash model and then we check if this already exists so the file in this folder so we can say if not os dot path dot exists and then we say our model folder path then we create this so we say os dot makers and we want to make this model folder path then we create this final file name so we say file name equals os dot path dot join and we want to join the model folder path and the file name that we use here as the input now this is the file name for saving and then we want to save this and we can do this with torch dot save and we want to save self dot state dict so i also have a tutorial about saving the model we only need to save this state dictionary and then as a path we use this file name so now this is all we need for our linear q net and now to do the actual training and optimization we also do this in a class that i call q trainer q trainer and now here what we want to do we want to implement a init function which gets self then it also gets the model then it should get a learning rate and it should get a gamma parameter and here we simply store everything self.lr equals lr self dot gamma equals gamma and we also store the model so we say self dot model equals model then to do a pie charge optimization step we need a optimizer so we can create this by calling self.optim or let's call this optimizer equals and we get this from the optin module and here you can choose one optimizer so i use the atom optimizer and we want to optimize model.parameters and this is a function and then it also needs the learning rate so lr equals self dot l r and now we also need a criterion or a loss function so let's call this self dot criterion equals and now if we go back to these slides at the very end we learned in the first part that this is nothing else than the mean squared error so that's very simple so we can create this here by saying self.criterion equals so this is nn.mse loss and now this is what we need in our initializer and then we also need to define a we call this train step function which gets self and then it needs to have all the stored um parameters from last time so it needs to have or let's have a look at this so here when we call this it needs the state the final move the reward the new states and done so let's copy and paste this in here and rename this slightly so this is just the state this is the action this is the reward so this is the new state this can uh let's call this next state here and then done can stay as it is and for now let's simply do pass here and before we implement this let's go back to the agent and now set this up so here we say from and we call this model and we want to import the linear i think we call this linear q net and q trainer and then here in the initializer we want to create an instance of the model and of the trainer so self.model equals our linear qnet and now this needs the input size the hidden size and the output size so here i use 11 256 and three so remember if we have a look at the slides again um the first one is the size of the state so this is 11 values and the output must be three because and we have three different um three different numbers in our action and you can play around with this hidden size but the other ones have to be eleven and three so this is the model and the trainer equals the q trainer and this gets the model so self.model then it gets the learning rate equals the learning rate which we specified here and we also pass on the gamma value so gamma equals self dot gamma and the gamma is the discount rate so i this has to be a value that is smaller than 1 and usually this is around 0.8 or 0.9 so in this case let's set this to 0.9 so you can play around with this as well but keep in mind that it must be smaller than one so now we have this and then i made one error in the last tutorial so this is very important that we fix this right now so here in the get action function i actually i called this self.model predict but actually pythog doesn't have a predict function so this would be the api for tensorflow for example so in pi torch we simply call self.model like this and then this will execute this forward function so this is actually the prediction then so yeah please make sure to fix this okay so now we have everything and if we have a look and go back then we see we call this self.trainer train step with only one parameter but also with multiple ones so we want to make sure that we can handle different sizes so now let's start implementing this function and now the first thing we want to do so right now this can be um either a tuple or a list or just a single value so let's convert this to a pi torch tensor so let me copy and paste this in here so we do this for the states the next state the action and reward and we can do this by calling torch.tensor and then the variable and we specify the data type to torch dot float and we don't have to do this for the done or game over value because we don't need this as a tensor and now we want to handle multiple sizes so we want to check if the length and then we can check state dot shape and if this is one then we only have one dimension and then we want to reshape this so right now we only have if this is the case then we only have one number but actually we want to have it in the form one and then the values so this is the number of um batches so if this is already if this has already multiple values then it's already in the in the size n and x so then it's already correct and now here we want to append one dimension and we can do this with the torch unsqueeze function so we can say state equals states dot or sorry not state but torch dot un squeeze squeeze and then the states and we want to put it in dimension zero or axis zero so this means that it appends one dimension in the beginning and this is then just one then i also wanted to do this for the other um tensors so for the next state and for action and reward and the done value we also want to convert this right now this is only a single value and we want to convert this to a tuple so we can do it like this so now we have a done so this is how you define a tuple with only one value and now um we have it in the correct shape so now what we have to implement is um from last time or from the very first tutorial where i showed this bellman equation and then we simplified this so we have the old queue where we simply call model predict with the old state and the new queue with this formula so let's do this so first let's um write a comment here so as first thing we want to get the predicted predicted q values with the current state and this is simply by doing let's call this prediction equals self dot model and then we want to do this with state 0 or we just call this state here and then for the second part we need this formula the reward plus the gamma value times the maximum of again model predict with state one so first let's write this as a new uh comment so the first thing is we want to apply this formula reward plus gamma times and then the next predicted q value and then we want to have the maximum so the maximum of this so maximum and then um this is a little bit tricky so the maximum of this um sorry let's do it like this maximum of next predicted q value so this is only one value but um if we do it like as first um parameter the predictions this has actually this is the action this is actually three different values so what we do to get the same here is we do a clone of this and then we set the index with this action to the new q value so this is let's call this q new like i showed you in the formula and then we set the let's call this predictions and then the index of the arc max of the action we set this to our q new value so again this might be tricky so again we want to calculate the new q value with this formula that i showed you but then we need to have it in the same format and for this we simply clone this so then we have three values again and two of the values are the same but the value with the action so the action is for example one zero zero so um the index of the one is then set to the new q value so this is what we want to do here so for this let's first let's create a clone target equals prediction dot clone so we can do this with a pi torch tensor and then um we want to iterate over our tensors and apply this formula so for this we say for index in range and then the length of the let's call this done and here everything should have the same size so then this works so now we iterate over this and then one thing that i didn't mention so far is that we only only want to do this only do this if not done um otherwise we simply take the whole reward so we say q new equals reward of the current current index and now we check if we are not done so we say if not done and the done is of the current index then we apply this formula so now we say q new is actually um the reward so the reward of the current index plus self dot gamma and then times torch dot max the maximum value of the next prediction so here's self dot model of next state of this index so this is exactly what we have written here and now we need to set the target of the maximum value of the action to this value so here we get the let's we call this target so the target of the current index and then of the arc max of the action so for this we can again say torch dot arc max of the of the action and we want to have this as a item so as a value and not as a tensor and now this is our q new value so this might be a little bit tricky to understand so i recommend that that you pause here and go over this again and now we have everything that we need so let's have a look at the slides again we have our q and our q new and then we apply the loss function so the mean squared error and in pi torch so what we have to do here we can simply use this optimizer and do a step and first we have to call this zero grad function to empty the gradient so this is just something that we have to remember in pi torch and then we calculate the loss by calling self dot criterion and here we put in the target and the prediction so this is q new and q and then we call loss dot backward and apply back propagation and then update our gradients and then we call self.optimizer.step and this is all that we need in this training step and now this is actually all that we need in this model file so now again let's go back to the agent and i guess we already set up the q trainer and then when we train this we call this train step function either for only one of those parameters or for a whole batch and now this function can handle different sizes and now the only thing left to do here is to actually to plot the results so for this let's create a new file and let's call this hell helper dot pi and then here let me actually copy and paste this in here so this is just a simple function with matplotlib and i python and yeah here we want to plot the scores so this is a list and we want to plot in the plot the mean score so let's create them so here in the agent we say from helper import the plot function and then down here in the training function so we already created an empty list for the scores and for the mean scores and now after each um game we want to append the score so let's remove the to do and implement this so we say plot scores dot appends and then the current score then let's calculate the new mean or average score so for this let's say total score plus equals the score and then let's call this mean score equals the total score divided by the number of games so agent and games and then we append this to plot mean scores dot append the mean score and then we simply call the plot function with the plot scores and then the plot mean scores and now let's save this file and also let's save this file and then let's try it out so in the terminal let's call agent dot pi and let's cross fingers so we have a syntax error in the model.pi file so um here we actually here we have two equal signs so let's fix this and save this and run it again and then we made another mistake name error so here this is actually called prediction.clone so again let's save this and run this and now it starts training without crashing and it also plots so let's let this run and see if this is improving all right so as we can see the algorithm works and the snake is getting better and better and the scores are getting higher and higher and also the mean or average score is getting higher so i forgot one important thing which i show you in a second but for now um so the snake is not perfect and the main issues are that it traps itself sometimes and also sometimes it gets stuck in an endless loop sequence so this is something that you can improve as a homework so yeah like this it now it trapped itself so yeah let me stop this actually and then show you what i forgot so in the game we can actually um set the speed so for the human controlled game when i want to play this i set this to 20 but now i recommend to set this to a larger number so that the training will be faster so for example you can use 40 here or even higher so i go with 40 and yeah i think that's the whole code you can also find this on github and yeah i hope you really enjoyed this little series about reinforcement learning and if you enjoyed this then please hit the like button and consider subscribing to the channel and then i hope to see you next time bye
