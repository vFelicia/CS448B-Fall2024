With timestamps:

00:00 - learn how to use mistal AI to build
00:01 - intelligent apps all the way from simple
00:03 - chat completions to Advanced use cases
00:06 - like Rag and function calling per borgan
00:09 - from scrimba created this course in
00:10 - collaboration with mistal AI you'll get
00:14 - hands-on experience with mistral's open-
00:16 - Source models including mistl 7B and
00:20 - mistl 8ex 7B and their commercial models
00:24 - by the end of this course you'll Master
00:26 - essential AI engineering paradigms
00:28 - enabling you to create sophisticated
00:30 - conversational user experiences and run
00:33 - AI models locally on your own
00:38 - computer hi there and welcome to this
00:40 - introduction to mistol AI my goal with
00:43 - this course is to teach you how to build
00:45 - magical stuff and more specifically how
00:48 - to do that using JavaScript and mistal
00:51 - AI if you don't know what mistal is it
00:53 - is a company that builds so-called
00:55 - foundational models that in 2023 twice
00:58 - managed to stun the AI Community by
01:00 - launching small open-source foundational
01:03 - models that were on par with the best
01:05 - close Source models out there so as an
01:08 - AI engineer mistel is definitely
01:10 - something that deserves your attention
01:12 - in this course we are going to start off
01:13 - by looking a little bit closer at mistel
01:15 - in general and their platform before we
01:17 - dive into the API Basics and how to use
01:20 - their JavaScript SDK as this course is
01:22 - based around JavaScript though their
01:24 - python SDK is similar so even if you
01:27 - prefer python over JavaScript you'll
01:29 - still get a ton of value value from this
01:30 - course we are also going to go through
01:32 - all of the models that mistl offers at
01:34 - the time of recording this course
01:36 - including their embedding model which
01:38 - lets you work with Vector databases
01:40 - which you'll also get an introduction to
01:42 - in order to give your AI apps domain
01:44 - knowledge which for example could be
01:46 - proprietary company data real-time
01:48 - information that the model hasn't been
01:50 - trained on or for example extra in-depth
01:53 - knowledge about a specific subject that
01:54 - is too narrow for the AI to have been
01:56 - trained on and we'll do this through a
01:58 - technique called retrieve augmented
02:00 - generation AKA rag you'll also learn how
02:03 - to build AI agents with function calling
02:06 - enabling your apps to take action based
02:08 - upon the user prompt a truly
02:10 - revolutionary Paradigm and finally
02:12 - you'll learn how to run your models
02:14 - locally on your computer and interact
02:16 - with them both via the terminal and a
02:18 - web page now who am I I've been a
02:21 - developer instructor and startup founder
02:23 - for almost 10 years now and I'm also the
02:26 - CEO of the learning platform you're on
02:28 - now which is scrimba I use create
02:30 - tutorials on JavaScript react and AI
02:32 - engineering and in total they have been
02:34 - watched by literally millions of people
02:37 - through the scrimba platform corsera and
02:39 - YouTube I love to connect with my
02:41 - students so please click on either of
02:43 - these links if you're interested in
02:44 - connecting on either X or LinkedIn now
02:47 - you'll also see lessons from two other
02:49 - teachers as well throughout this course
02:51 - namely from Gil Hernandez one of our
02:52 - brilliant instructors here at scrimba
02:54 - and we're also proud to have Sophia Yang
02:57 - the head of developer relations at
02:59 - mistol contributing to this course so as
03:02 - you probably understand now this course
03:04 - is a collaboration between mistol and
03:06 - scrimba so we're not pulling this
03:08 - curriculum out of thin air it has been
03:10 - created in partnership with the company
03:12 - itself if you ever find yourself lacking
03:14 - some JavaScript skills or AI engineering
03:17 - Concepts please check out our frontend
03:19 - developer career path or this AI
03:20 - engineering course as those will help
03:22 - you get up to speed so with that let's
03:25 - get
03:28 - started hello it's this is Sophia yam
03:30 - from Mr AI I like to welcome you to the
03:33 - course and give you a brief introduction
03:35 - of mrol Mr AI was founded last year by
03:39 - our three co-founders Arthur Tim and gam
03:42 - we first released our open W model Mr 7B
03:46 - in September last year we released a x7b
03:50 - mixture of experts model and that
03:53 - platform in December we currently have
03:56 - offices in Paris London and San
03:58 - Francisco Bay Area
04:01 - we offer six models for all use cases
04:04 - and business needs including two open
04:06 - source models mro 7B and mixol 8 x7b
04:11 - they're under open source AP par 2.0
04:13 - license they great to started
04:16 - experimenting with we also offer four
04:19 - optimized Enterprise grate models Mr
04:22 - small for low latency use cases Mr
04:25 - medium or language based tasks and Mr
04:27 - Large for your most sophisticated needs
04:30 - we also offer an embedding model which
04:32 - offers the State ofth art embeddings for
04:35 - text to get started you can use our chat
04:39 - assistant L to interact with our model
04:41 - right away just go to chat. m.ai and you
04:46 - can play with Lua there are several ways
04:48 - to use our models we offer API end
04:51 - points for all of our models through the
04:54 - platform you can subscribe and get an
04:56 - API key on the platform this is the
04:59 - easiest to use and deploy you can also
05:02 - use our model on cloud services which
05:05 - provide fastest deployment for
05:07 - Enterprise especially for those who
05:09 - already use cloud
05:11 - services you can also self- deploy our
05:14 - models on your own on Prem
05:16 - infrastructure this will give you more
05:18 - control and flexibility but it's the
05:21 - most complex among the three so it's a
05:24 - tradeoff between ease of deployment and
05:26 - level control so you can choose
05:28 - whichever you want for your own use
05:30 - cases and your business needs this
05:33 - course will focus on the platform and
05:35 - how to use Mr API for various tasks hope
05:39 - you enjoy the
05:43 - course okay in order to interact with
05:46 - the mistal API you need an API key which
05:49 - will'll get through their platform or La
05:51 - platform as they call it so click on
05:53 - this image right here and you'll be
05:54 - taken to the mistal homepage and there
05:56 - you can click on the build now option
05:59 - that'll take you to the authentication
06:00 - screen so choose however authentication
06:02 - method you want and then in the next
06:05 - step you're asked to create a worksspace
06:06 - name and check off whether you're a solo
06:09 - Creator or doing this as a team member
06:11 - in a company whatever you choose click
06:13 - create workspace and there we go this is
06:16 - the platform and in order to get access
06:18 - to the API you have to provide a card or
06:21 - subscribe as they say here however you
06:24 - only pay for what you use so this is not
06:26 - an ongoing fixed subscription so just
06:28 - add your card and once you done that
06:30 - this box will go away and you can click
06:32 - on API keys to create Keys you can
06:34 - authenticate with click on the create
06:36 - new key and give it a name and an
06:38 - expiration date and then create key now
06:41 - you'll only see this key once so be sure
06:44 - to save it as a scrimba environment
06:45 - variable you learn how to do that by
06:47 - clicking on this link right here and
06:50 - please don't take the time to try and
06:51 - copy this API key right here by the time
06:54 - you watch this scrim this key is no
06:56 - longer active as I've deleted it so go
06:58 - ahead and follow these steps and set the
07:00 - N variables in scrimba and then in the
07:02 - next scrim my colleague Gil will teach
07:04 - you the basics of how to interact with
07:06 - the mistal API through
07:10 - JavaScript hey in this tutorial we'll go
07:13 - over using the chat completion API which
07:16 - allows you to chat with a model that's
07:18 - fine-tuned to follow instructions so
07:20 - let's Dive Right In we're going to use
07:22 - mistral's JavaScript client which I've
07:24 - installed and set up in this interactive
07:26 - scrim I'm importing mistal AI at the top
07:29 - of the Javascript file and I've
07:31 - instantiated a mistal client using my
07:33 - API key which I've stored as an
07:35 - environment variable on scrimba so we're
07:38 - ready to go the chat completion endpoint
07:40 - is designed to handle back and forth
07:42 - conversations you feed it a prompt and a
07:44 - series of messages and it generates a
07:47 - completion or an appropriate
07:48 - continuation of that conversation so now
07:51 - let's make our first chat request using
07:53 - ml's chat method I'll declare a constant
07:55 - named chat response to store the
07:58 - response returned from the chat request
08:00 - which will await with await client. chat
08:04 - and pass the method an object containing
08:06 - the request body the chat completion API
08:08 - accepts various parameters the two
08:10 - required parameters are model and
08:13 - messages mistol has various pre-trained
08:16 - models you can use with the API for our
08:19 - purposes we'll use a model called mistal
08:21 - tiny then I'll set the messages
08:23 - parameter to an array and this is a key
08:26 - part of the chat request as it holds the
08:28 - prompts to generate completion for this
08:30 - should be an array of message objects
08:32 - each with role and content properties
08:35 - role defines the role of the message
08:38 - I'll set it to user indicating that the
08:40 - message is from the user's perspective
08:43 - then set content to the actual content
08:46 - of the user message this is usually a
08:49 - question like what is the best French
08:51 - cheese all right and this is all we need
08:53 - to generate a chat completion so let's
08:56 - log the response to the console and the
08:58 - way to access the message content
08:59 - directly is like this I'll run this code
09:02 - by clicking the Run button and good the
09:05 - API returns a humanlike response about
09:07 - the different types of French cheese all
09:10 - right so what I want you to do now is
09:12 - personalize the AI response by updating
09:14 - the content property to something that
09:17 - interests you you might not have
09:18 - realized this yet but this isn't your
09:20 - typical video player you are
09:22 - experiencing a fully interactive scrim
09:25 - that you can pause at any moment and
09:26 - jump right into the code and make
09:28 - changes to it so go ahead and ask the AI
09:30 - a question then click
09:36 - run okay hopefully that was fun and you
09:38 - got some great responses now let's
09:41 - experiment with other parameters to make
09:42 - our response more interesting we'll use
09:45 - the temperature parameter to set the
09:47 - creativity or randomness of the
09:48 - generated text and this should be a
09:50 - value between 0 and 1 Now the default
09:53 - temperature is
09:54 - 0.7 but as you get closer to one the
09:57 - output will be more random and creative
10:00 - while lower values make the response
10:02 - more focused and deterministic I'll set
10:05 - it right down the middle at 0.5 to
10:07 - strike a balance between creative and
10:09 - predictable responses and now I'll feed
10:12 - it a different question like I want a
10:14 - puppy what is the most kid-friendly
10:17 - dog I'll run this code and I get back a
10:20 - detailed conversational response about
10:22 - various dog breeds good all right I want
10:25 - you to go ahead and pause me now and try
10:27 - experimenting with different temperature
10:31 - values you can also provide custom
10:34 - system prompts to guide the behavior of
10:36 - the model this time I'll set roll to
10:38 - system then set content to the
10:41 - instructions or prompt for the model
10:43 - this is your chance to influence how the
10:44 - AI response so I'm instructing it that
10:47 - it's a friendly cheese kind of sore and
10:49 - that when asked about cheese to reply
10:52 - concisely and humorously now running
10:55 - this won't work because now we need to
10:56 - follow the system role with a user role
10:59 - in content I'll set the role property in
11:02 - this second message object to user then
11:05 - set this content property to ask what is
11:07 - the best French cheese I'll run this
11:09 - code and I get back a fun and witty
11:12 - response about French cheese fortunately
11:14 - it's always cheese season right all
11:16 - right so that's it for the basics of
11:18 - working with the chat completion
11:22 - API now that you've gotten to know the
11:24 - basics of how to set up a request to
11:26 - mistol let's have a look at some of the
11:28 - options and configurations you as a
11:30 - developer can adjust so that you tweak
11:32 - the response you get from mol to your
11:34 - needs and perhaps the most apparent one
11:37 - is adding support for streaming because
11:39 - that is often a key feature of AI apps
11:42 - for example here on hugging face the
11:44 - platform for open- Source AI models and
11:47 - data sets on the mistal organization
11:49 - there's a hosted version of one of their
11:51 - models along with a chat interface so
11:53 - that you can talk with it so here I'll
11:54 - ask it the question what's your favorite
11:56 - Taco ingredient and when I send that I
11:58 - immediately see the response getting
12:00 - built up token by token until it's done
12:03 - and this is a really Pleasant user
12:05 - experience so let's see how we can tweak
12:07 - this from just giving us the entire
12:09 - response to giving us one token at a
12:12 - time so the first thing we need to do is
12:14 - change this from chat to chat stream
12:17 - like that what then happens is that this
12:19 - chat response changes from being a
12:21 - regular object to being a so-called
12:23 - async iterable meaning that we have to
12:26 - await as every item in this iterable
12:28 - becomes available to us so chat response
12:31 - will kind of gradually be built out as
12:33 - we get token by token from the mystal
12:36 - API and the way to deal with this is to
12:38 - create an asynchronous for of loop so
12:41 - we'll do for A8 and then const chunk of
12:47 - chat response and every time the body of
12:50 - this for Loop is executed we get access
12:52 - to a new chunk and as for the chat
12:55 - response this is an object with many
12:57 - properties so we'll have to navigate all
12:59 - almost in the same way as we navigated
13:00 - into the chat response do choices though
13:03 - instead of message it's called Delta so
13:06 - if we now try to console log out this
13:09 - and comment this one out let's see what
13:12 - happens and yes we are getting a ton of
13:15 - stuff logged to the console super fast
13:18 - so this kind of buildup of the response
13:20 - would happen almost instantly and
13:22 - probably a lot faster than we could read
13:24 - it though it's a lot better user
13:26 - experience than having to wait until the
13:27 - entire thing is generated and and then
13:29 - get the response in one go okay let's
13:31 - have a look at another cool
13:32 - configuration you can make to the
13:34 - request and that is to tell mistl that
13:36 - you want the reply in the format of Json
13:39 - that is Javascript object notation here
13:41 - is an example of a Json string and if
13:44 - you don't know what is it is essentially
13:45 - a very common schema that developers use
13:47 - when sending and processing information
13:49 - so being able to get this kind of format
13:51 - from the AI is super helpful as you
13:53 - integrate it with your app and doing
13:56 - this only requires two small settings
13:58 - the first one being that you need to set
13:59 - the
14:00 - response format as an object of type
14:06 - Json object like that and then you also
14:08 - need to specify it in the prompt so here
14:11 - I'll write reply with Json like that
14:15 - here the data will be processed by code
14:17 - and not by a human first and foremost so
14:19 - let's skip this streaming here because
14:22 - it is mostly for the ux directed at
14:24 - humans and then go back to chat here and
14:26 - finally uncomment this one and then like
14:29 - that so let's run the code and yes there
14:32 - we get a Json object I'll copy it from
14:35 - the console paste it in here and there
14:37 - we can see it is an object with a key
14:39 - answer that talks a little bit about
14:41 - good cheese and then it also has a
14:42 - cheese key with a subsequent name key
14:45 - cheese key which is an object that has
14:47 - three keys name country and type so you
14:50 - can imagine it being a lot easier to
14:52 - extract the metadata from this reply as
14:54 - opposed to Simply getting a couple of
14:56 - sentences so I would recommend you to
14:58 - play around with this check out the
15:00 - documentation and see what other
15:01 - configurations and modifications you can
15:03 - make to this response and then once
15:05 - you're ready I'll see you in the next RM
15:07 - where we'll dive more into what we've
15:09 - configured on this specific line which
15:11 - is the models themselves that mistl
15:13 - provides as it's important to have a
15:15 - good overview in order to choose the
15:17 - right ones for the job so I'll see you
15:22 - there hey in this Grim we're going to
15:25 - look at the various models mistal offers
15:28 - now be aware though that these are the
15:29 - models it offers at the time of
15:31 - recording this scrim you should
15:33 - definitely click on this image right
15:35 - here so that you're taken to the landing
15:37 - page for their models as there you can
15:39 - click around and check out their latest
15:41 - optimized commercial models as well as
15:43 - their open models now speaking of open
15:46 - models mistol Rose to prominence in the
15:48 - AI community in 2023 when they launched
15:50 - their first model mistol 7B that is a
15:53 - model that has so-called open weights
15:55 - meaning that you can download it to your
15:58 - computer or upload it to a server and
16:00 - use it as a part of your application
16:02 - without paying mistel a dime one of the
16:05 - things that stunned the AI Community was
16:08 - how powerful it was despite only having
16:10 - 7 billion parameters as the leading open
16:13 - models back then had many more
16:15 - parameters than this even an order of
16:17 - magnitude more now a little later mistol
16:19 - launched the so-called mixol adex 7B
16:22 - which also is an open model and has a
16:25 - unique architecture that allows it to be
16:27 - much more powerful though only slightly
16:30 - more expensive to run inference on the
16:32 - core idea behind this one is that it
16:34 - uses a mix of eight different so-called
16:37 - experts so the total number of
16:39 - parameters here is actually 8 * 7 which
16:43 - is 46 though when you run inference it
16:45 - only Taps into one of these experts and
16:48 - it actually uses around 13 billion
16:50 - parameters when being run now at this
16:53 - point you might be a little bit confused
16:54 - and want to know more about this I don't
16:56 - want to go more into the technical
16:58 - details here because I don't think it's
17:00 - that important in order to use these
17:01 - Technologies though if you are
17:03 - interested feel free to click on this
17:05 - image right here and you'll be taken to
17:07 - a article which talks more in depth
17:09 - about the Mixel model moving on to the
17:11 - next models those are the mistal small
17:14 - mistal medium and mistal large and these
17:16 - are not so-called open weights meaning
17:18 - that you can simply download them from
17:20 - their website and get started locally
17:22 - you either have to use this VI cloud
17:24 - provider that supports these models or
17:26 - you can do self hosting as well though
17:28 - to to do that you have to talk with the
17:30 - mistal team now if we compare these
17:32 - models side by side with their
17:34 - performance on the MML U test as the
17:37 - height of each bar here you can see that
17:39 - the commercial models are more powerful
17:41 - than the open models though the small
17:43 - commercial model and the mix dra are
17:45 - quite within the same range now if you
17:47 - don't know what MML U is it is a common
17:50 - way to test llms it's short for massive
17:53 - multitask language understanding and it
17:55 - puts llms to the test through a range of
17:58 - different tasks giving them a score from
18:00 - 0 to 100% based upon how well they
18:03 - perform now looking at this image it
18:05 - seems that we always should go for the
18:07 - mistal large model but that's actually
18:10 - not the case because the flip side of
18:12 - using a better model is very often that
18:14 - it is more expensive so if we plot this
18:17 - models out on a two-dimensional space
18:19 - with the cost per million tokens on the
18:21 - x-axis and the ml U score on the Y AIS
18:25 - you can see that the picture is
18:26 - definitely different because mistal is
18:29 - by far the most expensive model over
18:31 - twice as expensive as the mistal medium
18:34 - so here if you are able to get the job
18:36 - done with medium you should definitely
18:38 - choose that one analogy you can think of
18:40 - here is when hiring people at a company
18:43 - in many cases you probably don't want to
18:45 - hire a person that is overeducated or
18:48 - over qualified for the job because most
18:50 - likely their hourly rate will be higher
18:53 - so how do you then decide which model to
18:55 - use if you want to dive more into this
18:57 - subject just click on this image here
18:59 - and you'll be taken to the guide in the
19:01 - docs which specifically talks about
19:03 - model selection there you can see some
19:05 - use case examples on what kinds of
19:07 - typical tasks a model is suitable for so
19:10 - for example the mistal small works well
19:12 - for things like classification and
19:14 - customer support whereas the mystal
19:16 - medium is the ideal model for
19:18 - intermediate tasks that require moderate
19:20 - reasoning that could be things like data
19:22 - extraction summarizing a document
19:25 - writing a job description and so forth
19:27 - and finally if you want to do more
19:29 - complex tasks Mr Large is your go-to
19:32 - model so later in this course we are
19:34 - going to create a little agent that can
19:36 - call functions on behalf of users in
19:38 - addition to doing so-called retrieval
19:40 - augmented generation AKA Rag and in
19:43 - those cases we are going to use the
19:44 - large model as those require significant
19:47 - reasoning capabilities and on that note
19:50 - what is exactly rag well you'll figure
19:53 - out in the next
19:57 - scrim here at scrim but we use an app
19:59 - called notion for notes taking and with
20:02 - a team of several teachers developers
20:05 - people in operations and so forth we
20:07 - have a lot of Internal Documentation and
20:10 - it quickly becomes chaotic so here we
20:12 - have a courses and teaching page which
20:15 - again contains a bunch of sub pages and
20:17 - they themselves also have sub Pages as
20:19 - well so it is actually quite hard at
20:21 - times to get to the answer you want to
20:23 - get to which is why I was really glad
20:25 - when lotion launched their ask AI
20:27 - feature which is essentially means that
20:29 - you can ask questions to notion so one
20:32 - day when I was working on our corsera
20:34 - exports I seemed to remember that we
20:36 - needed a widget for doing these exports
20:39 - and I asked it about exactly that it
20:42 - thought a little bit and then came with
20:43 - an answer yes you are correct for corera
20:46 - courses a type of item called plug-in is
20:49 - used to embed scrims and this is quite
20:51 - interesting because I asked for a widget
20:54 - but the AI understood that well actually
20:56 - I meant the plugins so it's shared with
20:59 - me through this footnote here the link
21:01 - to the document that talked about these
21:03 - corsera plugins and this kind of user
21:05 - experience is a GameChanger for web apps
21:08 - suddenly it is much easier to find the
21:11 - information you need and also you give
21:13 - the llm access to proprietary data as
21:17 - obviously the underlying model here does
21:20 - not have any knowledge about how we at
21:21 - scrimba internally embed our scrims in
21:23 - corsera courses now this whole
21:26 - experience was only possible through
21:28 - something called retrieval augmented
21:30 - generation which Probably sounds very
21:32 - complex but don't worry we'll go through
21:34 - it step by step and we won't refer to it
21:37 - through this long complex name here
21:39 - we'll use the popularized expression rag
21:42 - okay so rag contains of mainly two steps
21:44 - there's the retrieval step fetching the
21:46 - data you need to reply to the user's
21:48 - question and there's the generation
21:50 - taking whatever information you found
21:52 - and using that as context when
21:54 - generating the conversational reply back
21:57 - to the user so if if we zoom in on the
21:59 - retrieval first this is very often done
22:02 - in collaboration with a so-called Vector
22:04 - database that is a specific type of
22:06 - database that is optimized for storing
22:09 - information in a specific format that
22:11 - makes it easy for AI to reason about it
22:14 - so it stores so-called embeddings now at
22:17 - this point you're probably a little bit
22:19 - confused what's this thing about vectors
22:21 - and embeddings and all of that don't
22:23 - worry about it we'll get back to that
22:25 - later for now I just want to explain rag
22:27 - on a very high level so what you do is
22:30 - you take all of your data and shove it
22:32 - into a vector database in this specific
22:34 - embedded format and then you take the
22:36 - search query or the input from the user
22:38 - and turn that into an embedding as well
22:40 - as that gives you the opportunity to do
22:42 - a so-called semantic search and get
22:45 - these search results which intelligently
22:47 - for example understand that no pair
22:49 - wasn't looking for a widget he was
22:51 - actually looking for this and thus fetch
22:53 - the relevant data for the app that is
22:55 - the retrieval part once you've done that
22:57 - you take the user input that is the
22:59 - question I asked which was a very
23:01 - humanly written sentence about I seem to
23:03 - remember something about a corsera
23:05 - wouldit blah blah blah and then you
23:07 - combine that with the search results we
23:09 - got in the retrieval step and turn it
23:11 - into a singular prompt that the llm can
23:14 - use as input so mistal AI takes that
23:16 - prompt and the relevant context we
23:19 - retrieved and turns that into a very
23:22 - humanly readable response with in many
23:24 - cases a footnote or link to the
23:27 - underlying data as well thus providing
23:29 - the user a way of factchecking the claim
23:31 - that the AI comes with now there's one
23:34 - thing that all of this relies on which
23:36 - is our ability to turn data for example
23:39 - a sentence into numbers that the AI can
23:43 - understand now all of this relies in our
23:45 - ability to create something called
23:48 - embeddings and what is an embedding well
23:51 - it is what you get when you take a piece
23:52 - of data for example the string hello
23:55 - world and run it through an AI model
23:57 - that turns it into a long array of
24:00 - numbers also known as a vector and as we
24:03 - build out a rag solution in this course
24:05 - it is really important that you have an
24:07 - intuitive understanding of what this
24:09 - embedding concept is so before we
24:11 - continue on with our rag project I'll
24:14 - leave the mic to my colleague Gil
24:15 - Hernandez who will give you a primer on
24:17 - embeddings in the next
24:22 - scrim whether you realize it or not AI
24:25 - powered search shapes many parts of your
24:27 - daily lives every day you interact with
24:30 - platforms sifting through massive
24:31 - amounts of data from text and images to
24:34 - audio and video think about Amazon
24:36 - recommending products or search engines
24:38 - refining your queries social media
24:40 - platforms curate tailored content while
24:43 - services like YouTube Netflix and
24:45 - Spotify offer suggestions based on your
24:47 - preferences now Advanced AIS despite
24:50 - their capabilities don't truly
24:52 - understand the real world as we do they
24:54 - can't grasp the actual meaning or Nuance
24:56 - of a video title song or news article so
24:59 - how exactly do AIS and platforms like
25:01 - Spotify Netflix and YouTube truly get us
25:05 - how is it that they appear to understand
25:07 - predict and respond to us as effectively
25:09 - as if not better than people well the
25:11 - magic behind this capability involves a
25:13 - blend of algorithms AI models and huge
25:17 - amounts of data but a larger part of the
25:19 - answer involves embeddings you see when
25:22 - you present a question to an AI it first
25:24 - needs to translate it into a format it
25:26 - can understand so you can think of
25:28 - embeddings as the language that AI
25:31 - understands the term embedding is a
25:33 - mathematical concept that refers to
25:35 - placing one object into a different
25:37 - space think of it like taking a word or
25:40 - sentence which is in a Content space and
25:42 - transforming it into a different
25:44 - representation like a set of numbers in
25:46 - a vector space all while preserving its
25:48 - original meaning and the relationships
25:50 - between other words and phrases AI
25:53 - systems process lots of data from user
25:56 - inputs to information and databases at
25:58 - the heart of this processing are
26:00 - embeddings which are vectors
26:01 - representing that data transforming
26:03 - content like search queries photos songs
26:06 - or videos into vectors gives machines
26:09 - the power to effectively compare
26:11 - categorize and understand the content in
26:14 - a way that's almost human so how is all
26:17 - of this possible well it isn't exactly
26:19 - as easy as just turning data into
26:20 - vectors so before we go any deeper let's
26:23 - take a closer look at what vectors are
26:25 - think of a vector as a coordinate or
26:27 - point in space and to keep things simple
26:29 - we'll have a look at this 2D graph with
26:31 - an X and Y AIS let's say that a word
26:33 - like cat is translated into a vector
26:36 - like 4.5 12.2 which is this point this
26:40 - Vector encapsulates the meaning and
26:42 - nuances of the word cat in a way an AI
26:45 - model can understand and then we have
26:47 - the word feline represented by a nearby
26:49 - Vector of 4.7 12.6 so we'll place that
26:53 - point on the graph now words that have
26:55 - similar meanings are numerically similar
26:57 - and tend to be be closely positioned in
26:59 - the vector space so this closeness
27:01 - implies that cat and Feline have similar
27:03 - meanings now let's say we have the word
27:05 - or vectors for kitten which might also
27:08 - be close to cat and Feline but maybe
27:10 - slightly further apart due to its age
27:12 - related Nuance now a dog is different
27:16 - but still in the same general domain of
27:18 - domesticated animals so the word dog
27:20 - might be represented by a vector that's
27:23 - not too distant but clearly in a
27:24 - different region let's say 7.5 10.5
27:28 - and even a phrase like Man's Best Friend
27:30 - which is a colloquial term for a dog
27:32 - could be represented by a vector that's
27:34 - close to the vector for dog on the other
27:37 - hand a word like building is not related
27:39 - in meaning to any of these so its Vector
27:42 - would be much further apart let's say
27:44 - 15.3
27:46 - 3.9 here's another example that
27:48 - demonstrates how embeddings might
27:49 - capture semantic meaning and
27:51 - relationships between words let's say we
27:53 - have the word King represented by the
27:55 - vector 25 then man man is the vector 13
28:00 - and woman is represented by the vector
28:03 - 14 now let's do some quick Vector
28:05 - arithmetic we'll start with the vector
28:07 - for King then subtract the vector for
28:09 - man to remove the male context and add
28:11 - the vector for woman to introduce new
28:13 - context after performing this Vector
28:15 - math our resulting Vector is
28:18 - 26 so we'll plot that point on the graph
28:21 - and let's say there's another word in
28:22 - our space queen represented by the
28:24 - vector 2 6.2 right here well this Vector
28:28 - is extremely close to the resulting
28:30 - Vector so we might identify queen as the
28:33 - most similar word based on that Vector
28:36 - just as a trained AI model would now a
28:39 - two-dimensional graph is a massive
28:41 - simplification as real world embeddings
28:43 - often exist in much higher dimensional
28:46 - spaces sometimes spanning hundreds or
28:48 - even thousands of dimensions for example
28:50 - the actual Vector embedding for the word
28:52 - Queen might have values across multiple
28:54 - Dimensions each Dimension or number in
28:56 - this Vector might capture a different
28:58 - semantic or contextual aspect of the
29:00 - word Queen for instance royalty
29:02 - Cleopatra or even chess this is what
29:05 - allows the AIS to recognize and
29:07 - differentiate between these contexts
29:09 - when the word is used in different
29:10 - scenarios now imagine embedding hundreds
29:13 - of thousands of words and phrases into
29:15 - this high-dimensional space some words
29:17 - will naturally gravitate closer to one
29:19 - another due to their similarities
29:21 - forming clusters While others are
29:23 - further apart or sparsely distributed in
29:25 - the space these relationships between
29:28 - vectors are extremely useful think back
29:30 - to spotify's method of embedding tracks
29:32 - in a vector space tracks that are
29:34 - positioned closely together are likely
29:35 - to be played one after the other all
29:37 - right so what else can we do with
29:39 - embeddings and how are they used in the
29:40 - real world well you can imagine how
29:42 - embeddings have revolutionized our daily
29:44 - experiences for example search engines
29:47 - have evolved to understand the essence
29:49 - of your queries and content moving
29:51 - beyond mere keyword matching and
29:53 - recommendation systems with the aid of
29:55 - embedding suggest products movies or
29:57 - songs that truly resonate with our
29:59 - preferences and purchase history for
30:01 - example Netflix uses them to create a
30:03 - tailored and personalized platform to
30:05 - maximize engagement and retention also
30:07 - in the healthcare industry embeddings
30:09 - are used to analyze medical images and
30:11 - extract information doctors can use to
30:14 - diagnose diseases and in the finance
30:16 - World embeddings help with analyzing
30:18 - financial data and making predictions
30:20 - about stock prices or currency exchange
30:22 - rates so every time you interact with an
30:24 - AI chatbot every time an app recommends
30:27 - something behind the scenes embeddings
30:29 - are at work translating data into
30:31 - meaning all right so how are these
30:33 - embeddings actually created well let's
30:35 - dive into that
30:39 - next before we create our embeddings
30:42 - there's one important thing you need to
30:43 - learn and that is how to split text
30:46 - because as an AI engineer you'll find
30:48 - yourself having to split text again and
30:50 - again because let's say that you are
30:52 - working on an internal employee handbook
30:55 - app which lets employees ask questions
30:57 - about the compan policies well in which
30:59 - casee you probably have a large data
31:02 - source like the one you can see here in
31:04 - handbook. text which contains all of the
31:06 - data that you need to embed however
31:08 - creating one embed of this entire thing
31:11 - would just be meaningless there's far
31:13 - too many subjects and themes talked
31:15 - about in this handbook so it wouldn't
31:17 - really have any specific semantic
31:19 - meaning of value it would be far too
31:21 - broad so what we're going to do is take
31:23 - this document and split it into chunks
31:26 - and then we'll create an embedding of of
31:28 - every single chunk now creating such
31:30 - chunks is actually a little bit complex
31:32 - though luckily we have a tool to help us
31:34 - with that and that is Lang chain one of
31:36 - the leading libraries for AI Engineers
31:39 - so what we'll do is enhance this
31:41 - function so that it uses the Lang chain
31:43 - text splitter because as you can see
31:45 - this doesn't do much at the moment it's
31:47 - simply an async function that fetches
31:49 - the handbook and calls do text on the
31:52 - response thus giving us all of the text
31:55 - in this handbook let's run the code and
31:58 - just see that it works yes there we have
32:00 - it so now we can use Lang chain to split
32:02 - this into smaller chunks I'll import the
32:04 - Lang chain Library here as a dependency
32:07 - and then let's figure out which specific
32:09 - tool we need to import from Lang chain
32:11 - the simplest one is the character text
32:13 - splitter though the recommended one to
32:15 - use is the recursive character text
32:17 - splitter so that's the one we're going
32:18 - to use so here we'll do import recursive
32:22 - character text Splitter from Lang chain
32:26 - SL text splitter like that
32:28 - now we can create a new recursive
32:31 - character text splitter this is a
32:32 - Constructor function that takes an
32:34 - object as the argument and here you
32:36 - define two things the size of the chunk
32:38 - and how much overlap you want between
32:40 - the chunks we'll try for example 250
32:43 - characters for the size of the chunk
32:45 - that feels like a sentence or two and
32:47 - will allow for some overlap for example
32:49 - 40 characters we'll call our splitter
32:52 - simply splitter like that and then we
32:55 - can do splitter. create document
32:58 - and pass in the text this is an async
33:01 - function so we have to await it and
33:03 - store the result in a variable called
33:05 - for example output like that now if we
33:09 - log out the output let's run the code
33:13 - and there I got an error and that is
33:14 - because I have a typo I called the text
33:16 - splitter which is wrong it should be
33:18 - text splitter like that let's run the
33:22 - code again yes there we go as you can
33:25 - see in the console there are a bunch of
33:27 - data there and if we open the dev tools
33:29 - we'll be able to inspect it a little bit
33:31 - more in detail so let's do that here as
33:33 - you can see it is an array which
33:34 - contains 2 180 objects let's open up one
33:38 - of these objects and there we can see
33:39 - that we have the text itself under the
33:42 - page content property and also under the
33:44 - lines property we get the exact lines
33:46 - this content comes from in the handbook.
33:49 - text file that is very handy in case you
33:52 - want to create footnotes or reference to
33:54 - the original Source in your app now what
33:56 - you want to make sure of when you
33:57 - respect your data like this is that each
33:59 - of these trunks ideally only deal with
34:02 - one subject or theme that is how you
34:04 - create good embeddings if a given trunk
34:06 - is quote unquote polluted by different
34:08 - themes it'll be harder for the embedding
34:10 - model to create a meaningful Vector of
34:13 - it so here you can see that this trunk
34:15 - deals with delegation of authority and
34:17 - responsibility and the administration
34:20 - and the executive director so definitely
34:22 - a coherent subject though it's actually
34:24 - been split in the middle of two
34:26 - sentences so it could probably be better
34:28 - as well we have probably not struck the
34:30 - perfect balance here you could argue
34:32 - that it would have been better to split
34:34 - this into two and then use the entire
34:35 - sentences or maybe expand it in both
34:38 - ends and include both of the complete
34:40 - sentences in general the shorter your
34:42 - chunks are the more precise meaning the
34:45 - embedding will get though you might also
34:47 - miss some wider context and the longer
34:50 - the trunks are the more context they
34:52 - contain but it can also produce too
34:53 - broad of a scope of information and this
34:55 - would reduce the quality of the
34:57 - similarity search that the vector
34:59 - database does as the underlying meaning
35:01 - of the embedding could be ambiguous it
35:03 - could point in two different directions
35:05 - so to speak in general you want your
35:07 - chunks to be as small as possible but
35:09 - you don't want to lose context so it's
35:11 - definitely a balance to strike and
35:13 - something you'll probably only find
35:15 - through experimentation creating smaller
35:17 - and bigger chunks and actually seeing
35:19 - how it plays out in action in your app
35:21 - for now we'll stick with this and see
35:23 - how it works as we continue on building
35:25 - this rag feature let's carry on
35:31 - in the previous scrim I wrote all of the
35:33 - code for you but as you know this is a
35:35 - scrimo course meaning that your job is
35:38 - to get your hands on the keyboard and
35:40 - write the code out yourself so I left
35:42 - out a couple of pieces for you which you
35:44 - now are to implement through this
35:46 - challenge I want you to refactor this
35:48 - function so that it first of all takes
35:50 - the path to the data or document as an
35:53 - argument so that is to the handbook.
35:55 - text here that'll make it a little bit
35:56 - more generalized
35:58 - as it's really not a good practice to
36:00 - have the path for the fetal request
36:01 - hardcoded in here on line seven and then
36:04 - secondly I want you to return the
36:06 - splitted data as an array of strings and
36:08 - just that because that's how we want our
36:10 - data in the next step of building out
36:12 - this feature so go ahead and solve this
36:14 - challenge right
36:20 - now okay hopefully that went well first
36:24 - we'll specify that it takes a path here
36:26 - as the argument which we'll use in the
36:28 - fetch request and then of course we'll
36:30 - need to specify in the function
36:31 - invocation that we indeed want to get
36:34 - the data from the handbook. text that
36:36 - was part one part two returning the data
36:39 - as an array of strings if you remember
36:40 - from the previous Grim when we inspected
36:42 - this data it is actually an array of
36:44 - objects right now but this time around
36:46 - we only want the data that is within the
36:48 - page content property because we do not
36:51 - care about the location metadata at this
36:53 - point so here we'll take the output and
36:56 - we'll map through it
36:58 - and for each of these trunk objects
36:59 - we'll return trunk. page content like
37:04 - that and here we can store that in a
37:07 - variable called text R for text array
37:10 - and then simply return it now you can of
37:12 - course condense these into fewer lines
37:14 - of code but I like to be explicit and
37:16 - only do one thing at a time on each line
37:19 - so with that we are done and ready to
37:21 - carry
37:24 - on now it is finally time to use the
37:27 - myal API to create our very first
37:29 - embedding as you can see I have imported
37:31 - the mystal client and added my API key
37:34 - so we are ready to get going the first
37:36 - thing I need is an example text trunk to
37:38 - create an embedding of I happen to have
37:40 - copied one of them into my clipboard so
37:42 - I'll paste it in here and call it
37:44 - example trunk as you can see it says
37:46 - professional ethics and behavior are
37:48 - expected of all ambri employees further
37:51 - ambri expects each employee to display
37:53 - good judgment so this is a quite good
37:55 - text for embedding because it deals with
37:57 - one subject which is the expectation of
37:59 - characters for ambri employees now I'll
38:02 - comment this one out as we won't call
38:04 - this function right now instead we'll
38:06 - down here at line 22 call the client do
38:11 - embeddings function that is an async
38:13 - function so we have to await it and
38:16 - inside of the parameter the object we'll
38:18 - specify first what kind of model we want
38:20 - to use and here mistol provides an
38:22 - embedding model called mistol embed and
38:26 - then the second key in the sub
38:29 - is the input now we can't just paste the
38:31 - example trunk like this as this input
38:34 - isn't expecting a string it's actually
38:36 - expecting an array of strings so we have
38:38 - to do like this we'll store the response
38:41 - we get back from this in a const called
38:43 - for example
38:45 - embeddings response like that and then
38:48 - let's finally log it out I'll run the
38:50 - code and yet again I had a typo Mistral
38:54 - with r is what we want to write not Mall
38:56 - we'll try again and there we go we got
38:59 - something very interesting back let's
39:01 - paste it into the editor to inspect it a
39:04 - bit more like that here we can see it
39:07 - has an ID and under the data property we
39:10 - have an array that holds an embedding
39:12 - and that embedding is a long array of
39:16 - floating Point numbers all of which
39:18 - seemingly are very close to zero though
39:21 - slightly more or slightly less so this
39:23 - Vector right here is an embedding of
39:25 - this specific text as transformed by
39:28 - this model and as we use this model to
39:30 - transform other pieces of text the
39:33 - mathematical distance from the various
39:35 - vectors will be a reflect of how similar
39:38 - or how different the semantical meaning
39:40 - of the sentences in the various trunks
39:42 - are so pretty cool and with that I think
39:46 - you are ready to take the next step in
39:47 - building this rag feature so in the next
39:50 - scrim I'll give you a challenge let's
39:52 - move
39:55 - on okay now it's your turn to create
39:58 - your very first embeddings and as you
40:00 - might have noticed already I have
40:01 - removed the code I wrote in the previous
40:04 - Grim because yeah this is scrimba you
40:06 - are going to write the code on your own
40:08 - that's how you really learn so the only
40:10 - thing I've done is called this split
40:13 - documents function and stored the
40:14 - results in a variable I'm calling
40:16 - handbook chunks because you're going to
40:18 - use that when you create and invoke this
40:20 - create embeddings function it takes the
40:23 - chunks that is these as a parameter and
40:26 - turns them all into embeddings using the
40:28 - mistal API once you've done that you are
40:30 - to return the data in a specific format
40:33 - so what we're doing here is prepping it
40:35 - before we'll upload it to the vector
40:37 - database and the service we are using
40:39 - for our Vector database is called
40:41 - superbase which you'll learn more about
40:43 - very soon now the structure superbase
40:46 - wants us to create is the following it
40:49 - should be an array of objects and each
40:51 - of the objects should contain two
40:53 - properties one called content that is
40:55 - just the raw text string that you find
40:57 - in each of the Trunks and secondly the
40:59 - embedding property should simply be the
41:01 - embedded version of that string so aka
41:04 - the vector once you have the data in
41:06 - this format just return it and then
41:08 - later on we'll take care of uploading it
41:10 - to superbase so go ahead and give this
41:12 - one Your Best Shot good
41:17 - luck okay hopefully this went well I'll
41:21 - start by defining the
41:22 - function like that this will be one with
41:25 - asynchronous operations so we need to
41:26 - define it as yes an async function and
41:29 - inside of it we'll start with the mistal
41:31 - client and the embeddings method it
41:34 - takes two arguments the model which
41:36 - should be mistal embed and the input
41:40 - which should be the chunks that we have
41:42 - passed into the function now previously
41:45 - I added a string here so I had to wrap
41:46 - it in square brackets like this because
41:48 - the input is expecting an array of
41:50 - strings here though the trunks is
41:52 - already in the shape of an array as it
41:54 - is this handbook trunks array right here
41:56 - so we don't need to do that but we do
41:59 - need to await this one and store the
42:01 - result in a
42:02 - variable like that let's now console log
42:05 - out embeddings and see if we get
42:08 - anything when we run the code let's call
42:11 - the function pass in the handbook Trunks
42:15 - and see what we get out here on line 24
42:17 - all right so in our console you can see
42:19 - we have an object which contains a data
42:21 - array which again contains their own
42:23 - objects with a property called embedding
42:25 - so the data we want exists in ins side
42:27 - of embeddings do data then we can
42:29 - navigate into a random item in this
42:31 - array for example the 12th one and then
42:34 - fetch out it's embedding like that if we
42:37 - run the code again we should see yes one
42:40 - vector being logged out to the console
42:42 - really good now we need to combine all
42:44 - of these vectors with all of our text
42:46 - chunks in this structure we've defined
42:49 - down here so to do that I'll map through
42:51 - each of the chunks and then return a new
42:54 - object which contains the chunk as the
42:56 - cont
42:57 - and the vector should be under the
42:59 - embedding key and we'll find it by
43:01 - navigating into embeddings Data into one
43:04 - of the items and then dot embedding so
43:07 - there's a lot of embedding words here
43:08 - right now just bear with me and we'll
43:10 - try to make this work so actually I'll
43:12 - I'm a little bit lazy I'll just copy
43:14 - this one right in here and then we need
43:15 - to replace this with whatever index we
43:17 - are at at every step in the iteration
43:20 - luckily map gives us the index as the
43:22 - second parameter of the Callback
43:24 - function so we can simply replace this
43:26 - with I like this let's store this in a
43:29 - variable called data and then finally
43:31 - return data like that I'll remove this
43:34 - one now we can call create embeddings
43:36 - and expect to get back the data and then
43:38 - log it out but if we want to do that we
43:39 - also have to await it because here we
43:41 - have a synchronous code so console log
43:45 - like that let's run this and see what we
43:50 - get yes there we have a beautiful array
43:53 - with objects that contain two keys
43:56 - content that contains the raw text
43:58 - string and embedding that contains the
44:00 - vector itself so we have the data just
44:03 - how we want it now if you solve this in
44:05 - a slightly different way that's totally
44:07 - okay there are certainly ways to
44:09 - condense this code and make it quote
44:11 - unquote drier I'm not going to worry
44:12 - about that right now but feel free to
44:14 - write this however you want the
44:16 - important thing is that you got the
44:17 - intended result not exactly that my code
44:20 - and your code are mirror images of each
44:22 - other so with that we are ready to take
44:24 - the next very exciting step in our rag
44:26 - Journey and that is to start learning
44:28 - about Vector databases for that I'll
44:30 - hand the bow over to my colleague Gil
44:32 - who will teach you about Vector
44:33 - databases over the next couple of
44:38 - scrims in this course we're going to use
44:41 - super base to manage our Vector database
44:44 - superbase is a full-fledged open source
44:46 - backend platform that offers a
44:48 - postgressql or postgress database which
44:51 - is a free and open- Source database
44:53 - system recognized for its stability and
44:56 - advanced capability
44:57 - while postgress is not a dedicated
44:59 - Vector database superbase does support a
45:02 - powerful postest extension called PG
45:05 - Vector for storing embeddings and
45:07 - Performing Vector similarity searches if
45:10 - you've worked with subase or postgress
45:12 - this should be pretty straightforward if
45:14 - not don't worry you don't have to be a
45:16 - database expert to start using superbase
45:18 - it's quick and easy to set up and the
45:20 - platform has a simple to use dashboard
45:22 - that makes postest as easy to use as a
45:24 - spreadsheet so the first thing you want
45:26 - to do is head over to superb.com once
45:29 - there click to sign in which you can do
45:32 - using your GitHub credentials next on
45:34 - your dashboard's project page click new
45:37 - project you'll first create a new
45:39 - organization with superbase you can use
45:41 - a company name or your own name choose
45:44 - the type of organization in my case
45:46 - personal set the pricing plan to free
45:49 - then click create organization after
45:52 - that superbase will ask you to create a
45:54 - new project which comes with its own
45:56 - dedicated instance and full postgress
45:58 - database it will also set up an API so
46:01 - you can easily interact with your new
46:03 - database so give your new project a name
46:06 - like vector embeddings create a password
46:09 - for your postgress database then choose
46:11 - a region that's geographically closer to
46:13 - you or your user base for best
46:15 - performance then click create new
46:17 - project and after a short moment your
46:19 - new project should be all set up from
46:22 - here you'll need to enable the PG Vector
46:24 - extension in your new project click the
46:26 - the database icon in the sidebar to go
46:29 - to the database page then on the pages
46:31 - sidebar click extensions in the search
46:34 - field search for vector and enable the
46:38 - extension and that should set you up to
46:40 - use superbase to store index and query
46:43 - Vector embeddings all right next you'll
46:45 - need to integrate superbase with your
46:47 - application or in our case the scrims
46:49 - for this course to do that click on the
46:51 - project setting icon and navigate to the
46:54 - API section in the sidebar here you'll
46:57 - find your project URL and API Keys these
47:00 - are essential for integrating superbase
47:02 - with your app so first copy your project
47:05 - URL then save it as an environment
47:07 - variable on scrimba remember you can
47:09 - access your environment variables with
47:11 - the keyword shortcut command or control
47:14 - shift e and be sure to name this
47:16 - variable super basore URL exactly as
47:19 - shown here finally copy your project API
47:22 - key then save it as a scrimba
47:24 - environment variable named superbase API
47:27 - key just like
47:31 - this Vector databases or vector stores
47:34 - possess unique superpowers for managing
47:36 - Vector embeddings with the capacity to
47:39 - store and retrieve embeddings quickly
47:41 - and at scale all right so how do Vector
47:43 - databases actually work well embeddings
47:46 - essentially allow us to match content to
47:48 - a question unlike traditional databases
47:50 - that search for exact value matches in
47:53 - rows Vector databases are powered by
47:55 - complex algorithm Ms that store search
47:58 - and quickly identify vectors so instead
48:01 - of looking for exact matches they use a
48:03 - similarity metric that uses all the
48:06 - information vectors provide about the
48:08 - meaning of the words and phrases to find
48:10 - the vectors most similar to a given
48:12 - query so storing custom information as
48:15 - edings in a vector database gives you
48:17 - the benefit of enabling users to
48:19 - interact with and receive responses
48:21 - exclusively from your own content you
48:23 - have complete control over your data
48:25 - ensuring it remains relevant and up
48:27 - toate this can also help reduce the
48:29 - number of calls and token usage and even
48:32 - allow the summarization and storage of
48:35 - chat histories which helps AIS maintain
48:37 - a type of long-term memory an important
48:40 - tool against the problem of
48:41 - hallucinations with AI models so with
48:44 - all that said Vector databases are
48:45 - becoming a central part of how we build
48:48 - AI powered software and play a massive
48:51 - role in the advancements of large
48:53 - language models these days you have
48:55 - various Vector database options from
48:57 - tools like chroma to Pine Cone superbase
48:59 - and several others all right so next up
49:02 - I'll guide you through setting up your
49:03 - own Vector database see you
49:07 - soon now we need to configure superbase
49:10 - in our project so that we can start
49:12 - interacting with the database as you can
49:14 - see I've installed superbase as a
49:16 - dependency and imported the create
49:18 - client from the superbase JavaScript SDK
49:21 - on line six we invoke this function
49:23 - passing in the superbase URL as the
49:25 - first parameter and the API key as the
49:27 - second and then we have our superbase
49:30 - client however now we have two clients
49:32 - here the mistal one and the superbase
49:34 - one so I want to make it a bit more
49:35 - apparent that this one here is dealing
49:37 - with mistol so I'll rename it like that
49:40 - and then change the name here as well
49:43 - now I want you to head over to your
49:44 - dashboard in superbase and click into
49:46 - the vector embeddings project from there
49:48 - choose the SQL editor from the menu on
49:50 - the left hand side as this allows you to
49:53 - create tables in the database using a
49:55 - SQL query and and having tables is
49:57 - absolutely necessary in a SQL database
49:59 - as that is how you store the data I
50:01 - happen to have the query right here for
50:04 - you as you can see it's pretty
50:05 - straightforward create table we're
50:07 - calling it handbook docs and then we
50:09 - Define the three columns we want our
50:11 - table to have an ID which has the data
50:13 - type big serial that is the primary key
50:15 - so the identification field in this
50:17 - table we'll have the content which will
50:19 - specify as plain text and finally
50:21 - there's the embedding which is a vector
50:22 - of 1,24 Dimensions if you think this
50:26 - resembles our data structure down here
50:27 - you are completely right that is exactly
50:30 - why we formatted our data this way so go
50:32 - ahead and take the SQL and paste it into
50:34 - the editor hit run and then you should
50:36 - see under the results here success no
50:38 - rows returned that means that your table
50:40 - has been created to view it simply click
50:42 - on the table editor in the menu on the
50:44 - left hand side there you can see this is
50:46 - the very beginnings of a table that has
50:49 - an ID column a Content column and an
50:51 - embedding column now to get our data all
50:54 - the way from the handbook via the embed
50:56 - end point and finally into the structure
50:58 - we want and then upload it to super base
51:00 - we only have one line of code to write
51:02 - and that is simply super
51:04 - base do from here we'll specify our
51:07 - table handbook docs dot insert cuz we
51:12 - want to insert something and what do we
51:14 - want to insert well that is the data
51:17 - this is also an async operation so we
51:18 - got to wait it and when this line has
51:21 - executed and JavaScript moves on we'll
51:23 - log out upload complete let's now run
51:26 - this and there we go the upload should
51:28 - be complete let's head over to super
51:29 - base and boom there we go we have our
51:32 - content and their corresponding
51:34 - embeddings in the vector database
51:36 - meaning that we are ready to take the
51:38 - final step in this rag feature which is
51:40 - to perform the retrieval so that we can
51:42 - generate replies to the users for any
51:44 - question they might have about our
51:46 - employee handbook so great job reaching
51:49 - this far let's carry
51:53 - on with all of our text Trunks and
51:56 - embeddings safely stored at superbase we
51:58 - are finally ready to write the code for
52:01 - our rag feature so as you can see here
52:03 - I've changed around on the index JS a
52:05 - little bit as I moved the old uploading
52:08 - code over to data.js as we won't be
52:11 - using that now since we're now actually
52:13 - going to do the retrieval and generation
52:15 - steps so let's start by going through
52:17 - this code so that we're both on the same
52:19 - page the flow of this app contains four
52:22 - steps the first one is getting the user
52:24 - input here I've just hardcoded it as a
52:27 - variable where the user is asking for
52:29 - whether or not they get an extra day off
52:31 - since December 25th falls on a Sunday
52:33 - now of course in real app the user would
52:35 - probably ask this in some kind of form
52:37 - and you do some Dom manipulation to
52:39 - fetch this though that's outside of the
52:41 - scope for this course so we'll just keep
52:42 - it simple and use this input variable
52:45 - next we need to take this input and turn
52:47 - it into an embedding as we need to see
52:49 - if the embedding of this string matches
52:53 - some of the embeddings we've created of
52:55 - the various chunks in our handbook now
52:58 - creating this embedding should be piece
52:59 - of cake for you for now so I didn't
53:01 - bother going through that code with you
53:03 - as you've done that before so once we
53:05 - have this embedding stored in this
53:07 - variable we'll pass it into another
53:08 - function that we've called retrieve
53:10 - matches and this is where we are going
53:12 - to do the similarity search now I've not
53:15 - written the body of this function yet
53:17 - let's just continue on with the flow and
53:18 - then get back to that because once we've
53:20 - gotten the matches or aka the context
53:24 - we'll pass both the context and the
53:26 - input into a function called generate
53:28 - chat response where we'll use these two
53:30 - in combination to get mistol to
53:32 - formulate a reply to the user so that is
53:35 - essentially the four steps of our rag
53:37 - feature now let's look at this retrieve
53:39 - matches function here we need to tell
53:41 - superbase to do a similarity search and
53:44 - if you read some of these descriptions
53:45 - you might be a little bit scared because
53:46 - they're called things like ukian
53:48 - distance negative inner product or coign
53:51 - distance that certainly sounds
53:54 - complicated though luckily we don't have
53:56 - to to worry about any of that as
53:57 - superbase provides us with a SQL
53:59 - function that we simply can copy paste
54:01 - so that we don't have to dive into the
54:03 - underlying complexity I've pasted this
54:05 - function into the function. SQL file
54:07 - right here changing around a little bit
54:09 - on a few things like the name of the
54:11 - function which I want to be match
54:12 - handbook docs as this is the name of our
54:15 - table and also I've changed the vector
54:17 - to account for the number of Dimensions
54:18 - mistal gives our embeddings plus updated
54:21 - this query down here to account for our
54:23 - handbook docs name so what I want you to
54:25 - do now is copy this entire function head
54:28 - over to superbase and click into the SQL
54:30 - editor there click on the new query
54:33 - button and then paste in the function
54:36 - click on run and if you see success no
54:38 - rows returned it means that this
54:40 - function is now available in your
54:42 - database but now the question is how do
54:44 - we access this function in our
54:46 - JavaScript and that is where superbase
54:48 - is really user friendly CU they have an
54:50 - RPC method a so-called remote procedure
54:53 - call which you can invoke anywhere in
54:55 - your code just like they do on this
54:57 - snippet right here so what we'll do is
54:59 - simply copy this and paste it into our
55:01 - code now our function was not called
55:03 - match documents it was called match
55:06 - handbook docks and to begin with I don't
55:08 - want 10 matches which is what you define
55:10 - here I want just one now the match
55:13 - threshold sets a threshold for how
55:15 - similar embedding should be in order to
55:17 - be included as a match the higher you
55:19 - put this the more picky you are so the
55:21 - less mattress you'll see but also the
55:23 - more similar they will actually be and
55:25 - here the aquarium embedding is the
55:27 - embedding that we passed into this
55:28 - function in other words the embedding of
55:30 - this string right here finally we can
55:33 - return data and I happen to know that
55:35 - inside of the first item in the data
55:37 - array there is a property called content
55:40 - and that is what we want so now let's
55:43 - comment out this line and console log
55:47 - out the context and try to run this code
55:51 - and there we go we get back a very
55:53 - relevant piece of context which says
55:55 - Christmas Day full-time employees
55:57 - parenthesis employees work at least 35
55:59 - hours per week receive one paid day off
56:02 - for each full day of holiday time
56:04 - holiday benefits for part-time employees
56:06 - and then it stops so we were able to
56:08 - retrieve very relevant information
56:10 - though it's not formulated as a good
56:12 - reply to the user and also there's some
56:14 - lacking information here as well ideally
56:17 - we would have seen the sentence that was
56:18 - cut off Midway as that would have given
56:20 - us information about the part-time
56:22 - employee vacation policy for these kinds
56:24 - of situations so that leaves us with a
56:27 - couple of tasks to be done down here in
56:29 - these retrieve matches and the generate
56:31 - chat response functions and who do you
56:33 - think is going to fix up that yes you
56:35 - guessed it that's yourself so in the
56:38 - next Grim you are going to complete the
56:39 - retrieval and the generation process of
56:42 - this feature let's move on I'll see you
56:47 - there welcome to this two-part challenge
56:50 - where you are going to complete the
56:52 - retrieve matches function and write the
56:54 - entire body of the generate chat chat
56:56 - response function in the first one we
56:58 - are to fix the fact that we didn't get
57:00 - enough data back by simply getting one
57:02 - match so instead we are going to return
57:04 - five matches and that involves updating
57:06 - this object and changing how you return
57:09 - the data for the Second Challenge you
57:10 - are to take whatever context you get
57:12 - back from the retrieval step and combine
57:14 - it with the user's query or input and
57:17 - turn that into a prompt this prompt
57:19 - should be sent to mistral's API and
57:21 - decide for yourself what models and what
57:23 - settings you'd like to use and here
57:25 - you're going to do a little bit of
57:26 - prompt engineering as you'll need to
57:28 - combine the context and the query into a
57:31 - single prompt and I don't want to
57:33 - dictate this for you instead just think
57:35 - of how you would take two pieces of data
57:37 - a context and a question and turn it
57:39 - into one prompt that instructs the AI to
57:42 - answer the question based upon the
57:44 - provided context now I can disclose that
57:46 - it doesn't have to be complex the AI is
57:48 - pretty capable of figuring out what
57:50 - you're trying to do so just make it your
57:52 - best chart and see how it works finally
57:54 - once you've done this you probably want
57:55 - to log out the response here to inspect
57:57 - what kind of reply the AI generated for
57:59 - you okay with that best of luck you got
58:07 - this okay hopefully this went well let's
58:10 - do it so I'll start with this one and
58:13 - the first thing we need to do is of
58:14 - course update this number to be five
58:16 - instead of one and then I'll check out
58:19 - the data here by logging it out and then
58:22 - I'll actually run the code as we're
58:24 - logging out the context here and then
58:26 - I'll actually remove this and just
58:28 - return the data so that we get to
58:30 - inspect the underlying structure here as
58:32 - we are logging out the context here on
58:34 - line 15 let's run the code opening up
58:37 - the console and there we go so this is
58:39 - an array of objects where we are looking
58:41 - for the string inside of the content
58:44 - keys so if we want to combine all of the
58:46 - content Keys into a single string I'll
58:49 - do data. map and for each chunk I'll
58:52 - return the chunk. content if we return
58:55 - this let's see what we get running the
58:58 - code and there we get an array of
59:00 - strings to combine that to a single
59:02 - string we'll just do dot join and then
59:05 - specify that we want a space in between
59:07 - each of the strings logging this out yes
59:11 - that looks good moving on to this one
59:13 - here we'll start by using the mystal
59:14 - client and call the chat method passing
59:17 - in the model and let's try with the most
59:20 - capable one first mistal large latest
59:25 - and the messages only needs one object
59:28 - or I'll at least try with that and if it
59:30 - doesn't work I'll perhaps try to add a
59:31 - system message but let's go straight to
59:33 - the user as a first solution and the
59:35 - content here is where we'll need to do a
59:37 - little bit of prompt engineering I'll
59:39 - try the easy way first and simply do
59:42 - handbook
59:43 - context like that and then passing in
59:45 - the context like that and then we'll do
59:49 - question colon and pass in the query now
59:53 - we could of course have written this is
59:55 - an extract from the the handbook that
59:56 - contains relevant background info for
59:58 - the user question though as you've
60:00 - probably understood I like to start off
60:02 - simple and then only make it more
60:03 - complex if needed so this is actually
60:06 - all the prompt engineering I was looking
60:07 - for so as the next step we want to
60:09 - return whatever result we get from this
60:11 - and to do that we of course have to
60:13 - await this function and store the
60:15 - response in a variable and then I happen
60:17 - to know that the real generated reply to
60:20 - the user lives inside response do
60:22 - choices and the first item in that array
60:25 - do message. content as I happen to know
60:28 - that this is the location of the
60:29 - generated response from the AI so with
60:32 - that it is the moment we've all been
60:33 - waiting for let's log out the response
60:37 - like that and run the code and see how
60:39 - this works and yes based on the handbook
60:43 - context provided if Christmas Day falls
60:45 - on a Sunday you as a full-time employee
60:47 - would still receive one pay day off for
60:49 - the holiday brilliant that is exactly
60:52 - what we were looking for it was able to
60:54 - both figure out that December 25th was
60:57 - semantically related to Christmas day so
60:59 - that it was able to retrieve the
61:01 - relevant information and use that to
61:03 - generate a nice and humanly readable
61:05 - reply so phenomenally well done you now
61:08 - have rag as a tool in your tool belt as
61:11 - an AI engineer and this will definitely
61:13 - come in handy throughout your career if
61:16 - you continue down the path of AI so give
61:18 - yourself a pad on the back perhaps take
61:20 - a break at this point as you've learned
61:22 - a lot and perhaps has a need to digest
61:24 - it if not in the next next part of this
61:26 - course you are going to learn about an
61:28 - insanely exciting concept which is
61:30 - function calling which enables you to
61:31 - create AI agents that interact with the
61:34 - world on the user's behalf so truly
61:37 - something that opens the door to a whole
61:39 - world of revolutionary user
61:44 - experiences hey and welcome to the
61:46 - section about function calling this is a
61:48 - very exciting field that opens the door
61:50 - for you to AI agents and by that I mean
61:53 - smart assistants that can interact act
61:56 - with the world on behalf of your users
61:58 - just by interpreting what they say so
62:01 - this is a new paradigm in terms of the
62:03 - user experience we developers can
62:05 - provide now let's start off at a very
62:08 - high level looking at how the
62:10 - architecture of such a agent typically
62:12 - is so let's say that you are running an
62:14 - e-commerce website where you sell
62:16 - products to people and you have a chat
62:18 - where users come to ask questions for
62:20 - example things like is my package on its
62:22 - way what we'll do then is send this to
62:24 - our llm along with some Specific
62:26 - Instructions as to what kind of tools it
62:28 - has available to figure out the answer
62:30 - for the user and then if it is a good
62:32 - model it'll look at this query and
62:34 - realize for itself that hm I actually
62:36 - need to call the fetch order function to
62:38 - give the user a good reply and then
62:40 - it'll instruct our software to actually
62:43 - perform this function call this is often
62:45 - done via regular code like if and else
62:47 - conditionals so through the code you
62:49 - have written you'll ensure that when the
62:51 - AI wants to call this function you will
62:53 - actually perform this function call and
62:54 - get the order data she then again will
62:56 - return to the llm it will then read that
62:59 - data which probably comes in the form of
63:01 - an object and then turn that into a
63:03 - human readable response like yes your
63:05 - order is expected to arrive and blah
63:06 - blah blah which again then results in a
63:09 - happy user all without you having to use
63:11 - manpower to do this so you can imagine
63:14 - the power of this technology is it can
63:16 - drastically improve the customer service
63:18 - users can get when talking with these
63:20 - chat Bots we've all come across over the
63:22 - last few years and of course that is
63:24 - just the start imagine how powerful it
63:26 - is when this is rolled out to all
63:28 - Industries okay let's now have a look at
63:31 - the code this here is by and large a
63:33 - very standard function call to the chat
63:35 - endpoint at mistl so all of this should
63:37 - be familiar to you except for this line
63:39 - 13 here where we've added an array of
63:42 - tools and that comes from the import
63:44 - statement here on line two and if we
63:46 - head over to
63:52 - tools.jar and searches through that data
63:55 - and as you can see down in the tools
63:56 - array we have described this function
63:59 - through a specific schema this entire
64:01 - object's sole purpose is to describe for
64:04 - the AI what this function does so it
64:07 - says that yes first of all it is a
64:09 - function and its name is this and here
64:11 - is a description of it as well plus it
64:13 - takes one parameter of type object this
64:16 - one right here and this object has a
64:19 - property called transaction ID which is
64:21 - of type string and also it is the
64:23 - required property so I think I think you
64:25 - can guess what I'm getting to here this
64:27 - is all our aices it never sees the
64:30 - content of this function it just looks
64:32 - at the description and tries to decide
64:34 - whether or not it should be invoked and
64:36 - with what kind of arguments based upon
64:37 - the input from the user so if we yet
64:40 - again head back to index.js and try to
64:42 - run this code with the prompt is the
64:45 - transaction t01 paid and run this then
64:48 - you can see in the console we get back
64:50 - an object and I happen to have copy
64:52 - pasted one of these objects into the
64:53 - output JS just to make it a little bit
64:55 - easier to read and here as you probably
64:57 - remember from earlier the choices
65:00 - message content property is actually
65:02 - empty now this is where mistol always
65:04 - adds the reply from the llm though now
65:08 - the llm has nothing to say instead it
65:10 - has given us some instructions about
65:12 - what we should do here under tool calls
65:15 - there's a function key which has the
65:16 - name of get payment function and the
65:19 - argument of transaction ID t01 so this
65:22 - is how it tells us that hey developer
65:24 - now you got to invoke a function and the
65:26 - llm isn't done reasoning about this
65:28 - issue if it was it would have said stop
65:31 - here but instead it wants us to call a
65:32 - tool and then send back the result so as
65:35 - you can understand we developers have
65:37 - some work to do here and we're also
65:39 - going to make this agent even more
65:40 - capable by adding another function so
65:42 - I'll leave it at this so that the two of
65:44 - us can get back to work and start coding
65:46 - and we'll do that in the next
65:51 - scrim so we've been able to get the
65:53 - mistal model to tell us to call function
65:56 - when someone asks about the payment
65:58 - status of an order however if we take a
66:01 - look at the data you can see that
66:03 - there's also other things going on here
66:05 - there's both an amount and a date for
66:07 - each of the orders so there's definitely
66:09 - potential to make our agent more
66:11 - powerful I'm now going to paste a
66:13 - function in here called get payment date
66:16 - it also takes in the transaction ID and
66:18 - does more or less the same thing as get
66:20 - payment status though it instead Returns
66:23 - the date for when it was paid now you
66:25 - could have of course also have done this
66:26 - by simply making this one a bit more
66:28 - robust to fetch various pieces from the
66:30 - data based upon the argument passed in
66:32 - but the point here is not to build a
66:34 - production ready system but rather to
66:36 - give you some practice in building
66:37 - agents so what I want you to do now is
66:40 - expand upon this tools array by adding a
66:42 - new object that describes the second
66:44 - function because as you might remember
66:46 - the AI doesn't read this code it only
66:48 - knows about the function through how you
66:50 - describe it in the tools schema so that
66:53 - is your first Challenge and once you've
66:54 - done that I want you to verify that your
66:56 - solution works by changing the prompt
66:59 - that we give the agent here in a way
67:00 - that would get the llm to instruct us to
67:02 - call the newly added function so go
67:05 - ahead and solve these two challenges and
67:07 - I'll show you the solution when you
67:08 - return back to
67:14 - me okay let's do this I'll head back to
67:17 - the tools file again and then I will
67:19 - simply copy this object as I'm a little
67:21 - bit lazy and I've understood that the
67:23 - second object here will look very
67:25 - similar similar as the first one so I'll
67:27 - do like that and simply change from get
67:30 - payment status to get payment date and
67:34 - the description to get the payment date
67:38 - of a transaction and then the parameter
67:41 - can stay just the same as it is
67:43 - identical to the previous function and
67:45 - actually that was about it heading back
67:47 - to index JS I'll change this
67:50 - to when was the transaction t01 paid
67:55 - let's run this open up the console and
67:58 - yes indeed you can see that it is now
68:00 - instructing us to call get payment date
68:02 - and not get payment status so mission
68:04 - accomplished hopefully that forced you
68:06 - to get to know the schema a little bit
68:08 - more as we're going to move to the next
68:10 - step now where we'll start acting upon
68:12 - the instructions we get from the mistal
68:17 - model right now we are in this step of
68:19 - our flow the llm has just told us that
68:22 - we need to call a function and now we're
68:24 - about to write the code we need in order
68:25 - to do that so the first thing I want to
68:27 - do is that we keep our messages array up
68:30 - to date as it should include every
68:32 - single piece of dialogue going back and
68:33 - forth between the user the app and
68:35 - mistol so what I'll do is
68:38 - messages. push and I'll push some part
68:41 - of this response though not the entire
68:44 - thing let's have a look at how it was
68:46 - structured if we head into the choices
68:48 - and the first object in this array we
68:50 - can see that there is a message object
68:53 - and that is what we want so we'll do
68:55 - response on. choices the first one and
68:58 - then message like that as you can see it
69:01 - has a role just like our user message
69:03 - has a role though this is from the
69:05 - assistant the content is empty since it
69:07 - didn't have anything to say to us
69:09 - instead it had some instructions about
69:11 - what tools we should call so the next
69:13 - thing we want to do is write the N
69:15 - statement that checks if we indeed are
69:17 - about to call a tool and then write the
69:19 - code for the specific tool call now I
69:21 - don't want to use the fact that there's
69:22 - a tool calls here in our conditional
69:24 - instead the right way to do this is to
69:26 - look at the Finish reason and the fact
69:27 - that this has the string value of tool
69:29 - calls so we'll do
69:32 - if
69:34 - response choices the first item and then
69:38 - then finish reason if this is equal to
69:41 - Tool calls well then our next step is to
69:43 - fetch out the name of the function we
69:45 - are to call and its argument and at this
69:47 - point I think I've written more than
69:49 - enough code for you it is your turn to
69:51 - take over so here is a challenge for you
69:53 - I want you to get a hold of the name of
69:56 - the function that we should call and its
69:58 - arguments and we want the function name
70:01 - as a string but the arguments as an
70:03 - object so I've set up the two variables
70:06 - for you the function name and the
70:07 - function arcs both are just initialized
70:09 - as empty strings but whatever expression
70:11 - you replace this with should be of the
70:14 - data type object so now you'll have to
70:16 - dig through this one yourself and fetch
70:18 - out the relevant information and once
70:20 - you're done just return back to me and
70:22 - then I will show you the solution the
70:24 - final thing I'm going to do here is make
70:26 - sure that I close this if statement
70:28 - properly like that and indent this and
70:31 - have the correct indentation for this
70:32 - one as well and with that you are good
70:34 - to go best of
70:41 - luck okay hopefully this went well let's
70:44 - do this together so first I'll start
70:47 - with the function name and here I'll
70:50 - need to navigate all the way down to
70:53 - Tool calls function and then name and I
70:56 - happen to see that both the name and the
70:58 - argument is in the same object here this
71:02 - function object So to avoid too much
71:04 - repetition I'm going to do const
71:07 - function object like that and then I'll
71:10 - paste this in adding
71:12 - dot tool calls which is an array and we
71:15 - want the first item and do function like
71:17 - that now I can do function object. name
71:22 - and function object do r ents so if you
71:26 - got to this point good job though we're
71:28 - not quite done yet as this function
71:31 - object. argument is a string and we want
71:34 - it as an
71:35 - object and the way to do that is to do
71:38 - json.parse and that should turn whatever
71:41 - string we have into an object let's
71:43 - consol out the function name and the
71:47 - arguments and then run the code to
71:49 - verify that it works and I'll comment
71:51 - out the response down here running the
71:53 - code and yes the first first one is get
71:55 - payment date as a string and the second
71:58 - one is indeed an object so very well
72:00 - done solving this challenge let's carry
72:06 - on it is time for us to do this step
72:09 - which is to call the function so that we
72:11 - get the data we eventually can send back
72:12 - to mistal and as you know we have the
72:15 - function name and the function arcs but
72:17 - this file doesn't yet have access to the
72:19 - functions as they live in the tools file
72:22 - so I will import these functions get
72:25 - payment date and get payment status like
72:27 - that though now the question is how do
72:29 - we go from just having the data as a
72:31 - string value for example get payment
72:34 - date into actually calling the function
72:37 - well to help you with that I'm going to
72:38 - wrap these functions in an object called
72:42 - available functions I'll add get payment
72:44 - date and get payment status what this
72:46 - gives us the opportunity to do is to use
72:48 - the bracket notation to get a hold of a
72:51 - reference to any of these functions
72:53 - because if we passed in a string called
72:55 - get payment date this would be a
72:58 - reference to this function and if we
73:00 - throw in parenthesis we'll invoke the
73:02 - function so that is pretty cool and it
73:03 - is exactly what I want you to do right
73:06 - now in your challenge your job is simply
73:08 - to perform the function call so go ahead
73:10 - and write the code to do that and then
73:11 - I'll see you when you return back to
73:17 - me okay hopefully this went well the way
73:20 - to do it is to grab a hold of the
73:21 - available functions object and then use
73:23 - bracket notation to pass in our function
73:26 - name call it with parenthesis and
73:29 - finally add function arcs like that
73:31 - we'll store this in a variable called
73:33 - function response and then finally log
73:36 - it out let's run the code and there we
73:39 - go we have turned a prompt from the user
73:42 - into a real function call that returns
73:44 - data that our assistant asked for really
73:47 - good job reaching this far we are making
73:49 - a ton of progress so let's just keep up
73:51 - the pace and carry on
73:57 - so we've called the function obtained
73:58 - the data we need and now we need to send
74:00 - it back to our assistant so how do we do
74:02 - that well as I've said earlier it's
74:04 - important that we keep track of all the
74:07 - dialogue in this app whether it's from
74:09 - the user from the assistant or in this
74:11 - case from the tool itself and where do
74:14 - we keep track of this dialogue take a
74:17 - guess yes it is in this messages array
74:20 - which we are passing along to mistal
74:22 - every time we interact with our API so
74:24 - what we're going to do here first is
74:26 - messages. push pass in an object and as
74:30 - you've seen before we always have a role
74:32 - though this time around it's not the
74:34 - role of user and also not the role of
74:37 - assistant which is what we had when we
74:39 - got the instructions to call the
74:41 - function this time around the role
74:43 - belongs to the tool and the next piece
74:45 - of information we need to pass along is
74:47 - the name of the tool which we have here
74:49 - in function name and finally the content
74:53 - after we've told mol that we worked with
74:55 - the tool and gave them the name of the
74:57 - tool what do you think the content here
74:59 - should be take a
75:01 - guess yes hopefully you understood that
75:04 - it is the response we got back from the
75:05 - function because when mistol gets all of
75:08 - this data it should be able to decide
75:10 - what the next step should be and
75:11 - speaking of which how do we then send
75:13 - this off to mistl well we could start a
75:16 - new client. chat down here and add all
75:18 - of the metadata again though that's a
75:20 - very hard-coded and hacky solution
75:23 - instead we want to rerun this piece of
75:25 - code and then yet again check if we're
75:28 - instructed to call yet another function
75:30 - and then keep on going until the
75:31 - assistant tells us that yes we are now
75:34 - done with the back and forth and I have
75:35 - a good response for the user and hearing
75:38 - that what kind of programming Paradigm
75:40 - does that sound like a job for and yes
75:42 - you guessed it the loop so we are going
75:45 - to wrap this entire thing in a loop and
75:47 - keep it running until we have a
75:49 - satisfying result we'll do that in the
75:51 - next Grim so I'll see you there
75:56 - okay we are ready to perform the final
75:58 - steps of our flow we'll take the result
76:01 - from the function and send it to our
76:03 - assistant who will then construct a
76:04 - reply and send it back so that our user
76:07 - is happy again and as I talked about in
76:09 - the previous Grim we'll do this through
76:11 - the help of a loop so this is a
76:13 - challenge where you are to start by
76:15 - creating a for Loop that runs a maximum
76:17 - of five times and inside of the for Loop
76:20 - if the Finish reason is ever set to the
76:22 - string stop then I want you to simp
76:24 - simply return the response from the
76:26 - assistant so then you are to return the
76:28 - entire function and that'll also then
76:30 - break out of the loop as you can see the
76:33 - Finish reason lives down here in the
76:35 - object you get from the assistant now
76:38 - you might ask at this point well why are
76:40 - we simply hardcoding in a for Loop that
76:42 - runs five times wouldn't it be better
76:44 - with a while loop that could run as many
76:46 - times as you need until the task is
76:48 - complete or for example a recursive
76:50 - solution that would do the same thing
76:52 - and yes those could be better Solutions
76:54 - but they also open up for the
76:56 - possibility of infinite Loops so it
76:58 - would require some guard rails in my
77:00 - opinion to implement such a solution
77:02 - which is why we're simply going for a
77:04 - naive for Loop that runs five times and
77:06 - that should be more than enough for our
77:08 - use case though of course if you want to
77:10 - build on this after you've solved the
77:12 - challenge you are more than free to do
77:14 - that and actually I would encourage you
77:16 - to do that anyway give this challenge
77:18 - your best shot and then I'll see you
77:19 - when you're done
77:26 - okay let's do this we'll do four let I
77:30 - equals z and I should be less than five
77:35 - and it should increment moving this all
77:38 - the way down here and indenting
77:42 - everything inside of the loop like that
77:45 - and checking here if response do choices
77:51 - the first item and yes it lives within
77:55 - that object if the Finish reason is stop
78:00 - then we'll simply return the content
78:04 - within the message and finally let's
78:07 - bring this up here and do else if like
78:10 - that okay the moment of truth let's see
78:13 - if we've been able to successfully
78:14 - implement this entire flow I'll comment
78:17 - out this console log we're asking when
78:20 - this transaction was paid let's run the
78:22 - code and yes the transaction 2001 was
78:26 - paid on October 5th 2021 wow congrats
78:30 - you've just built your very first AI
78:33 - agent and while this of course is a
78:35 - dummy example You Now understand the
78:37 - basic building blocks which gives you a
78:39 - foundation for building Real World
78:41 - products so give yourself a pat on the
78:43 - back and then I'll see you in the next
78:47 - scrim hi there now you are going to
78:50 - learn how to run mistal models locally
78:52 - on your computer and the tool we are
78:54 - going to use for that is called olama it
78:57 - is an app that wraps large language
78:59 - models on your computer and lets you
79:01 - interact with them in various ways so
79:03 - click on this image right here and
79:05 - you'll get to the AMA page there you can
79:07 - search for models for example mistl
79:09 - click into it and see the size of it
79:11 - this one is 4.1 GB and also read about
79:14 - how it performs compared to other open-
79:16 - source models for now let's head back to
79:18 - the homepage and click on the download
79:20 - button then choose whatever platform you
79:22 - use and click yet again on the download
79:25 - button so that you can complete the
79:26 - download and install AMA on your
79:29 - computer once you've done that open up
79:31 - the terminal on your computer and type
79:33 - AMA run mistal that'll start the
79:36 - download process and as it'll take some
79:38 - time I'll fast forward and once it's
79:40 - done we get this little UI where we can
79:42 - type a message so let's ask the model
79:45 - something for example trying to use it
79:46 - as a motivational coach which I often
79:48 - use large language models for so I'll
79:50 - type feeling a bit demotivated today can
79:52 - you help me get started with my day and
79:54 - when I hit enter mistl starts typing out
79:57 - tip after tip not through being run on a
80:00 - thirdparty server but being run by my
80:03 - computer and let's just take a minute
80:05 - and acknowledge how cool this is because
80:07 - aside for the cost of the hardware and
80:09 - the electricity these tokens are
80:11 - completely free and there's also 100%
80:14 - privacy as the data stays on the device
80:17 - now what's perhaps even cooler is that
80:20 - you can use this as a model for any AI
80:22 - projects you build locally as well so
80:24 - let's try to do that in the current
80:26 - scrim click on the Cog wheel in the
80:28 - bottom right corner and then click
80:30 - download as zip then in your downloads
80:33 - folder you'll see this ZIP file so just
80:35 - go ahead and double click on it to unzip
80:37 - it and that'll give you a folder with a
80:39 - weird looking name that is the
80:40 - underlying ID for the scrim so take this
80:43 - folder rename it and place it wherever
80:45 - you keep your Dev project for me that is
80:47 - in the dev directory and I've named this
80:49 - project AMA hello world so I'll navigate
80:52 - into it and there you should do mpm
80:54 - install and then do npm start that'll
80:57 - spin up the project on Port 3000 meaning
81:00 - that you can head over to Local Host
81:02 - 3000 and there you will see the browser
81:04 - telling you to ask a question via the
81:07 - question parameter and what's going on
81:09 - here is that this little Express router
81:12 - checks if there is a question in the URL
81:14 - parameter called question and if there
81:16 - isn't a question there for example if
81:17 - you just visited the root page without
81:19 - any URL params it'll just render out
81:22 - this string though if it is a question
81:24 - there it'll execute the following lines
81:25 - of code and here we're using theama SDK
81:29 - and the patterns that are being used
81:30 - here is probably quite familiar to you
81:32 - right now because yes this resembles the
81:34 - mistal SDK quite a lot we call the chat
81:37 - method and pass in an object where we
81:40 - specify the model in this case it's
81:42 - mistal and a messages array that has a
81:44 - role and some content and the content
81:46 - here is whatever Express finds in the
81:48 - URL parameter called question so if we
81:51 - now type in a question in the browser
81:53 - for example why do stars shine and hit
81:55 - enter then we'll see that the browser
81:57 - will work for a little while and boom
81:59 - eventually it gives you reply which says
82:01 - that star shine due to nuclear fusion
82:03 - where hydrogen atoms are turned into
82:05 - helium and thus releasing immense
82:08 - amounts of energy and actually this
82:09 - continues until all of these atoms have
82:11 - turned into iron which by the way means
82:15 - that the iron you used to fry your eggs
82:17 - with was created billions of years ago
82:20 - in the center of a star wow that is
82:24 - mindblowing blowing to think about
82:25 - almost as mind-blowing as the fact that
82:27 - you have reached the end of this mystal
82:30 - course most people who start a course
82:32 - here on scrimba give up before they
82:34 - reach the end but you my friend do not
82:37 - you are not a quitter so give yourself a
82:39 - pat on the back and in the next scrim
82:41 - we'll do a quick recap of everything
82:43 - you've
82:46 - learned wow you really did something
82:49 - special today you completed this course
82:51 - please remember that most people who
82:53 - start online courses
82:55 - give up you are not like them so let's
82:57 - have a quick recap of what you've
82:59 - learned starting out we looked at the
83:01 - basics of mistl and their platform along
83:03 - with the chat completion API that we
83:05 - interacted with through their JavaScript
83:07 - SDK you have a solid grasp of the
83:09 - various mistal models right now and also
83:11 - know how to work with embeddings and
83:13 - Vector databases with superbase you've
83:15 - also dipped your toes into Lang chain
83:17 - and using it for chunking text because
83:19 - you had to do that when you were
83:21 - learning about rag or retrieval
83:22 - augmented generation finally you know
83:25 - how to create AI agents with function
83:27 - calling and how to do local inference
83:30 - through the help of AMA now it's time
83:32 - for you to celebrate your win do share
83:34 - that you've completed this course on
83:36 - social media or if you want a less
83:38 - public way of doing that you can check
83:39 - out scribus Discord Community as there
83:41 - we have a today I did Channel where we
83:44 - love seeing people completing courses
83:46 - and whatever you do please keep on
83:48 - building you have a unique set of skills
83:50 - here and this is just a start the world
83:52 - of AI is exploding giving Developers
83:54 - like yourself the possibility to create
83:56 - entirely new experiences and apps the
83:59 - world is your oyster so happy building
84:02 - and best of luck

Cleaned transcript:

learn how to use mistal AI to build intelligent apps all the way from simple chat completions to Advanced use cases like Rag and function calling per borgan from scrimba created this course in collaboration with mistal AI you'll get handson experience with mistral's open Source models including mistl 7B and mistl 8ex 7B and their commercial models by the end of this course you'll Master essential AI engineering paradigms enabling you to create sophisticated conversational user experiences and run AI models locally on your own computer hi there and welcome to this introduction to mistol AI my goal with this course is to teach you how to build magical stuff and more specifically how to do that using JavaScript and mistal AI if you don't know what mistal is it is a company that builds socalled foundational models that in 2023 twice managed to stun the AI Community by launching small opensource foundational models that were on par with the best close Source models out there so as an AI engineer mistel is definitely something that deserves your attention in this course we are going to start off by looking a little bit closer at mistel in general and their platform before we dive into the API Basics and how to use their JavaScript SDK as this course is based around JavaScript though their python SDK is similar so even if you prefer python over JavaScript you'll still get a ton of value value from this course we are also going to go through all of the models that mistl offers at the time of recording this course including their embedding model which lets you work with Vector databases which you'll also get an introduction to in order to give your AI apps domain knowledge which for example could be proprietary company data realtime information that the model hasn't been trained on or for example extra indepth knowledge about a specific subject that is too narrow for the AI to have been trained on and we'll do this through a technique called retrieve augmented generation AKA rag you'll also learn how to build AI agents with function calling enabling your apps to take action based upon the user prompt a truly revolutionary Paradigm and finally you'll learn how to run your models locally on your computer and interact with them both via the terminal and a web page now who am I I've been a developer instructor and startup founder for almost 10 years now and I'm also the CEO of the learning platform you're on now which is scrimba I use create tutorials on JavaScript react and AI engineering and in total they have been watched by literally millions of people through the scrimba platform corsera and YouTube I love to connect with my students so please click on either of these links if you're interested in connecting on either X or LinkedIn now you'll also see lessons from two other teachers as well throughout this course namely from Gil Hernandez one of our brilliant instructors here at scrimba and we're also proud to have Sophia Yang the head of developer relations at mistol contributing to this course so as you probably understand now this course is a collaboration between mistol and scrimba so we're not pulling this curriculum out of thin air it has been created in partnership with the company itself if you ever find yourself lacking some JavaScript skills or AI engineering Concepts please check out our frontend developer career path or this AI engineering course as those will help you get up to speed so with that let's get started hello it's this is Sophia yam from Mr AI I like to welcome you to the course and give you a brief introduction of mrol Mr AI was founded last year by our three cofounders Arthur Tim and gam we first released our open W model Mr 7B in September last year we released a x7b mixture of experts model and that platform in December we currently have offices in Paris London and San Francisco Bay Area we offer six models for all use cases and business needs including two open source models mro 7B and mixol 8 x7b they're under open source AP par 2.0 license they great to started experimenting with we also offer four optimized Enterprise grate models Mr small for low latency use cases Mr medium or language based tasks and Mr Large for your most sophisticated needs we also offer an embedding model which offers the State ofth art embeddings for text to get started you can use our chat assistant L to interact with our model right away just go to chat. m.ai and you can play with Lua there are several ways to use our models we offer API end points for all of our models through the platform you can subscribe and get an API key on the platform this is the easiest to use and deploy you can also use our model on cloud services which provide fastest deployment for Enterprise especially for those who already use cloud services you can also self deploy our models on your own on Prem infrastructure this will give you more control and flexibility but it's the most complex among the three so it's a tradeoff between ease of deployment and level control so you can choose whichever you want for your own use cases and your business needs this course will focus on the platform and how to use Mr API for various tasks hope you enjoy the course okay in order to interact with the mistal API you need an API key which will'll get through their platform or La platform as they call it so click on this image right here and you'll be taken to the mistal homepage and there you can click on the build now option that'll take you to the authentication screen so choose however authentication method you want and then in the next step you're asked to create a worksspace name and check off whether you're a solo Creator or doing this as a team member in a company whatever you choose click create workspace and there we go this is the platform and in order to get access to the API you have to provide a card or subscribe as they say here however you only pay for what you use so this is not an ongoing fixed subscription so just add your card and once you done that this box will go away and you can click on API keys to create Keys you can authenticate with click on the create new key and give it a name and an expiration date and then create key now you'll only see this key once so be sure to save it as a scrimba environment variable you learn how to do that by clicking on this link right here and please don't take the time to try and copy this API key right here by the time you watch this scrim this key is no longer active as I've deleted it so go ahead and follow these steps and set the N variables in scrimba and then in the next scrim my colleague Gil will teach you the basics of how to interact with the mistal API through JavaScript hey in this tutorial we'll go over using the chat completion API which allows you to chat with a model that's finetuned to follow instructions so let's Dive Right In we're going to use mistral's JavaScript client which I've installed and set up in this interactive scrim I'm importing mistal AI at the top of the Javascript file and I've instantiated a mistal client using my API key which I've stored as an environment variable on scrimba so we're ready to go the chat completion endpoint is designed to handle back and forth conversations you feed it a prompt and a series of messages and it generates a completion or an appropriate continuation of that conversation so now let's make our first chat request using ml's chat method I'll declare a constant named chat response to store the response returned from the chat request which will await with await client. chat and pass the method an object containing the request body the chat completion API accepts various parameters the two required parameters are model and messages mistol has various pretrained models you can use with the API for our purposes we'll use a model called mistal tiny then I'll set the messages parameter to an array and this is a key part of the chat request as it holds the prompts to generate completion for this should be an array of message objects each with role and content properties role defines the role of the message I'll set it to user indicating that the message is from the user's perspective then set content to the actual content of the user message this is usually a question like what is the best French cheese all right and this is all we need to generate a chat completion so let's log the response to the console and the way to access the message content directly is like this I'll run this code by clicking the Run button and good the API returns a humanlike response about the different types of French cheese all right so what I want you to do now is personalize the AI response by updating the content property to something that interests you you might not have realized this yet but this isn't your typical video player you are experiencing a fully interactive scrim that you can pause at any moment and jump right into the code and make changes to it so go ahead and ask the AI a question then click run okay hopefully that was fun and you got some great responses now let's experiment with other parameters to make our response more interesting we'll use the temperature parameter to set the creativity or randomness of the generated text and this should be a value between 0 and 1 Now the default temperature is 0.7 but as you get closer to one the output will be more random and creative while lower values make the response more focused and deterministic I'll set it right down the middle at 0.5 to strike a balance between creative and predictable responses and now I'll feed it a different question like I want a puppy what is the most kidfriendly dog I'll run this code and I get back a detailed conversational response about various dog breeds good all right I want you to go ahead and pause me now and try experimenting with different temperature values you can also provide custom system prompts to guide the behavior of the model this time I'll set roll to system then set content to the instructions or prompt for the model this is your chance to influence how the AI response so I'm instructing it that it's a friendly cheese kind of sore and that when asked about cheese to reply concisely and humorously now running this won't work because now we need to follow the system role with a user role in content I'll set the role property in this second message object to user then set this content property to ask what is the best French cheese I'll run this code and I get back a fun and witty response about French cheese fortunately it's always cheese season right all right so that's it for the basics of working with the chat completion API now that you've gotten to know the basics of how to set up a request to mistol let's have a look at some of the options and configurations you as a developer can adjust so that you tweak the response you get from mol to your needs and perhaps the most apparent one is adding support for streaming because that is often a key feature of AI apps for example here on hugging face the platform for open Source AI models and data sets on the mistal organization there's a hosted version of one of their models along with a chat interface so that you can talk with it so here I'll ask it the question what's your favorite Taco ingredient and when I send that I immediately see the response getting built up token by token until it's done and this is a really Pleasant user experience so let's see how we can tweak this from just giving us the entire response to giving us one token at a time so the first thing we need to do is change this from chat to chat stream like that what then happens is that this chat response changes from being a regular object to being a socalled async iterable meaning that we have to await as every item in this iterable becomes available to us so chat response will kind of gradually be built out as we get token by token from the mystal API and the way to deal with this is to create an asynchronous for of loop so we'll do for A8 and then const chunk of chat response and every time the body of this for Loop is executed we get access to a new chunk and as for the chat response this is an object with many properties so we'll have to navigate all almost in the same way as we navigated into the chat response do choices though instead of message it's called Delta so if we now try to console log out this and comment this one out let's see what happens and yes we are getting a ton of stuff logged to the console super fast so this kind of buildup of the response would happen almost instantly and probably a lot faster than we could read it though it's a lot better user experience than having to wait until the entire thing is generated and and then get the response in one go okay let's have a look at another cool configuration you can make to the request and that is to tell mistl that you want the reply in the format of Json that is Javascript object notation here is an example of a Json string and if you don't know what is it is essentially a very common schema that developers use when sending and processing information so being able to get this kind of format from the AI is super helpful as you integrate it with your app and doing this only requires two small settings the first one being that you need to set the response format as an object of type Json object like that and then you also need to specify it in the prompt so here I'll write reply with Json like that here the data will be processed by code and not by a human first and foremost so let's skip this streaming here because it is mostly for the ux directed at humans and then go back to chat here and finally uncomment this one and then like that so let's run the code and yes there we get a Json object I'll copy it from the console paste it in here and there we can see it is an object with a key answer that talks a little bit about good cheese and then it also has a cheese key with a subsequent name key cheese key which is an object that has three keys name country and type so you can imagine it being a lot easier to extract the metadata from this reply as opposed to Simply getting a couple of sentences so I would recommend you to play around with this check out the documentation and see what other configurations and modifications you can make to this response and then once you're ready I'll see you in the next RM where we'll dive more into what we've configured on this specific line which is the models themselves that mistl provides as it's important to have a good overview in order to choose the right ones for the job so I'll see you there hey in this Grim we're going to look at the various models mistal offers now be aware though that these are the models it offers at the time of recording this scrim you should definitely click on this image right here so that you're taken to the landing page for their models as there you can click around and check out their latest optimized commercial models as well as their open models now speaking of open models mistol Rose to prominence in the AI community in 2023 when they launched their first model mistol 7B that is a model that has socalled open weights meaning that you can download it to your computer or upload it to a server and use it as a part of your application without paying mistel a dime one of the things that stunned the AI Community was how powerful it was despite only having 7 billion parameters as the leading open models back then had many more parameters than this even an order of magnitude more now a little later mistol launched the socalled mixol adex 7B which also is an open model and has a unique architecture that allows it to be much more powerful though only slightly more expensive to run inference on the core idea behind this one is that it uses a mix of eight different socalled experts so the total number of parameters here is actually 8 * 7 which is 46 though when you run inference it only Taps into one of these experts and it actually uses around 13 billion parameters when being run now at this point you might be a little bit confused and want to know more about this I don't want to go more into the technical details here because I don't think it's that important in order to use these Technologies though if you are interested feel free to click on this image right here and you'll be taken to a article which talks more in depth about the Mixel model moving on to the next models those are the mistal small mistal medium and mistal large and these are not socalled open weights meaning that you can simply download them from their website and get started locally you either have to use this VI cloud provider that supports these models or you can do self hosting as well though to to do that you have to talk with the mistal team now if we compare these models side by side with their performance on the MML U test as the height of each bar here you can see that the commercial models are more powerful than the open models though the small commercial model and the mix dra are quite within the same range now if you don't know what MML U is it is a common way to test llms it's short for massive multitask language understanding and it puts llms to the test through a range of different tasks giving them a score from 0 to 100% based upon how well they perform now looking at this image it seems that we always should go for the mistal large model but that's actually not the case because the flip side of using a better model is very often that it is more expensive so if we plot this models out on a twodimensional space with the cost per million tokens on the xaxis and the ml U score on the Y AIS you can see that the picture is definitely different because mistal is by far the most expensive model over twice as expensive as the mistal medium so here if you are able to get the job done with medium you should definitely choose that one analogy you can think of here is when hiring people at a company in many cases you probably don't want to hire a person that is overeducated or over qualified for the job because most likely their hourly rate will be higher so how do you then decide which model to use if you want to dive more into this subject just click on this image here and you'll be taken to the guide in the docs which specifically talks about model selection there you can see some use case examples on what kinds of typical tasks a model is suitable for so for example the mistal small works well for things like classification and customer support whereas the mystal medium is the ideal model for intermediate tasks that require moderate reasoning that could be things like data extraction summarizing a document writing a job description and so forth and finally if you want to do more complex tasks Mr Large is your goto model so later in this course we are going to create a little agent that can call functions on behalf of users in addition to doing socalled retrieval augmented generation AKA Rag and in those cases we are going to use the large model as those require significant reasoning capabilities and on that note what is exactly rag well you'll figure out in the next scrim here at scrim but we use an app called notion for notes taking and with a team of several teachers developers people in operations and so forth we have a lot of Internal Documentation and it quickly becomes chaotic so here we have a courses and teaching page which again contains a bunch of sub pages and they themselves also have sub Pages as well so it is actually quite hard at times to get to the answer you want to get to which is why I was really glad when lotion launched their ask AI feature which is essentially means that you can ask questions to notion so one day when I was working on our corsera exports I seemed to remember that we needed a widget for doing these exports and I asked it about exactly that it thought a little bit and then came with an answer yes you are correct for corera courses a type of item called plugin is used to embed scrims and this is quite interesting because I asked for a widget but the AI understood that well actually I meant the plugins so it's shared with me through this footnote here the link to the document that talked about these corsera plugins and this kind of user experience is a GameChanger for web apps suddenly it is much easier to find the information you need and also you give the llm access to proprietary data as obviously the underlying model here does not have any knowledge about how we at scrimba internally embed our scrims in corsera courses now this whole experience was only possible through something called retrieval augmented generation which Probably sounds very complex but don't worry we'll go through it step by step and we won't refer to it through this long complex name here we'll use the popularized expression rag okay so rag contains of mainly two steps there's the retrieval step fetching the data you need to reply to the user's question and there's the generation taking whatever information you found and using that as context when generating the conversational reply back to the user so if if we zoom in on the retrieval first this is very often done in collaboration with a socalled Vector database that is a specific type of database that is optimized for storing information in a specific format that makes it easy for AI to reason about it so it stores socalled embeddings now at this point you're probably a little bit confused what's this thing about vectors and embeddings and all of that don't worry about it we'll get back to that later for now I just want to explain rag on a very high level so what you do is you take all of your data and shove it into a vector database in this specific embedded format and then you take the search query or the input from the user and turn that into an embedding as well as that gives you the opportunity to do a socalled semantic search and get these search results which intelligently for example understand that no pair wasn't looking for a widget he was actually looking for this and thus fetch the relevant data for the app that is the retrieval part once you've done that you take the user input that is the question I asked which was a very humanly written sentence about I seem to remember something about a corsera wouldit blah blah blah and then you combine that with the search results we got in the retrieval step and turn it into a singular prompt that the llm can use as input so mistal AI takes that prompt and the relevant context we retrieved and turns that into a very humanly readable response with in many cases a footnote or link to the underlying data as well thus providing the user a way of factchecking the claim that the AI comes with now there's one thing that all of this relies on which is our ability to turn data for example a sentence into numbers that the AI can understand now all of this relies in our ability to create something called embeddings and what is an embedding well it is what you get when you take a piece of data for example the string hello world and run it through an AI model that turns it into a long array of numbers also known as a vector and as we build out a rag solution in this course it is really important that you have an intuitive understanding of what this embedding concept is so before we continue on with our rag project I'll leave the mic to my colleague Gil Hernandez who will give you a primer on embeddings in the next scrim whether you realize it or not AI powered search shapes many parts of your daily lives every day you interact with platforms sifting through massive amounts of data from text and images to audio and video think about Amazon recommending products or search engines refining your queries social media platforms curate tailored content while services like YouTube Netflix and Spotify offer suggestions based on your preferences now Advanced AIS despite their capabilities don't truly understand the real world as we do they can't grasp the actual meaning or Nuance of a video title song or news article so how exactly do AIS and platforms like Spotify Netflix and YouTube truly get us how is it that they appear to understand predict and respond to us as effectively as if not better than people well the magic behind this capability involves a blend of algorithms AI models and huge amounts of data but a larger part of the answer involves embeddings you see when you present a question to an AI it first needs to translate it into a format it can understand so you can think of embeddings as the language that AI understands the term embedding is a mathematical concept that refers to placing one object into a different space think of it like taking a word or sentence which is in a Content space and transforming it into a different representation like a set of numbers in a vector space all while preserving its original meaning and the relationships between other words and phrases AI systems process lots of data from user inputs to information and databases at the heart of this processing are embeddings which are vectors representing that data transforming content like search queries photos songs or videos into vectors gives machines the power to effectively compare categorize and understand the content in a way that's almost human so how is all of this possible well it isn't exactly as easy as just turning data into vectors so before we go any deeper let's take a closer look at what vectors are think of a vector as a coordinate or point in space and to keep things simple we'll have a look at this 2D graph with an X and Y AIS let's say that a word like cat is translated into a vector like 4.5 12.2 which is this point this Vector encapsulates the meaning and nuances of the word cat in a way an AI model can understand and then we have the word feline represented by a nearby Vector of 4.7 12.6 so we'll place that point on the graph now words that have similar meanings are numerically similar and tend to be be closely positioned in the vector space so this closeness implies that cat and Feline have similar meanings now let's say we have the word or vectors for kitten which might also be close to cat and Feline but maybe slightly further apart due to its age related Nuance now a dog is different but still in the same general domain of domesticated animals so the word dog might be represented by a vector that's not too distant but clearly in a different region let's say 7.5 10.5 and even a phrase like Man's Best Friend which is a colloquial term for a dog could be represented by a vector that's close to the vector for dog on the other hand a word like building is not related in meaning to any of these so its Vector would be much further apart let's say 15.3 3.9 here's another example that demonstrates how embeddings might capture semantic meaning and relationships between words let's say we have the word King represented by the vector 25 then man man is the vector 13 and woman is represented by the vector 14 now let's do some quick Vector arithmetic we'll start with the vector for King then subtract the vector for man to remove the male context and add the vector for woman to introduce new context after performing this Vector math our resulting Vector is 26 so we'll plot that point on the graph and let's say there's another word in our space queen represented by the vector 2 6.2 right here well this Vector is extremely close to the resulting Vector so we might identify queen as the most similar word based on that Vector just as a trained AI model would now a twodimensional graph is a massive simplification as real world embeddings often exist in much higher dimensional spaces sometimes spanning hundreds or even thousands of dimensions for example the actual Vector embedding for the word Queen might have values across multiple Dimensions each Dimension or number in this Vector might capture a different semantic or contextual aspect of the word Queen for instance royalty Cleopatra or even chess this is what allows the AIS to recognize and differentiate between these contexts when the word is used in different scenarios now imagine embedding hundreds of thousands of words and phrases into this highdimensional space some words will naturally gravitate closer to one another due to their similarities forming clusters While others are further apart or sparsely distributed in the space these relationships between vectors are extremely useful think back to spotify's method of embedding tracks in a vector space tracks that are positioned closely together are likely to be played one after the other all right so what else can we do with embeddings and how are they used in the real world well you can imagine how embeddings have revolutionized our daily experiences for example search engines have evolved to understand the essence of your queries and content moving beyond mere keyword matching and recommendation systems with the aid of embedding suggest products movies or songs that truly resonate with our preferences and purchase history for example Netflix uses them to create a tailored and personalized platform to maximize engagement and retention also in the healthcare industry embeddings are used to analyze medical images and extract information doctors can use to diagnose diseases and in the finance World embeddings help with analyzing financial data and making predictions about stock prices or currency exchange rates so every time you interact with an AI chatbot every time an app recommends something behind the scenes embeddings are at work translating data into meaning all right so how are these embeddings actually created well let's dive into that next before we create our embeddings there's one important thing you need to learn and that is how to split text because as an AI engineer you'll find yourself having to split text again and again because let's say that you are working on an internal employee handbook app which lets employees ask questions about the compan policies well in which casee you probably have a large data source like the one you can see here in handbook. text which contains all of the data that you need to embed however creating one embed of this entire thing would just be meaningless there's far too many subjects and themes talked about in this handbook so it wouldn't really have any specific semantic meaning of value it would be far too broad so what we're going to do is take this document and split it into chunks and then we'll create an embedding of of every single chunk now creating such chunks is actually a little bit complex though luckily we have a tool to help us with that and that is Lang chain one of the leading libraries for AI Engineers so what we'll do is enhance this function so that it uses the Lang chain text splitter because as you can see this doesn't do much at the moment it's simply an async function that fetches the handbook and calls do text on the response thus giving us all of the text in this handbook let's run the code and just see that it works yes there we have it so now we can use Lang chain to split this into smaller chunks I'll import the Lang chain Library here as a dependency and then let's figure out which specific tool we need to import from Lang chain the simplest one is the character text splitter though the recommended one to use is the recursive character text splitter so that's the one we're going to use so here we'll do import recursive character text Splitter from Lang chain SL text splitter like that now we can create a new recursive character text splitter this is a Constructor function that takes an object as the argument and here you define two things the size of the chunk and how much overlap you want between the chunks we'll try for example 250 characters for the size of the chunk that feels like a sentence or two and will allow for some overlap for example 40 characters we'll call our splitter simply splitter like that and then we can do splitter. create document and pass in the text this is an async function so we have to await it and store the result in a variable called for example output like that now if we log out the output let's run the code and there I got an error and that is because I have a typo I called the text splitter which is wrong it should be text splitter like that let's run the code again yes there we go as you can see in the console there are a bunch of data there and if we open the dev tools we'll be able to inspect it a little bit more in detail so let's do that here as you can see it is an array which contains 2 180 objects let's open up one of these objects and there we can see that we have the text itself under the page content property and also under the lines property we get the exact lines this content comes from in the handbook. text file that is very handy in case you want to create footnotes or reference to the original Source in your app now what you want to make sure of when you respect your data like this is that each of these trunks ideally only deal with one subject or theme that is how you create good embeddings if a given trunk is quote unquote polluted by different themes it'll be harder for the embedding model to create a meaningful Vector of it so here you can see that this trunk deals with delegation of authority and responsibility and the administration and the executive director so definitely a coherent subject though it's actually been split in the middle of two sentences so it could probably be better as well we have probably not struck the perfect balance here you could argue that it would have been better to split this into two and then use the entire sentences or maybe expand it in both ends and include both of the complete sentences in general the shorter your chunks are the more precise meaning the embedding will get though you might also miss some wider context and the longer the trunks are the more context they contain but it can also produce too broad of a scope of information and this would reduce the quality of the similarity search that the vector database does as the underlying meaning of the embedding could be ambiguous it could point in two different directions so to speak in general you want your chunks to be as small as possible but you don't want to lose context so it's definitely a balance to strike and something you'll probably only find through experimentation creating smaller and bigger chunks and actually seeing how it plays out in action in your app for now we'll stick with this and see how it works as we continue on building this rag feature let's carry on in the previous scrim I wrote all of the code for you but as you know this is a scrimo course meaning that your job is to get your hands on the keyboard and write the code out yourself so I left out a couple of pieces for you which you now are to implement through this challenge I want you to refactor this function so that it first of all takes the path to the data or document as an argument so that is to the handbook. text here that'll make it a little bit more generalized as it's really not a good practice to have the path for the fetal request hardcoded in here on line seven and then secondly I want you to return the splitted data as an array of strings and just that because that's how we want our data in the next step of building out this feature so go ahead and solve this challenge right now okay hopefully that went well first we'll specify that it takes a path here as the argument which we'll use in the fetch request and then of course we'll need to specify in the function invocation that we indeed want to get the data from the handbook. text that was part one part two returning the data as an array of strings if you remember from the previous Grim when we inspected this data it is actually an array of objects right now but this time around we only want the data that is within the page content property because we do not care about the location metadata at this point so here we'll take the output and we'll map through it and for each of these trunk objects we'll return trunk. page content like that and here we can store that in a variable called text R for text array and then simply return it now you can of course condense these into fewer lines of code but I like to be explicit and only do one thing at a time on each line so with that we are done and ready to carry on now it is finally time to use the myal API to create our very first embedding as you can see I have imported the mystal client and added my API key so we are ready to get going the first thing I need is an example text trunk to create an embedding of I happen to have copied one of them into my clipboard so I'll paste it in here and call it example trunk as you can see it says professional ethics and behavior are expected of all ambri employees further ambri expects each employee to display good judgment so this is a quite good text for embedding because it deals with one subject which is the expectation of characters for ambri employees now I'll comment this one out as we won't call this function right now instead we'll down here at line 22 call the client do embeddings function that is an async function so we have to await it and inside of the parameter the object we'll specify first what kind of model we want to use and here mistol provides an embedding model called mistol embed and then the second key in the sub is the input now we can't just paste the example trunk like this as this input isn't expecting a string it's actually expecting an array of strings so we have to do like this we'll store the response we get back from this in a const called for example embeddings response like that and then let's finally log it out I'll run the code and yet again I had a typo Mistral with r is what we want to write not Mall we'll try again and there we go we got something very interesting back let's paste it into the editor to inspect it a bit more like that here we can see it has an ID and under the data property we have an array that holds an embedding and that embedding is a long array of floating Point numbers all of which seemingly are very close to zero though slightly more or slightly less so this Vector right here is an embedding of this specific text as transformed by this model and as we use this model to transform other pieces of text the mathematical distance from the various vectors will be a reflect of how similar or how different the semantical meaning of the sentences in the various trunks are so pretty cool and with that I think you are ready to take the next step in building this rag feature so in the next scrim I'll give you a challenge let's move on okay now it's your turn to create your very first embeddings and as you might have noticed already I have removed the code I wrote in the previous Grim because yeah this is scrimba you are going to write the code on your own that's how you really learn so the only thing I've done is called this split documents function and stored the results in a variable I'm calling handbook chunks because you're going to use that when you create and invoke this create embeddings function it takes the chunks that is these as a parameter and turns them all into embeddings using the mistal API once you've done that you are to return the data in a specific format so what we're doing here is prepping it before we'll upload it to the vector database and the service we are using for our Vector database is called superbase which you'll learn more about very soon now the structure superbase wants us to create is the following it should be an array of objects and each of the objects should contain two properties one called content that is just the raw text string that you find in each of the Trunks and secondly the embedding property should simply be the embedded version of that string so aka the vector once you have the data in this format just return it and then later on we'll take care of uploading it to superbase so go ahead and give this one Your Best Shot good luck okay hopefully this went well I'll start by defining the function like that this will be one with asynchronous operations so we need to define it as yes an async function and inside of it we'll start with the mistal client and the embeddings method it takes two arguments the model which should be mistal embed and the input which should be the chunks that we have passed into the function now previously I added a string here so I had to wrap it in square brackets like this because the input is expecting an array of strings here though the trunks is already in the shape of an array as it is this handbook trunks array right here so we don't need to do that but we do need to await this one and store the result in a variable like that let's now console log out embeddings and see if we get anything when we run the code let's call the function pass in the handbook Trunks and see what we get out here on line 24 all right so in our console you can see we have an object which contains a data array which again contains their own objects with a property called embedding so the data we want exists in ins side of embeddings do data then we can navigate into a random item in this array for example the 12th one and then fetch out it's embedding like that if we run the code again we should see yes one vector being logged out to the console really good now we need to combine all of these vectors with all of our text chunks in this structure we've defined down here so to do that I'll map through each of the chunks and then return a new object which contains the chunk as the cont and the vector should be under the embedding key and we'll find it by navigating into embeddings Data into one of the items and then dot embedding so there's a lot of embedding words here right now just bear with me and we'll try to make this work so actually I'll I'm a little bit lazy I'll just copy this one right in here and then we need to replace this with whatever index we are at at every step in the iteration luckily map gives us the index as the second parameter of the Callback function so we can simply replace this with I like this let's store this in a variable called data and then finally return data like that I'll remove this one now we can call create embeddings and expect to get back the data and then log it out but if we want to do that we also have to await it because here we have a synchronous code so console log like that let's run this and see what we get yes there we have a beautiful array with objects that contain two keys content that contains the raw text string and embedding that contains the vector itself so we have the data just how we want it now if you solve this in a slightly different way that's totally okay there are certainly ways to condense this code and make it quote unquote drier I'm not going to worry about that right now but feel free to write this however you want the important thing is that you got the intended result not exactly that my code and your code are mirror images of each other so with that we are ready to take the next very exciting step in our rag Journey and that is to start learning about Vector databases for that I'll hand the bow over to my colleague Gil who will teach you about Vector databases over the next couple of scrims in this course we're going to use super base to manage our Vector database superbase is a fullfledged open source backend platform that offers a postgressql or postgress database which is a free and open Source database system recognized for its stability and advanced capability while postgress is not a dedicated Vector database superbase does support a powerful postest extension called PG Vector for storing embeddings and Performing Vector similarity searches if you've worked with subase or postgress this should be pretty straightforward if not don't worry you don't have to be a database expert to start using superbase it's quick and easy to set up and the platform has a simple to use dashboard that makes postest as easy to use as a spreadsheet so the first thing you want to do is head over to superb.com once there click to sign in which you can do using your GitHub credentials next on your dashboard's project page click new project you'll first create a new organization with superbase you can use a company name or your own name choose the type of organization in my case personal set the pricing plan to free then click create organization after that superbase will ask you to create a new project which comes with its own dedicated instance and full postgress database it will also set up an API so you can easily interact with your new database so give your new project a name like vector embeddings create a password for your postgress database then choose a region that's geographically closer to you or your user base for best performance then click create new project and after a short moment your new project should be all set up from here you'll need to enable the PG Vector extension in your new project click the the database icon in the sidebar to go to the database page then on the pages sidebar click extensions in the search field search for vector and enable the extension and that should set you up to use superbase to store index and query Vector embeddings all right next you'll need to integrate superbase with your application or in our case the scrims for this course to do that click on the project setting icon and navigate to the API section in the sidebar here you'll find your project URL and API Keys these are essential for integrating superbase with your app so first copy your project URL then save it as an environment variable on scrimba remember you can access your environment variables with the keyword shortcut command or control shift e and be sure to name this variable super basore URL exactly as shown here finally copy your project API key then save it as a scrimba environment variable named superbase API key just like this Vector databases or vector stores possess unique superpowers for managing Vector embeddings with the capacity to store and retrieve embeddings quickly and at scale all right so how do Vector databases actually work well embeddings essentially allow us to match content to a question unlike traditional databases that search for exact value matches in rows Vector databases are powered by complex algorithm Ms that store search and quickly identify vectors so instead of looking for exact matches they use a similarity metric that uses all the information vectors provide about the meaning of the words and phrases to find the vectors most similar to a given query so storing custom information as edings in a vector database gives you the benefit of enabling users to interact with and receive responses exclusively from your own content you have complete control over your data ensuring it remains relevant and up toate this can also help reduce the number of calls and token usage and even allow the summarization and storage of chat histories which helps AIS maintain a type of longterm memory an important tool against the problem of hallucinations with AI models so with all that said Vector databases are becoming a central part of how we build AI powered software and play a massive role in the advancements of large language models these days you have various Vector database options from tools like chroma to Pine Cone superbase and several others all right so next up I'll guide you through setting up your own Vector database see you soon now we need to configure superbase in our project so that we can start interacting with the database as you can see I've installed superbase as a dependency and imported the create client from the superbase JavaScript SDK on line six we invoke this function passing in the superbase URL as the first parameter and the API key as the second and then we have our superbase client however now we have two clients here the mistal one and the superbase one so I want to make it a bit more apparent that this one here is dealing with mistol so I'll rename it like that and then change the name here as well now I want you to head over to your dashboard in superbase and click into the vector embeddings project from there choose the SQL editor from the menu on the left hand side as this allows you to create tables in the database using a SQL query and and having tables is absolutely necessary in a SQL database as that is how you store the data I happen to have the query right here for you as you can see it's pretty straightforward create table we're calling it handbook docs and then we Define the three columns we want our table to have an ID which has the data type big serial that is the primary key so the identification field in this table we'll have the content which will specify as plain text and finally there's the embedding which is a vector of 1,24 Dimensions if you think this resembles our data structure down here you are completely right that is exactly why we formatted our data this way so go ahead and take the SQL and paste it into the editor hit run and then you should see under the results here success no rows returned that means that your table has been created to view it simply click on the table editor in the menu on the left hand side there you can see this is the very beginnings of a table that has an ID column a Content column and an embedding column now to get our data all the way from the handbook via the embed end point and finally into the structure we want and then upload it to super base we only have one line of code to write and that is simply super base do from here we'll specify our table handbook docs dot insert cuz we want to insert something and what do we want to insert well that is the data this is also an async operation so we got to wait it and when this line has executed and JavaScript moves on we'll log out upload complete let's now run this and there we go the upload should be complete let's head over to super base and boom there we go we have our content and their corresponding embeddings in the vector database meaning that we are ready to take the final step in this rag feature which is to perform the retrieval so that we can generate replies to the users for any question they might have about our employee handbook so great job reaching this far let's carry on with all of our text Trunks and embeddings safely stored at superbase we are finally ready to write the code for our rag feature so as you can see here I've changed around on the index JS a little bit as I moved the old uploading code over to data.js as we won't be using that now since we're now actually going to do the retrieval and generation steps so let's start by going through this code so that we're both on the same page the flow of this app contains four steps the first one is getting the user input here I've just hardcoded it as a variable where the user is asking for whether or not they get an extra day off since December 25th falls on a Sunday now of course in real app the user would probably ask this in some kind of form and you do some Dom manipulation to fetch this though that's outside of the scope for this course so we'll just keep it simple and use this input variable next we need to take this input and turn it into an embedding as we need to see if the embedding of this string matches some of the embeddings we've created of the various chunks in our handbook now creating this embedding should be piece of cake for you for now so I didn't bother going through that code with you as you've done that before so once we have this embedding stored in this variable we'll pass it into another function that we've called retrieve matches and this is where we are going to do the similarity search now I've not written the body of this function yet let's just continue on with the flow and then get back to that because once we've gotten the matches or aka the context we'll pass both the context and the input into a function called generate chat response where we'll use these two in combination to get mistol to formulate a reply to the user so that is essentially the four steps of our rag feature now let's look at this retrieve matches function here we need to tell superbase to do a similarity search and if you read some of these descriptions you might be a little bit scared because they're called things like ukian distance negative inner product or coign distance that certainly sounds complicated though luckily we don't have to to worry about any of that as superbase provides us with a SQL function that we simply can copy paste so that we don't have to dive into the underlying complexity I've pasted this function into the function. SQL file right here changing around a little bit on a few things like the name of the function which I want to be match handbook docs as this is the name of our table and also I've changed the vector to account for the number of Dimensions mistal gives our embeddings plus updated this query down here to account for our handbook docs name so what I want you to do now is copy this entire function head over to superbase and click into the SQL editor there click on the new query button and then paste in the function click on run and if you see success no rows returned it means that this function is now available in your database but now the question is how do we access this function in our JavaScript and that is where superbase is really user friendly CU they have an RPC method a socalled remote procedure call which you can invoke anywhere in your code just like they do on this snippet right here so what we'll do is simply copy this and paste it into our code now our function was not called match documents it was called match handbook docks and to begin with I don't want 10 matches which is what you define here I want just one now the match threshold sets a threshold for how similar embedding should be in order to be included as a match the higher you put this the more picky you are so the less mattress you'll see but also the more similar they will actually be and here the aquarium embedding is the embedding that we passed into this function in other words the embedding of this string right here finally we can return data and I happen to know that inside of the first item in the data array there is a property called content and that is what we want so now let's comment out this line and console log out the context and try to run this code and there we go we get back a very relevant piece of context which says Christmas Day fulltime employees parenthesis employees work at least 35 hours per week receive one paid day off for each full day of holiday time holiday benefits for parttime employees and then it stops so we were able to retrieve very relevant information though it's not formulated as a good reply to the user and also there's some lacking information here as well ideally we would have seen the sentence that was cut off Midway as that would have given us information about the parttime employee vacation policy for these kinds of situations so that leaves us with a couple of tasks to be done down here in these retrieve matches and the generate chat response functions and who do you think is going to fix up that yes you guessed it that's yourself so in the next Grim you are going to complete the retrieval and the generation process of this feature let's move on I'll see you there welcome to this twopart challenge where you are going to complete the retrieve matches function and write the entire body of the generate chat chat response function in the first one we are to fix the fact that we didn't get enough data back by simply getting one match so instead we are going to return five matches and that involves updating this object and changing how you return the data for the Second Challenge you are to take whatever context you get back from the retrieval step and combine it with the user's query or input and turn that into a prompt this prompt should be sent to mistral's API and decide for yourself what models and what settings you'd like to use and here you're going to do a little bit of prompt engineering as you'll need to combine the context and the query into a single prompt and I don't want to dictate this for you instead just think of how you would take two pieces of data a context and a question and turn it into one prompt that instructs the AI to answer the question based upon the provided context now I can disclose that it doesn't have to be complex the AI is pretty capable of figuring out what you're trying to do so just make it your best chart and see how it works finally once you've done this you probably want to log out the response here to inspect what kind of reply the AI generated for you okay with that best of luck you got this okay hopefully this went well let's do it so I'll start with this one and the first thing we need to do is of course update this number to be five instead of one and then I'll check out the data here by logging it out and then I'll actually run the code as we're logging out the context here and then I'll actually remove this and just return the data so that we get to inspect the underlying structure here as we are logging out the context here on line 15 let's run the code opening up the console and there we go so this is an array of objects where we are looking for the string inside of the content keys so if we want to combine all of the content Keys into a single string I'll do data. map and for each chunk I'll return the chunk. content if we return this let's see what we get running the code and there we get an array of strings to combine that to a single string we'll just do dot join and then specify that we want a space in between each of the strings logging this out yes that looks good moving on to this one here we'll start by using the mystal client and call the chat method passing in the model and let's try with the most capable one first mistal large latest and the messages only needs one object or I'll at least try with that and if it doesn't work I'll perhaps try to add a system message but let's go straight to the user as a first solution and the content here is where we'll need to do a little bit of prompt engineering I'll try the easy way first and simply do handbook context like that and then passing in the context like that and then we'll do question colon and pass in the query now we could of course have written this is an extract from the the handbook that contains relevant background info for the user question though as you've probably understood I like to start off simple and then only make it more complex if needed so this is actually all the prompt engineering I was looking for so as the next step we want to return whatever result we get from this and to do that we of course have to await this function and store the response in a variable and then I happen to know that the real generated reply to the user lives inside response do choices and the first item in that array do message. content as I happen to know that this is the location of the generated response from the AI so with that it is the moment we've all been waiting for let's log out the response like that and run the code and see how this works and yes based on the handbook context provided if Christmas Day falls on a Sunday you as a fulltime employee would still receive one pay day off for the holiday brilliant that is exactly what we were looking for it was able to both figure out that December 25th was semantically related to Christmas day so that it was able to retrieve the relevant information and use that to generate a nice and humanly readable reply so phenomenally well done you now have rag as a tool in your tool belt as an AI engineer and this will definitely come in handy throughout your career if you continue down the path of AI so give yourself a pad on the back perhaps take a break at this point as you've learned a lot and perhaps has a need to digest it if not in the next next part of this course you are going to learn about an insanely exciting concept which is function calling which enables you to create AI agents that interact with the world on the user's behalf so truly something that opens the door to a whole world of revolutionary user experiences hey and welcome to the section about function calling this is a very exciting field that opens the door for you to AI agents and by that I mean smart assistants that can interact act with the world on behalf of your users just by interpreting what they say so this is a new paradigm in terms of the user experience we developers can provide now let's start off at a very high level looking at how the architecture of such a agent typically is so let's say that you are running an ecommerce website where you sell products to people and you have a chat where users come to ask questions for example things like is my package on its way what we'll do then is send this to our llm along with some Specific Instructions as to what kind of tools it has available to figure out the answer for the user and then if it is a good model it'll look at this query and realize for itself that hm I actually need to call the fetch order function to give the user a good reply and then it'll instruct our software to actually perform this function call this is often done via regular code like if and else conditionals so through the code you have written you'll ensure that when the AI wants to call this function you will actually perform this function call and get the order data she then again will return to the llm it will then read that data which probably comes in the form of an object and then turn that into a human readable response like yes your order is expected to arrive and blah blah blah which again then results in a happy user all without you having to use manpower to do this so you can imagine the power of this technology is it can drastically improve the customer service users can get when talking with these chat Bots we've all come across over the last few years and of course that is just the start imagine how powerful it is when this is rolled out to all Industries okay let's now have a look at the code this here is by and large a very standard function call to the chat endpoint at mistl so all of this should be familiar to you except for this line 13 here where we've added an array of tools and that comes from the import statement here on line two and if we head over to tools.jar and searches through that data and as you can see down in the tools array we have described this function through a specific schema this entire object's sole purpose is to describe for the AI what this function does so it says that yes first of all it is a function and its name is this and here is a description of it as well plus it takes one parameter of type object this one right here and this object has a property called transaction ID which is of type string and also it is the required property so I think I think you can guess what I'm getting to here this is all our aices it never sees the content of this function it just looks at the description and tries to decide whether or not it should be invoked and with what kind of arguments based upon the input from the user so if we yet again head back to index.js and try to run this code with the prompt is the transaction t01 paid and run this then you can see in the console we get back an object and I happen to have copy pasted one of these objects into the output JS just to make it a little bit easier to read and here as you probably remember from earlier the choices message content property is actually empty now this is where mistol always adds the reply from the llm though now the llm has nothing to say instead it has given us some instructions about what we should do here under tool calls there's a function key which has the name of get payment function and the argument of transaction ID t01 so this is how it tells us that hey developer now you got to invoke a function and the llm isn't done reasoning about this issue if it was it would have said stop here but instead it wants us to call a tool and then send back the result so as you can understand we developers have some work to do here and we're also going to make this agent even more capable by adding another function so I'll leave it at this so that the two of us can get back to work and start coding and we'll do that in the next scrim so we've been able to get the mistal model to tell us to call function when someone asks about the payment status of an order however if we take a look at the data you can see that there's also other things going on here there's both an amount and a date for each of the orders so there's definitely potential to make our agent more powerful I'm now going to paste a function in here called get payment date it also takes in the transaction ID and does more or less the same thing as get payment status though it instead Returns the date for when it was paid now you could have of course also have done this by simply making this one a bit more robust to fetch various pieces from the data based upon the argument passed in but the point here is not to build a production ready system but rather to give you some practice in building agents so what I want you to do now is expand upon this tools array by adding a new object that describes the second function because as you might remember the AI doesn't read this code it only knows about the function through how you describe it in the tools schema so that is your first Challenge and once you've done that I want you to verify that your solution works by changing the prompt that we give the agent here in a way that would get the llm to instruct us to call the newly added function so go ahead and solve these two challenges and I'll show you the solution when you return back to me okay let's do this I'll head back to the tools file again and then I will simply copy this object as I'm a little bit lazy and I've understood that the second object here will look very similar similar as the first one so I'll do like that and simply change from get payment status to get payment date and the description to get the payment date of a transaction and then the parameter can stay just the same as it is identical to the previous function and actually that was about it heading back to index JS I'll change this to when was the transaction t01 paid let's run this open up the console and yes indeed you can see that it is now instructing us to call get payment date and not get payment status so mission accomplished hopefully that forced you to get to know the schema a little bit more as we're going to move to the next step now where we'll start acting upon the instructions we get from the mistal model right now we are in this step of our flow the llm has just told us that we need to call a function and now we're about to write the code we need in order to do that so the first thing I want to do is that we keep our messages array up to date as it should include every single piece of dialogue going back and forth between the user the app and mistol so what I'll do is messages. push and I'll push some part of this response though not the entire thing let's have a look at how it was structured if we head into the choices and the first object in this array we can see that there is a message object and that is what we want so we'll do response on. choices the first one and then message like that as you can see it has a role just like our user message has a role though this is from the assistant the content is empty since it didn't have anything to say to us instead it had some instructions about what tools we should call so the next thing we want to do is write the N statement that checks if we indeed are about to call a tool and then write the code for the specific tool call now I don't want to use the fact that there's a tool calls here in our conditional instead the right way to do this is to look at the Finish reason and the fact that this has the string value of tool calls so we'll do if response choices the first item and then then finish reason if this is equal to Tool calls well then our next step is to fetch out the name of the function we are to call and its argument and at this point I think I've written more than enough code for you it is your turn to take over so here is a challenge for you I want you to get a hold of the name of the function that we should call and its arguments and we want the function name as a string but the arguments as an object so I've set up the two variables for you the function name and the function arcs both are just initialized as empty strings but whatever expression you replace this with should be of the data type object so now you'll have to dig through this one yourself and fetch out the relevant information and once you're done just return back to me and then I will show you the solution the final thing I'm going to do here is make sure that I close this if statement properly like that and indent this and have the correct indentation for this one as well and with that you are good to go best of luck okay hopefully this went well let's do this together so first I'll start with the function name and here I'll need to navigate all the way down to Tool calls function and then name and I happen to see that both the name and the argument is in the same object here this function object So to avoid too much repetition I'm going to do const function object like that and then I'll paste this in adding dot tool calls which is an array and we want the first item and do function like that now I can do function object. name and function object do r ents so if you got to this point good job though we're not quite done yet as this function object. argument is a string and we want it as an object and the way to do that is to do json.parse and that should turn whatever string we have into an object let's consol out the function name and the arguments and then run the code to verify that it works and I'll comment out the response down here running the code and yes the first first one is get payment date as a string and the second one is indeed an object so very well done solving this challenge let's carry on it is time for us to do this step which is to call the function so that we get the data we eventually can send back to mistal and as you know we have the function name and the function arcs but this file doesn't yet have access to the functions as they live in the tools file so I will import these functions get payment date and get payment status like that though now the question is how do we go from just having the data as a string value for example get payment date into actually calling the function well to help you with that I'm going to wrap these functions in an object called available functions I'll add get payment date and get payment status what this gives us the opportunity to do is to use the bracket notation to get a hold of a reference to any of these functions because if we passed in a string called get payment date this would be a reference to this function and if we throw in parenthesis we'll invoke the function so that is pretty cool and it is exactly what I want you to do right now in your challenge your job is simply to perform the function call so go ahead and write the code to do that and then I'll see you when you return back to me okay hopefully this went well the way to do it is to grab a hold of the available functions object and then use bracket notation to pass in our function name call it with parenthesis and finally add function arcs like that we'll store this in a variable called function response and then finally log it out let's run the code and there we go we have turned a prompt from the user into a real function call that returns data that our assistant asked for really good job reaching this far we are making a ton of progress so let's just keep up the pace and carry on so we've called the function obtained the data we need and now we need to send it back to our assistant so how do we do that well as I've said earlier it's important that we keep track of all the dialogue in this app whether it's from the user from the assistant or in this case from the tool itself and where do we keep track of this dialogue take a guess yes it is in this messages array which we are passing along to mistal every time we interact with our API so what we're going to do here first is messages. push pass in an object and as you've seen before we always have a role though this time around it's not the role of user and also not the role of assistant which is what we had when we got the instructions to call the function this time around the role belongs to the tool and the next piece of information we need to pass along is the name of the tool which we have here in function name and finally the content after we've told mol that we worked with the tool and gave them the name of the tool what do you think the content here should be take a guess yes hopefully you understood that it is the response we got back from the function because when mistol gets all of this data it should be able to decide what the next step should be and speaking of which how do we then send this off to mistl well we could start a new client. chat down here and add all of the metadata again though that's a very hardcoded and hacky solution instead we want to rerun this piece of code and then yet again check if we're instructed to call yet another function and then keep on going until the assistant tells us that yes we are now done with the back and forth and I have a good response for the user and hearing that what kind of programming Paradigm does that sound like a job for and yes you guessed it the loop so we are going to wrap this entire thing in a loop and keep it running until we have a satisfying result we'll do that in the next Grim so I'll see you there okay we are ready to perform the final steps of our flow we'll take the result from the function and send it to our assistant who will then construct a reply and send it back so that our user is happy again and as I talked about in the previous Grim we'll do this through the help of a loop so this is a challenge where you are to start by creating a for Loop that runs a maximum of five times and inside of the for Loop if the Finish reason is ever set to the string stop then I want you to simp simply return the response from the assistant so then you are to return the entire function and that'll also then break out of the loop as you can see the Finish reason lives down here in the object you get from the assistant now you might ask at this point well why are we simply hardcoding in a for Loop that runs five times wouldn't it be better with a while loop that could run as many times as you need until the task is complete or for example a recursive solution that would do the same thing and yes those could be better Solutions but they also open up for the possibility of infinite Loops so it would require some guard rails in my opinion to implement such a solution which is why we're simply going for a naive for Loop that runs five times and that should be more than enough for our use case though of course if you want to build on this after you've solved the challenge you are more than free to do that and actually I would encourage you to do that anyway give this challenge your best shot and then I'll see you when you're done okay let's do this we'll do four let I equals z and I should be less than five and it should increment moving this all the way down here and indenting everything inside of the loop like that and checking here if response do choices the first item and yes it lives within that object if the Finish reason is stop then we'll simply return the content within the message and finally let's bring this up here and do else if like that okay the moment of truth let's see if we've been able to successfully implement this entire flow I'll comment out this console log we're asking when this transaction was paid let's run the code and yes the transaction 2001 was paid on October 5th 2021 wow congrats you've just built your very first AI agent and while this of course is a dummy example You Now understand the basic building blocks which gives you a foundation for building Real World products so give yourself a pat on the back and then I'll see you in the next scrim hi there now you are going to learn how to run mistal models locally on your computer and the tool we are going to use for that is called olama it is an app that wraps large language models on your computer and lets you interact with them in various ways so click on this image right here and you'll get to the AMA page there you can search for models for example mistl click into it and see the size of it this one is 4.1 GB and also read about how it performs compared to other open source models for now let's head back to the homepage and click on the download button then choose whatever platform you use and click yet again on the download button so that you can complete the download and install AMA on your computer once you've done that open up the terminal on your computer and type AMA run mistal that'll start the download process and as it'll take some time I'll fast forward and once it's done we get this little UI where we can type a message so let's ask the model something for example trying to use it as a motivational coach which I often use large language models for so I'll type feeling a bit demotivated today can you help me get started with my day and when I hit enter mistl starts typing out tip after tip not through being run on a thirdparty server but being run by my computer and let's just take a minute and acknowledge how cool this is because aside for the cost of the hardware and the electricity these tokens are completely free and there's also 100% privacy as the data stays on the device now what's perhaps even cooler is that you can use this as a model for any AI projects you build locally as well so let's try to do that in the current scrim click on the Cog wheel in the bottom right corner and then click download as zip then in your downloads folder you'll see this ZIP file so just go ahead and double click on it to unzip it and that'll give you a folder with a weird looking name that is the underlying ID for the scrim so take this folder rename it and place it wherever you keep your Dev project for me that is in the dev directory and I've named this project AMA hello world so I'll navigate into it and there you should do mpm install and then do npm start that'll spin up the project on Port 3000 meaning that you can head over to Local Host 3000 and there you will see the browser telling you to ask a question via the question parameter and what's going on here is that this little Express router checks if there is a question in the URL parameter called question and if there isn't a question there for example if you just visited the root page without any URL params it'll just render out this string though if it is a question there it'll execute the following lines of code and here we're using theama SDK and the patterns that are being used here is probably quite familiar to you right now because yes this resembles the mistal SDK quite a lot we call the chat method and pass in an object where we specify the model in this case it's mistal and a messages array that has a role and some content and the content here is whatever Express finds in the URL parameter called question so if we now type in a question in the browser for example why do stars shine and hit enter then we'll see that the browser will work for a little while and boom eventually it gives you reply which says that star shine due to nuclear fusion where hydrogen atoms are turned into helium and thus releasing immense amounts of energy and actually this continues until all of these atoms have turned into iron which by the way means that the iron you used to fry your eggs with was created billions of years ago in the center of a star wow that is mindblowing blowing to think about almost as mindblowing as the fact that you have reached the end of this mystal course most people who start a course here on scrimba give up before they reach the end but you my friend do not you are not a quitter so give yourself a pat on the back and in the next scrim we'll do a quick recap of everything you've learned wow you really did something special today you completed this course please remember that most people who start online courses give up you are not like them so let's have a quick recap of what you've learned starting out we looked at the basics of mistl and their platform along with the chat completion API that we interacted with through their JavaScript SDK you have a solid grasp of the various mistal models right now and also know how to work with embeddings and Vector databases with superbase you've also dipped your toes into Lang chain and using it for chunking text because you had to do that when you were learning about rag or retrieval augmented generation finally you know how to create AI agents with function calling and how to do local inference through the help of AMA now it's time for you to celebrate your win do share that you've completed this course on social media or if you want a less public way of doing that you can check out scribus Discord Community as there we have a today I did Channel where we love seeing people completing courses and whatever you do please keep on building you have a unique set of skills here and this is just a start the world of AI is exploding giving Developers like yourself the possibility to create entirely new experiences and apps the world is your oyster so happy building and best of luck
