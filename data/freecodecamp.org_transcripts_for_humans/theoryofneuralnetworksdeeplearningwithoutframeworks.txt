With timestamps:

00:01 - my talk is called theory of neuro
00:03 - networks and the alternate title would
00:05 - be deep learning without frameworks
00:08 - because i'm going to be um talking
00:11 - i'm going to be talking about the basics
00:13 - of deep learning and neural networks but
00:16 - i'm not going to be using any framework
00:18 - so i'm not going to be using
00:21 - keras or tensorflow
00:23 - and the whole point of this talk is to
00:25 - hopefully help you guys understand
00:28 - how deep learning and neural networks
00:30 - work kind of under the hood because
00:33 - usually when you're working with these
00:35 - things you're going to use a framework
00:36 - that kind of hides a lot of the
00:38 - implementation so hopefully after this
00:41 - talk
00:42 - even if you're you're completely new to
00:44 - deep learning and neural networks you'll
00:45 - have a
00:46 - better understanding of how how those
00:48 - things work
00:50 - and so the first part of my talk i'm
00:52 - going to be
00:53 - going over some of the basic concepts
00:56 - and showing how things work
00:59 - and then i have a second part after
01:01 - lunch where i'm going to bring it all
01:03 - together and do some live coding and
01:06 - show you how to solve an actual problem
01:09 - using just just straight python
01:12 - and
01:14 - showing you how to
01:16 - create a neural network from scratch
01:18 - so
01:20 - first a little bit about me
01:23 - let me get my
01:25 - there we go
01:26 - so my name is beau carnes like like you
01:29 - heard and i am from the united states
01:32 - i'm from the state of michigan and i
01:34 - currently work at freecodecamp.org
01:37 - it's a non-profit with the goal of
01:39 - providing a free curriculum for people
01:42 - learning software development
01:44 - and so anybody can go on and learn for
01:47 - free so i like a new thing that we're
01:50 - doing over at free code camp is we've
01:53 - translated the curriculum into five
01:55 - different world languages and we just
01:58 - use like this google automatic translate
02:01 - google automatic translator but we are
02:04 - an open source community so we're trying
02:06 - to get people to go in their native
02:09 - speakers and fix the automatic
02:11 - translations and russian is one of the
02:13 - languages so if anybody's interested in
02:15 - trying to help out this open source
02:17 - community and you speak russian you can
02:19 - go in and help with the translation
02:22 - besides that i also have made some
02:24 - courses for this book publishing company
02:26 - called manning publications
02:28 - i saw on the the booth out there i saw
02:31 - some manning publications books
02:33 - and i've created some video courses so i
02:37 - have one called algorithms in motion and
02:39 - then grocking deep learning in motion so
02:42 - the the second one is kind of the
02:44 - grocking deep learning emotion that's
02:45 - what this talk is based off of and the
02:48 - whole course um i mean the whole video
02:51 - course is actually based on this great
02:53 - book
02:54 - by another author named andrew trask so
02:57 - so this i would highly recommend this
02:59 - book for anybody trying to learn deep
03:02 - learning so my my whole my video course
03:04 - is based on the book and this talk is
03:06 - based on the course
03:08 - so let's kind of get into it here
03:11 - um
03:12 - oh first of all i wanted to tell you
03:14 - that i do so this is my personal website
03:16 - karns.cc and then i made a
03:19 - site a page called moscow
03:21 - so all my my slides are on there and all
03:24 - the code that i'm going to be going over
03:27 - during this part of the talk and the
03:28 - second part of the talk
03:30 - so you don't necessarily need to look at
03:31 - it now but if you ever want to refer to
03:33 - that you can
03:34 - and
03:36 - so i'm going to be going into more
03:38 - detail into code than some of the
03:39 - previous speakers
03:41 - so sometimes it's good to see the actual
03:43 - code
03:44 - so
03:45 - deep learning verse versus machine
03:47 - learning so machine deep learning is
03:50 - actually a subset of machine learning so
03:53 - if we go here um
03:55 - you can see that we have machine
03:56 - learning and the sub says deep learning
03:58 - and then the subset of that is
03:59 - artificial intelligence
04:01 - so um machine learning is kind of like
04:05 - what it sounds like machines or
04:06 - computers are are trying to learn
04:09 - something that they were not explicitly
04:11 - programmed for
04:12 - so machines observe a pattern and
04:14 - attempt to imitate it in some way
04:17 - so machine learning is often attempting
04:19 - to take an input data set and transform
04:22 - it into an output data set
04:24 - so let me show you some examples with an
04:27 - input dataset and an output data set so
04:29 - these are all examples that you would
04:31 - use machine learning or even deep
04:33 - learning to try to figure out so if you
04:35 - have pictures of a cat you have the
04:38 - input is pixels and then the output
04:40 - would be a presence or absence of a cat
04:43 - so you input the pixels and the output
04:45 - of the algorithm would be whether or not
04:47 - there's a cat there
04:49 - and the next one would be
04:51 - you could input movies you liked into
04:53 - the algorithm and then on output movies
04:55 - you may like so these are just some
04:58 - different possible use cases you would
05:00 - input words and then would output
05:02 - whether those words indicated that the
05:04 - person was happy or sad
05:06 - and then you would input weather sensor
05:08 - data and the output would be the
05:10 - probability of rain
05:13 - so
05:14 - these are all supervised machine
05:16 - learning tasks
05:18 - and the machine learning algorithm is
05:21 - attempting to imitate the pattern
05:23 - between two data sets in such a way that
05:26 - it can take use one data set to predict
05:28 - the other data set
05:30 - so i'm going to give you a summary of
05:32 - how machine learnings would do that and
05:35 - how a deep learning algorithm would do
05:37 - that let's say you have this data set of
05:40 - pictures and a lot of the pictures are
05:42 - pictures of cats but then some of the
05:44 - pictures are not cats
05:47 - so how would work is you would input all
05:49 - these pictures into into your algorithm
05:53 - and you would also tell it which
05:55 - pictures were cats
05:57 - and which pictures are not cats
06:00 - so um the supervised learning algorithm
06:02 - is going to extract patterns from that
06:04 - data set after it's learned the patterns
06:08 - we can show it a picture and algorithm
06:10 - will hopefully tell us whether the
06:11 - picture is of a cat or not so it takes
06:14 - all the information you give it and then
06:16 - hopefully
06:17 - later you can get a picture of a cat
06:19 - that it hasn't seen before and will be
06:21 - able to figure out whether it's a cat or
06:23 - not
06:24 - so deep learning is a subset of methods
06:28 - in the machine learning
06:29 - toolbox that uses artificial neural
06:32 - networks so let's get into more details
06:35 - about deep learning oh here would be an
06:37 - example is this a cat
06:39 - yes it's a cat
06:41 - so
06:44 - all
06:45 - deep learning and machine algorithm
06:47 - learning algorithms are going to be
06:49 - classified
06:50 - as either supervised or unsupervised
06:53 - learning
06:55 - uh so
06:57 - we take what we know and we transform
07:00 - into what we want to know so that would
07:01 - be a supervised learning we have
07:03 - something that we know to be true
07:05 - and we transform it into what we want to
07:08 - know
07:09 - so like an example would be
07:12 - we know
07:13 - that
07:15 - we know that
07:16 - this picture we know that this is this
07:19 - is a picture of a cat or maybe not a cat
07:22 - and we put into the algorithm to find
07:24 - out what we want to know like whether
07:26 - really it is a picture of a cat
07:28 - and so the um the goal of a supervised
07:31 - learning
07:33 - is is to learn a function that given a
07:35 - sample of data and the desired outputs
07:38 - best approximates the relationship
07:40 - between the input and the output in the
07:42 - data
07:44 - supervised learning is done using prior
07:46 - knowledge of what the output values for
07:48 - a sample should be
07:49 - like in the cat example
07:52 - we know that the
07:53 - that the output is either going to be
07:55 - cat or not cat or maybe you're trying to
07:58 - identify handwriting and you know it's
08:01 - going to be
08:02 - one of the letters of the alphabet you
08:04 - know you know the alphabet you know the
08:06 - the you already know what the output
08:08 - could be for supervised learning
08:10 - so for unsupervised machine learning
08:14 - we don't know what the output could be
08:17 - we're just import and putting a list of
08:20 - data points
08:21 - and we put into the algorithm and then
08:23 - out comes a list of cluster labels so
08:27 - we don't know for sure so the algorithm
08:30 - is trying to
08:32 - figure out and sort the data but it
08:35 - doesn't know what the categories that
08:37 - it's trying to sort into
08:39 - so unsupervised learning groups your
08:42 - data the goal is to infer a natural
08:44 - structure present within a set of data
08:46 - points
08:47 - and we put in a we put in a list of data
08:50 - points and we get the cluster labels out
08:52 - so
08:53 - basically an unsupervised learning
08:55 - algorithm says find patterns in this
08:58 - data and tell me about them so it's just
09:00 - trying to figure out some patterns
09:04 - let's see
09:05 - that was slightly out of order we'll get
09:07 - back to that slide so in this so this
09:09 - would be kind of an example of
09:11 - unsupervised data where you have
09:13 - all this all these data points and your
09:15 - algorithm groups them into these
09:17 - different
09:18 - groups
09:19 - and it basically so the algorithm
09:21 - basically says hey
09:22 - hey data scientist i found some
09:25 - structure in your data it looks like
09:27 - here are some groups in your data here
09:29 - are the groups
09:30 - and then it's up to the data scientist
09:32 - or the programmer to figure out what the
09:35 - groups are it doesn't tell you what the
09:36 - groups are you just kind of have to
09:38 - figure that out after you see the groups
09:41 - so
09:42 - the next thing i want to talk about is
09:43 - parametric versus non-parametric
09:46 - learning
09:47 - and
09:47 - you know pretty soon we'll get to some
09:49 - actual code examples so this will start
09:51 - to make more sense
09:53 - but for
09:54 - for this
09:56 - um you can just kind of imagine a cloud
09:59 - so with uh so this is our machine
10:01 - learning cloud there's two knobs we have
10:03 - supervised an unsupervised knob and a
10:05 - parametric and non-parametric so there's
10:08 - you can kind of switch your algorithm to
10:10 - be either supervised or unsupervised
10:12 - parametric or non-parametric supervision
10:15 - is about the type of pattern being
10:17 - learned and parametricism is the way the
10:20 - learning is stored
10:22 - so the two types parametric and
10:24 - non-parametric parametric
10:27 - is with a fixed number of parameters and
10:30 - non-parametric is possibly an infinite
10:33 - number of parameters so here's kind of
10:36 - an oversimplification a parametric is
10:38 - trial and error
10:40 - and non-parametric is about counting and
10:42 - probability
10:44 - so
10:45 - you could have
10:47 - you're just kind of counting and you
10:48 - don't know how many parameters there's
10:50 - going to be a non-parametric
10:51 - so let's do an example i'm going to go
10:54 - back to
10:56 - this slide so let's say you're trying to
10:58 - figure out you have this square and
11:00 - you're trying to figure out where it
11:02 - should go in this little toy here now
11:05 - again this is a pretty simple example
11:07 - but a some people like a baby may may
11:10 - just jam it into every hole until they
11:12 - find out where it fits so that's kind of
11:14 - like parametric learning
11:16 - now non-parametric learning would be
11:18 - you're counting the sides of the square
11:20 - and counting the sides of the the hole
11:22 - and then you kind of figure out where
11:24 - it'll go on that one so a parametric
11:27 - models tend to use trial and error like
11:29 - just trying everything a non-parametric
11:31 - tends to count to figure out where it
11:33 - should go
11:34 - but most algorithms
11:37 - are supervised parametric learning so
11:41 - from the rest of this talk and the next
11:42 - talk i'm gonna be talking about
11:44 - supervised parametric learning and
11:46 - giving you some examples to go along
11:48 - with that
11:49 - so
11:50 - this is basically trial and error using
11:53 - knobs
11:54 - and so there are three steps uh step one
11:57 - will be predict so
11:59 - um the three steps to supervise
12:01 - parametric learning is the first step is
12:03 - predict so for the predict step you take
12:05 - all your data in this case we're trying
12:08 - to predict if a sports team is going to
12:10 - win a game or not so we input this data
12:13 - like number of toes on the team number
12:15 - of players number of fans we put it into
12:19 - our algorithm which is our machine
12:21 - and then out comes a prediction so in
12:24 - this case
12:25 - our prediction is saying that there's a
12:27 - 98 chance given this data that our team
12:30 - is going to win
12:32 - so step two
12:34 - is going to be
12:36 - compared to the truth pattern so we have
12:39 - our prediction which is 98 chance that
12:41 - the team's gonna win and our truth
12:43 - pattern is we found out that in real
12:45 - life they lost the game so zero percent
12:47 - chance of that they're gonna win so we
12:49 - thought we our original prediction was
12:51 - 98 chance they're gonna win but actually
12:54 - they lost uh so we after and so then
12:57 - after we compare to the truth pattern
12:59 - this is where we are going to
13:02 - learn the pattern
13:04 - so
13:04 - learning um
13:05 - [Music]
13:08 - when we're going to just
13:10 - um take the information about how much
13:12 - we were wrong
13:14 - so we look at how much we missed by
13:17 - which is 98
13:19 - and we also look at the input data at
13:21 - the time of the prediction
13:22 - and then we're going to turn the knobs
13:24 - so these knobs would be weights so
13:27 - with the knob of the wind loss the home
13:29 - away number of toes number of fans so
13:32 - based on all the information we're going
13:34 - to adjust the weights
13:36 - so hopefully the next time the next
13:38 - iteration of the algorithm
13:40 - we can
13:41 - uh be a little closer instead of 98
13:43 - percent chance we're going to win that's
13:45 - going to that number is going to be
13:46 - lower because we know that in reality
13:48 - they didn't win with that information
13:51 - now
13:52 - um
13:53 - so now that you kind of know the basic
13:55 - steps i know it's kind of hard to
13:57 - understand all that how all that fits
13:59 - together but now i'm going to get into
14:01 - some actual code so hopefully it starts
14:04 - to make sense what i was talking about
14:06 - with the predict compare learn
14:08 - so this is a simple
14:10 - neural network
14:12 - and a neural network is one or more
14:14 - weights which we can multiply by our
14:17 - input data to make a prediction
14:19 - so it's all about trying to make a
14:21 - prediction on on the correct answer
14:23 - based on those weights so in the example
14:26 - that i gave the prediction will be
14:27 - whether or not the team is going to win
14:29 - a game based on the the input debt and
14:32 - the weights
14:33 - so here are our first few lines of code
14:36 - here
14:37 - we i start by just hard coding a weight
14:40 - number now generally your weights are
14:42 - going to start as random numbers which
14:44 - we'll see in the live coding section
14:46 - later
14:46 - but um here it's just a hard-coded so
14:49 - you can kind of see how it works better
14:51 - and we have our neural network function
14:54 - so
14:56 - we can see that our neural network
14:57 - function we just
14:59 - pass in the input and the weights and
15:01 - then it's going to set the prediction to
15:03 - input times weights and it returns the
15:05 - prediction
15:06 - so it's all about just multiplying
15:08 - inputs times weights so
15:10 - this diagram would kind of show to you
15:12 - how it works you input the information
15:14 - here multiplies by the weight of 0.1 and
15:17 - the prediction is the percent chance
15:19 - that the team is going to win
15:22 - so
15:23 - this next section of code we just talked
15:25 - about this part right here but here is
15:28 - our data so we have this
15:30 - array of number of toes this would be
15:33 - like average number of toes for each of
15:35 - the first four games in in the season so
15:38 - it's kind of a silly example but just
15:40 - kind of bear with me here
15:43 - our input is just going to be the first
15:44 - number because we can only run on one
15:46 - number at a time
15:48 - so we're inputting just as 8.5
15:50 - and then our prediction
15:52 - we just call the neural network function
15:54 - which multiplies the input times the
15:56 - weight and then we get out the
15:57 - predictions so it inputs so we have the
16:00 - 8.5 times the weight so our prediction
16:03 - is that it's going there's an 85 chance
16:06 - that our team is going to win
16:09 - so we can see that the prediction is the
16:11 - input times the weight here
16:14 - so let's talk more about what the input
16:16 - data is so an input data is a number
16:19 - recorded in the real world somewhere so
16:22 - it's usually something that's easily
16:23 - knowable like in the example average
16:25 - number of toes it could be today's
16:27 - temperature it could be yesterday's
16:29 - stock price
16:30 - um so it's just some we're going to
16:33 - input some data that we can see in the
16:35 - real world and then we are going to make
16:38 - a
16:39 - prediction so a prediction is what the
16:43 - neural network tells us given our input
16:45 - data so like given today's temperature
16:49 - we i think the
16:50 - network thinks there's a zero percent
16:52 - chance that people are going to wear
16:53 - swimsuits today or maybe given
16:56 - yesterday's stock price
16:57 - there's uh today's stock pro stock price
17:00 - is going to be 101
17:02 - 0.3 or something like that or given this
17:05 - many number of toes this team is going
17:08 - to
17:08 - win the game so that would be the the
17:11 - prediction
17:12 - and then
17:13 - [Music]
17:16 - the network learns through trial and
17:18 - error so in that example it was just it
17:21 - only went through at one time
17:23 - so in a real neural network which we
17:26 - will get to and i'll show you a network
17:28 - with a lot of iterations
17:30 - every time it runs through the code
17:32 - it updates the weight and updates the
17:35 - error and it keeps running through the
17:36 - code over and over
17:38 - to to learn
17:40 - so it first it's going to try to make a
17:43 - prediction then it sees whether it was
17:45 - too high or too low
17:47 - and then finally it changes the weight
17:49 - up or down to predict more accurately
17:51 - the next time it sees the same input
17:53 - so
17:54 - so far we've seen
17:56 - um
17:58 - we've seen the network making a
17:59 - prediction so remember the three steps
18:01 - were prediction and then comparing and
18:04 - then learning
18:06 - so we still need to i still need to show
18:08 - you how you would compare it to the
18:10 - truth pattern and learn and i'll get to
18:12 - that in a minute but i'm going to show
18:14 - you uh one another example before i get
18:17 - to the compare and learn
18:20 - so here i'm going to show you a neural
18:23 - network with multiple inputs and outputs
18:26 - so the first example there was just one
18:28 - input and one output
18:30 - in this example we're going to have
18:32 - three inputs so be like the number of
18:34 - toes per player the win loss record
18:37 - number of fans would be the inputs we're
18:38 - inputting all those things into the
18:40 - network and we're outputting
18:42 - the percentage of hurt players whether
18:44 - or not the team is going to win and
18:47 - whether or not the team is sad
18:49 - so this is just kind of to show that
18:52 - i started with showing a very simple
18:53 - network but they can get more complex
18:55 - you can have
18:56 - any number of inputs you want and you
18:58 - can be trying to predict multiple things
19:00 - you don't just have to predict one thing
19:02 - for the inputs you can predict multiple
19:04 - things
19:06 - so
19:07 - here's all the code now i'm going to
19:10 - kind of zoom in so you can see the
19:11 - different sections so first we're going
19:13 - to talk about the weights so before we
19:16 - just inputted one number for the weight
19:18 - but now that we have multiple inputs and
19:20 - multiple predictions we have to have a
19:22 - matrix so a matrix is just an array of
19:25 - arrays basically so you can see the
19:27 - first line
19:28 - is the no is the
19:31 - the weights for number of people hurt
19:33 - second line number of people that if we
19:35 - won or lost and then percentage of sad
19:38 - and then for kind of each column
19:41 - we have the weights for the number of
19:42 - toes
19:43 - what the win ratio is and the number of
19:45 - fans
19:47 - so then our neural network looks almost
19:50 - exactly the same as before remember
19:52 - before we just multiply the input times
19:54 - the weights
19:55 - and that's what we're doing now except
19:58 - we have this fancy function of vector
20:00 - matrix multiplication
20:02 - before we are just multiplying one
20:04 - number by another number now we're
20:06 - multiplying a a vector which is an array
20:10 - by a matrix which is an array of arrays
20:12 - and so we need an extra some extra code
20:15 - to make it so you can multiply a vector
20:17 - by a
20:18 - matrix and we'll see that in a little
20:20 - bit here so this is our information
20:24 - we're passing our this is our input data
20:26 - for the first four games but for our
20:28 - input we're just going to take the first
20:30 - number of each array so we're just going
20:32 - to do kind of one game at a time
20:34 - and
20:36 - so here's an example of what it would
20:39 - look like
20:41 - for we're passing in our input data
20:44 - and then we have all these predictions
20:46 - that kind of come out of the network
20:49 - so i talked to talk about the vector
20:51 - matrix multiplication function i'm not
20:53 - going to go into this and great detail
20:55 - it's just a way to
20:57 - multiply a vector by a matrix but in
21:00 - real and this function kind of uses this
21:02 - weighted sum function but in real life
21:05 - you're probably never going to write
21:06 - these functions yourself
21:08 - because most people are going to use
21:10 - numpy so numpy gives you
21:13 - just extra
21:15 - extra functions to do math like
21:17 - multiplying matrices and
21:20 - func matrices and
21:22 - vectors so if you this line right here
21:25 - from our neural network and numpy is
21:28 - going to be like this and when i do my
21:30 - live coding i'm actually going to use
21:31 - numpy so you'll see how that works so
21:34 - this dot dot here that's a numpy
21:37 - function and this just means multiply
21:39 - the input
21:40 - by weights
21:41 - and then we get the answer into our
21:43 - prediction here
21:46 - okay so at the end of our network we
21:47 - just set our prediction to equal the
21:50 - the return value of our neural network
21:52 - and we can print our predictions so you
21:55 - can see here here are predictions that
21:58 - we printed out based on this network
22:01 - here
22:02 - so if we go over here you can kind of
22:04 - see everything
22:05 - on the screen at once and then down here
22:08 - what you can see the prediction so that
22:09 - first and you can see how the math is
22:11 - calculated
22:12 - and so our her prediction was 0.55 so we
22:15 - think
22:16 - there's a
22:18 - probably 55 percent of players may be
22:20 - hurt a 98 chance of winning and a 97
22:24 - chance that people are sad on the team
22:27 - so we can predict three things at once
22:30 - so
22:31 - the next thing i want to talk about is
22:33 - error
22:34 - and gradient descent
22:37 - so
22:38 - this green descent is used while
22:40 - training a machine learning model and
22:43 - it's an optimization algorithm that
22:45 - tweaks its parameters iteratively to
22:48 - minimize a given function to its local
22:50 - minimum so
22:51 - in other words it allows us to calculate
22:53 - both the direction and the amount that
22:56 - we should change our weights so we
22:58 - reduce our error
22:59 - so the whole point of our neural network
23:01 - is trying to get our error to zero
23:04 - so our gradient descent which we'll see
23:06 - the code later it's pretty simple
23:08 - it helps us figure out
23:11 - which direction we should move our
23:12 - weights and how much we should move our
23:13 - weight so when we calculate the error it
23:16 - goes down it's just a way to every time
23:18 - we run the the code
23:20 - every iteration we're going to use our
23:22 - great descent function
23:24 - to
23:25 - get our weights to the right spot so our
23:27 - error goes to zero
23:33 - so
23:34 - we talked about supervised parametric
23:36 - learning in the three steps we talked
23:38 - about
23:39 - so but so far you've only seen predict
23:42 - but now i'm going to show you how to do
23:44 - the second two steps the compare to
23:46 - truth pattern and learning the pattern
23:50 - so
23:50 - for
23:51 - this step two is compared to the truth
23:54 - pattern
23:56 - to measure the error we determine how
23:58 - far our prediction is from our goal
24:00 - prediction
24:02 - and then step three learn the pattern it
24:05 - takes our air and tells each weight how
24:07 - how it can change to reduce it so the
24:11 - whole point is trying to figure out
24:13 - which weights are impacting our air so
24:16 - we can reduce our error
24:18 - to zero and by time when our air is zero
24:22 - that shows us that our weights are
24:23 - accurate because there's no error
24:26 - so now i'm going to show you
24:33 - pressing this button here hopefully i'll
24:35 - change slides just a second
24:38 - oh wait
24:44 - okay so learning is adjusting our weight
24:46 - to reduce the error to zero so that's
24:49 - what we're trying to do to actually
24:50 - that's what our learning is actually
24:52 - doing so here is our one iteration of
24:55 - gradient descent
24:57 - now in a real example you could have
24:59 - like hundreds of iterations or thousands
25:01 - of iterations and when i get to the live
25:03 - coding i'll show you one with a lot of
25:04 - iterations but let's just take it one
25:06 - iteration at a time so first we get our
25:08 - weight
25:09 - and our alpha
25:11 - and
25:12 - our alpha is the simplest way to prevent
25:15 - over correcting our weight updates
25:18 - so sometimes the goal is to get our air
25:20 - to zero but sometimes instead of our air
25:22 - going down it will start going up so
25:25 - alpha is something that's going to help
25:27 - it to always go down and go in the right
25:29 - direction and you'll see later how that
25:31 - fits into our code
25:32 - it's kind of trial and error for what
25:34 - alpha you're going to use but it's
25:36 - usually a multiple of ten so you would
25:38 - start with maybe point zero one then you
25:40 - could try point one and then maybe try
25:43 - one and just try to see which one is
25:45 - gonna make your air go down instead of
25:47 - up
25:48 - so
25:50 - uh let's see and then our neural network
25:52 - is the same as before which is input
25:54 - times weight and here's the diagram so
25:57 - our input the data goes here goes
25:59 - through a weight and then we outcomes
26:02 - our prediction of our wind prediction
26:05 - so
26:07 - now
26:08 - we are going to
26:10 - um
26:12 - take
26:13 - we we're going to do our prediction step
26:15 - so we pass in our information you can
26:17 - see there's only a few things different
26:19 - here which are goal prediction and error
26:22 - so we didn't talk about air in our last
26:24 - code but that's very important because
26:25 - we're trying to get our air to zero so
26:27 - right here we see that we have our
26:30 - number of toes is 8.5 just like before
26:32 - our input
26:34 - is our number of toes now our goal
26:35 - prediction is our winner loss binary
26:37 - which is a one
26:39 - means that the number one means that
26:41 - they won the game so our goal prediction
26:44 - is one so we think that with an input of
26:46 - 8.5 the goal prediction is one that we
26:50 - they won the game so we do our
26:53 - prediction just like before with our
26:54 - neural network we multiply the input
26:56 - times the weight
26:57 - and then our error is going to equal
27:00 - prediction minus goal prediction square
27:05 - so this is
27:07 - prediction minus gold prediction is the
27:09 - raw error it's it's how much we
27:11 - predicted was going to happen
27:12 - minus
27:14 - what we thought was going to happen and
27:16 - then the reason why we square it it
27:18 - forces the raw error to be positive by
27:21 - multiplying by itself so a a negative
27:24 - prediction a negative error when it
27:26 - makes sense so when we square it's
27:28 - always positive and also has added
27:30 - benefit of making
27:32 - large areas larger and small errors
27:35 - smaller which actually helps our neural
27:37 - network to go quicker to the correct
27:40 - uh
27:41 - to the correct thing by squaring it like
27:42 - that
27:44 - so
27:45 - if we
27:47 - run our neural network we do the 8.5
27:50 - times 0.1 we get the 0.85 but then our
27:52 - error ends up being 0.023
28:02 - okay
28:04 - so then we have our delta so this is the
28:07 - compare step we're going to calculate
28:09 - our node delta and the delta is how much
28:12 - this node missed so that's the raw error
28:14 - prediction minus goal prediction so so
28:17 - we need to know how much the node missed
28:19 - so we can
28:21 - calculate we can kind of put it into our
28:24 - output
28:25 - so
28:26 - we know uh we know we missed
28:30 - by negative 0.15
28:33 - because our prediction was
28:37 - 8.85
28:39 - and our goal prediction was 1. so 0.85
28:42 - minus 1 is negative 0.15
28:45 - so
28:46 - weight delta this is kind of where this
28:48 - is where the gradient descent happens so
28:49 - you can see it's actually a pretty
28:51 - simple line of code it's how much this
28:53 - weight caused the network to miss
28:56 - so we want to find out we know that
28:59 - are we missed but how much do we miss we
29:01 - just use the weight delta which is this
29:03 - the input times delta
29:06 - gives us the weight delta
29:08 - and we're trying to figure out how much
29:10 - to update the weight because we're
29:12 - trying to change the weight on every
29:14 - iteration now this actually does three
29:17 - good things for us it helps us with
29:18 - stopping
29:19 - negative reversal
29:21 - and scaling
29:23 - so stopping
29:25 - it's the first effect on our pure error
29:26 - caused by multiplying it by our input so
29:29 - just imagine you're listening to music
29:31 - or you're trying you're playing music on
29:33 - your computer
29:34 - or you're trying to play music and you
29:36 - have speakers so you turn up the
29:37 - speakers all the way but music still
29:39 - isn't coming out because you forgot to
29:41 - hit play on your computer so this would
29:44 - kind of be an example of stopping where
29:46 - it's kind of addresses in our neural
29:48 - network so if our input is zero like
29:50 - you've got to explain your computer
29:52 - it will force the weight delta to also
29:54 - be zero
29:55 - so we don't learn when our input is zero
29:58 - because there's nothing there's nothing
29:59 - to learn
30:01 - and so moving it makes no difference
30:03 - then negative reversal
30:05 - um
30:06 - when our input is positive moving our
30:09 - weight upward makes the prediction move
30:11 - upward but if our input is negative then
30:14 - all of a sudden our weight changes
30:15 - directions now we only want our weight
30:18 - to go in one direction so when our input
30:20 - is negative then moving our way up makes
30:22 - the prediction go goes down we don't
30:24 - want that so multiplying our delta by
30:28 - our input
30:29 - will reverse the sign of our weight
30:31 - delta in the event that our input is
30:33 - negative
30:34 - so this is the negative reversal
30:37 - this ensures that our weight moves in
30:38 - the correct direction
30:40 - um scaling is just when we mol anytime
30:43 - you multiply things together
30:45 - it's just it's either going to get a lot
30:47 - bigger or a lot smaller if the number is
30:49 - less than zero
30:50 - so this is this is good because we want
30:53 - our we want our weight delta to we want
30:56 - big errors
30:57 - to be really big and small to be really
31:00 - small
31:02 - and
31:02 - alpha is going to help it so it doesn't
31:05 - go out of control so we don't want it to
31:06 - get too big that's why we have the alpha
31:08 - which we'll just about to talk about
31:10 - here
31:10 - but first we can see how the weight
31:13 - delta it kind of gets applied to the
31:15 - weight here of negative 1.25
31:18 - okay so now we're getting toward the end
31:20 - of this we are going to update the
31:23 - weight so when it says weight minus
31:25 - equals that just means weight equals
31:27 - weight minus weight delta times alpha
31:31 - and so this this allows
31:34 - we're going to multiply the weight delta
31:36 - times the weight or we're going to do
31:38 - weight minus the weight delta
31:40 - and we multiply it by the alpha to
31:42 - control how fast the network learns
31:45 - because it can like i said can it can
31:47 - update weights too aggressively but the
31:50 - alpha is going to remember the alpha in
31:52 - this case was .01 so it makes it so the
31:55 - weight doesn't get updated as quick
31:57 - quickly and so it doesn't get too out of
31:59 - control so in this case the new weight
32:02 - is 0.11275
32:06 - so
32:07 - we're actually getting toward the end of
32:09 - our the first part of the talk i know
32:11 - this is kind of a lot of information all
32:14 - at once especially if you're kind of new
32:15 - to deep learning um so my hope is that
32:19 - in this far part of your the talk i gave
32:21 - you some like good
32:23 - foundations and background knowledge so
32:26 - when we get to the second part of the
32:28 - talk um it will you'll be able to
32:30 - understand how it all goes together so
32:33 - in the second part of the talk i'm
32:34 - basically we're going to use everything
32:36 - we've learned but we're going to use a a
32:38 - real problem and i'm going to live code
32:41 - um a full neural network complete with
32:44 - iterations so you'll be able to see how
32:45 - the weight updates you'll be able to see
32:48 - how the air updates you'll you'll be
32:50 - able to see in the output of the the
32:52 - neural network how the air is going to
32:53 - zero
32:54 - and
32:55 - hopefully all these things we've kind of
32:57 - talked about that you've seen for the
32:59 - first time will start to make more sense
33:01 - when you see how it all works in an
33:03 - actual neural network so we'll also in
33:06 - the second part of the talk talk more
33:08 - about the deep learning part of this so
33:11 - so far i haven't even talked about why
33:13 - it's called deep learning but that's
33:15 - something else we'll be talking about
33:17 - i'm glad to see so many people decide to
33:19 - come back to part two of my talk so this
33:21 - is where i'll be pulling it all together
33:23 - what we were talking about in the first
33:24 - talk and hopefully a lot of things will
33:26 - start to make a lot of sense once you
33:28 - see a full neural network
33:30 - based on trying to solve a problem let's
33:32 - um let's get into it first of all i'm
33:34 - going to explain
33:36 - the problem so this is a problem we're
33:38 - going to try to solve with a neural
33:39 - network
33:41 - and it's kind of um
33:43 - it's it's a real world problem but it's
33:46 - a simplified real world problem so we
33:48 - have enough time to get through
33:49 - everything and so you can understand all
33:51 - the components of it
33:52 - so the problem so we you have to imagine
33:55 - the scenario so imagine the situation
33:58 - where you go to a country you've never
34:00 - been to before
34:01 - and you see a stoplight
34:04 - and you don't know there's three lights
34:06 - but you don't know which configurative
34:08 - configuration of lights means to walk
34:11 - and which configuration of lights mean
34:13 - to stop
34:14 - so um since you're since you're a
34:17 - programmer and a data scientist you
34:18 - decide you want to collect some data
34:20 - so you just sit there and watch and you
34:22 - observe whether people stop or walk so
34:25 - after a little while you you collect
34:28 - this information so you can see this is
34:30 - the first stop light in that
34:32 - configuration of lights with the two on
34:34 - the side on and the middle one off
34:37 - people stopped and then you see this
34:40 - people walked in the the second
34:42 - configuration of lights and then the
34:44 - third was just this one on a person was
34:46 - stopping so you're trying to figure out
34:48 - which configuration means walk and stop
34:50 - but this isn't quite enough information
34:52 - so you keep looking and you collect all
34:54 - this information here
34:56 - and so we're trying to um
35:02 - we collected the information of what we
35:04 - know
35:04 - and what we want to know so what we know
35:07 - is the configuration of the stoplights
35:10 - what we want to know is whether they
35:12 - mean to walk or stop
35:15 - so we're going to use this data and see
35:18 - if our neural network that we're going
35:20 - to develop can figure out
35:22 - which configuration of lights means to
35:25 - walk or stop now this is a simple
35:27 - example so hopefully you can kind of
35:29 - already see a pattern that the the
35:31 - middle light is perfectly correlated to
35:34 - whether you should walk or stop and
35:36 - actually the left and right light don't
35:38 - have anything to do with whether you can
35:40 - walk or stop
35:41 - so when we develop our neural network
35:43 - we're not going to tell it that
35:45 - information we're going to see if our
35:46 - neural network can learn on its own
35:49 - that the middle light is perfectly
35:51 - correlated to the walk and stop
35:54 - data so
35:56 - to make a neural network we cannot just
35:59 - put in these notes or this picture here
36:01 - so we have to convert it to numbers so
36:04 - this is how we're going to convert our
36:06 - street light data to numbers we're going
36:09 - to make if the light is on it's a one if
36:12 - the light is on
36:13 - or if it's off it's a zero so you can
36:15 - kind of see how these
36:17 - correspond to each other
36:19 - so we've got this new data set and we're
36:22 - going to use it to create a neural
36:24 - network to solve it
36:26 - so i'm going to go over
36:28 - to my code editor here now this is
36:32 - something called google colab
36:35 - and google colab a lot it's basically
36:37 - like an online code editor for jupyter
36:40 - notebook so i if you got i don't know if
36:42 - you're familiar with jupiter notebook
36:44 - but it's used a lot for deep learning
36:46 - and machine learning and this is an
36:48 - online version so you don't have to have
36:50 - anything installed on your computer
36:52 - and so it's make things makes things a
36:55 - lot simpler when you're testing things
36:56 - out here so
36:58 - i'll zoom in a little bit more
37:01 - so this is where i'm going to start
37:02 - typing code if you want to follow along
37:04 - you can or you can just look up at the
37:06 - screen but the first thing we're going
37:07 - to do is import numpy
37:11 - as np now let me build
37:15 - okay let's can you guys can see that
37:18 - okay so
37:20 - before we didn't use numpy but now we're
37:22 - going to use numpy and that's going to
37:24 - help with our multiplication remember
37:26 - before i showed you that vector matrix
37:28 - multiplication function we're not going
37:30 - to need that because numpy has that
37:32 - built in
37:34 - and that's the only thing we're going to
37:35 - import the rest of it's just going to be
37:36 - straight python so now we're going to
37:38 - have our weights array
37:41 - and i'm going to do mp dot array
37:45 - and the mp.array means it's a it's a
37:48 - numpy array and this allows us to do um
37:52 - special operations with it like the
37:54 - vector matrix multiplication and stuff
37:56 - like that so
37:57 - a lot of neural networks their weights
38:00 - are going to start off randomly
38:03 - and so i thought this would be a cool
38:04 - chance for some audience participation
38:06 - and i'm gonna see if you guys can give
38:08 - me some numbers they have to be between
38:09 - negative one and one so give me a
38:11 - decimal point between negative one and
38:13 - one so does anybody want to call out a
38:15 - number for our first weight
38:20 - zero seven okay how about 0.7
38:24 - and then we need another it has to be
38:26 - between negative 1 and 1. anybody else
38:27 - have another number
38:29 - 0.2 okay
38:31 - and then our final number
38:38 - okay so this is just a fun way to get
38:40 - random numbers so you can see later that
38:42 - it kind of doesn't matter what you start
38:44 - your weights out as your network's going
38:47 - to be able to learn and get to the
38:48 - correct weight with just random weights
38:51 - so the next line
38:53 - is the alpha
38:56 - and we talked about that a little bit
38:57 - before
38:58 - the alpha helps make sure your network
39:00 - doesn't get out of control and the whole
39:02 - point of the network is to get our air
39:04 - to zero and but sometimes the air can
39:07 - start going up instead of down to zero
39:10 - and the alpha will make sure it doesn't
39:12 - go go out of control like that
39:14 - so now we're going to
39:17 - import our
39:18 - data that we collected
39:21 - so it's going to do mp.array
39:24 - now
39:25 - i'm going to be just
39:27 - this is going to be a vector or a matrix
39:29 - so that's an array of arrays
39:32 - and
39:32 - i'm just going to be typing in this
39:34 - information i kind of have some notes
39:36 - here but this is directly from the other
39:38 - slide
39:39 - of what this is going to look like
39:42 - [Applause]
39:44 - so this is kind of the
39:46 - sometimes the tedious part of machine
39:49 - learning and data science is like
39:51 - dealing with the data and inputting data
39:53 - but it's just a part of it is actually
39:55 - just trying to put everything into the
39:56 - network
39:59 - and usually you're not going to be
40:02 - having to type in the data you're
40:04 - usually going to get it from some other
40:06 - source
40:07 - so it may not be you typing in every
40:09 - single number in the data set
40:12 - let's see
40:14 - one
40:23 - and you at some point you may see me
40:25 - type something in wrong that lets it's
40:27 - obvious that's wrong so feel free to let
40:29 - me know like if i spell like a variable
40:30 - wrong or something like that
40:37 - okay so if we go back to our data this
40:40 - should
40:41 - look just like
40:42 - what we typed in right here
40:45 - and now our next piece of information is
40:48 - our walk
40:49 - verse stop array so this is again
40:52 - information we collected in the real
40:54 - world in this example
40:56 - and we're going to do another mp.array
40:59 - and then pass in the array
41:02 - which is going to be 0
41:04 - 1 zero
41:08 - one
41:09 - one
41:10 - zero so again that's the the walk versus
41:13 - if we actually have to go back a slide
41:16 - so this is anytime it's zero or stop is
41:18 - zero one is walk
41:22 - okay
41:23 - there we go
41:25 - and then we can start with our iteration
41:29 - so in the last example
41:31 - before the break i showed you a single
41:34 - iteration but in this example we're
41:37 - going to do an iteration
41:39 - of 40.
41:40 - so i'm going to do
41:42 - four iteration and range
41:44 - 40. now
41:45 - like i had said previously and sometimes
41:49 - you could have hundreds or thousands of
41:50 - iterations and the whole point is that
41:53 - every time we go through an iteration
41:55 - the error gets closer and closer to zero
41:58 - and the way you figure out how many
42:00 - iterations to do
42:02 - is just through trial and error
42:04 - so from like from
42:06 - preparing for the talk and other
42:08 - experiences kind of trying out different
42:10 - things i found out that in 40 iterations
42:13 - this network should be able to get the
42:14 - air to zero but there's no there's not
42:17 - really any magic number you just have to
42:18 - try different things and see the results
42:20 - and see how long it takes to get to zero
42:23 - so
42:25 - now
42:26 - we are going to
42:27 - have a variable called error for
42:30 - all
42:31 - lights and it's going to be set to zero
42:34 - so
42:35 - we we need to figure out the air because
42:37 - the whole point is to get the air to
42:38 - zero so we have to something to collect
42:40 - the air so we're going to be adding to
42:42 - that air later and now
42:45 - i'm going to have another iteration for
42:47 - row index
42:49 - and range
42:50 - and then it's going to just be the
42:52 - length of the stop
42:54 - verse
42:56 - walk array
42:57 - so we're going to do something for
43:00 - every
43:01 - element in this walk versus stop array
43:03 - and the number of elements in the walk
43:05 - for stop array is also the number of
43:07 - arrays or vectors in the streetlights
43:09 - array so we're going to do something for
43:11 - each one that you'll see in a second
43:12 - here
43:13 - so in our last example just like in our
43:15 - exam last example we need an input and a
43:18 - goal prediction
43:19 - so this is going to be pretty similar
43:23 - i'm going to the input is going to be
43:24 - from the streetlights array
43:26 - and i'm going to put
43:29 - the the row index
43:32 - and then the goal prediction
43:37 - is going to be from the walk verstappen
43:46 - and that's going to be the row index
43:49 - so you'll see we're going to go through
43:51 - we're going to iterate through each one
43:52 - because this array corresponds to this
43:55 - number the second array
43:57 - corresponds to the second number the
43:58 - third rate corresponds to the third
44:00 - number
44:05 - okay what part
44:14 - the space after
44:20 - what oh
44:23 - stop verse walk i should definitely
44:26 - oh like is this what we're talking about
44:32 - oh
44:33 - yeah okay
44:34 - thank you i don't know i was looking at
44:36 - it for some reason i was thinking
44:37 - there's a spelling here so it's like
44:38 - what's spelled wrong so yeah thanks for
44:40 - pointing that out so this is a this is
44:42 - an example of what i was talking about
44:44 - i'm gonna make some mistakes but luckily
44:45 - i have the audience here too to help me
44:47 - fix these mistakes here
44:49 - okay so
44:51 - now we have our input and our goal
44:54 - prediction
44:55 - we can
44:56 - make our prediction
44:59 - so let me see if i can
45:01 - can you guys still see down at the
45:03 - bottom of the screen there
45:05 - okay so
45:06 - our prediction
45:08 - now if you remember before when we had
45:11 - our prediction in the first part of the
45:13 - talk i showed that we called our neural
45:15 - network function and our neural network
45:17 - function just multiplied the input times
45:20 - the weight
45:21 - so instead of creating a neural network
45:23 - function we're just going to do the
45:24 - input times the weight
45:26 - but we're going to use
45:28 - numpy
45:32 - so
45:32 - this is this dot function
45:35 - just multiplies input times weights and
45:38 - it can multiply a vector times a matrix
45:41 - so we're getting the the prediction the
45:43 - same ways from before and now we have to
45:47 - calculate the error
45:49 - let's see
45:51 - okay there we can see a little better
45:54 - the error is going to equal
45:57 - prediction
45:59 - minus
46:00 - goal prediction
46:02 - and that's the same as before so that's
46:05 - how we figure how far away our
46:07 - prediction was from what we thought it
46:09 - was going to be is the error
46:11 - and then also oh yeah one last thing we
46:13 - have to
46:14 - take that to the power of two so
46:16 - remember the point of that is to make
46:17 - sure the error is positive if you square
46:20 - it it's going to be a positive number
46:21 - and then also it makes big errors even
46:25 - bigger and smaller areas even smaller
46:27 - and that's actually something we want
46:28 - because it can it will get to the
46:31 - correct answer even faster
46:33 - so
46:35 - now we are just going to add to our
46:37 - error for all lights
46:40 - let's see error
46:42 - for all
46:44 - lights
46:45 - and i'm just going to plus equals error
46:49 - so we're just
46:50 - we're just collecting all the errors
46:52 - into the air for all lights so we're
46:54 - getting an air for each one of the walk
46:56 - for stop array and we're putting them
46:58 - all together because we want the
46:59 - collective error to get as close to zero
47:02 - as possible
47:04 - and then we're going to calculate the
47:05 - delta
47:07 - and that's going to be prediction
47:09 - minus goal prediction
47:12 - this is just how far away our prediction
47:14 - is from our goal prediction and it's
47:16 - considered the raw error this is the
47:18 - same as before
47:19 - and now we are going to calculate the
47:22 - weights
47:24 - now in our previous example the weights
47:26 - equaled um weights minus weight delta
47:29 - times alpha and we're going to basically
47:31 - have the same
47:33 - same thing here but we're going to
47:34 - instead of creating the weight delta
47:36 - variable if you remember from before
47:38 - we're going to kind of put it in line
47:40 - and it's going to be
47:42 - weights
47:44 - minus
47:47 - alpha
47:49 - oh and we need a parenthesis here
47:56 - times
47:58 - input so before the weight delta was
48:01 - input times delta
48:03 - and instead of making that weight delta
48:05 - variable
48:08 - we're going to put it right in line so
48:09 - weights equals weights minus alpha times
48:11 - input times delta so that's the same as
48:13 - before
48:14 - and now
48:18 - we're going to just print out some data
48:19 - so this is actually the whole code and
48:22 - so i'm going to print out some data here
48:24 - and then we'll see what happens so we're
48:26 - going to do
48:29 - we're going to print the prediction
48:35 - so we want to see the prediction
48:37 - variable and i'll just do the string of
48:39 - prediction
48:42 - now i'm going to use some copying and
48:43 - pasting here
48:45 - the prediction will do for each of that
48:47 - iteration but the next two will put in
48:50 - to this next
48:52 - loop let's see
48:55 - if that lines up right
49:02 - oh right here okay thanks i appreciate
49:05 - that
49:05 - okay so now we have um
49:09 - weights
49:11 - and weights
49:14 - and then
49:17 - we'll print out the error
49:22 - and the error
49:25 - okay
49:26 - so we're about to run this code and see
49:27 - what happens but before we run the code
49:29 - i'm going to go back to
49:32 - the slides
49:34 - okay so this diagram
49:36 - demonstrates what our network looks like
49:39 - so we have three inputs
49:41 - and then we pass them through these
49:42 - weights
49:43 - and then we're going to get an output
49:45 - which is the walk versus opt
49:47 - whether the the output is going to show
49:50 - whether those inputs mean to walk or
49:53 - stop so that's the whole point of our
49:55 - network we're trying to learn from do
49:57 - these inputs of the light mean to walk
50:00 - or stop so that's what we're going to
50:03 - do here
50:04 - we go i'll just run this code
50:08 - and it's going to take a little bit but
50:10 - we should see
50:11 - if i didn't make any spelling mistakes
50:13 - we should see some stuff appearing at
50:14 - the bottom here which is the data
50:19 - okay so you'll see at the fir first the
50:23 - the prediction doesn't really correspond
50:25 - to anything at first and the weights are
50:27 - going to be pretty similar to the
50:28 - weights we passed in at the very
50:29 - beginning
50:31 - so
50:31 - you can see here here are the weights so
50:33 - we have 0.6 0.4 negative 0.2 that's
50:36 - pretty close to the weights we started
50:38 - with right here but if we keep going
50:41 - down remember we did 40 iterations so
50:43 - every time it gets the air we're to the
50:46 - next iteration so the air starts at
50:48 - point three we're trying to get as close
50:50 - as possible to zero
50:51 - so if we just keep going down we're not
50:54 - going to look at every single iteration
50:56 - we're going to just kind of go down to
50:57 - the bottom and see
51:00 - what we've gotten to
51:02 - okay so here's the the last
51:05 - the last thing that we want to look at
51:07 - and if you look at the prediction um
51:10 - this number is very close to zero then
51:12 - the second one is close to one then we
51:14 - have one close to zero one one zero so
51:17 - if you remember one
51:19 - this actually is the same as
51:24 - our stop first walk so we have the stop
51:26 - is zero one zero one one zero and that's
51:29 - what the prediction ended up being close
51:31 - to zero one zero one one zero
51:34 - um now that's not necessarily super
51:36 - exciting because we actually gave gave
51:38 - the network that information so of
51:40 - course it's going to be able to figure
51:41 - that out that information because we put
51:43 - it right in the code uh another thing i
51:46 - just kind of showed you first was the
51:48 - error
51:49 - this is a number very close to zero it
51:51 - starts with a three point but then when
51:52 - you see
51:53 - we this is just like a
51:56 - an exponent that makes it really it's a
51:58 - really small number very close to zero
52:00 - so and that's the whole point of our
52:02 - our algorithm is trying to get the air
52:04 - to zero so what's really important to
52:07 - look at is the weights
52:09 - so
52:10 - if you see the weights these weights
52:12 - correspond to each light so the first
52:14 - light the first weight is close to zero
52:17 - the second light is close to one and the
52:19 - third white is close to zero so one
52:22 - means it's perfectly correlated and zero
52:26 - means it's not correlated at all at all
52:28 - and if you remember from our diagram
52:31 - the middle light was perfectly
52:33 - correlated on whether you should walk or
52:35 - stop and the the right and left lights
52:37 - were not correlated at all so there was
52:40 - no point in the code that we gave the
52:43 - program that information
52:45 - but somehow it was able to learn that
52:48 - the middle light was perfectly
52:50 - correlated and the outer lights were not
52:52 - correlated at all now in this example it
52:55 - was something that you can easily figure
52:57 - out just by looking at the data which
52:59 - one's correlated but you can just kind
53:01 - of imagine in a more complex data set
53:05 - you may not be able to figure out
53:07 - how the data is correlated just by
53:09 - looking at it so you use the exact same
53:14 - type of ideas to make an even more
53:17 - complex neural network and so you can
53:19 - actually learn things that that's not
53:22 - that aren't as easy to just learn just
53:24 - by looking at the data
53:26 - so
53:28 - um
53:29 - [Music]
53:30 - our network like i said our network
53:32 - correctly identified the middle light by
53:35 - analyzing the final weight positions of
53:38 - the network so
53:40 - to like think about how it identified
53:42 - the correlation it was in the process of
53:45 - gradient descent
53:46 - each training example either is going to
53:48 - assert upward pressure or downward
53:51 - pressure on the weights so on average
53:53 - there was more upward pressure for the
53:55 - middle weight and more more downward
53:56 - pressure on the outer weights so they
53:59 - got to the correct numbers
54:01 - so
54:02 - this is pretty cool and now i'm going to
54:05 - show you something even a little a
54:07 - little more complex
54:09 - so the next part of the talk
54:12 - instead of live coding i'm going to show
54:13 - you some code that's already written
54:15 - that's pretty similar to the code we
54:17 - already had but i'm going to introduce a
54:19 - few more concepts
54:21 - first of all i'm going to tell you that
54:22 - some of the concepts could even be a
54:24 - whole talk to themselves so i'm going to
54:26 - do my best to like quickly overview the
54:28 - concepts but some of the things you'll
54:30 - just kind of get a taste of so you can
54:32 - learn more about on your own later
54:35 - but the reason why i'm going to show you
54:37 - one more program is to talk about the
54:40 - deep part of deep learning so nothing
54:43 - i've showed you so far
54:45 - is actually deep so let me deep refers
54:50 - to one or more hidden layers between the
54:53 - input and the output layers of a neural
54:55 - network so if i go to
54:59 - my code here so our input layer is right
55:02 - here our output layer is right there
55:03 - there's nothing in between
55:05 - but if i look we look at this example
55:07 - this is a true deep neural network
55:10 - because we have our input layer that
55:12 - looks the same there's three on the
55:13 - input and one on the output for layer
55:16 - two but now there's this middle layer
55:18 - which is a hidden layer and since
55:21 - there's one or more hidden layers this
55:23 - is a deep neural network
55:25 - so you may be wondering why would you
55:27 - even need a hidden layer if our other
55:29 - network was able to correctly predict
55:31 - the correlation between the lights
55:33 - because we're going to use the same
55:34 - example from before
55:35 - so if you remember the data the middle
55:38 - light was perfectly correlated with
55:40 - whether you should walk or stop
55:42 - but if the middle light wasn't so
55:44 - perfectly correlated our network would
55:46 - have had a much harder time figuring out
55:49 - how the how the input was related to the
55:52 - output so like for instance if our
55:54 - training data looked like this
55:56 - our other network wouldn't have worked
55:58 - so well if you look at this so the the
56:01 - numbers in red
56:02 - are the the light information whether
56:04 - the lights are on or off and then the
56:06 - numbers in black are the stop or walk
56:09 - information so you can see this first
56:12 - the first three up here means to walk
56:14 - and we have walk stop stop and one thing
56:17 - you'll notice is that the output data is
56:19 - not perfectly correlated with any of the
56:22 - rows
56:23 - so
56:24 - this is where you need to have a
56:27 - hidden layer
56:28 - um what is because since our input data
56:31 - set does not correlate with our
56:33 - correlate with our output data set we're
56:35 - going to use our input data set to
56:37 - create an intermediate data set that
56:40 - does have correlation with our output
56:42 - and this intermediate data set is our
56:45 - hidden layer it's almost going to be
56:47 - like two neural networks it's going to
56:49 - be
56:50 - from layer 0 to layer 1 is going to be
56:52 - one network that runs just like before
56:55 - and then the so
56:57 - the output to the layer 0 to layer 1
57:00 - network will become the input to the
57:03 - layer 1 to layer 2 network
57:05 - so let me
57:07 - show you
57:08 - okay
57:10 - so can you guys see this all right
57:12 - even in the back okay so
57:14 - a lot of this is going to look pretty
57:15 - similar here we have we're importing
57:18 - numpy as mp
57:20 - and then this line right here
57:22 - just make sure we're going to be using
57:24 - random numbers in this code and that
57:27 - line just makes sure the rand.seed makes
57:29 - sure every time we get random numbers
57:31 - it's the same random numbers so a lot of
57:34 - times when you're dealing with neural
57:36 - networks you're using random numbers for
57:37 - the weights but to make sure the output
57:40 - is always the same
57:42 - even though you're using random numbers
57:43 - you can make sure that every time the
57:45 - code runs the random numbers were the
57:47 - same random numbers that you used last
57:49 - time you ran the code and it helps for
57:51 - like comparing uh to make sure the code
57:54 - the results are exactly the same
57:58 - so this is called a this is an
58:01 - activation function it's the relu
58:02 - function
58:04 - we're going to pass our data into it
58:06 - it's going to return x or return it's
58:08 - going to return what the data we passed
58:11 - in if x is more than zero
58:13 - or return zero otherwise
58:16 - now
58:16 - this is a it's called an activation
58:19 - function because it activates some input
58:22 - by returning the input and deactivates
58:25 - other input by returning zero
58:28 - this code is going to show a three layer
58:30 - neural network
58:31 - and by turning any middle network
58:34 - off whenever it would be negative we
58:36 - allow the network to sometimes
58:39 - have the correlation from various inputs
58:41 - and sometimes not have the correlation
58:44 - so it's impossible for a two layer
58:45 - network to sometimes allow this input to
58:48 - correlate and sometimes not allow it to
58:50 - imp correlate so it adds more power to
58:53 - three layer networks so this sometimes
58:56 - correlation is important for things to
58:58 - work correctly and this is one of the
59:00 - areas that like i was saying could
59:02 - get a whole talk to itself as activation
59:04 - functions and how they work and why
59:06 - but i'm just going to kind of leave it
59:08 - at that for now and
59:10 - that the activation is functions
59:13 - important to make it sometimes correlate
59:14 - and sometimes not correlate the the relu
59:17 - two derive function is next
59:20 - and
59:21 - uh this returns one when the output is
59:24 - more than zero or zero otherwise and
59:26 - that's the slope of a reload function or
59:29 - its derivative and you'll see later how
59:31 - that's important
59:34 - so the next two this is just inputting
59:36 - the data just like we did before
59:38 - we do have the dot t that's a numpy
59:41 - thing it's the transpose operator and it
59:44 - just um changes so you have a matrix
59:47 - which is going to have like uh almost
59:49 - like an x and y coordinates like the so
59:52 - it's a if it's a three by four matrix
59:54 - the transpose operator would make it a
59:55 - four by three or if it's a one by four
59:57 - it'll become a
59:58 - four by one and when it's when you're
60:00 - multiplying a matrix and a vector it
60:03 - matters
60:04 - um which kind of direction the the
60:07 - matrix is if that makes sense
60:10 - and so
60:12 - the alpha is same before again sometimes
60:16 - you have to kind of try out different
60:17 - things to figure out what it's going to
60:18 - work
60:19 - and the hidden size is four so if we go
60:22 - back to our diagram you can see that our
60:24 - middle layer has four nodes so that's
60:27 - what this hidden size equals four is
60:29 - there's four nodes
60:30 - and then here is our weights so
60:36 - okay let me
60:39 - get back to that code here i
60:40 - accidentally did the
60:43 - keyboard the mouse shortcut for going
60:45 - back so if we go to our weights here
60:48 - before this is where we had the audience
60:50 - participation and i had people
60:52 - shout out different weights but now
60:54 - we're going to get truly random weights
60:56 - because we have a lot more weights this
60:59 - is going to be a matrix of weights and
61:02 - the size of the matrix like the matrix
61:04 - is similar to this this is a matrix
61:06 - right here up top a 3 by 4 matrix and
61:09 - this is going to be a three by four
61:11 - matrix and this is going to be a four by
61:13 - one matrix and each number in that
61:16 - matrix is going to be between negative
61:18 - one and zero
61:19 - and the reason why we have three by four
61:21 - is because you can see there's three
61:23 - nodes by four nodes and then we have
61:24 - four nodes by one node so the size of
61:27 - the weights has to correspond to the
61:30 - input and output layers because you can
61:32 - kind of you can see that this has
61:34 - there's a weight between layer one and
61:36 - layer zero and there's weights between
61:37 - layer one and layer two and the size of
61:39 - the weights have to correspond to those
61:41 - layers
61:42 - so
61:44 - now
61:45 - we're gonna you can see we're gonna
61:46 - iterate 60 times
61:49 - and then we're gonna keep track of the
61:50 - air just like before we were keeping
61:52 - track of the error
61:53 - it's just called something different so
61:56 - in this code there's gonna be a lot of
61:58 - things that are pretty much just like
61:59 - the code that i just did for the live
62:01 - coding but the names are slightly
62:03 - different
62:05 - so
62:06 - layer 0 here
62:09 - this is the same as the input from
62:11 - before so we're just getting the first
62:14 - street light which would be
62:16 - this matrix right here this vector right
62:18 - here
62:19 - and
62:20 - this is just
62:22 - a kind of fancy way of saying that we're
62:23 - going to get a
62:25 - nested mate a nested array so instead of
62:27 - an
62:28 - instead of an array of one zero one it
62:31 - will be an array of an array of one zero
62:33 - one so they'll just be like and if you
62:34 - output it there'd be an extra array on
62:36 - the sides
62:37 - um
62:38 - there you go so
62:42 - the re so that that just makes the kind
62:44 - of math work to have an array of arrays
62:46 - but it's just getting the first array
62:48 - from the streetlights
62:50 - and then layer one here
62:52 - is going to be the same as the input and
62:55 - the same as the prediction from before
62:57 - remember the other network we had one
62:59 - input and that input had one prediction
63:03 - but in this neural network it's
63:05 - basically two neural networks uh there's
63:08 - two two networks there's layer one to
63:09 - layer there's layer zero to layer one
63:11 - and there's layer one to layer two so
63:14 - the
63:14 - prediction or output
63:16 - from layer 0 to layer 1 becomes the
63:19 - input from layer 1 to layer 2.
63:22 - so
63:23 - this layer 1 is the input and
63:26 - the prediction so
63:28 - just like before we're always just mult
63:31 - to get a prediction you always multiply
63:34 - the input times weights so that's what
63:37 - this is it's another numpy function to
63:39 - just multiply the things that you pass
63:41 - into that function layer zero and weight
63:43 - zero one that's what you're always doing
63:45 - for your prediction multiplying inputs
63:47 - times weights
63:48 - then we run it by the relu function
63:50 - which will selectively activate like we
63:53 - talked about before layer two is our
63:55 - output layer
63:57 - and
63:58 - which is the same as our prediction from
64:00 - before and again we're just multiplying
64:02 - the input times the weight so the input
64:04 - is layer one
64:06 - and the weight is weights one two so
64:09 - this is the same thing we've been doing
64:10 - all along to get the prediction from the
64:12 - first stock from the very first code
64:14 - example
64:15 - this was the neural network we had a
64:17 - neural network function that just
64:18 - multiplied weights times input and
64:20 - that's exactly what we're doing here
64:23 - so now we have to get our error
64:28 - so our error is going to we have our
64:30 - layer 2 error here and we're going to
64:32 - calculate that pretty much just like
64:34 - before but we have different names so
64:38 - before it was the air was prediction
64:40 - minus goal prediction squared uh but now
64:44 - the prediction is that layer two
64:45 - variable the goal prediction is the
64:48 - getting something from the walk versus
64:49 - stop array and then we square it so air
64:52 - is always prediction my school
64:54 - prediction squared and then
64:57 - we're going to do a plus equal so we're
64:59 - going to add up all the errors together
65:02 - into our layer 2
65:05 - error variable
65:07 - okay so the next thing is our delta this
65:11 - is basically exactly like the the other
65:13 - example
65:14 - um so
65:16 - this
65:17 - we have the layer two
65:19 - so this is just
65:20 - input or wait a second yeah input minus
65:24 - goal prediction prediction minus goal
65:26 - prediction so layer two minus walk for
65:28 - stop is the same as prediction mice goal
65:30 - prediction
65:31 - and remember the goal here is um air
65:35 - attribution it's all about figuring out
65:37 - how much each weight contributed to the
65:39 - final error
65:40 - so in our first two layer neural network
65:42 - we calculated a a delta value a delta
65:45 - variable which told us how much higher
65:48 - or lower the output prediction is
65:50 - supposed to be so we're calculating that
65:52 - the same way
65:53 - so now we have how much that we that
65:57 - though we want the final prediction to
65:58 - move up or down that's the delta and now
66:01 - we need to figure out how much we want
66:02 - each middle layer layer one node to move
66:05 - up or down so
66:07 - one thing that's interesting about this
66:10 - is if we go to our diagram we can see
66:12 - there's an order layer 0 layer 1 layer
66:15 - 2.
66:16 - but in our code
66:18 - we're gonna calculate layer two first
66:21 - and then
66:22 - layer one so we're not calculating it in
66:25 - the same order as you would normally
66:28 - think going through the going through
66:30 - the layers
66:31 - this is another like really big concept
66:33 - called back propagation let me it's a
66:38 - let me zoom out so you can see all the
66:40 - notes i have here so
66:42 - in back propagation the layer 2 delta is
66:45 - back propagated to the layer one delta
66:48 - and this is going to give us a weighting
66:50 - of how much each weight contributed to
66:53 - that
66:54 - error
66:57 - that's supposed to be in the word error
66:59 - oh it is it's just off the side of the
67:01 - screen okay so
67:04 - we need some way for our layer two
67:08 - weights to impact layer one because we
67:10 - need to update everything so that's what
67:12 - back propagation is
67:14 - and so this is another pretty big
67:16 - concept i mean back propagation
67:19 - could be a whole talk of its own but
67:21 - it's all about taking uh information
67:24 - from later on in the the network and
67:27 - passing it back
67:28 - to uh to the earlier in the network so
67:30 - it can use that information when it's
67:32 - making the calculations
67:34 - and
67:36 - let's see
67:37 - if
67:38 - there's one more thing we need to talk
67:39 - about so this is for the layer one delta
67:43 - here we're just using this dot
67:46 - function to multiply layer two delta by
67:49 - the weights and we have the transpose
67:51 - operator
67:52 - but then the final thing
67:54 - is this relu to derive function
67:56 - so if the relu set the output to layer
67:59 - one to the layer one node to be zero
68:02 - then it didn't contribute to the air at
68:03 - all so when this was true we should also
68:07 - set the delta of that node to be zero
68:09 - and multiplying each layer one node by
68:13 - the relu two derived function
68:14 - accomplishes this the relu two derived
68:17 - is either a one or a zero depending on
68:20 - whether the layer one value was
68:23 - more than zero or not
68:26 - so now we're going to update the weights
68:30 - and we update the weights
68:31 - just like we did before
68:34 - so
68:35 - for each weight we are going to multiply
68:37 - its input value by its output delta
68:42 - and so it's just weights one two equals
68:45 - weights one two
68:46 - uh
68:48 - minus
68:49 - wait i'm thinking yeah yeah weights one
68:51 - two equals weights one two minus alpha
68:54 - times and then so this is just how we
68:57 - calculate the weight before where you
68:59 - multiply
69:00 - the
69:01 - input by the delta
69:03 - and then you multiply that by the alpha
69:06 - and then that's that's just how you
69:07 - update the weights
69:09 - so
69:11 - again this code is all all online so you
69:14 - can kind of review it later like some of
69:16 - this stuff it kind of takes going
69:17 - through a few times to really see how it
69:20 - all goes together but you'll see that
69:22 - we're updating the weights in the exact
69:23 - same way as we were in the other live
69:26 - coding sessions that we did
69:28 - so
69:30 - down here
69:34 - this is going to just make sure that
69:36 - we're only going to
69:38 - um print
69:39 - this information uh just every once in a
69:42 - while so every few iterations so when we
69:45 - run when we run this it's only going to
69:47 - give the information
69:48 - every few iterations so i'm just going
69:50 - to run this right now
69:52 - and
69:53 - i think it has that from last time i ran
69:55 - it
69:56 - but if the main thing to
69:59 - look at here
70:01 - is that
70:02 - if we go to the bottom
70:03 - [Music]
70:06 - our error
70:08 - gets very close to zero so that's the
70:11 - whole point of learning the learning is
70:13 - trying to get our air as close as
70:14 - possible to zero
70:16 - now if you remember before that that
70:18 - meant that our weights were correctly
70:20 - we're making a correct prediction in
70:22 - this case it's it's the same
70:24 - now
70:25 - i'm kind of getting close to to the
70:28 - the conclusion of kind of why all this
70:30 - matters so what i mean there's probably
70:33 - we could probably talk about this code
70:35 - for a few more hours to explain it every
70:37 - little part like the back propagation
70:39 - and and the weights but i kind of want
70:41 - to do kind of step back a little bit and
70:43 - give kind of an overview of the point of
70:47 - why we create intermediate data sets
70:49 - that have correlation
70:51 - so i go back over here
70:54 - okay so consider this image of this
70:57 - handwritten four
70:59 - that's kind of blown up it's supposed to
71:01 - be supposed to look like the number four
71:03 - and uh if you if we had a data set of a
71:06 - bunch of images of handwritten digits
71:09 - and they were all labeled like zero to
71:11 - nine um if we wanted to train a neural
71:14 - network to take the pixel values from
71:18 - this image and predict that this was a
71:21 - four
71:22 - our two layer neural network would have
71:24 - never been able to do it so they're just
71:26 - there's no
71:28 - individual pixel in this
71:30 - and this here so there's no individual
71:32 - pixel that perfectly correlates with
71:35 - whether it's a four or not
71:37 - just like in our second example that i
71:40 - just went over there was no
71:42 - light combination that perfectly
71:44 - correlated with walk or stop
71:46 - so there's
71:47 - there's only different configurations of
71:49 - pixels that correlate with whether or
71:50 - not this is a four so this is the
71:53 - essence of deep learning if i go to the
71:55 - next slide you'll see how this would
71:57 - work with kind of the multiple layers
71:59 - where deep learning is all about
72:01 - creating intermediate data sets or
72:03 - layers where each node in an
72:05 - intermediate layer
72:06 - represents the presence or absence of a
72:09 - different configuration of inputs
72:12 - so
72:13 - in this way no pixel has to perfectly
72:15 - correlate with whether or not it's a
72:18 - four or not instead we have these middle
72:20 - layers that
72:23 - they attempt to identify different
72:25 - configurations of pixels that may or may
72:27 - not correlate with the four so we go
72:29 - through a lot of different layers and
72:30 - finally
72:32 - it finds which layer correlates if it
72:34 - finds a configuration of pixels that
72:36 - correlates with whether it's a four not
72:38 - and then it outputs that this is a four
72:42 - so the presence of many four like
72:45 - configurations
72:46 - would then give the final layer the
72:48 - information or correlation it needs to
72:51 - correctly predict whether the digit is a
72:54 - four
72:55 - so we can take our three layer network
72:58 - and continue to stack
73:00 - many many more layers
73:02 - some networks even have hundreds of
73:04 - layers
73:05 - and each neuron plays its part in
73:07 - detecting different configurations of
73:09 - input data so you could start with like
73:12 - we started with these uh
73:14 - input of three layers but the hidden
73:15 - layers could get bigger and bigger and
73:17 - bigger
73:18 - until it finally has what it needs to
73:20 - correctly predict the output layer
73:23 - so i'm hoping that just kind of seeing
73:25 - this for example helps you kind of see
73:29 - how how adding additional layers can
73:32 - help you
73:34 - get the final prediction where you don't
73:35 - need just like our stop light example
73:38 - you don't need the perfect correlation
73:41 - to figure out the final answer you can
73:43 - make other layers and other layers after
73:45 - that that finally get the correlation to
73:47 - get the final
73:49 - answer so i'm getting kind of i'm cl to
73:51 - the end of the talk here so i'm hoping
73:54 - you just by this talk you've got a good
73:57 - introduction to deep learning and neural
73:58 - networks and you saw by the examples how
74:01 - we were able to write a neural network
74:03 - they actually learned what we're trying
74:05 - to learn
74:06 - and then i also hope you unders you
74:08 - start to understand how hidden layers
74:11 - can can give your neural network even
74:14 - extra power to figure out things it
74:15 - would not have normally
74:17 - figured out
74:18 - so yeah thank you for coming to the talk
74:21 - and i'm open for questions right now
74:24 - [Applause]

Cleaned transcript:

my talk is called theory of neuro networks and the alternate title would be deep learning without frameworks because i'm going to be um talking i'm going to be talking about the basics of deep learning and neural networks but i'm not going to be using any framework so i'm not going to be using keras or tensorflow and the whole point of this talk is to hopefully help you guys understand how deep learning and neural networks work kind of under the hood because usually when you're working with these things you're going to use a framework that kind of hides a lot of the implementation so hopefully after this talk even if you're you're completely new to deep learning and neural networks you'll have a better understanding of how how those things work and so the first part of my talk i'm going to be going over some of the basic concepts and showing how things work and then i have a second part after lunch where i'm going to bring it all together and do some live coding and show you how to solve an actual problem using just just straight python and showing you how to create a neural network from scratch so first a little bit about me let me get my there we go so my name is beau carnes like like you heard and i am from the united states i'm from the state of michigan and i currently work at freecodecamp.org it's a nonprofit with the goal of providing a free curriculum for people learning software development and so anybody can go on and learn for free so i like a new thing that we're doing over at free code camp is we've translated the curriculum into five different world languages and we just use like this google automatic translate google automatic translator but we are an open source community so we're trying to get people to go in their native speakers and fix the automatic translations and russian is one of the languages so if anybody's interested in trying to help out this open source community and you speak russian you can go in and help with the translation besides that i also have made some courses for this book publishing company called manning publications i saw on the the booth out there i saw some manning publications books and i've created some video courses so i have one called algorithms in motion and then grocking deep learning in motion so the the second one is kind of the grocking deep learning emotion that's what this talk is based off of and the whole course um i mean the whole video course is actually based on this great book by another author named andrew trask so so this i would highly recommend this book for anybody trying to learn deep learning so my my whole my video course is based on the book and this talk is based on the course so let's kind of get into it here um oh first of all i wanted to tell you that i do so this is my personal website karns.cc and then i made a site a page called moscow so all my my slides are on there and all the code that i'm going to be going over during this part of the talk and the second part of the talk so you don't necessarily need to look at it now but if you ever want to refer to that you can and so i'm going to be going into more detail into code than some of the previous speakers so sometimes it's good to see the actual code so deep learning verse versus machine learning so machine deep learning is actually a subset of machine learning so if we go here um you can see that we have machine learning and the sub says deep learning and then the subset of that is artificial intelligence so um machine learning is kind of like what it sounds like machines or computers are are trying to learn something that they were not explicitly programmed for so machines observe a pattern and attempt to imitate it in some way so machine learning is often attempting to take an input data set and transform it into an output data set so let me show you some examples with an input dataset and an output data set so these are all examples that you would use machine learning or even deep learning to try to figure out so if you have pictures of a cat you have the input is pixels and then the output would be a presence or absence of a cat so you input the pixels and the output of the algorithm would be whether or not there's a cat there and the next one would be you could input movies you liked into the algorithm and then on output movies you may like so these are just some different possible use cases you would input words and then would output whether those words indicated that the person was happy or sad and then you would input weather sensor data and the output would be the probability of rain so these are all supervised machine learning tasks and the machine learning algorithm is attempting to imitate the pattern between two data sets in such a way that it can take use one data set to predict the other data set so i'm going to give you a summary of how machine learnings would do that and how a deep learning algorithm would do that let's say you have this data set of pictures and a lot of the pictures are pictures of cats but then some of the pictures are not cats so how would work is you would input all these pictures into into your algorithm and you would also tell it which pictures were cats and which pictures are not cats so um the supervised learning algorithm is going to extract patterns from that data set after it's learned the patterns we can show it a picture and algorithm will hopefully tell us whether the picture is of a cat or not so it takes all the information you give it and then hopefully later you can get a picture of a cat that it hasn't seen before and will be able to figure out whether it's a cat or not so deep learning is a subset of methods in the machine learning toolbox that uses artificial neural networks so let's get into more details about deep learning oh here would be an example is this a cat yes it's a cat so all deep learning and machine algorithm learning algorithms are going to be classified as either supervised or unsupervised learning uh so we take what we know and we transform into what we want to know so that would be a supervised learning we have something that we know to be true and we transform it into what we want to know so like an example would be we know that we know that this picture we know that this is this is a picture of a cat or maybe not a cat and we put into the algorithm to find out what we want to know like whether really it is a picture of a cat and so the um the goal of a supervised learning is is to learn a function that given a sample of data and the desired outputs best approximates the relationship between the input and the output in the data supervised learning is done using prior knowledge of what the output values for a sample should be like in the cat example we know that the that the output is either going to be cat or not cat or maybe you're trying to identify handwriting and you know it's going to be one of the letters of the alphabet you know you know the alphabet you know the the you already know what the output could be for supervised learning so for unsupervised machine learning we don't know what the output could be we're just import and putting a list of data points and we put into the algorithm and then out comes a list of cluster labels so we don't know for sure so the algorithm is trying to figure out and sort the data but it doesn't know what the categories that it's trying to sort into so unsupervised learning groups your data the goal is to infer a natural structure present within a set of data points and we put in a we put in a list of data points and we get the cluster labels out so basically an unsupervised learning algorithm says find patterns in this data and tell me about them so it's just trying to figure out some patterns let's see that was slightly out of order we'll get back to that slide so in this so this would be kind of an example of unsupervised data where you have all this all these data points and your algorithm groups them into these different groups and it basically so the algorithm basically says hey hey data scientist i found some structure in your data it looks like here are some groups in your data here are the groups and then it's up to the data scientist or the programmer to figure out what the groups are it doesn't tell you what the groups are you just kind of have to figure that out after you see the groups so the next thing i want to talk about is parametric versus nonparametric learning and you know pretty soon we'll get to some actual code examples so this will start to make more sense but for for this um you can just kind of imagine a cloud so with uh so this is our machine learning cloud there's two knobs we have supervised an unsupervised knob and a parametric and nonparametric so there's you can kind of switch your algorithm to be either supervised or unsupervised parametric or nonparametric supervision is about the type of pattern being learned and parametricism is the way the learning is stored so the two types parametric and nonparametric parametric is with a fixed number of parameters and nonparametric is possibly an infinite number of parameters so here's kind of an oversimplification a parametric is trial and error and nonparametric is about counting and probability so you could have you're just kind of counting and you don't know how many parameters there's going to be a nonparametric so let's do an example i'm going to go back to this slide so let's say you're trying to figure out you have this square and you're trying to figure out where it should go in this little toy here now again this is a pretty simple example but a some people like a baby may may just jam it into every hole until they find out where it fits so that's kind of like parametric learning now nonparametric learning would be you're counting the sides of the square and counting the sides of the the hole and then you kind of figure out where it'll go on that one so a parametric models tend to use trial and error like just trying everything a nonparametric tends to count to figure out where it should go but most algorithms are supervised parametric learning so from the rest of this talk and the next talk i'm gonna be talking about supervised parametric learning and giving you some examples to go along with that so this is basically trial and error using knobs and so there are three steps uh step one will be predict so um the three steps to supervise parametric learning is the first step is predict so for the predict step you take all your data in this case we're trying to predict if a sports team is going to win a game or not so we input this data like number of toes on the team number of players number of fans we put it into our algorithm which is our machine and then out comes a prediction so in this case our prediction is saying that there's a 98 chance given this data that our team is going to win so step two is going to be compared to the truth pattern so we have our prediction which is 98 chance that the team's gonna win and our truth pattern is we found out that in real life they lost the game so zero percent chance of that they're gonna win so we thought we our original prediction was 98 chance they're gonna win but actually they lost uh so we after and so then after we compare to the truth pattern this is where we are going to learn the pattern so learning um when we're going to just um take the information about how much we were wrong so we look at how much we missed by which is 98 and we also look at the input data at the time of the prediction and then we're going to turn the knobs so these knobs would be weights so with the knob of the wind loss the home away number of toes number of fans so based on all the information we're going to adjust the weights so hopefully the next time the next iteration of the algorithm we can uh be a little closer instead of 98 percent chance we're going to win that's going to that number is going to be lower because we know that in reality they didn't win with that information now um so now that you kind of know the basic steps i know it's kind of hard to understand all that how all that fits together but now i'm going to get into some actual code so hopefully it starts to make sense what i was talking about with the predict compare learn so this is a simple neural network and a neural network is one or more weights which we can multiply by our input data to make a prediction so it's all about trying to make a prediction on on the correct answer based on those weights so in the example that i gave the prediction will be whether or not the team is going to win a game based on the the input debt and the weights so here are our first few lines of code here we i start by just hard coding a weight number now generally your weights are going to start as random numbers which we'll see in the live coding section later but um here it's just a hardcoded so you can kind of see how it works better and we have our neural network function so we can see that our neural network function we just pass in the input and the weights and then it's going to set the prediction to input times weights and it returns the prediction so it's all about just multiplying inputs times weights so this diagram would kind of show to you how it works you input the information here multiplies by the weight of 0.1 and the prediction is the percent chance that the team is going to win so this next section of code we just talked about this part right here but here is our data so we have this array of number of toes this would be like average number of toes for each of the first four games in in the season so it's kind of a silly example but just kind of bear with me here our input is just going to be the first number because we can only run on one number at a time so we're inputting just as 8.5 and then our prediction we just call the neural network function which multiplies the input times the weight and then we get out the predictions so it inputs so we have the 8.5 times the weight so our prediction is that it's going there's an 85 chance that our team is going to win so we can see that the prediction is the input times the weight here so let's talk more about what the input data is so an input data is a number recorded in the real world somewhere so it's usually something that's easily knowable like in the example average number of toes it could be today's temperature it could be yesterday's stock price um so it's just some we're going to input some data that we can see in the real world and then we are going to make a prediction so a prediction is what the neural network tells us given our input data so like given today's temperature we i think the network thinks there's a zero percent chance that people are going to wear swimsuits today or maybe given yesterday's stock price there's uh today's stock pro stock price is going to be 101 0.3 or something like that or given this many number of toes this team is going to win the game so that would be the the prediction and then the network learns through trial and error so in that example it was just it only went through at one time so in a real neural network which we will get to and i'll show you a network with a lot of iterations every time it runs through the code it updates the weight and updates the error and it keeps running through the code over and over to to learn so it first it's going to try to make a prediction then it sees whether it was too high or too low and then finally it changes the weight up or down to predict more accurately the next time it sees the same input so so far we've seen um we've seen the network making a prediction so remember the three steps were prediction and then comparing and then learning so we still need to i still need to show you how you would compare it to the truth pattern and learn and i'll get to that in a minute but i'm going to show you uh one another example before i get to the compare and learn so here i'm going to show you a neural network with multiple inputs and outputs so the first example there was just one input and one output in this example we're going to have three inputs so be like the number of toes per player the win loss record number of fans would be the inputs we're inputting all those things into the network and we're outputting the percentage of hurt players whether or not the team is going to win and whether or not the team is sad so this is just kind of to show that i started with showing a very simple network but they can get more complex you can have any number of inputs you want and you can be trying to predict multiple things you don't just have to predict one thing for the inputs you can predict multiple things so here's all the code now i'm going to kind of zoom in so you can see the different sections so first we're going to talk about the weights so before we just inputted one number for the weight but now that we have multiple inputs and multiple predictions we have to have a matrix so a matrix is just an array of arrays basically so you can see the first line is the no is the the weights for number of people hurt second line number of people that if we won or lost and then percentage of sad and then for kind of each column we have the weights for the number of toes what the win ratio is and the number of fans so then our neural network looks almost exactly the same as before remember before we just multiply the input times the weights and that's what we're doing now except we have this fancy function of vector matrix multiplication before we are just multiplying one number by another number now we're multiplying a a vector which is an array by a matrix which is an array of arrays and so we need an extra some extra code to make it so you can multiply a vector by a matrix and we'll see that in a little bit here so this is our information we're passing our this is our input data for the first four games but for our input we're just going to take the first number of each array so we're just going to do kind of one game at a time and so here's an example of what it would look like for we're passing in our input data and then we have all these predictions that kind of come out of the network so i talked to talk about the vector matrix multiplication function i'm not going to go into this and great detail it's just a way to multiply a vector by a matrix but in real and this function kind of uses this weighted sum function but in real life you're probably never going to write these functions yourself because most people are going to use numpy so numpy gives you just extra extra functions to do math like multiplying matrices and func matrices and vectors so if you this line right here from our neural network and numpy is going to be like this and when i do my live coding i'm actually going to use numpy so you'll see how that works so this dot dot here that's a numpy function and this just means multiply the input by weights and then we get the answer into our prediction here okay so at the end of our network we just set our prediction to equal the the return value of our neural network and we can print our predictions so you can see here here are predictions that we printed out based on this network here so if we go over here you can kind of see everything on the screen at once and then down here what you can see the prediction so that first and you can see how the math is calculated and so our her prediction was 0.55 so we think there's a probably 55 percent of players may be hurt a 98 chance of winning and a 97 chance that people are sad on the team so we can predict three things at once so the next thing i want to talk about is error and gradient descent so this green descent is used while training a machine learning model and it's an optimization algorithm that tweaks its parameters iteratively to minimize a given function to its local minimum so in other words it allows us to calculate both the direction and the amount that we should change our weights so we reduce our error so the whole point of our neural network is trying to get our error to zero so our gradient descent which we'll see the code later it's pretty simple it helps us figure out which direction we should move our weights and how much we should move our weight so when we calculate the error it goes down it's just a way to every time we run the the code every iteration we're going to use our great descent function to get our weights to the right spot so our error goes to zero so we talked about supervised parametric learning in the three steps we talked about so but so far you've only seen predict but now i'm going to show you how to do the second two steps the compare to truth pattern and learning the pattern so for this step two is compared to the truth pattern to measure the error we determine how far our prediction is from our goal prediction and then step three learn the pattern it takes our air and tells each weight how how it can change to reduce it so the whole point is trying to figure out which weights are impacting our air so we can reduce our error to zero and by time when our air is zero that shows us that our weights are accurate because there's no error so now i'm going to show you pressing this button here hopefully i'll change slides just a second oh wait okay so learning is adjusting our weight to reduce the error to zero so that's what we're trying to do to actually that's what our learning is actually doing so here is our one iteration of gradient descent now in a real example you could have like hundreds of iterations or thousands of iterations and when i get to the live coding i'll show you one with a lot of iterations but let's just take it one iteration at a time so first we get our weight and our alpha and our alpha is the simplest way to prevent over correcting our weight updates so sometimes the goal is to get our air to zero but sometimes instead of our air going down it will start going up so alpha is something that's going to help it to always go down and go in the right direction and you'll see later how that fits into our code it's kind of trial and error for what alpha you're going to use but it's usually a multiple of ten so you would start with maybe point zero one then you could try point one and then maybe try one and just try to see which one is gonna make your air go down instead of up so uh let's see and then our neural network is the same as before which is input times weight and here's the diagram so our input the data goes here goes through a weight and then we outcomes our prediction of our wind prediction so now we are going to um take we we're going to do our prediction step so we pass in our information you can see there's only a few things different here which are goal prediction and error so we didn't talk about air in our last code but that's very important because we're trying to get our air to zero so right here we see that we have our number of toes is 8.5 just like before our input is our number of toes now our goal prediction is our winner loss binary which is a one means that the number one means that they won the game so our goal prediction is one so we think that with an input of 8.5 the goal prediction is one that we they won the game so we do our prediction just like before with our neural network we multiply the input times the weight and then our error is going to equal prediction minus goal prediction square so this is prediction minus gold prediction is the raw error it's it's how much we predicted was going to happen minus what we thought was going to happen and then the reason why we square it it forces the raw error to be positive by multiplying by itself so a a negative prediction a negative error when it makes sense so when we square it's always positive and also has added benefit of making large areas larger and small errors smaller which actually helps our neural network to go quicker to the correct uh to the correct thing by squaring it like that so if we run our neural network we do the 8.5 times 0.1 we get the 0.85 but then our error ends up being 0.023 okay so then we have our delta so this is the compare step we're going to calculate our node delta and the delta is how much this node missed so that's the raw error prediction minus goal prediction so so we need to know how much the node missed so we can calculate we can kind of put it into our output so we know uh we know we missed by negative 0.15 because our prediction was 8.85 and our goal prediction was 1. so 0.85 minus 1 is negative 0.15 so weight delta this is kind of where this is where the gradient descent happens so you can see it's actually a pretty simple line of code it's how much this weight caused the network to miss so we want to find out we know that are we missed but how much do we miss we just use the weight delta which is this the input times delta gives us the weight delta and we're trying to figure out how much to update the weight because we're trying to change the weight on every iteration now this actually does three good things for us it helps us with stopping negative reversal and scaling so stopping it's the first effect on our pure error caused by multiplying it by our input so just imagine you're listening to music or you're trying you're playing music on your computer or you're trying to play music and you have speakers so you turn up the speakers all the way but music still isn't coming out because you forgot to hit play on your computer so this would kind of be an example of stopping where it's kind of addresses in our neural network so if our input is zero like you've got to explain your computer it will force the weight delta to also be zero so we don't learn when our input is zero because there's nothing there's nothing to learn and so moving it makes no difference then negative reversal um when our input is positive moving our weight upward makes the prediction move upward but if our input is negative then all of a sudden our weight changes directions now we only want our weight to go in one direction so when our input is negative then moving our way up makes the prediction go goes down we don't want that so multiplying our delta by our input will reverse the sign of our weight delta in the event that our input is negative so this is the negative reversal this ensures that our weight moves in the correct direction um scaling is just when we mol anytime you multiply things together it's just it's either going to get a lot bigger or a lot smaller if the number is less than zero so this is this is good because we want our we want our weight delta to we want big errors to be really big and small to be really small and alpha is going to help it so it doesn't go out of control so we don't want it to get too big that's why we have the alpha which we'll just about to talk about here but first we can see how the weight delta it kind of gets applied to the weight here of negative 1.25 okay so now we're getting toward the end of this we are going to update the weight so when it says weight minus equals that just means weight equals weight minus weight delta times alpha and so this this allows we're going to multiply the weight delta times the weight or we're going to do weight minus the weight delta and we multiply it by the alpha to control how fast the network learns because it can like i said can it can update weights too aggressively but the alpha is going to remember the alpha in this case was .01 so it makes it so the weight doesn't get updated as quick quickly and so it doesn't get too out of control so in this case the new weight is 0.11275 so we're actually getting toward the end of our the first part of the talk i know this is kind of a lot of information all at once especially if you're kind of new to deep learning um so my hope is that in this far part of your the talk i gave you some like good foundations and background knowledge so when we get to the second part of the talk um it will you'll be able to understand how it all goes together so in the second part of the talk i'm basically we're going to use everything we've learned but we're going to use a a real problem and i'm going to live code um a full neural network complete with iterations so you'll be able to see how the weight updates you'll be able to see how the air updates you'll you'll be able to see in the output of the the neural network how the air is going to zero and hopefully all these things we've kind of talked about that you've seen for the first time will start to make more sense when you see how it all works in an actual neural network so we'll also in the second part of the talk talk more about the deep learning part of this so so far i haven't even talked about why it's called deep learning but that's something else we'll be talking about i'm glad to see so many people decide to come back to part two of my talk so this is where i'll be pulling it all together what we were talking about in the first talk and hopefully a lot of things will start to make a lot of sense once you see a full neural network based on trying to solve a problem let's um let's get into it first of all i'm going to explain the problem so this is a problem we're going to try to solve with a neural network and it's kind of um it's it's a real world problem but it's a simplified real world problem so we have enough time to get through everything and so you can understand all the components of it so the problem so we you have to imagine the scenario so imagine the situation where you go to a country you've never been to before and you see a stoplight and you don't know there's three lights but you don't know which configurative configuration of lights means to walk and which configuration of lights mean to stop so um since you're since you're a programmer and a data scientist you decide you want to collect some data so you just sit there and watch and you observe whether people stop or walk so after a little while you you collect this information so you can see this is the first stop light in that configuration of lights with the two on the side on and the middle one off people stopped and then you see this people walked in the the second configuration of lights and then the third was just this one on a person was stopping so you're trying to figure out which configuration means walk and stop but this isn't quite enough information so you keep looking and you collect all this information here and so we're trying to um we collected the information of what we know and what we want to know so what we know is the configuration of the stoplights what we want to know is whether they mean to walk or stop so we're going to use this data and see if our neural network that we're going to develop can figure out which configuration of lights means to walk or stop now this is a simple example so hopefully you can kind of already see a pattern that the the middle light is perfectly correlated to whether you should walk or stop and actually the left and right light don't have anything to do with whether you can walk or stop so when we develop our neural network we're not going to tell it that information we're going to see if our neural network can learn on its own that the middle light is perfectly correlated to the walk and stop data so to make a neural network we cannot just put in these notes or this picture here so we have to convert it to numbers so this is how we're going to convert our street light data to numbers we're going to make if the light is on it's a one if the light is on or if it's off it's a zero so you can kind of see how these correspond to each other so we've got this new data set and we're going to use it to create a neural network to solve it so i'm going to go over to my code editor here now this is something called google colab and google colab a lot it's basically like an online code editor for jupyter notebook so i if you got i don't know if you're familiar with jupiter notebook but it's used a lot for deep learning and machine learning and this is an online version so you don't have to have anything installed on your computer and so it's make things makes things a lot simpler when you're testing things out here so i'll zoom in a little bit more so this is where i'm going to start typing code if you want to follow along you can or you can just look up at the screen but the first thing we're going to do is import numpy as np now let me build okay let's can you guys can see that okay so before we didn't use numpy but now we're going to use numpy and that's going to help with our multiplication remember before i showed you that vector matrix multiplication function we're not going to need that because numpy has that built in and that's the only thing we're going to import the rest of it's just going to be straight python so now we're going to have our weights array and i'm going to do mp dot array and the mp.array means it's a it's a numpy array and this allows us to do um special operations with it like the vector matrix multiplication and stuff like that so a lot of neural networks their weights are going to start off randomly and so i thought this would be a cool chance for some audience participation and i'm gonna see if you guys can give me some numbers they have to be between negative one and one so give me a decimal point between negative one and one so does anybody want to call out a number for our first weight zero seven okay how about 0.7 and then we need another it has to be between negative 1 and 1. anybody else have another number 0.2 okay and then our final number okay so this is just a fun way to get random numbers so you can see later that it kind of doesn't matter what you start your weights out as your network's going to be able to learn and get to the correct weight with just random weights so the next line is the alpha and we talked about that a little bit before the alpha helps make sure your network doesn't get out of control and the whole point of the network is to get our air to zero and but sometimes the air can start going up instead of down to zero and the alpha will make sure it doesn't go go out of control like that so now we're going to import our data that we collected so it's going to do mp.array now i'm going to be just this is going to be a vector or a matrix so that's an array of arrays and i'm just going to be typing in this information i kind of have some notes here but this is directly from the other slide of what this is going to look like so this is kind of the sometimes the tedious part of machine learning and data science is like dealing with the data and inputting data but it's just a part of it is actually just trying to put everything into the network and usually you're not going to be having to type in the data you're usually going to get it from some other source so it may not be you typing in every single number in the data set let's see one and you at some point you may see me type something in wrong that lets it's obvious that's wrong so feel free to let me know like if i spell like a variable wrong or something like that okay so if we go back to our data this should look just like what we typed in right here and now our next piece of information is our walk verse stop array so this is again information we collected in the real world in this example and we're going to do another mp.array and then pass in the array which is going to be 0 1 zero one one zero so again that's the the walk versus if we actually have to go back a slide so this is anytime it's zero or stop is zero one is walk okay there we go and then we can start with our iteration so in the last example before the break i showed you a single iteration but in this example we're going to do an iteration of 40. so i'm going to do four iteration and range 40. now like i had said previously and sometimes you could have hundreds or thousands of iterations and the whole point is that every time we go through an iteration the error gets closer and closer to zero and the way you figure out how many iterations to do is just through trial and error so from like from preparing for the talk and other experiences kind of trying out different things i found out that in 40 iterations this network should be able to get the air to zero but there's no there's not really any magic number you just have to try different things and see the results and see how long it takes to get to zero so now we are going to have a variable called error for all lights and it's going to be set to zero so we we need to figure out the air because the whole point is to get the air to zero so we have to something to collect the air so we're going to be adding to that air later and now i'm going to have another iteration for row index and range and then it's going to just be the length of the stop verse walk array so we're going to do something for every element in this walk versus stop array and the number of elements in the walk for stop array is also the number of arrays or vectors in the streetlights array so we're going to do something for each one that you'll see in a second here so in our last example just like in our exam last example we need an input and a goal prediction so this is going to be pretty similar i'm going to the input is going to be from the streetlights array and i'm going to put the the row index and then the goal prediction is going to be from the walk verstappen and that's going to be the row index so you'll see we're going to go through we're going to iterate through each one because this array corresponds to this number the second array corresponds to the second number the third rate corresponds to the third number okay what part the space after what oh stop verse walk i should definitely oh like is this what we're talking about oh yeah okay thank you i don't know i was looking at it for some reason i was thinking there's a spelling here so it's like what's spelled wrong so yeah thanks for pointing that out so this is a this is an example of what i was talking about i'm gonna make some mistakes but luckily i have the audience here too to help me fix these mistakes here okay so now we have our input and our goal prediction we can make our prediction so let me see if i can can you guys still see down at the bottom of the screen there okay so our prediction now if you remember before when we had our prediction in the first part of the talk i showed that we called our neural network function and our neural network function just multiplied the input times the weight so instead of creating a neural network function we're just going to do the input times the weight but we're going to use numpy so this is this dot function just multiplies input times weights and it can multiply a vector times a matrix so we're getting the the prediction the same ways from before and now we have to calculate the error let's see okay there we can see a little better the error is going to equal prediction minus goal prediction and that's the same as before so that's how we figure how far away our prediction was from what we thought it was going to be is the error and then also oh yeah one last thing we have to take that to the power of two so remember the point of that is to make sure the error is positive if you square it it's going to be a positive number and then also it makes big errors even bigger and smaller areas even smaller and that's actually something we want because it can it will get to the correct answer even faster so now we are just going to add to our error for all lights let's see error for all lights and i'm just going to plus equals error so we're just we're just collecting all the errors into the air for all lights so we're getting an air for each one of the walk for stop array and we're putting them all together because we want the collective error to get as close to zero as possible and then we're going to calculate the delta and that's going to be prediction minus goal prediction this is just how far away our prediction is from our goal prediction and it's considered the raw error this is the same as before and now we are going to calculate the weights now in our previous example the weights equaled um weights minus weight delta times alpha and we're going to basically have the same same thing here but we're going to instead of creating the weight delta variable if you remember from before we're going to kind of put it in line and it's going to be weights minus alpha oh and we need a parenthesis here times input so before the weight delta was input times delta and instead of making that weight delta variable we're going to put it right in line so weights equals weights minus alpha times input times delta so that's the same as before and now we're going to just print out some data so this is actually the whole code and so i'm going to print out some data here and then we'll see what happens so we're going to do we're going to print the prediction so we want to see the prediction variable and i'll just do the string of prediction now i'm going to use some copying and pasting here the prediction will do for each of that iteration but the next two will put in to this next loop let's see if that lines up right oh right here okay thanks i appreciate that okay so now we have um weights and weights and then we'll print out the error and the error okay so we're about to run this code and see what happens but before we run the code i'm going to go back to the slides okay so this diagram demonstrates what our network looks like so we have three inputs and then we pass them through these weights and then we're going to get an output which is the walk versus opt whether the the output is going to show whether those inputs mean to walk or stop so that's the whole point of our network we're trying to learn from do these inputs of the light mean to walk or stop so that's what we're going to do here we go i'll just run this code and it's going to take a little bit but we should see if i didn't make any spelling mistakes we should see some stuff appearing at the bottom here which is the data okay so you'll see at the fir first the the prediction doesn't really correspond to anything at first and the weights are going to be pretty similar to the weights we passed in at the very beginning so you can see here here are the weights so we have 0.6 0.4 negative 0.2 that's pretty close to the weights we started with right here but if we keep going down remember we did 40 iterations so every time it gets the air we're to the next iteration so the air starts at point three we're trying to get as close as possible to zero so if we just keep going down we're not going to look at every single iteration we're going to just kind of go down to the bottom and see what we've gotten to okay so here's the the last the last thing that we want to look at and if you look at the prediction um this number is very close to zero then the second one is close to one then we have one close to zero one one zero so if you remember one this actually is the same as our stop first walk so we have the stop is zero one zero one one zero and that's what the prediction ended up being close to zero one zero one one zero um now that's not necessarily super exciting because we actually gave gave the network that information so of course it's going to be able to figure that out that information because we put it right in the code uh another thing i just kind of showed you first was the error this is a number very close to zero it starts with a three point but then when you see we this is just like a an exponent that makes it really it's a really small number very close to zero so and that's the whole point of our our algorithm is trying to get the air to zero so what's really important to look at is the weights so if you see the weights these weights correspond to each light so the first light the first weight is close to zero the second light is close to one and the third white is close to zero so one means it's perfectly correlated and zero means it's not correlated at all at all and if you remember from our diagram the middle light was perfectly correlated on whether you should walk or stop and the the right and left lights were not correlated at all so there was no point in the code that we gave the program that information but somehow it was able to learn that the middle light was perfectly correlated and the outer lights were not correlated at all now in this example it was something that you can easily figure out just by looking at the data which one's correlated but you can just kind of imagine in a more complex data set you may not be able to figure out how the data is correlated just by looking at it so you use the exact same type of ideas to make an even more complex neural network and so you can actually learn things that that's not that aren't as easy to just learn just by looking at the data so um our network like i said our network correctly identified the middle light by analyzing the final weight positions of the network so to like think about how it identified the correlation it was in the process of gradient descent each training example either is going to assert upward pressure or downward pressure on the weights so on average there was more upward pressure for the middle weight and more more downward pressure on the outer weights so they got to the correct numbers so this is pretty cool and now i'm going to show you something even a little a little more complex so the next part of the talk instead of live coding i'm going to show you some code that's already written that's pretty similar to the code we already had but i'm going to introduce a few more concepts first of all i'm going to tell you that some of the concepts could even be a whole talk to themselves so i'm going to do my best to like quickly overview the concepts but some of the things you'll just kind of get a taste of so you can learn more about on your own later but the reason why i'm going to show you one more program is to talk about the deep part of deep learning so nothing i've showed you so far is actually deep so let me deep refers to one or more hidden layers between the input and the output layers of a neural network so if i go to my code here so our input layer is right here our output layer is right there there's nothing in between but if i look we look at this example this is a true deep neural network because we have our input layer that looks the same there's three on the input and one on the output for layer two but now there's this middle layer which is a hidden layer and since there's one or more hidden layers this is a deep neural network so you may be wondering why would you even need a hidden layer if our other network was able to correctly predict the correlation between the lights because we're going to use the same example from before so if you remember the data the middle light was perfectly correlated with whether you should walk or stop but if the middle light wasn't so perfectly correlated our network would have had a much harder time figuring out how the how the input was related to the output so like for instance if our training data looked like this our other network wouldn't have worked so well if you look at this so the the numbers in red are the the light information whether the lights are on or off and then the numbers in black are the stop or walk information so you can see this first the first three up here means to walk and we have walk stop stop and one thing you'll notice is that the output data is not perfectly correlated with any of the rows so this is where you need to have a hidden layer um what is because since our input data set does not correlate with our correlate with our output data set we're going to use our input data set to create an intermediate data set that does have correlation with our output and this intermediate data set is our hidden layer it's almost going to be like two neural networks it's going to be from layer 0 to layer 1 is going to be one network that runs just like before and then the so the output to the layer 0 to layer 1 network will become the input to the layer 1 to layer 2 network so let me show you okay so can you guys see this all right even in the back okay so a lot of this is going to look pretty similar here we have we're importing numpy as mp and then this line right here just make sure we're going to be using random numbers in this code and that line just makes sure the rand.seed makes sure every time we get random numbers it's the same random numbers so a lot of times when you're dealing with neural networks you're using random numbers for the weights but to make sure the output is always the same even though you're using random numbers you can make sure that every time the code runs the random numbers were the same random numbers that you used last time you ran the code and it helps for like comparing uh to make sure the code the results are exactly the same so this is called a this is an activation function it's the relu function we're going to pass our data into it it's going to return x or return it's going to return what the data we passed in if x is more than zero or return zero otherwise now this is a it's called an activation function because it activates some input by returning the input and deactivates other input by returning zero this code is going to show a three layer neural network and by turning any middle network off whenever it would be negative we allow the network to sometimes have the correlation from various inputs and sometimes not have the correlation so it's impossible for a two layer network to sometimes allow this input to correlate and sometimes not allow it to imp correlate so it adds more power to three layer networks so this sometimes correlation is important for things to work correctly and this is one of the areas that like i was saying could get a whole talk to itself as activation functions and how they work and why but i'm just going to kind of leave it at that for now and that the activation is functions important to make it sometimes correlate and sometimes not correlate the the relu two derive function is next and uh this returns one when the output is more than zero or zero otherwise and that's the slope of a reload function or its derivative and you'll see later how that's important so the next two this is just inputting the data just like we did before we do have the dot t that's a numpy thing it's the transpose operator and it just um changes so you have a matrix which is going to have like uh almost like an x and y coordinates like the so it's a if it's a three by four matrix the transpose operator would make it a four by three or if it's a one by four it'll become a four by one and when it's when you're multiplying a matrix and a vector it matters um which kind of direction the the matrix is if that makes sense and so the alpha is same before again sometimes you have to kind of try out different things to figure out what it's going to work and the hidden size is four so if we go back to our diagram you can see that our middle layer has four nodes so that's what this hidden size equals four is there's four nodes and then here is our weights so okay let me get back to that code here i accidentally did the keyboard the mouse shortcut for going back so if we go to our weights here before this is where we had the audience participation and i had people shout out different weights but now we're going to get truly random weights because we have a lot more weights this is going to be a matrix of weights and the size of the matrix like the matrix is similar to this this is a matrix right here up top a 3 by 4 matrix and this is going to be a three by four matrix and this is going to be a four by one matrix and each number in that matrix is going to be between negative one and zero and the reason why we have three by four is because you can see there's three nodes by four nodes and then we have four nodes by one node so the size of the weights has to correspond to the input and output layers because you can kind of you can see that this has there's a weight between layer one and layer zero and there's weights between layer one and layer two and the size of the weights have to correspond to those layers so now we're gonna you can see we're gonna iterate 60 times and then we're gonna keep track of the air just like before we were keeping track of the error it's just called something different so in this code there's gonna be a lot of things that are pretty much just like the code that i just did for the live coding but the names are slightly different so layer 0 here this is the same as the input from before so we're just getting the first street light which would be this matrix right here this vector right here and this is just a kind of fancy way of saying that we're going to get a nested mate a nested array so instead of an instead of an array of one zero one it will be an array of an array of one zero one so they'll just be like and if you output it there'd be an extra array on the sides um there you go so the re so that that just makes the kind of math work to have an array of arrays but it's just getting the first array from the streetlights and then layer one here is going to be the same as the input and the same as the prediction from before remember the other network we had one input and that input had one prediction but in this neural network it's basically two neural networks uh there's two two networks there's layer one to layer there's layer zero to layer one and there's layer one to layer two so the prediction or output from layer 0 to layer 1 becomes the input from layer 1 to layer 2. so this layer 1 is the input and the prediction so just like before we're always just mult to get a prediction you always multiply the input times weights so that's what this is it's another numpy function to just multiply the things that you pass into that function layer zero and weight zero one that's what you're always doing for your prediction multiplying inputs times weights then we run it by the relu function which will selectively activate like we talked about before layer two is our output layer and which is the same as our prediction from before and again we're just multiplying the input times the weight so the input is layer one and the weight is weights one two so this is the same thing we've been doing all along to get the prediction from the first stock from the very first code example this was the neural network we had a neural network function that just multiplied weights times input and that's exactly what we're doing here so now we have to get our error so our error is going to we have our layer 2 error here and we're going to calculate that pretty much just like before but we have different names so before it was the air was prediction minus goal prediction squared uh but now the prediction is that layer two variable the goal prediction is the getting something from the walk versus stop array and then we square it so air is always prediction my school prediction squared and then we're going to do a plus equal so we're going to add up all the errors together into our layer 2 error variable okay so the next thing is our delta this is basically exactly like the the other example um so this we have the layer two so this is just input or wait a second yeah input minus goal prediction prediction minus goal prediction so layer two minus walk for stop is the same as prediction mice goal prediction and remember the goal here is um air attribution it's all about figuring out how much each weight contributed to the final error so in our first two layer neural network we calculated a a delta value a delta variable which told us how much higher or lower the output prediction is supposed to be so we're calculating that the same way so now we have how much that we that though we want the final prediction to move up or down that's the delta and now we need to figure out how much we want each middle layer layer one node to move up or down so one thing that's interesting about this is if we go to our diagram we can see there's an order layer 0 layer 1 layer 2. but in our code we're gonna calculate layer two first and then layer one so we're not calculating it in the same order as you would normally think going through the going through the layers this is another like really big concept called back propagation let me it's a let me zoom out so you can see all the notes i have here so in back propagation the layer 2 delta is back propagated to the layer one delta and this is going to give us a weighting of how much each weight contributed to that error that's supposed to be in the word error oh it is it's just off the side of the screen okay so we need some way for our layer two weights to impact layer one because we need to update everything so that's what back propagation is and so this is another pretty big concept i mean back propagation could be a whole talk of its own but it's all about taking uh information from later on in the the network and passing it back to uh to the earlier in the network so it can use that information when it's making the calculations and let's see if there's one more thing we need to talk about so this is for the layer one delta here we're just using this dot function to multiply layer two delta by the weights and we have the transpose operator but then the final thing is this relu to derive function so if the relu set the output to layer one to the layer one node to be zero then it didn't contribute to the air at all so when this was true we should also set the delta of that node to be zero and multiplying each layer one node by the relu two derived function accomplishes this the relu two derived is either a one or a zero depending on whether the layer one value was more than zero or not so now we're going to update the weights and we update the weights just like we did before so for each weight we are going to multiply its input value by its output delta and so it's just weights one two equals weights one two uh minus wait i'm thinking yeah yeah weights one two equals weights one two minus alpha times and then so this is just how we calculate the weight before where you multiply the input by the delta and then you multiply that by the alpha and then that's that's just how you update the weights so again this code is all all online so you can kind of review it later like some of this stuff it kind of takes going through a few times to really see how it all goes together but you'll see that we're updating the weights in the exact same way as we were in the other live coding sessions that we did so down here this is going to just make sure that we're only going to um print this information uh just every once in a while so every few iterations so when we run when we run this it's only going to give the information every few iterations so i'm just going to run this right now and i think it has that from last time i ran it but if the main thing to look at here is that if we go to the bottom our error gets very close to zero so that's the whole point of learning the learning is trying to get our air as close as possible to zero now if you remember before that that meant that our weights were correctly we're making a correct prediction in this case it's it's the same now i'm kind of getting close to to the the conclusion of kind of why all this matters so what i mean there's probably we could probably talk about this code for a few more hours to explain it every little part like the back propagation and and the weights but i kind of want to do kind of step back a little bit and give kind of an overview of the point of why we create intermediate data sets that have correlation so i go back over here okay so consider this image of this handwritten four that's kind of blown up it's supposed to be supposed to look like the number four and uh if you if we had a data set of a bunch of images of handwritten digits and they were all labeled like zero to nine um if we wanted to train a neural network to take the pixel values from this image and predict that this was a four our two layer neural network would have never been able to do it so they're just there's no individual pixel in this and this here so there's no individual pixel that perfectly correlates with whether it's a four or not just like in our second example that i just went over there was no light combination that perfectly correlated with walk or stop so there's there's only different configurations of pixels that correlate with whether or not this is a four so this is the essence of deep learning if i go to the next slide you'll see how this would work with kind of the multiple layers where deep learning is all about creating intermediate data sets or layers where each node in an intermediate layer represents the presence or absence of a different configuration of inputs so in this way no pixel has to perfectly correlate with whether or not it's a four or not instead we have these middle layers that they attempt to identify different configurations of pixels that may or may not correlate with the four so we go through a lot of different layers and finally it finds which layer correlates if it finds a configuration of pixels that correlates with whether it's a four not and then it outputs that this is a four so the presence of many four like configurations would then give the final layer the information or correlation it needs to correctly predict whether the digit is a four so we can take our three layer network and continue to stack many many more layers some networks even have hundreds of layers and each neuron plays its part in detecting different configurations of input data so you could start with like we started with these uh input of three layers but the hidden layers could get bigger and bigger and bigger until it finally has what it needs to correctly predict the output layer so i'm hoping that just kind of seeing this for example helps you kind of see how how adding additional layers can help you get the final prediction where you don't need just like our stop light example you don't need the perfect correlation to figure out the final answer you can make other layers and other layers after that that finally get the correlation to get the final answer so i'm getting kind of i'm cl to the end of the talk here so i'm hoping you just by this talk you've got a good introduction to deep learning and neural networks and you saw by the examples how we were able to write a neural network they actually learned what we're trying to learn and then i also hope you unders you start to understand how hidden layers can can give your neural network even extra power to figure out things it would not have normally figured out so yeah thank you for coming to the talk and i'm open for questions right now
