With timestamps:

00:00 - hey this is andrew brown your cloud
00:02 - instructor exam pro bringing you another
00:04 - complete study course and this time it's
00:05 - the aws certified cloud practitioner
00:07 - made available to you here on free code
00:09 - camp and if you think you've seen this
00:10 - course before that's because this is a
00:12 - major update from the very popular
00:14 - 2019-20
00:16 - course that had over 2 million views and
00:19 - this time around we have three times
00:21 - more content so this course is designed
00:23 - to help you pass and achieve it was
00:25 - issued certification and the way we're
00:28 - going to do that is by going through
00:29 - lecture content doing labs in our own
00:31 - account utilizing a practice exam
00:34 - downloading the cheat sheets on the day
00:35 - of the exam and then once you pass you
00:37 - can improve on your resume and linkedin
00:39 - you have that either business knowledge
00:40 - to get that cloud job or to get that
00:42 - promotion to tell you a bit about me i
00:44 - was previously the cto of multiple
00:46 - edtech companies with 15 years industry
00:48 - experience five years specializing in
00:50 - the cloud i'm ava's community hero i
00:53 - publish multiple free cloud courses i
00:55 - love star trek and coconut water and i
00:57 - just want to take a moment to thank
00:59 - people like you because it's you that
01:01 - make these free courses possible and if
01:03 - you want to know how to support more
01:05 - free courses like this one the best way
01:07 - is to buy or extra study materials and
01:10 - so for this course it's at exam pro dot
01:12 - co four slash clf hyphen c01 this is
01:16 - where you'll get study notes flash cards
01:18 - quizlets downloadable lecture slides
01:20 - downloadable cheat sheets uh prax exams
01:23 - you can ask questions and get support
01:26 - and i also just want to tell you if you
01:27 - do sign up you're going to get
01:29 - additional stuff
01:30 - already so you'll get the free practice
01:32 - exam and cheat sheet there's no credit
01:34 - card required required and there's no
01:36 - trial limit so there's no reason not to
01:37 - sign up
01:39 - and if there are course updates check
01:41 - the description in the youtube to see if
01:43 - there are any updates okay so there
01:45 - might be corrections additions
01:47 - modifications and this is just going to
01:49 - ensure that you're using utilizing the
01:51 - latest version of this course and so to
01:54 - keep up to date with upcoming courses
01:57 - follow me on twitter at andrew brown and
01:59 - if you are over there i'd love to hear
02:01 - if you have passed your exam and what
02:03 - you'd like to see next so there you go
02:06 - [Music]
02:10 - hey this is andrew brown from exam pro
02:12 - and we're at the start of our journey
02:13 - asking the most important question first
02:15 - which is what is the aws certified cloud
02:18 - partitioner so the cloud partitioner is
02:20 - the entry level aida certification
02:22 - teaching cloud fundamentals such as
02:23 - cloud concepts architecture deployment
02:26 - models it will take a close look at
02:27 - database core services a quick look at
02:30 - the vast amount of data services and
02:32 - will cover topics like identity security
02:34 - governance billing pricing support of
02:37 - aws services the course code for this
02:39 - exam is the clf c01 but it's commonly
02:42 - referred to as the ccp
02:45 - and aws is the leading cloud service
02:47 - provider in the world and that makes the
02:49 - certified cloud petitioner the most
02:51 - common starting point for people
02:52 - breaking into the cloud industry no
02:54 - matter what their path is
02:57 - so who is this certification for well
03:00 - you should be considering the aws cloud
03:01 - partitioner if you are new to cloud and
03:04 - need to learn the fundamentals if you
03:06 - are in the executive management or sales
03:08 - level and you need to acquire strategic
03:10 - information about cloud for adoption or
03:12 - migration
03:13 - or you are a senior cloud engineer or
03:15 - solutions architect who needs to reset
03:18 - or refresh their aws knowledge after
03:20 - working for multiple years and just
03:22 - seeing how the landscape has changed
03:25 - so what value does this certification
03:27 - bring well the aws certified cloud
03:29 - practitioner provides the most expensive
03:31 - view possible
03:32 - of cloud architectures and advanced and
03:34 - when we're talking about that expansive
03:36 - view what you should be thinking about
03:38 - is
03:38 - it being a bird's eye view or a 50 000
03:41 - foot view
03:42 - looking onto a panoramic landscape where
03:45 - you can see everything and the idea of
03:47 - this expansive view is to promote big
03:50 - picture thinking so the idea here is
03:52 - you're zooming out and assessing the
03:54 - cloud it was landscape for changes
03:57 - trends opportunities and being strategic
04:00 - about the approach and process for our
04:03 - cloud journey
04:05 - the innovations cloud practitioner is
04:06 - not a difficult exam it will not
04:08 - validate that you can build cloud
04:10 - workloads for technical implementation
04:12 - roles like a developer engineer
04:15 - devops role it will not be enough to
04:17 - obtain a cloud role but it can help
04:19 - shortlist your resumes for interviews
04:21 - the exam covers content not found in
04:23 - other certifications and it is
04:25 - recommended as an essential study for
04:27 - your aws journey
04:30 - so now let's take a look at the awesome
04:31 - certification roadmap to see where we
04:33 - would go after the cloud petitioner and
04:35 - what kind of uh cloud roles
04:38 - would be associated with those
04:39 - certifications so at the start you get
04:41 - your cloud practitioner which is at the
04:43 - fundamental level after that we have the
04:46 - associate level such as the sysop
04:48 - administrator the developer and the
04:49 - solutions architect followed by the
04:51 - professional level the devops engineer
04:53 - the solutions architect professional and
04:56 - then the specialties such as security
04:58 - advanced networking database machine
05:00 - learning data analytics and sap which
05:02 - just is not on here yet because it's
05:04 - such a new certification so after the
05:06 - cloud practitioner generally people will
05:08 - go for an associate and it's up to you
05:11 - to choose one of the three
05:13 - because they're all great routes but the
05:15 - most common one is the solutions
05:16 - architect associate
05:18 - because the most common role in the
05:20 - industry is a cloud engineer so even
05:22 - though it's called solution to architect
05:24 - they really should have named it cloud
05:25 - engineer because that is really what it
05:27 - is uh if you were to go the developer
05:29 - route you're basically becoming a cloud
05:30 - developer and then if you are going the
05:33 - sysops admin route you are becoming a
05:35 - junior devops engineer
05:38 - and it's not uncommon for people to
05:40 - obtain all three associates and a lot of
05:43 - times the order will be the solution
05:44 - architect first because it's the easiest
05:46 - and and has the broadest services
05:48 - followed by the developer
05:49 - um which adds uh
05:52 - practical programming skills and um
05:56 - life cycle stuff of like deployment for
05:58 - apps followed by the sysops
05:59 - administrator which is considered the
06:01 - hardest of the three in the associate
06:03 - tier
06:04 - from there you can go for the solutions
06:06 - architect professional and that would be
06:08 - associated with a solutions architect or
06:10 - cloud architect role that's basically
06:12 - like a harder version of the cloud
06:14 - engineer with a lot more
06:16 - responsibilities if you were going to
06:18 - devops route you'd go for the devops
06:20 - engineer professional and so this would
06:22 - open you up to roles such as the devops
06:24 - engineer or the site reliable
06:26 - reliability engineer an sre
06:29 - and some people like to get both of the
06:32 - professionals
06:33 - and that could be if you want to be a
06:35 - cloud architect or devops engineer
06:37 - because having adjacent skills and the
06:39 - professionals is always very useful now
06:41 - you don't have to go for a professional
06:43 - after the associate a lot of people will
06:45 - jump over to the specialties and so when
06:47 - we're looking at the solutions architect
06:49 - you basically have any pic after that
06:51 - but generally what i see are people
06:53 - going for data analytics or machine
06:55 - learning so for data analytics this
06:58 - would be if you want to be a data
07:00 - analyst
07:01 - or if you're doing machine learning this
07:03 - is where data scientists will go through
07:05 - the solutions architect route
07:07 - okay
07:08 - for the junior devops you could jump
07:10 - over to security and become a
07:13 - cloud security engineer if you want to
07:16 - go into devsecops so the automation of
07:19 - security operations you probably want to
07:21 - get the devops engineer um or you may be
07:25 - if you're after the devops engineering
07:27 - you might be transitioning to the
07:28 - advanced networking for roles like in
07:30 - netdevops where you're specializing in
07:32 - migration or hybrid
07:34 - engineer for architectures that both use
07:37 - on-premise and the cloud from the devops
07:40 - engineer position you can still go for
07:42 - the database or machine learning
07:44 - certification if you want to become
07:45 - either a data engineer or an ml ops
07:48 - engineer so there's a lot of
07:50 - opportunities here
07:51 - and there is no perfect route but just
07:54 - these are suggestions for you to decide
07:56 - on your own okay so how long is it going
07:59 - to take to pass this certification well
08:02 - it's going to really depend on your
08:04 - background but if we had to generalize
08:06 - it we can look at it uh as kind of a
08:09 - scale and so if you are at the beginner
08:11 - level you're looking at 30 hours of
08:13 - studying and when we say beginner we're
08:15 - saying someone that has never used aws
08:17 - or any cloud provider i have never
08:19 - written code or held a tech role and
08:21 - when we're looking at the other side of
08:23 - it someone that is experienced we're
08:25 - looking at a six hour study time and
08:27 - when i say that i'm talking about
08:28 - somebody that's watching on two times
08:30 - speed and are able to absorb this
08:32 - information uh very quickly so they have
08:34 - practical working experience with aws or
08:38 - they have equivalent experience in
08:39 - another cloud service provider like
08:40 - azure gcp where they can translate that
08:42 - knowledge or they have a very strong
08:44 - background in uh technology where
08:46 - they've worked in the industry for many
08:48 - years and so you know their study time
08:50 - is going to be a lot shorter
08:52 - and so on average most people are going
08:54 - to take about 24 hours to study for this
08:58 - course and when we talk about the kind
09:00 - of stuff that you'll be doing it's going
09:02 - to be 50 lectures and labs and we call
09:05 - our labs follow alongs where the idea is
09:06 - you follow along in your own account and
09:08 - then 50 is the practice exams so if you
09:11 - look at the length of the content which
09:13 - is around uh 12 hours then you know you
09:17 - should expect to spend as much time
09:19 - doing practice exams uh to pass okay and
09:22 - the rem recommended time to study is one
09:24 - to two hours a day for 14 days okay
09:28 - so what kind of effort are we going to
09:30 - have to put in to pass this exam well
09:33 - you have to watch the lecture videos and
09:35 - memorize key information you'll need to
09:37 - do hands-on labs and follow along with
09:39 - your own account and you will need paid
09:42 - online practice exams that simulate the
09:44 - real exam and the last two here were
09:47 - things that i used to never suggest
09:49 - because you could literally just watch
09:50 - the videos and pass however edibus has
09:53 - made this exam a lot more difficult and
09:56 - so for these last two points you do have
09:59 - to do these two things for the paid
10:02 - online practice exams uh that can be a
10:05 - hard for some people so i've made it
10:06 - easier for you by providing you a full
10:09 - free practice exam on exam pro at four
10:12 - slash clf c01 and so you just have to
10:15 - sign up no credit card required and
10:17 - you'll get a full set of 65 questions
10:20 - that simulate the real exam okay
10:23 - so for the contents of the exam it is
10:26 - composed of four domains and each domain
10:28 - has its own weighting which determines
10:30 - how many questions in the domain that
10:31 - will appear so for domain one which is
10:34 - cloud concepts we're looking at 26
10:35 - percent for domain 2 security and
10:37 - compliance we should expect to see 25
10:40 - percent of the questions from there for
10:41 - domain 3 which is technology and where
10:44 - we will see the most amount of questions
10:46 - that we're sitting at 33 percent for
10:49 - domain four billing and pricing we have
10:51 - 16 of the exam there so just to
10:54 - emphasize for domain 3 you need to know
10:56 - a wide range of services but you also
10:59 - need to know
11:00 - in-depth the core services
11:03 - so where do you take the exam well at an
11:05 - in-person test center or online from the
11:07 - convenience of your own home aws is
11:10 - partnered with two different test center
11:12 - networks the first being psi and the
11:15 - second being pearson vue and they both
11:17 - offer in-person or online and these
11:20 - exams are proctored meaning there is
11:22 - somebody watching you to ensure that you
11:24 - are not cheating okay
11:26 - in order to pass this exam you have to
11:28 - score 700 points out of a thousand and
11:31 - so 700 generally equates to 70 percent
11:34 - but it's around 70 percent because aws
11:37 - uses scaled scoring meaning that they
11:39 - could adjust it based on how many people
11:41 - are passing or failing so always aim to
11:44 - uh get higher than 70 percent the exam
11:47 - contains 65 questions 50 scored and 15
11:50 - unscored and you can afford to get about
11:53 - 15 questions wrong there is no penalty
11:56 - for wrong questions so you should always
11:58 - choose an answer and the questions come
12:00 - in two formats multiple choice and
12:02 - multiple answers for these unscored
12:05 - questions there are 15 on the exam they
12:07 - will not count towards your final score
12:10 - why is there unsword questions on the
12:12 - exam well unscored questions are used to
12:14 - evaluate the introduction of new
12:16 - questions
12:17 - they can determine if the exam is too
12:19 - easy and the passing score or question
12:21 - difficulty needs to be increased and
12:23 - they can discover users who are
12:25 - attempting to cheat the exam or steal
12:27 - dump exam questions so if you encounter
12:30 - questions you've never studied for that
12:31 - seem really hard keep your cool and
12:33 - remember they may be unscored questions
12:36 - the duration of this exam is 1.5 hours
12:39 - so you have about 1.5 minutes per
12:42 - question the exam time is 90 minutes but
12:44 - the seat time is 120 minutes seat time
12:47 - refers to the amount of time you should
12:49 - allocate for the exam so that means
12:51 - including things like time to review
12:53 - instructions show online proctor your
12:55 - workspace read and accept the nda and
12:58 - complete the exam and provide feedback
13:01 - and when you do pass this exam is valid
13:03 - for 36 months and that equates to three
13:06 - years before re-certification
13:09 - [Music]
13:14 - hey this is andrew brown from exam pro
13:16 - and i'm on the aws certified cloud
13:17 - partitioner page because what i want to
13:19 - show you here is the exam guide if
13:21 - you're wondering how to book your exam
13:22 - you go to schedule exam there and that's
13:24 - the way you can do it but if you scroll
13:25 - on down there's this download exam guide
13:27 - and this will download a pdf that will
13:29 - tell you everything about the exam and
13:31 - so just make note of the course code
13:33 - this is the clf c01
13:35 - because if this exam has a major major
13:38 - change they'll call it the co2 okay and
13:40 - then you'll know that this exam might
13:42 - not fit for the new uh
13:45 - the new exam guide okay so if we scroll
13:47 - on down there is a basic introduction
13:49 - they'll say you have to have six months
13:50 - which is totally not true you can get in
13:52 - the cloud with no experience uh and uh
13:55 - be passing this exam within two to three
13:57 - weeks so you can just kind of ignore
13:59 - that so it will just state that there is
14:01 - multiple choice multiple responses also
14:03 - known as multiple answer there are 50
14:05 - questions of the exam with 15 unscored
14:07 - questions so you'll get 65 questions in
14:10 - total
14:11 - uh it's scored between 100 to 1000. the
14:13 - passing grade is 700 it explains about
14:16 - scaled scoring there then it goes onto
14:18 - the course our content outline where we
14:20 - have the four domains and it has a big
14:22 - breakdown of all the things that could
14:24 - appear on the exam and the thing about
14:26 - this is is that um
14:28 - you know there's only 65 questions but
14:30 - there if you break down all these points
14:32 - there's like three times more
14:34 - information than could possibly show up
14:36 - on the exam so just understand that you
14:38 - are going to be studying a lot of
14:40 - information but only one third of it's
14:41 - going to show up on your exam so what i
14:43 - did is i went through every single one
14:44 - of these things and i made sure that we
14:46 - are covering them some stuff i just
14:48 - never saw an exam and also other people
14:50 - i never saw were design principles um i
14:53 - mean they are generally covered in the
14:55 - well architected framework but it's
14:57 - unusual because some of the things in
14:59 - here i just feel they aren't actually on
15:01 - the exam and they just kind of cram this
15:02 - exam guide together but i was very
15:04 - thorough to make sure to add everything
15:06 - here um so for security and compliance
15:08 - it's just knowing a collection of um
15:11 - database security services and some
15:13 - security concepts
15:15 - for technology this is our largest
15:16 - section you need to know so much stuff
15:19 - but we spent a lot of time in the course
15:21 - just covering technology then you have
15:23 - your billing and pricing and you could
15:25 - also say support
15:26 - and so that covers a lot of interesting
15:28 - thing a lot of stuff around ec2 pricing
15:31 - and then they just have a big list of
15:33 - stuff so this is a bit a bunch of
15:36 - random
15:37 - technologies and concepts that might be
15:38 - covered and then they talk about
15:39 - services and so again we cover basically
15:42 - everything just in case for you
15:44 - but yeah there you go
15:46 - [Music]
15:50 - hey this is andrew brown from exam pro
15:52 - and what we're looking at here is a free
15:55 - practice exam that i provide with you uh
15:57 - for this course and all you have to do
15:59 - is sign up on exam pro you don't even
16:01 - need a credit card and you can redeem uh
16:04 - the free available content here and this
16:06 - is really up to date and very well
16:08 - simulates what you will see on the
16:10 - actual exam and it's a full set full 65
16:13 - questions so you're getting a real
16:15 - simulation here but what i'm going to do
16:17 - is just start it off here we're not
16:18 - going to do the whole thing i'm just
16:19 - going to click through and show you a
16:20 - couple of them so you have an idea um
16:23 - the level of difficulty these questions
16:25 - are so the first question we got
16:26 - presented with here is which support
16:28 - plans provide access to the seven core
16:31 - trusted advisor checks and so that is a
16:34 - question that you might need to answer i
16:36 - don't want to spell this for you so i'm
16:37 - not going to tell you the answer i will
16:39 - go to the next one so a large accounting
16:41 - firm wants to utilize aws to store
16:43 - customer accounting information in
16:44 - archive storage and must store this
16:46 - information for seven years due to
16:47 - regulatory compliance which database
16:49 - service
16:50 - meets this requirement so the first one
16:52 - you'll notice this one is multiple
16:54 - choice or sorry multiple answers so you
16:56 - have to select multiples before you can
16:58 - submit your answer
16:59 - and the next one here is just a single
17:02 - choice so those are the two types of
17:03 - questions you will see on the exam
17:06 - they're not going to ask you anything
17:07 - about coding you're not going to see any
17:09 - kind of code
17:10 - in terms of length that's pretty much
17:12 - what we'll see in terms of the questions
17:15 - i think in many cases i wrote a little
17:17 - bit more more like um in the style the
17:20 - solutions architect associate to make it
17:22 - slightly more difficult just so that
17:24 - you're a little bit over prepared so if
17:26 - you do well on these practice exams
17:28 - you're going to do a well on the real
17:30 - exam okay so i just wanted to kind of
17:33 - get you that exposure there okay
17:34 - [Music]
17:39 - hey this is andrew brown from exam pro
17:41 - and we are at the start of our journey
17:42 - asking the most important questions
17:44 - first which is what is cloud computing
17:46 - so cloud computing is the practice of
17:48 - using a network of remote servers hosted
17:50 - on the internet to store manage and
17:52 - process data rather than a local server
17:55 - or personal computer and so when we're
17:57 - talking about on-premise you own the
17:59 - servers you hire the i.t people you pay
18:02 - or rent the real estate you take all the
18:04 - risks but with a cloud provider
18:06 - someone else owns the servers someone
18:08 - else hires the it people someone else
18:10 - pays or rents the real estate and you
18:12 - are responsible for configuring cloud
18:14 - services and code and someone takes care
18:16 - of the rest of it for you okay
18:19 - [Music]
18:23 - so to understand cloud computing we need
18:25 - to look at the evolution of cloud
18:26 - hosting going all the way back to 1995
18:29 - where if you wanted to host your website
18:31 - or web app you'd have to get a dedicated
18:33 - server so that would be one physical
18:35 - machine dedicated to a single business
18:37 - running a single project a site or an
18:40 - app
18:40 - and as you can imagine these are
18:42 - expensive because you have to
18:44 - buy out write the hardware have a place
18:46 - to store it the network connection
18:48 - having a person to maintain it
18:50 - but it did give you a guarantee of high
18:52 - security
18:53 - and they still do as of today so this
18:55 - model hasn't gone away but it's been
18:57 - specialized for a particular use case
18:59 - then came along the virtual private
19:00 - server so the idea is we still had one
19:03 - physical machine but now we were able to
19:05 - subdivide
19:07 - our machine into submachines via
19:09 - virtualization and so essentially you're
19:12 - running a machine within a machine and
19:13 - so you had better utilization of that
19:16 - machine
19:17 - running multiple web apps as opposed to
19:19 - having a physical machine per project so
19:21 - you got better utilization and isolation
19:24 - of resources
19:25 - and so
19:26 - these two options still required you to
19:28 - purchase a machine a dedicated machine
19:31 - and so that was still kind of expensive
19:33 - but then came along shared hosting and
19:34 - so if you remember
19:36 - the mid-2000s like with godaddy or
19:39 - hostgator or any of those sites where
19:41 - you had really cheap hosting the idea is
19:43 - that you had this one physical machine
19:45 - shared by hundreds of businesses and the
19:48 - way this worked it relied on tenants
19:50 - under utilizing their resources so you
19:53 - know you wouldn't have a sub machine in
19:54 - there but you'd have a folder with
19:56 - permissions that you could use
19:58 - um and so you would really share the
20:00 - cost and this was very very cheap
20:02 - but you were limited to whatever that
20:05 - machine could do and you were very
20:06 - restricted in terms of the functionality
20:08 - you had and there was this poor
20:10 - isolation meaning that you know if one
20:12 - person decided to utilize the server
20:14 - more they could hang up all the all the
20:16 - websites on that single server then came
20:18 - along cloud hosting and the idea is that
20:21 - you have
20:22 - multiple physical machines that act as
20:23 - one system so this is distributed
20:25 - computing and so the system is
20:27 - abstracted into multiple cloud services
20:30 - and the idea is that you basically get
20:32 - the advantages of a lot of the things
20:33 - above so it's flexible you can just add
20:36 - more servers um it's scalable it's very
20:39 - secure because you get that virtualized
20:42 - isolization you get it extremely at a
20:44 - low cost because you're sharing that
20:45 - cost with the users where in the shared
20:47 - hosting it might be hundreds of
20:49 - businesses we're looking at thousands of
20:51 - businesses and it was also highly
20:53 - configurable because it was a full
20:54 - virtual machine now
20:56 - cloud actually
20:57 - still includes all of these types of
20:59 - hosting they haven't gone away but it's
21:02 - just the idea that you now have more of
21:04 - a selection for your use case uh but
21:05 - hopefully that gives you an idea uh what
21:07 - cloud hosting looks like and it really
21:09 - has to come down to distributed
21:10 - computing okay
21:15 - [Music]
21:16 - hey this is andrew brown from exam pro
21:18 - and before we talk about aws we need to
21:20 - know what is amazon so amazon is an
21:22 - american multinational computer
21:23 - technology corporation headquartered in
21:25 - seattle washington and so this is the
21:28 - seattle skyline with the space needle
21:30 - and amazon was founded in 1994 by jeff
21:33 - bezos and the company started as an
21:35 - online store for books and expanded to
21:37 - other products
21:38 - so as you can see this is jeff bezos a
21:41 - long time ago and he has this
21:42 - interesting spray painted sign and his
21:45 - desk is held up by cinder blocks and it
21:47 - looks like his uh desk is like an old uh
21:50 - table or something and he's working
21:53 - really late and he used to be a
21:55 - millionaire at this time and he would be
21:56 - driving into work in his honda accord
21:59 - because you know he just his motivation
22:02 - was always to put all the money back
22:03 - into the company so it really shows that
22:05 - he worked really hard and it did pay off
22:07 - because amazon has expanded beyond just
22:09 - an online commerce store into a lot of
22:11 - different things
22:13 - such as cloud computing which is amazon
22:15 - web services digital streaming such as
22:17 - amazon prime video prime music they
22:19 - bought twitch.tv they own the whole
22:22 - foods market grocery store they have all
22:24 - this artificial intelligence
22:26 - they own low orbit satellites
22:29 - and a lot more stuff it's hard to list
22:31 - at all and so jeff bezos today is not
22:35 - the um the ceo it's actually andy jassy
22:39 - is the current ceo of amazon he was
22:40 - previously the ceo of aws so jeff bezos
22:42 - can focus on space travel so there you
22:45 - go
22:46 - [Music]
22:50 - hey this is andrew brown from exam pro
22:52 - and we are taking a look at amazon web
22:54 - services and this is the name that
22:56 - amazon calls their cloud provider
22:58 - service and it's commonly referred to
23:00 - just as aws so here is the old logo
23:03 - where we see the full name and here is
23:05 - the new logo but i like showing the old
23:07 - logo because it has these cubes which
23:09 - best represent what aws is and it is a
23:12 - collection of cloud services that can be
23:14 - used together under a single unified api
23:17 - to build a lot of different kinds of
23:19 - workloads so aws was launched in 2006
23:23 - and is the leading cloud service
23:24 - provider in the world i put an asterisk
23:26 - there because technically
23:28 - aws existed before 2006 and a cloud
23:31 - service provider
23:33 - which is what aws is is often
23:35 - initialized as csp so if you hear me
23:37 - saying csp i'm just saying cloud service
23:39 - provider okay
23:42 - so just trying to look at the timeline
23:44 - of when services rolled out the first
23:45 - one came out in uh 2004 and was simple
23:49 - queue service sqs and this service still
23:52 - exists as of today but at the time it
23:54 - was the only service that was publicly
23:55 - available so it wasn't exactly a cloud
23:58 - service provider at this time and it was
24:00 - neither aws it was just sqs but then a
24:03 - couple years later we had simple storage
24:05 - service also known as s3 which was
24:08 - launched in march of 2006 and then a
24:10 - couple months later we had elastic
24:12 - compute cloud also known as ec2
24:15 - and ec2 is still
24:17 - like the most used service within aws
24:20 - and is like the backbone for pretty much
24:21 - everything there
24:22 - then in 2010 it was reported that all of
24:25 - amazon.com's retail sites had migrated
24:28 - to aws so even amazon was using aws full
24:31 - steam and to support industry-wide
24:34 - training and and skill standardization
24:35 - it was began offering a certification
24:38 - program for computer engineers on april
24:41 - 2013
24:42 - and this is the type of certifications
24:44 - that we are doing as we speak um so i
24:47 - just want you to know that aws was the
24:48 - one leading uh cloud certifications and
24:51 - we just want to take a look here at the
24:52 - executive level as of today the ceo is
24:55 - adam he's the former cto of tableau and
24:58 - he spent a decade with aws as a vp of
25:00 - marketing sales and support so he was
25:02 - there he had left for a bit and now he
25:04 - is back then we have uh werner and he's
25:07 - the cto of aws he's been uh the cto for
25:10 - pretty much
25:11 - the entire time it was existed with the
25:12 - exception of some time of the first year
25:15 - he's famous for quoting everything fails
25:18 - all the time and then there's jeff barr
25:20 - who's the chief evangelist so um if
25:22 - you're ever wondering who is writing all
25:24 - the blog posts and talking about databus
25:26 - it's always jeff barr okay
25:29 - [Music]
25:33 - all right so what i want to do here is
25:34 - expand on what is a cloud service
25:37 - provider also known as a csp just
25:39 - because there's a lot of things out in
25:40 - the market there that might look like a
25:42 - csp
25:43 - but they actually are not so let's go
25:46 - through this list and see what makes a
25:48 - csp so this is a company which provides
25:51 - multiple cloud services ranging from
25:53 - tens to hundreds of services those cloud
25:56 - services can be chained together to
25:57 - create cloud architectures those cloud
25:59 - services are accessible via a single
26:01 - unified api so databases cases that is
26:04 - the aws api
26:07 - and from that you can access the cli the
26:09 - sdk the management console those cloud
26:11 - services utilize metered billing based
26:14 - on usage so this could be per second per
26:16 - hour
26:17 - vpcus memory storage things like that
26:20 - those cloud services have rich
26:22 - monitoring built in so you know every
26:24 - api
26:26 - action is tracked and you have access to
26:28 - that so in aws's case it's ableist
26:30 - cloudtrail
26:31 - and the idea here is those cloud
26:33 - services have infrastructure as a
26:34 - service offering so iaas that means they
26:37 - have networking compute
26:40 - storage databases things like that
26:43 - those cloud services offers automation
26:45 - via infrastructure as code so you can
26:47 - write code to set everything up and so
26:50 - here's just kind of an example of an
26:52 - architecture where we have a very simple
26:54 - uh web application running on ec2 behind
26:57 - the load bouncer with the domain with
26:58 - rough d3 but the idea is just to show
27:00 - you that you know you're chaining these
27:02 - things together if a company offers
27:04 - multiple cloud services under a single
27:06 - ui but do not meet most or all of these
27:09 - requirements it would just be referred
27:10 - to as a cloud platform so when you hear
27:12 - about twilio or hashicorp or databricks
27:15 - those are cloud platforms and aws azure
27:19 - gcp are cloud service providers okay
27:22 - [Music]
27:26 - let's take a look here at the landscape
27:28 - of cloud service providers and the
27:29 - industry likes to break these down into
27:31 - three tiers so we have tier one so this
27:33 - is top tier
27:34 - these were early to market they have a
27:36 - wide service offering they have strong
27:38 - centers used between services and
27:39 - they're well recognized in the industry
27:41 - and in the leading spot is amazon web
27:43 - services and there's no surprise to this
27:46 - because they were the first to develop
27:48 - the technology and so they pretty much
27:50 - dominated the market for multiple years
27:52 - before anyone entered and so it's going
27:54 - to be very hard for anyone to catch up
27:55 - or even overtake them but right behind
27:57 - them is microsoft azure then we have
27:59 - google cloud platform and these three
28:01 - are known as the big three because
28:03 - they're the most used around the world
28:06 - and we actually have a fourth one that's
28:08 - in the tier one and that's alibaba cloud
28:10 - you might not know about it just because
28:12 - it really is based in mainland china and
28:15 - in the asia region so it is really big
28:18 - but it's just the fact that there's that
28:20 - divide between mainland china and the
28:21 - rest of the world okay
28:24 - you have tier two so these are the
28:26 - mid-tiers so at one point you know they
28:28 - could have been topped here but um you
28:30 - know they were just slow to innovate and
28:32 - so they had to turn to specialization
28:34 - but they're all backed by well-known
28:36 - tech companies have been around for a
28:37 - long time well before aws existed so we
28:40 - have ibm cloud oracle cloud or rackspace
28:44 - and so rackspace is offering is actually
28:46 - their software called openstack which
28:48 - allows you to run a cloud service
28:50 - provider-like environment uh on your
28:52 - on-premise okay
28:54 - and so you know these are still in use
28:56 - so oracle cloud what they usually do is
28:58 - they try to fight on price and ibm cloud
29:00 - they they fight on ai and ml uh
29:03 - solutions against the top tier then you
29:06 - have the uh tier three the light tier
29:08 - and so these were virtual private
29:09 - servers that turned to offer core iias
29:13 - infrastructure as a service offerings
29:15 - and so they're simple and cost effective
29:17 - and a lot of people that are getting
29:19 - into cloud or even just trying to deploy
29:21 - apps are probably using these and not
29:23 - realizing their cloud service providers
29:25 - so we have a
29:26 - vulture digital ocean and lynnoids so
29:29 - they started with a single offering just
29:31 - virtual machines then they added a load
29:33 - balancer and so they're starting to get
29:35 - more so like digitalocean i think is
29:37 - getting a serverless
29:39 - offering and then linoid
29:41 - or sorry
29:42 - vulture is getting a kubernetes managed
29:45 - service and so you know they kind of
29:47 - live in this realm of are they csps and
29:50 - i would classify them as they are i
29:52 - would say they are a tier three they're
29:54 - just a light tier and i'm sure they'll
29:56 - expand their services to have more of
29:58 - the core but they're just going to stay
30:00 - i think very small in general okay
30:02 - [Music]
30:06 - so how do we know who is the leader in
30:08 - the market well all comes down to the
30:10 - madric quadrant and this is a series of
30:13 - market research reports published by it
30:15 - consulting firm gardner that rely on
30:18 - proprietary
30:19 - qualitative data analysis methods to
30:22 - demonstrate market trends such as
30:23 - direction maturity and participants
30:25 - people take these
30:27 - graphs very seriously and so
30:29 - this is what it looks like and as you
30:31 - can see amazon web services is marked as
30:34 - the leader and the closer you are to
30:36 - this
30:37 - top corner here is the better you are
30:39 - off as you can see microsoft is not too
30:42 - far behind followed by google then
30:44 - followed by alibaba cloud then by oracle
30:48 - ibm tencent which we don't uh ever talk
30:50 - about and then there's the other ones
30:52 - that just don't show up because they're
30:53 - so small like digital ocean and linoid
30:55 - there so generally that gives you kind
30:57 - of an idea how the market is growing and
30:59 - stuff like that um but as you can see
31:01 - you know there's still
31:02 - a lot for the other ones to do to catch
31:04 - up to aws okay
31:06 - [Music]
31:11 - so a cloud service provider can have
31:14 - hundreds of cloud services that are
31:16 - grouped into various types of services
31:18 - but the four most common types of cloud
31:20 - services for infrastructure as a service
31:22 - uh and i call these the four core would
31:25 - be compute so imagine having a virtual
31:28 - computer that can run applications
31:30 - programs and code networking so imagine
31:32 - having virtual network defining internet
31:35 - connections or network isolation between
31:37 - services or outbound to the internet
31:39 - storage so imagine having a virtual hard
31:41 - drive that can store files
31:43 - databases so imagine a virtual database
31:46 - for storing reporting data or a database
31:48 - for general purpose web applications and
31:51 - aws in particular has 200 plus cloud
31:54 - services
31:55 - and i want to clarify what cloud
31:58 - computing means because notice that we
31:59 - have cloud computing cloud networking
32:01 - cloud storage cloud databases
32:04 - but the industry
32:05 - often just says cloud computing to refer
32:07 - to all categories even though
32:10 - it has computer in the name so just
32:11 - understand when someone says cloud
32:12 - computing
32:13 - they don't just generally mean the
32:14 - subcategory they're talking about all of
32:16 - cloud okay
32:18 - [Music]
32:22 - so awes has a lot of different cloud
32:24 - services and i just want to kind of go
32:26 - quickly over the types of categories
32:28 - that we can encounter here and just
32:30 - mention the four core so any csp that
32:33 - has ias will always have these four core
32:36 - service offerings we have compute so
32:37 - nato s this would be ec2 vms storage
32:40 - this could be something like ebs virtual
32:42 - hard drives database so that could be
32:44 - rds sql databases networking and content
32:47 - delivery but really it's networking uh
32:50 - and this would be vpc so private cloud
32:52 - network okay so
32:54 - uh let's just look at all the categories
32:56 - that are outside the four core so there
32:57 - could be analytics application
32:59 - integration ar vr it was cost management
33:02 - blockchain business application
33:03 - containers customer engagement developer
33:06 - tools and user computing game tech iot
33:10 - machine learning management governance
33:12 - media services migration
33:14 - and transfer mobile quantum technologies
33:18 - robotics satellites security identity
33:20 - and compliance if there was more i would
33:22 - not be surprised but you can see there's
33:24 - a lot of stuff that's going on here
33:26 - [Music]
33:30 - so let's take a look at all the services
33:32 - that are available to us so if you're on
33:33 - the marketing website which is
33:34 - adabus.amazon.com
33:37 - what you'll see in the top left corner
33:38 - is products and so these are all the
33:41 - categories and for whatever we want if
33:43 - it's like ec2 we can go into here
33:46 - and we can read all about it so usually
33:48 - we'll have our overview all right and
33:52 - that's not very useful and then we'll go
33:53 - over to features and so this is can be
33:56 - kind of useful to get some basic
33:58 - information and pricing which is
34:00 - something you'll do a lot in aws is
34:02 - you're always going to be going to a
34:03 - service and looking up its price and so
34:06 - you'll make your way over here every
34:08 - single one is different a very important
34:10 - page would be like getting started so
34:12 - this will give you basic information but
34:14 - what i do is i like to go all the way
34:16 - down to the bottom here and find my way
34:18 - over to the documentation so i'll go
34:19 - here to documentation to get that deeper
34:22 - knowledge about that service and as you
34:24 - can see things get pretty deep with aws
34:27 - in terms of the information they have so
34:29 - hopefully that gives you an idea of the
34:31 - scope also when you're logged into aws
34:33 - and this will be when we create our
34:34 - account
34:35 - you can explore all the services this
34:37 - way as well so these are all the awesome
34:39 - services
34:40 - but you just notice that there's two
34:41 - ways to explore them where this is
34:44 - actually you just actually utilizing the
34:45 - services and then the marketing website
34:47 - is you reading about them and learning
34:49 - all about them okay
34:50 - [Music]
34:54 - hey this is andrew brown from exam pro
34:56 - and we are looking at the evolution of
34:57 - computing your cloud service provider
34:59 - has all of these offerings and the idea
35:01 - is that you need to choose the one that
35:03 - meets your use case a lot of times this
35:05 - all has to come around the utilization
35:07 - of space that's what we're trying to
35:08 - illustrate here in this section here and
35:10 - the trade-offs of why you might want to
35:12 - use some of these offerings okay
35:14 - for dedicated we're talking about a a
35:17 - physically a physical server wholly
35:19 - utilized by a single customer that's
35:21 - considered single tenant
35:23 - and uh for google cloud we're talking
35:25 - about
35:27 - single node clusters and bare metal
35:29 - machines where you have control of the
35:31 - virtualization so you can sell any kind
35:33 - of hypervisor or virtualization you want
35:35 - the system the trade-off here though is
35:37 - that you have to guess up front what
35:39 - your capacity is going to be and you're
35:41 - never going to 100 utilize that machine
35:43 - because it's going to have to be a bit
35:44 - under in case the utilization goes up
35:46 - that's you choosing the cpus and the
35:48 - memories you're going to end up
35:49 - overpaying because you're uh you'll have
35:51 - under underutilized server uh it's not
35:54 - going to be easy to vertically scale
35:55 - it's not like you can just say resize it
35:57 - because the machine you have is what you
35:58 - have right you can't add more i mean i
36:01 - suppose they can insert more memory for
36:03 - you but that's a manual migration so
36:05 - it's very difficult um and replacing the
36:08 - server is also very difficult okay so
36:11 - you're limited by the host operating
36:12 - system it's not virtualized so whatever
36:15 - is on there is on there
36:17 - and that's what your apps are going to
36:18 - have access to
36:19 - if you decide to run more than one app
36:21 - which is not a good practice for these
36:22 - kind of machines you're going to end up
36:25 - with resource sharing where one machine
36:27 - might utilize more than the others
36:28 - technically with a dedicated machine you
36:30 - have a guarantee of security privacy and
36:32 - full utility of the underlying resources
36:34 - i put an asterisk there because yes it's
36:36 - more secure but
36:38 - but it's up to you to make sure that
36:40 - it's more secure so you have that's up
36:42 - to your skills of security right whereas
36:44 - if you had a virtual machine or anything
36:46 - above that there's more responsibility
36:48 - on the cloud service provider to just
36:50 - provide a circuit secure machine and
36:52 - they can do a better job than you so why
36:54 - would you use a dedicated machine well
36:56 - maybe you're doing high performance
36:58 - computing where you need these machines
37:00 - like very close together and you have to
37:02 - choose what kind of virtualization you
37:03 - need to have okay
37:06 - so then we're looking at virtual
37:07 - machines the idea here is you can run a
37:09 - machine within a machine the way that
37:11 - works is we have a hypervisor this is a
37:13 - software layer that lets you run the
37:15 - virtual machines uh the idea here is now
37:17 - it's a multi-tenant you can share the
37:18 - cost with multiple customers you're
37:20 - paying for a fraction of the server uh
37:22 - you'll still end up overpaying for the
37:24 - unrealized virtual machine because a
37:25 - virtual machine is just like you have to
37:27 - still say how many vcpus how much memory
37:31 - and your app is you know you don't want
37:33 - an app that uses 100 right you want to
37:35 - use exactly the amount you need but you
37:36 - can see here you know there's still
37:38 - going to be some underutilization
37:40 - uh you're limited by the guest operating
37:42 - system now but now it's virtualized so
37:44 - at least it's very easy to uh
37:46 - possibly migrate away if you choose to
37:49 - run more than one app on a virtual
37:51 - machine it can still run into resource
37:54 - sharing conflicts
37:55 - it's easier to export or import images
37:57 - for migration it's easier to vertically
37:59 - or horizontally scale okay and virtual
38:02 - machines are the most common and popular
38:04 - offering for compute because people are
38:06 - just very comfortable with those then
38:08 - you have containers and the idea is you
38:09 - have a virtual machine running
38:11 - these things called containers the way
38:13 - they do that is similar to a hypervisor
38:15 - but instead you have um like here is a
38:17 - docker demon so it's just a um
38:20 - a container uh software layer okay to
38:23 - run those containers there's different
38:24 - kinds docker is the most popular
38:26 - and the great thing is you can maximize
38:27 - the uh the capacity because you can
38:31 - easily add new containers resize those
38:33 - containers use up the rest of the space
38:35 - it's a lot more flexible okay
38:37 - your containers will share the same
38:39 - underlying os but they are more
38:42 - efficient than multiple vms
38:44 - multiple apps can run side by side
38:45 - without being limited by the same os
38:47 - requirements and not cause conflicts
38:49 - during resource sharing so containers
38:51 - are really good but you know the
38:53 - trade-off is there a lot more work to
38:54 - maintain
38:55 - then you have functions
38:57 - functions go even step further and the
39:00 - idea is that you uh the the containers
39:03 - where we where we talked about that's a
39:05 - lot of work to maintain now the cloud
39:07 - service provider is taking care of those
39:09 - containers generally sometimes not it
39:11 - depends if it's serviced or not but the
39:13 - idea is that you don't even think about
39:15 - this is called service compute but you
39:17 - don't even think about
39:18 - uh the os or anything you just know that
39:20 - what your runtime is you run ruby or
39:22 - python or node and you just upload your
39:25 - code and you just say uh i want this to
39:27 - be able to run
39:29 - for this long
39:30 - and use this amount of memory okay
39:33 - you're only responsible for your code
39:34 - and data nothing else it's very cost
39:36 - effective you only pay for the time the
39:38 - code is running
39:39 - and vms only run when there is code to
39:41 - be executed but because of that there is
39:43 - this concept of cold starts and this is
39:45 - uh where the virtual machine has to spin
39:48 - up and so sometimes requests can be a
39:50 - bit slow so there's a bit of trade-off
39:51 - there but functions or serverless
39:53 - compute is generally one of the best
39:55 - offerings as of today but most people
39:57 - are still getting kind of comfortable
39:58 - with that paradigm okay
40:00 - [Music]
40:04 - hey this is andrew brown from exam pro
40:06 - and we are taking a look at the types of
40:07 - cloud computing and the best way to
40:09 - represent this is a stacked pyramid and
40:12 - we'll start our way at the top with sas
40:14 - also known as software as a service so
40:16 - this is a product that is run and
40:18 - managed by the cloud service provider
40:20 - you don't have to worry about how the
40:22 - service is maintained it just works and
40:24 - remains available so examples of this
40:26 - and actually uh the first company to
40:28 - coin this was actually salesforce uh
40:30 - then there's things like gmail office
40:32 - 365 so i think microsoft word excel
40:35 - things like that and they run the cloud
40:37 - okay and sas is generally designed for
40:40 - customers in mind
40:42 - then came along platform as a service
40:44 - also known as pass and these focus on
40:47 - the development or sorry the deployment
40:49 - and management of your apps so you don't
40:51 - worry about provisioning configuring or
40:53 - understanding the hardware or operating
40:55 - system
40:56 - and so here we'd have things like
40:58 - elastic beanstalk heroku which is very
41:01 - popular among developers that just want
41:03 - to launch their code or google app
41:05 - engine and that is the old logo but
41:07 - that's the logo i like to use because i
41:09 - think it looks cool and so these are
41:11 - intended for developers the idea is that
41:13 - you just deploy your code
41:15 - and the platform does the rest
41:17 - then there is infrastructure as a
41:19 - service
41:21 - there's no way to say that like it's
41:22 - easy to say sas or pass but there's no
41:24 - easy way to say iaas
41:27 - so this is the basic building blocks for
41:28 - cloud it it provides access to
41:30 - networking features computers and data
41:32 - storage space and the idea here is you
41:34 - don't worry about the it staff data
41:36 - centers and hardware and so that would
41:39 - be like microsoft azure aws
41:42 - oracle cloud things like that and these
41:44 - are for administrators okay so there you
41:46 - go
41:48 - [Music]
41:52 - hey this is andrew brown from exam pro
41:54 - and we are taking a look at cloud
41:55 - computing deployment models starting
41:57 - with public cloud and the idea here is
42:00 - that everything when i say everything
42:01 - i'm talking about the workloads the
42:03 - projects the code is built on the cloud
42:05 - service provider so here is a diagram
42:08 - where we have a ec2 instance a virtual
42:11 - machine running our application and then
42:13 - we have our database in rds and we have
42:16 - the internet coming into our aws account
42:18 - and so everything is contained all of
42:20 - our infrastructure is within aws all
42:23 - right
42:24 - and so this is known as being cloud
42:26 - native or cloud first and i put an
42:28 - asterisk beside cloud native because
42:30 - that was a term uh that was uh used
42:33 - prior to classroom providers to refer to
42:35 - containers or open source um
42:38 - models being deployed and being mobile
42:41 - other places so just understand that it
42:42 - has two meanings but in the context of
42:44 - this cloud native just being like native
42:46 - to the cloud like using cloud to begin
42:48 - with okay
42:49 - then we have private cloud so everything
42:51 - built on a company's data center uh and
42:54 - being built on a data center is known as
42:56 - being on premise because that is where
42:58 - the data center resides near where you
43:00 - work
43:01 - and so here you could be using cloud but
43:03 - you'd be using openstack which would be
43:05 - a private cloud so here we have our
43:07 - on-premise data center and the
43:09 - internet's coming into our data center
43:11 - and we're running on openstack where we
43:13 - can launch virtual machines and a
43:14 - database okay
43:17 - then there's the concept of a hybrid
43:19 - cloud so using both on-premise and a
43:21 - cloud service provider together and so
43:24 - the idea here is we have our on-premise
43:26 - data center and then we have an
43:28 - established connection maybe it's a vpn
43:30 - connection maybe it is a direct
43:32 - connection um but the idea is that we're
43:34 - bridging that connection and utilizing
43:37 - both our private and our public uh stuff
43:40 - to uh create a cloud workload then there
43:44 - is a fourth one called crosscloud
43:46 - sometimes it's known as multi-cloud
43:49 - and sometimes it's erroneously referred
43:51 - to as hybrid cloud but it generally is
43:53 - not uh hybrid cloud okay the idea here
43:56 - is when you're using multiple cloud
43:57 - providers and so one example here could
44:00 - be using services like azure arc so
44:03 - azure arc allows you to extend your
44:05 - control plane uh so that you can deploy
44:08 - containers for kubernetes in
44:11 - azure
44:12 - within amazon eks within gcp kubernetes
44:16 - engine but you know being cross cloud
44:18 - doesn't necessarily mean that you're
44:20 - running a
44:21 - using a service that used works across
44:23 - the cloud and manages it it could just
44:24 - mean using multiple providers at the
44:26 - same time
44:27 - another service that is similar to azure
44:29 - arc but is for a google cloud
44:31 - platform is also known as anthos
44:34 - aws has traditionally not been um
44:37 - cross-cloud friendly and so we haven't
44:39 - seen any kind of developments there
44:41 - where we see uh these other services
44:43 - that are or cloud service providers
44:45 - behind aws trying to promote it to
44:48 - grab more of the market share okay
44:51 - [Music]
44:55 - so let's talk about the different
44:56 - deployment models and what kind of
44:58 - companies or organizations are still
45:00 - utilizing uh for these particular
45:02 - categories so for cloud again this is
45:04 - where we're formally utilizing cloud
45:05 - computing hybrid is a combination of
45:08 - public cloud and on-prem or private
45:10 - cloud and then on-prem is deploying
45:12 - resources on-premise using
45:13 - virtualization resource management tools
45:15 - sometimes called private cloud or it
45:17 - could be utilizing something like
45:18 - openstack so for companies that are
45:20 - starting out today or are small enough
45:22 - to make the leap from a virtual private
45:24 - server to a cloud service provider this
45:26 - is where we're looking at cloud so we're
45:28 - looking at startups sas offerings new
45:30 - projects and companies so maybe this
45:32 - would be like base camp dropbox
45:34 - squarespace then for hybrid these are
45:36 - organizations that started with their
45:37 - own data center but can't fully move to
45:39 - cloud due to the effort or migration or
45:41 - security compliance so we're talking
45:43 - about banks fintech investment
45:44 - management large professional service
45:46 - providers legacy on-prem so maybe cibc
45:49 - which is a bank deloitte
45:51 - the ccp or cpp investment board
45:55 - and then for on-premise these are
45:56 - organizations that cannot run on cloud
45:58 - due to strict regulatory compliance or
46:00 - the share size of the organization or
46:02 - they just have like an outdated idea of
46:05 - what cloud is so they just have a lot of
46:07 - difficulties in terms of politics
46:09 - adopting cloud
46:10 - so this would be public sector like
46:12 - government super sensitive data like
46:13 - hospitals large enterprise with heavy
46:16 - regulation insurance companies um so
46:18 - again hospitals maybe aig the government
46:21 - of canada
46:22 - and so i shouldn't say that they aren't
46:24 - using cloud but um
46:26 - you know because uh aws and all the
46:29 - cloud providers have um uh public sector
46:32 - offerings so
46:34 - um you know i'm just trying to stage as
46:36 - an example of things that could be still
46:37 - using on-premise so you know i know the
46:40 - government canada definitely uses uh
46:42 - cloud in a lot of ways same with aig and
46:45 - hospitals but you know generally these
46:46 - are the last holdouts of on-prem because
46:48 - there really isn't a a good reason to be
46:51 - fully on premise anymore
46:53 - but again there are some things that are
46:55 - still doing that okay
46:56 - [Music]
47:00 - hey this is andrew brown from exam pro
47:02 - and we are at the start of our journey
47:04 - creating ourselves an aws account so
47:06 - what you need to do is go to
47:07 - aws.amazon.com
47:09 - if you don't have a lot of confidence
47:10 - how to get there just type in adabus
47:12 - into google and then click here on the
47:14 - link where it says adabusamazon.com
47:16 - it'll take you to the same place now
47:18 - notice we have a big orange button in
47:20 - the top right corner so it says sign
47:22 - into the adwords console
47:24 - it's the if it's the first time you've
47:26 - ever been to this website so if i go to
47:28 - adabus.amazon.com
47:29 - incognito it will have the create
47:32 - enables account button
47:34 - i don't know why they don't keep this
47:35 - consistent across the board but i wish
47:37 - they did but if you are on the screen
47:39 - you can click here or there um but if
47:41 - you do see something that doesn't say uh
47:44 - you know create an account or or et
47:45 - cetera you can just sign in
47:48 - okay and then down below you can hit
47:50 - create a new aws account so that's the
47:52 - way you're going to get in there and so
47:54 - you're going to put an email a password
47:56 - and create a database account name
47:58 - i've created this so many times and it's
48:00 - so hard to set up new emails i'm not
48:02 - going to do this again it's not
48:03 - complicated but one thing i need to tell
48:05 - you is that you do need to have a credit
48:07 - card you cannot create an account
48:08 - without a credit card um and for those
48:11 - who are in places where maybe you don't
48:13 - have a traditional credit card maybe you
48:15 - can get a prepaid one so up here in
48:16 - canada we have a company called coho and
48:19 - so coho is
48:21 - a visa debit card and so it's basically
48:24 - a virtual prepaid credit card and so
48:26 - these do work on the platform as well so
48:28 - if you have a traditional credit card or
48:30 - possibly could find one of these you
48:32 - still have to load up with money but it
48:33 - does give you a bit more flexibility to
48:34 - create that account so
48:36 - what i want you to do is go through that
48:37 - process yourself it's not complicated
48:40 - and i'll see you on the other end okay
48:42 - [Music]
48:46 - so once you've finished creating your
48:48 - account you should be within the adwords
48:49 - management console and this is the page
48:51 - you're always going to see when you log
48:53 - in it's always going to show the most
48:55 - recent services here
48:57 - and you'll notice in the top right
48:58 - corner that i have my account called
49:00 - exam pro if you're wondering how do you
49:03 - change that name what you do is to go to
49:05 - my accounts here and once there you'll
49:07 - have your account settings up here if
49:09 - you go to edit
49:11 - you can change that name here okay so
49:14 - you know sometimes when you create your
49:16 - account you don't like the account name
49:17 - that you gave it and so that's your
49:18 - opportunity to fix it
49:20 - but once we're in our account what i
49:22 - want you to do is immediately log out
49:24 - because i want you to get familiar with
49:26 - the way you log into aws because it is a
49:29 - bit um different than other providers
49:32 - and so i don't want you to
49:33 - get hung up later on with your account
49:35 - so i've logged out i'm going to go ahead
49:37 - and log back in so you can click the
49:39 - orange button or what i like to do is
49:41 - drop down my account
49:43 - and go to aws management console
49:45 - it's a lot more clear and you'll notice
49:47 - we're going to have two options root
49:48 - user and iam user so
49:51 - this is what i'm talking about for the
49:52 - confusion so when you log into your root
49:55 - user account you all are always using an
49:58 - email and when you're logging as an
50:00 - imuser you're actually going to be
50:01 - entering the account id or account alias
50:04 - but what we'll do is go to the root user
50:05 - and this is the email you use to sign up
50:07 - with the account so for me
50:10 - i
50:10 - called this one andrew plus sandbox at
50:13 - exam pro dot co i'm gonna go to next
50:16 - sometimes you get this character box
50:18 - it's very annoying but it happens time
50:20 - to time and so what i'm gonna do is just
50:22 - go ahead and type that in
50:24 - okay and hopefully it likes it and then
50:28 - i'm just going to enter in my password
50:31 - all right
50:32 - and i'll be back into my account and so
50:33 - notice it takes me back to about
50:35 - management console so the root account
50:37 - is not something we want to be generally
50:39 - using except for
50:41 - very particular use cases and we do
50:43 - cover that in the course but what i want
50:46 - you to do is go set yourself up with a
50:48 - proper account and so
50:50 - what we'll do is go to the top here and
50:52 - type in iam and this stands for identity
50:54 - and access management and we'll click on
50:56 - iem here
50:58 - and on the left hand side we're going to
51:00 - see a bunch of options here
51:02 - and so notice right away we get to the i
51:04 - am dashboard where it's going to start
51:06 - to make some recommendations for us the
51:08 - first one is always to add mfa
51:10 - multi-factor authentication
51:12 - another thing you can do is set an
51:14 - account alias so you can see that i've
51:15 - set one here prior so if i just go ahead
51:17 - and remove it the way we'd have to log
51:20 - in is via the account alias
51:22 - which is the same as the account id and
51:24 - so i don't really like that so i can
51:25 - just rename it to deep space nine
51:28 - and these are unique so you have to pick
51:30 - something that is unique to you so it
51:32 - could be your company name or things
51:33 - like that it's gonna make it a lot
51:35 - easier to log in
51:37 - when we create our additional user here
51:38 - so we'll come back to mfa at some point
51:40 - here what i want you to do is go over to
51:42 - users and go ahead and make yourself a
51:44 - new user
51:46 - and so i'm going to call this one andrew
51:47 - brown
51:49 - and i'm going to enable programmatic
51:50 - access i'm going to enable aws
51:53 - management console so this one's going
51:54 - to allow me to use the apis to
51:56 - programmatically work with aws and this
51:58 - one here is going to allow me to just
52:00 - log into the console which is pretty
52:02 - fair here so now that i have this we can
52:05 - auto generate it or give it a custom
52:06 - password i'm just going to auto generate
52:08 - it for the time being and here it says
52:09 - you must create a new password at the
52:11 - next sign in which sounds fair to me
52:13 - and we go ahead and create ourselves a
52:15 - new group so it's pretty common to
52:17 - create a group called admin and notice
52:19 - here this is where we're going to have a
52:21 - bunch of different policies so the first
52:23 - one here which is admin and access
52:25 - provides full access to able services
52:27 - and resources and this pretty much gives
52:28 - you
52:29 - almost nearly almost the same
52:32 - capabilities as the um aws root user
52:35 - account
52:36 - and so that's going to be okay because
52:38 - we are an admin in our account so i'll
52:40 - check box that on but i just want to
52:42 - show you here if you drop down filter
52:43 - policies and you went to invest manage
52:46 - job functions these are a bunch of
52:48 - pre-made
52:49 - aws
52:51 - policies that you could apply
52:53 - to different users so what's really
52:55 - popular after the administrator access
52:57 - is to usually give the power user access
52:59 - and so this one allows
53:01 - a user to do basically anything they
53:04 - want with the exception of management of
53:05 - users and groups so you know it could be
53:08 - that that's something that you'd want to
53:10 - do for some of your users i just don't
53:11 - want to have any trouble so i'm going to
53:12 - give us
53:13 - admin access here and we're going to go
53:15 - ahead and create this group
53:18 - and so here is the group that we are
53:20 - creating we're going to go next we can
53:22 - apply our tags if we want i'm not going
53:24 - to bother we hit next review and then
53:26 - hit create user
53:28 - all right and so now what it's doing is
53:30 - it's showing us the access id and the
53:32 - access key secret that we can use to
53:34 - pragmatically access aws and then
53:36 - there's a password here so i'm going to
53:37 - go ahead and show it and what i'm going
53:39 - to do is just copy this into a clipboard
53:42 - anywhere
53:48 - and so i'm just copying that off screen
53:51 - here because i'm going to need it to log
53:52 - in and i'm just going to remember my
53:54 - username as well alright and so what
53:56 - we'll do is go ahead and hit close
54:00 - so what i'll do is go back to my
54:02 - dashboard here and remember i set my
54:04 - account alias as deep space 9 but we
54:05 - could also use the account id to log in
54:08 - i'm just going to grab my account id off
54:09 - screen here and what i want to do now is
54:12 - go ahead and log out and now log into
54:15 - this im user and this is the one that
54:17 - you should always be using within your
54:19 - aws account you shouldn't be using your
54:21 - root user account so what i'll do is go
54:23 - over to i am user here and notice now
54:25 - that it says account id so 12 digits or
54:28 - the account alias so here i can enter in
54:31 - these numbers here or i can enter in my
54:34 - alias which is deep space 9 and again
54:37 - you'll have to come up with your own
54:38 - creative uh one there for yourself and
54:41 - we'll go ahead and hit next and so
54:43 - notice what it's going to do is now ask
54:44 - me what my imuser name is so i defined
54:47 - mine as andrew brown
54:48 - and then we had an auto-generated
54:51 - password there so that we had saw and so
54:53 - i'm going to place that in there we'll
54:55 - go ahead and hit sign in
54:57 - and so now right away it's going to ask
54:58 - me to reset the password so i'm going to
55:00 - put the old password in there and so now
55:02 - i need a new password i strongly
55:04 - recommend that you generate out
55:06 - your passwords to be very strong i like
55:08 - to go to password generator and i'll
55:10 - drop this down and i'll do something
55:12 - really long like 48 characters and
55:15 - if you don't like uh weird characters
55:17 - you can take those out there sometimes
55:19 - it loads here so you gotta try it twice
55:21 - um and we're gonna go down to whoops 48
55:25 - there we go and so that's pretty darn
55:26 - long so i'm going to copy that off
55:28 - screen here so i do not forget
55:31 - and you probably would want to put this
55:32 - in a password manager something like
55:34 - dashlane or some sort of thing like that
55:38 - and we'll go ahead and we will paste
55:40 - that in and we'll see whoops i don't
55:43 - want google to save it
55:45 - and we'll see if it takes it and so
55:47 - there we go so what i'll do is now log
55:50 - out
55:51 - and i'll make sure my new password works
55:54 - because you really don't want to have
55:55 - problems later so we'll type in deep
55:56 - space nine
55:58 - andrew brown again this is going to be
56:00 - based on what your
56:02 - what you have set
56:04 - and we'll go ahead and log in and there
56:05 - i am and so now notice that it doesn't
56:08 - say
56:09 - example or whatever it says andrew brown
56:11 - at deep space nine so it's using the
56:12 - account alias and showing the name and
56:14 - that's how i'm going to know whether i'm
56:15 - the root account user or whether i'm
56:18 - logged in as an iam user all right
56:21 - so there we go
56:22 - [Music]
56:27 - okay so now that we have the proper user
56:30 - account to log in i just want to point
56:32 - out
56:33 - about regions so in the top right corner
56:35 - you'll notice it says north virginia
56:36 - here it possibly will say something
56:39 - completely else for you but what you'll
56:41 - do is you'll click and drop that down
56:43 - and you'll see a big list of regions and
56:45 - so sometimes when i log into aws it
56:48 - likes to default me to u east uh us east
56:51 - ohio but i honestly like to launch all
56:53 - my stuff in u.s east north virginia even
56:56 - though i'm in canada i probably should
56:57 - be using the canada central region down
56:59 - here
57:00 - but the default region is going to be
57:02 - based on your locality okay so just
57:04 - understand that it might be different i
57:07 - strongly recommend for
57:08 - all of our follow alongs you run in u.s
57:11 - east one because usc swan is the
57:13 - original
57:14 - the original region and it also has the
57:18 - most access to aws services and some aws
57:21 - services um such as like billing and
57:24 - cost and things like that are only going
57:25 - to show up in u.s east north virginia so
57:29 - just to make our lives a lot easier
57:30 - we're going to set it there but i want
57:32 - you to understand that some services are
57:34 - global services meaning that it doesn't
57:36 - matter what region you're in it's going
57:37 - to default to global and one example
57:39 - could be cloudfront so if i jump over to
57:41 - cloudfront here for a moment
57:44 - and we do seem to have uh some
57:47 - cloudfront distributions here from a
57:49 - prior
57:50 - follow along but notice up here that it
57:52 - now says global so cleft front does not
57:54 - require a region selection let's make
57:56 - our way over to s3
58:00 - all right and this one's also global so
58:03 - again this one does not require a region
58:05 - selection but if you go over to
58:07 - something like ec2
58:10 - okay this has a region dependency so
58:13 - just be really careful about that
58:15 - because a lot of times you'll be doing a
58:17 - follow along and you'll be like why
58:19 - aren't these resources here or whatever
58:20 - and it's because this got switched on
58:22 - you and it can happen at any time so
58:23 - just be
58:24 - cautious or aware of that okay
58:26 - [Music]
58:30 - so one of the major advantages of using
58:32 - aws or any cloud service provider is
58:35 - that it utilizes metered billing so that
58:38 - is different from a fixed cost where
58:39 - you'd say okay i want a server for x
58:41 - amount of dollars every month but the
58:43 - way nms works is that it's going to bill
58:45 - you on the hour on the second based on a
58:48 - bunch of factors and so you're going to
58:50 - be able to get services at a lower cost
58:52 - however if you choose an expensive
58:55 - service and you forget about it or if
58:58 - there's misconfiguration where you
58:59 - thought you were launching something
59:01 - that was cost effective but turned out
59:03 - to be very expensive you could end up
59:04 - with a very large bill very very quickly
59:07 - and so
59:08 - that is a major concern for a lot of
59:10 - people utilizing cloud but there's a lot
59:12 - of great toolings built into aws to
59:15 - allow you to catch yourself if you
59:17 - happen to make that mistake and before
59:19 - we go ahead and learn how to do that i
59:22 - want to show you
59:23 - some place where you could end up having
59:26 - excessive spend without knowing it so
59:28 - one example and this is actually
59:30 - happened to me when i first started
59:31 - using aws
59:33 - before i even knew about all the billing
59:35 - tools is i wanted to launch a redis
59:37 - instance and so you just have to watch
59:40 - you don't have to do this but
59:42 - elasticash is a service that allows you
59:43 - to launch either a memcache or redis uh
59:46 - database and i just wanted to store a
59:48 - single value and so i went here and i
59:52 - scrolled down it looked all good and i
59:54 - hit create but i wasn't paying attention
59:56 - because apparently it was like the
59:57 - default the node type here to the cache
60:01 - r6g.large all right and
60:04 - you know you might think that a bus has
60:06 - your best interest in play and most
60:08 - services are pretty good they make sure
60:10 - that they're either free or very low
60:11 - spend but some of these and elastic is
60:13 - an older service where they just have
60:16 - these weird defaults so
60:18 - um you know if we were to go look up
60:20 - this the rg6
60:23 - large
60:24 - all right and look at its spend
60:30 - all right and we would go over here
60:32 - whoops
60:33 - i think i went to the china one
60:34 - but if we were to go over here and look
60:37 - for that instance i'm just trying to
60:38 - find it here for cost
60:40 - this one down below
60:43 - um this doesn't say pricing does it say
60:46 - our pricing here
60:49 - here it is so this one cost
60:51 - um
60:53 - this one costs about two cents per hour
60:55 - it doesn't sound like a lot but if we go
60:57 - here and we do the math we say 7 30 7 30
61:00 - is the amount of hours in a month that
61:02 - is 150
61:04 - okay so if you don't know about that and
61:06 - forget about that that's gonna be 150
61:08 - and i'm going to tell you that it used
61:10 - to be a lot higher i'm pretty sure they
61:12 - used to have it defaulted to something
61:13 - like
61:14 - like this or that because i remember i
61:16 - did this
61:17 - and i had a bill that came in that was
61:19 - like 3 000 usd dollars and i'm in canada
61:22 - so like 3 000 usd is like a million
61:24 - dollars up here and so i remember um it
61:28 - was a big concern and i freaked out but
61:30 - that was okay because all i had to do
61:31 - was go to support
61:33 - and what i had done is i went to the
61:35 - support center
61:40 - and i had opened a support case and i
61:43 - just said hey i have this really big
61:45 - bill so you go here right
61:47 - and you look for billing
61:49 - and
61:50 - you look for something like charging
61:52 - query or misspend and you say you know
61:56 - um
61:58 - you know like help
61:59 - my bill's too high
62:02 - and you just say like you explain the
62:04 - problem saying hey you know i was using
62:06 - elastic cash and it was set to a large
62:08 - default and i wasn't aware about it can
62:10 - you please give me back the money and
62:11 - the great thing is that aws is going to
62:13 - give you a free pass if it's your first
62:15 - time where you've had a misspending they
62:17 - generally will say
62:19 - okay you know don't do it again and if
62:22 - it happens again you will get billed but
62:23 - go ahead and learn how to set up billing
62:25 - alerts or things like that okay so just
62:27 - so you know don't freak out if you do
62:29 - have a really high bill you're going to
62:30 - get a single free pass but now that we
62:32 - know that let's go learn how to set up a
62:35 - budget okay
62:37 - [Music]
62:41 - all right so now that we've had a bit of
62:43 - a story about
62:45 - over span for misconfiguration let's
62:48 - learn how to protect ourselves against
62:49 - it and we're going to go ahead and set
62:52 - up a budget so go to the top here and
62:54 - type in budget
62:56 - and what that will do is bring us over
62:57 - to the billing dashboard another way to
62:59 - get here is to go click at the top here
63:01 - and go to my billing dashboard and then
63:03 - you'll see the left-hand menu here
63:06 - and so the great thing about budgets is
63:08 - that the first two are free it says
63:10 - there is no additional charge for any of
63:11 - his budgets you pay for configured use
63:13 - usage but i'm pretty sure that that's
63:15 - not true
63:17 - because it used to be
63:19 - abs budget reports okay so that costs
63:21 - something
63:23 - it used to be that aws budgets um after
63:26 - subscription enabled will occur 10 cents
63:28 - daily so in addition to budget monitor
63:30 - you can add actions to your budgets
63:32 - the first two action-enabled budgets are
63:35 - free okay so just be aware that just
63:37 - because it says there's no additional
63:39 - charge read into it because sometimes
63:41 - the fine line will tell you it does cost
63:43 - something but i know that the first two
63:45 - are free what we'll do is go ahead and
63:46 - create a budget i'm going to close these
63:48 - other tabs here since we have no need
63:50 - for them and we're going to be presented
63:51 - with a bunch of budget types
63:53 - we're concerned about cost today so
63:54 - we're going to go with a cost budget
63:57 - and notice we can change the period from
63:58 - monthly to daily to quarterly to
64:00 - annually if you change it to daily um
64:02 - you won't get forecasting so i don't
64:04 - want that today but a monthly is pretty
64:06 - good you can have a reoccurring which is
64:08 - strongly recommended and then you can
64:10 - put a fixed cost notice that i already
64:12 - have some spend on this account so it
64:14 - was like 25 bucks last month i'm going
64:16 - to set it my budget here to a hundred
64:18 - dollars
64:20 - and you can add filters here to um
64:24 - filter that cost out so if you want to
64:25 - say only for this region or things like
64:27 - that you could do that
64:28 - uh notice that this is my spend over
64:30 - here um so this is my budget and that's
64:32 - the actual cost notice my cost has been
64:34 - going up the last few months because
64:35 - i've been doing things with this account
64:37 - and so i'll do is say simple budget here
64:41 - we'll hit next
64:44 - and so now it's asking us if we want to
64:46 - configure alerts we probably do so you'd
64:48 - hit add alert and then you'd set a
64:50 - threshold like 80 percent
64:52 - or you could say an absolute value
64:54 - and then you put in your emails like
64:55 - andrew exam pro dot co
64:58 - and i want to point out that this is
65:00 - using um
65:02 - it was sns
65:04 - or it should be anyway so amazon sns has
65:06 - no upfront cost based on your stuff here
65:08 - so even though you're filling out an
65:09 - email
65:10 - you know
65:11 - and maybe it doesn't show it but i'm
65:13 - pretty sure that this would create an
65:14 - sns
65:16 - topic but what we'll do is hit next here
65:18 - we have an alert so we're just
65:21 - reviewing actually this is for attaching
65:22 - any action so maybe we want some kind of
65:25 - follow-up thing to happen here so we say
65:27 - add action
65:28 - and
65:29 - uh
65:30 - requires specific i am permissions on
65:32 - your behalf
65:34 - okay sure
65:36 - so i guess you could follow up actions
65:38 - that's no different than
65:39 - um on a building alarm but we're not
65:42 - really worried about that right now
65:44 - i'm not going to bother with an action
65:47 - we'll go ahead and create a
65:48 - budget and so here it's going to say
65:51 - that our budget is 100 it's going to
65:52 - show us the amount used forecast amount
65:54 - current budget sometimes this takes time
65:56 - to show up so i'm going to hit refresh
65:59 - and see if it shows up yet
66:02 - there we go so notice we have forecast
66:04 - amount 23 current budget etc forecasted
66:07 - budget
66:08 - uh forecasted versus budget so it's
66:10 - pretty straightforward on how that works
66:12 - i'm just curious if it actually created
66:14 - an sns event so i'm going to go over
66:16 - here
66:17 - because a lot of services utilize sns so
66:20 - if i go over here
66:21 - default cloud watch alarm um
66:24 - so i think this is something i had
66:26 - created before so i'm gonna go ahead and
66:28 - just delete it
66:29 - so default cloudwatch alarms
66:31 - actually i'm going to just click into
66:32 - here and see what i have
66:35 - confirmed
66:37 - so i think it might have used this when
66:39 - we created it but um the reason i'm
66:41 - bringing up sns is that there's a lot of
66:43 - services that allow you to
66:45 - email yourself for alerts and it always
66:47 - integrates with this service and so i
66:48 - just want kind of want to point that out
66:50 - so that you remember what sns is for
66:52 - but yeah so setting up a budget is not
66:54 - too hard so there you go
66:59 - [Music]
67:00 - all right so now that we've set a budget
67:02 - what i want to talk to you about is the
67:04 - free tier and the free tier is something
67:06 - that is available to you uh for the
67:08 - first 12 months of a new abs account and
67:10 - allows you to utilize the services
67:12 - without incurring any cost to you and so
67:15 - it's in your advantage to utilize this
67:17 - free tier
67:18 - as you are experimenting and learning
67:20 - cloud so if you want to learn about all
67:22 - the offerings what you do is go to
67:23 - google type in aws free tier and you'll
67:25 - get this page that explains all the
67:27 - sorts of things here so you can get
67:30 - 750 hours on ec2 rds things like that
67:33 - there are stipulations in terms of what
67:35 - it would be so here this is a t2 or t3
67:38 - michael mic
67:39 - micro running linux red hat
67:42 - or other type of os's okay
67:45 - so there are details you have to read
67:47 - the fine print some services are only
67:49 - available for the first two months
67:51 - things like that so it's going to highly
67:54 - vary based on service but it's worth
67:56 - giving us a read in areas that you are
67:58 - interested in now the thing is is how do
68:00 - you know that you are still in the free
68:02 - tier or you go outside of it and that's
68:04 - what i want to talk to you about right
68:06 - now so i am actually in another aws
68:08 - accounts that knows in the top right
68:09 - corner says brown.lap or hyphen laptop
68:11 - exam pro dot co sometimes i will switch
68:14 - into different abs accounts during these
68:15 - follow along so i can best show you
68:18 - um you know these settings so if you
68:20 - make your way over to billing
68:23 - and actually i should show you up here
68:25 - if we go to my dealing dashboard just
68:27 - trying to be consistent here and you go
68:29 - to the left-hand side to billing
68:30 - preferences what you can do is enable
68:33 - receive free tier usage alerts and then
68:35 - put your email in there and save that
68:37 - and so turn on this feature to receive
68:39 - email alerts a when your abs service
68:41 - usage is approaching or exceeded
68:43 - database free tier usage limits if you
68:44 - wish to receive these alerts etc etc etc
68:48 - right
68:48 - and while you're there
68:50 - i want you to also check box receive
68:52 - billing alerts
68:54 - so i can show you how to set a billing a
68:56 - billing alert and adabas says you know
68:58 - budgets are a new thing but billing
69:00 - alerts are still something that we use
69:02 - as of today so if you checkbox that on
69:04 - we'll be able to see your cost if we go
69:07 - back here
69:08 - it should show you
69:09 - um it's because i'm out of the free tier
69:11 - on this account but it would show you in
69:14 - the alerts you know your usage there so
69:16 - example here is if we scroll down this
69:18 - is the documentation tracking your image
69:20 - free tier usage you would see like a box
69:23 - like this and would say hey your free
69:24 - tier usage limit is here and you're over
69:27 - it okay so that generally would show up
69:30 - on this panel here but again i'm outside
69:32 - of the free tier so i'm not seeing it
69:34 - here
69:36 - today okay
69:38 - so you know hopefully that is clear
69:40 - um but yeah there you go
69:43 - [Music]
69:47 - all right so we created ourselves a
69:49 - budget we're monitoring our free tier
69:52 - but there's another way that we can
69:53 - monitor our spend and that is through
69:55 - building alerts or alarms and it is the
69:58 - old way before we had it was budgets
70:01 - this was the only way you could do it
70:02 - but i still recommend it because there
70:04 - is a bit more flexibility here with this
70:06 - service and so i wanted to teach you
70:08 - early on so that you know what's
70:09 - available to you or if you want to play
70:11 - around with it in the future so what
70:13 - you'll do is go to the top here and type
70:15 - in cloudwatch
70:17 - and cloudwatch is one of those services
70:18 - where it's actually a collection of
70:20 - services so there's cloudwatch alarms
70:22 - cloudwatch logs cloudwatch metrics those
70:24 - are all individual services and animus
70:27 - loves to update
70:29 - their interface so sometimes you'll be
70:30 - presented this option to
70:32 - change the latest interface i'm going to
70:34 - try out the new interface here
70:36 - and that is one challenge with databases
70:37 - you always have to expect that they're
70:39 - going to change the ui on you and you're
70:41 - going to work through it so just
70:42 - understand that i try to keep my videos
70:44 - up to date as best i can but part of the
70:47 - challenge is getting used to that so
70:48 - this is what they have today i don't
70:50 - know if they're going to stick with this
70:51 - but this is what it looks like but what
70:53 - i want you to do is make your way over
70:54 - to alarms on the left hand side
70:57 - and notice that we actually have a
70:58 - section just for billing which is
71:00 - interesting i remember them having that
71:01 - before so it's new so uh here it says it
71:04 - was cloudwatch help can help you monitor
71:06 - the charges of the spill remember that
71:08 - we had to turn that on get 10 free
71:10 - alarms with a thousand free email
71:12 - notifications each month as part of the
71:14 - free tier so understand that if you
71:16 - create billing alarms they do cost money
71:19 - um as well if you go over that limit but
71:21 - you sure get a lot 10 free alarms is
71:23 - quite a bit but we'll do is go ahead
71:25 - here and create our sales alarm we're
71:26 - going to go and choose a metric and so
71:29 - here are the options we could choose
71:30 - from and so we i think would like um
71:35 - billing
71:36 - and so we can do by service or total
71:39 - estimated charge we're going to do a
71:40 - total estimated charge we can only
71:42 - select usd i've never seen any other
71:44 - currency ever there and so here we kind
71:46 - of get this little graph where we can
71:48 - see stuff
71:49 - but this is a lot more powerful than
71:51 - budgets because you can do anomaly
71:52 - detection uh so like here it will
71:55 - actually check base between a range as
71:57 - opposed to just going through a
71:59 - particular value but what i'll do is
72:00 - just set a value here like
72:02 - fifty dollars right
72:05 - so notice that it sets the line up here
72:07 - and this is my current spend here right
72:09 - and so back to anomaly detection this is
72:11 - a lot smarter so
72:13 - the idea is that if something is outside
72:15 - this band of a certain amounts then it
72:17 - would alert okay
72:19 - but i'm going to go back here i'm just
72:21 - going to set this to 50
72:23 - and that looks okay to me you can change
72:25 - the period six hours is fine um
72:29 - there's additional configuration that's
72:30 - fine as well we're going to go ahead and
72:32 - hit next
72:33 - and so the idea is that you know if it
72:36 - passes that red line it will go to an in
72:38 - alarm state and then what it will do is
72:41 - uh we want to
72:43 - have it to trigger an sns topic so i
72:46 - would generally just create a new one
72:47 - here and we'll just say my billing alarm
72:51 - okay and then here we'll just set the
72:53 - email
72:54 - and your exam pro.co
72:56 - and we'll go ahead and create that topic
73:00 - and so that is now set i don't know if
73:02 - it would confirm it we might have to go
73:03 - to our email to confirm it so notice it
73:05 - says pending confirmation so what it has
73:07 - done is it sent me out an email and it
73:10 - wants me to click that link to confirm
73:13 - that i want to subscribe to it so i
73:15 - might just do that off screen to show
73:16 - you here okay
73:19 - so i'm just going to pull up my email
73:20 - here just give me a moment
73:25 - okay and so if i come back here this is
73:27 - the email that came in so i'm just going
73:28 - to confirm that subscription says i'm
73:31 - confirmed good
73:32 - and if i refresh this page
73:35 - we can now see that that is confirmed
73:37 - all right
73:38 - so we'll scroll down here so we can
73:40 - trigger an auto scaling action so maybe
73:43 - you know if you have too many servers
73:44 - you say hey the cost is too much shut
73:46 - down those servers there's ec2 actions
73:48 - things like that so these are kind of
73:49 - similar to
73:51 - budgets right
73:53 - they're system manager actions i imagine
73:55 - all these things are available in
73:56 - budgets as well but budgets just makes
73:58 - it a little bit easier to look at so i'm
73:59 - going to say my simple building alarm
74:02 - here
74:05 - we'll hit next
74:07 - all right
74:08 - we'll hit create alarm
74:09 - and there you go so billy alarms don't
74:11 - have like forecasting things like that
74:14 - um but you know they are they do have
74:16 - their own kind of special utility and so
74:18 - i utilize both okay so there we go let's
74:21 - go back to our management console move
74:22 - on to the next one
74:26 - [Music]
74:28 - so one of the strongest recommendations
74:29 - that abuse gives you is to say to set
74:32 - mfa on your database root user account
74:35 - so that's something we're going to do
74:36 - right now so make sure you're logged
74:38 - into the root user account so i'm going
74:39 - to go log out as my im user i'm going to
74:42 - go back and log in
74:44 - and i'm going to log in as my root user
74:47 - here so to do that no sometimes it will
74:50 - be expanded as the imuser click and sign
74:53 - into root user here we'll have root user
74:55 - i'm going to go ahead and enter my email
74:57 - that i used
74:58 - and if you do switch accounts frequently
75:00 - they will ask you these silly captchas
75:03 - which drive me crazy but uh you know it
75:05 - happens you probably won't encounter it
75:07 - as much as i do and so i'm going to go
75:09 - ahead and grab my password here and
75:11 - paste it on in
75:13 - and so now that i'm in what i want to do
75:15 - is make my way over to iam
75:18 - and i'm going to go and look for
75:21 - users actually sorry just right here add
75:23 - an mfa root user we're going to go ahead
75:26 - and hit add mfa
75:28 - all right so that's going to bring us to
75:30 - this screen and so here we can activate
75:32 - our mfa and so we have a few options
75:35 - here so we have virtual mfa device u2f
75:38 - security key other hardware like a
75:42 - gem gym
75:43 - gemalto token so you know i generally
75:46 - use this because i have a security key
75:48 - and i want to show you what i'm talking
75:50 - about so this
75:51 - is how i log into my machine or my aws
75:54 - account
75:55 - this is a security key an ubi key that
75:57 - sits on my desk i tape it so it doesn't
75:59 - fall fall off the cord but the idea is
76:01 - that when i log in i have to press this
76:04 - little button here to double confirm
76:06 - before i get into my account but if you
76:08 - don't have a security key you can just
76:09 - use a virtual mfa and all that means is
76:12 - you're going to
76:14 - use something on your phone to log in so
76:16 - we'll click continue here
76:18 - and so it says install a compatible app
76:20 - on your mobile phone or device and so if
76:22 - you click and open this what it will do
76:24 - is tell you about some things that you
76:26 - can use
76:27 - um so if we scroll down to virtual
76:31 - here this suggests uh if you have
76:32 - android iphone so authy dual mobile last
76:35 - path microsoft authenticator google
76:38 - authenticator so google authenticator
76:40 - microsoft authenticator and authy i have
76:42 - all those three installed um honestly
76:44 - authy has the the nicest simplest
76:47 - ui
76:48 - but i'm using microsoft authenticate
76:50 - authenticator quite a bit so anyway
76:52 - whichever you want to do it's fine but
76:54 - what we'll have to do is go back here
76:56 - and then it says use your virtual mfa
76:58 - app on your device camera to scan your
77:00 - qr code so once you have one of those
77:03 - apps installed like authy or whatever
77:05 - one you want
77:07 - what you're going to do is open up the
77:09 - application and i can't tell you exactly
77:12 - where it is but you'll have to hit add
77:13 - account in your in your app and then
77:16 - from there it will ask you to scan your
77:18 - qr code and so
77:20 - once you're ready you hit show the qr
77:22 - code you hit scan the qr code on your
77:24 - phone i'm holding my phone up to my my
77:27 - um
77:28 - my computer screen here and it's going
77:30 - to find it and i'm just going to take a
77:32 - moment here to rename the account so i
77:34 - can tell what it is
77:36 - so i'm just naming it aws
77:38 - sandbox because that's what i call this
77:40 - account
77:43 - and i'm gonna go ahead and save that and
77:45 - so now what i can do is enter uh two
77:47 - consecutive mfa codes now this always
77:49 - confused me what they wanted here but
77:51 - the idea is that you're gonna see one
77:53 - code
77:54 - right whatever's on the screen right now
77:55 - so i'm gonna type in it it says seven
77:56 - 734051
77:59 - and i'm going to wait
78:01 - until the new code shows up
78:03 - so there's like a timer in all these
78:04 - apps and they go across the screen or
78:06 - they count down and so you have to wait
78:08 - for that to happen and so i'm just going
78:10 - to wait here a little bit
78:17 - and once i get the new number here this
78:19 - one is zero seven one
78:22 - five three zero i'm gonna hit assign mfa
78:25 - and there we go and i can't tell you how
78:27 - many times i like messed that up because
78:28 - i didn't understand the consecutive
78:30 - numbers but you're just waiting for uh
78:32 - the number that's on the screen it
78:33 - entered in and then entered the next one
78:35 - in to turn on mfa and so now your
78:37 - account is protected and every time you
78:39 - log in you're going to have to enter in
78:41 - mfa so let's log out and see what that
78:44 - looks like
78:46 - so we'll go ahead and sign in
78:49 - and
78:50 - again we'll put in our root user account
78:53 - here we'll type in
78:54 - 74m32t
78:58 - submit
78:59 - and i need to go grab my password so
79:01 - that's in my password managers just give
79:03 - me a moment here
79:07 - and now it wants the mfa code so this is
79:09 - in my phone
79:10 - and so i'm going to go enter it in so
79:12 - this one says four seven five
79:14 - eight four one all right we'll hit
79:17 - submit
79:19 - okay there we go so that's gonna happen
79:21 - every single time we want to log in
79:23 - i'm going to tell you that if you get
79:24 - one of these they're so much easier to
79:26 - use because you just press the button
79:28 - okay so that's why i have this because i
79:30 - cannot stand entering the code in time
79:32 - and time again
79:34 - but you know those are your options
79:35 - there okay
79:37 - [Music]
79:42 - hey this is andrew brown from exam pro
79:43 - and we're looking at the concept of
79:45 - innovation waves so when we're talking
79:47 - about innovative waves we're talking
79:48 - about chondrativia or k waves which are
79:51 - hypothesized cycle-like phenomena in the
79:54 - global world economy and the phenomenon
79:57 - is closely connected with technology
79:58 - life cycles so here is an example where
80:01 - each wave is irreversibly changes the
80:03 - society on a global scale and if you
80:06 - look across the top we can kind of see
80:08 - what they're talking about so we have
80:09 - steam engine cotton
80:11 - railway and steel electric engineering
80:14 - chemistry petrochemicals automobiles
80:16 - information technology
80:18 - and so the idea is that cloud technology
80:21 - is the latest wave and i'm not sure if
80:23 - you'd fit web 3 in there as well ml ai
80:26 - but maybe they're all part of the same
80:28 - wave or their separate waves but
80:30 - generally they're broken up based on
80:32 - this p r d e here where it says
80:35 - perspective recession depression and
80:37 - movement uh improvement sorry and so
80:39 - this is the common pattern of wave where
80:41 - we see a change of supply and demand and
80:43 - so if we're seeing this we know that we
80:45 - are in a wave in where we are in a wave
80:48 - okay
80:49 - [Music]
80:53 - hey this is andrew brown from exam pro
80:55 - and we are looking at the concept of a
80:56 - burning platform so burning platform is
80:58 - a term used when a company abandons old
81:00 - technology for new technology with the
81:03 - uncertainty of success and can be
81:05 - motivated by fear the organization's
81:07 - future
81:08 - survival hinges on digital
81:10 - transformation and just to kind of give
81:11 - you a visualization here is a literal
81:13 - burning platform so imagine you have to
81:15 - jump to it uh jump from it to make a
81:18 - change so
81:19 - um you know burning platform could be
81:21 - you know
81:22 - stop using on-prem and start using cloud
81:24 - or maybe going from cloud to web 3
81:27 - and that's generally the idea when we
81:28 - talk about a burning platform
81:31 - [Music]
81:35 - so i just want to quickly show you that
81:37 - digital transformation checklist that i
81:39 - mentioned and the way you can get to it
81:41 - is by typing in digital transformation
81:43 - aws and so it should bring you to the
81:45 - public sector page and here it is so we
81:48 - click there and all it is is a pdf uh so
81:50 - it's not news from 2017 but that doesn't
81:52 - mean that it's not valid anymore uh it's
81:55 - just that that's when it was made so we
81:57 - scroll on down and we can see
81:58 - transforming vision and so we have a
82:00 - checklist there so if we click into this
82:02 - uh we can see things like communicate a
82:04 - vision of what success looks like define
82:06 - a clear governance strategy including
82:08 - the framework of achieving goals uh
82:10 - build a cross-functional team identify
82:13 - technical uh partners they talk about
82:15 - shifting the culture and then down below
82:17 - i assume that this one is related to
82:19 - that one it's unusual because
82:21 - you know they just have a checklist here
82:23 - but then they have a sub checklist which
82:24 - must be clear to that so reorganize
82:26 - staff into smaller teams things like
82:28 - that so it's not super complicated
82:30 - you'll see each category go go cloud
82:32 - native they'll have a checklist
82:34 - um you know and if you are at the
82:36 - executive level or the sales level or
82:38 - trying to convince your vps and stuff
82:40 - like that give this a read it might give
82:42 - you something useful in the end
82:44 - to help better communicate that
82:46 - transformation for you okay
82:52 - hey this is andrew brown from exam pro
82:54 - and we are looking at the evolution of
82:56 - computing power so what is computing
82:58 - power it's the throughput measured at
82:59 - which a computer can complete
83:01 - computational tasks and so uh what we're
83:04 - pretty much used to right as of these
83:06 - days is general computing so a good
83:08 - example here would be a zeon cpu
83:10 - processor uh that's more of a high-end
83:12 - processor not something you'd find in
83:14 - your home computer but we're talking
83:15 - about data centers specifically uh um
83:18 - you know innovative data centers xeon
83:20 - cpu processors are what you're going to
83:22 - come across uh then came along a new
83:24 - type of compute which is gpu computing
83:27 - um when we're talking about google cloud
83:30 - they have tensor computing and so this
83:31 - is where i get the 50 times faster based
83:33 - on that metric and so i didn't have an
83:35 - exact metric here for aws as a solution
83:38 - for this mid tier of computing power so
83:41 - i just borrowed that 50 times there but
83:42 - the idea is that gpu computing or tensor
83:46 - computing is is 50 times faster than
83:49 - traditional cpu and generally that's
83:51 - going to be used for very specialized
83:53 - tasks when you're doing machine learning
83:56 - or ai so it's not something you're going
83:57 - to be doing for your regular
84:00 - web workloads but just understand that
84:02 - all these fit so we're not getting rid
84:04 - of general computing we're just adding
84:07 - new levels of compute then there's the
84:09 - latest which is uh quantum computing and
84:11 - so here we have an example of the rigid
84:15 - rig
84:16 - right getty 16q aspen 4 and so it
84:19 - literally looks like it's out of um
84:21 - science fiction and this thing is like a
84:24 - hundred million times faster it is super
84:27 - cutting edge and we don't even know
84:28 - exactly how it works and there's not
84:30 - even anything that's very applicable
84:32 - that we can use this for but the idea is
84:34 - that we're not done with the evolution
84:36 - of computing power things are going to
84:38 - get a lot faster once we solve this last
84:40 - one here
84:42 - and so above service offering here would
84:43 - be for general computing you're looking
84:45 - at elastic compute cloud ec2 so we have
84:47 - a variety of different uh instance types
84:50 - and they're all going to have different
84:51 - types of hardware with different types
84:52 - of general computing
84:54 - for gpu computing this is a specialized
84:57 - chip that aws has produced called the
84:59 - edibus and i don't know how to say it
85:01 - but we'll just abbreviate it to infer so
85:04 - aws infer chip
85:06 - and this was designed as a direct
85:07 - competitor to gcp's
85:10 - tensor computing uh unit the tpu um and
85:14 - so this is intended for ai ml workloads
85:16 - but it works with not just um tensorflow
85:19 - but it works with any
85:21 - machine learning framework so that is
85:22 - one advantage it has over uh tpus um and
85:26 - then the last one here is aws brackets
85:28 - so you can actually use quantum
85:29 - computing as a service on your bus you
85:31 - uh as of even today um the way aws is
85:35 - able to do this is they work with
85:36 - caltech so that's the california
85:38 - technology university or institute i'm
85:41 - not sure the name of it there
85:43 - so it's not exactly aws producing this
85:45 - but itabus is doing this as a
85:46 - partnership to give quantum computing
85:48 - accessible to you okay
85:50 - [Music]
85:54 - so i'm here in the aws console because i
85:55 - just want to prove to you that you can
85:57 - use quantum computing on aws it's that
86:00 - accessible so all you'd have to do is go
86:01 - to the top here type in bracket
86:04 - and then you make it over to amazon
86:06 - bracket and so here you can like set up
86:09 - quantum tasks the first time you set it
86:11 - up you've got to go through this process
86:13 - here
86:14 - i think i have to go through this
86:15 - onboarding to be able to show you the
86:17 - next step so i'm going to go ahead and
86:18 - enable bracket in this abs account
86:22 - okay and i'm not going to launch
86:23 - anything i'm just going to try to just
86:25 - kind of show you a little bit of what is
86:27 - accessible to you because it's not super
86:29 - exciting but the fact that you can do it
86:31 - is kind of interesting so here i am on
86:33 - the inside here and we have all these
86:35 - different types of quantum computing so
86:37 - d wave i know i i o n q
86:41 - righty things like that and then down
86:44 - below these are the quantum processing
86:46 - units the q q p u's and then down below
86:49 - you have the simulator so you can kind
86:50 - of simulate uh these things here um so i
86:54 - think that's kind of interesting
86:56 - but in terms of the cost like if you
86:58 - scroll on down here
86:59 - um so it was bracket is part of that it
87:01 - was free tier it gives you one free hour
87:03 - of quantum circuit simulation time per
87:05 - month during the first 12 months so
87:08 - it's free to do a circuit simulation but
87:12 - if you actually want to run it on the
87:13 - actual hardware you can see the cost
87:15 - there's the per task price the per shot
87:17 - price things like that
87:19 - what could you do with this i don't know
87:21 - there's things called like quad bits or
87:23 - something like that and i can't imagine
87:24 - that you're going to be doing anything
87:25 - useful but i think it's just more so
87:27 - like you are sending out quad bits or
87:29 - whatever they are and you're observing
87:31 - them but what you can do with them i
87:33 - have no idea but it's just exciting that
87:35 - you can do that
87:36 - i didn't have any spend just by
87:38 - activating that i'm just kind of just
87:39 - showing you there okay
87:41 - [Music]
87:46 - hey this is andrew brown from exam pro
87:48 - and we are looking at the benefits of
87:50 - cloud and this is a summary of reasons
87:52 - why an organization would uh consider
87:54 - adopting or migrating to utilizing
87:56 - public cloud and so we'll quickly go
87:58 - through the list here uh because in the
88:00 - follow-up slides we actually go into
88:01 - them a bit more detailed so we have
88:03 - agility page ago economy of scale global
88:06 - reach security reliability high
88:08 - availability scalability um and
88:11 - elasticity so the thing is is that eight
88:15 - of us had this before it was called the
88:17 - six advantages of cloud but they have
88:18 - reworked it to include additional items
88:21 - um and so where you see these uh sub
88:24 - bullets here those are the original six
88:26 - as you see one two three four five six
88:28 - and so i kind of just put them where
88:30 - they kind of fall under the new
88:32 - categories there and you'll notice that
88:34 - database has included high availability
88:36 - elasticity reliability and security as
88:39 - uh new ones here okay and so the thing
88:43 - is is that
88:44 - um i have always always even in my
88:47 - original uh i think my original cloud
88:49 - practitioner had cloud architecture as a
88:52 - separate section and included all these
88:54 - things in here so it's a great thing to
88:55 - see that ableist has included it
88:57 - but
88:58 - in terms of how i organize this course
89:00 - we're not going to cover them in this
89:02 - section because i have the cloud
89:03 - architecture section so just understand
89:04 - that we will come to those eventually
89:07 - and i would just say that aws is still
89:09 - missing something on this list which is
89:10 - fault tolerance so you know my list
89:13 - looks like this except i would add fault
89:15 - tolerance to it so you have everything
89:17 - there
89:18 - and disaster recovery okay so the
89:21 - benefits of cloud is a reworking
89:22 - expansion of the six advantages of the
89:24 - cloud and we will look at the original
89:25 - six advantages um and then look at
89:28 - another one that is more of a
89:30 - generalized one that i i've used across
89:32 - my courses so that we fully understand
89:34 - the benefits okay
89:38 - [Music]
89:40 - all right let's take a look here at the
89:41 - six advantages to cloud defined by aws
89:44 - and so these are still uh part of aws
89:46 - marketing pages um but you know it's
89:48 - interesting because you can't find the
89:50 - benefits of the cloud in a single page
89:52 - on any of this at least the time of
89:53 - making this so there's a bit of
89:55 - disconnect between the um exam guide and
89:58 - the actual marketing material but that's
89:59 - okay i fill it all in for you so you
90:01 - know i'm just again noting that the
90:02 - sixth advantage of cloud was the
90:04 - original description for cloud benefits
90:06 - and we'll go through them okay so the
90:08 - first is trade capital expense for
90:09 - variable variable expense so you can pay
90:12 - on demand meaning that there is no
90:14 - upfront cost and you pay for only what
90:16 - you consume or you pay by the hour
90:18 - minutes or seconds so instead of paying
90:19 - for upfront costs of data centers and
90:21 - servers the next is benefit from uh
90:24 - massive uh economies of scale so
90:27 - you are sharing the cost with other
90:29 - customers to get unbeatable savings
90:31 - hundreds of thousands of customers
90:32 - utilizing a fraction of the server stop
90:34 - guessing capacity so scale up or down to
90:36 - meet the current needs
90:38 - launch and destroy services whenever so
90:40 - instead of paying for idle or
90:41 - underutilized servers we have increased
90:43 - speed and agility so launch resources
90:46 - within a few clicks and minutes instead
90:47 - of waiting days or weeks of your it to
90:50 - implement the solution on premise we
90:52 - have stopped spending money on running
90:53 - and maintaining data centers so focus on
90:56 - your customers developing and
90:57 - configuring applications so instead of
90:59 - operations such as racking stacking and
91:02 - powering servers the last is go global
91:04 - in minutes so deploy your app in
91:06 - multiple regions around the world with a
91:08 - few clicks provide low latency and a
91:10 - better experience for your customers at
91:11 - minimal cost the six advantages of cloud
91:13 - still apply and i like to include them
91:16 - here because they just have a different
91:18 - kind of lens or
91:20 - or or
91:22 - angle when you're looking at this stuff
91:24 - and so we've looked at the six
91:25 - advantages of cloud and now let's take a
91:27 - look at the next slide my reworking of
91:29 - the sixth advantage of the cloud to be
91:31 - more generalized okay
91:32 - [Music]
91:36 - all right i just wanted to show you
91:37 - where that sixth advantage of cloud
91:38 - computing comes from it's part of it it
91:41 - was documentation so i typed it in here
91:43 - and you can see that it is still around
91:45 - and so it's unusual because this used to
91:47 - be part of the marketing website it had
91:49 - those nice little graphics
91:51 - but for whatever reason it's over here
91:52 - now in the overview of amazon web
91:55 - services and by the way if you're
91:56 - starting starting out with databus this
91:58 - is a very light read but it is a good
92:00 - read to get started with we obviously
92:03 - cover all this stuff in the course um
92:05 - but you know maybe you'll get something
92:06 - different here but the idea is that it
92:07 - was definitely expanded on this but for
92:09 - whatever reason this documentation
92:11 - hasn't changed so just understand that
92:13 - i've polyfilled that for you in this
92:15 - course okay
92:16 - [Music]
92:20 - all right so this is the seven
92:22 - advantages to cloud i said six but i
92:24 - meant to say seven and so um you know
92:26 - since i've created fundamental courses
92:28 - for all these cloud service providers i
92:30 - started to notice kind of a trend and so
92:32 - what i did is i normalized it into my
92:35 - own seven advantages and this actually
92:37 - maps up really well to the new benefits
92:40 - of the cloud so it looks like invoice
92:42 - was thinking the same as i was um with
92:44 - the exception of those cloud architect
92:46 - stuff which i keep in a separate section
92:48 - but let's go through it and see what is
92:50 - here so the first is cost effective you
92:52 - pay for what you consume no upfront
92:54 - costs on demand pricing so pay as you go
92:56 - p-a-y-g with thousands of customers
92:58 - sharing the on uh sharing the cost of
93:00 - resources any of us used to refer to
93:03 - this always as on-demand pricing and
93:05 - azure always said pay as you go and so
93:08 - it looks like aws now uses both
93:10 - on-demand and pay-as-you-go to describe
93:11 - them which is great um but there you go
93:14 - then we have global so launch workloads
93:16 - anywhere in the world just choose a
93:17 - region it's secure so cloud provider
93:20 - takes care of physical security cloud
93:22 - services can be secured by default or
93:23 - you have the ability to configure access
93:26 - down to a granular level uh it's
93:28 - reliable so data backup disaster
93:30 - recovery data replication fault
93:32 - tolerance it's scalable increase or
93:34 - decrease resources and services based on
93:35 - demand elastic so automate scaling
93:38 - during spikes and drop in demand current
93:40 - so the underlying hardware and and
93:42 - managed uh software is patched upgraded
93:45 - and replaced by the cloud provider
93:46 - without interruption to you so i think
93:48 - this is one that isn't on the benefits
93:50 - of the cloud which is a really good one
93:52 - um but uh yeah that's the seven
93:55 - [Music]
93:59 - hey this is andrew brown from exam pro
94:01 - and we are taking a look at what is
94:03 - able's global infrastructure so global
94:05 - infrastructure is globally distributed
94:06 - hardware and data centers that are
94:08 - physically networked together to act as
94:10 - one large resource for the end customers
94:12 - so if you see here on the right hand
94:14 - side we have a picture of a globe and
94:16 - the idea is that we have a bunch of
94:17 - these regions and these regions are
94:19 - containing a bunch of data centers and
94:21 - then you have those lines going in
94:23 - between them which kind of represents
94:24 - the network okay so the global
94:26 - infrastructure is made up of the
94:27 - following resources so they have regions
94:30 - availability zones direct connection
94:32 - locations point of presence so those are
94:35 - pops local zones wavelength zones and
94:38 - we're going to cover all of these in
94:40 - this section here
94:41 - and one thing i want to note is that
94:43 - airbus has millions of active customers
94:45 - and tens of thousands of global partners
94:47 - that are constantly using this
94:48 - infrastructure so you know that it is
94:50 - rock solid okay
94:52 - [Music]
94:57 - all right so i'm over here on the global
94:59 - infrastructure page if you type in aws
95:00 - global infrastructure you'll make your
95:02 - way here and so i just wanted to point
95:04 - out that aws is always updating their
95:06 - global infrastructure so these numbers
95:08 - are increasing all the time but if
95:10 - you're over here what you probably want
95:12 - to do is make your way to regions and
95:13 - azs so you can kind of see what's in
95:15 - your area
95:17 - so i'm in canada and we have canada
95:19 - central region here and it has three
95:20 - availability zones have launched in
95:22 - 2016.
95:24 - you'll notice that it has a couple
95:25 - asterisks if you scroll on down here
95:27 - explain that it's in the montreal
95:29 - metropolitan area so saying it's in the
95:32 - downtown it's in the city uh that could
95:34 - matter to you for whatever reason um but
95:36 - just kind of pointing out where that
95:38 - stuff is you can read about all this
95:40 - stuff but of course we cover this all in
95:42 - the course but there you go
95:43 - [Music]
95:48 - hey this is andrew brown from exam pro
95:50 - and we are taking a look at above
95:51 - regions and regions are geographically
95:53 - distinct locations consisting of one or
95:55 - more availability zone and so here is a
95:58 - world map showing you all the regions
96:00 - that abuse has in the world and the blue
96:02 - ones represent regions that are already
96:04 - available to you and the orange ones
96:06 - represent ones that ableis is planning
96:08 - to open so aws is always expanding their
96:10 - infrastructure uh in the world so always
96:13 - expect there to be more upcoming ones
96:15 - every region is physically isolated from
96:17 - independent of every other region in
96:19 - terms of location power and water supply
96:22 - and the most important region that you
96:23 - should give attention to is u.s east one
96:26 - uh in particular so this is northern
96:28 - virginia it was italy's first region
96:30 - where we saw the launch of sqs and s3 uh
96:33 - and there are a lot of special use cases
96:36 - where things only work in u.s east ones
96:38 - and we'll find that out here in a moment
96:40 - what i do want to show you is what it
96:41 - looks like for an architectural diagram
96:43 - when you are seeing a region so notice
96:46 - that we have this
96:48 - little flag here it says us east one us
96:50 - west one and inside of it we have an ec2
96:52 - instance so that is going to represent a
96:54 - region in our architectural diagrams uh
96:56 - but let's look at some of the facts here
96:58 - and understand why u.s east or u.s east
97:00 - 1 is so important
97:01 - so each region generally has three
97:03 - availability zones and that is by
97:05 - intention and we will talk about that
97:07 - when we get to the availability zone
97:08 - section some new users are limited to
97:10 - two or uh to two uh but generally
97:13 - there's always three okay new services
97:15 - almost always become available first in
97:17 - u.s east and specifically u.s east one
97:20 - not all services are available in all
97:21 - regions all your billing information
97:23 - appears in u.s east one so that's a usc
97:25 - one particular thing uh the cost of
97:27 - aidable services vary per region and so
97:30 - if you're on the marketing website or uh
97:32 - for with global infrastructure you can
97:33 - see uh
97:35 - here in north america they'll say like
97:36 - when it launched how many availability
97:38 - zones and there might be some conditions
97:40 - so you'll notice there's like asterisks
97:41 - uh beside these things here or um in
97:44 - this one particular there's an asterisk
97:45 - saying hey there are three zones but
97:48 - generally you're limited to two okay
97:50 - when you choose a region there are four
97:52 - factors you need to consider uh what are
97:54 - the regulatory compliance does this
97:56 - region meet what is the cost of this
97:58 - enable service in this region what input
98:00 - services are available in this region
98:02 - and what is the distance distance or
98:04 - latency to my end users and those are
98:06 - those four factors that you should
98:07 - remember okay
98:09 - [Music]
98:13 - all right so we just talked about adabus
98:15 - regions now let's talk about uh how that
98:18 - affects our services versus regional and
98:20 - global services so regional services are
98:23 - scoped based on what is set in the
98:24 - database management console on the
98:26 - selected region so you have this drop
98:28 - down and that's what you'll do you'll
98:29 - say okay i want to have resources in
98:32 - canada or in europe
98:34 - so this will determine where a native
98:36 - service will be launched and what will
98:38 - be seen within the airbus services
98:40 - console you generally don't explicitly
98:42 - set the region for a service at the time
98:44 - of creation i explicitly mentioned this
98:46 - because when you use something like gcp
98:48 - or azure when you create the resource
98:50 - that's when you select the region but
98:51 - aws is it has this kind of global thing
98:53 - which is unique to their platform
98:56 - then there's the concept of global
98:57 - services so some aw services operate
99:00 - across multiple regions and the region
99:02 - will be fixed to the word global and for
99:04 - these that's services like s3 cloud
99:06 - front row 53 iam
99:09 - so the idea is if you were to go over to
99:10 - cloud cloudfront and go into the
99:12 - cloudfront console you'll notice that it
99:14 - will just say global and you can't
99:15 - switch out of that
99:16 - for these global services at the time of
99:18 - creation it's a bit different so we were
99:20 - saying up here for regional ones that
99:22 - you don't select the region but when you
99:25 - are clearing global services if you're
99:26 - using something like iam there is no
99:28 - concept of region because they're just
99:30 - globally available so you don't have to
99:32 - determine
99:33 - a subset of regions if you're using s3
99:35 - bucket that has to be in one region so
99:37 - you actually do have to select a region
99:39 - at time of creation um and then there's
99:41 - something like cloud form distributions
99:42 - where you were choosing a group of
99:44 - regions so you either say all of the
99:45 - world or only north america which is
99:48 - more like geographic distribution so you
99:50 - don't say the region in particular but
99:52 - you know hopefully that gives you a
99:53 - distinction between regional services
99:54 - and global services
99:56 - [Music]
100:01 - hey this is andrew brown from exam pro
100:02 - and we are taking a look at availability
100:04 - zones so availability zones commonly
100:06 - abbreviated as a z and i'll frequently
100:09 - use b using the term a z is physical
100:11 - locations made up of one or more data
100:14 - centers so a data center is a secured
100:16 - building that contains hundreds or
100:18 - thousands of computers and this is one
100:21 - of my favorite graphics i like to show
100:22 - of course uh you know aws would never
100:24 - have a dog um in their data center but i
100:27 - just thought that would be fun a region
100:29 - will generally contain three
100:31 - availability zones and i say generally
100:33 - because there are some cases where we
100:35 - will see less than three so there might
100:37 - be two
100:38 - data centers within a region will be
100:40 - isolated from each other
100:41 - so there will be different buildings but
100:43 - they will be close enough to provide low
100:45 - latency and that is within the
100:48 - 10 milliseconds or less so it's very
100:50 - very low uh it's common practice to run
100:52 - workloads in at least three azs to
100:54 - ensure services remain available in case
100:56 - one or two data centers fail and this is
100:58 - known as high availability and this
101:00 - generally is driven based on regulatory
101:03 - compliance so a lot of companies uh you
101:05 - know they have to at least be running in
101:07 - three az's and that's why aws tries to
101:09 - always have at least three azs within a
101:11 - region uh azs are represented by a
101:14 - region code followed by a letter so here
101:16 - you know you'd have us east one which
101:18 - would be the region and then the a would
101:20 - represent the particular availability
101:22 - zone in that region
101:24 - um so a
101:26 - subnet which is related to availability
101:28 - zones is associated with
101:30 - two availability zones so you never
101:31 - choose an az when launching resources
101:34 - you always choose a subnet which is then
101:36 - associated uh two and a z a lot of
101:38 - services um you know
101:41 - don't even require you to choose a
101:42 - subnet because they're fully managed by
101:43 - aws but in the case of like virtual
101:45 - machines you're always choosing a subnet
101:47 - okay so here is a graphical uh
101:50 - representation or a diagram that's
101:52 - representing two availability zones so
101:55 - here we have the region usc 1 and us
101:58 - west 2 and then we have our 2az so here
102:00 - is 1a and 1b and so these are
102:03 - effectively the subnets okay
102:06 - and so within those subnets then you can
102:08 - see or availability zones you will see
102:10 - that we have two virtual machines okay
102:13 - so the usc s1 region has six azs and i
102:16 - thought that's just kind of like a fun
102:17 - fact because it is the most out of every
102:19 - single one um i don't think anyone comes
102:22 - close to usc 1 but of course it is the
102:24 - most popular it is the first uh
102:28 - region or so it's not a surprise that
102:30 - that one has that many a
102:33 - [Music]
102:37 - okay so we just covered regions and
102:39 - availability zones but i really want to
102:40 - make it clear what they look like so i
102:43 - kind of have a visual representation so
102:45 - let's say we have our aws region and in
102:47 - this particular one we have canada
102:48 - central which in particular is montreal
102:50 - so ca central one
102:53 - and the idea here is that a region has
102:55 - multiple availability zones so here you
102:57 - can see that we have uh one a one b and
103:01 - one d for some reason aws decided to uh
103:04 - not launch one c maybe it's haunted who
103:07 - knows you know
103:08 - and then within your um availability
103:11 - zones they are made up of one or more
103:13 - data centers so just understand that az
103:15 - is not a single data center but could be
103:16 - a collection of buildings
103:18 - and that these azs are interconnected
103:20 - with high bandwidth low latency
103:22 - networking they're fully redundant
103:24 - dedicated to metro fiber providing high
103:27 - throughput low latency networking
103:28 - between so just very fast connections in
103:30 - between
103:31 - and all traffic between azs is encrypted
103:34 - and these azs are within a hundred
103:35 - kilometers so about 60 miles of each
103:38 - other okay
103:40 - [Music]
103:44 - so what i want to do here is just show
103:46 - you uh how regions and availability
103:48 - zones work with some different database
103:50 - services so you have a general idea when
103:53 - you are selecting a region or a z and
103:55 - when you're not so within aws when you
103:58 - want to select a region you're going to
103:59 - go up here and change it and this is
104:01 - going to apply to regional services a
104:04 - very famous example of a regional
104:06 - service would be ec2 so we go over to
104:08 - ec2 which is elastic
104:11 - cloud computing or compute whatever
104:13 - let's forget the name of it and what we
104:15 - can do is go over to instances
104:17 - i'm going to launch an instance i'm not
104:19 - going to complete the process i just
104:20 - want to show you
104:22 - what would happen when you go select
104:23 - some things here so i'm going to go with
104:25 - amazon x2
104:27 - we're going to just go to
104:29 - next here and so here is where we're
104:31 - going to select
104:33 - our availability zone so up here we have
104:35 - north virginia that's our region and
104:37 - when i say we're selecting our
104:38 - availability zone we're actually
104:39 - selecting the subnet so so here we are
104:42 - choosing a subnet and a subnet is
104:45 - associated to a availability zone and
104:48 - every single
104:50 - um
104:51 - region has a default vpc and that vpc
104:54 - has
104:55 - subnets set up and the subnets are
104:57 - defaulted to each of the availability
104:59 - zones available so usc 1 has six of them
105:02 - so this server is going to launch in u.s
105:04 - east 1b
105:06 - so this is a regional service okay
105:09 - then we have global services like s3 so
105:11 - we go over to s3
105:13 - and it says it's global right and so
105:16 - we're going to go ahead and create our
105:17 - bucket
105:19 - and so here we choose the region so we
105:22 - go down we're going to say the region we
105:24 - want to be in but we don't choose the
105:26 - availability zone because there's
105:28 - nothing to um
105:31 - choose because aws is going to run these
105:34 - in
105:35 - multiple azs and it doesn't matter to
105:37 - you what it's doing there okay
105:40 - so there's that and then there's
105:41 - something like cloudfront so
105:42 - cloudfront's a little bit different here
105:44 - so we go over to cloudfront
105:47 - and we create ourselves a distribution
105:49 - um and so yeah if you don't have that
105:52 - option there because sometimes database
105:53 - has like a splash screen just click on
105:54 - the left hand side then go to
105:55 - distributions
105:57 - okay and so here well they changed it
106:00 - again on me they're always changing this
106:02 - ui but if we scroll on down it should
106:04 - allow us to change
106:07 - um change where this is going to launch
106:09 - it's like global stuff like that
106:11 - literally they just recently changed
106:12 - this and that's why i'm confused
106:15 - ah we'll scroll on down here
106:18 - it used to be
106:21 - maybe it's under legacy
106:24 - additional
106:26 - customized
106:28 - oh it's here sorry okay so notice here
106:30 - the price class that says use the edge
106:32 - locations for best performance north
106:34 - america and europe north america europe
106:36 - asia middle uh middle east and africa so
106:39 - we're not choosing a particular region
106:41 - we're picking a geographical area and so
106:44 - those are pretty much the major um uh
106:48 - examples of that uh then there's of
106:51 - course things like in iem where you
106:52 - don't even say where it is so you go to
106:54 - i am
106:56 - you know if i create something like a
106:57 - group
106:59 - over here a user group whoops
107:01 - here
107:03 - i say create group you know i'm not
107:05 - saying oh this is for this particular
107:07 - region or something like that okay
107:10 - so yeah hopefully that makes sense
107:12 - [Music]
107:16 - hey this is andrew brown from exam pro
107:18 - and let's take a look here at fault
107:20 - tolerance specifically for global
107:22 - infrastructure and so before we jump
107:23 - into that let's just define some fault
107:25 - terminology here uh so let's describe
107:28 - what a fault domain is so a fault domain
107:30 - is a section of a network that is
107:32 - vulnerable to damage if a critical
107:34 - device or system fails and the purpose
107:36 - of a fault domain is that if a failure
107:38 - occurs it will not cascade outside that
107:40 - domain limiting the possible damage and
107:43 - so there's this very popular meme called
107:45 - this is fine where there's obviously a
107:48 - serious problem but the person's not
107:50 - freaking out and i gave it some context
107:52 - to say well the reason they're not
107:53 - freaking out because they know that
107:54 - there's a fault domain and nothing
107:56 - outside of this room is going to be
107:57 - affected okay
107:59 - so you can have fault domains nested
108:00 - inside of other fault domains
108:02 - but generally they're grouped in
108:04 - something called fault level so a fault
108:05 - level is a collection of fault domains
108:08 - and the scoping of a fault domain could
108:09 - be something like a specific specific
108:11 - servers in a rack an entire rack in a
108:14 - data center an entire room in a data
108:16 - center the entire data center building
108:18 - and it's really up to the cloud service
108:19 - provider to define those boundaries of
108:21 - the domain it's abstracts it all away so
108:23 - you don't have to think about it but
108:24 - just to compare it against something
108:25 - else when you're using azure you
108:27 - actually define your fault domain so you
108:29 - might say like okay uh make sure that
108:31 - this workload is never running on the
108:33 - same vm on the same rack for these
108:34 - things uh and you know you might like to
108:37 - have this level of control but i really
108:38 - like the fact that it was just abstracts
108:40 - it away i'm not sure how they segment
108:42 - their uh their their fault domains but
108:44 - they
108:45 - definitely are some broader ones which
108:47 - we'll describe right now so when we're
108:49 - looking at an enables region
108:51 - this would be considered a fault level
108:53 - and then within that fault level you
108:54 - would have your availability zones and
108:56 - these would be considered fault domains
108:58 - and of course those data centers can
109:00 - have uh fault domains within them okay
109:02 - like maybe you know they have everything
109:04 - in a particular room and that room is
109:06 - secure so like if there's a fire in that
109:07 - room it's not gonna affect the other
109:08 - room things like that
109:10 - um so each amazon region is designed to
109:12 - be completely isolated from the other
109:14 - amazon region
109:16 - they achieved this with the greatest
109:18 - possible fault tolerance and stability
109:19 - uh each availability availability zone
109:22 - is also isolated but the availability
109:23 - zone in a region are connected through
109:25 - low latency links each availability zone
109:28 - is designed as an independent failure
109:30 - zone and so here we have some kind of
109:32 - different language that database is
109:34 - using
109:35 - i've never experienced this terminology
109:36 - in other any other cloud service
109:38 - providers so i kind of feel like it's
109:39 - something that it was made up but
109:41 - basically a failure zone they're just
109:42 - basically saying a fault domain but
109:44 - let's kind of expand on their fault
109:46 - failure zone terminology so availability
109:49 - zones are physically separated within a
109:51 - typical metropolitan region and are
109:53 - located in lower risk flood plains
109:56 - discrete uninterruptible power supply so
109:58 - ups and an on-site backup generation
110:01 - facilities uh data centers located in
110:03 - different azs are
110:05 - designed to be supplied by independent
110:07 - substations to reduce the risk of an
110:10 - event on the power grid impacting more
110:12 - than one availability zone
110:14 - availability zones are all redundantly
110:15 - connected to multiple tier one transit
110:18 - providers and we'll talk about what
110:19 - those are
110:20 - in an upcoming slide
110:22 - and just one thing i want to note here
110:24 - is that when you adopt multi-az you get
110:26 - high availability so if an application
110:28 - is partitioned across azs
110:30 - companies are better isolated and
110:32 - protected from issues such as power
110:34 - outages lightning strikes tornadoes
110:36 - earthquakes and more so that's the idea
110:38 - behind you know why we want to run in
110:40 - multi-az okay because of these fault
110:42 - domains
110:43 - [Music]
110:47 - hey this is andrew brown from exam pro
110:49 - and we're talking about the global
110:51 - network so the global network represents
110:53 - interconnections between aws global
110:55 - infrastructure and it's commonly
110:57 - referred to as the backbone of aws so is
111:00 - ec2 so just understand that that could
111:02 - be used in more than one way but think
111:04 - of it as a private expressway where
111:06 - things can move fast between data
111:08 - centers and uh one thing that is
111:11 - utilized a lot to get data in and out of
111:14 - aws very quickly is edge locations they
111:16 - can act as on and off ramps to the abs
111:18 - global network of course you can get to
111:21 - the network through pops which we'll
111:22 - talk about um you know in the upcoming
111:25 - slides here but let's just talk about
111:26 - edge locations and what services use
111:27 - them so uh when we're talking about
111:29 - things that are getting on to the
111:31 - database network we're looking at things
111:32 - like abus global accelerator aws s3
111:35 - transfer acceleration and so
111:38 - these use edge locations as an on-ramp
111:40 - to quickly reach able's resources and
111:42 - other regions by traversing the fast
111:44 - away global network notice that the
111:46 - names in it's a accelerator acceleration
111:48 - so the idea is that they are moving
111:50 - really fast okay
111:52 - on the other side when we talk about
111:53 - like an off-ramp we're looking at amazon
111:55 - cloudfront which is a content
111:56 - distribution network this uses edge
111:58 - locations to as an off-ramp to provide
112:00 - an at the edge storage and compute near
112:03 - the end user
112:05 - and one other thing that is kind of
112:06 - always utilizing the global network are
112:08 - vpc endpoints now these aren't using
112:10 - edge locations but the idea here is that
112:12 - this ensures your resources stay within
112:14 - the aws network and do not traverse over
112:16 - the public internet so you know if you
112:18 - have uh you know a resource running in
112:20 - u.s east one and one in uh eu it would
112:23 - and they never have to go to the
112:24 - internet it would make sense to always
112:26 - enforce it to stay within the database
112:27 - network because it's going to be a lot
112:29 - faster so there you go
112:31 - [Music]
112:36 - hey this is andrew brown from exam pro
112:38 - and we are taking a look at point of
112:39 - presence also known as pop and this is
112:41 - an intermediate location between a
112:43 - database region and the end user and
112:45 - this location could be a data center or
112:47 - a collection of hardware so for aws a
112:50 - point of presence is a data center owned
112:51 - by aws or trusted partner that is
112:54 - utilized by itabus services related for
112:56 - content delivery or expedited upload so
112:59 - a pop resource could be something like
113:01 - an edge location or a regional edge
113:02 - cache so as an example over here we see
113:05 - an s3 bucket and it has to go through a
113:07 - regional edge cache and then cut to an
113:08 - edge location let's go define what those
113:10 - are so an edge location are data centers
113:13 - that hold cached copies on the most
113:15 - popular files so web pages images and
113:17 - videos
113:19 - so that the delivery of the distance to
113:20 - the end users are reduced then you have
113:23 - regional edge locations and these are
113:25 - data centers that hold much larger
113:27 - caches of less popular files to reduce a
113:29 - full round trip and also to reduce the
113:32 - cost of transfer fees
113:34 - [Music]
113:38 - so to kind of help put pops more in
113:41 - presence just in the general sense here
113:43 - is a uh diagram i got from wikipedia
113:45 - that kind of just shows a bunch of
113:46 - different networks and notice where the
113:48 - pop is it's on the edge or the
113:50 - intersection of uh two networks so here
113:53 - you know we have
113:54 - um you know tier three and then there's
113:56 - tier two and there's this pop that is in
113:58 - between them okay
113:59 - so tier one networks is a network that
114:01 - can reach every other network on the
114:03 - internet without purchasing iptransit or
114:05 - paying for peering and so the innovas
114:08 - availability zones or azs are all
114:10 - redundantly connected to multiple tier
114:11 - one transit providers okay
114:13 - [Music]
114:18 - all right so let's take a look at some
114:19 - state of the services that are utilizing
114:21 - pops or edge locations for content
114:24 - delivery or expedited uploads so amazon
114:26 - cloudfront is a content delivery network
114:28 - service and the idea here is you point
114:30 - your website to cloudfront so it will
114:32 - write requests to the nearest edge
114:33 - location cache it's going to allow you
114:36 - to choose an origin so that could be a
114:37 - web server or storage that'll be the
114:39 - source of the cache and caches the
114:41 - content of what origin would return to
114:43 - various edge locations around the world
114:45 - then you have amazon s3 transfer
114:47 - acceleration this allows you to generate
114:49 - a special url that can be used by the
114:51 - end users to upload files to a nearby
114:53 - edge location once a file is uploaded to
114:56 - an edge location it can move much faster
114:58 - within the aws network to reach s3
115:00 - then at the end here you have aws global
115:03 - accelerator you can find the optimal
115:04 - path from the end user to your web
115:06 - servers so global accelerators are
115:08 - deployed within edge locations so you
115:10 - send user traffic to an edge location
115:12 - instead of directly to your web
115:13 - application this service is really
115:15 - really great for if let's say you're
115:17 - running a web server usc 1 and you just
115:20 - don't have the time to set up
115:22 - infrastructure in other regions you turn
115:24 - this on and you basically get a boost
115:26 - okay
115:27 - [Music]
115:32 - this is andrew brown from exam pro and
115:33 - let's take a look at it was direct
115:35 - connect so this is a private or
115:37 - dedicated connection between your data
115:38 - center office co-location and aws and so
115:41 - the idea here is imagine if you had a
115:43 - fiber optic cable
115:45 - running from your data center all the
115:47 - way to your aws so that it feels like
115:49 - when you're using your stuff on your
115:51 - data center like your local virtual
115:52 - machines that there's like next to no
115:55 - latency okay so direct connect has two
115:57 - very fast network connection options we
116:00 - have the lower bandwidth which is at 50
116:01 - to 500 megabytes per second and then you
116:04 - have the higher bandwidth which is one
116:06 - gigabytes to 10 gigabytes per second so
116:10 - using direct connect helps reduce
116:12 - network costs increase bandwidth
116:13 - throughput so great for high traffic
116:15 - networks it provides a more consistent
116:17 - network experience than a typical
116:18 - internet-based connection so reliable
116:20 - and secure
116:22 - i do want to point out the term
116:23 - co-location if you never heard of that
116:24 - before a co-location or a carrier hotel
116:27 - is a data center where equipment space
116:29 - and bandwidth are available for rental
116:31 - uh to retail customers i do want to also
116:34 - point out that even though it says
116:35 - private up here and this is the language
116:37 - that aws used i usually just say
116:38 - dedicated but the connection is private
116:41 - but that doesn't necessarily mean it's
116:42 - secure okay so uh we'll talk about that
116:45 - when we reach above vpns and how we can
116:47 - use that with direct connect to make
116:48 - sure our connections are secure okay
116:50 - [Music]
116:55 - all right so let's take a look at what a
116:56 - direct connect location is so a direct
116:59 - connect location are trusted partner
117:01 - data centers that you can establish a
117:03 - dedicated high-speed low-latency
117:05 - connection from your on-premise to aws
117:08 - so an example of a partner data center
117:10 - would be one like here in toronto the
117:11 - allied data center so you can tell
117:13 - that's right down in uh the toronto
117:15 - center and so you would use this uh uh
117:18 - as part of direct connect service to
117:20 - order and establish a connection okay
117:23 - [Music]
117:27 - hey this is andrew brown from exam pro
117:29 - and we're taking a look at local zones
117:31 - which are data centers located very
117:32 - close to densely populated areas to
117:34 - provide single digit millisecond low
117:36 - latency performance so thinks like seven
117:38 - milliseconds for that area so here is a
117:41 - map of uh local zones that exist and
117:43 - ones that are coming out i believe the
117:45 - orange ones are probably ones that are
117:46 - on their way and so to use a local zone
117:48 - you do need to opt in so you gotta go
117:50 - talk to aws probably open a support
117:51 - ticket to get access to it the first one
117:53 - to ever be launched was uh the la one uh
117:56 - and so
117:57 - um you know when you want to see it it
118:00 - looks just like a
118:01 - availability zone it's going to show up
118:03 - under whatever region that is because
118:04 - these are always tied to existing
118:06 - regions so the la-1 is tied to u.s west
118:08 - uh region and the az would look like u.s
118:12 - west 2 hyphen la x hyphen 1a okay so
118:17 - only specific ab services have been made
118:19 - available so there's a particular ec2
118:21 - types ebs amazon fsx application load
118:25 - balancer amazon vpc
118:27 - they probably have extended it to more
118:29 - services do you need to know that for
118:31 - the exam no but you know the point is is
118:33 - that there's a limited subset of things
118:35 - that are available the purpose of local
118:37 - zone is to support highly demanding
118:39 - applications sensitive to latency so
118:41 - media and entertainment electronic
118:43 - design and automation ad tech machine
118:45 - learning so it kind of makes sense like
118:47 - you look at la they're in the media
118:48 - entertainment and so they're dealing
118:49 - with lots of media content so it has to
118:52 - be really low for them okay
118:54 - [Music]
118:58 - hey this is andrew brown from exam pro
119:00 - and we are taking a look at abus
119:01 - wavelength zones and these allow for
119:03 - edge computing on the 5g networks and
119:06 - applications will have ultra low latency
119:08 - being as close as possible to the users
119:10 - so abus has partnered with various
119:12 - telecom companies to utilize their 5g
119:14 - networks so we're looking at verizon
119:16 - vodafone kddi sk telecom and so the idea
119:20 - here is that you will create a subnet
119:22 - tied to a wavelength zone
119:24 - and then and just think of it as like an
119:26 - availability zone but it's a wavelength
119:27 - zone and then you can launch your vms to
119:29 - the edge of the targeted 5g network so
119:32 - that's the network you're using aws to
119:35 - deploy an ec2 instance and then when
119:38 - users connect to you know those radio
119:40 - towers those
119:41 - cell towers they're going to be routed
119:43 - to
119:44 - you know nearby hardware that is running
119:46 - those virtual machines okay and that's
119:49 - all it is it's just it's just ec2
119:51 - instances um but you know the advantage
119:53 - here is that it's like super super low
119:55 - latency okay
119:57 - [Music]
120:01 - hey this is andrew brown from exam pro
120:03 - and we are taking a look at data
120:04 - residency so this is the physical or
120:07 - geographical location of where an
120:09 - organization or cloud resources reside
120:11 - and then you have the concept of
120:13 - compliance boundaries so a regulatory
120:15 - compliance so legal requirement by
120:17 - government or organization that
120:19 - describes where data and cloud resources
120:21 - are allowed to reside and then you have
120:23 - the idea of data sovereignty so data
120:25 - sovereignty is the jurisdictional
120:27 - control or legal authority that can be
120:29 - asserted over data because its physical
120:32 - location is within a jurisdictional
120:34 - boundary and so the reason we care about
120:36 - this stuff is that if we want to work
120:38 - with the canadian government or the us
120:40 - government and they're like hey you got
120:41 - to make sure that you know if you want
120:43 - to work with us all the data has to stay
120:45 - in canada and you need to give them that
120:47 - guarantee so data residency is not a
120:49 - guarantee it just says where your data
120:50 - is right and compliance boundaries are
120:53 - those controls that are in place to say
120:55 - okay this is going to make sure that
120:57 - data stays where we want to be and date
120:59 - of sovereignty is just like the idea of
121:00 - the scope of the legal the legal stuff
121:03 - that ties in with compliance boundaries
121:06 - so how do we do that on aws well there's
121:08 - a few different ways but um let's just
121:10 - take a look at some ways that we can
121:12 - meet those compliance boundaries one
121:14 - which is very expensive but also very
121:16 - cool is aws outposts so this is a
121:19 - physical rack of servers that you can
121:20 - put in your data center and you'll know
121:22 - exactly where the data resides because
121:24 - you know it's physical if it's in your
121:26 - data center and you're in canada that's
121:28 - where it's going to be okay
121:29 - and i believe that you know there is
121:31 - only a subset of aws services that are
121:33 - available here but you know that is one
121:35 - option to you another is using like
121:38 - services for governance so like one
121:40 - could be abs config this is a policy as
121:42 - a code service so you can create rules
121:44 - to continuously check database resource
121:46 - configuration so if they deviate from
121:48 - your expectations you are alerted or
121:50 - image config can in some cases auto
121:52 - remediate so if you were expecting you
121:54 - know um you know you had an aws account
121:56 - and you're saying this account is only
121:58 - to be used for candid resources and
122:00 - somebody launches let's say something in
122:02 - another region then you could get an
122:04 - alert or to tell it was config to go
122:06 - delete that resource okay now if you
122:08 - want to prevent people from doing it all
122:11 - together that's where i am policies come
122:13 - into play so these can be written
122:14 - explicitly to deny access to specific
122:16 - aws regions and you know this is great
122:18 - if you're applying it to users or roles
122:21 - but if you wanted to have it
122:22 - organizational wide across all of your
122:25 - abus accounts you can use something
122:26 - called a service control policy that is
122:29 - just an i am policy that is used within
122:31 - its organizations that makes it
122:33 - organizational wide okay
122:35 - [Music]
122:39 - hey this is andrew brown from exam pro
122:41 - and we are looking at it for government
122:43 - so to answer that we first have to
122:45 - understand what is public sector so
122:47 - public sector includes public goods and
122:49 - government services such as military law
122:52 - enforcement infrastructure public
122:54 - transit public education health care and
122:57 - the government itself so abus can be
122:59 - utilized by the public sector or
123:00 - organizations developing cloud workloads
123:02 - for the public sector enables achieves
123:04 - this by meeting regulatory compliance
123:06 - programs along with specific governance
123:08 - and security controls
123:10 - so this could be i meet the requirements
123:12 - with hipaa fedramp um cjis and fips okay
123:17 - so amaz has a special regions or special
123:19 - regions for us regulation called
123:21 - govcloud which we'll talk about next
123:23 - okay
123:27 - [Music]
123:28 - hey this is andrew brown from exam pro
123:30 - and we are taking a look at govcloud and
123:32 - to understand what govcloud is we need
123:33 - to know what fedramp is so fedramp
123:35 - stands for federal risk and
123:36 - authorization management program it's a
123:38 - u.s government-wide program that
123:40 - provides a standardized approach to
123:41 - security assessment authorization
123:44 - continuous monitoring for cloud products
123:45 - and services so that we know what
123:47 - fedramp is
123:48 - what is govcloud well
123:51 - and again it's not particular to aws
123:53 - because azure has govcloud as well but
123:55 - a cloud service provider like aws or
123:57 - azure general will offer an isolated
123:59 - region to run fedramp workloads and so
124:02 - in aws it's called govcloud and these
124:05 - are specialized regions that allow
124:07 - customers to host sensitive controlled
124:09 - unclassified information and other types
124:11 - of regulated workloads so govcloud
124:13 - regions are only operated by you by u.s
124:15 - citizens on u.s soil they are only
124:18 - accessible to u.s entries and root
124:20 - account holders who pass a screening
124:21 - process
124:22 - customers can architect secure cloud
124:24 - solutions that comply with fedramp uh do
124:28 - the doj's
124:29 - credible justice information systems uh
124:32 - security policy the u.s international
124:35 - traffic and arms regulation
124:38 - export administration regulations the
124:40 - department of defense cloud computing
124:42 - security requirements and guides so if
124:44 - you want to work with the us government
124:46 - you want to
124:47 - engineer and use govcloud okay
124:50 - [Music]
124:54 - hey this is andrew brown from exam pro
124:56 - and we're taking a look at uh running
124:58 - ada bus in china so eight of us china is
125:01 - the ito's cloud offering in mainland
125:02 - china enemies china is completely
125:04 - isolated intentionally from adamus
125:06 - global to meet regulatory compliance
125:08 - from mainland china so that means that
125:09 - if you make a workload on the awesome
125:11 - global you can't
125:13 - interact with it within the aws china
125:15 - one okay it's basically treated like a
125:18 - completely separate service like adabus
125:20 - has its own chinese version uh and so it
125:23 - was china is on its own domain at amazon
125:26 - aws.cn and for everybody else that's
125:28 - what's considered it is global so when
125:30 - i'm using adabus from canada or use it
125:32 - from the u.s or from india or from
125:35 - europe or wherever that is the adabus
125:37 - global okay
125:39 - so in order to operate in aws china
125:41 - regions you need to have a chinese
125:44 - business license so icp license not all
125:47 - services are available in china so
125:49 - you will not have the use of route 53
125:52 - and you might say well why not just run
125:54 - in
125:55 - singapore and it was global and you
125:56 - could do that but the advantage of
125:58 - running in mainland china means that you
126:00 - would not have to traverse the great
126:03 - firewall okay so all your traffic is
126:05 - already within china so you don't have
126:06 - to
126:07 - deal with that airbus has two regions in
126:09 - mainland china so uh there's this one
126:12 - here which is the northwest region
126:13 - operated by nswc
126:16 - and then you have the one in beijing
126:18 - north one operated by uh synnet so you
126:20 - know itabus just could not meet the the
126:23 - compliance requirements so they had to
126:24 - partner with local providers uh or data
126:27 - centers and so that is how that works
126:29 - okay
126:30 - [Music]
126:34 - all right so i want to show you how you
126:36 - get over to the
126:38 - chinese database management console so
126:40 - this one is
126:41 - adabus.amazon.com that is the global one
126:43 - for everyone outside of mainland china
126:46 - but if you want to run resources uh on
126:49 - data centers within mainland china this
126:50 - is at amazon awesome.cn
126:53 - and so it looks very similar if you go
126:55 - to create a free account you're going to
126:57 - fill in this stuff but notice that you
126:59 - need to have your business registration
127:01 - certificate uh and additional
127:03 - information in order to run these data
127:05 - centers down below that aws is partnered
127:06 - with also notice that the logo doesn't
127:08 - say aws in it and there's a good reason
127:12 - for that if i type in aws trademark
127:13 - china
127:15 - inbus is actually banned from using the
127:17 - aws logo in china uh for whatever reason
127:20 - it's a weird reason if you ever want to
127:21 - read about it but that's why you don't
127:23 - see aws here all right
127:26 - so yeah there you go
127:28 - [Music]
127:33 - hey this is andrew brown from exam pro
127:35 - and we are looking at sustainability for
127:36 - aws global infrastructure and before we
127:38 - talk about that let's talk about the
127:40 - climate pledge so amazon co-founded the
127:42 - climate pledge to achieve net zero
127:44 - carbon emissions by 2040 across all of
127:46 - amazon's businesses which includes aws
127:49 - if you all want to find out more
127:50 - information go to
127:52 - sustainability.about amazon.com there's
127:55 - a lot of great information there and
127:56 - you'll learn exactly how
127:58 - uh aws is achieving this in particular
128:00 - like their data centers it's very
128:01 - interesting okay so aws cloud
128:04 - sustainability goals are composed of
128:05 - three parts the first is renewable
128:07 - energy so eight of us is working towards
128:09 - having their abs global infrastructure
128:11 - powered by 100 renewable energy by 2025
128:15 - and abbas purchases and retires
128:17 - environmental attributes to cover the
128:19 - non-renewable energy for abyss global
128:21 - infrastructure
128:22 - so they would purchase things like
128:23 - renewable energy credits also known as
128:25 - recs guarantees of origin so gos the
128:29 - second point here is cloud efficiency so
128:31 - abyss infrastructure is 3.6 times more
128:33 - energy efficient than the medium of u.s
128:36 - enterprises data centers surveyed so
128:38 - that's going to really rely on that
128:39 - survey surveys are not always that great
128:41 - so you know take that with a grain of
128:43 - salt okay then we have water stewardship
128:46 - so
128:47 - direct evaporative technology to cool
128:49 - our data centers use of non-uh potable
128:53 - water for cooling purposes so the
128:54 - recycling water on-site water treatment
128:56 - allows us to remove
128:58 - us them to remove scale forming minerals
129:01 - and reuse waters
129:03 - for more cycles water efficiency metrics
129:05 - to determine and monitor optimal water
129:07 - use for each adibus region and you'll
129:10 - find that water plays a large part on
129:13 - making these
129:14 - um
129:15 - these data centers very efficient okay
129:18 - [Music]
129:22 - so i just wanted to show you where you
129:24 - get to that sustainability information
129:25 - so i just went to itabus global
129:27 - infrastructure you click sustainability
129:29 - and that's going to bring us over to
129:31 - oops i have my twitter open there to the
129:33 - sustainability in the cloud so if you
129:35 - want to read a bunch of stuff here about
129:38 - things that are going on that database
129:39 - is up to see
129:41 - how they are progressing with renewable
129:43 - energy
129:44 - there's cloud efficiency up here so you
129:46 - know how they being efficient it's worth
129:48 - the read to really understand that
129:50 - there's a lot of water involved like
129:52 - reducing water and data centers i
129:53 - thought that was really interesting
129:55 - um i mean they have native podcasts but
129:58 - i don't think there's really much to it
130:00 - a bi-weekly podcast of bite-sized
130:02 - stories about how tech makes the world
130:04 - better that's not necessarily a
130:06 - sustainability podcast it's just the
130:08 - endless podcast in general there's a
130:10 - download center
130:12 - um amazon's 2020 sustainability reports
130:14 - so i guess you can download the reports
130:16 - to see what is going on there so we
130:18 - could download the progress here and see
130:20 - what they've been up to
130:23 - okay so there's a bunch of numbers
130:24 - things like that okay very short reports
130:27 - but hey at least you can download them
130:28 - okay so just in case you're
130:31 - very interested in sustainability all
130:32 - right
130:33 - [Music]
130:37 - hey this is andrew brown from exam pro
130:39 - and we are taking a look at abus ground
130:41 - station so this is a fully managed
130:43 - service that lets you control satellite
130:45 - communications process data and scale
130:47 - your operations without having to worry
130:49 - about building or managing your own
130:51 - ground station infrastructure and so
130:54 - when we're talking about ground station
130:55 - a really good way to cement what the
130:57 - service is is just think of a big
130:59 - antennae dish that's pointing to the sky
131:01 - trying to communicate with satellites
131:03 - because that's essentially what the
131:04 - service is doing so the use cases here
131:06 - could be for weather forecasting surface
131:09 - imaging communications video broadcasts
131:12 - and to use ground station the idea is
131:13 - that you would schedule a contact so
131:15 - that's where you're selecting a
131:16 - satellite a start and end time in the
131:18 - ground location and then you use an
131:20 - abuse ground station ec2 ami and amazon
131:23 - machine image to launch ec2 instances
131:25 - that will uplink and downlink
131:28 - data during the contact or receive
131:30 - downlink data in an amazon s3 bucket a
131:33 - use case could be something like you are
131:35 - a company you've reached an agreement
131:37 - with a satellite image provider to use
131:39 - their satellites to take photos for a
131:40 - specific region or time or whatever and
131:43 - so the idea is that you are using aws
131:45 - ground station to communicate to that
131:47 - company satellite and download that as
131:50 - that image data to your s3 bucket okay
131:53 - [Music]
132:00 - hey this is andrew brown and we are
132:01 - looking at able's outposts and this is a
132:03 - fully managed service that offers the
132:05 - same ableist infrastructure services
132:07 - apis tools to virtually any data center
132:09 - co-location space or on-premise facility
132:12 - for a truly consistent hybrid experience
132:14 - and just to kind of summarize it it's a
132:15 - rack of servers running aws stuff on
132:18 - your physical location okay
132:20 - so before we jump into the service or
132:22 - technology itself uh let's talk about
132:24 - what is a rack server or just a rack so
132:27 - it's a frame designed to hold and
132:29 - organize it equipment so here's an
132:31 - example of a four to u rack
132:34 - and there's a concept of rack heights so
132:37 - the u stands for rack units or u spaces
132:40 - uh with it equal to 1.75 inches and the
132:44 - industry standard rack is a 4 8 u um so
132:48 - that is a seven foot rack so um a full
132:52 - uh size rack cage is commonly the four
132:54 - two high
132:56 - okay and uh in it you might have
132:58 - equipment that is of different sizes so
133:00 - there could be one u two u three u or
133:03 - four u high so here's an example of you
133:05 - know of an interior of a rack and notice
133:07 - that like one u two u for u they're all
133:10 - a little bit shaped differently uh but
133:11 - they give you kind of an idea of um you
133:14 - know what those are
133:15 - so it's outpost comes in three form
133:18 - factors the four to you the one you and
133:20 - the two you so the uh the first one here
133:23 - the four to you this is basically a full
133:26 - rack of servers provided by aws so
133:27 - you're not just getting the frame it
133:29 - actually comes with you know servers uh
133:32 - and so abs delivers it to your preferred
133:33 - physical site fully assembled and ready
133:35 - to be rolled into the final position it
133:37 - is installed by aws and the rack needs
133:39 - to be simply plugged in to the power and
133:41 - network and there's a lot of details
133:43 - about um the specs on this on the adabus
133:45 - website so you know i'm not going to go
133:47 - through them all here um then there are
133:49 - servers that you can just place into
133:50 - your existing racks so we have the 1u so
133:53 - this is suitable for 19 inch wide 24
133:56 - inches deep cabinets it's using it with
133:58 - gravitron 2
134:01 - cpus and you can have up to 64 virtual
134:04 - cpus we have 128 gigabytes
134:07 - 4 terabytes of local nvm
134:10 - storage um and then you have the u or
134:13 - sorry the 2u
134:15 - so suitable for 19 inch wide 36 inch
134:18 - deep intel processors up to 128 virtual
134:21 - cpus 256 gigabytes of memory eight
134:24 - terabytes of local nvme storage so there
134:27 - you go
134:28 - [Music]
134:32 - let's take a look at cloud architecture
134:35 - terminologies before we do let's talk
134:36 - about some of the roles that are around
134:38 - doing cloud architecture so the first is
134:40 - solutions architect this is a role in a
134:42 - technical organization that architects a
134:45 - technical solution using multiple
134:47 - systems via researching documentation
134:49 - and experimentation and then you have
134:51 - the cloud architect this is a solutions
134:53 - architect that is focused solely on
134:55 - architecting technical solutions using
134:57 - cloud services understand that in the
135:00 - actual marketplace a lot of times
135:01 - solutions architect is used to describe
135:03 - both a cloud architect and a solutions
135:05 - architect
135:06 - and you know
135:07 - these are going to highly vary based on
135:09 - your locality and how companies want to
135:11 - use these terms but this is just me
135:13 - broadly defining them here so just don't
135:14 - take them as a perfect word in terms of
135:17 - what they're representing so a cloud
135:19 - architect needs to understand the
135:20 - following terms and factors
135:22 - and factor them into their designed
135:24 - architecture based on the business
135:25 - requirements so we have the idea of
135:27 - availability your ability to ensure
135:29 - service remains available scalability
135:31 - your ability to grow rapidly or
135:32 - unimpeded elasticity your ability to
135:34 - shrink and grow to meet the demand fault
135:36 - tolerance your ability to prevent a
135:38 - failure disaster recovery your ability
135:40 - to recover from a failure and there are
135:42 - a couple other things that uh you that
135:44 - should be considered they're not
135:46 - terminologies but they're definitely
135:47 - important to a solutions architect or
135:49 - cloud architect
135:50 - and uh these are things you always need
135:52 - to consider
135:54 - as well and this is just me talking to
135:56 - my solutions architect friends where
135:57 - they'll always ask me these two
135:59 - questions after presentation they'll say
136:01 - how secure is the solution and how much
136:03 - is this going to cost all right and so
136:06 - for the terminologies up here we're
136:08 - going to define these right away and
136:10 - we're going to figure these out
136:11 - throughout the course we have two giant
136:13 - sections just on cost and security alone
136:15 - uh so there we go
136:20 - the first term we're looking at is high
136:22 - availability and this is your ability
136:24 - for your service to remain available by
136:26 - ensuring there is no single point of
136:27 - failure
136:29 - and or you ensure a certain level of
136:30 - performance so the way we're going to do
136:33 - that on aws is you'd want to run your
136:35 - workload across multiple availability
136:37 - zones to ensure that if one or two
136:39 - availability zones became unavailable
136:41 - your servers or applications remain
136:43 - available because those other
136:45 - those other servers are going to be
136:47 - there and the way we would accomplish
136:48 - that is via elastic load bouncer so a
136:50 - load balancer allows you to evenly
136:52 - distribute traffic to multiple servers
136:54 - in one or more data center if a data
136:56 - center or server becomes unavailable or
136:58 - unhealthy the load bouncer will route
137:00 - the traffic to only the available data
137:02 - centers within the server and understand
137:05 - that just because you have additional
137:06 - servers doesn't mean that you are you're
137:08 - available you have to you might need to
137:10 - meet a particular threshold of
137:11 - availability so you might need to have
137:13 - at least two servers always running to
137:15 - meet the demand so it's based on the the
137:17 - demand of traffic okay
137:19 - [Music]
137:23 - let's take a look here at high scale
137:25 - abilities so this is your ability to
137:26 - increase your capacity based on the
137:29 - increasing demand of traffic memory and
137:31 - computing power and we have the terms
137:33 - vertical scaling so scaling up
137:36 - this is where you upgrade to a bigger
137:37 - server and then there's horizontal
137:39 - scaling scaling out this is where you
137:41 - add more servers of the same size and
137:43 - the great thing about scaling out or
137:45 - adding additional servers is that you're
137:46 - also going to get high availability so
137:48 - if you do need two servers it's always
137:50 - better to you know add an additional
137:52 - server as opposed to having a larger
137:54 - server but it's going to be very
137:55 - dependent on a lot of factors okay
137:58 - [Music]
138:02 - so scalability and elasticity seem very
138:05 - similar but there is a crucial
138:06 - difference and this is your ability to
138:08 - automatically increase or decrease your
138:10 - capacity based on the current demand of
138:12 - traffic memory and computing power again
138:14 - it's the it's the fact that it happens
138:16 - automatically and you can go both ways
138:18 - increase or decrease so for horizontal
138:20 - scaling we have the concept of scaling
138:22 - out so add more servers of the same size
138:25 - and then scaling in removing
138:27 - underutilized servers of the same size
138:30 - and vertical scaling is generally hard
138:32 - for traditional architectures so you'll
138:34 - usually only see horizontal scaling
138:35 - described with elasticity
138:38 - and the way we would accomplish uh being
138:40 - highly elastic is using auto scaling
138:42 - groups asgs and this is a database
138:44 - feature that will automatically add or
138:46 - remove servers based on scaling rules
138:48 - you define based on those metrics okay
138:51 - [Music]
138:55 - let's talk about being highly fault
138:57 - tolerant so this is your ability for
138:58 - your service to ensure there is no
139:00 - single point of failure preventing the
139:02 - chance of failure and the way we could
139:04 - do that is with failovers so this is
139:06 - when you have a plan to shift traffic to
139:08 - a redundant system in case the primary
139:10 - system fails a very common example is
139:13 - having a copy or secondary uh
139:17 - of your database where all ongoing
139:19 - changes are synced the secondary system
139:21 - is not in use until a failover occurs
139:23 - and it becomes the primary database so
139:26 - when we're talking about databases on
139:27 - abs this is the concept of rds multi-az
139:31 - so this is when you run a duplicate
139:32 - standby database in another availability
139:35 - zone in the case your primary database
139:37 - fails
139:38 - [Music]
139:42 - and last here is high durability so this
139:44 - is your ability to recover from a
139:45 - disaster and to prevent the loss of data
139:48 - so solutions that recover a disaster uh
139:50 - from a disaster is known as disaster
139:52 - recovery so do you have a backup how
139:54 - fast can you restore the backup does
139:55 - your backup still work how do you ensure
139:57 - current live data is not corrupt and so
139:59 - maybe a solution aws would be using
140:01 - cloud endure which is a disaster
140:02 - recovery service which continuously
140:05 - replicates your machines in a low cost
140:06 - staging area in your target apes account
140:08 - and preferred region enabling fast and
140:10 - reliable recovery in the case of an i.t
140:12 - data center fails okay
140:18 - [Music]
140:19 - so to understand disaster recovery we
140:21 - need to know more about uh things around
140:24 - it like business
140:25 - continuity plans bcps and rtos and rpos
140:29 - so a bcp is a document that outlines how
140:33 - a business will continue operating
140:34 - during unplanned disruption in services
140:37 - so it's basically the plan that you're
140:38 - going to execute
140:39 - if that happens and so here we have a
140:42 - disaster and you can see that there's a
140:44 - chance of data loss and downtime and
140:46 - these two
140:48 - factors as rpo and rto are going to
140:50 - define the length of these durations so
140:53 - recovery point objective is the maximum
140:55 - acceptable amount of data loss after an
140:57 - unplanned data loss incident expressed
140:59 - this amount of time so how much data are
141:01 - you willing to lose
141:02 - and then recovery time objective so the
141:04 - maximum amount of downtime your business
141:06 - can tolerate without incurring a
141:08 - significant financial loss so how much
141:11 - time you're willing to go down okay
141:13 - so those are the two there and now let's
141:15 - go take a look at the disaster recovery
141:17 - options that we can use to define in our
141:20 - our bcp
141:21 - [Music]
141:25 - so let's take a look at our disaster
141:27 - recovery options uh and based on what
141:29 - you choose they're going to be a trade
141:30 - of cost versus time to recover based on
141:33 - the rpos your rtos of course and so
141:35 - sometimes this is re represented
141:37 - vertically like a a thermostat or you
141:39 - can do it horizontally here both are
141:42 - valid ways of displaying this
141:43 - information but i just have it
141:44 - horizontally here today and so we have
141:47 - low or high or you could say
141:50 - even though i don't have it written here
141:51 - this could be cold or this could be hot
141:54 - okay
141:55 - so um on the left hand side we got back
141:57 - up and restore pilot light warm standby
142:00 - multi active site notice we're using the
142:02 - like the words like pilot light warm
142:04 - things that are relating to temperature
142:06 - so again cold and hot all right
142:09 - so let's just walk through what each of
142:11 - these things us conceptually do uh in
142:13 - terms of architecture so when you're
142:15 - doing a backup restore you're back you
142:17 - basically back up your data
142:19 - and at the time of disaster recovery
142:21 - you're just going to restore it to new
142:22 - infrastructure
142:23 - for a pilot light the data is replicated
142:25 - to another region with the minimal
142:27 - services running to keep on replicating
142:29 - that data and so you might have some
142:31 - core services running a warm standby is
142:34 - a scaled down copy of your
142:35 - infrastructure so you basically have
142:36 - everything that you would absolutely
142:38 - need to run an application but the idea
142:40 - is it's not at scale and so at any time
142:42 - when there's an incident you're going to
142:44 - scale up to the capacity that you need
142:46 - and then you have multi-site active
142:48 - active where you you have a scaled up
142:50 - copy of your infrastructure in other
142:51 - regions so basically everything you have
142:54 - identically in another region and so in
142:56 - terms of the rpos and the rtos for back
142:58 - and restore you're looking at hours uh
143:00 - with the pilot light you're looking at
143:02 - 10 minutes with a warm standby you're
143:04 - looking at minutes and multi-site active
143:06 - active you're looking at real time so
143:09 - you know hopefully that gives you an
143:10 - idea of you know the difference in terms
143:12 - of scale but let's just look at more
143:14 - detail so for a backup and restore this
143:16 - is for low priority use cases restore
143:18 - data after event deploy resources after
143:21 - an event and it's very cost effective
143:23 - for pilot light you this is where you
143:26 - have less stringent rtos and rpos so
143:28 - that you're going to be just running
143:29 - your core services
143:30 - you're going to start and scale
143:31 - resources after the event and this is a
143:33 - little bit more expensive this is
143:36 - very good for warm standby is good for
143:38 - business critical services so you scale
143:40 - resources after the event uh and it's
143:43 - almost very it's very it's costly but
143:45 - it's not as expensive as a multi-site
143:48 - active active so you get zero downtime
143:50 - near zero loss uh you have it's great
143:53 - for mission critical services and it's
143:55 - just as expensive as your original
143:57 - infrastructure so you're basically
143:59 - doubling the cost there okay
144:00 - [Music]
144:05 - so we already defined rto but let's
144:07 - redefine it again based on what aws
144:10 - describes in their white paper and just
144:12 - look at how it maps against the disaster
144:14 - recovery options so
144:16 - recovery time objective is the maximum
144:18 - acceptable delay between the
144:19 - interruption of service and restoration
144:21 - of service this objective determines the
144:24 - what is considered an acceptable time
144:25 - window when service is unavailable and
144:27 - is defined by the organization and so
144:29 - this is the diagram found in the white
144:31 - paper and so on the left-hand side we
144:33 - have cost and complexity here and then
144:36 - lengths of service interruption and what
144:38 - you can see here is that the cost and
144:40 - complexity for a multi-site active
144:42 - active is very high but the length of
144:44 - service interruption is zero
144:47 - and then as we go down we have warm
144:49 - standby so it's significantly like at
144:51 - least half the complexity of that one
144:54 - then we have our pilot light down here
144:56 - and backup and restore but notice backup
144:58 - restore takes the longest amount of time
145:01 - and notice here we have a recovery time
145:03 - objective so in your bcp you kind of
145:05 - define where that is based on the cost
145:06 - of business impact so you might have to
145:08 - calculate that saying okay what is our
145:10 - cost over time based on the length of
145:11 - service interruption where do we want
145:13 - our rto to be what is the acceptable
145:16 - recovery cost and this is where you're
145:18 - going to decide uh what you want to do
145:20 - so here we have pilot light and backup
145:22 - and restoring so this company
145:24 - he has to decide whether they want to do
145:26 - a pilot light or they're going to do a
145:28 - back and restore but it sounds like this
145:29 - is where they're going to be which is at
145:30 - the pilot
145:32 - light for what is acceptable in their
145:34 - business use case okay
145:40 - let's do the same for rpo so recovery
145:42 - point objective is the maximum
145:43 - acceptable amount of time since the last
145:45 - data recovery point the objective
145:46 - determines what is considered an
145:48 - acceptable loss of data between the last
145:50 - recovery point and the interruption of
145:51 - service and is defined by the
145:52 - organization again we pulled this from
145:55 - the aws white paper for disaster
145:57 - recovery and uh we have cost and
146:00 - complexity but this time it's replaced
146:01 - with data loss before service
146:03 - interruption
146:04 - so uh for multi-site again it's going to
146:07 - be very expensive and high up here as
146:09 - you notice it's not like a perfect um
146:13 - curve it's just it's a bit different in
146:14 - terms of what it looks like so here we
146:16 - have warm stand standby pilot light and
146:20 - so you'll see that the data loss is um
146:22 - not a big deal but for backup and
146:24 - restore it really juts out there so you
146:26 - can see that you can get pretty good
146:28 - results just with the pilot light and
146:30 - the cost and complexity is very low
146:31 - again we have to look at our cost and
146:33 - business impact so we got to follow that
146:35 - line and we need to see where our
146:37 - acceptable recovery cost is
146:40 - and so
146:41 - you're going to notice that we have a
146:43 - bit of an intersection here
146:46 - okay and so we need to determine you
146:47 - know like are we going to be doing a
146:49 - warm standby it looks like we have the
146:51 - cost to do it
146:52 - um
146:53 - but you know it just really depends you
146:55 - know do we want to be down here or down
146:56 - there okay so
146:58 - hopefully that helps and visualize that
146:59 - information for you
147:03 - [Music]
147:05 - hey this is andrew brown from exam pro
147:07 - and what i want to show you here is a
147:08 - real world architectural diagram i
147:10 - created this a while ago this is a
147:11 - previous version of the
147:13 - example or technically teacher see
147:14 - platform that powers the learning
147:16 - experience for my cloud certifications
147:19 - and so i'm hoping that by giving you
147:20 - some exposure you'll absorb some
147:22 - information here
147:23 - and that will carry through to really
147:25 - help you cement what these services do
147:26 - and how they work together
147:28 - now you might be asking how did i make
147:30 - this well i'm in adobe xd it's by
147:31 - photoshop or sorry adobe it's free to
147:34 - download but there's a lot of options
147:35 - out there and but the first thing you'll
147:37 - need is those aws architectural icons so
147:39 - these are free on aws you can download
147:41 - them in powerpoint download those assets
147:43 - as svgs and pngs which is what i have
147:45 - done and start using them in your um
147:48 - whatever software you like there's also
147:50 - third party providers out there so like
147:52 - there's lucidcharts i love lucidcharts
147:54 - but i don't use it to make architectural
147:55 - diagrams for aws um but you know you can
147:59 - drag drop and stuff and they already
148:00 - have the library there and there's a
148:02 - bunch of them that you can choose from
148:04 - so
148:05 - uh you know that's interesting but let's
148:06 - take a look at one that we can download
148:08 - maybe everyone's familiar with
148:09 - powerpoint so here is the aws
148:11 - architectural icons and the reason i'm
148:13 - showing you this is not because it just
148:15 - contains icons but it also suggests how
148:17 - you should build them so if i go through
148:19 - here they'll give you a definition of
148:21 - those system elements uh how they would
148:23 - look like here so we have our group
148:25 - icons our layer group our service icons
148:27 - resource icons where they should go
148:29 - and then they have some interesting
148:31 - guidelines of like do's and don'ts so
148:33 - here's like a simple example of a get to
148:35 - an s3 bucket
148:37 - here's an example of using vpc subnets
148:40 - and things like that on the inside
148:42 - um and then
148:44 - you can see kind of like all the groups
148:45 - that we have
148:47 - and they'll show all like the uh the
148:50 - arrows it's a big faux pas to make a
148:53 - diagonal arrows that's just something
148:55 - that it was defined but you'll see a lot
148:56 - of people do them anyway
148:58 - and then you'll see all the icons so do
149:00 - you have to make them like eight of us
149:02 - suggest no but you know if you like the
149:04 - way they look that is fine everyone just
149:06 - does whatever they want honestly so
149:08 - anyway now that we've seen you know how
149:10 - we can go get the resources to make our
149:12 - own i have adobe xd opened up here and
149:14 - so i just kind of want to walk you
149:15 - through what's going on here so again i
149:17 - said this is a
149:18 - traditional
149:21 - architecture meaning that it's powered
149:22 - by virtual machines and so what we need
149:25 - to look for uh is ec2 because that's
149:27 - where it's going to start that's our
149:28 - virtual machine and you'll notice we
149:30 - have one here so there's a t2 um
149:33 - that's running over here and then over
149:35 - here we have a t2 okay so
149:37 - we have a blue and a green environment
149:39 - so this is our running environment so
149:41 - i'm just going to zoom on in here
149:43 - okay so the web app would be running on
149:45 - this
149:46 - and
149:47 - and then on the outside here we have an
149:49 - auto scaling group and so auto scaling
149:50 - groups allow us to
149:52 - manage a group of ec2 instances and they
149:55 - will automatically scale if the demand
149:56 - increases or
149:58 - or decline so if this machine can't
150:00 - handle it it will just automatically
150:02 - provision a new one and so i've
150:03 - contained it in this environment here
150:05 - because i'm representing a blue green
150:07 - deploy meaning that when i deploy this
150:09 - will get this will be the environment
150:11 - that replaces things and so you can see
150:13 - i have a lot of lines being drawn around
150:15 - here so
150:16 - over here we have uh
150:19 - parameter store so parameter store is a
150:21 - place where we can store our environment
150:23 - variables
150:24 - um or application configuration
150:25 - variables and so i have this line going
150:27 - here and it's just saying we're going to
150:30 - take these environment variables and put
150:32 - them into the application okay
150:35 - and then there's also uh the database
150:37 - credentials so here we are using
150:39 - postgres over here so
150:41 - and then we need the database
150:42 - credentials so we're grabbing those
150:43 - database credentials those are stored in
150:45 - secrets manager and we're giving to the
150:47 - application so the app knows how to
150:48 - connect to the database and this one
150:50 - knows how to uh configure it okay
150:53 - then we have a bunch of
150:55 - uh buckets here for different
150:57 - organizations and so you know s3 is for
151:00 - storage so this is a way we're going to
151:02 - um store a variety of things so like
151:04 - user data assets artifacts
151:06 - cloudformation templates so some of this
151:08 - is for the app some of them is for the
151:10 - infrastructure so that's one thing there
151:12 - okay then over here we have a ci cd
151:15 - pipeline so
151:17 - we have code pipeline and so code
151:19 - pipeline is triggered by github so we
151:21 - put our code in github and when that
151:23 - happens it's going to do a code build so
151:25 - that's going to build out a server
151:28 - and then from there it's going to run
151:30 - another code build server and then from
151:32 - there it's going to then um
151:36 - use codeploy and so codeploy is going to
151:39 - trigger a deploy what it will do is
151:41 - create a new environment so it's going
151:42 - to create a copy of this
151:44 - um
151:46 - sorry it's going to create a cop this is
151:47 - actually the environment that's running
151:48 - so we'll copy that and that will be our
151:50 - new environment right
151:52 - okay
151:53 - and so when the deploy is done it will
151:55 - swap and that environment will become
151:56 - this new one
151:58 - um and so you know again this is
152:00 - actually really the the running server
152:02 - it's just kind of easy to get hung up on
152:03 - this one
152:04 - but the idea here is that um you know
152:07 - that's how deployment works but let's
152:08 - say
152:09 - you know we want to get uh
152:11 - traffic to this actual instance this is
152:13 - going to come through the internet
152:15 - and the internet is going to probably go
152:16 - to revit three so reference three is
152:18 - used for domain names so this would be
152:20 - like example teacherseek.com
152:23 - we pass that over to our elastic load
152:24 - bouncer which in this case is an
152:26 - application load bouncer that's why it's
152:28 - called alb and that's going to
152:30 - distribute the traffic there if we
152:32 - wanted to run the server in another
152:34 - um
152:35 - in another availability zone so that we
152:37 - make it highly available
152:39 - you know alb the elastic load balancer
152:41 - application load bouncer is going to
152:43 - have some traffic go here and some
152:45 - traffic go there so this is just
152:47 - the blue environment or whichever the
152:48 - current environment is over here
152:50 - now when we want to deploy new versions
152:53 - we're going to use launch templates and
152:55 - launch templates um
152:57 - uh are necessary when using auto scaling
152:59 - groups so um you know you do have to
153:01 - define launch template it just says like
153:03 - what is the shape of this instance type
153:05 - like what's this family what should it
153:06 - be
153:07 - and then we need an amazon machine image
153:09 - so our amazon machine image is custom
153:11 - built because we are installing all the
153:13 - stuff that we want on it and so in order
153:16 - to automate that process we are using um
153:18 - ssm automation documents so ssm stands
153:21 - for systems manager and automation
153:23 - allows you to automate that step so what
153:25 - it's going to do is launch an instance
153:26 - install ruby install postgres download
153:28 - the code base then it's going to create
153:30 - that ami
153:32 - and then um it will do a bunch of other
153:34 - stuff here as well
153:36 - and this is going to run weekly or
153:38 - actually at the time uh it was running
153:40 - nightly so we're doing nightly builds so
153:42 - that we would always get the latest um
153:44 - updates to our server
153:46 - because it's a virtual machine there
153:47 - could always be uh new updates for that
153:49 - linux version or amazon machine limit
153:52 - next version we were using
153:54 - and then there's a bunch of other stuff
153:55 - here so you know hopefully that kind of
153:58 - gives you an idea of like the complexity
153:59 - of it and you know this is how i like to
154:01 - make my architectural diagrams very in
154:02 - detail so that we can um look at them
154:05 - but yeah if that was too much that's
154:07 - fine but you know that's just the
154:09 - complexity of it if you build your own
154:10 - you'll start to really grasp the stuff
154:12 - pretty well okay
154:17 - so what i want to do is just show you
154:19 - how high availability is built into some
154:21 - aws services where in other cases you
154:23 - have to explicitly choose that you want
154:25 - something to be highly available so what
154:27 - i'm going to do is make my way over to
154:28 - s3 and so with s3 this is where you can
154:31 - create s3 buckets and this allows you
154:34 - to store things and so the great thing
154:36 - about s3 is that it's basically
154:38 - serverless storage so the idea is that
154:40 - you're just going to choose your region
154:41 - and by default it's going to replicate
154:43 - your data across multiple
154:45 - um uh data centers or azs and so this
154:48 - one's already highly available by
154:50 - default with the standard tier and so
154:52 - that is something that's really nice but
154:54 - other services uh you know like ec2 the
154:57 - idea is that you are going to launch
154:59 - yourself an ec2 instance so we would
155:02 - launch that one and the problem with
155:03 - this is that if you launch a single ec2
155:06 - that is not highly available because
155:08 - it's a single server running in a single
155:12 - um a z so here you know we would choose
155:15 - our subnet our subnet is our
155:16 - availability zone but you'd have to
155:18 - launch at least two additional servers
155:20 - and then you'd have to route um
155:23 - you'd have to have something that would
155:24 - balance uh the traffic to the to the
155:26 - three which is a load bouncer and so in
155:28 - this case you have to construct your
155:30 - high availability then you have services
155:33 - like elastic bean stock this is a
155:35 - platform as a service
155:37 - um and we'll go to environments here i'm
155:39 - not sure it wasn't showing up there uh
155:41 - and so the idea is that with elastic
155:42 - bean stock
155:44 - i'm just going to click on the main
155:45 - service here you're going to go ahead
155:46 - and
155:48 - create your application or create your
155:49 - environment you probably want to create
155:50 - environment first here
155:52 - okay and so i would choose a web server
155:55 - and then the idea is i'll just name it
155:57 - so my application here my
156:00 - environment and then down below you go
156:03 - configure more options whoops wants me
156:05 - to choose everything that's totally fine
156:09 - and we say configure more options we're
156:11 - not going to create it because um we
156:12 - don't want to create one but the idea is
156:14 - that you'd you could choose whether you
156:17 - want this to be highly available or not
156:19 - so see it's a single instance of free
156:21 - tier and then if you choose this what
156:23 - it's going to do
156:24 - is set up a bunch of stuff for you so
156:26 - it's going to set up an application load
156:28 - balancer for you it's going to set up
156:29 - auto scaling groups for you to make it
156:31 - highly available it's going to run at
156:32 - least between one to four instances so
156:36 - this does everything that ec2 you'd have
156:39 - to do manually setting up so that's
156:40 - really nice
156:42 - okay so you know some options have that
156:44 - if we make it our way over to rds and
156:46 - again we're not creating anything we're
156:47 - just looking at the options it gives us
156:50 - when we start things these up here
156:53 - we'll make our way over to rds and it
156:54 - gives us a moment here
156:58 - and if we go ahead and create ourselves
157:00 - a new database
157:04 - and we look at something like a postgres
157:06 - database
157:08 - notice that we have a production option
157:09 - and a dev test option and so i i mean
157:12 - usually it shows us the price down here
157:14 - so even test dev is 118 which is not
157:16 - true it can get cheaper than that but
157:18 - the idea is that when you choose between
157:20 - these two options
157:22 - um it's going to set up a multi-az it's
157:25 - going to that means that it's going to
157:27 - run an additional
157:29 - database and another availability zone
157:31 - replicate that data over so that it
157:34 - stays highly available um you know it's
157:37 - going to have auto scaling uh
157:39 - part of it and so some services you just
157:41 - choose it abstractly so you just have to
157:43 - understand what highly availability is
157:45 - going to mean underneath so hopefully
157:47 - that kind of gives you a picture of high
157:49 - availability on aws
157:50 - [Music]
157:55 - hey this is andrew brown from exam pro
157:57 - and we are looking at abuse application
157:59 - programming interface also known as
158:01 - database api
158:02 - so before we talk about the api let's
158:05 - describe what application programming
158:06 - interface is so an api is software that
158:09 - allows two applications or services to
158:11 - talk to each other and the most common
158:13 - type of api is via http requests and so
158:17 - the aws api is actually an http api and
158:21 - you can interact with it by sending
158:23 - https requests using an application
158:25 - interacting with apis like postman
158:28 - and so here's kind of an example of what
158:30 - a request would be that would be sent
158:31 - out and so the way it works is that each
158:34 - database service generally has a service
158:36 - endpoint so see where it says monitoring
158:38 - that's going to be cloudwatch so
158:40 - sometimes they're named after the
158:41 - services sometimes the name is a bit
158:42 - obscure and of course you can't just
158:45 - call and uh call a api request without
158:48 - authenticating or authorizing and so you
158:50 - have to sign your request and so that's
158:52 - a process of making a separate request
158:55 - with your idioms credentials to get back
158:56 - a a temporary token in order to
158:59 - authorize that and i don't have room to
159:02 - show it but the thing is is that what
159:03 - you'd be
159:04 - also going along with those requests
159:06 - would be to provide an action so when
159:09 - you look at um
159:11 - the aws api it will show you a bunch of
159:13 - actions that you can call they're
159:15 - basically the same ones you'll see in
159:16 - the in policies so it could be like
159:18 - describe ec2 instances or list buckets
159:22 - and they can also be accompanied with
159:24 - parameters okay
159:25 - so you know we're probably not going to
159:28 - show you how to make an api request
159:30 - directly because that's not something
159:31 - that you would generally do
159:33 - um but what you would do
159:36 - is you probably use the aws management
159:38 - console which is powered by the api use
159:40 - the abyss sdk which is powered by the
159:42 - api or using the aws cli so we'll cover
159:45 - all those three okay
159:46 - [Music]
159:51 - all right so what i want to do is just
159:52 - point you to where you'd find the
159:54 - resources to use the api
159:56 - programmatically uh we're not going to
159:58 - actually use the api because there's a
160:00 - lot more to it uh than what i'm going to
160:02 - show you here but at least you'll be
160:04 - familiar with how the api works so i'm
160:06 - on the
160:07 - aws.amazon.com website if you type in
160:09 - docs the type top there it's going to
160:12 - bring you to the main documentation and
160:14 - what we're looking for if we scroll on
160:16 - down there should be a general reference
160:17 - area where we have service endpoints if
160:20 - we click into here it's going to talk
160:23 - about
160:24 - how a server's endpoint is structured
160:26 - and if we go down to ibis api we can see
160:28 - some additional information of course to
160:30 - use um the api you're going to have to
160:33 - sign api requests first which is not a
160:36 - super simple process but you have to use
160:38 - an authorization header
160:40 - and send along some credentials and
160:42 - things like that so if you want to know
160:44 - what service endpoints are available to
160:46 - you if you search service endpoints list
160:48 - for aws this is the big list and so if i
160:51 - was to go down here and look for ec2 uh
160:53 - might be a common example here it's
160:55 - going to tell us what the endpoints are
160:57 - and as you can see they are regional
160:59 - based but the idea here is that i could
161:02 - take something like this okay i could
161:04 - grab that and using something like
161:06 - postman
161:07 - i could go and create a new request and
161:10 - it's probably a post i'm not sure what
161:12 - it's supposed to be it's probably a post
161:14 - and then you'd set your authorization
161:16 - header there might even be one in here
161:17 - for aws see where it says adab signature
161:19 - so you can go here and put your access
161:21 - key and secret within here um
161:24 - so that's something nice about postman
161:25 - so it's going to do the signing requests
161:27 - for you so it makes your life a lot
161:29 - easier and then from there what you do
161:31 - is you go to your body and you'd want to
161:33 - enter in json so to do json would
161:36 - probably be raw you drop down the format
161:38 - json and then you'd send your payload
161:40 - whatever it is so again i haven't done
161:42 - this in a while because it's not a very
161:44 - common uh thing that i have to do like
161:45 - describe ec2 instances but there
161:48 - probably is like an action and some
161:49 - additional information that you would
161:51 - send along um so you know hopefully that
161:53 - gives you kind of an idea how the api
161:56 - works but you know you should never pro
161:58 - in practice ever have to really work
162:00 - with the api uh this way directly okay
162:03 - [Music]
162:07 - hey this is andrew brown from exam pro
162:09 - and we are looking at the database
162:10 - management console so the italo's
162:12 - management console is a web-based
162:13 - unified console to build manage and
162:15 - monitor everything from simple web apps
162:17 - to complex cloud deployments so when you
162:20 - create your apps account and you log in
162:22 - that is what you're using the aws
162:23 - management console and i would not be
162:26 - surprised if you're watching this video
162:27 - and they've already changed um the
162:29 - default page here since adobe's loves to
162:31 - change the ui on us all the time
162:34 - but uh the way you would access this is
162:36 - via console.ableis.amazon.com
162:38 - when you click sign in or go to the
162:40 - console that's the link that it's going
162:42 - to
162:42 - uh and so the idea here is that you can
162:44 - point and click to manually launch and
162:46 - configure aws resources with limited
162:48 - programming knowledge this is known as
162:50 - click ops since you can perform all your
162:51 - system operations via clicks okay
162:54 - [Music]
162:58 - let's talk about the aws management
163:00 - console in brief here so you know of
163:02 - course when you're on the home page you
163:03 - go to aws management console and you
163:04 - will end up logging in and from there we
163:07 - will make our way over to the edwards
163:09 - management console when i say [ __ ]
163:11 - management console i'm referring to
163:13 - this homepage but i'm also referring to
163:15 - anything that i'm doing in this web ui
163:18 - whether it's a sub service or not so you
163:21 - know a lot of times people just call
163:22 - this the dashboard uh or the home page
163:25 - but you know it is technically the us
163:27 - management console but everything
163:29 - is the aws management console you can
163:31 - drop down services here if there's some
163:33 - that you like you can favorite them on
163:34 - the left hand side i don't find that
163:36 - particularly useful you can see the most
163:38 - recent ones here they'll also show
163:40 - recently up here as well we have the
163:42 - search at the top notice that there's a
163:43 - hotkey for alt s i don't think i ever
163:46 - use it if i was to type in a service
163:47 - like ec2 it's going to get me the
163:49 - services and then down below it's the
163:52 - sub features of it so if i just click
163:53 - into that there into this use this is
163:56 - the main this is a service console so i
163:58 - would call this the ec2 console or the
164:01 - ec2 service console
164:03 - so if you ever hear me saying go to the
164:04 - ec2 console that's what i'm saying and
164:06 - you'll notice here like there is stuff
164:08 - on the left hand side so i come back
164:10 - here ec2 image builder you see two
164:12 - global views these are considered
164:13 - services but if you drop down it says
164:15 - top features or you go down here it says
164:17 - dashboard limits amis you go over here
164:21 - the ec2 dashboard limits amis are here
164:24 - and limits are somewhere here right
164:26 - there so okay so those kind of map over
164:29 - pretty well polls and documentation
164:30 - knowledge based articles marketplace i
164:32 - don't think i've ever touched those in
164:33 - my life
164:34 - this here is the cloud shell so if you
164:36 - click it it will launch a cloud shell
164:37 - we'll cover that when we get to that
164:39 - section here we have this little bell it
164:41 - tells us about open issues i think this
164:43 - is for the personal health dashboard
164:46 - yeah it says phd in the bottom left
164:48 - corner or left corner so if i open that
164:49 - up it'll bring up the phd the personal
164:52 - health dashboard all right
164:55 - our region selector our support so
164:57 - nothing super exciting here but just
164:59 - kind of giving you a bit of a tour so
165:01 - that you know there are some things you
165:03 - can do
165:04 - um can you change the look of this i
165:07 - don't think right now as of yet um there
165:10 - is any way i'm sure it was thinking
165:12 - about it because it's been a high
165:14 - request that's in demand but this is
165:16 - what it looks like as of today okay
165:18 - [Music]
165:22 - all right so i just want to describe
165:23 - what a service console is so an aws
165:26 - service each have their own customized
165:28 - console and you can access these
165:29 - consoles by searching the service name
165:31 - so you would go ahead and type in ec2
165:33 - and then what we refer to this screen as
165:35 - as the ec2 console the reason i'm
165:37 - telling you this is that when you're
165:38 - going through a lot of labs or follow
165:40 - alongs you'll hear the instructor say go
165:42 - to the ec2 console go to the sagemaker
165:44 - console go to the rds console what
165:46 - they're telling you is to go type the
165:48 - the name of the service and go to
165:51 - that particular services console okay
165:54 - some interest service consoles will act
165:56 - as an umbrella console containing many
165:58 - aws services so uh you know vpc console
166:02 - ec2 console systems manager console
166:04 - sagemaker console uh cloudwatch console
166:07 - these all contain multiple services so
166:09 - you know for um
166:11 - for ec2 you might say okay well i need a
166:14 - security group there's no security group
166:16 - console it's under the ec2 console okay
166:19 - so just be aware of that
166:23 - [Music]
166:25 - so now i want to show you some of these
166:26 - service consoles to kind of distinguish
166:28 - how they might vary per per service okay
166:31 - so if we were to look up ec2
166:33 - um and we just did look at this but the
166:35 - interesting thing is that some consoles
166:38 - the ec2 console
166:39 - is the home for other database services
166:42 - and you just have to learn this over
166:44 - time to know that so for instance
166:45 - elastic block store is its own service
166:48 - but it's tightly linked to ec2 instances
166:50 - so that's why they always have it here
166:52 - same thing with amis
166:55 - security group same thing with that so
166:56 - these are interesting because these are
166:58 - basically part of virtual networking
167:00 - and so you'd think they'd be under the
167:02 - vpc console but they're actually under
167:04 - here with ec2
167:06 - and so load balancing auto scanning
167:08 - groups tightly coupled to
167:10 - to ec2 if we make our way over to vpc
167:16 - you know here it's going to contain all
167:18 - the new stuff does it have a new
167:20 - experience no i guess this is the newest
167:21 - one
167:22 - it looks a bit old and a little bit new
167:24 - here but you know we have a lot of
167:25 - different things here like firewalls
167:27 - vpns transit gateways traffic mirroring
167:30 - we make our way over to cloudwatch
167:34 - okay and cloudwatch has
167:36 - uh
167:36 - very uh focused services they're all
167:39 - actually named and this is more like a
167:41 - feels more like a single service where
167:42 - you have these very focused
167:44 - services where you have alarms logs
167:46 - metrics events insights right but you're
167:49 - going to notice that like the ui highly
167:51 - varies so we had looked at cloudwatch
167:53 - and then we had looked at
167:55 - vpc and it looks like this
167:57 - and then we looked at ec2 and it looked
168:00 - like that and so there is
168:01 - inconsistencies because each um service
168:05 - team like that work on per service or
168:08 - whatever they have full control over
168:09 - their ui and so
168:11 - some of them are in
168:13 - different states of updating so some
168:14 - people might have updated the left-hand
168:16 - column but this part is old or you might
168:18 - click around like under something else
168:20 - like the ec2 dashboard
168:22 - or maybe a better example might be amis
168:24 - i remember we're in here and something
168:26 - looked old here yeah see these are the
168:27 - old buttons and that's just how it is so
168:29 - everything is very uh modular and so
168:31 - they get updated over time so that is
168:33 - the challenge that you're dealing with
168:35 - you're always having like three
168:36 - different versions that are cobbled
168:38 - together in each uh
168:41 - ui one thing that i found really
168:43 - interesting is that um vpc has its own
168:45 - console management console but if you
168:47 - were to look up this in the uh the sdk
168:50 - so if i was to look up abs sdk
168:53 - ec2
168:55 - okay i'm just looking up ruby here as an
168:57 - example because that's what i know how
168:58 - to do
169:00 - if you look under here let's say you
169:01 - want to pragmatically work with vpcs
169:03 - you'd think that it would have its own
169:05 - top-level vpc because it has in the
169:07 - console its own
169:09 - its own
169:10 - management console but actually vpc is
169:13 - tightly coupled ec2 and so when you want
169:15 - to pragmatically use vpc you're going to
169:18 - be um using actually ec2
169:21 - as as how it was built so
169:23 - the the the what i'm trying to get is
169:25 - the apis don't one-to-one match with
169:27 - this kind of stuff and so it's just kind
169:29 - of interesting that there's those kind
169:31 - of uh differences uh but again it's not
169:33 - that big of a deal i'm just trying to
169:35 - say like you know keep your mind open
169:37 - when you look at the stuff okay
169:42 - [Music]
169:44 - so every aws account has a unique
169:46 - account id and the account id can be
169:48 - easily found by dropping down the
169:50 - current user in the global navigation so
169:52 - what i'm going to do is pull up my pen
169:53 - tool here and just show you it's right
169:55 - there uh the imbus account id is
169:56 - composed of 12 digits and so it could
169:59 - look like this or this or this the
170:01 - universal account id is used when
170:03 - logging in with a non-root user account
170:06 - but generally a lot of people like to
170:07 - set their own alias because it's tiring
170:09 - to remember your account id the you use
170:11 - it when you're creating cross account
170:13 - roles so you'd have the target account e
170:15 - the source account id to gain access to
170:17 - resources in another's account when
170:19 - you're
170:20 - dealing with support cases
170:22 - awast will commonly ask you what your
170:24 - account id is so they can identify
170:26 - the account that they want to look at
170:29 - and it is generally good to keep your
170:30 - account id private as it is one of the
170:32 - many components used to identify an
170:34 - account for attack by malicious actor
170:36 - so you don't have to be overly sensitive
170:38 - with it but you know try to hide it when
170:40 - you can when it's easy okay
170:42 - [Music]
170:46 - all right so let's talk about the
170:47 - account id which appears up here in the
170:50 - top right corner uh where you can get
170:52 - the account id it also appears in im so
170:54 - if we go over to iam
170:56 - and you look on the right hand side it
170:58 - should show you the example here it
171:00 - keeps on trying to take us to the old
171:02 - dashboard that's fine
171:04 - but you'll notice that it's over here
171:05 - and
171:06 - i don't have mfa turned on because i'm
171:08 - in my imuser account but it should be
171:10 - turned on on everything that's a given
171:12 - but you know i just want to show you
171:14 - where it is and also where you might be
171:16 - using it so one example where you would
171:18 - use
171:19 - you would need to know your account id
171:20 - would be something like creating a cross
171:23 - account policy so i went here and went
171:24 - to policy and went create policy
171:27 - um
171:28 - and we
171:30 - went to maybe it's a role i think we
171:32 - actually sorry we want to cross account
171:34 - roles not the policy sorry
171:36 - we go here
171:37 - and
171:39 - we say i want to access something in
171:40 - another abs account what we have to do
171:42 - is specify the account id specify the
171:44 - accounts that can use this role so you
171:46 - give i think the
171:48 - the id of the other account
171:51 - okay and so that is one place where
171:53 - you'd use it another place would be when
171:55 - you're creating policies
171:57 - so if i go back to policies here i can
172:00 - create a policy here
172:02 - and i can just choose something like s3
172:05 - okay
172:07 - and i'll just choose list
172:09 - and under the request conditions
172:12 - i might specify i think the account id
172:15 - it should be in here
172:17 - um
172:19 - i know i can limit
172:21 - based on account id
172:23 - principal account
172:28 - you could do principal account so if i
172:29 - just looked up this here
172:31 - address principal account
172:35 - and you just got to get used to google
172:36 - and things because that's always what's
172:38 - happening here
172:39 - and so we should be able to specify an
172:41 - account id yeah like that so that would
172:44 - be the principle there so if i just took
172:46 - that and it doesn't matter what it is we
172:48 - just put the value in here
172:51 - um string equals this
172:54 - add
172:55 - i should be able to go over here and now
172:56 - see the full statement nope sometimes
172:58 - that happens because we don't have it
172:59 - fully filled out
173:03 - but um yeah so that pretty much
173:05 - that's pretty much how we use it like it
173:07 - would normally show up as that so if i
173:09 - just go ahead and go next the policy
173:11 - contains an error you are required to
173:13 - choose a resource
173:15 - what do you mean the resource is this
173:16 - right oh down here okay sorry
173:19 - so we'll just say all resources then we
173:21 - flip over now it's valid and so here we
173:23 - can see our condition saying only from
173:25 - this account id that it is allowed
173:28 - um other places we're going to see
173:29 - account ids are in um
173:32 - arn's right so if we had an ec2 instance
173:36 - we don't have one launched right now
173:38 - but if i was to go ahead and
173:41 - oh maybe we have some prior ones yeah so
173:43 - if i was to check box this here
173:46 - and you might not have any prior ones so
173:48 - there might not be nothing for you to
173:49 - see but if you look for the arn
173:53 - where is our iron
173:57 - sometimes it doesn't show the iron in
173:59 - the services sometimes it does
174:01 - i wish that abuse always showed the iron
174:03 - to make our lives a bit easier but it
174:05 - could be because of other reasons why
174:07 - but
174:08 - even though we don't have the rn i think
174:09 - it shows it shows us the owner id
174:12 - and so that's the account the count id
174:14 - number you can tell because it's 12
174:15 - digits so hopefully that gives you kind
174:17 - of a tour of the account id and what its
174:20 - purpose is in the account okay
174:22 - [Music]
174:26 - all right let's take a look at aws tools
174:28 - for powershell so what is powershell
174:30 - powershell is a task automation
174:32 - configuration management framework is a
174:34 - command like shell and a scripting
174:36 - language so here it is over here uh if
174:39 - you're a windows user you're used to
174:40 - seeing this because it has a big blue
174:42 - window so unlike most shells which
174:44 - accept and return text powershell is
174:46 - built on top of the dot net common
174:48 - language runtime clr accepts and returns
174:51 - the dotnet objects so
174:54 - aws has a thing called the interbus
174:56 - tools for powershell and this lets you
174:57 - interact with the aws api via powershell
175:00 - commandlets
175:02 - is a special type of command in
175:03 - powershell in the form of the
175:05 - capitalized verb and noun so in this
175:07 - case it'd be new uh hyphen s3 buckets so
175:10 - you know we looked at the awcli and that
175:13 - is generally for bash um you know shells
175:17 - and so powershell is just another type
175:18 - of shell that's very popular and i just
175:20 - wanted to highlight it for those people
175:22 - that are uh you know used to using
175:24 - microsoft workloads or azure workloads
175:26 - uh that this actually exists okay
175:32 - all right let's take a look at the
175:33 - powershell tools um i actually haven't
175:35 - used this one yet so i'm kind of curious
175:37 - i am on a windows machine so if i was to
175:40 - open cm or
175:42 - powershell
175:43 - and you probably can't see this but if i
175:44 - just bring this over here if i type in
175:47 - powershell on my computer
175:49 - you'll notice that i have it um so
175:51 - that's how you would launch it looks
175:52 - like a blue screen here
175:53 - okay um if you're on a mac you're not
175:56 - going to have that but that's totally
175:57 - fine we don't need to have a windows
175:59 - machine to use powershell because we can
176:00 - go ahead and use cloud shell so make
176:02 - sure you're in a region that supports
176:04 - cloud shell so i switch back to north
176:06 - virginia
176:07 - this is not important for the exam but
176:09 - it's just kind of fun for me to go
176:10 - through this with you if you just like
176:12 - want to watch uh here and so i want to
176:14 - change this over to powershell so i
176:15 - imagine that it must be over here
176:18 - um so
176:20 - how do we change to powershell so we'll
176:23 - type in
176:24 - advanced power or aws cloud shell
176:29 - power shell like how do we do it
176:33 - okay and so we're just going to scroll
176:34 - down here
176:37 - so the following shells are
176:38 - pre-installed uh the bash the powershell
176:40 - the z-shell you can identify them by
176:42 - that yeah of course
176:43 - to switch to new shell enter the shell's
176:45 - program name in the command line prompt
176:46 - oh wow that's easy so um if we want pwsh
176:50 - do we just type pwsh let's find out
176:57 - give it a moment to think oh there we go
176:59 - okay so now we're using powershell and
177:01 - so i would think that databus would give
177:03 - this pre-installed for us so if we go
177:05 - over here to the instructions and we
177:07 - scroll on down there's probably like oh
177:09 - wait like i don't use powershell a lot
177:11 - it's very easy to install modules i've
177:13 - done it before
177:14 - but i never remember how to do it but
177:16 - let's just see what we can find here so
177:19 - i want
177:20 - the documentation for powershell here
177:22 - and i'm going to go to the um
177:25 - the maybe the reference here
177:28 - because i just want to see some examples
177:30 - for the commandlets and so we'll look
177:31 - for s3 again never done this before but
177:34 - i'm always great at jumping into these
177:36 - things and all i want to do is just list
177:38 - out the buckets so i'm going to just
177:39 - search for the word list
177:42 - and just see if i can find something
177:43 - very simple here
177:46 - and calls to get the list buckets api
177:49 - operation so i think that is what we're
177:50 - going to be doing here so i'm going to
177:53 - click into that
177:54 - okay
177:57 - and then from there
177:59 - what i'm going to do is just see if i
178:01 - can copy this command so we will go
178:03 - ahead and copy this and paste it in here
178:06 - and i like how we got this little shell
178:08 - here so we can tweak it so we need the
178:10 - bucket name but i don't want to
178:12 - return a list of all the buckets owned
178:13 - by the author so
178:15 - we don't have a bucket name that we want
178:16 - to explicitly set here so it's required
178:18 - false so we can remove that
178:21 - okay we'll look at the next one select
178:24 - required false use the select command to
178:25 - control the command line output the
178:26 - default is bucket specifying select will
178:29 - result in
178:30 - turning all the whole buckets
178:33 - for that specifying the name
178:36 - but it says it's not required so let's
178:39 - just take that out as well
178:40 - i don't think we need any of these
178:42 - actually let's just go and put that in
178:44 - there and i think that
178:46 - there must be something we need to put
178:48 - in front of that right well let's just
178:49 - see what happens
178:53 - uh the term is not recognized as the
178:55 - name of the command function script is
178:57 - operable so i think we're missing
178:59 - something in front of here
179:03 - we'll go to the user guide here quickly
179:06 - and we'll get to the getting started
179:10 - i just want a super simple example here
179:14 - new bucket get bucket
179:17 - well let's try this one here because
179:18 - they have it here
179:20 - and so it should just work right
179:24 - i'm going to change this to usc 1.
179:29 - the term new bucket is not recognized as
179:30 - the name of the commandlet function so
179:32 - i'm guessing that the commandlet's not
179:34 - installed i would have thought that they
179:35 - would have installed it by default so i
179:37 - guess what we'll do is look at how to
179:39 - install it
179:40 - so
179:41 - installing on
179:43 - linux i suppose
179:47 - so
179:47 - [Music]
179:49 - you can install the modulized version of
179:50 - the powershell on computers to install
179:53 - aws tools on linux pwsh to start
179:56 - powershell core session so i guess
179:57 - that's how you must start it on linux
179:59 - and then install the module this way so
180:02 - yeah i said it's easy to install these
180:03 - things we'll hit enter
180:06 - cross your fingers hope this works hope
180:07 - this is fast
180:14 - i'm just going to take a look here peek
180:16 - forward here if you are not if you're
180:17 - notified the repository is untrusted
180:19 - you're asked if you want to trust anyway
180:21 - just hit y so we're waiting for that
180:23 - here um you're installing this module
180:25 - from untrusted repository it's funny
180:27 - that it's untrusted by but it's by aws
180:30 - maybe that's some kind of drama between
180:31 - microsoft not letting a bus have an
180:33 - official module there but it looks like
180:34 - it should be installed now so if i type
180:36 - in get s3 buckets here
180:40 - um
180:41 - unless i typed it wrong that still
180:42 - doesn't seem to be working if i go up
180:44 - here and try to create a new bucket
180:46 - still does not recommend recognize the
180:48 - command command lit here so there must
180:50 - be more going on here
180:54 - [Music]
180:56 - if you are notified you can now install
180:58 - the module for each service
181:00 - okay
181:02 - what did we do
181:04 - you're installing the the modules from
181:06 - untrusted if you trust it change the uh
181:07 - change its installation policy value by
181:09 - running set policy command are you sure
181:11 - you want to install this module from the
181:13 - ps gallery so i said yes
181:15 - and i gave it a capital y
181:18 - and
181:19 - it didn't do anything else
181:22 - so
181:25 - oh hold on here so this is
181:28 - the installer
181:29 - and then here is the actual tool that we
181:31 - want to solve so it installed oh so we
181:33 - just installed this thing and now we use
181:35 - this thing to install s3 okay
181:38 - great not hard okay
181:41 - and so we'll just say yes to all
181:45 - and so that's going to install i guess
181:48 - everything oh we said ec2 and s3 well we
181:51 - didn't need both but that's fine
181:52 - and so what i'm going to do is go get
181:54 - bucket and so now recognize it it lists
181:56 - out the items here we can go and create
181:58 - ourselves a new bucket
182:00 - so we'll do that okay we'll make our way
182:03 - back over the database management
182:04 - console we'll go to s3 just because i
182:07 - don't need all these buckets lying
182:08 - around here
182:10 - and i'm going to go ahead and delete
182:12 - some of these buckets here so we'll say
182:13 - delete
182:15 - my bucket great
182:18 - and we'll go to this one here and say
182:19 - delete
182:21 - my bucket excellent
182:24 - all right so we have an idea how to use
182:26 - powershell and so powershell is just
182:28 - really popular because
182:29 - it's the way you do inputs it's very
182:32 - standardized and the outputs that come
182:33 - so it's very popular um and a very
182:36 - powerful scripting tool that's our cli
182:38 - tool as well so
182:40 - you know hopefully that's that was
182:41 - interesting for you but what we'll do is
182:43 - just close these off here and go back to
182:45 - our home page always just clicking that
182:47 - logo there and there we go
182:48 - [Music]
182:53 - so amazon resource names uniquely
182:55 - identify aws resources and arms are
182:57 - required to specify resource
182:59 - and ambiguously across all of all of aws
183:03 - so the iron has the following format
183:04 - variations so there's a few different
183:06 - things here but just notice here that
183:08 - sometimes it has a resource id or it has
183:10 - a path so with the resource type or
183:12 - could be separated by a colon so the
183:14 - partition
183:15 - can either be aws china or gov cloud
183:18 - because this is basically the aws portal
183:21 - or url that are completely separated
183:23 - from each other
183:25 - as we talked about those earlier in the
183:26 - course
183:27 - then there's the service identifier so
183:29 - ec2 s3 iam pretty much every service has
183:32 - their own
183:34 - service that name here that would be
183:36 - identified so the region would be pretty
183:38 - obvious usc 1 ca central 1 you'd have a
183:41 - count id which would be 12 digits
183:43 - the resource id
183:44 - could be a name or a path so like for
183:48 - imusers we have user bob this is an ec2
183:51 - instance and most of the irons are
183:53 - accessible via the airbus management
183:55 - console and you can usually click the rn
183:57 - to copy to your clipboard so here is it
183:59 - is for an s3 bucket and notice that it's
184:02 - a little bit different because it is a
184:04 - global service aws there's no reason to
184:06 - specify the region or the account id
184:09 - or
184:10 - anything else there like the resource
184:11 - type so straight away we already know
184:13 - it's a bucket so we can just say my
184:14 - bucket so that one's really short but in
184:16 - other cases it's really long so here it
184:18 - is for a load bouncer and it has all the
184:20 - information there and notice that like
184:22 - this as it passes load bouncer app my
184:24 - server
184:25 - will be and then it has the id okay
184:28 - for paths and arms they can also include
184:31 - a wildcard asterisk and we'll see these
184:33 - like with im policies or or paths these
184:36 - are really useful when you are doing
184:38 - um
184:39 - policies where you have to specify an
184:41 - army you want to say a group of things
184:42 - and things like that so there you go
184:44 - [Music]
184:48 - all right so now let's take a look at
184:50 - amazon resource name or also known as
184:52 - arn
184:53 - and so arns are used to reference
184:55 - objects they're very commonly used when
184:57 - you're using the cli or the sdk to
184:59 - reference to something um the easiest
185:01 - example is s3 right so if we go over to
185:03 - s3 here and we create ourselves a new
185:06 - bucket um so i'll go ahead and create
185:08 - ourselves a new one here
185:10 - we'll say my new bucket
185:12 - i'm just going to put a bunch of numbers
185:14 - in here it doesn't matter we'll hit
185:15 - create bucket
185:17 - and what we will see if we click into
185:19 - this
185:20 - is the orange should be under properties
185:23 - and there it is okay so there are many
185:26 - cases where you might want to use the
185:28 - iron and a lot of times you'll just copy
185:30 - it and a very common example would be
185:33 - again with i am policy so we go over to
185:35 - i am policies
185:37 - right
185:39 - and i want to get to policies here to
185:40 - save myself some trouble
185:42 - and we create a policy
185:44 - you know i might want to restrict
185:45 - someone to use only that bucket so let's
185:47 - say s3
185:49 - okay
185:50 - and then i'm going to say
185:51 - i want to be able to read and write from
185:53 - a particular bucket we go drop down
185:55 - these resources here
185:57 - and so
185:58 - here we have a lot of options
186:01 - maybe i'll just get rid of the read
186:02 - option
186:05 - and i'm going to actually expand right
186:06 - because it's just creating too much work
186:08 - for me here and i just want to have
186:12 - put put object that's that's what we use
186:14 - to put something into a bucket so we
186:16 - expand the resource here and notice it
186:18 - says add the iron so we go here
186:20 - and we could type the bucket name so
186:23 - do that or we just paste it on in here
186:25 - at the top so it's probably easier just
186:26 - to grab it sometimes
186:29 - but if you don't know an iron a lot of
186:30 - times you can just expand this and then
186:31 - fill it in and that's how you get an
186:32 - iron
186:33 - so put that there let's list oh you
186:36 - could also do it that way which is
186:37 - easier too
186:38 - and so now if i go to json is it valid
186:40 - there we go so here it's saying
186:42 - um this policy allows somebody to put an
186:45 - object into this particular bucket and
186:47 - so that would be an example where we
186:49 - would use
186:50 - an iron okay or if you're doing uh if
186:52 - you're using uh itabus support you might
186:55 - have to use an arm to um to get help
186:58 - from support saying hey look at this
187:00 - particular resource exactly here and
187:01 - then the the cloud support engineer can
187:03 - help you okay
187:04 - [Music]
187:08 - hey this is andrew brown from exam pro
187:11 - and we are looking at the abs command
187:12 - line interface before we do that we got
187:14 - to define some terms so what is a cli so
187:16 - a command line interface processes
187:18 - commands to a computer program in the
187:20 - form of lines of text operating system
187:22 - implement a command line interface in a
187:25 - shell okay so we have a terminal say
187:27 - terminal is a text only interface so it
187:29 - has input output environment then you
187:31 - have a console this is the physical
187:32 - computer to physically input information
187:34 - into a terminal then you have the shell
187:37 - a shell is the command line program that
187:39 - users interact uh with uh to input
187:42 - commands popular shell programs or bash
187:44 - zsh powershell and uh you might remember
187:47 - this one ms dos prompt so this has been
187:51 - around for obviously a very long time so
187:52 - maybe this kind of primes your mind for
187:54 - what is a shell and just so you know
187:57 - people commonly erroneously use terminal
187:59 - shell or console generally describe
188:01 - interacting with the shell so if we say
188:03 - shell or console or terminal we're just
188:04 - talking about the same thing but there
188:06 - is technically a difference between
188:08 - these three things but most people do
188:09 - not care and i wouldn't worry about it
188:11 - too much okay so now let's take a look
188:13 - at the database command line interface
188:15 - which allows you to pragmatically
188:16 - interact with the adobe's api via
188:18 - entering single or multi-line commands
188:20 - into a shell and then here i say or
188:22 - terminal but really it's just the shell
188:24 - okay
188:25 - so uh here is an example of one so we're
188:27 - trying to describe uh ec2 instances and
188:30 - then we're getting the output because we
188:31 - asked to have it back in this table like
188:33 - view
188:34 - so the abcli is a python executable
188:36 - program so python is required to install
188:39 - the awcli the awcli can be installed on
188:41 - windows mac linux unix the name of the
188:44 - cli program is aws you'll notice that up
188:46 - here in the top left corner there's a
188:48 - lot more to this but this is all we need
188:50 - for now okay
188:51 - [Music]
188:55 - hey this is andrew brown from exam pro
188:57 - and we are taking a look at the abyss
189:00 - cli and the easiest way to get started
189:02 - with this is actually via the cloud
189:04 - shell so you'll notice this little icon
189:06 - here in the top right corner that is
189:08 - cloud shell and it's going to allow us
189:09 - to um uh pragmatically do things without
189:12 - having to set up our own environments so
189:14 - if i just click that there okay
189:16 - uh and i say do not show again close and
189:20 - by the way if you don't see cloud shell
189:22 - here it could be your region so like if
189:24 - i go to canada central
189:26 - it doesn't have it there and so if i was
189:28 - to search cloud shell here
189:31 - okay it's going to say it's only
189:33 - supported in those regions so that's a
189:34 - bit annoying but once cloud shell loads
189:37 - it already has our credentials loaded
189:39 - within our account and so this is going
189:41 - to save us a lot of time in terms of
189:44 - you know trying to get set up with the
189:46 - exception that you have to wait for this
189:47 - environment to create so it takes a
189:49 - little bit of time but it's not that bad
189:51 - um and while that is waiting what i'll
189:53 - do is show you actually how you'd
189:54 - install the cli yourself so if we typed
189:56 - in about cli install
189:59 - all right and we went here
190:01 - the way you install i believe it's a
190:03 - python library but if we went to version
190:05 - 2 and we just said linux
190:08 - you go down here they'll have
190:08 - instructions so you just curl it unzip
190:10 - it and do that
190:12 - um so you know it's if it's this and
190:15 - then once it's installed you'll have the
190:17 - 8 of cli commands
190:18 - this is still going so you know maybe i
190:21 - can show you what it would be like to
190:22 - install the cli by hand so if we wanted
190:25 - to do that one easy way to do this is if
190:27 - we just go to github it doesn't matter
190:29 - what repository i'm just looking for
190:30 - anything here and if i open up git pods
190:32 - so if we go on the top here and type in
190:34 - gitpod.com
190:37 - maybe
190:38 - that
190:40 - i just want to see whoops
190:43 - maybe it's get pods
190:46 - like that
190:49 - oh get pod you're not giving me oh you
190:51 - know what it's dot io that's why okay so
190:53 - if we go back here
190:54 - sorry and we type in dot io
190:58 - what this will do is launch me a
191:00 - temporary environment and so this is
191:01 - outside of aws so i'd actually have to
191:03 - install the cli so this would be a great
191:05 - opportunity to show you
191:06 - how to install the cli i'm just doing it
191:08 - this way because git pod is free to use
191:10 - and
191:11 - um you know it's going to set up an
191:13 - environment and how let us simulate
191:15 - installing the cli so here is the cli
191:17 - here i'm going to see if i can bump up
191:19 - the font
191:20 - let's make the font as large as we can
191:22 - go
191:22 - light or dark dark sounds good to me
191:25 - and so if we type in aws
191:30 - and give it a moment we can see that we
191:31 - have uh the command here so if i say abs
191:34 - s3
191:35 - ls whoops
191:37 - that should be able to list things out
191:39 - in a bucket so this is what's currently
191:40 - in the bucket if you're wondering how do
191:41 - i know what these commands are i can
191:43 - just type in able cli commands
191:46 - okay and we go here
191:48 - and we go to the cli ref reference
191:51 - then we have um anything we want here
191:53 - right so we go down here and i just want
191:55 - to see what's running in s3 and i go
191:58 - here
191:59 - and i scroll on down it's going to show
192:00 - me commands like copy move remove
192:03 - sync uh mbrb
192:06 - list
192:07 - right
192:08 - and
192:10 - if you're looking for a particular
192:11 - command you go down say okay i'll look
192:13 - at ls here and it will explain to me all
192:15 - the little options that we can do with
192:17 - it and then it will always give me
192:18 - examples right so i can see examples
192:21 - like that so if i wanted to move
192:22 - something into an s3 bucket so let's say
192:24 - i want to create a new s3 bucket um
192:27 - we'll type in aws s3 and just hit enter
192:30 - and it should tell us um the sub
192:32 - commands maybe if i do like help like
192:34 - this
192:39 - and if we scroll on down so i guess it
192:41 - just pulls up documentation let's open
192:43 - it we give us like a tiny summary
192:48 - okay so what we can do here because i
192:50 - want to create a bucket
192:52 - type in like buckets
192:54 - if you don't know something you just go
192:56 - about s3 cli
192:58 - create bucket we'll go here
193:02 - and then what i do is i always just go
193:04 - to examples here so we have aws s3 api
193:08 - create bucket and i know it's unusual
193:09 - there's an s3 and there's an s3 api i
193:11 - don't know why that is but it's always
193:13 - been that way and i just don't question
193:15 - it anymore and so here i can go ahead
193:17 - and create a new bucket so i'll just go
193:18 - ahead and paste that command in i do
193:20 - want to change it up a bit here because
193:22 - this name could be that has to be unique
193:24 - so just to make sure i get what i want
193:26 - i'm putting random numbers in here we're
193:27 - going to choose the region as us east
193:29 - one if i wanted to do other things here
193:31 - i could scroll up and look at some flags
193:34 - here so
193:36 - uh it looks all fine to me
193:39 - so i think i'll go back here and just
193:40 - hit
193:42 - paste
193:44 - okay and so it created that bucket for
193:46 - me
193:47 - if i go over to s3
193:53 - and we'll wait here a moment we can see
193:56 - that bucket now exists if i wanted to
193:58 - place something in that bucket what i
193:59 - can do is just like touch a file so i'll
194:00 - just say touch touches a linux command
194:03 - to make just an empty file so we'll say
194:06 - hello.txt
194:08 - and then it would be a bus s3 um
194:10 - [Music]
194:12 - it would be sp to copy it and i'm going
194:14 - to give it the local path hello dot txt
194:16 - and then i need to give it the bucket
194:18 - address so it'd be s3 colon forward
194:21 - slash forward slash the bucket name so
194:24 - we named it this i'm not even going to
194:26 - try to type that in by hand because it's
194:27 - too hard and then i want to say where i
194:29 - want to put this file so i'm going to
194:30 - say hello.txt and if i'm right that
194:32 - should work as expected
194:34 - and so it says i uploaded that file i
194:36 - make my way back over to s3 i refresh
194:38 - there is the file
194:40 - if i want to copy this file back locally
194:43 - all i have to do i'm just going to
194:45 - remove i'm going to delete the original
194:46 - hello txt file
194:48 - ls to show you that there's nothing
194:50 - there
194:51 - and what i need to do
194:53 - oops
194:54 - is just revert this so instead of saying
194:56 - the address here
194:59 - we can go and type in
195:02 - hello.txt
195:04 - and if i do ls there's the file if you
195:07 - don't know what the address is of the
195:08 - bucket um a lot of times you can go here
195:10 - and find it so
195:11 - it should be
195:13 - because they're always changing this ui
195:14 - on me but we'll go to properties here
195:16 - and there that's the iron
195:20 - uh usually a good way to find it is if
195:21 - you go into an actual object so if you
195:23 - go here it will give you the full url so
195:25 - i could have grabbed that and i could
195:26 - have just pasted that in there
195:28 - um but you know you learn after time
195:31 - it's not hard to remember this s3 colon
195:33 - forward slash forward slash the unique
195:34 - name i do want to show you how to
195:36 - install it by hand so here i'm in get
195:38 - pods
195:39 - i'm not sure how i can change this to a
195:41 - dark theme
195:42 - because i really don't like this on my
195:44 - eyes we'll go down below here to color
195:46 - theme and we'll say get dark there we go
195:51 - and so
195:52 - this is a temporary workspace so when i
195:53 - close it it'll be gone so i'll be
195:55 - totally fine and so i'm going to type in
195:56 - abs to see that it's not installed we're
195:58 - going to go over here this runs linux by
196:00 - default so i already know that i'm going
196:02 - to use linux we want to use version 2
196:04 - here
196:06 - so
196:07 - for the latest version use this command
196:10 - for a specific version no we just want
196:12 - the generic one so i'm going to go ahead
196:13 - and copy this
196:14 - whoops yes allow we'll paste that in
196:16 - we'll hit enter
196:18 - okay then we'll take the next command
196:23 - paste that in hit enter
196:25 - we'll go take the next command here
196:28 - we'll hit enter
196:31 - you can now run uh aws so we type aws
196:35 - and there's a command so
196:37 - uh the only thing is that if we do a bus
196:38 - s3 ls it's not going to work because we
196:41 - don't have any credentials set so we'll
196:44 - give it a moment to think so it says
196:46 - unable to locate credentials you can
196:47 - configure credentials by running it was
196:49 - configured so we type in ito's configure
196:52 - and by the way if this font is too small
196:54 - i believe i can bump it up like this
196:58 - not a great way to do it but it works
197:03 - and so it says databus access key id so
197:05 - what we can do is go over to iam
197:10 - and what i'm looking for is my
197:12 - particular user over here
197:15 - and if you remember when we first
197:17 - created our account it generated out
197:18 - access key so i go to security
197:19 - credentials and so
197:22 - we have a key here but i need the secret
197:23 - so this key is useless to me so i'm
197:25 - going to go ahead and deactivate it
197:28 - just because i don't even want this key
197:31 - and i'm going to create myself a new key
197:32 - so i'm going to have an access id and
197:34 - secret whenever you generate these out
197:36 - never ever ever ever ever show anyone
197:38 - what these are these are your
197:41 - yours and yours alone okay so this is
197:44 - cloud shell we're fine we're just gonna
197:45 - close that for now
197:47 - and i'm gonna go back over to get pods
197:49 - here and hit enter so that's the id
197:52 - i'm gonna go grab the secret
197:55 - hit enter paste
197:56 - and i want it to go to us east 1 to save
197:59 - myself some trouble
198:00 - you can change the output from json to
198:02 - tables i'm going to leave it as the
198:03 - default here and so now if i type a bus
198:05 - s3 ls
198:09 - i get a list and so if i want to grab
198:11 - that file there and grab that s3 uri and
198:13 - we type in aws s3
198:16 - api or sorry it's just ls sorry or sorry
198:18 - cp
198:20 - and we're going to paste that link in
198:21 - and we're going to say hello.txt
198:25 - and i must have done the command wrong
198:26 - it's because we're missing s3 here
198:28 - i just hit up on the keyboard to get
198:30 - that command back and so i type in ls
198:32 - for list
198:33 - and i mean i have some other code here
198:35 - so you know again any repo you want on
198:37 - github it doesn't really matter
198:39 - but you'll see there is that file
198:40 - probably shouldn't use this one because
198:42 - it makes a bit of a mess
198:44 - um but yeah it's pretty straightforward
198:46 - just to one thing to show you is where
198:48 - those credentials are stored so by
198:50 - default they're going to be stored in
198:53 - um
198:54 - it's going to be in the
198:56 - hidden directory in your root or your
198:58 - home directory called above stock
198:59 - credentials so if i just do like ls here
199:02 - you can see there's a config file and a
199:04 - credentials file cat lets me print out
199:06 - the contents of that file so i go here
199:08 - and it's saying the default region is
199:10 - usc 1. this is a tombl file even though
199:12 - it doesn't have a dot tom along the end
199:14 - of it i just know by looking at it
199:15 - that's what it is config lets you set
199:18 - defaults that are going to apply to all
199:19 - of your credentials
199:20 - and then within the credential file here
199:22 - is the actual credentials so if you
199:25 - wanted to just set them you could go in
199:27 - here and just set them in here you can
199:29 - also set multiple credentials so if i go
199:31 - here and i'm going to open up and buy
199:33 - because i'm not sure how to open it up
199:35 - here in the main one but if you wanted
199:37 - multiple accounts you would do like exam
199:39 - pro and then you just repeat these with
199:41 - different keys
199:43 - right and then when you wanted to use a
199:45 - cli command actually i'm going to go
199:46 - back here for a second
199:50 - okay
199:51 - and if you want to
199:53 - um
199:55 - and by the way i'm using vi if you never
199:57 - use vim it's it's a bit tricky to use uh
199:59 - you might want to use nano instead if
200:01 - you're if you're kind of new to this
200:03 - because this will use like regular key
200:05 - key cuts and then down below it shows
200:06 - you what it is so this is like control x
200:08 - or alt x
200:10 - alt text no control x there we go
200:13 - um but anyway so if i go into this file
200:15 - and i delete the original one right and
200:17 - now i try to do
200:21 - um
200:22 - this command here even though we already
200:24 - have that file it should either hang or
200:27 - complain
200:28 - i could just kill that by doing control
200:30 - c if i do a bus s3 ls
200:34 - just notice that it's hanging so unable
200:36 - to locate credentials because there's no
200:37 - default one but if i go and i put
200:39 - profile and i say exam pro
200:44 - all right it'll now use that profile so
200:46 - that's the way we do it
200:48 - but hopefully that gives you kind of a
200:50 - crash course into the cli
200:53 - so yeah there you go okay so i'm just
200:56 - going to go ahead and
200:57 - close these off you can delete this
200:59 - bucket if you don't want it
201:01 - it's probably a good idea to delete this
201:03 - here
201:04 - and i'm just going to say permanently
201:05 - delete
201:06 - okay
201:08 - very very good okay close that off and
201:11 - yeah that's the introduction to the cli
201:13 - so yeah there you go
201:16 - [Music]
201:20 - hey this is andrew brown from exam pro
201:23 - and we are taking a look at software
201:24 - development kits uh so a software
201:26 - development kit or sdk is a collection
201:28 - of software development tools and one
201:30 - installable package so you can use the
201:33 - aws sdk to programmatically create
201:36 - modify delete or interact with aws
201:38 - resources so the innovas sdk is offered
201:40 - in a variety of programming languages so
201:43 - we have java python node.js ruby
201:47 - go.net php javascript c
201:50 - plus and so here would be an example of
201:54 - some ruby code where we are creating
201:55 - ourselves um an s3 bucket so we're just
201:58 - uploading a file there okay
202:01 - [Music]
202:05 - okay so now what i'm going to do is show
202:07 - you how to use the abyss sdk and so uh
202:10 - to do that uh we're going to need some
202:11 - kind of ide
202:13 - a a basically code editor and so we had
202:16 - looked at get pods which is a third
202:17 - party service and that's fine but let's
202:19 - take a look at cloud9 because that is
202:20 - built into aws
202:22 - so if i just type in cloud9 here and go
202:24 - over to ide i'm going to launch myself a
202:26 - new environment so i'll hit create i'm
202:28 - going to say my sdk environment
202:33 - env if you if you have our timetable
202:35 - environment like me
202:37 - and we have some options so create an
202:38 - ec2 instance for direct access create it
202:41 - via systems manager run a remote with
202:42 - ssh i'm going to leave it as the default
202:44 - then we have the option to choose what
202:46 - size i want to leave it on t2 micro
202:48 - because that is the free tier then we're
202:50 - going to scroll on down we have amazon
202:52 - x2 linux ami i'm going to stick with uh
202:55 - amazon linux 2 and we can have it turn
202:57 - off after 30 minutes a great option for
202:59 - us here we'll go ahead and hit next and
203:02 - we'll hit create environment
203:04 - and so we're going to have to wait a
203:06 - little bit for this to launch it'll take
203:08 - a few minutes as that is going let's go
203:10 - to google type in in-bus sdk
203:13 - to get to the main page and so the idea
203:15 - here is that there are a bunch of
203:17 - different languages you can use c plus
203:19 - plus go java javascript.net node.js php
203:22 - python and ruby
203:23 - uh and so i'm a really big fan of ruby
203:26 - i've been using ruby since 2005 and so
203:28 - that's what we're going to do it in it's
203:30 - also really easy to use and
203:32 - it's a really great language so um
203:34 - you know down below it's just showing
203:36 - you that there's all these different
203:37 - things if we go down to the sdk here and
203:39 - we click on ruby
203:42 - we'll we have examples where you have
203:43 - the developer guide the api reference
203:46 - and so this tells you how to get started
203:47 - even here it's saying like hey go get
203:49 - started with cloud nine which is great
203:51 - as well i suppose
203:53 - um and so here might show you how to
203:55 - install it um and when we open up the
203:59 - api references this is what it looks
204:01 - like so a lot of times when i want to do
204:02 - something i know it's like i want to do
204:04 - something with
204:05 - s3 so i scroll on down here and i look
204:07 - for s3
204:09 - right
204:11 - and then i just kind of like
204:13 - uh scroll around and look you know what
204:15 - i mean sometimes you have to expand it
204:16 - go into the client every api is slightly
204:18 - different
204:19 - so you do have to kind of figure out how
204:21 - to navigate that i'm actually under s3
204:23 - right now so i'm looking for the client
204:25 - and i just know this from memory that
204:27 - this is where it is so first you create
204:28 - yourself a client and then you can do
204:30 - api operations so if i wanted to like
204:32 - list buckets
204:34 - i just searched the word list and i just
204:35 - scroll on down and there it is i click
204:37 - into that and i have an example of how
204:39 - to list a bucket so
204:41 - i'm going to go back to cloud9 and it is
204:42 - ready and it started in dark mode if
204:44 - yours is not in dark mode which really
204:47 - honestly why wouldn't you want dark mode
204:49 - if we go up to i think it's like file
204:52 - where is it uh preferences here gotta
204:54 - click the cloud9 option
204:56 - and
204:58 - i'm just seeing if it like remembers my
204:59 - settings i really like two two soft tabs
205:01 - here
205:02 - but uh there should be something for
205:04 - themes down below and so
205:07 - um
205:09 - that doesn't seem like that's it
205:11 - it used to be like a oh here it is if
205:13 - you go here
205:14 - and just choose like whatever you want
205:15 - i'm on jet dark here and so if it's on
205:17 - classic light or something you don't
205:20 - like you can fix that there but i'm just
205:22 - going to go here and just fiddle with my
205:24 - settings
205:25 - because i really like to use vim
205:28 - keys i don't recommend this if you are
205:30 - to change this if you are not a
205:32 - programmer but i'm just going to change
205:34 - it so that i can type here efficiently
205:36 - so i'm just looking for the option here
205:40 - and they moved it on me where did they
205:41 - move it
205:43 - it'd probably be like key bindings
205:46 - ah bin mode there we go again don't do
205:48 - that this is just for me so i can move
205:50 - around in a different way so
205:52 - what i want to do and by the way it
205:54 - looks like this default screen we could
205:55 - have just changed it here
205:57 - i just clicked through all that for
205:58 - nothing was here the entire time but
206:01 - what we need is we need to make sure
206:03 - that we have our credentials so if you
206:04 - type in aws
206:06 - s3 ls that's like my sanity check that i
206:09 - always like to do to make sure i have
206:10 - credentials notice that we didn't have
206:12 - to set up any credentials it was already
206:13 - on this machine which was really nice
206:16 - and so i'm going to create a new file
206:17 - here
206:18 - and it's okay if you don't know anything
206:19 - about ruby we're just going to have fun
206:21 - here and just follow along so i'm going
206:22 - to do example.rb i'm going to make sure
206:24 - ruby's installed by doing ruby hyphen v
206:26 - so it is installed which is great
206:28 - uh you need a gem file so say new
206:31 - gem file here
206:34 - and if we go back to
206:36 - the installation guide
206:38 - we need the gem sdk here
206:42 - actually i'm going to look at how to
206:43 - generate a gemfile gem file because
206:44 - there's some stuff that goes to the top
206:46 - of those files
206:48 - like this here
206:50 - i think we just need this line here so
206:52 - i'm just going to grab that
206:54 - whoops paste that in allow good
206:58 - and
206:59 - i you can do gem aws sdk that will
207:02 - install everything but uh we only want
207:04 - to work with
207:07 - s3 and so this is going to vary based on
207:09 - each language but i know that if we type
207:11 - in s3 we'll just get s3 and that's all
207:13 - we really need
207:14 - and so once we have that what we'll need
207:16 - to do is use a bundle install so we're
207:18 - going to make sure we're in the correct
207:19 - directory i'm going to type in ls down
207:20 - below notice the gem file is there and
207:23 - by the way if the fonts are too small i
207:25 - should probably bump those up
207:26 - let's see how we can do that
207:30 - uh editor size font
207:33 - user settings
207:39 - good luck trying to find
207:40 - today um
207:43 - project no
207:46 - you think it'd have to be under user
207:47 - settings right
207:50 - ah here it is okay so
207:52 - this is for
207:54 - probably the editor so we'll go to 18
207:55 - here
207:56 - co code editor here
208:00 - i'm trying to find the one for the
208:01 - terminal probably over here
208:04 - there we go
208:06 - much easier okay so notice we have
208:08 - example.rb and gemfile so we're in the
208:10 - correct directory make sure i save that
208:12 - i'm going to type in bundle install
208:15 - and that's going to install the gems
208:17 - give it a moment there it's going to
208:18 - fetch notice that it installed
208:21 - the aws sdk s3 and everything that it
208:24 - was dependent on
208:25 - and so now if we go over to our
208:26 - example.rb file really when you're
208:29 - coding for the cloud you can pretty much
208:30 - copy paste everything so over here we
208:33 - found this code here for s3 list buckets
208:36 - and so i'm going to go ahead and paste
208:38 - that on in
208:39 - okay
208:41 - and i know it looks really complicated
208:43 - but we can quickly simplify this so i
208:45 - know that this is just the output so i
208:47 - don't need that
208:48 - okay
208:49 - and in ruby you don't need parentheses
208:51 - or curlies if uh if you don't have any
208:53 - things there and so all i need to do is
208:55 - define a client
208:57 - so if i click
208:58 - uh if i go to the top here of this file
209:00 - i think we're in the client right now
209:02 - all the way the top all the way the top
209:05 - here
209:06 - that's what we need okay
209:10 - and so
209:11 - i'm going to paste that in now
209:12 - we can set the region here so i'm going
209:14 - to say us east one
209:16 - right and then you'd have your
209:17 - credentials
209:19 - because the credentials are on the
209:21 - machine in the
209:24 - credentials file they're going to auto
209:25 - load here i believe so i don't think i
209:27 - need to set them
209:28 - so i'm just going to take that out here
209:30 - for a second
209:32 - okay and i can do this if i want this is
209:34 - just slightly different syntax it might
209:35 - be easier to read if i do it this way
209:37 - for you
209:39 - okay and
209:41 - i don't need double client there so we
209:43 - have the client i like to name this like
209:44 - s3 so i know what it is
209:46 - and i put puts for the response
209:49 - i'm gonna do inspect
209:51 - and so puts is like print okay and so
209:53 - now if i type in bundle exect let's just
209:56 - make sure that it's in the context of
209:58 - our bundler file ruby
210:00 - example.rb um we have a syntax error on
210:03 - this line here unexpected thing here
210:07 - oh it's because of this it's because i
210:09 - commented it out so i'm just going to do
210:10 - curly parentheses comment out here
210:14 - okay
210:16 - actually to make it a bit easier i'm
210:18 - just going to bring this down like this
210:21 - okay and we'll paste that there
210:25 - okay and we'll try this again
210:29 - initialize constants a to bus oh yeah we
210:30 - have to require it so we have to require
210:33 - abs sdk s3 i think
210:36 - we'll hit up
210:39 - and uh we got a struck back so it is
210:41 - working
210:42 - we are getting an object back if we want
210:44 - to play around with this a bit more i'm
210:46 - just going to install another gem called
210:47 - pry pry allows us to um inspect code so
210:50 - we're going to do bundle install
210:52 - and i'm going to go
210:53 - back to ruby here i'm going to put a
210:56 - binding pry in here
210:58 - and then if i hit
211:00 - up and i do bundle exec ruby example.rb
211:04 - um
211:05 - i installed it right
211:06 - bundle install
211:10 - yes undefined method pry
211:14 - oh because i have to require it again
211:17 - bad habit here
211:19 - okay we'll hit up
211:21 - and so
211:22 - now i have an interactive shell and i
211:24 - can kind of analyze that object so we
211:25 - have a response if i type in rsp here i
211:28 - have the structure object i can type in
211:30 - buckets here
211:31 - okay and it's showing me a bucket i can
211:33 - give it get its name
211:36 - um
211:38 - oh i think it's an array so i think i'd
211:40 - say like i'd say like zero here
211:43 - or i could say first this is just how
211:45 - the ruby language works we say name i
211:46 - get the name
211:48 - creation date
211:49 - okay so you get the idea whatever you
211:51 - want to do
211:53 - you know you search for it you just say
211:54 - i want to delete a bucket i want to
211:56 - create a bucket right and you look for
211:58 - it so i say create bucket here
212:00 - i click on this
212:02 - and i can see the options and they are
212:04 - always really good about giving me an
212:05 - example and then down below they always
212:07 - tell you all the parameters that you
212:08 - have there so that's how the sdk works
212:11 - uh but yeah the credentials were soft
212:14 - loaded here but you could easily provide
212:15 - them yourself i should just show you
212:17 - that before anything else
212:19 - just because there's some variations
212:20 - there
212:23 - and i'm just trying to look for it
212:24 - because it is separate code
212:29 - so you could do this this is one way of
212:31 - doing it so you could do it separate
212:32 - from the code so if you only wanted to
212:34 - configure it once
212:36 - right because you could you could have a
212:37 - lot of clients you wouldn't want to keep
212:38 - on like for each client you wouldn't
212:40 - want to put region in every time so i
212:42 - could take this
212:43 - and put this right here okay
212:47 - and this is the file here where we have
212:49 - the credentials so this would be our
212:51 - um
212:52 - our access key and our id
212:54 - and so
212:56 - you never want to put your code directly
212:58 - just in here so if i open up if you go
213:00 - cat
213:02 - you would never want to do this but i'm
213:03 - just going to show as an example here
213:06 - credentials
213:08 - oops i got to get out of this exit
213:10 - address credentials
213:14 - oh did they not even show it on this
213:16 - machine which would be smart we wouldn't
213:17 - really want to see our credentials here
213:19 - uh hit up say ls
213:22 - oh no it's there okay
213:25 - cat whoops
213:28 - cru
213:30 - credentials there it is okay so
213:33 - you know if we look here we can see that
213:34 - there are credentials set it's a little
213:36 - bit different we have this like session
213:37 - token i guess it's to make sure that
213:39 - this expires over time but if i was to
213:41 - take these
213:42 - okay and i was just to paste them in
213:44 - here
213:52 - that's one way you would do it
213:54 - you never ever want to do this ever ever
213:57 - ever ever you never want to do this
213:58 - because you'll end up committing that to
213:59 - your code
214:01 - so this is really dirty to do so i don't
214:02 - ever recommend to do it
214:05 - if you wanted to have this applied to
214:07 - everything you could put it up here and
214:08 - so now when we call the clients
214:11 - we don't have to do it
214:12 - um of course if the they're loaded on
214:14 - the machine you don't have to do it the
214:16 - other thing is like if you if you want
214:18 - you could load them in via environment
214:19 - variables that's usually what you want
214:21 - to do so you say a bus access key
214:26 - right and then you say environment
214:28 - databus
214:30 - access secret
214:32 - and so you'd set those by doing i think
214:34 - it's like an export
214:36 - environment variables
214:39 - set
214:40 - in linux
214:41 - you think i know after like 15 years of
214:43 - doing this but i never remember so you
214:44 - type in export
214:46 - so you go down into whoops here you type
214:49 - in export and you just say something
214:51 - like i'm going to show an example to see
214:53 - if it works so i'm going to say hello
214:54 - world
214:56 - okay and if i do
214:57 - hello like that
214:59 - echo
215:00 - see it prints it out so that's how you
215:02 - would set it you'd set those there's but
215:04 - there's actually very specific ones that
215:06 - aws uses for
215:08 - the api and it's these ones here so you
215:11 - always want to use those
215:13 - okay so you put that in there
215:17 - and then there
215:18 - but of course you know like if they're
215:19 - already set in your machine you don't
215:20 - have to even specify those because it
215:22 - would auto load those environment
215:24 - variables i don't think they're set
215:25 - right now if we type in echo
215:27 - just take a look here is are they going
215:29 - to get auto loaded here
215:32 - no so
215:33 - but anyway so we could go here
215:35 - just as an
215:36 - example
215:38 - and well actually they just show them
215:39 - right here so you see your access key
215:41 - but we go and we type in
215:43 - export
215:45 - and i'm going to paste the key in there
215:46 - and i'm going to go to the front of it
215:48 - we're going to type a bus access
215:50 - key id equals
215:53 - enter
215:54 - and so now if i did echo on this aws
215:56 - access key
215:58 - id
215:59 - okay shows up but i just want to show
216:00 - you how it can kind of vary and those
216:02 - conditions around it so yeah that is the
216:04 - abuse sdk um and yeah a lot of times
216:07 - you're just copying pasting code and
216:08 - just kind of tweaking it you're not
216:09 - really writing
216:11 - real programming okay so hopefully that
216:12 - is less intimidating so i'm just going
216:14 - to close these off and i want to close
216:16 - down this cloud9 environment
216:18 - um
216:20 - i might have to reopen this up in
216:22 - another tab
216:24 - and go to the management console here
216:26 - and then go over to cloud9 and just
216:28 - close this tab
216:30 - and then i'll go ahead and delete this
216:31 - environment oops i'll just type delete
216:33 - here
216:34 - even if you didn't it would turn off
216:35 - after 30 minutes and you have that free
216:37 - tier so it's not that big of a deal it's
216:39 - up to you whether you want to use cloud9
216:41 - or git pods cloud9 is really good
216:43 - because it allows you to
216:46 - it allows you to uh
216:49 - use it runs on a virtual machine right
216:51 - so you have a
216:53 - a container runtime there and so it's
216:55 - very easy to run containers on it um
216:57 - whereas in like i've had some issues
216:58 - with git pods but um yeah those are the
217:01 - two okay
217:02 - [Music]
217:06 - well let's take a look at adam's cloud
217:08 - shell which is a browser-based shell
217:10 - built into the database management
217:11 - console and so cloud shell is scoped per
217:14 - region it has the same credentials as
217:15 - the logged in user and it's a free
217:17 - service so this is what it looks like
217:19 - and the great thing about this is that
217:21 - you know if you have a hard time setting
217:23 - up
217:23 - your own shell or terminal on your
217:25 - computer
217:26 - or maybe you just don't have access or
217:28 - privilege to do so it's just great that
217:30 - abuse makes this uh available to you and
217:33 - so what you can do is click the shell
217:34 - icon up at the top and that will expand
217:36 - this here some things to note about
217:38 - cloud shell is that it has some
217:40 - pre-installed tools so it has the cli
217:42 - python node.js kit make pip pseudo tar
217:45 - tmux vmwget vim and more it includes one
217:49 - gigabyte of storage free per aws region
217:52 - it will save your files in a home
217:54 - directory available for future sessions
217:56 - for the same in this region
217:58 - and it can support more than a single
218:00 - shell environment so it has bash
218:02 - powershell and zsh um and so enemies
218:06 - cloud shell is available in select
218:07 - regions so when i was in my canada
218:09 - region i was like where's the little
218:11 - shell icon but i realized it's limited
218:13 - for some areas okay
218:15 - [Music]
218:19 - hey this is andrew brown from exam pro
218:21 - and we're taking a look at
218:22 - infrastructure as code also known as iac
218:25 - and this allows you to write a
218:26 - configuration script to automate
218:28 - creating updating or destroying your
218:30 - cloud infrastructure the way you can
218:32 - think of isc it's a blueprint of your
218:34 - infrastructure and it allows you to
218:36 - easily share version or inventory your
218:38 - cloud infrastructure
218:40 - so aws has two different offerings
218:42 - for iac
218:44 - the first is cloud formation uh
218:47 - commonly abbreviated to cfn and this is
218:49 - a declarative iec tool and then you have
218:52 - abs cloud development kit commonly known
218:54 - as cdk which is an imperative iac tool
218:57 - so let's just talk about the difference
218:58 - between declarative and imperative and
219:00 - then we'll look at these tools a little
219:01 - bit closer uh each okay so declarative
219:04 - means what you see is what you get it's
219:06 - explicit it's more verbose but there's
219:08 - zero chance of misconfiguration unless
219:11 - the file's so big that you're missing
219:12 - something uh commonly declarative files
219:15 - are written in things like json yaml xml
219:17 - so for cloud formation it's just json
219:19 - and yaml
219:20 - and so that's that side there so for
219:22 - imperative you say what you want and the
219:24 - rest is filled in so it's implicit uh
219:26 - it's less verbose you could end up with
219:28 - some misconfiguration that's totally
219:30 - possible uh but it does more than
219:32 - declarative and you get to use your
219:34 - favorite programming language maybe
219:35 - python javascript actually cdk does not
219:38 - support ruby right now but i just have
219:39 - that in there just as a general
219:41 - description of what imperative is okay
219:44 - [Music]
219:48 - all right so just a quick look at
219:49 - cloudformation so cloudformation allows
219:51 - you to write infrastructure as code as
219:52 - either json or yaml the reason why was
219:55 - aws started with json and then everybody
219:57 - got sick of writing json and so they
219:59 - introduced yaml which is a lot more
220:01 - concise which you see on the right hand
220:03 - side so cloud formation is simple but it
220:05 - can lead to large files or is limited in
220:07 - some regards to creating dynamic or
220:09 - repeatable infrastructure compared to
220:10 - cdk a confirmation can be easier for
220:13 - devops engineers who do not have a
220:14 - background in web programming languages
220:16 - a lot of times they just know scripting
220:18 - and this basically is scripting since
220:20 - cdk generates out cloudformation it's
220:22 - still important to be able to read and
220:23 - understand cloud information in order to
220:25 - debug iac stacks knowing cloudformation
220:28 - is kind of a cloud essential when you go
220:31 - into the other tiers of aws
220:33 - like solutions architect associate
220:35 - professional or any of the associates
220:36 - you need to know cloud information
220:38 - inside and out okay
220:39 - [Music]
220:44 - okay so what i want to do now is
220:46 - introduce you to infrastructure as code
220:49 - and so we're going to take a look at
220:50 - cloud formation and so we were just
220:52 - using cloud9 for the sdk so we're going
220:54 - to go back and create ourselves a new
220:56 - cloud9 environment because we do have to
220:57 - write
220:58 - some code so i'll go ahead and hit
221:00 - create here and i'm going to just say uh
221:02 - cfn that's sort for cloudformation
221:04 - example
221:05 - and we'll hit next step
221:08 - and we'll create ourselves a new
221:09 - environment t2 micro
221:10 - amazon x2 is totally fine we'll hit next
221:13 - it'll delete after 30 minutes we'll be
221:14 - fine we're within the free tier we're
221:16 - going to give this a moment to load up
221:18 - and remember you can set your theme your
221:21 - your keyboard mode whatever you want as
221:23 - that loads and as that's going we're
221:24 - going to look up cloud formation
221:26 - and so cloud formation
221:29 - is very intimidating at first but once
221:31 - you get through the motions of it it's
221:33 - not too bad
221:34 - um so we'll go to the user guide here as
221:36 - we always do if you go to getting
221:38 - started
221:40 - it's going to just tell us some things
221:41 - it's going to read about yaml files
221:44 - um i don't think i really need to read
221:46 - much about this here so i think we'll
221:48 - just go start looking up some codes so
221:49 - something that might be interesting to
221:50 - launch is an ec2 instance
221:53 - cloudformation so that's what i'll do is
221:54 - i'll type in what i want so an ec2
221:56 - instance and i'll just start pasting in
221:58 - code so if we scroll on down below here
222:02 - i'm going to go to examples because i
222:03 - want a small example here this is
222:05 - something that i might want to do
222:07 - and we're going to give that a moment
222:08 - here
222:09 - it's almost done
222:12 - you can do a database come on
222:14 - as that is going i'm going to open a new
222:16 - tab
222:17 - i'm going to make my way over to
222:18 - cloudformation
222:21 - okay
222:25 - and
222:26 - you can see i have some older stacks
222:28 - here
222:29 - notice cloud9 when we create an
222:31 - environment actually creates a
222:32 - cloudformation stack which is kind of
222:33 - interesting
222:35 - um but if we go here
222:38 - we can create a stack and we can create
222:40 - a file and upload it here so
222:43 - okay this is good i'm going to go ahead
222:45 - and make a new file
222:47 - we're going to call it template
222:49 - dot yaml
222:51 - just so you know yaml can be yml or
222:53 - y-a-m-ml
222:56 - there's a big debate as to which one you
222:57 - use um i think that adabus likes it when
223:00 - you use the full version so i just stick
223:02 - with y-a-m-l
223:05 - i'm going to double click into that
223:07 - and so in the cc2 example i'm just going
223:09 - to copy this okay and i'm going to paste
223:12 - this in here
223:14 - and i'm going to type in resources
223:17 - oops capital
223:21 - okay so that's a resource i want to
223:22 - create
223:23 - um
223:24 - when you create cloud formation you
223:26 - always have a template version so
223:29 - i just need
223:31 - a basic example here at the top
223:35 - i guess that's a simple one is like a
223:36 - hello world bucket
223:40 - maybe we should do a bucket because
223:41 - it'll be a lot easier
223:45 - we don't have to make our lives super
223:47 - hard here
223:48 - okay um but what i'm looking for is the
223:52 - version because that's the first thing
223:53 - that you specify
223:55 - i'm just trying to find it within an
223:57 - example here
224:00 - oh for frick's eggs cloudformation
224:02 - version
224:04 - so they don't have the format version
224:05 - it's going to complain there it is okay
224:07 - so we'll copy that
224:10 - we'll go back over here
224:12 - we'll paste that in there
224:14 - it might be fun to do like an output
224:15 - here so i'm gonna do like an output
224:17 - outputs
224:19 - and uh maybe instead of doing this we'll
224:21 - type in a bus s3
224:24 - confirmation
224:27 - because what i'm looking for is what we
224:29 - can set as output so we'll say return
224:31 - values here
224:34 - um
224:37 - maybe we just want
224:40 - returns the domain name
224:42 - so we'll just say
224:44 - uh
224:46 - value
224:49 - ref that that's going to get the
224:50 - reference for it and we have to say
224:52 - hello bucket
224:56 - uh type string
225:02 - i'll say outputs confirmation example
225:06 - and even though i've written tons of
225:08 - cloud information it's just like if
225:09 - you're not doing it on day in day out
225:11 - you start to forget what it is
225:12 - so here for outputs we need a logical id
225:15 - description value and export so
225:18 - um
225:19 - that is what i want so i'm going to go
225:21 - ahead and copy that back here
225:23 - this is just so that when we run it
225:25 - we're going to be able to observe an
225:26 - output from the cloud formation file
225:29 - okay so the logical id is whatever we
225:30 - want so hello bucket domain
225:35 - it's funny because this is how you do
225:36 - do kind of that would be the format for
225:38 - terraform i was getting that mixed up
225:40 - so the domain
225:43 - of the bucket
225:45 - the value here is going to be ref
225:48 - hello
225:51 - bucket
225:52 - domain name
225:58 - that's the output
226:01 - export
226:03 - value to export
226:07 - uh can i get an example here
226:13 - else name
226:16 - oh you know what export is for uh cross
226:18 - stacks we don't need to do that okay so
226:20 - that's fine so what we'll do is set that
226:22 - and we'll take out our old one and so
226:24 - this should create us an s3 bucket so
226:26 - with cloudformation you can
226:28 - provide a template here by providing a
226:30 - url or you can upload a file directly
226:33 - so
226:35 - i'm just trying to decide here how i
226:36 - want to do this you can also use a
226:38 - sample file or create a template in the
226:40 - designer i'm going to go over to the
226:41 - designer
226:43 - because then we can just like paste in
226:45 - what we want so if i go over to yaml
226:46 - here
226:47 - and we go back over here i copy this
226:51 - i'm just going to paste this in here
226:55 - and we're going to hit the refresh
226:56 - button nobody ever uses the designer but
226:58 - this is just kind of an easy example for
227:00 - me to
227:01 - place this in here
227:09 - it's not really working maybe i go to
227:11 - template dude here
227:13 - refresh
227:16 - there we go so there's our bucket it's
227:17 - nice to have a little visualization and
227:19 - i believe this is going to work as
227:20 - expected so now that we have our
227:23 - designer template i think if we hit
227:25 - close what's this button say
227:27 - validate template probably good idea
227:28 - validating the template
227:30 - template contains errors unresolved
227:32 - resource dependency in the output block
227:34 - of the template hello
227:37 - domain
227:38 - bucket
227:41 - seems like it should be fine
227:46 - let's go oops
227:48 - let's go back over here
227:51 - that's what i did i said reference
227:54 - that value
227:56 - oh uh maybe it's get a trib okay
228:00 - it's get att sorry
228:04 - get a trib
228:06 - cloud formation
228:07 - i can't remember if there's an r on the
228:08 - end of it oh it's just att this is if
228:10 - you're trying to get a return intrinsic
228:12 - value so a reference is like what the
228:14 - default one is but every time we do like
228:16 - a logical name and attribute that's how
228:18 - we get that there so
228:19 - what i'm going to do here is just hit
228:21 - refresh
228:22 - and i'm going to validate that one more
228:24 - time
228:25 - now it's valid if i hover over this is
228:27 - it going to upload it create the stack
228:30 - we could save this save it
228:33 - but we can save it in s3 bucket so we'll
228:34 - say hello
228:37 - bucket
228:38 - and so now we have this url so i'm going
228:40 - to copy it honestly i never use this
228:42 - editor so it's kind of interesting
228:44 - i'm going to leave
228:47 - and we're probably going to hit create
228:48 - stack but i just find it a bit easier if
228:49 - we just kind of do it through
228:51 - this here so go back create the stack
228:53 - we're going to paste in the url we're
228:55 - going to say next
228:56 - and we're going to say
228:58 - my new stack
229:01 - and i didn't see what the name of the
229:02 - bucket was
229:04 - oh there's no name so it's going to
229:05 - randomize that's perfect so we'll go
229:07 - next
229:09 - we have a bunch of options here we'll
229:10 - hit next
229:12 - we'll give it a moment here i guess we
229:13 - have to review it create the stack
229:16 - and this is the part where we watch so
229:17 - it says create in progress and we wait
229:19 - and we hit refresh
229:22 - and we can see what's happening it's
229:24 - trying to create a bucket
229:28 - and if we go to resources this is this
229:30 - is a lot easier to track because you can
229:31 - see all the resources that are being
229:32 - created
229:40 - if you notice that when you use the cl
229:42 - uh when you're using the abs management
229:43 - calls in korean s3 bucket it's
229:44 - instantaneous but like with cloud
229:46 - formation there's a bit of delay because
229:47 - there's some communication going on
229:48 - board but here it is and notice if we go
229:50 - to our outputs this is the the value of
229:53 - the bucket domain name if we were to
229:55 - make it with uh self-hosting which is
229:57 - not what we're doing with it we could
229:58 - also have an export name which would be
230:00 - used for cross-referencing stacks which
230:01 - is not something we care to do
230:04 - but yeah that's how you create a stack
230:06 - that way
230:07 - um but you know we can also do it via
230:09 - the sdk here so
230:12 - what i can do
230:13 - um
230:14 - is look up what is the inves
230:17 - cli cloud formation
230:19 - because they have their own commands
230:20 - here if i go here
230:23 - there's a new one and there's an old one
230:26 - so
230:28 - if we go create stack
230:32 - yeah there's things like this like
230:33 - create stack update
230:35 - um so if we wanted to do it this way
230:41 - okay and i copied this here
230:44 - i'm just gonna put this in my readme
230:45 - here for a second
230:48 - uh so here what you do is you say my new
230:51 - stack
230:52 - and you can provide the template url or
230:55 - you could specify the local path here
230:58 - so we have like a template body so i'm
231:00 - gonna go ahead and grab that
231:03 - okay
231:04 - this would be like yaml
231:06 - and
231:07 - um i need to specify this file here so
231:10 - template.yaml
231:13 - and i'm just gonna go pwd here to get
231:16 - the full path
231:18 - okay
231:20 - and i'm going to just paste that in
231:21 - there oops
231:24 - okay i'm going to do ls
231:26 - okay so that gives us the full path of
231:28 - the file you can also specify the
231:29 - template url
231:31 - and so this should work as well if i
231:33 - take this and paste that on as a command
231:39 - it's unable to locate parameter file
231:41 - there's three
231:42 - three triple slashes there we'll just
231:43 - fix that there
231:46 - paste
231:48 - unable to load param file
231:51 - no such file of directory and there's a
231:53 - t missing
231:55 - okay
231:56 - be like don't be like me and make sure
231:58 - you don't have spell any mistakes okay i
232:00 - can type clear down here so i can see
232:01 - what i'm doing we'll hit enter whoops
232:06 - unable to load the parameter file notice
232:08 - file or directory
232:12 - home well i you didn't want the forward
232:14 - slash
232:15 - so another thing we can try to do i
232:16 - think it will take it relative so if i
232:18 - do this it should work
232:22 - i don't ever remember having to specify
232:23 - the entire path an error occurred while
232:26 - calling the crate stack my new stack
232:28 - name already exists if i go back over
232:29 - here give this a refresh oh that's what
232:31 - we named our stack the the one that we
232:33 - did so i'm going to say stack2
232:36 - okay
232:40 - format unsupported structure when
232:42 - calling the create stack operation
232:46 - are you kidding me i do this all the
232:48 - time
232:50 - template body
232:52 - yaml file cloudformation
232:59 - unsupported structure take a look here
233:08 - oh you know what i think uh this one's
233:10 - out of date that's why so what we can do
233:12 - is go to our old stack here and we can
233:15 - actually see the template i can go ahead
233:16 - and copy this whoops and we can go ahead
233:19 - and paste that in there and then now
233:21 - what i can do
233:23 - so you know that's that's the reason why
233:24 - it wasn't working okay so we'll hit
233:26 - enter
233:28 - um
233:28 - unsupported structure
233:31 - it should be supported
233:36 - let's see if cloudformation can help us
233:37 - out
233:39 - um
233:40 - apparently there was very unhelpful
233:41 - error message formatting so try the
233:43 - validate template option
233:45 - i wonder if we could just do this
233:48 - maybe if that would help here
233:51 - i'm just heading up to try to run it
233:52 - again
233:54 - nope i guess we can try to validate it
233:56 - here
233:57 - it's like i'm not having much luck here
233:59 - today
234:01 - so we'll just say this here
234:03 - maybe it's not even loading that file
234:04 - where it is i
234:11 - so there's no errors
234:16 - i'm just going to make this one line
234:25 - okay created so for whatever reason i
234:27 - must have had a bug there and so
234:29 - sometimes putting on one line helps that
234:31 - out because i must have had an obvious
234:33 - mistake there and now we can see the
234:35 - stack is creating it's doing the exact
234:36 - same thing it's creating a different
234:38 - bucket though if we go over to our s3
234:40 - here
234:44 - again you know you don't need to be able
234:46 - to do this yourself to pass the exam
234:48 - it's just so i'm just trying to show you
234:50 - like what it is so you kind of absorb
234:52 - any kind of knowledge about what's going
234:53 - on here notice down below it uses the
234:55 - stack name followed by uh the read the
234:58 - logical name of the resource there
235:00 - okay
235:02 - and what we'll do is wait for that to
235:03 - create once that's created we can go
235:04 - ahead and delete these stacks we could
235:06 - also use the aws cloud formation to say
235:08 - like delete stack but i don't want to
235:10 - bore you with that today
235:13 - and so we'll hit refresh here wait for
235:15 - those stacks to vanish
235:18 - okay those are gone uh what i'm going to
235:20 - do is kill this cloud9 environment
235:22 - if there's a way to do it from here i
235:24 - have never known how to do it go back to
235:26 - your dashboard well that's nice to know
235:29 - we'll go ahead and just delete this
235:33 - okay
235:34 - we'll close that tab
235:35 - and so now we are all in good shape and
235:37 - so that was our introduction to
235:40 - cloudformation okay
235:44 - [Music]
235:46 - let's take a look here at cdk so ctk
235:48 - allows you to use your favorite
235:49 - programming language to write
235:50 - infrastructure as code and technically
235:52 - that's not true because they don't have
235:54 - ruby and that's my favorite but anyway
235:56 - some of the languages include node.js
235:58 - typescript python
236:00 - java.net and so here's an example of
236:02 - typescript typescript was the first
236:04 - language that was introduced for cdk
236:07 - it's usually the most up-to-date so
236:09 - not always does cdk reflect exactly
236:11 - what's in cloud formation but i think
236:13 - they're getting better at that okay so
236:15 - cdk is powered by cloudformation it
236:17 - generates outcloud formation templates
236:18 - so there is an intermediate step uh it
236:20 - does sometimes feel a bit slow so i
236:22 - don't really like that but you know it's
236:23 - up to you cdk has a large library of
236:26 - reusable cloud components called cdk
236:28 - constructs at constructs.dev this is
236:30 - kind of the concept of terraform modules
236:32 - it is really really useful uh and
236:34 - they're really well written and they can
236:37 - just reduce a lot of your effort there
236:38 - ct cdk comes with its own cli um and i
236:42 - didn't mention this before but cloud
236:43 - formation also has its own uh
236:45 - cli okay cdk pipelines uh are
236:49 - allow you to quickly set up ci cd
236:51 - pipelines for cdk projects that is a big
236:53 - pain point for cloud formation where you
236:55 - have to write a lot of code to do this
236:57 - whereas
236:58 - the cdk has that off the bat makes it
237:00 - really easy for you cdk also has a
237:03 - testing framework for unit and
237:04 - integration testing i think this might
237:06 - be only limited to typescript because i
237:07 - didn't see any for the rest of the
237:08 - languages but um you know i wasn't 100
237:11 - sure there
237:12 - this
237:13 - one thing about cdk is that it can be
237:15 - easily
237:16 - confused with sdk because they both
237:18 - allow you to pragmatically work with aws
237:21 - uh using your favorite language but the
237:23 - key difference is that cdk ensures uh it
237:26 - opponents
237:28 - of your infrastructure so what that
237:29 - means that's such a hard word to say but
237:32 - what that means is that um
237:35 - you know if you use this cdk to say give
237:38 - me a virtual machine you'll always have
237:40 - a single virtual machine uh because it's
237:43 - trying to manage the state of the file
237:45 - whereas uh when you use sdk if you run
237:47 - it every time you'll end up with more
237:48 - and more servers uh and it's not really
237:50 - managing state so hopefully that is
237:52 - clear between the difference there
237:54 - [Music]
237:58 - okay so we looked at cloud formation but
238:00 - now let's take a look at cdk cloud
238:02 - formation or confirmation cloud
238:04 - development kit it's just like cloud
238:06 - formation but you use a programming
238:08 - language in order to implement your
238:10 - infrastructure as a code i don't use it
238:12 - very often i don't particularly like it
238:14 - but you know if you are a developer and
238:16 - you don't like writing cloud formation
238:17 - files and you want to have something
238:19 - that's more pragmatic you might be used
238:20 - to that um this i think should be
238:22 - deleting because we were deleting the
238:24 - last one here and notice how it's grayed
238:25 - out i can't select it so don't worry
238:27 - about that create a new one it will say
238:29 - example we'll hit next
238:32 - t2 micro ec2 instance amazon x2 you know
238:35 - the drill it's all fine here we'll go
238:37 - ahead and create ourselves a new
238:38 - environment we're going to let that spin
238:40 - up there and as that's going we're going
238:42 - to look up
238:43 - adabus cdk
238:45 - so it was cdk
238:48 - and we probably want to go to github for
238:50 - this
238:52 - okay because it is open source
238:54 - and so i want to go to getting started
238:58 - and i have used this before but i never
239:00 - can remember how to use it
239:01 - probably the easiest way to
239:03 - use this is by using typescript
239:06 - so
239:08 - here's an example initialize a project
239:10 - make directory cdk oh first we gotta
239:12 - install it right
239:14 - so give that a moment so this is node
239:15 - you know how we did like bundle install
239:17 - this is like the same thing but for uh
239:20 - typescript installer update the it was
239:22 - cdkcli from npm we recommend using this
239:25 - version
239:26 - etc etc
239:28 - so again we're just waiting for that to
239:29 - launch but as we wait for that it's very
239:32 - simple we're just going to install it
239:34 - create a directory
239:36 - go into that directory initialize the
239:38 - example
239:39 - here it's setting up an sqsq
239:43 - which is um that's quite a complex
239:45 - example
239:46 - but you can see it's code right and then
239:48 - we run cdk deploy and we'll deploy it
239:50 - and then hopefully we'll have that
239:52 - resource so again we're just waiting for
239:56 - cloud nine
239:59 - there we go so cloud nine is more or
240:01 - less ready uh terminal seems like it's
240:04 - still thinking
240:06 - and we have a javascript one which i do
240:08 - not care about there we go there's our
240:09 - environment we're going to make sure we
240:10 - have npm so we can type in npm
240:14 - great it says version 8.1.0
240:18 - and so
240:19 - this is asking for 10.
240:22 - okay i don't know if this gives us like
240:24 - nvm installed mvm
240:26 - it does so what we can do is do mvm list
240:28 - that stands for node version manager
240:30 - ruby has one as well and so it's telling
240:32 - us what version we're on i want to
240:35 - update um looks like we have a pretty uh
240:37 - pretty new version but what i want is
240:39 - the latest version of
240:41 - oh but that's node version that's not
240:43 - necessarily npm so we'll do node version
240:45 - oh 17 okay we're well
240:48 - well in the uh range of the new stuff so
240:50 - what i'm going to do is scroll on down
240:52 - we're going to grab this link here or
240:54 - this code here hit enter
240:56 - and that's going to install the adabus
240:58 - cdk
240:59 - so it says
241:02 - file already exists oh so maybe it's
241:03 - already installed on the machine
241:08 - um
241:09 - cdk let's type in cdk
241:12 - because of course aws wants to make it
241:14 - very easy for us this software has not
241:16 - been tested with
241:17 - what was that warning
241:19 - with node 1701 you may encounter runtime
241:21 - issues great aws you're like the one
241:23 - that installed this stuff here so we get
241:25 - a bunch of the commands which is great
241:27 - and so what we'll do is follow their
241:29 - simple instructions we'll say hello cdk
241:32 - we will cd into this
241:35 - and
241:37 - now what we can do is run cdk init
241:41 - and this language here
241:44 - and so that's going to do a bunch of
241:45 - stuff creates tons of files it's going
241:48 - to vary based on what you're using
241:50 - like which language because cdk comes
241:53 - available in a variety of languages so
241:55 - if you type in aws cdk
241:58 - documentation here
242:04 - notice up here
242:05 - python java.net
242:06 - so i think it has more than just those
242:08 - three languages but um
242:11 - you know i wish it supported more like
242:13 - yeah i see here is c-sharp java
242:16 - but i really wish there was a ruby
242:20 - so we'll give this a moment here to get
242:22 - installed and i will see you back here
242:24 - when it is done okay
242:29 - okay uh it turns out i only had to wait
242:31 - like a second there but it says there's
242:33 - a newer version of the cdk you probably
242:35 - should install it but
242:37 - i just want to get going here so as long
242:39 - as i don't run into any issues i do not
242:40 - care um but anyway so looking at this
242:44 - and again i rarely ever look at this but
242:46 - i'm a developer so it's not too hard for
242:48 - me to figure out but under the lib this
242:49 - is our stack that we're creating and
242:51 - here is it is loading in sqs it's
242:54 - loading in sns
242:56 - and then the core library it's creating
242:58 - an sqsq
242:59 - and it's setting the visibility of that
243:01 - timeout it's also creating an sns topic
243:03 - so those are two resources that we
243:04 - expect to be created
243:06 - if we scroll on down to the getting
243:08 - started it just says cdk deploy
243:11 - so what we'll do is go ahead and hit
243:14 - enter
243:15 - and let that do whatever it wants to do
243:22 - and it is thinking there we go so here
243:25 - we have i am statement changes so it's
243:27 - saying this deployment will potentially
243:28 - make potential sensitive changes
243:30 - according to your current security
243:31 - approval options
243:33 - there is there may be security related
243:34 - changes not in this list do you want to
243:36 - deploy sure we'll hit y
243:40 - deploying creating cloud information
243:42 - change that so cdk is using
243:44 - cloudformation underneath
243:46 - it's not complicated
243:50 - and as that is going what we'll do is
243:52 - we'll make our way over to our aws
243:54 - amazon.com console
243:57 - and if we go over to cloudformation
244:00 - we'll see if we see anything yet
244:05 - so it's creating a stack here we can
244:06 - click into it we can go over to our
244:08 - events
244:09 - see that things are being created this
244:11 - is always confusing so i always go to
244:12 - resources to see what is individually
244:14 - being created and they're all done
244:16 - so we go over here and they exist
244:18 - so here it says
244:20 - that we have a queue called this
244:23 - right sometimes they have links you can
244:25 - link through it so
244:26 - notice here i can click on the topic and
244:28 - get to that resource in sns which is
244:30 - nice for sqs i'm just going to type in
244:32 - sqs enter
244:36 - and there it is okay so we don't really
244:38 - understand what those are we could
244:40 - delete the stack this way there's
244:41 - probably a cdk way to delete the stack
244:44 - so
244:45 - cdk destroy
244:49 - i assume that's what it is
244:52 - destroy okay so we'll type in cdk
244:54 - destroy
245:01 - given a moment
245:03 - we're going to say yes
245:08 - okay it's deleting in progress
245:10 - we can even go back here and double
245:12 - check
245:21 - still thinking
245:30 - again you know if we deleted these for
245:32 - real it would take like a second
245:34 - but you know sometimes they're just slow
245:39 - sometimes it's because a resource can
245:40 - get hung
245:41 - as well
245:43 - but uh i don't think anything is a
245:45 - problem so here we can see what the
245:46 - problem is
245:47 - not necessarily a problem but it's just
245:49 - the sqs is taking a long uh longer time
245:52 - to delete where the s subscription is a
245:54 - lot faster
246:03 - so i'll just see you back here in a
246:04 - moment okay okay so after a short little
246:06 - wait there it finally finished uh i just
246:08 - kept on hitting refresh until i saw it
246:10 - deleted and so it's out of there and so
246:12 - we'll get rid of our cloud9 environment
246:14 - since we are done with it
246:16 - so type in cloud9 up at the top
246:19 - and we'll go ahead and delete
246:21 - and we will go ahead and delete this
246:23 - here thank you
246:26 - and we will go back to our aws
246:28 - amazon.console here just so we can get
246:30 - our
246:31 - bearings straight here
246:33 - and there we go
246:34 - [Music]
246:39 - all right let's take a look here at the
246:40 - aws toolkit for vs code so aws toolkit
246:43 - is an open source plugin for vs code to
246:45 - create debug deploy it was resources
246:47 - since vs code is such a popular
246:50 - editor these days i use vim but it's
246:52 - very popular um i figured i should make
246:54 - sure you're aware of this um plugin so
246:57 - it can do four things you get the abyss
246:59 - explorer this allows you to explore a
247:00 - wide range of database resources linked
247:02 - to your aws account
247:04 - uh and sometimes you can view them
247:06 - sometimes you can delete them it's going
247:08 - to vary per service and what's available
247:10 - there then you have the aws cdk explorer
247:12 - this allows you to explore your stacks
247:14 - defined by cdk
247:16 - then you have amazon elastic
247:18 - container service ecs this provides
247:20 - intellisense for ecs task definition
247:22 - files intellisense means that when you
247:24 - type
247:25 - and you you'll get like autocompletion
247:27 - but you'll also get a description as to
247:29 - what it is that you're typing out then
247:31 - there is serverless applications and
247:32 - this is pretty much the main reason to
247:34 - have database toolkit it allows you to
247:36 - create debug deploy service applications
247:38 - via sam and cfn and so
247:41 - there you can see the command palette
247:42 - and you can kind of access stuff there
247:44 - okay
247:45 - [Music]
247:49 - let's take a look here at access keys so
247:51 - an access key is a key and secret
247:53 - required to have pragmatic access to
247:55 - database resources when interacting with
247:57 - the awps api outside of the aws
248:00 - management console so uh access key is
248:03 - commonly referred to as aws credentials
248:04 - so if someone says database credentials
248:06 - so you generally are talking about the
248:08 - access key not necessarily your
248:10 - username and password to log in
248:13 - so a user must be granted access to use
248:15 - access key so when you're creating a
248:17 - user you can just check box access key
248:20 - um you can always do this after the fact
248:21 - but it's good to do that as you're
248:23 - creating the user and then you can
248:25 - generate an access key and secret so you
248:27 - should never share your access keys with
248:29 - anyone they are yours if you give them
248:31 - to someone else it's like giving them
248:32 - the keys to your house it's dangerous
248:34 - never commit access keys to a code base
248:37 - because that is a good place for it to
248:40 - get leaked at some point you can have
248:42 - two active keys at any given time you
248:45 - can deactivate access keys obviously
248:47 - delete them as well access keys have
248:49 - whatever access a user has to aims
248:51 - resources so
248:52 - you know you can do the database
248:54 - management console so can the key
248:56 - so access keys are to be stored in the
248:59 - aws.aws credentials file so um and if
249:03 - you're not familiar with linux this
249:05 - tilde here this actually represents your
249:07 - home folder so whether you're on windows
249:09 - or a linux that's going to be your home
249:11 - folder and then you have this period aws
249:13 - that means that it's a hidden folder but
249:15 - you can obviously access it and so in
249:17 - the it's just a tommel like file i think
249:20 - it's tommel um but i never
249:22 - uh 100 verified that it's tommle it
249:24 - looks just like tarmal
249:26 - and so what you'll have here is your uh
249:28 - default profile and so this is
249:31 - what you would use um or this is what
249:33 - any of your tools you use like the cli
249:35 - or anything else would automatically use
249:38 - if um
249:39 - if you did not specify a profile you can
249:41 - of course store multiple access keys and
249:44 - then give it a profile name
249:46 - um so if you are doing this for the
249:48 - first time you might just want to type
249:49 - in aws config and it'll prompt you and
249:51 - you'll just enter them in there as well
249:52 - i think that sets the default one when
249:54 - you're using the sdk
249:57 - you would rather probably use
249:59 - environment variables because this is
250:00 - the safest way to access them when you
250:03 - are writing code all right
250:05 - so there you go
250:07 - [Music]
250:11 - all right let's talk about access keys
250:12 - access keys are very important to your
250:14 - account um and so what we'll do is go to
250:16 - im if you are the root user you can go
250:18 - in and you can uh generate access keys
250:20 - for people um but uh generally you're
250:22 - doing it yourself for your own account
250:24 - so i go to users i'm going to click into
250:26 - mine here and we'll go over to security
250:28 - credentials and here you're going to
250:30 - notice access keys and one thing that is
250:32 - interesting is that you can only ever
250:33 - have two access keys at a time so hit
250:35 - create i'm just going to close that
250:37 - notice that the button is grayed out i
250:39 - can
250:40 - deactivate them if i feel that i haven't
250:43 - used them in a while and i can make them
250:45 - active again so i can bring them back
250:47 - into access or what i can do is
250:50 - make them inactive
250:52 - right
250:53 - and then i can delete them
250:55 - and so
250:56 - what i recommend
250:58 - right even if you do not want to
250:59 - pragmatically be using your account for
251:01 - anything you always want to fill up both
251:03 - these and the reason why and this is for
251:06 - security reasons is that if somebody
251:08 - wanted to come in
251:09 - and uh uh get into your account what
251:12 - they would do is they would try to find
251:14 - a user
251:16 - where they have access to them and then
251:17 - they would try to generate out a key so
251:19 - if both these keys are taken up so if
251:21 - you generate both these keys
251:24 - okay and this is the one you want to use
251:26 - you deactivate the other one okay we're
251:28 - not going to use that one and so now
251:29 - there's no way for them to fill up that
251:31 - other slot okay
251:33 - and so that is my strong recommendation
251:35 - to you but there's again only ever two
251:37 - here i'm just going to
251:39 - uh delete both of these so that
251:42 - when we want to uh do whatever next in a
251:45 - tutorial we'll go generate that out okay
251:47 - so go ahead and clear that out
251:50 - so hopefully that is
251:53 - enough for you to understand what to do
251:55 - with these axis keys okay
251:57 - so i'm gonna go back here
251:59 - there you go
252:03 - [Music]
252:05 - let's take a look here at aws
252:07 - documentation which is a large
252:08 - collection of technical documentation on
252:10 - how to use aws services which we can
252:12 - find at doc.abs.amazon.com
252:15 - and so this is kind of like the landing
252:17 - page where you can see all the guides
252:18 - and api references if you expand them in
252:20 - there
252:21 - into ec2 and you click on the user guide
252:23 - you can see html in pdf format kindle
252:27 - and you'll notice there's a link to
252:28 - github and that's because all of these
252:30 - docs are open source and you can
252:31 - contribute to them if you choose to do
252:33 - so i've done so multiple times in the
252:35 - past it's quite fun so aws is very good
252:37 - about providing detailed information
252:39 - about every individ service and the
252:40 - basis of this course and any aws
252:42 - certification will derive mostly from uh
252:45 - the adabus documentation so i like to
252:48 - say that i'm not really coming up with
252:49 - new information i'm just taking what's
252:51 - in the docs and trying to make it more
252:53 - digestible and i think that's the thing
252:54 - is like the docs are really good you can
252:57 - read them end to end but they are very
252:58 - dense
253:00 - and so it can be a bit hard to figure
253:01 - out what you should read and what you
253:02 - should not um but they are a really
253:05 - great resource and you should spend some
253:06 - time in there okay
253:08 - [Music]
253:12 - so i just want to quickly show you the
253:13 - aws documentation like give you a bit of
253:16 - a tour of it so if we go to
253:17 - about.amazon.com and type in docs
253:20 - i'm sure you might have seen this
253:21 - through other tutorials but the idea is
253:23 - that you have basically documentation
253:25 - for basically any possible service that
253:27 - you want and a lot of times you'll click
253:29 - into it and what you'll get are these
253:31 - little boxes and they'll show you
253:32 - different guides and it's going to vary
253:34 - based on service but a lot of times
253:35 - there's a user guide there's an api
253:37 - reference
253:39 - those are the two that you'll see there
253:41 - maybe go to something simpler like s3
253:44 - that might be a simple example yeah user
253:45 - guide api api reference and so
253:48 - all of these are on github right if you
253:50 - open these up the documentation is here
253:52 - if you find something you don't like you
253:54 - can submit issues
253:55 - and uh and correct things you can even
253:58 - submit your own examples i have um
254:01 - i have committed uh example code to the
254:04 - docs specifically for ai services so you
254:07 - might be looking examples that i
254:09 - implemented or even ruby examples since
254:10 - i really like to promote ruby on aws you
254:13 - can download it as a pdf or you can take
254:15 - it as html a lot of times you're going
254:17 - to the user guide and the way i build
254:19 - the courses here is i actually go
254:21 - through and i read these end to end so
254:23 - you know if you wanted to do that you
254:24 - want to be like me you can do that or
254:26 - you can just watch my courses and save
254:28 - yourself the trouble and not worry about
254:30 - everything that is here but generally
254:32 - the documentation is extremely extremely
254:34 - good there are some exceptions like
254:36 - amazon cognito where the content is good
254:39 - but it's just not well organized so i
254:42 - would say aws out of every other
254:43 - provider they have the most complete
254:46 - documentation
254:47 - they generally don't keep their examples
254:49 - or like tutorials within here it's
254:51 - usually pretty light they'll have some
254:52 - examples
254:53 - um but like they like to have items labs
254:56 - separately so you type if it's labs
254:57 - github right you go here
255:00 - and a lot of stuff is in here instead so
255:02 - you have a lot of great tutorials and
255:04 - examples over there okay
255:06 - um but yeah pretty much that's all there
255:08 - is to it is there consistency between
255:10 - documentations no they kind of vary
255:13 - um you know but uh it's all there is my
255:16 - point and they're always keeping up to
255:17 - date so that's all you need to know
255:19 - about the aws documentation
255:21 - [Music]
255:25 - hey this is andrew brown from exam pro
255:27 - and we are taking a look at the shared
255:29 - responsibility model which is a cloud
255:31 - security framework that defines the
255:32 - security obligations of the customer
255:35 - versus the cloud service provider in
255:36 - this case we're talking about aws and
255:38 - they have their own shared
255:39 - responsibility model it's this big ugly
255:41 - blob here
255:43 - and the thing is is that every single
255:44 - csp has their own variant on the model
255:48 - so they're generally all the same but
255:49 - some visualizations make it a little bit
255:51 - easier to understand or they kind of
255:54 - include a little bit more information at
255:56 - different parts of it and so just to get
255:58 - make sure that you have well-rounded
256:00 - knowledge i'm going to go beyond the
256:01 - aws's shared responsibility model and
256:03 - just show you some variants uh there's
256:05 - also variance not just per uh csp but
256:08 - also the type of cloud deployment model
256:10 - and sometimes these are also scoped
256:12 - based on a cloud service category like
256:14 - compute or machine learning and these
256:16 - can result in specialized share
256:18 - responsibility models so that's what
256:19 - we'll look at in this section okay
256:22 - [Music]
256:26 - all right so let's take a look at the
256:27 - adab shared responsibility model and so
256:29 - i've reworked the graphic because it is
256:31 - a bit hard to uh digest and so i'm
256:34 - hoping that this way will be a little
256:35 - bit easier for you i cannot include the
256:37 - in and of here just because we're
256:38 - limited for space but don't worry we'll
256:40 - follow that up with the next slide here
256:42 - so there are two people that are
256:43 - responsible or two
256:44 - organizations that are responsible the
256:46 - customer and aws and on investors side
256:49 - they're going to be responsible for
256:51 - anything that is physical so we're
256:53 - talking about hardware global
256:54 - infrastructure so the regions the
256:56 - availability zones the edge locations
256:58 - the physical security so think of all
257:00 - that hardware that's there those data
257:02 - centers um everything like that then
257:05 - there's also software the services that
257:07 - they're offering and so
257:09 - you know this extends to all their
257:10 - services but generally it breaks down to
257:12 - the four core and so we're talking about
257:13 - compute storage database and networking
257:16 - okay and when we say networking we're
257:18 - talking about like physically setting up
257:20 - the wires and also you know the software
257:22 - to set up the routing and all that kind
257:23 - of stuff there
257:25 - now looking at the customer side of it
257:27 - they're responsible for configuration of
257:29 - managed services or third-party software
257:31 - so the platforms they use so whether
257:34 - they choose to use a particular type of
257:35 - os
257:36 - the applications so if they want to use
257:38 - like ruby on rails uh iam so identity
257:41 - and access management so if you uh
257:44 - create a user and you grant them
257:46 - permissions if you give them things
257:47 - they're not supposed to have access to
257:48 - that's on you right then there's
257:50 - configuration of virtual infrastructure
257:52 - and systems so that would be choosing
257:53 - your os that would be the networking so
257:56 - there could be networking on the um
257:59 - the virtual machines themselves or we
258:01 - could be talking about cloud networking
258:03 - in this case then there are firewalls so
258:05 - we're talking about virtual firewalls
258:06 - again they could be on the virtual
258:08 - machine or it could be configuring like
258:10 - knuckles or security groups on aws then
258:12 - there's security configuration of data
258:15 - uh and so there is client-side data
258:17 - encryption so if you're moving something
258:18 - from s3 from your local machine to s3
258:21 - you might need to encrypt that first
258:22 - before you send it over then there's
258:24 - server side encryption so that might be
258:26 - turning on server-side encryption within
258:28 - s3 or turning it encryption on your ebs
258:31 - volume then there's networking traffic
258:33 - protection so you know that's turning on
258:35 - vpc flow logs so you can monitor them
258:38 - turning on aws guard duties so that it
258:40 - can detect anomalies with your traffic
258:42 - or or activities within your
258:45 - aws account and then there's customer
258:47 - data so that's the data that you upload
258:49 - on the behalf of your customers or
258:51 - yourself and what you decide to um you
258:54 - know like what levels of sensitivity
258:56 - that you want to lock it down do you
258:57 - want to use amazon macy to
259:00 - see if there's any public facing uh
259:02 - personally identifiable information
259:03 - that's up to you so there's a lot here
259:06 - and honestly it's a lot easier than you
259:08 - think um instead of thinking about this
259:10 - big diagram what i do is i break it down
259:12 - into this and so we have the in and the
259:14 - oven that's what i said i could not fit
259:16 - on the
259:17 - previous slide there but the idea is
259:19 - customers are responsible for the
259:20 - security in the cloud so that's your
259:23 - data and configuration so if it's data
259:26 - that's residing on there or is this
259:27 - something you can configure you are
259:28 - responsible for it on the adaba side
259:31 - they are responsible for the security of
259:33 - the cloud so if it's anything physical
259:35 - or hardware the operation of managed
259:37 - services or global infrastructure that's
259:39 - going to be on them and this in and of
259:41 - thing is very important for the exam so
259:43 - you should absolutely know the
259:44 - difference between the two this is kind
259:46 - of an aws concept i don't see any other
259:48 - cloud service provider talking about in
259:49 - and of uh so you definitely need to know
259:52 - it okay
259:53 - [Music]
259:57 - so one variant we might see for the
260:00 - uh shared responsibility model would be
260:02 - on the types of cloud computing this
260:04 - could also be applicable to the types of
260:07 - deployment models but we're doing types
260:08 - of cloud computing here and so we have
260:10 - the customer's responsibility and then
260:12 - the cloud service provider's
260:13 - responsibility so we're seeing
260:15 - on-premise
260:16 - infrastructure as a service platform as
260:18 - a service and software as a service and
260:21 - so when you are on-prem you're basically
260:25 - responsible for everything apps data
260:27 - runtime middleware os virtualization
260:29 - servers storage networking basically
260:32 - everything and just by adopting the
260:35 - cloud you're almost cutting your
260:36 - responsibilities in half here so now the
260:39 - cloud service provider is going to be
260:40 - responsible for the physical networking
260:43 - uh the physical storage those physical
260:45 - servers and because they're offering
260:48 - virtual machines to you they're setting
260:49 - up a hypervisor
260:51 - on your behalf so virtualization is
260:53 - taken care for you and so um you know if
260:56 - you launch an ec2 instance you know
260:58 - you're going to have to choose the os
260:59 - that's why you're responsible whatever
261:01 - middleware there the run time so
261:03 - whatever kind of programs you install on
261:05 - it
261:05 - the data that resides on it and any kind
261:07 - of like major applications okay
261:10 - then we have platform as a service uh
261:13 - and so you know the cloud service
261:14 - provider is gonna take even more
261:16 - responsibility there so when we're
261:17 - talking about this we're thinking like
261:18 - abos elastic bean stock right so you
261:21 - know the you just choose what you want
261:23 - and it's all managed so you might say i
261:24 - want a ruby on a rail server but you're
261:26 - not saying what os you need um you're
261:29 - not
261:30 - saying exactly you might say what
261:32 - version of ruby you want but you don't
261:33 - have to manage it if it breaks
261:35 - or it might be managed updates and
261:37 - things like that the last thing here is
261:39 - like software as a service and this is
261:41 - something where the csp is responsible
261:43 - for everything so if you're thinking of
261:45 - a software as a service think of like
261:47 - microsoft word where
261:49 - you're just writing uh you know writing
261:52 - stuff in there and you know you you are
261:55 - responsible for where you might choose
261:56 - to store your data but the data is like
261:59 - still handled by the cloud service
262:01 - fighter because you know it's on the
262:02 - cloud so on their servers right
262:05 - so yeah hopefully that gives you kind of
262:06 - an idea across types of cloud
262:08 - computing responsibilities
262:10 - [Music]
262:15 - all right so what i want to do here is
262:17 - just shift the lens a bit and look at
262:19 - the shared responsibility model if we
262:21 - were just
262:22 - observing a subset of cloud services
262:24 - such as compute and so we're going to
262:27 - see infrastructure as a service platform
262:29 - as a service software as a service and
262:31 - now we have function as a service and so
262:33 - that's what i mean when we shift the
262:34 - lens we get new information
262:36 - and so you can just see that you really
262:38 - don't want to look at this from one
262:39 - perspective okay so starting at the top
262:42 - here we have bare metal uh and so abs's
262:44 - offering is called the ec2 bare metal
262:46 - instance and this is where you basically
262:49 - get the whole machine uh you can
262:51 - configure the entire machine with with
262:53 - the exception of the physical machine
262:54 - itself so as the customer you can
262:56 - install the host os
262:59 - the host os so the operating system that
263:01 - runs on the physical machine and then
263:03 - you can install your own hypervisor um
263:06 - and then awesome is going to be
263:07 - responsible for the rest the physical
263:08 - machine now normally the next step up
263:10 - would be dedicated but dedicated doesn't
263:13 - exactly give you more responsibility it
263:15 - gives you more assurance because it's a
263:17 - single tenant virtual machine and that's
263:19 - why i kind of left it out here but we'll
263:22 - see it in the next slide that it is kind
263:23 - of on the model and shares the same spot
263:25 - as uh ec2
263:27 - but ec2 is a virtual machine and so um
263:30 - here the customer is responsible for the
263:33 - guest os so that means that you can
263:36 - choose what os you want whether it is
263:38 - ubuntu or debian or windows but that's
263:41 - not the actual os that is running on the
263:43 - physical machine and so you're not going
263:45 - to have control of that aws is going to
263:47 - take care of that then there's the
263:48 - container runtime so you know you you
263:51 - can install docker on this or any kind
263:53 - of container layer that you want um so
263:56 - that's another thing that you can do so
263:57 - aws is going to be responsible for the
263:59 - hypervisor uh the physical machine and
264:01 - the host os all right
264:04 - then looking at containers it just has
264:06 - more than one offering for containers
264:07 - but we'll just look at ecs here and so
264:11 - this is where you are going to
264:13 - have uh you don't you don't install the
264:15 - guest os right the guest os is already
264:18 - there for you what you are going to do
264:20 - is choose your configuration of
264:22 - containers you're going to
264:24 - deploy your containers you're going to
264:26 - determine where you need to access
264:28 - storage for your containers or attach
264:30 - storage to your containers and databus
264:32 - is going to be responsible for
264:34 - the guest os
264:36 - the there might not even be a guest os
264:38 - but they're the host os the guest os the
264:41 - hypervisor the container runtime and
264:44 - you're just responsible for your
264:45 - containers okay
264:47 - then going to the next level here we
264:48 - have platform as a service and so this
264:51 - one also is a little bit odd where it
264:53 - fits um because the thing is is that
264:55 - this could be using anything underneath
264:57 - it could be using containers it could be
264:59 - using virtual machines
265:01 - and so that's where it doesn't exactly
265:03 - fit well on a linear graph but let's
265:05 - just take a look at some things here so
265:07 - this is where you're just uploading your
265:08 - code
265:09 - you have some configuration of the
265:11 - environment you have options of
265:13 - deployment strategies
265:14 - the configuration of the associated
265:16 - services and then a bus is going to be
265:18 - responsible for the servers the os the
265:20 - networking the storage the security so
265:22 - it is taking on more responsibility than
265:24 - infrastructure as a service um uh
265:27 - whereas you know
265:28 - aws is just gonna be responsible for
265:30 - that so if it's a virtual machine that's
265:31 - being under uh under the use their
265:33 - business is going to be responsible for
265:35 - this customer stuff okay you're not if
265:37 - it's containers that abuse is going to
265:38 - be responsible for this but it just
265:40 - depends on how that platform as a
265:41 - service is set up actually the way
265:43 - elastic bean stock is set up is that you
265:45 - actually have access to all that
265:46 - infrastructure and you can fiddle with
265:48 - it and so in that case uh whereas like
265:50 - if you were to use heroku which is a a
265:52 - third-party provider
265:54 - you know they would take care of all
265:55 - this stuff up here um and so you would
265:57 - not have to worry about it but on aws
265:59 - you actually are responsible for uh the
266:02 - underlying infrastructure because you
266:04 - can you can configure it you can touch
266:05 - it so that's where you know again these
266:07 - do not fit perfectly and you can't look
266:09 - at platform as a service meaning that um
266:11 - you're not responsible for certain
266:13 - things it really comes down to the
266:15 - service offering okay then we're taking
266:17 - a look at software as a service so on
266:18 - aws
266:19 - this is going to be something like um
266:21 - amazon work docs which is i believe a
266:23 - competitor
266:25 - not a very popular competitor but a
266:26 - competitor to
266:28 - microsoft sharepoint and this is for
266:29 - content collaboration says the customer
266:32 - you're responsible for the contents of
266:33 - the document management of the files
266:35 - configuration of sharing access controls
266:37 - and the database is responsible for the
266:39 - servers the os networking the the
266:41 - storage the security and everything else
266:43 - so you know if you use the microsoft
266:44 - word doc and you type stuff in it you
266:46 - say where to save it that's what you're
266:47 - responsible for okay the last one here
266:49 - on the list is our uh functions here and
266:52 - so aws's offer is it was lambda and so
266:55 - as the customer all you're doing is
266:56 - you're uploading your code and database
266:58 - is going to take care of the rest so
266:59 - deployment container runtime networking
267:01 - storage security physical machine
267:03 - basically everything um
267:06 - and so you're really just left to
267:08 - develop okay so you know hopefully that
267:10 - gives you kind of an idea and again you
267:12 - know we could have thrown in a few other
267:13 - services like what we could not fit on
267:15 - this slide here was
267:17 - um
267:18 - it was fargate which is a serverless
267:21 - container as a function or sorry
267:23 - serverless serverless container as a
267:25 - service or container as a service so
267:27 - you know that has its own unique
267:29 - properties in the model as well okay so
267:31 - let's just have kind of a visualization
267:33 - on a linear graph here so we have the
267:35 - customer's responsibility on the
267:36 - left-hand side and it was a
267:37 - responsibility on the right and we'll
267:39 - look at our broad category so we got
267:40 - bare metal dedicated virtual machines
267:43 - containers and functions and so no
267:46 - matter
267:47 - which
267:48 - type of compute you're using you're
267:50 - always responsible for your code for
267:53 - containers you know if uh you know like
267:56 - uh the functions when you're using
267:58 - functions there are pre-built containers
268:00 - so you'd say i want to use ruby and
268:02 - there's a ruby container and you don't
268:04 - have to configure it but obviously um
268:06 - you know when you're using container
268:08 - service you are configuring that
268:09 - container you are responsible for it for
268:12 - virtual machines you know you're
268:13 - responsible for the run time so you can
268:15 - install a container runtime on there or
268:17 - install a bunch of different packages
268:19 - like ruby and stuff like that
268:21 - the operating system you have control
268:23 - over in the virtual machines for the
268:24 - dedicated and we saw with bare metal you
268:26 - have both uh controls of the host os and
268:29 - the guest os and then only bare metal
268:32 - allows you to have control of the
268:33 - virtualization where you can install
268:35 - that hypervisor so hopefully that gives
268:37 - you an idea of compute and databases
268:39 - offering there and also kind of how
268:41 - there's a lot of little caveats when
268:42 - we're looking at the shared
268:43 - responsibility model okay
268:48 - [Music]
268:50 - all right so i have one more variant of
268:51 - the share responsibility model and this
268:53 - one is actually what is used by google
268:55 - so um we're going to apply to aws and uh
268:58 - see how it works so let's just kind of
269:00 - redefine shared responsibility model or
269:02 - just in a slightly different way so we
269:03 - fully understand it so the share
269:05 - responsibility model is a simple
269:06 - visualization that helps determine what
269:08 - the customer is responsible for and what
269:10 - the csp is responsible for related to
269:13 - aws and so across the top we have
269:15 - infrastructure service platform as a
269:17 - service software as a service but
269:19 - remember there's other ones out there
269:20 - like function as a service it's just not
269:21 - going to fit on here
269:23 - okay so and then along the side here we
269:26 - have content access policies usage
269:29 - deployment web application security
269:32 - identity operations access and
269:34 - authentication
269:36 - network security remember that's cloud
269:37 - networking security the guest os data
269:39 - and content audit logging now we have
269:42 - the actual traditional networking or
269:44 - physical networking storage and
269:45 - encryption and here we're probably
269:47 - talking about the physical storage
269:50 - hardened kernel ipc
269:52 - uh the boot the hardware and so then
269:55 - here we have our bars so we have the
269:57 - csp's responsibility and the customers
269:59 - responsibility so when we're looking at
270:00 - a sas software as a service
270:03 - uh the customer is gonna be responsible
270:04 - for the content remember like think of
270:06 - like a word processor you're writing the
270:07 - content the access policies like say i
270:09 - want to share this document with someone
270:11 - the usage like how you utilize it can
270:13 - you upgrade your plan things like that
270:15 - then next on our list here is platform
270:17 - as a service so generally uh you know
270:20 - platform is a services for developers to
270:22 - develop and deploy applications and so
270:25 - they will generally have more than one
270:26 - deploy strategy
270:28 - and uh you know there might be some cost
270:30 - saving measures to choose like uh you
270:32 - might have to pay additional for
270:33 - security uh or you or it's up to you to
270:36 - configure in a particular way or you
270:37 - might have to integrate it with other
270:39 - services
270:40 - uh and you know we saw that pass is not
270:42 - a perfect uh definition or fit because
270:45 - you know when we look at elastic bean
270:46 - stock if you have access to those
270:48 - resources and you can change them
270:49 - underneath then you might have more
270:52 - responsibility there than you think that
270:53 - you would
270:54 - okay the next one here is infrastructure
270:57 - as a service and so this is extending to
270:59 - identity so who's allowed to uh you know
271:02 - log into your aws account
271:05 - operations the things that they're
271:07 - allowed to do in the account access and
271:08 - authentication do they have to use mfa
271:11 - things like that network security
271:13 - obviously you can configure the security
271:15 - of your cloud infrastructure or cloud
271:17 - network um you know so you know do you
271:20 - isolate everything a single vpc how do
271:22 - you set up your security groups things
271:23 - like that
271:24 - we know with virtual machines you can
271:26 - set up the guest os there's data and
271:28 - content but remember that bare metal is
271:30 - part of the uh infrastructure service
271:32 - offering and so that's where we'd see
271:34 - hardware or not hardware but you'd have
271:36 - the host of the host os or
271:38 - virtualization and so this again is not
271:40 - a perfect representation
271:42 - but it generally works okay and then
271:44 - last and list there or just
271:47 - looking at what the aws is responsible
271:49 - for auto logging so of course database
271:51 - has cloudtrail which is for uh
271:54 - logging api um events but auto logging
271:58 - could be things that are internally
272:00 - happening with those physical servers
272:02 - then the networking the physical storage
272:04 - hardening the kernel airbus has i think
272:06 - what's called the nitro system where
272:07 - they have like a security chip that's uh
272:10 - installed on all their servers then it's
272:13 - the the boot os uh and then the hardware
272:15 - itself okay
272:17 - so just remember the customer is
272:18 - responsible for the data and
272:20 - configuration of access controls that
272:22 - reside in aws so if you can configure it
272:25 - or you can put data on it you're
272:26 - responsible for it okay the customer is
272:29 - responsible for the configuration of
272:30 - cloud services and granting access to
272:32 - users via permissions right so if you
272:35 - give
272:36 - one of your employees access to do it
272:39 - you know even if it's their fault it's
272:40 - your fault so remember that
272:43 - again the csp is generally responsible
272:45 - for the underlying infrastructure we say
272:47 - generally because you know there's edge
272:48 - cases like bare metal and coming back to
272:51 - aws is in the cloud and of the cloud so
272:54 - in the cloud so if you configure it or
272:56 - store it then you the customer are
272:57 - responsible for it and of the cloud if
272:59 - you cannot configure it then the csp is
273:02 - probably responsible for it okay
273:05 - [Music]
273:09 - hey this is andrew brown from exam pro
273:11 - and we are looking at the shared
273:12 - responsibility model from the
273:13 - perspective of architecture and if
273:16 - you're getting sick of share
273:17 - responsibility model don't worry i think
273:18 - this will be the last uh slide in this
273:20 - section but let's take a look here so uh
273:23 - we have uh
273:24 - less responsibility more responsible at
273:26 - the bottom so what we have down here is
273:28 - traditional or virtual machine
273:30 - architecture so global workforce is most
273:32 - familiar with this kind of architecture
273:34 - and there's lots of documentation
273:35 - frameworks and support so maybe this
273:37 - would be using elastic beanstalk with
273:38 - platform as a service or using ec2
273:41 - instances alongside with auto scaling
273:43 - groups code deploy
273:45 - load balancers things like that the next
273:47 - level here is micro services or
273:49 - containers this is where you mix and
273:51 - match languages better utilization of
273:52 - resources so maybe you're using fargate
273:54 - which is serverless containers or
273:56 - elastic container service or elastic
273:58 - kubernetes service for containers on the
274:01 - top here we have serverless or commonly
274:03 - with functions as a service so there are
274:05 - no more servers you just worry about the
274:07 - data
274:08 - and the code right so literally just
274:10 - functions of code and so you could be
274:12 - using the amplify serverless framework
274:13 - or maybe able lambda for creating
274:16 - serverless architecture so there you go
274:18 - [Music]
274:22 - hey this is andrew brown from exam pro
274:24 - and we're looking computing services and
274:26 - before we jump into uh the entire suite
274:29 - of computing services database have
274:30 - let's just talk about ec2 for a moment
274:33 - which allows you to launch virtual
274:34 - machines so what is a virtual machine
274:37 - well a virtual machine or vm is an
274:39 - emulation of a physical computer using
274:41 - software server virtualization allows
274:43 - you to easily create copy resize or
274:45 - migrate your server multiple vms can run
274:48 - on the same physical server so you can
274:49 - share the cost with other customers so
274:51 - imagine if your server or computer was
274:53 - an executable file on your computer okay
274:56 - so that's the kind of way you want to
274:57 - think about it when we launch a vm
275:00 - we call it an instance and so ec2 is
275:02 - highly configurable server where you can
275:04 - choose the ami so the amazon machine
275:06 - image that affects options such as
275:08 - amount of cpus or vcpus virtual cpus
275:12 - amount of memory so ram the amount of
275:14 - network bandwidth the operating system
275:16 - so whether it's windows ubuntu amazon s2
275:20 - the ability to attach multiple virtual
275:22 - hard drives for storage so elastic block
275:25 - store
275:26 - um and so the amazon machine image is a
275:28 - predefined configuration for a vm so
275:30 - just remember that
275:32 - and so ec2 is also considered the
275:34 - backbone of aws because the majority of
275:36 - services are using ec2 as the underlying
275:38 - servers whether it's s3 rds dynamodb or
275:41 - lambdas that is what it's using so
275:44 - um what i say also is just because when
275:46 - we talk about the aws network that is
275:48 - the backbone for
275:49 - global infrastructure
275:51 - and the networking at large and so ec2
275:53 - is for the services okay
275:56 - [Music]
276:01 - hey this is andrew brown from exam pro
276:03 - so we just looked at what ec2 is but
276:05 - let's look at more of the broader
276:06 - services for computing and these are the
276:08 - more common ones that you'll come across
276:10 - there's definitely more than just what
276:12 - we're going to see on the single slide
276:14 - here so break this down with virtual
276:15 - machines containers and then serverless
276:17 - for virtual machines remember that's an
276:19 - emulation of a physical computer using
276:20 - software and ec2 is the main one
276:24 - but for our vm category we have amazon
276:26 - light sale this is a managed virtual
276:28 - server service it is the friendly
276:30 - version of ec2 virtual machines so when
276:32 - you need to launch a linux or windows
276:34 - server but you don't have much invoice
276:36 - knowledge you could launch a wordpress
276:38 - here and you could hook up your domain
276:40 - and stuff like that
276:42 - so this is a very good option for
276:43 - beginners we have containers so
276:45 - virtualizing an operating system or os
276:48 - to run multiple workloads on a single os
276:50 - instance so containers are generally
276:51 - used in microservice architecture when
276:54 - you divide your application into smaller
276:56 - applications that talk to each other so
276:58 - here we would have ecs elastic container
277:00 - service this is a container
277:02 - orchestration service that supports
277:04 - docker containers
277:05 - launches a cluster of servers on these
277:07 - two instances with docker installed so
277:08 - when you need docker as a service or you
277:11 - need to run containers we have elastic
277:13 - container registry ecr this is a
277:15 - repository of container images so in
277:18 - order to launch a container you need an
277:20 - image an image just means a safe copy a
277:23 - repository just means a storage that has
277:26 - version control we have ecs fargate or
277:30 - just fargate now people are kind of
277:31 - forgetting that it's it runs on ecs
277:33 - these days that's why i have it in there
277:35 - it is a service orchestration container
277:38 - service is the same as ecs
277:40 - accept you pay on demand per running
277:43 - container so with ecs you have to keep a
277:46 - ec2 server running even if you have no
277:49 - containers running so it is manages the
277:51 - underlying server so you don't have to
277:53 - scale or upgrade the ec2 server so
277:55 - there's the advantage over ecs okay then
277:58 - we have elastic kubernetes service eks
278:00 - this is a fully managed community
278:02 - service criminal or so kubernetes
278:04 - commonly abbreviated to k8 is an open
278:06 - source orchestration software that was
278:08 - created by google as generally the
278:11 - standard for managing microservices so
278:13 - when you need to run kubernetes as a
278:15 - service then we have serverless
278:16 - categories so when the underlying
278:18 - servers are managed by device you don't
278:20 - worry or configure servers soybes lambda
278:23 - is a servless function service you can
278:25 - run code without provisioning or
278:26 - managing servers you upload small pieces
278:29 - of code choose much uh how much memory
278:31 - how long you want the function to run is
278:33 - allowed to run before timing out and you
278:35 - are charged based on the runtime of the
278:36 - service function rounded to the nearest
278:38 - 100 milliseconds so there you go
278:40 - [Music]
278:44 - hey this is andrew brown from exam pro
278:46 - and what i want to do is just show you a
278:48 - variety of different computing services
278:50 - on aws so i'm going to try to launch
278:51 - them and we're not going to do anything
278:53 - with them i'm just going to simply
278:54 - launch them okay so the first i want to
278:56 - show you is ec2 and by the way we will
278:58 - go more in depth and ec2 later on in
279:00 - this course here
279:02 - but what i'm going to do is go ahead and
279:03 - launch the instance don't worry about
279:05 - all this stuff but just choose the
279:06 - amazon linux 2 so it's in the free tier
279:09 - all right we're going to choose an
279:10 - instance type of a t2 micro so that's
279:12 - part of the free tier it's going to be
279:14 - set as one all these options are fine i
279:16 - want you to go ahead and review and
279:17 - launch we're going to launch
279:19 - and i don't want to generate any key
279:21 - pair i'm going to proceed without a key
279:23 - pair i'm going to acknowledge that
279:25 - because i don't want it and that's all
279:26 - there is to launching an ec2 instance
279:29 - and so i can go here and view my
279:30 - instances
279:31 - and what you'll see is it's pending okay
279:35 - and usually it has like a little
279:36 - spinning icon maybe they've updated it
279:38 - since then
279:40 - so i go here it's hard to see because
279:42 - there's all these terminated ones but i
279:44 - don't need to do anything with it i just
279:45 - wanted to show you the actions that
279:47 - you'd have to do to launch it actually
279:49 - we'll leave it alone maybe we'll see it
279:50 - when it's launched the next one i want
279:52 - to show you is
279:53 - elastic container service
279:55 - um and wow this this is all let's go
279:58 - let's get the new experience please
280:00 - that's so old
280:02 - okay check box that on
280:05 - and we'll hit get started
280:07 - and we'll say create a cluster
280:10 - and we have some options here networking
280:11 - only ec2 linux plus networking uh
280:15 - for use with either aws fargate or
280:17 - external windows
280:19 - um
280:21 - this is if you're doing fargate which
280:23 - we're not doing right now fargate is
280:25 - part of elastic container service it
280:26 - used it well it used to be it is called
280:29 - ecs fargate but it was markets it as a
280:31 - separate service we'll go to next we'll
280:33 - say my ecs cluster
280:36 - um we can create an empty cluster but
280:38 - that would make it a fargate cluster
280:39 - which we don't want there's an on-demand
280:41 - server look it's m6i large if you're
280:44 - very afraid of a lot of spend here you
280:46 - don't have to do this you can just watch
280:48 - me do it and just learn
280:49 - well what i'm going to do is try to find
280:51 - something super cheap so i want a t2
280:53 - micro or a t3 micro t2 micro is part of
280:56 - the free tier i don't know if we get to
280:58 - choose t2 anymore in here they might not
281:00 - let you
281:03 - there it is
281:04 - but you know t3 micro is great too i
281:06 - just
281:06 - whatever says it's free that's what i'm
281:08 - going to go for
281:09 - number of instances one the amazon linux
281:12 - version is fine i don't care about a key
281:14 - pair
281:14 - [Music]
281:16 - use existing vpc i don't want to have to
281:18 - make a new one select the existing ones
281:20 - okay
281:23 - let it create a new security group
281:25 - that's totally fine
281:27 - allow those to be fine create a new role
281:29 - that's fine create
281:32 - okay
281:34 - and so that's going to create ourselves
281:35 - a cluster
281:37 - i'm going to just make a new tab here
281:38 - let's just check on our ec2 instance
281:42 - and so if we look at our ec2 instance it
281:44 - is running
281:45 - okay great so it has a private ip
281:48 - address it has a public ip address all
281:51 - right
281:52 - there's not much we can do with it i
281:53 - can't even log into it because we didn't
281:55 - generate it out of key pair a lot of
281:56 - times you want to name these things so
281:57 - let's go here name it my server okay
282:02 - go back to our ecs instance and the
282:04 - cluster is ready so we'll go here and oh
282:08 - nice we got a new ui and so if we wanted
282:10 - to deploy something as a service or a
282:13 - task
282:14 - um
282:16 - we would need to
282:18 - create a template like a task definition
282:20 - file
282:23 - uh they don't have a new ui for this
282:24 - you're being redirected to the previous
282:25 - version console because this isn't
282:27 - available in the new experience yet of
282:28 - course it isn't so we can create a new
282:30 - task definition file that's what's used
282:32 - to run it it's basically like a docker
282:33 - file compose file whatever you want um
282:36 - we have fargate or ec2 we are doing ecs
282:38 - so we're going to have to do ec2 so
282:40 - we'll say my ecs
282:42 - task def file
282:44 - um task role
282:46 - optional i am role i don't need one
282:48 - network mode i don't care
282:50 - and then this is the idea is that
282:52 - because a container allows you to use up
282:54 - a particular amount of the
282:57 - thing we don't have to use all of the
282:58 - memory so we should look up what a t2
283:00 - micro is
283:01 - because i don't even remember what size
283:03 - it is okay t2 micro aws
283:06 - so we go here we look at the instance
283:08 - types
283:09 - and we're gonna flip over to t2
283:11 - and it says that it's one vcpu
283:14 - one gigabyte of memory so what i'll do
283:17 - one
283:18 - yeah one okay that's fine so what we
283:21 - want and this is in megabytes so we'll
283:23 - say 500 megabytes
283:25 - and um
283:27 - i don't know if we can do less than one
283:28 - but i'm going to do one here
283:33 - the task cpu must be an integer greater
283:36 - than or equal to 128 okay fine 128. oh i
283:38 - guess it's 1024 would utilize the whole
283:40 - thing so i could say 512
283:43 - okay
283:45 - and this is where we would add our
283:46 - container
283:48 - so
283:49 - i don't do this every day so i don't
283:51 - remember how to do this we'll say my
283:53 - container
283:55 - and i need a repository here so i need
283:57 - like docker hub hello world
284:02 - okay i don't care what it is i just need
284:04 - a image that's simple
284:08 - and i'm looking for the address here
284:12 - um
284:16 - i'm hoping that's just this
284:19 - docker hub url
284:25 - so it'd be something like this right
284:26 - docker io probably
284:28 - docker io docker image
284:30 - docker hub url in ecs
284:40 - okay it goes to show how often i'm
284:41 - launching these things
284:43 - so repository url
284:45 - docker image so i think that what we're
284:47 - going to do here
284:51 - hmm
284:54 - i would really just like the url please
284:58 - reviews
285:00 - tags
285:02 - where is it
285:04 - where is it it's somewhere here right
285:09 - uh
285:10 - uh
285:13 - well let's just try it we'll go and
285:15 - we'll type in
285:17 - says image and tag so docker dot io
285:22 - hello world i really need an image id
285:25 - image url hello world
285:29 - docker hub
285:31 - they're not making my life easy here
285:33 - today
285:38 - anything i just want to see like a
285:40 - single example
285:41 - docker dot io
285:46 - docker io
285:50 - url examples
285:54 - ecs
285:56 - this is what it's like you know this is
285:57 - what you're going to be doing if you are
286:00 - um you know a cloud engineer you're
286:01 - going to be googling a lot and just
286:03 - trying to find examples
286:04 - here
286:06 - so here it says docker io the name the
286:08 - hostname
286:09 - okay so we'll just try it okay so i
286:11 - think that
286:12 - the the
286:14 - the name here is underscore and then
286:16 - it's hello world and that's what's
286:17 - throwing me off here right docker io
286:24 - just hold on here
286:27 - repository url and then there's the tag
286:30 - i don't know if like is the tag gonna be
286:32 - like latest view available tags
286:35 - latest okay so what i'll do here
286:39 - and that's the thing you have to have a
286:40 - lot of confidence too so hard limit soft
286:43 - limit
286:43 - do i have to set it
286:46 - do i have to set any of these things can
286:47 - i just go to the bottom and hit add
286:51 - looks like i can
286:53 - okay so we'll scroll on down create
286:57 - we create our task definition file which
286:58 - is fine we're going to go back to our
287:00 - cluster it's going to bring us back to
287:02 - the new experience we're going to click
287:04 - into this cluster
287:06 - holy smokes uh we're going to hit deploy
287:10 - and we're going to choose service that
287:12 - means it's going to continuously run
287:14 - task means that when it's done running
287:15 - it ends we're going to choose our family
287:17 - or version that's the task definition
287:19 - file there
287:20 - it's not compatible with the selected
287:22 - compute strategy
287:26 - my task file
287:31 - what if i just choose task
287:33 - take that
287:37 - okay so maybe some you have to like code
287:39 - it so that it continuously runs i don't
287:41 - care we don't need to run a service here
287:43 - the selected task definition is not
287:45 - compatible with the selected compute
287:46 - strategy
287:49 - okay
287:50 - let's see why
287:57 - can you double check if you're using
287:58 - fargate strategy instead of the ec2 uh
288:00 - blog design for the ec2 strategy so
288:02 - probably what it's suggesting is that
288:04 - the strategy file i made is not for the
288:06 - right one here
288:07 - task definitions
288:10 - go back over here
288:13 - well what's wrong with it
288:19 - taskroll none
288:21 - my container so what i'm going to do
288:23 - because i don't trust this
288:25 - i'm going to go ahead and delete this
288:27 - can i delete this how do i delete this
288:32 - oh boy
288:33 - actions
288:35 - deregister deregister
288:38 - we'll create a new one and so it was has
288:41 - tools like it was copilot
288:43 - cli to make this a lot easier because
288:45 - you can see this is very frustrating but
288:46 - i chose this
288:49 - so my task def
288:53 - requires compatibility of ec2
288:57 - default
289:01 - 512 512
289:05 - add
289:06 - container we're going to
289:11 - uh was it docker dot io underscore
289:16 - what's it called hello world
289:23 - i will just say hello world here
289:28 - and we'll just say
289:30 - 512 which is fine i don't care about any
289:33 - port mappings i'm just reading it
289:34 - carefully here to see what it wants
289:36 - we'll say 512 maybe because i didn't
289:38 - specify them it's complaining
289:41 - this looks fine we'll hit add
289:45 - okay
289:47 - constraints type this all looks fine so
289:49 - we'll try this again
289:51 - and so we now have our file let's see we
289:54 - can just run this task from here
289:57 - ec2
289:59 - this is just another way to do it so we
290:00 - just choose the cluster this is actually
290:01 - a lot easier to do it this is old old
290:04 - old eh this is ugly
290:07 - and so now it launches so you know if
290:09 - you have trouble one way then just do it
290:11 - another way and sometimes it'll work
290:13 - here so i don't expect this task to
290:15 - really work in any particular way
290:17 - if it's pending that's fine if it fails
290:19 - it's fine if it's successful that's fine
290:21 - i don't care
290:23 - i just want to go through the motion so
290:25 - it was successful
290:26 - it ran and then it stopped
290:28 - i don't know if we could see like the
290:29 - output anywhere probably what it would
290:31 - do is it would log out something like
290:34 - into
290:35 - somewhere
290:36 - and so i don't know if like there's logs
290:38 - turned on for this if i go over to like
290:40 - cloud watch logs
290:42 - maybe i could see something
290:45 - a lot of these services will
290:46 - automatically create cloud watch logs so
290:48 - sometimes you can just go look at them
290:49 - there
290:50 - so we'll drop down we'll go to log
290:52 - groups here
290:54 - there is some stuff here um there's a
290:56 - couple that i created from before just
290:58 - go ahead delete those
291:02 - and so what i'm looking for is like ecs
291:04 - so no there's no logging happening here
291:05 - which is totally fine so that is ecs um
291:08 - for fargate it's pretty much
291:10 - the same the difference is that fargate
291:12 - is like it has to start up and run so
291:15 - it's a lot slower to watch
291:17 - okay
291:19 - and now let's go take a look at
291:21 - lambda
291:22 - okay
291:24 - so this is our serverless compute
291:26 - so go ahead and create ourselves a
291:28 - function uh we can start from a
291:30 - blueprint
291:31 - that doesn't sound too bad
291:33 - and i personally like ruby so no i'm not
291:36 - getting much here
291:37 - but we can do is look for something like
291:40 - hello do we have like a hello world
291:44 - there we go hello world and we'll click
291:46 - that we'll say my hello world
291:50 - uh it's going to create those
291:51 - permissions that's fine it's showing us
291:53 - the code it's very simple okay it's
291:56 - going to console log out these values
291:58 - not a very good hello world function
291:59 - doesn't even say hello world
292:01 - how can you call it a hello world
292:03 - function if it doesn't say hello world i
292:04 - don't understand
292:06 - so i'm going to go ahead and create this
292:07 - function usually doesn't take this long
292:12 - okay so uh here is our function here is
292:15 - our code notice that this is cloud9
292:18 - okay and you can even move that over to
292:19 - cloud9 they didn't have this button here
292:21 - before that's kind of cool i hit test
292:23 - they used to have it up here
292:27 - but i guess they wanted to make it more
292:28 - obvious so they moved it down here which
292:29 - is nice so what i can do is
292:32 - hit this oops my test
292:34 - it's going to send a payload here to the
292:36 - actual function and it's going to
292:39 - tell us if it worked
292:43 - okay so can i run my test
292:46 - go over here to test
292:48 - it changed a bit so i guess i created
292:50 - there it succeeded so i have my logs
292:52 - okay so it's going to output those
292:54 - values there
292:55 - so there are the three values which
292:57 - basically is nothing
292:59 - maybe you were supposed to set those an
293:00 - environment variable but you can see
293:01 - you're just uploading uh some code right
293:04 - it's just a bit of code it's not like a
293:07 - full app or anything so we
293:09 - launched an ec2 container we did a
293:12 - a um
293:13 - sorry an ec2 instance a container we did
293:15 - a serverless function there's other
293:17 - things like eks but that is really
293:19 - really hard to set up
293:21 - okay because you'd have to use like
293:23 - kubernetes commands and stuff like that
293:25 - and my kubernetes knowledge is always
293:26 - very poor um i'm just taking a peek here
293:29 - to see if they've updated it so yeah you
293:30 - create the cluster but like deploying it
293:32 - is
293:33 - forget it i'm just trying to think of
293:35 - there's anything else i kind of want to
293:36 - show you um no those are the main three
293:38 - i would say so i'm pretty happy with
293:41 - that um what i'm gonna do is
293:44 - go and kill all these things so we're
293:45 - gonna go over to lambda
293:47 - okay
293:49 - and i'm going to go ahead and delete
293:51 - this
293:53 - as you saw ecs was the hardest and no
293:55 - matter how many times i've built things
293:57 - in ecs and i've deployed full things on
293:59 - ecs i can't remember i always have so
294:02 - much trouble with task definition files
294:04 - it's unbelievable we'll go over to our
294:06 - cluster here
294:09 - and
294:11 - ecs cluster up here
294:13 - make sure you're not in the fargate
294:14 - cluster i know i'm clicking really fast
294:16 - but there's just so many things to click
294:18 - and i'm going to click into this cluster
294:19 - we're going to hit edit because this is
294:21 - running an ec2 instance right i need to
294:23 - destroy it
294:24 - um
294:27 - it just took me back to the old one here
294:29 - i want to delete no i want to delete the
294:31 - cluster
294:32 - click back here
294:34 - where do i delete it
294:35 - up here
294:39 - here
294:40 - i can't checkbox anything
294:44 - uh
294:46 - how do i
294:48 - delete this do i have to delete the task
294:49 - first maybe so we'll go here
294:51 - i mean it's already stopped there's
294:53 - nothing to do
294:55 - edit
294:59 - uh
295:02 - account settings
295:05 - wow this is confusing
295:07 - okay
295:09 - how to delete ecs cluster
295:14 - you gotta be kidding me i have to
295:15 - actually look this up so open the ses
295:17 - console from navigation in the
295:18 - navigation choose clusters
295:21 - and the new turn off the
295:23 - turn off new ecs experience and choose
295:26 - the old console the delete cluster
295:27 - workflow is not supported in the ec ecs
295:30 - console are you serious
295:32 - then why
295:34 - why do you have it like why even let
295:36 - people use the new experience if that
295:37 - you don't have all the functionality
295:38 - there um
295:40 - oh i was gonna give it feedback but it
295:42 - didn't let me here's it says uh
295:44 - i need to delete an ecs cluster
295:52 - no okay so i'm here
295:56 - there's my big ugly cluster
295:59 - delete cluster
296:01 - okay
296:02 - so yeah it's a struggle okay like things
296:05 - are always changing on me but you just
296:07 - have to have confidence and if you've
296:08 - done it a few times you know that you
296:10 - can do it
296:11 - right um and that's one of the biggest
296:13 - hang-ups to cloud i would say so it's
296:14 - going to take a few minutes apparently
296:16 - to delete the cluster as that is going
296:17 - let's go over to ec2
296:20 - i didn't close it i kept this tab open
296:23 - and uh
296:25 - there's our ec2 instance
296:27 - we can go ahead and terminate that
296:29 - instance terminate
296:31 - okay
296:35 - and
296:37 - if this says it's terminating then we're
296:38 - in good shape terminator shutting down
296:40 - that's fine
296:41 - and notice here that's the ecs instance
296:43 - just make sure you shut down the my
296:45 - server not the um the ecs instance
296:47 - because that's going to stop and so this
296:49 - has already terminated but if we go back
296:50 - here
296:51 - notice that it says that it's not done
296:54 - but clearly
296:56 - clearly has shut down
296:59 - okay
297:00 - so i'm going to wait here for a bit even
297:02 - though i know it's been deleted maybe
297:03 - it's deleting things like the auto
297:05 - scaling group so we go down below here
297:08 - right so that's probably what it's doing
297:09 - it's probably trying to destroy the auto
297:10 - scaling group
297:12 - but it doesn't show any here so it must
297:14 - have already destroyed it
297:17 - yeah so task services delete so i'll be
297:19 - back here in a bit but i know it's safe
297:21 - it's already deleted but i'll see you
297:22 - back here in a bit okay
297:24 - so i waited literally a second and it's
297:26 - now deleted so we deleted our lambda we
297:29 - deleted our oh did we delete our lambda
297:33 - good question
297:35 - now i'm not really worried about the
297:36 - lambda because
297:38 - i guess we did but i'm not really
297:40 - worried about it because um
297:42 - you know
297:44 - when it rests at idle it's not
297:46 - costing us anything where the ecs and
297:48 - the ec2 are backed by ec2 instances so
297:51 - we do have to shut those down okay and
297:53 - again remember you make sure you're in
297:55 - the correct region sometimes that gets
297:56 - flipped over and then you think those
297:58 - resources are gone but they're actually
298:00 - not they're just running in another
298:01 - region so
298:02 - there you go
298:04 - [Music]
298:08 - hey this is andrew brown from exam pro
298:09 - and we're taking a look at higher
298:10 - performance computing services on aws so
298:13 - before we do we've got to talk about the
298:14 - nitro system so this is a combination of
298:16 - dedicated hardware and lightweight
298:18 - hypervisor enabling faster innovation
298:20 - and enhanced security all new ec2
298:22 - instant types use the nitro system and
298:24 - the nitrous system is designed
298:26 - by aws okay so this is made up of a few
298:29 - things we have
298:30 - nitro cards
298:32 - these are specialized cards for vpcs ebs
298:34 - instant storage and controller cards you
298:37 - have nitro security chips these are
298:39 - integrated into the motherboard protects
298:40 - hardware resources and we have the nitro
298:42 - hypervisor this is the lightweight hyper
298:44 - visor memory and cpu allocation bare
298:46 - metal like performance there's also
298:49 - nitro enclaves but that's a bit out of
298:51 - scope here but that has to do with like
298:53 - ec2 isolation okay
298:55 - then we have bare metal instances so you
298:57 - can launch ec2 instances that have no
298:59 - hypervisor so you can run workloads
299:01 - directly on the hardware for maximum
299:03 - performance and control we have the m5
299:05 - the r5
299:07 - ec2 instances that can run bare metal
299:09 - there's other ones i believe i've seen
299:10 - as well but you know
299:12 - if you are running bare metal you can
299:14 - just go investigate at the time of okay
299:16 - we have bottle rocket this is a linux
299:18 - based open source operating system that
299:19 - is purpose built by adabus for running
299:21 - containers on vms or bare metal hosts
299:25 - then let's just define what hbc is so
299:27 - it's a cluster of 100 of thousands of
299:30 - servers with fast connections between
299:32 - each of them with the purpose of
299:33 - boosting computing capacity so when you
299:36 - need a super computer to perform
299:37 - computational problems too large to run
299:40 - on a standard computer or computers or
299:42 - would take too long this is where you
299:44 - know hbc comes into play one solution
299:47 - here is abs parallel cluster which is an
299:50 - ada supported open source cluster
299:51 - management tool that makes it easy for
299:53 - you to deploy and manage high
299:55 - performance computing hpc clusters and
299:57 - aws so hopefully that gives you an idea
300:00 - of this stuff okay
300:04 - [Music]
300:06 - all right so let's take a look at hpc or
300:08 - high performance computing on aws so hpc
300:12 - is for uh running large complex
300:13 - simulations and deep learning workloads
300:15 - in the cloud with a complete suite of
300:16 - high performance computing product
300:18 - services gains insight faster and
300:20 - quickly move from idea to market blah
300:22 - blah blah it's for ml or very complex
300:24 - scientific computing stuff these run at
300:27 - least on c5 ends
300:30 - okay and the way it works is that you
300:32 - use this cli called p cluster variable's
300:35 - parallel compute
300:36 - or
300:37 - it was parallel cluster stuff and so
300:39 - let's see if we can get this installed
300:40 - very easily
300:42 - um so what i'm going to do
300:45 - is see how hard it is to install
300:48 - now i don't recommend you running this
300:50 - because i don't know what it's going to
300:51 - cost me and if i make a misconfiguration
300:53 - i don't want you to have that spent here
300:55 - but i don't think it's that dangerous so
300:57 - i'm going to go back over to usc 1 here
301:00 - i'm going to open up cloud shell
301:03 - and i'm going to give it a moment to
301:05 - load
301:06 - and so as that is loading let's take a
301:08 - look at how we would go ahead and
301:09 - install this so install the current
301:11 - parallel
301:12 - it is parallel i think we just copy that
301:14 - line
301:15 - okay
301:16 - and so we have to wait for environment
301:18 - to spin up alright so once it has spun
301:21 - up we will install it
301:23 - and then we will jump over
301:26 - to this tutorial
301:28 - here okay so we'll give this a moment
301:34 - and after waiting a little while here it
301:35 - looks like our shell is ready it looks
301:37 - like it's in bash um i'm just going to
301:39 - type in aws s3 ls that's a sanity check
301:44 - okay
301:45 - and it works that's great so go back
301:47 - over here and i'm going to go back up to
301:49 - install for linux
301:51 - and what i need
301:53 - is that single command
301:56 - where is it
301:58 - so
301:59 - i'm certain that we already have linux
302:01 - or python installed
302:04 - but i just
302:05 - want the command to install it
302:09 - we saw it a moment ago here i'm just
302:10 - going to back out until i can find it
302:16 - one more
302:17 - there it is so it's under oh it's this
302:19 - link here and that's what i talk about
302:21 - the documentations being tricky
302:22 - sometimes you have to click these uh
302:24 - headings here to find stuff so
302:27 - this is the first time installing it so
302:28 - we'll grab that usually you're supposed
302:30 - to create in virtual environments with
302:31 - python i don't care this is my cloud
302:33 - shell it doesn't matter to me so we're
302:35 - going to go ahead and download that and
302:36 - hopefully it is fast and it was super
302:38 - fast which was really nice and so what
302:40 - we'll do is go check out the p cluster
302:42 - version
302:46 - okay and that looks fine to me i'm going
302:47 - to go down below here to run our first
302:50 - job
302:51 - the returns the it gives outputs i don't
302:54 - think we need to configure it because we
302:56 - already have our cli so what i'm going
302:57 - to do is go ahead and create ourselves a
302:59 - new cluster
303:00 - um beginning cluster creation
303:03 - configuration file config not found so i
303:05 - guess we do have to configure this
303:10 - configure
303:12 - and it's asking what region do we want
303:14 - to be in um if i have usc 1 i would
303:17 - choose it for some reasons all the way
303:18 - for number 13 that is not a lucky number
303:21 - but i'm going to choose it anyway anyway
303:23 - no key pair found in us east 1 region
303:25 - please create one of the following
303:27 - so create an ec2 key pairs
303:31 - no options found for ec2 key pairs
303:33 - that's fine so what i'll do is go over
303:35 - here
303:37 - and we'll go over to ec2
303:43 - and we will go over to key pairs key
303:45 - pairs key pairs key pairs we'll create
303:47 - ourselves a new one here so say
303:50 - hpc key pair
303:52 - or just my hpc
303:55 - so we know what it is for
303:57 - we have putty or pem we're going to do
303:59 - pem because we're on linux we'll create
304:02 - that
304:03 - and notice that it downloaded the pen
304:05 - down down here and we're going to need
304:07 - that for later
304:08 - um and so what i'll do
304:11 - is i'll type in p cluster here again
304:13 - configure we'll choose 13
304:15 - we'll choose number one here
304:18 - allowed values for the scheduler i have
304:20 - no idea
304:22 - what these are
304:24 - uh let's choose the number one allowed
304:26 - values for the operating system amazon
304:28 - linux 2. i know what that is minimum
304:31 - cluster size
304:32 - one
304:33 - maximum cluster size
304:36 - two
304:37 - head notice instance oh t2 micro you can
304:40 - do that yeah let's do it i didn't know
304:42 - we could do that enter compute type uh
304:44 - t2 micro sure
304:46 - so i thought that we'd have to use a c5n
304:48 - but i guess apparently not automate vpn
304:51 - vpc creation yes of course
304:53 - network configuration so allowed values
304:55 - for the network configuration
304:57 - a head node in a public subnet and can
305:00 - and compute fleet in a private subnet
305:02 - a head node and compute that will do in
305:04 - the both just to make our lives easier i
305:06 - don't care
305:07 - first one sounds more secure of course
305:09 - and so
305:10 - oh it's creating cloud information sack
305:12 - wow this is easy i thought this was
305:14 - going to be super painful okay so we'll
305:16 - go over here we'll go take a look at
305:18 - what cloudformation's doing
305:20 - all right
305:21 - now i don't care if we actually run a
305:23 - task on here but it was just interesting
305:24 - to go through the process to see how
305:26 - hard it was and we will go look at what
305:29 - resources are being created so it's
305:31 - creating an internet gateway so it's
305:33 - literally creating a isolate vpc for it
305:35 - which is totally fine i guess
305:38 - it's creating a subnet it's creating a
305:39 - route table refresh here
305:43 - um i'm not sure how much it wants to
305:44 - create here
305:46 - it just looks like vpc that's all it's
305:48 - creating i thought maybe the ec2
305:49 - instances would show up here but maybe
305:51 - it's going to launch that at
305:53 - on a need be basis
305:57 - okay so that's all created oh now it's
305:59 - doing a vpc gateway
306:03 - i think vpc gateways cost money let's go
306:05 - take a look here people say pricing
306:11 - yeah there's a
306:13 - transfer fee so just be careful about
306:15 - that
306:17 - you know again you just can just watch
306:18 - along here you don't have to do it
306:21 - default route depends on public so now
306:24 - it's creating ec2 route
306:28 - i don't know what an aws ec2 route is
306:32 - i've never seen that before sometimes
306:33 - what we can do is go into ec2 and then
306:35 - take a look on the left hand side you
306:37 - see anything in here we don't know what
306:38 - it is we just type in ec2 route cloud
306:40 - formation sometimes cloudformation is
306:42 - great for figuring out what a component
306:44 - is not all components are represented in
306:46 - the um
306:48 - um
306:49 - management console so specify route in
306:51 - the row table oh it's just a route okay
306:55 - and we'll go back here we'll refresh
306:59 - so that is done
307:00 - is the stack done
307:02 - created complete good we'll go back to
307:04 - our cloud shell
307:05 - it says you can edit your configuration
307:07 - file or simply do etc so now let's see
307:09 - if we can create the cluster
307:11 - i assume this would create ec2 instances
307:15 - so the job scheduler you are using is
307:16 - sge this is deprecated in future use
307:19 - parallel cluster well should have told
307:21 - me okay
307:22 - there is a new version of three zero one
307:25 - parallel available i don't understand
307:27 - because i just installed it right
307:29 - we'll go back to cloudformation we're
307:30 - just gonna probably create nested stacks
307:32 - which
307:33 - that's what i thought it would do nessa
307:34 - stacks means that it's reliant so
307:36 - there's one main one and then there's uh
307:38 - children stack
307:39 - so go here see what resources it's
307:41 - creating
307:42 - a whole bunch of stuff wow
307:45 - so many things that sqsq sns
307:49 - a network interface
307:51 - a dynamodb table
307:53 - yeah you probably don't want to run this
307:55 - you just want to watch me do it
307:57 - and then we go into here it's creating
307:59 - an ec2 volume so that's going to be ebs
308:03 - and then here we have
308:07 - a log group i don't know why they
308:08 - separated those out it seemed very
308:10 - necessary
308:13 - we are waiting on the elastic ip that
308:16 - always takes forever creating elastic ip
308:19 - root instance profile that is the item
308:21 - role for it
308:23 - that didn't take too long
308:25 - these these take a long time i never
308:27 - know why
308:28 - you create a role it's really easy but
308:30 - attaching an iron policy you're always
308:32 - waiting for those
308:34 - um
308:36 - so
308:37 - i'm gonna just stop it here i'll be back
308:39 - in a second because i don't want to have
308:40 - to make you watch me
308:42 - stare at the screen here okay
308:44 - all right so after a really really long
308:46 - wait um
308:47 - and it always takes some time there it
308:49 - finally created i'm not sure what it's
308:51 - made i mean we generally saw over here
308:54 - in the outputs
308:55 - but usually the cost that i'm worried
308:56 - about is whatever it's launching under
308:58 - ec2 it might not even have launched any
309:01 - servers here we're going to take a look
309:02 - here and see if there's anything
309:04 - so
309:05 - we have a master and a compute and
309:07 - they're t2 micro so
309:09 - seems pretty safe here
309:12 - this compute is not running yet so i'm
309:14 - assuming that this is
309:16 - like the machine that does the computing
309:18 - and maybe if you had multiple machines
309:20 - here like that would be the cluster like
309:22 - would manage multiple computes
309:24 - i'm not particularly sure but let's just
309:26 - keep going through the tutorial and see
309:27 - what we can do the next step is we need
309:29 - to get this pen key
309:31 - in our cloud shell here so this i don't
309:33 - know where this is but what i'm going to
309:35 - do
309:36 - is i'm going to move it to my desktop
309:38 - i'm doing this off screen by the way so
309:40 - i'm moving it to my desktop and then i'm
309:41 - just going to go and upload the file
309:43 - okay
309:45 - and there it is so we'll say open
309:47 - and we'll say upload
309:50 - and it's going to upload it here onto
309:52 - this machine and i believe this is on
309:54 - like uh i think this uses an efs
309:56 - instance
309:57 - like if you're wondering where the
309:58 - storage for cloud shell is if we go over
310:01 - here i think it's efs
310:03 - is it
310:05 - uh i don't know where it is okay maybe
310:07 - it's just uh maybe it's somewhere else
310:09 - okay i can't remember where it is but
310:11 - anyway um so now
310:14 - it's created the cluster can i hit enter
310:17 - here
310:19 - okay
310:21 - can i create a tab
310:23 - like if i quit this is it going to kill
310:25 - it
310:26 - it exited it which is i think it's fine
310:28 - i don't think it stopped running
310:30 - and so now if i do an ls there's my key
310:33 - and so we can go back to our
310:36 - instructions we just have too many tabs
310:38 - open here
310:39 - drag this all the way to the left here
310:41 - and so we can try to use our key here to
310:43 - log in
310:45 - so what i'm going to do is
310:49 - go here and we'll say my hpc pem and see
310:52 - if that works we'll say yes
310:56 - and permission denied it is required
310:58 - your private key is not accessible
310:59 - that's because we have to mod it
311:02 - um
311:04 - i never remember the command anymore
311:06 - because i rarely ssh into machines but
311:09 - if we go to connect
311:11 - and we go to ssh client it will tell us
311:14 - that we need to run
311:16 - chamod 400 okay so that's what we need
311:18 - to do is we need to do a chamat400
311:21 - just wanted to grab that code there
311:24 - okay and now if we hit up we should ssh
311:27 - into the machine there we are
311:30 - we are in the instance
311:32 - we'll type it exit and so now we want to
311:35 - run our job on this machine
311:38 - and if we go back over to here
311:42 - i guess we can go create our first job
311:45 - so i'm just doing this in vi
311:49 - and i'm gonna paste that in yep
311:52 - and i don't want the first line oh okay
311:55 - that's perfect oh great
311:58 - right
311:58 - quit oh there's no file name hold on
312:01 - here
312:02 - so i need to name this file something so
312:04 - i'm going to say job.sh
312:08 - and we're going to paste that again here
312:10 - we'll say paste
312:12 - and i don't know if that's cut off yeah
312:14 - it is okay great
312:15 - is that one okay
312:20 - i don't trust that the first line is
312:21 - there
312:23 - so what i'm gonna do
312:26 - is go back to our tutorial here
312:29 - it's shebang forward slash bin forward
312:32 - slash bash
312:33 - uh
312:36 - this then that
312:37 - forward slash bin forward slash bash
312:40 - just double check it looks good to me
312:42 - we're going to quit that i'm just going
312:44 - to make sure that it is what we said it
312:46 - is so job.sh
312:48 - looks correct to me good and so we'll
312:50 - try to run our job here so i'm going to
312:52 - say q
312:54 - um job.sh
312:58 - ls
313:00 - and i guess it really depends on what we
313:01 - decided to use when we set up that thing
313:03 - i can't remember what we choose as our
313:05 - queue
313:06 - we do qstat
313:09 - oh you okay okay okay so i think the
313:11 - thing is like you see how we have sg i
313:13 - think that that's what we use to queue
313:15 - up jobs and so we have to have that
313:16 - installed probably so install
313:21 - configure sun grid engine
313:26 - sg install
313:28 - linux
313:33 - oh boy that looks like a lot of work
313:37 - so i don't think we need to do anything
313:39 - further here but as far as i understand
313:41 - the idea is that you're choosing
313:43 - some kind of way to manage these and so
313:45 - i'm not sure what q q sub is let's just
313:48 - go look at what that is what is q sub
313:50 - oh that is the sun grid engine
313:52 - okay so
313:54 - how do we install that
314:00 - um
314:01 - [Music]
314:02 - i'm just gonna see if we can install it
314:04 - so i'm gonna do
314:05 - i think this is using yum
314:07 - so if i do clear here
314:09 - clear
314:11 - yum install q sub let's see if i can do
314:13 - it
314:16 - sudo yum install qsum no package
314:19 - available
314:20 - amazon linux 2
314:23 - q sub because that's probably what we're
314:25 - running in cloud shell
314:32 - q sub doesn't tell us how to install it
314:37 - that's great
314:40 - so that's probably what it is and so in
314:42 - order to use this we would have to
314:43 - install that
314:45 - sun whatever whatever
314:46 - and then we go through we do q sub it
314:48 - would cue it up um you could do q stat
314:51 - cat hello destroy it that's pretty much
314:53 - all we really need to know to understand
314:55 - this
314:55 - it would have been nice to queue up a
314:56 - job and see it work but you know we're
314:58 - getting kind of into a hairy territory
315:00 - here and i think that we fundamentally
315:02 - understand how this does work so what
315:04 - i'm going to do is i'm going to go here
315:05 - i'm going to remove the job.sh here and
315:08 - i want to destroy this cluster
315:12 - so i'm going to do p cluster commands
315:16 - to figure out what all the commands are
315:18 - and there's probably a delete command so
315:21 - we'll go back up here
315:25 - be cluster
315:28 - where is our credit so we'll say delete
315:34 - okay and so what that's going to do is
315:35 - just tear down all the stuff now
315:38 - so if we go over to cloudformation
315:47 - okay and
315:49 - it looks like it's destroying so
315:52 - yeah i'll see you here uh back in a bit
315:54 - when it's all destroyed okay
315:55 - all right so after a short little wait
315:56 - there it has destroyed it has been so
315:58 - long that i uh my connection vanished
316:00 - but just make sure if you did follow
316:02 - along for whatever reason uh you know
316:04 - make sure that the stuff is deleted and
316:06 - it looks like it did not destroy uh this
316:08 - so i'm going to go ahead and delete that
316:09 - that's just vpc stuff so i'm not too
316:12 - worried about it i know that's going to
316:13 - roll back no problem and so i'm going to
316:14 - consider this done so i'm going to make
316:16 - my way back to the management console
316:18 - close this stuff up and we are good to
316:21 - go for our next thing
316:27 - hey this is andrew brown from exam pro
316:29 - and we're taking a look at edge and
316:30 - hybrid computing services so what is
316:33 - edge computing when you push your
316:35 - computing workloads outside of your
316:37 - network to run close to the destination
316:38 - location
316:40 - so an example would be pushing computing
316:42 - to run on phones iot devices external
316:44 - servers not within your cloud network
316:47 - what is hybrid computing when you're
316:48 - able to run workloads on both your
316:50 - on-premise data center and the above
316:53 - vpc okay
316:55 - so we have a few services here starting
316:57 - with abus outposts this is a physical
316:58 - rack of servers that you can put into
317:00 - your data center invoice outputs allows
317:02 - you to use aws api and services uh such
317:06 - as ec2 right in your data center then we
317:09 - have abs wavelength this allows you to
317:10 - build and launch your applications in a
317:12 - telecom data center by doing this your
317:14 - applications will have ultra low latency
317:17 - since they will be pushed over the 5g
317:18 - network and be closest as possible to
317:20 - the end user
317:22 - so they've partnered with things like
317:24 - verizon vodafone business and a few
317:27 - others but those are the two noticeable
317:28 - ones okay we have vmware cloud on aws so
317:31 - this allows you to manage on-premise
317:33 - virtual machines using vmware
317:35 - within ec2 instances the data center
317:38 - must be using vmware for virtualization
317:41 - for this to work okay
317:43 - then we have abs local zones which are
317:44 - edged data centers located outside of
317:47 - the database region so you can use it as
317:49 - closer to the edge destination when you
317:51 - need faster computing storage databases
317:53 - in populated areas that are outside of
317:55 - aws region you could do this there's
317:57 - some other edge offerings on aws that
317:59 - aren't listed here like sagemaker has
318:01 - was called like neo stage maker unless
318:04 - you do edge computing with
318:06 - ml but i mean this is good enough okay
318:09 - [Music]
318:13 - all right so i wanted just to show an
318:15 - example of edge computing because we
318:17 - didn't cover it in our generic compute
318:19 - and so there's a variety of services
318:20 - that allow you to do edge computing like
318:22 - wavelength and so
318:24 - i've never actually launched wavelength
318:26 - before and i think that you have to
318:29 - request it so
318:31 - if i go over to support here again i've
318:33 - never done this before but i'm sure we
318:34 - can figure it out pretty easily i feel
318:36 - that if we create a case
318:40 - um maybe it's like service limit
318:44 - we type in wavelength here well nope not
318:46 - there
318:48 - so how do we get wavelength wavelength
318:50 - request
318:58 - so that's what i'm looking for here
319:03 - okay
319:04 - how do i use wavelength aws
319:08 - whoops
319:14 - and sometimes what i'll do is go to the
319:16 - docs here
319:17 - opt into wavelength zones before you
319:19 - specify wavelength zone for resource or
319:21 - service you must opt into it to opt in
319:24 - go to the aws console
319:26 - okay so we'll go to ec2
319:31 - and
319:32 - then it's going to say use the region
319:34 - selector in the navigation bar to select
319:36 - the region which supports your
319:37 - wavelength
319:40 - so i know that there's stuff in
319:44 - uh us west because
319:47 - of las vegas right or not las vegas but
319:49 - los angeles right
319:50 - so if we go over here there's definitely
319:52 - that over there on the navigation pane
319:54 - of the ec2 dashboard under account
319:56 - attribute select zones
320:00 - okay do we see zones here
320:04 - zones
320:07 - oh ec2 dashboard
320:11 - zones let's go check here again
320:13 - on the navigation pane choose ec2
320:15 - dashboard we are there right
320:20 - and under account attributes settings
320:22 - account attributes
320:26 - oh over here okay
320:27 - oh it's here zones
320:30 - and so there we have two zones and we
320:33 - see
320:34 - switch regions to make
320:36 - zones a different region
320:39 - okay so
320:40 - under zone groups turn on wavelengths
320:44 - zone groups
320:47 - okay nothing there so i'm just going to
320:48 - switch over to another one here
320:51 - oh maybe oregon
320:54 - maybe cs west 2. oh look at all the
320:56 - stuff we have here i've never seen these
320:59 - before okay so
321:01 - here is the wavelength one so that is
321:03 - the los angeles one
321:06 - we can go ahead and enable this before
321:08 - december the zone group i'm not sure
321:10 - what zone groups cost so
321:12 - wavelength zone pricing
321:15 - again you might just want to watch me do
321:17 - this because it might cost money
321:20 - and so you might not want to
321:22 - have to spend for that
321:24 - pricing
321:29 - provides mobile networks wavelengths are
321:31 - available across whatever learn about
321:34 - the data transfers enterprise about ec2
321:36 - instances
321:38 - okay so what's the price
321:40 - we're going to here
321:44 - alright so what i'm going to suggest to
321:46 - you is don't do this but i'm going to do
321:48 - it and we're just going to see what the
321:49 - experience is like okay
321:51 - so i'm going to update my zone so now i
321:53 - have this one so we'll say enable
321:55 - i'm going to assume that it has to do
321:57 - with like data transfer costs
322:00 - okay
322:01 - and uh we're going to go over to ec2
322:06 - and we're going to go over to instances
322:07 - here
322:09 - we're going to launch an instance and
322:11 - we're going to see if we have that
322:13 - available now i don't know if we're
322:14 - restricted to particular to particular
322:16 - uh instances i assume we can launch a
322:18 - linux machine
322:20 - it'd be really weird if we couldn't you
322:21 - know we'll go over to configuration and
322:23 - what we want to do is choose
322:27 - the zone so how do we do it so once it's
322:29 - turned on
322:31 - confirmation confirm it configure your
322:33 - network so create a vpc create a carrier
322:35 - gateway so you can connect your
322:37 - resources into the vpc to the
322:38 - telecommunication network
322:40 - holy smokes this is complicated
322:43 - but it's just kind of interesting to see
322:45 - like the process right
322:47 - you know it's not for our use case but
322:49 - uh
322:50 - carrier gateway right
322:52 - and as i do this i always check up all
322:53 - the costs here so i say
322:55 - carrier gateway
322:57 - pricing aws because maybe that's where
322:59 - the price is
323:03 - okay if you don't get a pricing page
323:05 - then usually that's hard to say
323:06 - logically isolated virtual networks
323:10 - again it's not telling me what
323:13 - um to use carrier you need to opt into
323:15 - at least one wavelength zone but i did
323:18 - right
323:20 - and sometimes what happens is that it
323:21 - just takes time for the opt-in to to go
323:26 - so go here manage the zone settings
323:29 - that was a lot easier way so we have one
323:31 - it's we're opted in right here
323:33 - okay
323:35 - and
323:38 - okay we'll go here again if that one
323:40 - didn't work um
323:43 - we can try
323:45 - so i guess these are all the regions
323:46 - denver things like that
323:50 - can i
323:51 - opt into this one opt-in
323:57 - it's not super exciting like all we're
323:59 - going to do is launch an ec2 instance
324:00 - but you know we'll go through the
324:02 - process here a bit
324:04 - and i don't know why i can't create one
324:06 - so we'll go back over to the
324:07 - instructions here
324:09 - credit so you can connect so create a
324:11 - route table using the vpc to the route
324:12 - table so i think that's as far as we're
324:14 - going to get here because i'm not seeing
324:15 - any options here but the idea was that
324:18 - we would have to create a carrier
324:19 - gateway we'd update our route tables and
324:21 - all we would be doing is launching an
324:23 - ec2 instance so you know it's no
324:25 - different than launching it you just
324:27 - choose a different subnet so i think
324:29 - you'd have to create a subnet for that
324:30 - zone and launch it in there and that
324:32 - would be edge computing another example
324:34 - of edge computing would be something
324:36 - like via cloudfront
324:37 - which we have these
324:40 - edge functions or
324:42 - not edge functions have functions here
324:44 - and so these are functions that are
324:46 - deployed to cloudfront so
324:48 - my cloudfront function
324:53 - and these would be deployed to um
324:56 - edge locations right and all you can use
324:58 - here is javascript so here's an example
325:00 - of one
325:01 - and
325:03 - um i'm fine with this
325:05 - development live this function is not
325:07 - published we'll go to test
325:10 - test the function it's good
325:14 - publish publish that function and so
325:16 - the advantage of this is that you know
325:18 - if you have functions that are in it
325:20 - with lambda there's a chance of cold
325:22 - start
325:23 - um whereas if they're deployed on the
325:26 - edge here there's still probably a cold
325:27 - start but it's going to be a lot faster
325:29 - because it's a lot closer to
325:31 - the edge location so
325:33 - um you know
325:34 - it's just the different uh different
325:35 - cases but yeah there was one where we're
325:37 - launching ec2 workload into wavelengths
325:40 - which we couldn't complete which is
325:41 - totally fine and then we have these
325:43 - functions on the edge there's other edge
325:45 - computing services like within sagemaker
325:47 - you can deploy i think it's called like
325:49 - neo sagemaker and then for iot devices
325:51 - those are obviously on the edge so you
325:53 - can deploy those as well
325:55 - but generally that gives you an idea of
325:56 - edge computing okay
326:00 - [Music]
326:02 - hey it's andrew brown from exam pro and
326:03 - we're looking at cost and capacity
326:05 - management computing services so before
326:07 - we talk about them let's define what is
326:09 - cost management so this is how do we
326:11 - save money and we have capacity
326:13 - management how do we meet the demand of
326:15 - traffic and usages through adding or
326:17 - upgrading servers so let's get to it the
326:19 - first are the different types of ect
326:22 - pricing models so you got spot instances
326:24 - reserved instances saving plans these
326:27 - are ways to save on computing by paying
326:29 - up in full or partially or by committing
326:32 - to a yearly contract or multi-year
326:34 - contract
326:35 - or by being flexible about the
326:37 - availability interruption to computing
326:38 - services we have it was batch so this
326:41 - plan schedules and executes your batch
326:43 - compute workloads across the full range
326:45 - of aws computing services which can
326:47 - utilize spot instances to save money we
326:50 - have abyss compute optimizer so suggest
326:52 - how to reduce costs and improve
326:54 - performance by using machine learning to
326:56 - analyze
326:58 - your previous usage history we have ec2
327:01 - auto scan groups so asgs these
327:03 - automatically add or remove ec2 servers
327:05 - to meet the current demand all of
327:08 - traffic they will save you money and
327:10 - meet capacity since you only run the
327:12 - amount of servers you need then we have
327:14 - elb so elastic load bouncer so this
327:17 - distributes traffic to multiple
327:18 - instances we can reroute traffic from
327:20 - unhealthy instances to healthy instances
327:23 - and can route traffic to ec2 instances
327:25 - running in different availability zones
327:28 - and then we have elastic beanstalk here
327:30 - which is easy for deploying web
327:31 - applications without developers having
327:33 - to worry about setting up and
327:35 - understanding the io underlying aweso
327:37 - services similar to heroku it's a
327:39 - platform as a service so not all these
327:41 - are about cost some of them are about
327:43 - capacity management like elb
327:45 - but yeah there you go
327:47 - [Music]
327:51 - hey this is andrew brown from exam pro
327:53 - and we are looking at the types of
327:54 - storage services and no matter what
327:56 - cloud service provider using they're
327:57 - usually broken down into these three
327:59 - where we have blocks file
328:01 - and um
328:02 - uh object okay so let's take a look at
328:04 - the first so this is going to be for
328:06 - block storage so for aws this is called
328:08 - elastic block store data is split into
328:11 - evenly split blocks directly accessed by
328:13 - the operating system and supports only a
328:15 - single right volume so imagine you have
328:17 - an application
328:19 - over here and that application is using
328:21 - a virtual machine that has a specific
328:23 - operating system and then it has a drive
328:25 - mounted to it uh it could be using fc or
328:29 - scuzzy here
328:30 - but the idea here is when you need a
328:32 - virtual drive attached to your vm is
328:34 - when you're going to be using block okay
328:36 - the next one here is for um file or it's
328:39 - just basically a file system so this is
328:41 - about elastic file storage so the file
328:43 - is stored with data and metadata
328:46 - multiple connections via a network share
328:48 - supports multiple reads writes locks the
328:51 - file so over here we could have an
328:54 - application but it doesn't necessarily
328:55 - have to be an application and so it's
328:57 - using nasa exports as the means to
329:00 - communicate and so the protocols here
329:02 - can be nfs or smb which are very common
329:05 - uh file system protocols and so the idea
329:08 - here is when you need a file share where
329:10 - multiple users or vms need to access the
329:12 - same drive so this is pretty common
329:14 - where you might have multiple virtual
329:15 - machines and you just want to act as
329:17 - like one
329:18 - drive one example that could be like
329:20 - let's say you're running a minecraft
329:21 - server you're only allowed to have one
329:23 - world on a particular single drive but
329:25 - you want to be able to have multiple
329:26 - virtual machines to maximize that
329:28 - compute that'd be a case for that um so
329:31 - there you go then the last one here is
329:33 - like object storage and so for aws this
329:35 - is called amazon simple storage service
329:37 - or
329:38 - also known as s3 so object is stored
329:40 - with data metadata any unique id scales
329:43 - with limited uh
329:45 - with limited no file limit or storage
329:48 - limit
329:49 - so there's really very there's very
329:51 - little limit to this it just basically
329:53 - scales up supports multiple reasons
329:55 - right so there are no locks and so the
329:57 - protocol here we're going to be using
329:59 - https and api so when you just want to
330:02 - upload files and not have to worry about
330:04 - the underlying infrastructure not
330:05 - intended for high
330:07 - iops so input and outputs per seconds
330:10 - okay so depending on how fast you have
330:11 - to do your read and writes are going to
330:13 - determine uh you know whether you're
330:14 - going uh this direction or the other way
330:17 - um or you know how many
330:19 - need to actually connect at the same
330:21 - time and whether it has to be connected
330:22 - as a mount drive to the virtual machine
330:24 - okay
330:29 - hey it's andrew brown from exam pro and
330:31 - we're going to do a short introduction
330:33 - into s3 because on the certified cloud
330:35 - partitioner they ask you a little bit
330:36 - more than they used to and so we need to
330:38 - be a bit familiar with s3 because it is
330:41 - um at least i think that abel's
330:43 - considers its flagship uh storage uh
330:46 - service and it really is one of the
330:48 - earliest services is the second one ever
330:50 - launched okay so what is object storage
330:52 - or object-based storage so data storage
330:54 - architecture that manages data as
330:56 - objects as opposed to other storage
330:58 - architectures so file systems where
331:00 - these are others right so which manages
331:02 - data as files and a hierarchy and block
331:05 - storage which manages data as blocks
331:06 - within sectors and tracks that get
331:08 - stored on an actual uh drive
331:11 - and so uh the idea here is we have s3
331:13 - which provides basically unlimited
331:15 - storage you don't need to think about
331:16 - the underlying infrastructure the s3
331:18 - console provides interface for you to
331:19 - upload and access your data okay so we
331:22 - have the concept of an s3 object so
331:24 - objects contain your data they are like
331:27 - files but objects may consist of a key
331:29 - this is the name of the object a value
331:31 - the data itself made up of a sequence of
331:33 - bytes the version id when versioning
331:35 - enabled the version of the object
331:37 - metadata additional information attached
331:39 - to the object and then you have your s3
331:41 - buckets the buckets hold objects buckets
331:43 - can also have folders which in turn hold
331:45 - objects s3 is a universal namespace so
331:47 - bucket names must be unique it's like
331:49 - having a domain name okay and one other
331:52 - interesting thing is an individual
331:54 - object can be between zero bytes and up
331:56 - to five terabytes so you have unlimited
331:58 - storage but you can't have uh files of
332:01 - uh incredible size uh i mean five
332:03 - terabytes is a lot but nothing beyond
332:05 - that for a single file but just
332:07 - understand that you can actually have a
332:09 - zero byte file uh and for like associate
332:12 - certifications that can be
332:13 - a an actual question so that's why it's
332:16 - there
332:21 - all right let's take a look at s3
332:22 - storage glasses um and so for the
332:24 - certified cloud partitioner we need to
332:26 - know generally what these are for
332:27 - associated levels we need more detail
332:29 - than we have here but let's get through
332:30 - it so adabus offers a range of s3
332:32 - storage classes the trade retrieval time
332:35 - accessibility durability for cheaper
332:37 - storage and so the farther down we go
332:39 - here the more cost effective
332:41 - it should get
332:42 - pending uh you know certain conditions
332:45 - okay so when you put something to s3
332:47 - it's going to go into the standard uh
332:49 - tier the default tier here
332:51 - and this is uh incredibly fast it has
332:53 - 99.99
332:54 - availability 11 9's durability
332:57 - and it's replicated across three azs and
333:00 - so
333:01 - uh you know we have this cheaper meter
333:02 - here here on the left-hand
333:04 - side that would apply this is very
333:06 - expensive and it's not actually
333:07 - expensive but it is expensive at scale
333:09 - when you can uh better optimize it with
333:11 - these other tiers so just understand
333:13 - that then you have the s3 intelligent
333:15 - tiering so this uses ml to analyze
333:17 - objects and usage and determine the
333:19 - appropriate storage class it is moved to
333:21 - the most cost effective access tier
333:23 - without any performance impact or added
333:25 - overhead then you have s3 standard ia
333:28 - which stands for infrequent access this
333:30 - is just as fast as s3 standard but it's
333:32 - cheaper if you access the files less
333:34 - than once a month there's going to be an
333:36 - additional retrieval fee applied so if
333:38 - you do try to retrieve data as
333:40 - frequently as s3 standard it's going to
333:42 - actually end up costing you more so you
333:44 - don't want to do that okay then you have
333:46 - s3 one zone ia so as it says it's
333:49 - running in a single zone so it's as fast
333:51 - as s3 standard but it's going to have
333:53 - lowered availability but you're going to
333:55 - save money okay there is one caveat
333:57 - though your data could get destroyed
333:58 - because it's remaining in a single uh a
334:00 - z so if that
334:02 - a z or data centers um suffer a
334:04 - catastrophe you're not going to have a
334:06 - duplicate of your data to retrieve it
334:08 - okay and then you have s3 glacier so for
334:11 - long-term cloud storage retrieval of
334:14 - data can take minutes to hours but it's
334:16 - very very very cheap and then you have
334:18 - esri glacier deep archive which is the
334:20 - lowest cost storage class but the data
334:22 - retrieval is 12 hours and so you know um
334:26 - all of these here to here these are all
334:28 - going to be in the same abyss s3 console
334:31 - or amazon s3 console s2 glacier is
334:33 - basically like its own service but it's
334:35 - part of s3 so kind of lives in this
334:37 - weird state there's one here that we
334:39 - didn't have a list here which is s3
334:41 - outputs because it has its own storage
334:42 - class it doesn't exactly fit well into
334:46 - this kind of leaner cheaper
334:48 - thing here okay
334:50 - [Music]
334:54 - hey it's andrew brown from exam pro and
334:55 - we're taking a look at the aws snow
334:57 - family so this is storage and compute
334:59 - devices used to physically move data in
335:01 - or out of the cloud when moving data
335:03 - over the internet or provide private
335:05 - connection that is too slow difficult or
335:07 - costly so we have snow cone snow ball
335:10 - edge and snowmobile and so there
335:13 - originally was just snowball and then
335:15 - they came out with snowball edge
335:17 - and edge introduced edge computing
335:19 - that's why there's edge in the name
335:21 - but pretty much all of these devices
335:23 - have edge computing uh and they do
335:25 - individually come with some variants so
335:27 - with the snowball snow cone it comes in
335:28 - two sizes where it has eight terabytes
335:30 - of usable storage and then there's one
335:32 - with 14 terabytes of usable storage for
335:35 - snowball edge it technically has like
335:37 - four versions but i'm going to break it
335:38 - down to two for you we have storage
335:40 - optimize where we have 80 terabytes of
335:42 - use
335:44 - of usable storage there and then compute
335:47 - optimize
335:49 - 30.9.5 terabytes and even though it's
335:51 - not here you get a lot of vcpus and
335:53 - increased memory which could be very
335:55 - important if you need to do edge
335:56 - computing before you send that over to
335:58 - aws and then last here we have
336:00 - snowmobile which can store up to 100
336:03 - petabytes of storage um in the
336:07 - associates i cover these in a lot more
336:09 - detail because there's so much more
336:10 - about these like the security of them
336:12 - how they're tamper proof something like
336:14 - how they have networking built in the
336:15 - the connection to them but you know for
336:17 - this exam that's just too much
336:19 - information um you just need to know
336:21 - that there are three uh three ones in
336:23 - the family and generally what the sizes
336:24 - are and that they're going to be all
336:26 - placed into amazon s3 uh what's
336:28 - interesting is that you know snowmobile
336:30 - only does a hundred petabytes but adabus
336:32 - markets it as you can move exabytes of
336:35 - of um content because you can order more
336:37 - than one of these devices so uh they'll
336:39 - market it saying like snowball edge is
336:41 - when you want to move uh petabytes of
336:43 - data and snowball mobile is when you
336:44 - want to move exabytes but you can see
336:46 - that a single thing isn't in the
336:48 - exabytes just in the petabyte okay
336:51 - [Music]
336:55 - hey this is andrew brown from exam pro
336:57 - and we are taking a look at all the
336:58 - innova storage services in brief here so
337:00 - let's get to it so the first is simple
337:02 - storage service s3 this is a serverless
337:04 - object storage service you can upload
337:06 - very large files and an unlimited amount
337:08 - of files you pay for what you store you
337:10 - don't worry about the underlying file
337:11 - system or upgrading the disk size you
337:13 - have s3 glacier this is a cold storage
337:15 - service it's designed as a low-cost
337:17 - storage solution for archiving and
337:18 - long-term backup it uses previous
337:20 - generation hdd drives to get that low
337:23 - cost
337:24 - it's highly secure and durable we have
337:26 - elastic block store ebs this is a
337:28 - persistent block storage service it is a
337:30 - virtual hard drive in the cloud and you
337:32 - attach to ec2 instances you can choose
337:34 - different kinds of hard drives so ssd
337:36 - iops ssd throughput hdd and
337:40 - a cold hhd okay we have elastic file
337:43 - storage so efs it is a cloud native nfs
337:46 - file system service so file storage uh
337:49 - you can mount to multiple ec2 instances
337:50 - at the same time when you need to share
337:52 - files between multiple servers we have
337:55 - storage gateway this is a hybrid cloud
337:56 - storage service that extends your
337:58 - on-premise storage to the cloud we've
337:59 - got three offerings here file gateway so
338:01 - extend your local storage to amazon s3
338:04 - volume gateway cache is your local drive
338:06 - to s3 so you have a continuous backup of
338:08 - the local files in the cloud tape
338:10 - gateway so stores files onto virtual
338:12 - tapes for backing up your files on very
338:15 - cost-effective long-term storage we got
338:17 - warmer page here because there's a lot
338:18 - of services here we have eight of us
338:21 - snow family so these are storage devices
338:23 - used to physically migrate large amounts
338:25 - of data to the cloud and so we have
338:27 - snowball and snowball edge these are
338:29 - briefcase size data storage devices
338:31 - between 50 to 80 terabytes i don't
338:33 - believe snowball is available anymore
338:35 - it's just snowball edge but it's good to
338:37 - have all of them in here so we can see
338:39 - what's going on we have snowmobile this
338:41 - is a cargo container filled with racks
338:43 - of storage a compute that is transported
338:45 - via a semi-trailer tractor truck to
338:47 - transfer up to 100 petabytes of data per
338:50 - trailer i don't think we're going to be
338:51 - ordering that anytime soon because
338:52 - that's pretty darn expensive but that's
338:54 - cool we have snow cone this is a very
338:56 - small version of snowball that can
338:57 - transfer eight terabytes of data we have
339:00 - aws backup a fully managed backup
339:01 - service that makes it easy to centralize
339:04 - and automate the backup of data across
339:05 - multiple services so ec2 ebs rds
339:08 - dynamodb efs storage gateway you create
339:11 - the backup plans we have cloud endure
339:14 - disaster recovery so continuously
339:16 - replicates your machine in a low cost
339:17 - staging area in your target able's
339:19 - account and preferred region enabling
339:21 - fast and reliable recovery in case of
339:23 - i.t data center failures we have amazon
339:26 - fsx this is a feature rich and highly
339:28 - performant file system that can be used
339:30 - for windows so that would be using smb
339:32 - or linux which uses luster
339:35 - and so there we have the amazon
339:36 - fsx for windows file server so use smb
339:39 - protocol and allow you to mount fsx to
339:42 - windows servers and then the luster one
339:44 - which uses a linux luster file system
339:47 - and allows you to mount ffsx linux
339:49 - servers are there any storage services
339:51 - missing here not really i mean you could
339:53 - count elastic container repositories one
339:56 - but that's kind of something else or you
339:58 - could also count
339:59 - maybe um
340:01 - code commit but you know i kind of put
340:03 - those in a separate category where we
340:05 - those are in our developer tools or our
340:07 - containers okay
340:08 - [Music]
340:13 - all right so what i want to do is show
340:14 - you around s3 so we'll make our way up
340:16 - here and type in s3
340:19 - and
340:20 - we'll let it load here and what we're
340:22 - going to do is create a new bucket if
340:23 - you do not see the screen just click on
340:25 - the side here go to buckets and we'll
340:27 - create ourselves a new bucket so bucket
340:29 - names are unique so let's say my buckets
340:33 - and we'll just pound in a bunch of
340:34 - numbers i'm sure you're getting used to
340:36 - making buckets in this
340:38 - in this course so far
340:41 - so if we scroll on down notice that it
340:42 - says block public access settings for
340:44 - this bucket and this is turned on
340:46 - uh like the blocking is turned on by
340:48 - default because s3 buckets are the
340:50 - number one thing
340:52 - that are a point of entry for malicious
340:54 - actors where people leave their buckets
340:55 - open so if we want to
340:58 - grant access to this bucket for people
341:00 - to see this publicly we'd have to turn
341:01 - this off okay but for now we're going to
341:03 - leave that on you can version things in
341:05 - buckets which is pretty cool you can
341:07 - turn on encryption which you should turn
341:09 - on by default and use the amazon s3 key
341:11 - on the certified cloud partitioner it's
341:13 - going to ask you about client-side
341:15 - encryption and server-side encryption so
341:17 - you definitely want to know what these
341:18 - are i'm going to turn it off for the
341:20 - time being so we can kind of explore uh
341:22 - here by ourself here then there's object
341:24 - lock so we can lock files so that um
341:27 - you know there you know people aren't
341:29 - writing to them multiple times so go
341:31 - ahead and create a bucket
341:33 - and it's very quick so here's the new
341:35 - bucket we made
341:36 - and you'll notice we have nothing here
341:38 - which is totally fine if i go to
341:39 - properties
341:41 - um
341:42 - you know we can see that we can turn on
341:44 - bucket versioning turn on encryption but
341:46 - what i'm going to do is i'm going to go
341:48 - grab some files i remember i saved
341:51 - some files recently here i'm just going
341:52 - to make a new folder called star trek i
341:54 - just have some graphics you can pull
341:56 - anything off the internet you want to do
341:57 - this yourself
341:59 - but i'm just going to prepare a folder
342:01 - here it'll take me a moment
342:04 - okay
342:06 - just a moment
342:09 - okay great so now i have my folder
342:11 - prepared and so what i want to do is
342:13 - upload my first file so i can go here
342:15 - and upload
342:16 - and actually i can upload multiple files
342:18 - you can add a folder which is nice and
342:19 - so in here if i want to upload these
342:22 - files here whoops i'll just select
342:24 - multiples i'll hit open it'll queue them
342:26 - up which is really nice we can see the
342:28 - destination details here if we want to
342:29 - turn it versioning on we could there
342:32 - we could apply permissions for outside
342:34 - access but we have uh things turned on
342:36 - but what's really important is the
342:37 - properties where we have these different
342:40 - tiers and so based on the tier that you
342:42 - use the the lower you go at least it
342:45 - should be the cheaper it's going to get
342:48 - but it's going to have some trade-offs
342:49 - let me cover that through the course
342:51 - then there's that server side encryption
342:53 - um and i'm going to hit upload we'll
342:55 - just individually turn it on so
342:57 - you're going to see this progress go
342:58 - across the top these have all been
342:59 - uploaded i'm going to click click on my
343:01 - destination bucket
343:03 - and so we can do is we can
343:05 - open these if they're images they'll
343:07 - show us
343:08 - right here in the browser
343:10 - we can download them so if we need to
343:12 - get them again
343:14 - all right we can create a folder here
343:16 - and just say star trek
343:18 - or enterprise d
343:22 - enter prize d here
343:25 - okay but it's not really easy it's not
343:27 - like i can drag this into there um i
343:29 - might be able there's no move option so
343:31 - you'd actually have to copy it into the
343:33 - destination and then delete the old one
343:35 - it's not like using a file system you
343:37 - know there's a lot more work involved
343:40 - but you know it's a great storage
343:41 - solution
343:43 - um so let's look at encryption so i have
343:45 - this selected here if i click into it
343:48 - i can go to permissions i can go to
343:50 - versions see that i'm looking for
343:53 - encryption here we go so if i turn it on
343:56 - i can enable encryption and i can choose
343:58 - whether i want to use an amazon s3 key
344:00 - so ss e
344:02 - s3 so an encryption key that amazon s3
344:05 - creates manages and uses for you then
344:07 - you have itabus
344:09 - ssc kms
344:10 - and i believe this uses aes up here
344:12 - which is totally fine then you have kms
344:15 - down here and it's interesting because
344:17 - they're like database will manage the
344:18 - key for you and then this one abyss will
344:19 - manage the key for you it's just
344:21 - slightly different this one of course is
344:22 - a lot simpler it's not many reasons not
344:25 - to turn on encryption but i'm going to
344:26 - go turn this one so that it is encrypted
344:28 - here
344:30 - and just because it's encrypted doesn't
344:31 - mean we can't access the file i can
344:33 - still download it i can still view it
344:35 - because aws is going to decrypt it right
344:37 - so if i go i click on this one and i say
344:39 - open
344:40 - okay even though it's encrypted i can
344:42 - still view it right it just means that
344:43 - it's encrypted on the storage right so
344:46 - if somebody were to steal that hard
344:47 - drive or whatever hard drive it's
344:49 - sitting on on a bus if they can't even
344:50 - figure it out it's encrypted they're not
344:52 - going to be able to open up the file
344:54 - right so that is the logic there but
344:55 - through here
344:57 - i can get it
344:58 - something that's really interesting with
345:00 - um
345:02 - s3 is the ability to
345:04 - um
345:05 - have life cycle events so i'm just kind
345:07 - of looking where that is it's usually in
345:08 - the bucket so if i go to management up
345:10 - here i can set up a lifecycle rule and
345:13 - what i can do is say like
345:14 - move this to deep storage
345:18 - okay
345:19 - and then i can say what it is that i
345:22 - want to filter so maybe it's like
345:23 - data.jpg
345:27 - or i can say apply to all objects in the
345:28 - bucket i acknowledge that and we say
345:30 - move current versions of objects between
345:31 - storage classes and i check box that on
345:33 - and i can say move them to glacier after
345:36 - 30 days
345:37 - i think if i go lower it'll complain
345:40 - probably when i save there and so the
345:42 - idea is that we can move things into
345:44 - storage so maybe you have files coming
345:45 - in down below it's showing you here
345:47 - right so
345:48 - a file is uploaded and then after 30
345:50 - days then move them into glacier so we
345:51 - save money okay that's a big advantage
345:53 - of s3 there's a lot of things going on
345:56 - in s3 here
345:57 - like you can turn on um
346:00 - uh wherever it is you can turn on
346:04 - web hosting so you can turn this into
346:05 - like a website down below here there's a
346:08 - whole a whole bunch of things that you
346:09 - can do okay so we're not going to get
346:11 - into that because that's just too much
346:13 - work
346:13 - but
346:14 - you know we learned the basics of s3 so
346:16 - what i want to do to delete this i have
346:18 - to empty it first watch it'll be like
346:19 - you cannot delete it you need to empty
346:21 - the bucket first so go ahead and empty
346:23 - it
346:24 - and i'll save my bucket empty
346:29 - or sorry i guess i have to type in
346:30 - permanently delete
346:34 - perm
346:34 - [Music]
346:38 - delete
346:39 - no
346:40 - they used to oh yeah i can copy it okay
346:42 - great
346:43 - and so once the bucket is emptied i can
346:45 - go back to the bucket
346:49 - and
346:50 - i'll go back one layer and then i'll go
346:52 - ahead and delete my bucket
346:56 - and you can only have so many buckets i
346:57 - think it's like a hundred you get like
346:59 - 100 buckets
347:02 - how many buckets can you have
347:04 - in aws
347:07 - 100 buckets yeah i was right
347:10 - and i think if you wanted to know how
347:11 - many you pro there's probably like a
347:12 - service limits page service limits
347:15 - service quotas
347:18 - so you go here you say aw services s3
347:24 - how many buckets 100 right there
347:27 - okay so you know that gives you kind of
347:29 - an idea what's going on there but there
347:31 - you go that's s3
347:32 - [Music]
347:35 - all right so let's go take a look at
347:37 - elastic block store which is uh virtual
347:40 - hard drives for ec2 so what i'm going to
347:42 - do is make my way over to the ec2
347:43 - console because that is where it's at
347:45 - and on the left hand side if we scroll
347:47 - on down you'll see elastic block volumes
347:49 - or elastic block store volumes and so we
347:51 - can go here and the idea is we can go
347:53 - ahead and create ourselves a volume and
347:55 - what you'll notice is that we have a few
347:56 - different options here we have general
347:58 - purpose provisioned iops cold hdd
348:01 - throughput optimized magnetic
348:03 - magnetic beam basically like physical
348:06 - tape that you can use to back up like
348:08 - the old school stuff and so you have all
348:10 - these options here and you can choose
348:12 - the size so when you change these
348:13 - options you're going to notice that some
348:15 - things are going to change like the
348:16 - through throughput or iops so notice
348:18 - that
348:19 - general purpose is fixed at between 300
348:21 - to 3000 and notice that it goes from one
348:24 - gigabyte to
348:25 - how many ever that is that's a lot there
348:27 - and so it's not too complicated but in
348:29 - practicality i don't really create
348:31 - volumes this way what i do
348:33 - is i'll just go launch an ec2 instance
348:34 - so i'll say launch ec2 instance and
348:37 - we'll choose amazon linux 2
348:39 - and again you know if we haven't done
348:40 - the ec2 follow along we'll cover all
348:42 - this stuff in more detail don't worry
348:44 - about it
348:45 - we go to configure instance then we go
348:47 - to add storage and this is what you're
348:49 - going to
348:50 - be doing when adding ebs volumes um to
348:53 - your ec2 instances and you'll notice we
348:55 - always have a root volume that's
348:56 - attached to the ec2 instance that we
348:58 - cannot remove we can change the size up
349:01 - here i believe the oh it shows us right
349:03 - here that we have up to 30 gigabytes so
349:05 - sometimes you might want to max that out
349:07 - to take advantage of the free tier you
349:09 - notice we can also change this there
349:11 - might be some limitations in terms of
349:12 - the root volume so notice that we have a
349:15 - few more options here we can't have a
349:17 - cold hdd or
349:19 - hdd as our
349:20 - root volume
349:22 - uh notice we have a delete on
349:23 - termination so ebs volume persists
349:25 - independently from the running life so
349:27 - you can choose to automatically delete
349:30 - ebs volume when the associated instance
349:31 - is terminated so if you take this off if
349:33 - the ec2 instance is deleted the volume
349:35 - will still remain which could be
349:36 - something that's important to you
349:38 - uh for encryption here um you might want
349:40 - to turn it on and so generally aws
349:43 - always has a kms managed key which is
349:45 - free so you check box that on it will be
349:47 - encrypted
349:48 - you can turn it on later
349:50 - but you can never turn encryption off
349:51 - but you should always uh turn encryption
349:54 - on and so just be aware to turn that on
349:56 - you can also add file systems down below
349:57 - here but maybe we'll talk about that
349:59 - later because i think that gets into um
350:03 - efs okay so that is a different type of
350:05 - file storage there but that's pretty
350:06 - much
350:07 - all there is to it uh you just go ahead
350:10 - and create your volume there and then it
350:12 - would show up under ebs we could take
350:14 - snapshots of them to back them up that
350:16 - goes to s3 but that's all we really need
350:18 - to know here okay
350:20 - [Music]
350:24 - all right let's take a look at elastic
350:26 - file
350:26 - system or efs
350:28 - storage manage file storage
350:31 - what is efs stand for efs system elastic
350:34 - file system okay sorry and so what we
350:36 - can do is go ahead and create a file
350:38 - system here so i'm going to say my efs
350:41 - and the great thing is that it basically
350:42 - is serverless so it's only going to be
350:44 - what you consume right so what you store
350:46 - and what you consume
350:48 - and i think that's what's going to be
350:49 - based on we have to choose a vpc i want
350:51 - to launch it in my default vpc
350:53 - and we have the choice of regional or
350:56 - one zone
350:58 - i guess this is going to be based on
351:00 - what gets backed up to s3 possibly so
351:02 - one zone probably is more cost effective
351:03 - but i'm going to choose regional and
351:05 - that's a new option i never noticed
351:06 - before i just opened it up to see a few
351:08 - more things here we have general max io
351:11 - bursting provision things like that
351:12 - we'll hit next
351:14 - we'll choose our azs
351:17 - and uh then you might have to set up a
351:19 - policy so i'm going to hit next here
351:22 - you'll go ahead and hit create so you
351:25 - know this is really interesting but the
351:26 - trick to it is really mounting it to
351:29 - a dc2 instance
351:31 - and that's kind of the pain okay
351:32 - so if we go into this
351:34 - um you have to mount it
351:36 - and
351:38 - there are commands for it so like efs
351:40 - mounting linux commands
351:44 - okay
351:45 - i've done this in my solutions architect
351:47 - associate uh but you know again i'm not
351:49 - doing on a regular basis so i don't
351:50 - remember
351:51 - and so if we go here i'm just trying to
351:54 - see if we can see some code that tells
351:55 - us how to mount it
351:57 - so mounting on an ec2
352:00 - ec2 linux instance with the ef-s mount
352:03 - helper
352:04 - um so i don't know if they had that
352:06 - before but that sounds interesting so
352:07 - pseudo mount hyphen t the file system
352:10 - the efs mounting point
352:12 - yeah this looks a lot easier than what
352:15 - we had before okay so before i had to
352:17 - enter a bunch of weird commands but now
352:19 - it looks like they've boiled it down to
352:20 - a single command but once you have your
352:22 - efs instance
352:23 - um
352:25 - i'm going to assume that there is an
352:27 - entry point here
352:29 - just clicking around here seeing what we
352:30 - can see
352:32 - i would imagine we have to create an
352:33 - access point
352:34 - so my access point
352:38 - sure
352:39 - i don't know if it's going to let me
352:40 - just do that
352:41 - it did and so i would imagine that you'd
352:44 - probably use an access point let's go
352:45 - back here if that's mount point
352:48 - i think that's the same thing i think
352:49 - the mount point and the access point you
352:50 - create access points and that's what you
352:52 - use uh we can go here we can attach it
352:55 - so oh yeah here's the command so
352:58 - um mount via dns or mount via ip address
353:02 - so it doesn't look too hard
353:06 - we can try to give it a go i haven't
353:07 - done it in a while it looks like they've
353:09 - made it easier so
353:11 - maybe we'll try it out okay
353:12 - so go to ec2 here
353:15 - and
353:17 - i'm going to launch an instance
353:19 - i'm going to choose amazon linux2
353:22 - okay we're going to go and
353:24 - choose that and then
353:27 - we want to choose a file system
353:30 - and so
353:33 - it's going to mount to here okay
353:36 - and storage is fine all this is fine and
353:40 - i'm going to go ahead and launch this
353:43 - and
353:45 - i need a new key pair so create a new
353:47 - key pair
353:49 - this will be for efs example
353:51 - okay
353:53 - we're going to download that key pair
353:55 - there we're going to launch this
353:56 - instance
354:01 - okay and then we're going to go view
354:03 - this and as that is launching what i'm
354:05 - going to do is open up my cloud shell
354:09 - and i'm going to want to upload this pen
354:11 - so again like before i'm going to drag
354:13 - it to my desktop off screen
354:16 - and then what i'm going to do is upload
354:18 - this file so i have it
354:22 - efs example
354:24 - okay we're going to upload it
354:28 - i just want to see if we can access that
354:29 - efs volume
354:31 - and so
354:32 - if i do ls
354:34 - that's our old one which i can delete by
354:36 - the way i'm never going to use that
354:37 - anytime soon yes
354:40 - ls and i'm just delete the hello text
354:42 - there so it's a bit cleaner for what
354:44 - we're doing and so we need to mod that
354:46 - 400
354:49 - uh efs example
354:52 - and we saw that's how like if you want
354:54 - to try to connect to a server remotely
354:56 - that's what you do right so i believe
354:58 - that the drive is mounted
355:01 - if i go to storage does it show up here
355:04 - it doesn't show up under here
355:07 - but
355:09 - what we're waiting for are these two
355:10 - status checks to pass and then we can
355:12 - ssh into this machine
355:16 - and i'm just going to go back here and
355:17 - take a look here so using the efs mount
355:20 - helper
355:21 - so sudo mount hyphen t efs tls this
355:25 - volume to efs and so i imagine it's
355:27 - going to mount it to efs here using the
355:29 - nfs client
355:30 - so i guess it just depends on what we're
355:31 - going to have available to us
355:34 - even if the sas checks haven't passed
355:35 - i'm going to try to get into this anyway
355:39 - so
355:39 - what we can do is click on this
355:42 - grab the public ip address we'll type in
355:44 - ssh
355:46 - ec2 hyphen user at sign paste this in
355:50 - hyphen i efs example pem i usually don't
355:53 - log in via ssh
355:55 - um but you know just for this example i
355:57 - will and so i want to see if this drive
356:00 - exists
356:02 - it usually be under mount right
356:05 - there it is
356:06 - okay so it already mounted for us
356:09 - so i can do touch
356:11 - hello world
356:13 - dot text
356:16 - say sudo here
356:18 - i can say sudo vi i'm going to open up
356:19 - the file and say
356:21 - hello from another
356:23 - computer okay
356:26 - and so i've saved that file and what i
356:28 - want to do now
356:30 - oops
356:32 - oh okay sorry i'm in the cloud shell
356:34 - here but what i want to do now is i want
356:36 - to kill this machine
356:38 - okay and what i'm going to do is spin up
356:39 - another ec2 instance
356:41 - i'm going to see if when i mount that if
356:43 - that file is there if it actually worked
356:46 - but wow that is so much easier than
356:47 - before i can't tell you how hard it was
356:49 - to attach an efs volume the last time i
356:52 - did it um so we'll go ahead we'll add
356:54 - that and the storage is fine we're gonna
356:56 - go to review here
356:57 - we're gonna say launch and i'm just
356:59 - gonna stick with the same key pair there
357:03 - we're going to give that moment to
357:05 - launch
357:06 - and we're going to go to view instances
357:09 - and so now this one is launching as
357:11 - that's launching let's just go peek
357:12 - around and see what we can see so you
357:14 - know i imagine if we didn't add that
357:15 - file system during the the boot um and
357:18 - we were we're adding it after the fact
357:20 - we probably could just ran that line and
357:22 - added it really easily
357:23 - i'm not going to bother testing that
357:25 - because i just don't want to go through
357:27 - that trouble to do that
357:29 - i still can't remember what these access
357:30 - points are for
357:32 - um
357:33 - but uh that's okay it's kind of out of
357:35 - the scope for the certified cloud
357:36 - partitioner
357:37 - and so i'm just curious
357:39 - so we have some nice monitoring here
357:42 - right so that's kind of nice
357:45 - um i guess they're trying to suggest
357:47 - here like aws backup data sync transfer
357:52 - so that would just be backing up
357:54 - simplify
357:55 - automates accelerates moving data okay
357:57 - that's pretty straightforward
357:58 - transfer family fully managed sftp okay
358:01 - so nothing exciting there
358:05 - and we're going to refresh that there
358:07 - and this is initializing so let's go see
358:09 - if we can connect to this one so i'm
358:10 - going to go ahead and grab that public
358:12 - ip address i'm going to hit up
358:14 - okay i'm going to swap out that ip
358:16 - address and we're going to see if we can
358:17 - connect to that
358:18 - machine yet so we'll say yes
358:21 - and we got into it so that's great and
358:22 - so what i'm going to do is go again into
358:24 - the mount directory efs fs1 ls and there
358:27 - it is i'm going to do cat hello world
358:29 - and so it works
358:31 - and so that's the cool thing about dfs
358:33 - is that you have a file system that you
358:34 - can share among other
358:38 - ec2 instances i'm sure users could
358:40 - connect to it using the nfs protocol i'm
358:42 - not the best at like networking or
358:44 - storage networking so i'm not going to
358:45 - show that here to you today but that
358:47 - gives you a general idea how efs works
358:50 - again you only pay for what you store it
358:51 - is serverless so we'll go here and type
358:54 - delete because i'm done with this i'll
358:56 - probably destroy the instance first
358:59 - it doesn't get mixed up
359:02 - and just so we clean up a little bit
359:04 - better here i'm going to delete these
359:05 - keys
359:06 - here delete
359:12 - okay
359:13 - and we'll go ahead and delete this one
359:15 - as well
359:18 - delete
359:19 - so i'm done with that
359:22 - uh we'll make sure that that is tearing
359:24 - down that is good and we'll make our way
359:26 - back over here and it says enter
359:29 - probably the id's name in so we'll enter
359:31 - that in and hit confirm
359:33 - and we'll see is it deleting i'm not
359:35 - confident with it i'm going to do it one
359:37 - more time confirm that by entering the
359:38 - the file system's id so we'll put it in
359:40 - again
359:46 - is it destroying i cannot tell there we
359:48 - go so it's destroying we are in good
359:50 - shape it is gone our data is gone
359:53 - um but yeah that is efs
359:55 - [Music]
359:59 - all right let's take a look at um the
360:01 - snow family in aws so if we type in snow
360:03 - up here and we click into into the snow
360:06 - family this is where we can probably
360:08 - order ourselves a device
360:10 - i might not be able to order them at
360:12 - least when i originally looked at this
360:14 - like way back in the day it wasn't
360:16 - available in canada so i'm kind of
360:17 - curious to see what there is but the
360:18 - idea is that you're going to go here in
360:20 - order and you have some options so you
360:22 - can import into s3 or export from s3 and
360:25 - then down below we have local compute
360:26 - storage so perform local compute storage
360:28 - workloads without transferring data you
360:30 - can order multiple devices and clusters
360:32 - for increased durability and storage
360:34 - capacity so it sounds like you're not
360:36 - you're not um
360:37 - transferring data you're just using it
360:39 - locally on to um it's like basically
360:42 - buying renting temporary computers which
360:44 - is kind of interesting i never saw that
360:46 - option before but we're going to choose
360:48 - import into aws s3 and we're just going
360:50 - to read through this stuff and it's not
360:52 - my expectation that we're going to be
360:53 - able to submit a job here and you
360:55 - probably don't want to because it's
360:56 - going to cost money but i just want to
360:58 - show you the process so we can see what
361:00 - there is here so snow job assistance if
361:02 - you're new to snow family run a pilot of
361:04 - one to two devices so batch file smaller
361:06 - than one megabyte benchmark and optimize
361:09 - deploy
361:10 - staging workstations
361:12 - discover remediate environmental uh
361:14 - issues early files and folders name must
361:17 - conform to amazon s3 prepare your ami
361:19 - once the pilot is completed confirm the
361:21 - number of snow family devices that you
361:23 - can copy devices to simultaneously
361:25 - follow the best practices use the
361:27 - following resources to manage your snow
361:29 - devices so we have aws open hub
361:31 - and then there's the edge client cli
361:35 - so open hub is a graphical user
361:36 - interface you can use to manage snow
361:38 - devices so that's kind of cool and then
361:40 - we have the cli which i imagine is
361:42 - something that's very useful to use
361:44 - so just close those off here and then we
361:46 - have other things so i'm going to say i
361:47 - acknowledge i know what i'm doing which
361:49 - i don't really but that's okay and then
361:51 - here we are going to enter in our
361:53 - address so we say andrew brown
361:56 - and i'm not gonna i'm not gonna enter
361:57 - this in for real just whatever so it'll
361:59 - be toronto
362:01 - exam pro
362:02 - um canada
362:04 - oh see so there's there's the thing you
362:06 - can only ship it to the us and so that's
362:08 - as far as i can get okay
362:11 - um and that's the thing is like if you
362:13 - really want to know aws inside and out
362:15 - you got to be in the us but let's
362:17 - pretend that we do have an address in
362:19 - the states what's a very famous address
362:21 - so what is the address of the white
362:24 - house
362:26 - okay
362:28 - there it is
362:29 - so i'm just going to copy that in
362:33 - because again we're not going to submit
362:34 - this for real i just want to
362:37 - see what's farther down the line here
362:38 - okay
362:41 - uh
362:42 - what's nw
362:44 - is that the state it's in washington
362:45 - right
362:48 - is is this part of it nw northwest is
362:50 - that a thing i'm from canada so i
362:53 - couldn't tell you um so we'll go down
362:55 - here and we have washington do we have a
362:57 - second address line it doesn't look like
362:59 - it um
363:02 - we have a zip code i believe this is the
363:03 - zip code
363:06 - and
363:07 - do we need a phone number looks like we
363:08 - do four one six
363:10 - uh one one
363:12 - one one one one one okay
363:14 - we have one day or two day shipping
363:16 - why not just have one right and so then
363:18 - we can choose our type of device so we
363:20 - have snow cone snow cone ssd snow cone
363:24 - optimized i'm surprised i never took a
363:25 - screenshot of this earlier um compute
363:28 - optimized things like that so you can
363:29 - choose which one you want it looks like
363:31 - we're going to see some different
363:32 - options but we'll go with snow cone
363:34 - my snow cone
363:38 - and
363:39 - snow cones do not ship with a power
363:40 - supply or ethernet cable snow cone
363:42 - devices are powered by 45 watt cb
363:45 - c
363:46 - usb c power supply i'll provide my own
363:49 - power supply and cable do not ship with
363:51 - a power supply or ethernet cable that's
363:53 - fine
363:54 - uh snow cone wireless no can connect
363:56 - your wireless connection connect the
363:58 - buckets you want there's a bucket we
363:59 - created earlier
364:01 - computing
364:02 - use compute using ec2 instances use a
364:05 - device as a mobile data center by
364:07 - loading ec2 ami so here's an ami that i
364:10 - might want to use
364:12 - uh aws iot green grass validated ami not
364:14 - interested remote device management you
364:16 - can use
364:17 - opshub or etc to monitor reboot your
364:20 - device that's fine
364:21 - and so then we need to choose our
364:24 - security key
364:26 - i don't know if i'll have to set the
364:27 - service role we'll see what happens here
364:30 - and
364:31 - we'll let it
364:32 - update that's fine
364:34 - and so then i guess we just hit create
364:35 - job and so i don't really want to order
364:37 - one um so i'm not going to hit that
364:40 - button and also it's going to go to the
364:41 - white house and they're going to be like
364:42 - andrew brown why did you do that so
364:44 - that's not something i feel like doing
364:46 - today but at least that gives you an
364:48 - idea of that process there and i imagine
364:50 - that uh if you go the other way it's
364:52 - gonna be pretty similar yeah it's just
364:54 - like
364:55 - same stuff i think
364:57 - so it saved that address that it's not a
364:59 - real address and the the options are a
365:02 - little bit uh
365:03 - limited here and it's like nfs space s3
365:06 - based so it's slightly different but
365:08 - it's basically the same process just
365:09 - curious we'll take a look at the last
365:10 - one there
365:13 - since there are three options just
365:15 - curious
365:18 - okay similar thing okay so
365:20 - yeah that's pretty much all i want to
365:22 - know about um the snow family and that's
365:24 - about it okay
365:29 - hey this is andrew brown from exam pro
365:31 - and we are taking a look at what is a
365:33 - database so a database is a data store
365:36 - that stores semi-structured and
365:38 - structured data and just to emphasize a
365:40 - bit more a database stores more complex
365:43 - data stores because it requires using
365:45 - formal design and modeling techniques so
365:48 - databases can generally be categorized
365:49 - as either being relational so structured
365:52 - data that strongly represents tabular
365:54 - data so we're talking about tables rows
365:56 - and columns so there's a concept of row
365:59 - oriented or columner oriented and then
366:01 - we have non-relational databases
366:04 - so these are semi-structured that may or
366:06 - may not distinctly resemble tabular data
366:09 - so here is a very simple example the
366:11 - idea is that you might use some kind of
366:13 - language like sql put in your database
366:15 - and you'll get back out tables for
366:16 - relational databases let's just talk
366:18 - about some of the functionality that
366:19 - these databases have so they can be
366:22 - using a special specialized language to
366:25 - query so retrieve data so in this case
366:26 - sql specialized modeling strategies to
366:29 - optimize retrieval for different use
366:31 - cases
366:32 - more fine-tuned control over the
366:33 - transformation of the data into useful
366:36 - data structures or reports and normally
366:38 - a database infers someone is usually
366:40 - using a relational row-oriented data
366:43 - store so
366:45 - you know just understand that when
366:46 - people say database that's usually what
366:47 - they're talking about like postgres
366:48 - mysql relational row store is usually
366:51 - the default but obviously there's a lot
366:53 - more broader terms there okay
366:55 - [Music]
366:59 - hey this is andrew brown from exam pro
367:01 - and we are taking a look at what is a
367:03 - data warehouse so it's a relational data
367:05 - store designed for analytical workloads
367:08 - which is generally column oriented data
367:10 - store okay so companies will have
367:12 - terabytes and millions of rows of data
367:15 - and they'll need a fast way to be able
367:16 - to produce analytics reports so data
367:19 - warehouses generally perform aggregation
367:21 - so aggregation is the idea of grouping
367:23 - data together so find a total or an
367:25 - average
367:26 - and data warehouses are optimized around
367:28 - columns since they need to quickly
367:30 - aggregate column data and so here's kind
367:32 - of a diagram of
367:35 - a data warehouse and so the idea is that
367:37 - it could be ingesting data
367:39 - from a regular database here i'm just
367:41 - getting out my pen tool so it could be a
367:42 - regular database or it'd be coming from
367:44 - a different data source that isn't
367:45 - compatible in terms of the schema and
367:47 - you use like etl or elt
367:50 - or etl to get that data into
367:53 - that data warehouse so data warehouses
367:55 - are generally designed to be hot so hot
367:58 - means that they can return queries very
367:59 - fast even though they have vast amounts
368:01 - of data data warehouses are infrequently
368:04 - accessed meaning they aren't intended
368:05 - for real-time reporting but maybe once
368:07 - or twice a day or once a week to
368:09 - generate business and user reports of
368:12 - course it's going to vary based on the
368:14 - the service that is offering the data
368:16 - warehouse a data warehouse needs to
368:18 - consume data from a relational database
368:19 - on a regular basis and again it can
368:21 - consume it from other places but you'll
368:22 - have to transform it to get it in there
368:24 - okay
368:29 - hey this is andrew brown from exam pro
368:31 - and we're taking a look at a key value
368:32 - store so a key value store or database
368:34 - is a type of non-relational database or
368:36 - nosql that uses a simple key value
368:39 - method to store data and so key value
368:41 - stores are dumb and fast but they
368:43 - generally lack features like
368:44 - relationships indexes aggregation of
368:47 - course there are going to be providers
368:48 - out there have managed solutions that
368:50 - might polyfill some of those uh issues
368:52 - there but i want to show you the
368:54 - underlying way that key value stores
368:55 - work to kind to kind of distinguish them
368:58 - between document stores so a key value
369:00 - stores literally a unique key alongside
369:03 - a value and the reason i'm representing
369:05 - that is zeros and ones is because i want
369:07 - you to understand that that's what it is
369:10 - it's basically just some kind of of data
369:13 - there and how the key value store
369:16 - interprets it is going to determine what
369:18 - it is so when you look at a document
369:19 - database that is just a key value store
369:22 - that
369:23 - interprets the value as being documents
369:26 - right and so key value stores can and do
369:29 - commonly store
369:31 - um
369:32 - multiple uh like an associative array
369:34 - that's pretty common so even for
369:36 - dynamodb that's how it does it and so
369:38 - that's why when you look at a key value
369:39 - store it looks like it uh a a table but
369:43 - it's not actually a table it's
369:45 - schema-less because underneath it's
369:47 - really just
369:48 - um you know that associative array and
369:50 - so that's why you can have columns or
369:52 - sorry rows that have uh
369:55 - different amounts
369:56 - of columns okay so due to the design
369:59 - they are able to scale very well beyond
370:02 - a relational database and they can kind
370:03 - of work like a relational database
370:05 - without all the bells and whistles so
370:06 - hopefully you know that makes sense okay
370:08 - [Music]
370:13 - all right let's take a look at document
370:15 - stores so a document store is a nosql
370:17 - database that stores documents as its
370:19 - primary data structure and a document
370:22 - could be an xml
370:24 - type of structure but it also could be
370:26 - something like json or json-like
370:29 - document stores are sub-classes of key
370:31 - value stores
370:32 - and the components of a document store
370:34 - are very uh comparable to relational
370:37 - databases so just kind of an example
370:39 - here where in a relational database
370:42 - they'd be called tables now you have
370:43 - collections they were called rows now
370:45 - they're called documents you had columns
370:47 - they had fields they may have indexes
370:50 - and then joins might be called embedding
370:51 - and linking so you can translate that
370:53 - knowledge over uh you know they they're
370:56 - not as
370:57 - they don't have the same kind of feature
370:58 - set as a relational database but you
371:00 - have better scalability and honestly
371:02 - document stores are just key value
371:04 - stores with some additional features
371:05 - built on top of it okay
371:07 - [Music]
371:12 - hey it's andrew brown from exam pro and
371:13 - we're going to take a look at the girl
371:15 - database services that are available on
371:17 - aws so we have dynamodb which is a
371:19 - serverless noaa skill key value and
371:20 - document database it is designed to
371:22 - scale to billions of records with
371:24 - guaranteed consistent data returned in
371:27 - at least a second you do not have to
371:29 - worry about managing shards and dynamodb
371:32 - is adabus's flagship database service
371:35 - meaning whatever we think of a database
371:37 - service that just scales is cost
371:38 - effective and very fast we should think
371:40 - of dynamodb and in 2019 amazon the
371:43 - online shopping retail uh shut down
371:45 - their last oracle database and completed
371:47 - their migration to dynamodb so they had
371:50 - 7500 oracle databases with 75 petabytes
371:54 - of data and with dynamodb they reduced
371:56 - that cost by 60 and reduce the latency
371:58 - by 40 percent so that's kind of to be
372:00 - like a testimonial between relational
372:03 - and a no school database so when we want
372:05 - a massively scalable database that is
372:08 - what we want dynamodb for and i really
372:10 - just want to put that there because if
372:12 - you remember that you're going to always
372:14 - be able to pass
372:15 - or get those questions right on the exam
372:16 - okay then we have documentdb so this is
372:19 - a no scroll document database that is
372:21 - mongodb compatible so mongodb is very
372:24 - popular nosco among developers there
372:27 - were open source licensing issues around
372:29 - using open source mongodb so aws got
372:31 - around it by just building their own
372:32 - mongodb database basically so when do
372:35 - you want a mongodb like database you're
372:37 - going to be using documentdb we have
372:40 - amazon key spaces this is a fully
372:42 - managed apache cassandra database so
372:44 - cassandra is an open source nosql key
372:46 - value database similar to dynamodb that
372:48 - is columnar store database but has some
372:51 - additional functionality so when you
372:52 - want to use apache cassandra you're
372:54 - using amazon key spaces
372:56 - [Music]
373:00 - hey this is andrew brown from exam pro
373:02 - and we are taking a look at relational
373:04 - database services starting with
373:05 - relational database service rds and this
373:07 - is a relational database service that
373:09 - supports multiple sql engines
373:12 - so relational is synonymous with sql and
373:14 - online transactional processing oltp
373:17 - and relational databases are the most
373:19 - commonly used type of database among
373:22 - tech companies and startups just because
373:23 - they're so easy to use i use them i love
373:26 - them
373:26 - rds supports the following sql engines
373:29 - we first have mysql so this is the most
373:32 - popular open source sql database and it
373:34 - was purchased and is now owned by oracle
373:37 - uh and there's an interesting story
373:39 - there because when oracle purchased it
373:41 - they weren't supposed to have it um
373:42 - mario db was or sorry my squad was sold
373:45 - to oracle sun systems and then within
373:47 - the year
373:48 - um uh oracle purchased it from them and
373:52 - the original creators never wanted it to
373:53 - go to oracle just because of their uh
373:56 - the way they do licensing and things
373:58 - like that and so
374:00 - the original creators came back and they
374:01 - decided to fork mysql and then maintain
374:04 - it as mariodb just so that you know
374:07 - oracle never kind of pushed away the
374:09 - most popular database so that everyone
374:12 - had to go to a paid solution then you
374:14 - have postgres so psql as it's commonly
374:16 - known is the most popular open source
374:18 - sql database among developers this is
374:20 - the one i like to use because it has so
374:22 - many rich features over mysql but but it
374:25 - does come with added complexity then
374:27 - oracle has its own sql proprietary
374:29 - database which is well used by
374:31 - enterprise companies but you have to buy
374:32 - a license to use it
374:34 - then you have microsoft sql so
374:36 - microsoft's proprietary sql database
374:39 - and with this one you have to buy a
374:40 - license to use it then you have aurora
374:43 - so this is a fully managed database uh
374:46 - and there's a lot more to going on here
374:48 - with aurora so we'll talk about it
374:50 - almost acts as a separate service but it
374:51 - is powered by rds so aurora is a fully
374:55 - managed database of either mysql so five
374:57 - times faster or postgres
374:59 - sql three times faster database so when
375:01 - you want a highly available durable and
375:04 - scalable and secure relational database
375:07 - for postcode to mysql you want to use
375:09 - aurora
375:10 - then you have aurora serverless so this
375:12 - is a serverless on-demand version of
375:14 - aurora so when you want the most of the
375:16 - benefits of aurora but you can trade
375:18 - off to have cold starts or you don't
375:20 - have lots of traffic or demand this is a
375:22 - way you can use aurora in a serverless
375:24 - way then you have rds on vmware so this
375:27 - allows you to deploy rds supported
375:28 - engines to on-premise data centers
375:31 - the data center must be using vmware for
375:34 - server virtualization so when you want
375:36 - databases managed by rds on your own
375:38 - database center
375:39 - uh and yeah i realize that this is a
375:41 - small spelling mistake should say just
375:43 - on here but yeah there you go
375:45 - [Music]
375:49 - hey this is andrew brown from exam pro
375:51 - and we're looking at the other database
375:52 - services that abuse has because there's
375:54 - just a few loose ones here so let's talk
375:56 - about redshift so it is a petabyte size
375:58 - data warehouse and data warehouses are
376:01 - for online analytical processing olap
376:04 - and data warehouses can be expensive
376:06 - because they are keeping data hot
376:07 - meaning that they can run a very complex
376:10 - query and a large amount of data and get
376:12 - that data back very fast so when you
376:14 - need to quickly generate analytics or
376:16 - reports from a large amount of data
376:17 - you're going to be using redshift then
376:20 - you have elastic cache so this is a
376:21 - managed database of an in-memory and
376:23 - caching open source databases such as
376:26 - redis or memcache so when you need to
376:28 - improve the performance of an
376:29 - application by adding a caching layer in
376:30 - front of your web servers or database
376:32 - you're going to be using elastic cache
376:34 - then you have neptune this is a managed
376:37 - graph database the data is represented
376:39 - as interconnected nodes i believe that
376:41 - it uses gremlin as the way to interface
376:43 - with it which is no surprise because
376:45 - that's what it looks like most clusters
376:47 - providers are using so when you need to
376:49 - understand the connections between data
376:51 - so mapping fraud rings or social media
376:53 - relationships
376:54 - very relational database heavy
376:56 - information you're gonna want to use
376:58 - neptune we have amazon time streams it's
377:00 - a fully managed time series database so
377:02 - think of devices that send lots of data
377:04 - that are time sensitive such as iot
377:06 - devices so when you need to measure how
377:08 - things change over time we have amazon
377:11 - quantum ledger database this is a fully
377:13 - managed
377:14 - ledger database that provides
377:15 - transparent immutable cryptographically
377:18 - variable transaction logs so when you
377:20 - need to record a history of financial
377:22 - activities that can be trusted
377:24 - and the last one here is database
377:26 - migration service dms it's not a
377:28 - database per say but it's a migration
377:31 - service so you can migrate from
377:33 - on-premise database to aws from two
377:35 - databases in different or same database
377:38 - accounts using different sql engines and
377:39 - from an esque wall to a nosql database
377:41 - and i'm pretty sure we cover this in a
377:43 - bit
377:44 - greater detail in this course okay
377:49 - [Music]
377:50 - all right let's go take a look at
377:52 - dynamodb
377:54 - which is awesome's nosql database so
377:55 - we'll go over to dynamodb
378:00 - and what we'll do is create ourselves a
378:03 - new table
378:04 - and we'll just say my dynamodb
378:07 - table
378:08 - and you always have to choose a
378:09 - partition key you don't necessarily have
378:11 - to have a sort key but it could be
378:12 - something like
378:14 - um
378:16 - like it you want to be really unique so
378:18 - it could be like email and this one
378:19 - could be created at right
378:23 - and so we have string binary notice that
378:25 - the the types are very sim simple then
378:28 - for settings we have default settings or
378:29 - customized settings so the default is
378:31 - use provision capacity mode rewrite five
378:33 - rules etc custom
378:36 - no secondary indexes use kms so i'm
378:38 - gonna just expand that to see what i'm
378:40 - looking at
378:41 - we have two options here on demand so
378:44 - simplify billing by paying the actual
378:46 - reads and rights that you use or
378:47 - provisioned
378:48 - which is this is where you get a
378:50 - guarantee of performance so if you want
378:51 - to be able to do
378:53 - you know whatever it is a thousand i
378:55 - don't know what it goes up to but like a
378:56 - thousand read writes per second
378:58 - then that's what you're paying for okay
378:59 - you're paying for being able having a
379:01 - guarantee
379:03 - of that
379:04 - um of that capacity okay i'm not going
379:06 - to create any secondary indexes but
379:08 - that's just like another way to
379:10 - look at data notice down below that we
379:12 - have a cost of two dollars and
379:13 - ninety-one cents
379:14 - uh then we have encryption at rest so
379:16 - you can do owned by amazon dynamodb
379:18 - that's pretty much the same as like a
379:21 - bus has or s3 has
379:24 - ssc
379:25 - s3 there you could use uh
379:28 - actually i guess most of these are
379:29 - probably kms i would imagine
379:31 - we'll go ahead and create the table here
379:34 - and that's going to create the table
379:36 - this is usually really really fast
379:39 - we'll go here
379:40 - and
379:41 - what we can do is insert some data so as
379:44 - it's just starting up here we can go
379:45 - over to
379:47 - our tables they recently changed this ui
379:49 - so that's why i look a bit confused
379:53 - view items up here okay and then from
379:56 - here we can create an item so i can add
379:58 - something say so andrew at exam pro dot
380:01 - co
380:02 - and
380:04 - 2021 uh
380:08 - well we'll just do the future so let's
380:09 - say 20 25
380:12 - 0.505 i don't want to have to think too
380:13 - hard here but we can add additional
380:15 - information so i can say like
380:17 - uh
380:18 - [Music]
380:19 - today true
380:22 - we could say
380:23 - um
380:25 - make it like a list
380:29 - you know
380:30 - food
380:31 - and then i could go here and then add a
380:33 - string
380:35 - it is not working oh there we go there
380:38 - we are so we could say like
380:40 - um banana
380:42 - and then we could say pizza right we can
380:44 - go ahead and create that item
380:47 - so now that item is in our database uh
380:50 - we can do a scan that will return all
380:51 - items we can query we can actually have
380:53 - some limitations of what we're choosing
380:55 - there's the party cue editor so we can
380:58 - use sql to select it
381:00 - um i have not used this before
381:05 - party q
381:07 - aws or partyq
381:10 - dynamodb
381:12 - examples
381:16 - i'm hoping i can just find like an
381:17 - example of some of the language getting
381:18 - started here i don't need to i don't
381:21 - need an explanation just show me an
381:22 - example
381:24 - query here and i will i'll get to it
381:25 - here
381:28 - okay so here's some examples right so
381:30 - maybe we can give this a go um so we
381:33 - have our table here so my
381:35 - dynamo
381:38 - db table
381:41 - and i just want the email back
381:44 - we don't need a where
381:48 - we'll run this
381:50 - see if it works
381:54 - there we go
381:55 - i'm not sure if we could select
381:56 - additional data there so i know that we
381:58 - had some other things like food
382:03 - there it is okay so that's really nice
382:06 - um
382:07 - addition to it
382:08 - dynamodb can stream things into a
382:10 - dynamodb stream to go to kinesis and do
382:12 - a lot of fun things so there's all sorts
382:14 - of things you can do with dynamodb but
382:17 - i'm pretty much done with this so i'm
382:18 - going to go ahead and delete this table
382:22 - and notice that it also creates some
382:23 - cloudwatch alarms so we want to delete
382:25 - this as well create a backup no we do
382:26 - not care go ahead and delete that
382:30 - and that is dynamodb
382:32 - [Music]
382:37 - okay so now i want to show you uh rds or
382:40 - relational database service so go to the
382:41 - top here type in rds
382:43 - and we'll make our way over there
382:46 - and so rds is great because it allows us
382:48 - to launch relational databases
382:51 - sometimes the ui is slow i'm not sure
382:53 - why it's taking so long to load today
382:55 - but every day is a bit different and so
382:57 - what we're going to do is go ahead and
382:59 - create a new database
383:00 - you're going to notice that we're going
383:01 - to have the option between creating a
383:03 - standard or easy i stick with standard
383:05 - just because i don't like how easy hides
383:08 - a lot of stuff from us even here like it
383:11 - says two cents per hour but it's not
383:13 - giving us the full cost so i really
383:15 - don't trust it because if you go down
383:16 - here and you chose their dev test here
383:19 - look it's like a hundred dollars it's
383:20 - not showing the the
383:22 - cost preview right now maybe because we
383:24 - didn't choose the database type sorry i
383:26 - wanted to choose postgres but before we
383:28 - do that let's look at the engine types
383:29 - we have amazon aurora so we have between
383:31 - mysql and postgres mysql maritab
383:34 - postgres oracle
383:35 - microsoft sql notice for microsoft sql
383:38 - it comes with a license you don't have
383:40 - to do anything with that
383:42 - it might change based on the addition
383:44 - here
383:46 - nope comes with a license for all of
383:47 - them which is great
383:48 - if you want to bring your own license
383:50 - that's where you need a dedicated host
383:52 - right running
383:53 - microsoft sql for oracle uh you have to
383:56 - bring your own license that's going to
383:58 - be based on um importing with the abs
384:00 - license manager but we go over to
384:02 - postgres which is what i like to use
384:05 - we're going to set it to dev test to try
384:06 - to get the cheapest cost scroll down
384:07 - look 118 dollars we can get it cheaper
384:10 - we get super cheap so here the password
384:12 - is going to be testing one two three
384:14 - capital on the t so an explanation mark
384:17 - on the end okay because it has a bunch
384:19 - of requirements of what it wants
384:21 - here i want a t2 micro
384:23 - so i'm just going to scroll down here
384:27 - what is going on here standard oh look m
384:30 - classes
384:31 - i don't want an m class i want a
384:32 - burstable class that's the cheap ones
384:35 - and so we go here can we still do a t2
384:37 - micro or is it now t3
384:40 - so i don't see t2
384:44 - so i imagine a t3 micro must be the new
384:46 - it was free tier so we go it was three
384:47 - tier here
384:49 - right
384:51 - and if i go to
384:52 - [Music]
384:53 - databases
384:57 - um rds
384:59 - on the t2 micro 750 hours but i can't
385:02 - select it
385:04 - so
385:06 - i'm going to assume
385:08 - that the t3 micro must be the new
385:10 - tier if it's not there right
385:12 - let's just say include previous
385:13 - generations
385:16 - and then maybe i can see it then
385:21 - okay so i don't see it there
385:25 - i really don't like how they've changed
385:27 - this on me
385:30 - okay so the oldest i can choose is a t3
385:32 - micro which is fine i just i just know
385:34 - t2 being the free tier that's all
385:36 - uh this is fine we don't want auto
385:38 - scaling turned on for our example here
385:40 - we do not want
385:42 - a multi-az so do not create a standby
385:44 - that's going to really jump up our cost
385:46 - we don't need public access
385:48 - it will create a vpc that's fine
385:50 - password authentication is fine we have
385:52 - to go in here which i don't know why
385:53 - they just don't keep that expanded
385:54 - because you always have to come in here
385:56 - name your database so my database
385:59 - we choose our postgres version here i'm
386:01 - going to turn backups off because if we
386:03 - don't
386:04 - if we don't it's going to take forever
386:06 - to launch this thing
386:07 - encryption is turned on you can turn it
386:09 - off
386:10 - but generally it's not recommended
386:13 - we can have performance insights turned
386:14 - on i'm going to turn the retention
386:16 - i will leave it to seven days because we
386:18 - can't turn that off we don't need
386:20 - enhanced monitoring so i'm just going to
386:21 - turn that off
386:23 - and uh that's fine we're not going to
386:25 - enable delete protection here and
386:28 - so we are good we can now go ahead and
386:31 - create our database
386:37 - and what we'll do here is wait for that
386:39 - database to be created so the thing is
386:41 - is like
386:42 - if we're doing the solutions architect
386:44 - or the
386:45 - developer associate stuff i'd actually
386:47 - show you how to connect to the database
386:49 - um it's not that hard to do like you
386:51 - just have to connect uh grab all the
386:53 - database information so it's going to
386:55 - have an endpoint a port stuff like that
386:57 - and you'd use something like table plus
386:59 - or something to connect to the database
387:01 - but that's out of scope of the certified
387:03 - cloud partitioner i'm just going through
387:04 - the motions to show you that you can
387:06 - create an rds database very easily but
387:09 - not how to connect to it and actually
387:11 - utilize it okay
387:13 - and so that would spin up and we would
387:15 - have a server and after that we can just
387:17 - go ahead and delete the server here so
387:18 - just say delete me
387:20 - okay
387:23 - and that's all there really is to it
387:26 - there is the special type of database
387:28 - like aurora doesn't have its own like
387:30 - console page it's part of rds so if you
387:32 - want to spin up aurora you just choose
387:34 - the compatibility you want you can
387:35 - choose between provisioned or serverless
387:38 - the serverless is supposed to be really
387:39 - good for
387:41 - scaling to zero cost so that's something
387:43 - there so you'd fill that all out but the
387:45 - initial cost is a lot more expensive you
387:47 - can't choose a t2 micro here um unless
387:50 - it lets you now
387:52 - it is for
387:54 - provisioned it's uh
387:57 - oh t2 t3 medium is the smallest you can
388:00 - go okay so if you reach the point we're
388:03 - using a medium-sized database then you
388:05 - might consider moving over to aurora
388:07 - just because it's going to be highly
388:08 - scalable et cetera like that
388:10 - um so that's a consideration there
388:12 - there's also something called babelfish
388:13 - um that it was announced last year when
388:16 - i when i shot this
388:18 - or when i'm shooting this as of now and
388:20 - the idea was to make it compatible with
388:22 - mysql sql server to migrate over to
388:24 - aurora post sql which is kind of
388:26 - interesting um but that's about it so if
388:29 - our database is destroying i think it is
388:31 - just going to go back over here to rds
388:36 - it's taken a long time to load today
388:43 - and i think it's already deleted maybe
388:45 - we go to databases here it's deleting so
388:48 - i'm confident it's going to delete so
388:49 - there we go
388:50 - [Music]
388:54 - all right let's take a look at redshift
388:56 - so redshift is a data warehouse and it's
388:59 - generally really expensive so it's not
389:00 - something that you're going to want to
389:02 - launch
389:02 - uh day to day here but let's see how far
389:04 - we can get with it um just by running
389:06 - through it so what we'll do is go ahead
389:08 - and create a cluster and again you can
389:10 - just watch me do this you don't have to
389:11 - create you don't have to create one
389:12 - yourself
389:13 - so free trial configure for learning
389:15 - that sounds good to me
389:16 - it's free for a limited time if your
389:18 - organization has never created a cluster
389:19 - well i rarely ever create these so when
389:21 - the trial ends delete your cluster to
389:23 - avoid the charges of on-demand okay that
389:25 - sounds fair
389:27 - so here we're going to have two v3 cu's
389:30 - it's going to launch a dc a
389:31 - dc too large
389:35 - so let's look that up for pricing
389:42 - me prices please please please
389:47 - um
389:50 - i think it's loading right here okay
389:52 - so i don't know how much it is but i
389:54 - know it is not cheap
389:56 - and down below we have sample data is
389:58 - loaded into your redshift cluster that
390:00 - sounds good to me ticket is the sample
390:02 - data okay
390:04 - ticket sample data
390:07 - redshift i just imagine they probably
390:09 - have like a tutorial for it here
390:12 - they do right here
390:15 - and so because i want to know what we
390:16 - need to do to query it right if we can
390:18 - even query it via the interface here so
390:20 - the admin user is adabus user
390:23 - and the password is going to be capital
390:24 - t testing one two three four five six
390:26 - exclamation
390:28 - and we'll hit create cluster
390:31 - oh cool we can query the data right in
390:32 - here so that's what i wasn't sure about
390:34 - whether we would be able to just query
390:36 - it in line because before
390:38 - you'd have to use java with jdbc or an
390:41 - odbc driver
390:44 - and download the jar and it's not as fun
390:46 - as it sounds of course but looks like we
390:48 - can query data once the data is loaded
390:52 - so that looks really good i guess we can
390:54 - pull data in from
390:56 - the marketplace so that's looks pretty
390:58 - nice too
391:00 - and i guess we could probably integrate
391:01 - it into other things like quicksite
391:03 - because you probably want to adjust your
391:04 - data over there
391:06 - again i usually don't spend a lot of
391:08 - time in redshift but it looks like it's
391:10 - a lot easier to use i'm very impressed
391:12 - with this so i don't know how long it
391:14 - takes to
391:15 - launch a redshift cluster i mean it is
391:17 - 160 gigabytes of
391:20 - storage there it's
391:22 - even at the smallest it's pretty large
391:23 - so what i'm going to do is to stop the
391:24 - video and i'll be back when this is done
391:27 - okay
391:28 - okay so after a short little wait here
391:30 - um it was a lot faster than i was
391:32 - expecting but uh it's available and so
391:34 - looks like here it says to query the
391:35 - sample data use redshift version 2. so
391:38 - i'm going to click that
391:39 - and i'm sure there's tons of buttons to
391:40 - get here and it'd be great if it just
391:42 - populated the query for me
391:44 - it doesn't but this looks really nice
391:47 - really nice ui i wonder if it has like
391:49 - some existing queries
391:51 - no
391:52 - that's okay so what i'm going to do here
391:55 - is i'm going to go ahead and pull out
391:57 - this query and see if we can get this to
391:59 - work here
392:01 - never found out what those prices were
392:02 - though
392:04 - okay
392:05 - and what we'll do is hit run i like how
392:08 - there's like a limit of 100 but here it
392:09 - has that so we'll go ahead and hit run
392:12 - and see what data we get so relation
392:14 - sales does not exist
392:17 - okay so
392:19 - what's going on here
392:23 - um we'll go up here so most of the
392:26 - examples in the redshift documentation
392:27 - uses a sample database called ticket the
392:30 - sample the small database consists of
392:32 - seven tables you can load the ticket
392:34 - data set by following the this here
392:38 - okay so to load the sample data from
392:41 - amazon s3
392:46 - okay so
392:49 - i would have thought it already had the
392:50 - data in there i could have swore it
392:52 - would have
392:55 - dev
392:57 - public
393:01 - tables
393:04 - zero tables
393:06 - okay so
393:07 - i don't think there's any data in here
393:09 - and so we're going to have to load it
393:10 - ourselves
393:12 - i really thought it would have added it
393:13 - for us
393:15 - let's go ahead and create these tables
393:16 - and see if this is as easy as we think
393:19 - so run that create that table
393:23 - cool okay
393:25 - we got it down here
393:27 - we'll run that we'll just run each at a
393:29 - time
393:31 - i think there's seven of them so
393:43 - date already exists okay that's fine
393:45 - event already exists saying all these
393:47 - tables exist
393:50 - maybe i just wasn't patient
393:56 - hmm okay
394:00 - um
394:02 - interesting all right so maybe we'll go
394:04 - back and uh run that query maybe we just
394:06 - had to wait a little while for that data
394:08 - to load
394:10 - run
394:13 - okay so you know what i think it was
394:15 - doing this for us if if it did not
394:18 - create it for us we would have to go
394:19 - through all these steps which is fine
394:21 - because we're learning a little bit
394:22 - about
394:25 - redshift but
394:27 - looks like we just had to wait there so
394:28 - it looks like you would run those you
394:29 - download that you use the copy command
394:32 - to bring it over there
394:34 - it looks like you can do all of this via
394:35 - the uh this interface here and we've
394:37 - done a query so that's kind of cool
394:40 - um i imagine you probably could like
394:42 - save it or export it what if we chart it
394:43 - what happens
394:45 - okay you can chart it
394:47 - it's kind of fun
394:50 - can we export it out to just we can save
394:52 - it i thought maybe it could export out
394:54 - to quicksite but i i suppose you'd
394:55 - rebuild it in quickside a
394:57 - but yeah i guess that's it right there
394:59 - so that's pretty darn simple so what i'm
395:01 - going to do is make my way back over to
395:03 - redshift because we are done for this
395:05 - example
395:09 - and we will go over to clusters here
395:12 - and i'm going to go ahead and
395:15 - delete my cluster
395:20 - delete
395:23 - create file snapshot nope
395:25 - delete
395:27 - delete the cluster
395:28 - there we go
395:30 - so i'm pretty sure that will succeed no
395:31 - problem there and we are done with
395:33 - redshift and redshift is super expensive
395:35 - so just make sure that thing deletes
395:37 - okay
395:38 - [Music]
395:42 - hey this is andrew brown from exam pro
395:44 - and we are taking a look here at cloud
395:46 - native networking services um and so i
395:49 - have this architectural diagram i
395:50 - created which has a lot of networking
395:51 - components uh when people create
395:54 - networking diagrams for aws they don't
395:55 - always include all these things here
395:57 - even though they're there so we're just
395:58 - being a little bit verbose so you can
396:00 - see okay the first thing is our vpc our
396:02 - virtual private cloud this is a
396:04 - logically isolated section of the
396:05 - database cloud where you can launch
396:07 - database resources that's where your
396:09 - resources are going to reside not all
396:12 - services uh require you to select a vpc
396:14 - uh because they're managed by aws but i
396:17 - wouldn't be surprised if under the hood
396:18 - they are in their own vpc okay
396:20 - then if you want
396:22 - the internet to reach your services
396:23 - you're gonna need an internet gateway um
396:25 - then you need to figure out a way to
396:27 - route things to your various subnets and
396:30 - that's where route tables come in
396:33 - then we need to
396:35 - define a region that it's going to be
396:36 - which is a geographical location on your
396:38 - network then you have your availability
396:41 - zones which are basically your data
396:42 - centers where your resources are going
396:44 - to reside then you have subnets which is
396:46 - a logical partition of an ip network
396:48 - into multiple smaller network segments
396:52 - and these pretty much map to your
396:54 - availability zones if you're making one
396:56 - per a z
396:57 - and then we have knuckles these act as a
396:59 - firewall at the subnet level then we
397:01 - have security groups that act as a
397:02 - firewall at the instance level so
397:04 - hopefully that gives you a good overview
397:06 - okay
397:10 - [Music]
397:11 - all right so now let's take a look at
397:12 - enterprise or hybrid networking so we
397:14 - have our on-premise uh environments or
397:16 - your private cloud and then we have our
397:18 - aws account or our public cloud so
397:20 - there's a couple services here that we
397:21 - can bridge them together the first is
397:24 - aws virtual private network vpn it's a
397:26 - secure connection between on-premise
397:28 - remote offices and mobile employees then
397:31 - you have direct connect this is a
397:32 - dedicated gigabit connection from
397:34 - on-premise data center to aws so it's a
397:36 - very fast connection a lot of times a
397:38 - direct we say it's a
397:40 - private connection but that doesn't
397:41 - necessarily mean secure it's not
397:42 - encrypting uh the data in transit so
397:45 - very commonly these services are used
397:47 - together not just singular okay
397:50 - and then uh we have private links and so
397:52 - this is where you already uh are using
397:55 - aws but you want to keep it all within
397:56 - databus never going out to the internet
397:58 - okay so these are generally called vpc
398:01 - interface endpoints and then the
398:02 - marketing pages call them private links
398:04 - which is a bit confusing but you know it
398:06 - just keeps traffic within the database
398:08 - network so it does not transverse out to
398:09 - the internet okay
398:10 - [Music]
398:15 - hey this is andrew brown from exam pro
398:17 - and we are taking a look at vpcs and
398:19 - subnets so a vpc is a logically isolated
398:22 - section of the database network where
398:23 - you launch your aws resources and you
398:25 - choose a range of ips using a cider
398:27 - range so cider range is an ip address
398:30 - followed by this netmaster subnet sub
398:33 - mask that's going to determine how many
398:34 - ip addresses there are
398:36 - and there's a bunch of math behind that
398:38 - which we're not going to get into
398:41 - but anyway so here is an architectural
398:43 - diagram just showing a vpc with a couple
398:45 - subnets so subnets is a logical
398:47 - partition of an ip network into multiple
398:49 - uh smaller network segments and so
398:52 - you're essentially breaking up your ip
398:54 - ranges for vpcs into smaller networks so
398:56 - just thinking about cutting up a pi okay
398:58 - so subnets need to have a smaller cider
399:01 - range to
399:03 - the vpcs represent for their portion so
399:06 - uh four slash 24 is actually smaller
399:09 - which is interesting the the higher the
399:11 - number gets the smaller it gets and so
399:12 - this would allocate 256 ip addresses and
399:15 - so that's well smaller than 16 okay
399:18 - we have the concept of a public subnet
399:21 - so this is one that can reach the
399:22 - internet and a private subnet the one
399:24 - that cannot reach the internet and um
399:26 - these are not strictly enforced by aws
399:29 - so the idea is that when you have a
399:31 - subnet you can just say don't by default
399:34 - assign publicly assignable ip addresses
399:37 - but it's totally possible to launch an
399:38 - ec2 instance into your private subnet
399:42 - and then turn on um
399:44 - the ip address so you've got to do other
399:46 - things to ensure that they stay private
399:48 - or public okay
399:49 - [Music]
399:53 - hey it's andrew brown from exam pro and
399:55 - we are comparing security groups versus
399:57 - knackles so i have this nice
399:59 - architectural diagram that has both
400:00 - knuckles and security groups in them and
400:02 - we'll just kind of talk about these two
400:04 - so knackles stand for network access
400:06 - control lists and they act as a virtual
400:08 - firewall at the subnet level and so here
400:10 - you can create and allow uh and deny
400:13 - rules and this is really useful if you
400:15 - want to block a specific ip address
400:17 - known for abuse
400:19 - and i'm going to just kind of
400:20 - compare that against security groups
400:22 - because that's going to be a very
400:23 - important difference okay so security
400:25 - groups act as a firewall at the instance
400:27 - level and they implicitly deny all
400:29 - traffic so you create only allow rules
400:32 - so
400:33 - you can allow an ec2 instance to access
400:35 - port on uh
400:37 - port 22 for ssh but you cannot block a
400:39 - single ip address and the reason i say
400:41 - that is because in order for you to
400:43 - block a single ip address and secure
400:45 - group you would literally have to block
400:46 - or you'd literally have to allow
400:48 - everything but that ip address and
400:50 - that's just not feasible okay and so if
400:52 - you can remember that one particular
400:54 - example you'll always be able to
400:55 - remember the difference between these
400:56 - two one other thing that
400:59 - aws likes to do is is ask which ones are
401:01 - stateless which ones are stateful but at
401:03 - the cloud particular level they're not
401:05 - going to be asking you that okay
401:10 - all right let's learn a bit about
401:12 - networking with aws so what i want you
401:14 - to do is go to the top and type in vpc
401:17 - which stands for virtual private cloud
401:18 - and what we'll do is set up our own vbc
401:20 - it's not so important that you remember
401:22 - all the little bit of details but you
401:23 - get through this so that you can
401:25 - remember the major components so what
401:27 - i'll do is create a new vpc i'm going to
401:28 - call this my vpc
401:31 - uh tutorial and here i'm going to say
401:35 - forward slash 10.0.0.06
401:36 - the reason you're wondering why i'm
401:38 - doing that if we go to x y
401:40 - x y z here
401:43 - this tells you the size of it so i go
401:44 - here i put 16 so you can see we have a
401:46 - lot of room if we do 24
401:49 - it takes up it it's smaller see so this
401:52 - is basically the size of it right the
401:53 - empty blocks over here so we're gonna
401:55 - have a lot of room so we do ten zero
401:57 - zero zero sixteen we don't need ipv6
402:00 - we're gonna go ahead and create that
402:02 - and once we have that we can go ahead
402:04 - and create a subnet which we will need
402:06 - so we're going to choose our vpc we'll
402:08 - go down here and say my subnet tutorial
402:12 - and we'll choose the first say z you can
402:14 - leave it blank it'll choose it random
402:16 - and then we need to choose a block that
402:17 - is smaller than the current one so 16
402:20 - would be definitely um
402:23 - well 16 is the size that we have now so
402:25 - we can match that size but
402:27 - 10.0.0.0 forward slash 24 would be
402:30 - absolutely smaller okay so go ahead and
402:32 - create that subnet
402:35 - and so that is all set up now
402:38 - um
402:40 - let's see if our route table is hooked
402:41 - up so our route table says where it
402:44 - links to
402:45 - and it says to local so it's not going
402:46 - anywhere and that's because we need to
402:48 - attach a
402:50 - internet gateway that allows us to reach
402:52 - the internet so if we go over here and
402:54 - create a new internet gateway we'll say
402:56 - my igw
402:59 - and we'll go ahead and create that
403:01 - and what we'll do is
403:03 - associate that with our vpc we created
403:06 - here
403:07 - okay
403:08 - and so now that we have the internet
403:10 - gateway attached we want that subnet to
403:12 - make its way out to the internet so if
403:15 - we go to the route table we can edit the
403:18 - route table association here
403:20 - i like how it keeps on showing me this
403:21 - as if i don't know what i'm doing but i
403:23 - do
403:24 - and so
403:27 - this would change that particular
403:29 - association but i want to add to that
403:31 - route table
403:32 - so i thought when i clicked that it
403:34 - would allow me to add more but
403:35 - apparently i got to go to route tables
403:37 - over here
403:38 - and i'm looking for the one that is ours
403:40 - we can see that it's over here
403:42 - you could even name it if we wanted to
403:43 - like my route table
403:47 - notice that we apply
403:49 - names it's actually just applying a tag
403:51 - see over here it's always what that is
403:54 - so go over to routes and we want to edit
403:57 - the routes and we want to add a route
403:59 - and we want this to go to zero zero zero
404:01 - and we're gonna choose the internet
404:02 - gateway
404:03 - okay
404:06 - we're gonna say save changes
404:08 - and what that's going to allow us to do
404:09 - is to reach the internet
404:13 - um and so what i want to do is go back
404:16 - to subnet i was just curious about this
404:17 - i've never used this before
404:20 - um so it looks like we could just choose
404:22 - some options here i'm not too concerned
404:24 - about that but i assume like that's used
404:26 - for debugging azure's had those kind of
404:28 - services for a long time and so it was
404:30 - been starting to add those so you can
404:32 - easily debug your network which is nice
404:35 - so
404:35 - we have a subnet the subnet
404:38 - can reach the internet because there's a
404:40 - there's a
404:43 - internet gateway and it's hooked up via
404:44 - the route table one thing that matters
404:46 - is will it assign a public ip address
404:50 - so that is something that we might want
404:52 - to look into
404:53 - it's not the default subnet which is
404:55 - totally fine
404:56 - so it says auto assign
404:58 - is no so that might be something that
404:59 - you might want to change
405:01 - so here we would go to edit the rope
405:03 - table association no it's not there they
405:06 - changed it on me
405:07 - it used to be part of the setup
405:09 - instructions you should just checkbox it
405:10 - now and they moved it modify the
405:12 - autoassign so we'll say enable so that
405:14 - means it's always going to give it a
405:15 - public ip address on launch
405:19 - and while we're here i'm just going to
405:20 - double check if i have any elastic ips i
405:22 - did not release okay just double
405:23 - checking here
405:24 - and so
405:26 - this is all set up and we should be able
405:28 - to launch a
405:30 - ec2 now within our our new vpc so i'll
405:33 - go over here to ec2
405:36 - okay
405:38 - and i'm going to launch a new instance
405:41 - let's say amazon links 2
405:44 - we're going to choose this tier here
405:47 - and now what we should be able to do is
405:49 - select that
405:51 - and that is our subnet there okay
405:55 - go ahead and launch that i don't care if
405:57 - we use a key whatsoever so i'm gonna go
405:59 - ahead and launch that there
406:02 - okay we'll go back
406:07 - and so there you go it is launching so
406:09 - we created our vpc and we launched uh in
406:11 - it no problem whatsoever
406:14 - so hopefully that is pretty darn clear
406:16 - um so yeah
406:18 - what i'm going to do is i'm going to let
406:19 - that launch because i want to show you
406:21 - security groups
406:22 - so within aws you can set security
406:25 - groups and knackles
406:26 - and that's going to allow or deny access
406:29 - based on stuff and when we launched this
406:30 - ecto instance it has a default security
406:32 - group that was assigned we could have
406:34 - created a new one but what i might want
406:36 - to do is create myself a new security
406:38 - group here
406:39 - okay and you can end up with a lot
406:42 - really fast like here's a bunch
406:44 - and i can't even tell what's what so
406:46 - like this bunch for load balancers and
406:48 - things like that
406:49 - and so i might just go ahead and delete
406:51 - a bunch of these because i cannot tell
406:52 - what is going on here
406:55 - and
406:57 - we'll delete these security groups
407:00 - and sometimes they won't let you delete
407:01 - them because they're associated with
407:02 - something like a network interface or
407:03 - something
407:09 - all right but
407:11 - we need to find out which one we're
407:12 - using right now
407:13 - so the one that we are using is the
407:15 - launch wizard4 so we'll go into here
407:19 - and i don't know if you can rename them
407:21 - after they've been created i don't think
407:22 - so which is kind of frustrating because
407:24 - if you want to rename it it's like i
407:25 - don't want that to be the name
407:28 - so what's interesting is you can go here
407:29 - and you can edit the routes
407:32 - the rules sorry the inbound rules and
407:34 - the outbound rules and so here it's open
407:36 - on port 22 so that allows us to ssh in
407:39 - we could drop this down and choose
407:41 - different things so if we want people to
407:42 - access a website we go port 80 and we
407:45 - save from anywhere ipv46 so now anyone
407:47 - can access it
407:50 - you might want to do something like
407:52 - give it access to postgres that runs on
407:55 - port 5432 things like that
407:57 - um could be something else
407:59 - like maybe you need to connect a
408:00 - redshift that's on that port you can go
408:02 - ahead and save those rules we're just
408:04 - going to say uh from anywhere it can
408:06 - even say my ip so maybe only i'm allowed
408:09 - to connect to it right
408:10 - so you added inbound rules you don't
408:12 - really ever have to touch outbound rules
408:14 - it's set for all traffic so it's stuff
408:16 - that's leaving
408:17 - uh the
408:19 - that there one interesting thing to note
408:21 - about security groups is that
408:26 - you don't have a deny
408:27 - option right so let's say you only
408:29 - wanted a particular ip address you only
408:31 - wanted let's say what's my ip my ip
408:34 - address
408:36 - so that is my ip address and let's say
408:39 - i wanted to block it
408:42 - right so i go here and i say okay i want
408:44 - to block
408:46 - on all tcp i want to block this number
408:49 - right
408:50 - but i can't do that all i can say is i
408:52 - allow this number so in order to do it i
408:54 - would have to enter everything but this
408:55 - number in here and you can enter ranges
408:57 - in
408:58 - with like these forward slashes and
409:00 - stuff like that but you would imagine
409:01 - that'd be really hard because you have
409:02 - to starting to like
409:03 - you'd have to start and go through every
409:04 - single ip address in the world to get it
409:06 - out of here and that's almost impossible
409:08 - and that's the key thing i want to
409:09 - remember about security groups
409:12 - so that's security groups and there's
409:14 - also knackles
409:17 - knackles they're associated with subnets
409:19 - so they probably show up under vpc i
409:22 - rarely touch knackles rarely ever have
409:24 - to
409:25 - um
409:26 - i mean they're great tools but you know
409:28 - for me i just don't ever need them
409:30 - so knackles are associated with subnets
409:34 - so we can go here and try to see my
409:36 - subnet tutorial so we created our subnet
409:38 - we got a knackle for free and we can set
409:40 - inbound and outbound rules and so here
409:44 - here is where we could say okay i want
409:46 - to add a new rule
409:48 - and i want to and i want to make the
409:50 - rule number 150
409:52 - you always do these in the hundreds okay
409:54 - or the power of tens so that you can
409:55 - move them around easily
409:57 - and i can say all
409:58 - traffic that comes from
410:01 - this ip address
410:03 - i'm gonna put the forward slash zero
410:04 - that just means a single ip address
410:07 - and i say deny right and so now
410:10 - uh this at my address i can't access
410:13 - that ec2 instance okay if i try to go
410:15 - there's nothing running on the server
410:16 - but if i was to try to use it i wouldn't
410:18 - be able to do it
410:19 - and and this applies to anything for
410:20 - that subnet it's not for a particular
410:22 - instance it's for anything in that
410:24 - subnet so hopefully that is is pretty
410:26 - clear there
410:27 - but that's pretty much all you really
410:28 - need to know i mean there's lots of
410:30 - other stuff like network firewalls all
410:32 - these other things it gets pretty
410:33 - complicated
410:35 - it's well beyond what we need to learn
410:37 - here but what we'll do is tear down that
410:39 - ec2 instance
410:42 - okay
410:43 - we'll terminate that
410:46 - and once that instance is destroyed we
410:47 - can get rid of our security group and a
410:49 - bunch of other stuff
410:52 - and there's always a bunch of these darn
410:54 - things
410:57 - so we'll say delete
411:02 - one security group associated
411:07 - so we go here this is the one we are
411:08 - using but i wanna get rid of all these
411:10 - other ones
411:16 - okay if i go here it could be because
411:17 - like of inbound rules
411:20 - so see this one because you can
411:21 - reference
411:23 - another security group within a security
411:24 - group so i'm just going to go save that
411:25 - there
411:26 - see any my ip there oops
411:31 - it's set to
411:32 - nfs so that might have been set up for
411:34 - our access point
411:39 - i could just delete it that would
411:40 - probably be easier
411:43 - okay
411:44 - so that's one that's kind of of a pain
411:47 - so i'm just looking for rules that might
411:49 - be referencing other security groups
411:54 - to get rid of them
411:57 - okay let's try this again
412:06 - we'll go ahead and delete
412:07 - i'm leaving the um
412:11 - i'm leaving the uh the defaults alone
412:13 - because those come with your vpcs and
412:15 - you don't want to get rid of those
412:20 - it won't let me delete
412:21 - this one so i'm going to go edit that
412:23 - rule
412:24 - delete it save it
412:27 - you might not have this kind of cleanup
412:28 - to do it's just might be me here you
412:31 - know
412:33 - um outbound inbound
412:36 - let's try this again here
412:44 - delete
412:48 - and i'll open this one up
412:53 - must be this one that is referencing the
412:55 - other one
413:01 - i'm just going to delete the rule
413:07 - and this is something that's just kind
413:08 - of frustrating with aws but it's just
413:10 - how it is where
413:11 - sometimes it's hard to get rid of
413:13 - resources because you have to click
413:14 - through stuff so it's not always a clean
413:16 - you might have like lingering resources
413:18 - and this isn't going to cost us anything
413:20 - but it's just the fact that um
413:23 - that it just makes things
413:25 - harder to see what you're doing you know
413:31 - this last one really doesn't want to go
413:32 - away
413:36 - so i'm just trying to delete all the
413:38 - rules out of here get rid of it
413:40 - can i delete this one now
413:44 - one group associated it will not show me
413:46 - what it's talking about okay here it is
413:49 - um
413:53 - ah okay this is referencing it
413:56 - it was the one there was an old one i
413:57 - don't know what this is
414:07 - we'll go down here
414:10 - and we'll go here and delete that and
414:12 - while i've been cleaning all these up
414:14 - now we can go over to our instance make
414:16 - sure that it's terminated it is good
414:18 - because if our instance is not
414:19 - terminated we cannot destroy the vpc uh
414:22 - prior the vpc could not be destroyed
414:23 - unless you detach the internet gateway i
414:25 - wonder if it's going to still complain
414:27 - about that
414:29 - we'll say yes it actually looks like it
414:31 - includes it in the cleanup
414:34 - type delete here
414:40 - there we go so we're all good we're all
414:42 - cleaned up there you are
414:44 - [Music]
414:48 - hey this is andrew brown from exam pro
414:50 - and in this video i just want to show
414:51 - you cloud front so let's make our way
414:53 - over to cloudfront
414:54 - and cloudfront is a content delivery
414:56 - network and it's used to cache your data
414:58 - all over the place as you can see i have
415:00 - some older ones here if you have a
415:02 - splash screen what you can do is just
415:04 - look for the left-hand side there might
415:05 - be a hamburger menu open that up and
415:07 - then click on distributions and what
415:09 - we're going to do is create a new
415:10 - distribution if you don't want to create
415:12 - one because these do take forever to
415:13 - create
415:14 - you can just kind of watch along i don't
415:16 - even feel like i'm going to hit the um
415:18 - the create distribution button because i
415:19 - just hate waiting for so long but the
415:21 - idea is that you have to choose an
415:22 - origin and so the origin could be
415:24 - something like an s3 bucket a load
415:25 - bouncer media store this is where um the
415:29 - the content distribution network is
415:30 - going to source
415:32 - its content right so if i say
415:34 - this bucket here
415:36 - and i just it will probably default to
415:38 - the root path the idea is that it's
415:39 - going to be able to pull content from
415:41 - there and then cache it everywhere
415:43 - and then down below you can say
415:45 - okay set the type of protocol redirect
415:48 - to here you can set up caching rules or
415:51 - like how often do you want it to cache
415:53 - like cache a lot don't cache a lot but
415:56 - the great thing is like you have these
415:57 - edge or these um lambda edge functions
416:00 - so you can
416:02 - read and modify the requests and
416:04 - response to the cdn which is very
416:06 - powerful but what i'm going to do is i'm
416:07 - just going to go look at what we already
416:09 - have because again i said they take
416:10 - forever to spin up and we're not going
416:12 - to see too much if we do so once it's
416:15 - spun up this is what it looks like so
416:18 - you'll have an origin it says where it's
416:19 - pointing to you can create multiple
416:21 - origins group them uh you can modify
416:24 - your behavior so that was basically what
416:25 - we're looking at before as you can see
416:26 - we have our behavior there nothing super
416:29 - exciting
416:30 - we can set up error pages you can
416:32 - restrict based on geographical locations
416:34 - so if you're for whatever reason if you
416:35 - if you're not allowed to serve content
416:38 - in uk you could say exclude this
416:40 - geographical region right so you have an
416:42 - allow list
416:43 - or a block list saying like okay we
416:45 - can't do uk because like let's say you
416:47 - just don't want to do um
416:49 - england you don't want to do um uh gdpr
416:52 - for whatever reason you could block out
416:54 - i don't know i'm having a hard time here
416:55 - britain
416:57 - england it's england right united
416:59 - kingdom there we go so you just say okay
417:02 - forget united kingdom i don't have to do
417:03 - gdpr now uh for invalidations the idea
417:06 - is that you know it is a cache so things
417:08 - can get stale or just persist and so
417:11 - here you can just type in say i want to
417:12 - get rid of
417:14 - image.jpg and then you create that
417:16 - invalidation and then it will go delete
417:18 - it out of the cache and so the next time
417:20 - someone requests they'll get the fresh
417:22 - content this usually doesn't take that
417:23 - long but that's pretty much cloudfront
417:25 - in a nutshell okay
417:27 - [Music]
417:31 - hey this is andrew brown from exam pro
417:33 - and we are taking a look at ec2 also
417:35 - known as elastic compute cloud and so
417:36 - this is a highly configurable virtual
417:39 - server or it's also known as a virtual
417:41 - machine and that's what we're going to
417:42 - generally refer to it uh ec2 is
417:45 - resizable compute capacity it takes
417:47 - minutes to launch new instances and
417:49 - anything and everything on database uses
417:51 - ec2 instances underneath that's why we
417:53 - generally call it the backbone to all
417:55 - the eight of the services and uh you're
417:57 - gonna just have to choose a few options
417:58 - here so the first thing you'll need to
418:00 - do is choose your os via your amazon
418:03 - machine image so that's where you get
418:04 - red hat ubuntu windows amazon linux zeus
418:08 - it might also come with pre-installed
418:09 - libraries and things like that then you
418:11 - can choose your instance type that's
418:12 - going to determine things like your
418:14 - vcpus your memory so here you can see
418:17 - how many there are and you'll have like
418:19 - a monthly cost
418:20 - and that's the name of the instance type
418:22 - then you have to add storage so very
418:24 - commonly you're attaching elastic block
418:26 - storage or elastic files
418:29 - system or service
418:30 - and so you know if you do choose your
418:32 - ebs
418:34 - you are going to have to determine what
418:35 - type it is so whether it's a solid state
418:37 - drive a hard disk drive a virtual
418:40 - magnetic tape or even attaching multiple
418:42 - volumes not just a single one and the
418:44 - last thing is configuring your instance
418:46 - so this is configuring the security
418:47 - groups the key pairs user data im rolls
418:49 - placement groups all sorts of things so
418:52 - we will experience in that because we
418:53 - will show you how to launch an ec2
418:55 - instance and it'll make a lot of sense
418:57 - if it does not make sense right now okay
418:59 - [Music]
419:04 - all right let's take a look here at ec2
419:05 - instance families so what are instance
419:07 - families well instant families are
419:08 - different combinations of cpu memory
419:11 - storage and networking capacity and
419:14 - instance families allow you to choose
419:15 - the appropriate combination of capacity
419:18 - to meet your application's unique
419:19 - requirements different instance families
419:21 - are different because of the varying
419:23 - hardware used to give them their unique
419:25 - properties and we do talk about this
419:27 - thing about uh
419:29 - capacity reservation where aws can
419:31 - actually run out of a particular type of
419:32 - instance family because they just don't
419:34 - have enough hardware in that data center
419:35 - so you have to reserve it but let's go
419:37 - through the different types of instance
419:38 - families the first is general purpose
419:41 - and these are the names of the different
419:43 - families uh very popular ones is the t2
419:46 - the t2 and one that's really interesting
419:49 - is the mac which actually allows you to
419:51 - run
419:52 - a
419:52 - mac server
419:54 - so these are great balance of compute
419:55 - memory and network resources so you're
419:57 - going to be using these most of the time
419:59 - the use cases here would be web servers
420:01 - code repositories things like that then
420:03 - you have compute optimize so um they all
420:06 - start with c uh no surprise there
420:08 - they're ideal for compute bound
420:09 - applications that benefit from high
420:11 - performance processor thread cases here
420:13 - are scientific modeling dedicated gaming
420:15 - servers ad server engines things like
420:16 - that then you have memory optimized um
420:20 - and so there's a variety here these are
420:21 - fast performance for workloads that
420:23 - process large data sets and memory
420:26 - they're great for in-memory caches and
420:28 - memory databases real-time big data
420:29 - analytics then you have accelerated
420:31 - optimize so this is your p2 p3 p4 things
420:34 - like that these are hardware
420:36 - accelerators or coprocessors these are
420:39 - great for machine learning computational
420:40 - finance seismic analysis speech
420:43 - recognition if you're doing um uh ml on
420:46 - aws you'll start coming across these
420:48 - types aws technically has a separate
420:50 - page on sagemaker ml machines but
420:52 - they're all pulling from these instance
420:54 - families okay then we have storage
420:56 - optimized so i3 i3en things like that
420:59 - these are highly high sequential read
421:01 - and write access to very large data sets
421:03 - on local storage the use cases here
421:05 - would be nosql in memory or
421:06 - transactional databases data warehousing
421:09 - for the certified cloud partitioner you
421:11 - just need to generally know these five
421:12 - categories not the names of the instance
421:15 - families if you're doing
421:17 - associates or above you definitely want
421:19 - to know these things in a bit more
421:20 - detail and i want to say that commonly
421:22 - instant families are called instance
421:23 - types but an instance type is a
421:25 - combination of size and family but even
421:28 - aws documentation doesn't make this
421:30 - family distinction clear but i know this
421:32 - because you know in azure they make that
421:34 - very clear and and gcp and so i'm
421:36 - bringing that language over here to just
421:38 - kind of normalize it for you okay
421:40 - [Music]
421:44 - let's take a look at what ec2 instance
421:46 - types are so an instance type is a
421:48 - particular instant size and instance
421:50 - family and a common pattern for instance
421:52 - sizes you'll see is things like nano
421:54 - micro small
421:56 - medium large x large 2x large 4x large
422:01 - 8x large and you know generally they're
422:03 - you to the power of twos but sometimes
422:05 - it'll be like 12 14 16 where it's even
422:08 - uh and so when you go to launch your ec2
422:10 - instance you're going to have to choose
422:12 - that instance type and so here you can
422:14 - see
422:15 - you know here's our ttmicro and then we
422:18 - have um the small the bdm the large
422:21 - the x large
422:22 - okay but there are exceptions to this
422:24 - pattern for sizes so you know there is
422:26 - one particular one called uh dot metal
422:29 - and so that's going to indicate that
422:30 - this is a bare metal machine and then
422:32 - sometimes you get these oddball ones
422:33 - like
422:34 - 9x large so you know the rule of power
422:37 - of two or even numbers is not always the
422:38 - case uh but generally it'll be pretty
422:41 - even for you know the start here okay uh
422:44 - just talking about instant sizes so the
422:46 - easy two instance sizes generally double
422:48 - in price and attributes so uh just
422:50 - bringing up these numbers a little bit
422:51 - closer starting at the small here you're
422:53 - gonna notice one
422:55 - two
422:55 - it doesn't maybe double there but four
422:57 - and here we see twelve twenty four uh
423:00 - almost doubles there almost doubles
423:02 - there but i want to show you that the
423:04 - price is generally almost double so 16
423:06 - 33 67
423:08 - 135 and so a lot of times like you
423:11 - always have the option to say okay do i
423:13 - want to go to the next instance size up
423:14 - or have
423:15 - an additional distance of the same size
423:18 - and sometimes it's a better approach to
423:19 - get an additional instance because then
423:21 - you can distribute it across another az
423:24 - but then you also meet additional
423:25 - capacity so there you go
423:30 - [Music]
423:32 - so we talked about dedicated instances
423:34 - and hosts a little bit but let's just
423:35 - make that distinction very clear so
423:37 - dedicated hosts are single tenant ec2
423:39 - instances designed to let you bring your
423:41 - own license so byol based on machine
423:44 - characteristics and so we'll compare the
423:46 - dedicated instance to the dedicated host
423:48 - across isolation billing
423:50 - physical characteristics visibility
423:52 - affinity between a host and instance
423:54 - targeted instance placement automatic
423:56 - instance placement and add capacity
423:59 - using allocation request so for
424:01 - isolation for dedicated instance you're
424:03 - going to get instance isolation so you
424:05 - can have the same customer on the same
424:07 - physical machine but there is
424:08 - virtualization there for them and
424:10 - there's a guarantee of that
424:12 - for a dedicated host you have physical
424:14 - server isolation so you get the whole
424:15 - server
424:17 - for billing uh on a dedicated instance
424:19 - it's per instance billing and it's gonna
424:21 - have an additional fee of two dollars
424:22 - per region and for dedicated hosts it's
424:25 - per host billing so it's a lot more
424:26 - expensive but you get the whole machine
424:28 - uh for visibility of physical
424:30 - characteristics you're not going to get
424:31 - any of that information for a dedicated
424:33 - instance for dedicated hosts you are
424:35 - such as sockets core host host id and
424:38 - this is really important when you have a
424:40 - bring your own license and they're
424:41 - saying this license is for x amount of
424:44 - cores or x amount of sockets
424:46 - then we have affinity so there's no
424:48 - affinity for dedicated instance for
424:50 - dedicated hosts you'll have consistency
424:52 - with deploys to the same instance the
424:54 - same physical server there's no control
424:56 - of target instance placement for
424:58 - dedicated instance you do have control
425:00 - on a dedicated host
425:02 - for automatic instance placements you
425:04 - have it for both
425:05 - and to add capacity using allocation
425:07 - requests it's a no for dedicated
425:09 - instance and it's a yes for dedicated
425:11 - host
425:12 - so i want to come back to the main point
425:14 - that's what's highlighted here is that
425:15 - on a dedicated host you have visibility
425:17 - of sockets core host id
425:20 - and this is really really important when
425:22 - you're bringing your own license byol
425:24 - such as
425:26 - you know
425:26 - microsoft sql servers where you have to
425:29 - specify the mana cores and things like
425:31 - that okay
425:32 - [Music]
425:36 - so we've been talking about uh tendency
425:38 - and i just wanted to make it very clear
425:40 - uh the difference between the different
425:42 - levels of tendency on aws so we have
425:45 - three okay so we got dedicated hosts so
425:47 - your server lives here and you have
425:49 - control of the physical attribute so
425:51 - basically the whole server okay
425:54 - then we have dedicated instances so your
425:56 - server is on the same uh physical
425:59 - machine as other customers but the
426:01 - actual slot that you have the dedicated
426:03 - instance will always be the same and
426:06 - then we have the default so your
426:08 - instance will live somewhere on the
426:10 - server uh and when you reboot it's going
426:12 - to be somewhere else so there's no
426:14 - guarantee that it's going to be in the
426:15 - same place every single time okay
426:20 - [Music]
426:21 - hey this is andrew brown from exam pro
426:23 - and in this follow along we're going to
426:24 - be looking at ec2 and also um services
426:27 - that are adjacent to it so like auto
426:29 - scaling groups load bouncers elastic ips
426:31 - things like that so we fully understand
426:33 - ec2
426:34 - you don't have to know tons for the exam
426:37 - but you should be able to go through the
426:38 - motions of this with me so you can
426:40 - cement that knowledge
426:41 - for some of those deeper concepts like
426:43 - working with key pairs and things like
426:44 - that
426:46 - so let's make our way over to the ec2
426:48 - console and learn what we can learn
426:50 - um and generally when you go over the
426:52 - ec2 console it'll bring it to the
426:54 - dashboard for whatever reason to bring
426:56 - me there and then the idea here is that
426:58 - on the left hand side we can make our
426:59 - way over to instances
427:01 - okay and this is where we can launch our
427:04 - first instance
427:06 - so we go here and launch our instance
427:07 - the first thing we're going to be
427:08 - presented with is to choose our mi or
427:11 - amazon machine image and so that is a
427:14 - template that contains the software
427:15 - configuration so the operating system
427:17 - applications and other binaries that
427:19 - would be installed on that os by default
427:22 - all right and so we have a variety that
427:24 - we can choose from in the quick starts
427:26 - and generally the ones that you're going
427:27 - to see first are the ones that it'll
427:28 - support so there are
427:31 - amis or operating systems that aws will
427:33 - support when you contact them and then
427:35 - there's ones that are outside that where
427:37 - they'll still help you with but they
427:38 - might not have the knowledge on so just
427:40 - understand that if you pick from these
427:42 - core ones you're going to be in good
427:43 - shape the most popular is the amazon
427:45 - linux 2 because it's part of the free
427:47 - tier and it is very minimal and well
427:50 - hardened by aws so it's a very good
427:52 - choice there but you can see you can
427:53 - install a bunch of things
427:54 - so like if you want to launch a mac os
427:57 - server you can absolutely do that a red
427:59 - hat
428:00 - susie ubuntu a windows server you name
428:04 - it they have it if you wanted something
428:06 - more
428:07 - farther out there you can go to the
428:09 - marketplace and
428:10 - subscribe to one that is managed by
428:12 - company basically everything exists
428:14 - under the sun here or you can get a
428:16 - community ami so these are ones that are
428:18 - contributed by the community for free
428:20 - but we're going to go back to quickstart
428:21 - here and what i want you to notice is
428:23 - that there is this ami id that's how we
428:25 - can uniquely identify what we're using
428:28 - if we were to change region even with
428:30 - the same amazon x2 instance this thing
428:32 - will change so just understand that it
428:33 - is regional based and it comes in a
428:35 - 64-bit variant and a arm variant and so
428:38 - we're going to be using the x86 here
428:41 - you can notice here you can change it on
428:43 - the right hand side we're going to stick
428:44 - with x86 i'm going to go ahead and hit
428:47 - next
428:48 - so now we're going to choose our
428:49 - instance type and so this is going to
428:51 - decide um
428:52 - greatly how much we're going to be
428:54 - spending because the larger it is the
428:56 - more we're going to spend so see this t2
428:58 - micro if we wandered into the pricing
428:59 - for that we go to ec2
429:01 - pricing aws
429:04 - and once we get to ec2 pricing we want
429:08 - to go to on demand
429:11 - and from here this will load
429:13 - and so down below we can kind of go find
429:16 - our price it should show us
429:18 - it should show us the list ah here it is
429:20 - okay so i can say a t2 micro
429:23 - and we can see the on demand is this
429:25 - so it seems really cheap what you got to
429:27 - do is do the math so if you do time 730
429:29 - that's how many hours there are in a
429:31 - month
429:32 - if we launch a ttmicro and let's say we
429:34 - didn't have the free tier we you do if
429:36 - you first made your account you're going
429:38 - to have 700 750 hours for free for the
429:41 - free tier but if you didn't it would
429:43 - only cost you eight dollars and and 46
429:46 - cents usd okay
429:48 - so just be aware of that if you ever
429:49 - need to figure something out go there
429:50 - copy it do the math 730 it's pretty easy
429:53 - so here we have a t2 micro in the t2
429:56 - family it's going to have one v v cpu
429:58 - notice it has a v for virtual so there
430:01 - could be more than a single cpu on the
430:04 - underlying hardware but we're only going
430:06 - to have access to one virtual cpu we
430:09 - have one gigabyte of memory
430:11 - it's for low to moderate network
430:13 - performance so that's a factor that can
430:14 - change if you need like
430:16 - gigabit stuff like really fast
430:18 - connections for on-prem hybrid
430:19 - connections and you have specialized
430:21 - servers for that but for this this is
430:23 - fine the ct micro is great uh if you
430:25 - want you can also search this way to see
430:27 - all the instance families and things
430:29 - like that you can filter for current
430:30 - generations all generations so this is
430:33 - fine okay
430:34 - so from there we're going to go to
430:35 - configure our instance type you can say
430:37 - let's launch multiples of these
430:39 - instances let's turn on spot to save
430:42 - money and try to bid for a particular
430:44 - price
430:45 - we can change our vpc it's going to
430:46 - default to the default vpc um if you
430:49 - have no subnets just going to pick one
430:51 - at random here which is fine
430:54 - whether to auto assign a public ip
430:56 - address if you do not have an ip address
430:58 - you cannot reach the internet so
431:00 - generally you want this to be enabled
431:01 - this is dependent on the subnet whether
431:03 - it will default to enabled but it
431:05 - doesn't matter if you have an ec2
431:06 - instance in a private or public subnet
431:09 - you can always override this and give it
431:10 - a public ip address you have placement
431:13 - groups which allows you to place servers
431:14 - together closely not something for the
431:16 - certified cloud partitioner there's
431:18 - capacity reservations so if you're
431:19 - worried about
431:20 - database running out of this you can
431:22 - reserve capacity so that's kind of
431:24 - interesting domain join directory this
431:26 - isn't something that i've done much with
431:28 - but i imagine that has something to do
431:29 - with um direct active directory or
431:31 - something like that to join information
431:34 - then you need to uh
431:36 - have an im role and we absolutely do
431:38 - need an item rule here so what i want
431:39 - you to do is create a new role it's
431:41 - going to close off these other tabs here
431:43 - and we will go
431:45 - wait a moment create a new role here and
431:47 - we want to do this for ec2 so we say ec2
431:50 - is what we're creating the rule for
431:52 - we'll hit next and
431:54 - i don't know if i have a policy but i'm
431:56 - gonna go ahead and um oh well i don't
431:58 - need to make a new policy but i just
431:59 - want ssm and the reason i want ssm is so
432:02 - that
432:03 - i can um
432:06 - use sessions manager to log in so we
432:07 - don't have to use key pairs we will use
432:09 - key pairs but if we didn't want to use
432:11 - it that's what we could do and this used
432:12 - to be the old roll it'll tell you hey go
432:14 - use this new one here so i just want to
432:17 - make sure i know which one it is
432:19 - and so we'll just checkbox that on we'll
432:20 - hit next we can add tags right here it'd
432:23 - be well actually we don't need to add
432:25 - any tags here so that's fine we'll sit
432:26 - next and then i'll just say my
432:29 - ssm ec2 role
432:31 - okay
432:32 - and we'll create that role
432:34 - and now that we have created that role
432:36 - we can go back to our first tab here and
432:39 - give this a refresh and then drop down
432:40 - and it should show up here
432:43 - if we go down here a little bit we could
432:44 - turn on extra monitoring there is
432:46 - monitoring built in but if you wanted to
432:49 - monitor it to a lower uh like it more
432:52 - frequently you could do that as well we
432:54 - want share tenancy right this is where
432:56 - you change the dedicated instance or
432:58 - dedicated host obviously these costs
432:59 - more but we're gonna stick with shared
433:01 - elastic conference so this is for um
433:04 - uh attaching a a fractional gpu great
433:07 - for ml not something that we want
433:10 - there's credit specification i don't
433:11 - remember seeing this before selecting
433:12 - unlimited for credit specification
433:14 - allows for uh to burst beyond the
433:16 - baseline so it's for bursting
433:18 - here you can attach an uh efs so if you
433:21 - need a file system that you want to
433:22 - mount or attach um
433:24 - then there's the enclave option so nitro
433:26 - enclave enables you to create isolated
433:28 - compute environments to further protect
433:29 - your and securely process highly
433:31 - sensitive data so it might be something
433:33 - you might want to checkbox on um based
433:35 - on your use case
433:37 - and then down below are we have the
433:38 - ability to enter our user data and this
433:40 - is something we want to do because we
433:42 - want to install
433:44 - apache so that we have something to work
433:46 - with here so what i'm going to do is
433:47 - make a shebang so that is a pound and an
433:50 - exclamation mark i know that's really
433:52 - small so i'll try to bump up my font
433:53 - here so you can see what i'm doing
433:55 - and we're going to do a forward slash
433:56 - bin and a forward slash bash on the next
433:58 - line here we're going to do yum install
434:00 - hyphen y httpd
434:03 - that's going to install apache and why
434:05 - it's not called apache i don't know why
434:07 - but they call it httpd
434:10 - there's no apache in the name there and
434:12 - so we'll say systemctl start httpd
434:15 - system ctl enable httpd so we're saying
434:19 - startup apache and then make sure that
434:21 - it stays running if we restart our
434:23 - machine
434:24 - very simple so from there we will go to
434:27 - our storage we'll say add or storage and
434:30 - this is at 8 gigabytes by default we
434:31 - could
434:32 - turn that up to 30 if we like so you can
434:35 - go all the way up to 30 if you like and
434:37 - you might want to do that but i'm going
434:38 - to leave it at 8. we could change our
434:40 - volume type i'm fine with gp2 because
434:42 - that's very cost effective and if we
434:44 - want to turn encryption and you should
434:45 - always turn on encryption there's no
434:47 - reason not to
434:48 - and so we'll turn that on it's not like
434:49 - it's going to cost you more it's going
434:51 - to be the same cost it's just your
434:52 - choice there if we want to add a tag yes
434:55 - we're going to add a name and we're
434:56 - going to say my ec2 instance
434:59 - okay
435:01 - and so that's going to give us a name
435:03 - which is something we would really like
435:04 - to have then we have a security group
435:06 - i'm going to just create a new query
435:07 - book called my
435:09 - ec2sg here
435:11 - and you'll say my ec2
435:13 - sg something you cannot do is rename a
435:16 - security group once you've made it so
435:17 - make sure you don't make a spelling
435:18 - mistake up here
435:20 - and we want to be uh
435:21 - accessing that http http
435:25 - or it's going to launch a website so in
435:27 - order to do that we need to make sure we
435:28 - have http as the type with the port 80
435:31 - open and we want it from anywhere so
435:33 - we'll say anywhere and that will be
435:35 - 0.0.0.0.0
435:38 - and that's for the ipv4 this is for the
435:40 - ipv6 okay so
435:42 - we'll just say internet
435:45 - and this is for ssh right
435:47 - and for this i would probably suggest to
435:49 - say my ip but since we might be using a
435:52 - cloud shell to do that we're going to
435:53 - leave it as anywhere so that we don't
435:55 - have any issues connecting so from here
435:57 - we'll review and launch
435:59 - and you can review
436:01 - what it is that's going on here it's
436:03 - going to say here hey you have an open
436:05 - port that's okay we we want the internet
436:07 - to see our website because that's the
436:09 - whole point there and we'll go ahead and
436:11 - launch it it's going to ask for a key
436:12 - pair we can go down and say proceed
436:14 - without key pair but what i'm going to
436:15 - do is i'm going to create a new key pair
436:17 - because i want to show you how those
436:18 - work and i'm sure we've already done in
436:20 - this course once but we'll do it again
436:22 - and so i'm going to just name this as my
436:24 - ec2 instance here and then we're going
436:27 - to go download that key pair it's going
436:28 - to download a pem file
436:31 - there and so now we can go ahead and
436:33 - launch that instance
436:36 - and while that is launching so i'm going
436:37 - to just close this other tab here we're
436:39 - going to click on the view instances and
436:41 - so here is that instance that's why we
436:43 - put the tag so we can have a name there
436:45 - we're going to wait for that to start
436:46 - but as that's going i'm going to make a
436:47 - new tab by just right clicking here on
436:49 - the logo
436:50 - click anywhere pretty much to do that
436:52 - and once we do that we'll click on cloud
436:54 - shell
436:58 - and as that is going what i want to do
437:00 - is take this pim down below i'm going to
437:02 - move it to my desktop to make it easier
437:04 - for me to upload i'm doing this off
437:06 - screen
437:07 - okay
437:10 - and
437:11 - uh once this environment's running i'm
437:13 - going to go ahead and upload that okay
437:16 - so we'll just give it a moment to do
437:18 - that we're also waiting for the server
437:20 - to spin up as you'll notice there is a
437:23 - public ip address here it says it's
437:25 - running so if we want we can copy it
437:27 - we're looking for those two checks to
437:29 - pass so the server could be available
437:32 - but generally you want to wait for those
437:33 - two system checks because one says hey
437:35 - the hardware is fine the network's fine
437:37 - things like that okay but if i take that
437:39 - ip address paste it on and up here we
437:41 - have the web page so that is working uh
437:44 - no problem there so that's great
437:46 - and we'll go over to cloud shell and
437:48 - that is still starting uh it's not the
437:50 - fastest but that's just how it is
437:52 - and um you know we'll get going here in
437:55 - a second as soon as
437:58 - this decides to load
438:01 - there we go so it's loaded i can type
438:03 - clear here just to clear that screen out
438:05 - and so what i want to do is upload that
438:07 - pem file so i'm going to go and upload
438:09 - that file we're going to go ahead and
438:10 - select it i'm going to go to my desktop
438:12 - here whoops my desktop and we are going
438:14 - to choose my ec2 instance pem
438:17 - all right and from there we'll hit
438:18 - upload that's going to upload that pem
438:20 - file
438:23 - once that is uploaded we're going to do
438:24 - ls
438:27 - okay and so this is from a previous
438:28 - tutorial so i'm going to go ahead and
438:30 - just delete that other one there we'll
438:31 - say remove efs example pem
438:35 - yes
438:36 - okay we'll type clear
438:39 - and then what we can do here is type in
438:40 - chamod and
438:42 - i believe it's 400
438:44 - and what do we call this my ec2 instance
438:47 - pem if you hit tab it will auto complete
438:48 - which is nice and if you do ls hyphen la
438:51 - we can take a look at that file and see
438:54 - it should look like this should have
438:56 - only one r here so the idea is you're
438:58 - locking it down so it's not writable or
439:00 - executable it's just readable because
439:02 - that's what you have to have it if you
439:03 - want to ssh and so if we want ssh what
439:06 - we'll do is hit the connect button here
439:09 - and we have four options they just give
439:11 - you too many options it's gonna be a
439:12 - fifth one for sure soon but right now
439:14 - we're talking about ssh so for ssh um we
439:17 - had the chamod or file which we did and
439:19 - then we need to use this dns to connect
439:22 - to it and so this is the full line here
439:23 - if you click on this copy that over and
439:25 - paste it in
439:27 - that should be everything and notice
439:28 - we're doing ec2 user
439:30 - followed by this you could put the ip
439:32 - address in here it said if you preferred
439:35 - so if you were over here
439:39 - you could go and take that ip address
439:40 - which is
439:41 - i think shorter nicer but um you know if
439:43 - you just click that one button it works
439:45 - that's fine you always have to accept
439:47 - the fingerprint then you'll be inside
439:50 - the instance you can type who am i to
439:51 - see which user you are you're the ec2
439:53 - user that's the user that aws creates
439:56 - for their amazon linux instances
439:58 - it's going to vary per
440:01 - ami so not all amis have an ec2 user it
440:04 - might be something else but that's
440:05 - generally the ones that adas uses for
440:07 - their supported ones and so if we do um
440:09 - an ls
440:11 - again we're in the server right now we
440:12 - can tell because it says right here or
440:14 - if we do a pwd we can kind of just kind
440:16 - of look around so i think it's going to
440:17 - be at var ww that's where ht httpd or
440:21 - apache always puts their files here
440:24 - so i go in here whoops
440:26 - i'm just looking for
440:28 - the index file
440:30 - so i thought the index file was in
440:34 - cd bar www
440:38 - hmm
440:39 - html
440:41 - well where the heck is it so i'm going
440:43 - to just touch a file here and see if it
440:44 - overrides it
440:47 - oh i don't care i'll just type sudo
440:50 - and what we can do is just try to
440:51 - restart this system ctl um
440:55 - there's a very similar command that's
440:57 - like uh service and so i always forget
440:59 - the order of it so
441:00 - i think it'd be i'm just checking
441:03 - probably
441:04 - restart
441:05 - httpd
441:08 - and so fail to restart the policy was
441:11 - not provided as the name service
441:13 - service
441:20 - uh maybe sudo
441:23 - there we go and so if we go back here
441:25 - i'm gonna see if it changed
441:27 - because it will take whatever is in the
441:28 - index.html file so if there's no file
441:30 - there it's going to show that there and
441:32 - so what i can do
441:34 - is i can edit this file i'm going to
441:35 - type vi index html and
441:38 - um i'm going to hit i for insert mode
441:41 - oh it says it's read only so what we
441:43 - have to do
441:44 - cue colon cue quit
441:47 - oops clear
441:49 - ls and so what we need to do is do sudo
441:52 - vi
441:53 - index html
441:55 - and so vim every single key is a hot key
441:58 - okay um
441:59 - i'm not teaching vim here but i'm going
442:00 - to teach you the basics but the idea is
442:01 - that when you're here notice that the
442:03 - cursor is blinking when i hit i it
442:06 - enters insert mode now i can type
442:08 - normally so i'd say hello
442:11 - uh hello cloud okay and i'm gonna hit
442:14 - escape to go back to um
442:17 - navigation mode whatever you wanna call
442:18 - it i'm gonna hit colon so it brings up
442:20 - the command i'm gonna type in uh write
442:23 - and quit okay and hit enter
442:26 - and so i'll type clear and so oops clear
442:30 - and so we'll hit up till we get that
442:32 - command
442:33 - sudo systemctl restart httpd we'll hit
442:36 - that hit enter
442:39 - okay and it should restart
442:42 - pretty fast
442:43 - there it is this is hello cloud i
442:45 - probably didn't even have to restart it
442:46 - to do that but anyway so now that
442:48 - instance uh you can see how we're
442:50 - updating that so what i want to do is
442:52 - just do a sanity check and make sure
442:54 - that if we restart this instance that
442:56 - we're going to be able to
442:58 - have apache running that's something you
443:00 - should always do if you have an app and
443:01 - you or anything you install it restart
443:03 - your server make sure that everything
443:05 - works so what i'm going to do
443:06 - is uh just hit exit here so we go back
443:10 - to the top level cloud shell type clear
443:12 - i'm going to go back over to my ec2
443:14 - instance
443:15 - i'm gonna have to click around to find
443:17 - it here and what i want to do is reboot
443:19 - it
443:20 - okay and if i reboot the machine the ip
443:22 - address is going to stay the same okay
443:25 - so if we reboot it the ip address is
443:26 - going to stay the same and the reboot's
443:28 - going to happen really fast if we want
443:30 - to observe that reboot we could go over
443:33 - to
443:34 - um here on the right hand side go to the
443:35 - system log and it would show us that it
443:38 - it had rebooted
443:40 - i think so yeah it does cloud in it
443:42 - there i think it rebooted
443:44 - not sure
443:45 - but anyway if it's rebooted then we can
443:47 - go ahead and connect and make sure
443:48 - everything's fine so let's just go here
443:50 - and hit enter
443:52 - and let's see if the what the webpage is
443:54 - here
443:58 - notice that it's hanging right so it's
443:59 - probably because it's still restarting
444:02 - even though it doesn't look like it is
444:03 - and that's something that you have to
444:05 - understand about the cloud is that
444:07 - you have to think about what you're
444:08 - doing and have confidence that it is
444:10 - happening and also just double check it
444:12 - but uh that's something that can be kind
444:14 - of frustrating because these are
444:16 - globally available services uh they're
444:18 - massively scalable and so one of the
444:20 - trade-offs is that you don't always have
444:21 - the most
444:22 - uh responsive uh uis aws has one of the
444:25 - most responsive uis out of all the major
444:27 - providers but even still like sometimes
444:29 - i have to second-guess myself but the
444:31 - page right now is not working now it is
444:33 - so it's fine so it just took time for
444:35 - that to reboot
444:36 - and so um what i want to do is connect a
444:38 - different way so we're going to go here
444:40 - and we're going to hit um we're going to
444:42 - checkbox that on we're going to hit
444:43 - connect and instead of using ssh client
444:45 - we're just going to go to sessions
444:46 - manager and hit connect
444:48 - and this is the preferred way of
444:49 - connecting because you don't have to
444:52 - have this this ssh key and that's a lot
444:55 - more secure because if someone has that
444:57 - key and you you know you hand it to
444:58 - someone they could hand it to somebody
445:00 - else and then you have a big problem on
445:01 - your hands so here this looks very
445:03 - similar but if you type who am i it
445:05 - actually logs in as the ssm user which
445:07 - is kind of annoying so i type in sudo su
445:10 - i have to do this hyphen here and then
445:11 - i'm going to say the user i want to be
445:12 - which is ec2 user
445:14 - and then if i type umi we are the
445:16 - correct user you can't do anything in
445:17 - that ssm hyphen user or
445:19 - ssm user so you got to switch that over
445:22 - and i can bump this up to make it a bit
445:23 - larger so this is obviously not as nice
445:25 - as working over here or even in your own
445:27 - terminal but
445:29 - it's a lot more secure and it's tracked
445:31 - and all these other things so we really
445:32 - should be using it okay
445:35 - and um i really don't like having to
445:37 - bump this up with my html i'm just go
445:40 - back to zero there there's probably a
445:41 - way to configure that but anyway
445:43 - let's just go
445:45 - and take a look at our file
445:47 - i'm gonna type buy again and we're gonna
445:48 - do var
445:49 - www.html
445:51 - index html
445:53 - i could put pseudo in front of there
445:55 - and again remember you have to hit i to
445:57 - go into insert mode
446:00 - and what i'm going to do is just
446:03 - capitalize that hello cloud give that
446:04 - exclamation mark colon wq to quit right
446:07 - quit i'm going to go back here refresh
446:09 - okay so we don't have to restart our
446:11 - server which is nice
446:12 - all right
446:13 - so um
446:15 - that's that that's pretty clear so i'll
446:17 - hit terminate here
446:19 - and i don't think we need cloud shell
446:20 - for anything so i'm just gonna close
446:22 - that
446:23 - and so that's pretty much it when it
446:25 - when it comes to working with an ec2
446:27 - instance and so the next thing i want to
446:29 - show you is elastic ip okay
446:31 - [Music]
446:35 - okay so now i want to show you elastic
446:37 - ip
446:38 - commonly abbreviated to eip and so all
446:41 - that is it's just a
446:43 - a static ip and ip that does not change
446:45 - because this ec2 instance here notice
446:47 - that it's 54 163 4 104
446:50 - and what would happen if we were to stop
446:52 - this instance not reboot it but stop it
446:54 - because for whatever reason we had to or
446:57 - or um for whatever reason
446:59 - and if we were to stop this instance and
447:02 - we were to restart it
447:05 - okay
447:07 - and we have to wait for it to stop but
447:09 - that ip address is going to change okay
447:13 - so 54 163 4104 hopefully we can observe
447:16 - that
447:17 - i'm just going to write that down so we
447:19 - do not forget
447:21 - so i can prove to you that it does
447:23 - change
447:27 - and now that it it's still stopping here
447:30 - so as that's stopping we're just going
447:31 - to go ahead and get our elastic ip and i
447:34 - will prove that as we go here so i'm
447:35 - going to go over to here
447:37 - and so what i want to do is reserve or
447:39 - allocate an elastic ip address and so
447:41 - i'm going to say us east 1
447:43 - and it's going to say from the amazon
447:45 - pool of ipv4 addresses so eight of us
447:47 - has a bunch of ip addresses they're
447:49 - holding on to and so you can just
447:51 - allocate one
447:53 - and once you've allocated that's your ip
447:55 - address so coming back to here
447:58 - okay this has stopped
448:00 - notice there is no public ip address
448:02 - we're going to start it again
448:07 - okay and then we'll just checkbox it on
448:08 - and we just have to wait a little while
448:10 - to see what the ip address is going to
448:13 - be i'm going to tell you it's going to
448:14 - be something else
448:17 - so if i go back here this is 54 235 12
448:21 - 110 and our original one was 54 163 for
448:24 - 104. so the reason why it's important to
448:27 - have the same address is that if uh you
448:30 - have a load balancer well not a load
448:31 - bouncer but if you have a domain
448:33 - pointing to your i your server and you
448:36 - reboot then the route you have a
448:39 - dangling um
448:40 - a path or route where revenue 3 which is
448:43 - going to be pointing to nothing and so
448:45 - it was does have things to mitigate that
448:47 - like aliases and things like that but in
448:49 - general you know there's cases where you
448:51 - just have to have a static ip address
448:53 - and so we had allocated one over here
448:55 - and if we want to assign it we're going
448:58 - to associate that elastic ip address
449:00 - we're going to drop it down choose the
449:01 - cc2 instance
449:03 - um i suppose the private ips as well and
449:06 - then we're going to go ahead and hit
449:07 - allocate or associate
449:10 - and once it's associated it should now
449:12 - have 34 199 121
449:15 - 116. so we go over here
449:19 - and we're going to take a look here and
449:21 - that's its ip address we can pull it up
449:24 - okay and that's that so yeah that's
449:27 - elastic ip
449:28 - [Music]
449:32 - okay so now that we
449:34 - have our elastic ip we have our ec2
449:36 - instance running let's say um you know
449:38 - we lose the server we terminate it so we
449:40 - would lose all of our configuration so
449:41 - if we wanted to bake this ami to save it
449:44 - for later what we'd have to do is go and
449:46 - create an image so to do that we go to
449:47 - the top here and we go to images and
449:49 - templates and we can create an image or
449:51 - we can create a a template which is a
449:53 - lot better but for the time being we're
449:54 - going to go ahead and create an image
449:56 - and when you create an image you're
449:56 - basically creating an ami and so here
449:59 - i'm just going to say my ec2
450:02 - and i'm going to 000 to just kind of
450:04 - like number it so that's a very common
450:06 - numbering just do three zeros and then
450:07 - increment by one and so here i'm going
450:09 - to say my apache server
450:12 - and so it's going to save some settings
450:14 - like the fact that there is a volume you
450:17 - could save some tags there and so i
450:18 - might go ahead and add a tag and it'll
450:20 - say name and we'll just say my ec2
450:22 - server or
450:24 - so that it remembers that
450:27 - okay
450:28 - and then what we'll do is go ahead and
450:29 - create our image
450:31 - and so this can take a little bit of
450:32 - time if we go over to
450:34 - uh images here
450:37 - it's going to be spinning for a while
450:38 - and we'll just wait until it's done okay
450:41 - all right so after waiting a little
450:42 - while here our ami is ready so we're
450:44 - just waiting for it to go available if
450:46 - you do not see it just make sure you hit
450:47 - the refresh
450:49 - because sometimes aws will just spin
450:50 - forever
450:51 - and so that's just something you'll have
450:53 - to do
450:54 - so you know hopefully that makes sense
450:55 - what we'll do is go make our way back
450:57 - over to instances here and we can launch
451:00 - one this way well actually we can do it
451:02 - over from
451:04 - the ami page so what i'm going to do is
451:06 - just terminate this instance we're all
451:08 - done with it
451:09 - okay and we'll hit terminate it's
451:11 - totally fine and it had a message about
451:13 - elastic ips about releasing them so when
451:15 - it does that the elastic ip is still
451:17 - over here so it did not release it so
451:20 - what we're going to do is go ahead and
451:22 - disassociate the elastic ip
451:25 - okay
451:26 - and then we're also going to release the
451:28 - ip address because if we don't we're
451:30 - going to have this ip addresses sticking
451:31 - around that we're not using it this is
451:33 - going to charge us a dollar a month over
451:34 - month so just be aware of those because
451:36 - that's just kind of like a hidden cost
451:37 - there but what we're going to do is go
451:39 - over to ami
451:41 - and we're going to select it here we're
451:42 - going to go to actions we're going to go
451:43 - ahead and launch
451:45 - and what it's going to do is make us
451:47 - fill all this other stuff again so if
451:49 - you had made a launch template we
451:51 - wouldn't have to fill out all the stuff
451:52 - it'd be part of it but that's what i'm
451:53 - trying to show you with this ami stuff
451:55 - so
451:56 - instead of filling out all this what i'm
451:58 - going to do is now go create a launch
452:00 - template just to kind of show you that
452:02 - that would be a much easier way to work
452:06 - so we go over to ec2 instances and then
452:08 - on the left-hand side we're looking for
452:10 - a launch template launch launch
452:12 - configurations is the old thing
452:15 - um launch templates here we go
452:17 - so what we'll do is create ourselves a
452:19 - launch template we'll just say my apache
452:22 - server
452:23 - and
452:24 - then down below we need to choose our
452:26 - ami so we're going to go here and we
452:29 - need to type it in so what did we call
452:30 - it my ec2
452:35 - i really don't like this search here
452:36 - it's very slow and frustrating but once
452:38 - we find it whoops
452:40 - that's why i don't like it because a lot
452:41 - of times you'll be loading and you'll
452:43 - end up clicking the wrong thing
452:45 - okay so
452:48 - uh i don't like this okay we'll type in
452:51 - my
452:53 - give it a second
452:57 - there it is and just wait because it
452:58 - will keep loading and then once it's
453:00 - loaded hit enter
453:03 - and so it has that instance selected and
453:05 - then from there
453:06 - uh don't include in the launch template
453:08 - so here we could be explicit i would say
453:11 - i want this to be
453:12 - t2 micro but we could exclude it if we
453:14 - wanted to we could specify the key pair
453:16 - here um not that we really want to use
453:18 - key pairs we'll say my ec2 instance then
453:21 - down down here for the networking we can
453:23 - specify that security group we created
453:25 - so we created one here called myec2sg
453:28 - um storage is fine it's going to be
453:31 - encrypted network interface is fine
453:33 - advanced details what i want to do is
453:35 - set the i am instance profile that's
453:37 - really important because we don't want
453:38 - to have to figure out that role every
453:40 - single time
453:41 - so put that there
453:43 - and that should be
453:45 - everything and we could put user data in
453:47 - there but it's already baked into our
453:48 - ami so we don't have to worry about
453:50 - anything so what i'm going to do here is
453:51 - go ahead and create this launch template
453:54 - and then we're going to view this launch
453:55 - template and so now what we can do is
453:58 - then use it to
454:00 - launch an instance
454:01 - okay
454:02 - and so we're going to look here and it's
454:04 - very similar to dc tube except it's
454:06 - vertical so we're going to have one
454:08 - instance it's going to use that ami that
454:09 - instance type so you can see how you can
454:11 - override them which is nice we're going
454:13 - to check the advanced details and make
454:14 - sure that iom profile is set and we'll
454:17 - go ahead and launch this from a template
454:20 - so
454:20 - from there we can go ahead and click the
454:22 - instance value there
454:24 - and just be aware that when you do click
454:26 - through links like that you'll end up
454:27 - with a search so i was just check box
454:28 - that off so i can see what i'm doing
454:30 - and so we're just waiting for this
454:32 - instance to show up and the only thing i
454:33 - noticed is it didn't set our darn tags
454:35 - so i wanted the name in there and i
454:37 - think it's because we set it in the ami
454:39 - but it didn't carry over to the launch
454:41 - template so i'd have to go back to the
454:42 - launch template and update it probably
454:44 - so if i go into here into the launch
454:46 - template
454:49 - we can probably modify create a new
454:51 - version
454:53 - and then add tags there
454:55 - so we say name
454:59 - my apache server
455:02 - i realize i'm changing between them and
455:04 - so that should allow us to have a
455:06 - version two so we'll create that
455:09 - and but anyway that will be for the next
455:11 - time we launch it okay
455:13 - and so this instance is running i'm
455:15 - gonna go grab the ip address
455:17 - the server may or may not be ready we'll
455:19 - take a look here
455:21 - and so it's just spinning if it's
455:22 - spinning it's either the server is not
455:24 - ready or um our port's not open so it
455:27 - was just
455:28 - getting ready to work there so it is
455:29 - working now
455:31 - so that is our launch template so now
455:33 - you know we don't have to worry about
455:34 - losing our stuff and if we need to make
455:36 - new versions we can just
455:38 - bake new amis
455:39 - and increment them
455:41 - and attach them as new versions of the
455:43 - launch template okay
455:45 - [Music]
455:49 - all right so what i want to show you in
455:51 - this follow along is to set up an auto
455:53 - scaling group for our ec2 instance and
455:55 - the idea behind this is that
455:57 - we'll be able to always ensure that a
455:59 - single server is running or increase the
456:02 - capacity if the demand requires it so in
456:05 - order to create an auto scaling group we
456:06 - can go all the way down below to here
456:10 - and so you know i really don't like the
456:12 - autoscaling group form but it's okay
456:14 - we'll work our way through it so the
456:15 - first thing is we'll have to create our
456:17 - or name our auto screen group so let's
456:18 - just say my asg and then we'll have to
456:21 - select a launch template which is great
456:22 - because we already have one and then
456:24 - we'll have to select the version i'm
456:25 - going to select version two so that it
456:26 - applies that tag name
456:28 - and we'll go to next
456:30 - and so here
456:32 - it's going to need to select a vpc and
456:34 - then we need some subnets so we're going
456:36 - to choose three just because to have
456:39 - high availability you have to be running
456:41 - at least three different availability
456:42 - zones so that's why we have three
456:43 - different subnets and then down below we
456:45 - have the instance type requirements so
456:47 - uh t2 micro
456:49 - launch template looks good to me so
456:51 - we'll go ahead and hit next
456:55 - and then from here we can choose to do a
456:57 - load balancer and so i want to do the
456:59 - load balancer separate so we won't do it
457:01 - as of yet but very often if you're going
457:03 - to have an auto selling group you're
457:04 - going to usually have a load balancer
457:05 - but we'll talk about that when we get to
457:07 - that point there so we'll just go to the
457:10 - bottom here and hit next and so this is
457:12 - what's important so
457:13 - how many do you want to be always
457:15 - running and so we always want to have
457:17 - one and maybe the maximum capacity is
457:19 - two and you want the desired cast
457:20 - capacity to be around a particular
457:23 - number so if you had three and you said
457:24 - the desired is two
457:25 - there are things that could try to work
457:27 - to always make sure there's two but we
457:28 - just want to have one for this example
457:30 - we can set up scaling policy so i do
457:33 - target tracking scaling policy and so
457:35 - here we could do it based on a bunch of
457:36 - different things so if the cpu
457:38 - utilization went over 50 percent it
457:40 - would launch another server so that
457:42 - might be something we might want to set
457:43 - so we're not going to try to trigger the
457:45 - scaling policy but we might as well just
457:47 - apply because it's not too hard and you
457:49 - can also do a scaling scale in
457:51 - protection policy so if you want to make
457:53 - sure it does not
457:56 - reduce the amount of servers that's
457:57 - something you could do
457:58 - we could add a notification to say hey
458:00 - there's a scaling policy happening here
458:02 - which is fine we don't have to worry
458:03 - about that and there's tags so add text
458:06 - to help you search filter etc
458:08 - so i'm going to put a tag here i'm going
458:10 - to say name
458:11 - i'm just wondering if this is going to
458:12 - attach to the ec2 instance or this is
458:14 - for the auto scanning group you can
458:15 - optionally choose to add tags to
458:17 - instances by specifying tags in your
458:19 - launch template so we already did that
458:20 - so i don't need to put a tag here
458:23 - and so we can review
458:25 - our
458:26 - auto scaling group and go ahead and
458:28 - create that auto scaling group
458:30 - okay and so that auto scaling group
458:33 - expects there to be a single instance so
458:35 - what's going to do is it's going to
458:36 - start a launching an instance and so
458:38 - what i'm going to do is just get rid of
458:40 - this old server because we don't need it
458:42 - anymore this old one here
458:44 - okay and you can already see
458:48 - okay that the load balancer
458:50 - is launching this new one here and
458:52 - remember we updated our version two to
458:53 - have that name so that's how we know
458:55 - that it is so if we go back over to our
458:57 - auto scaling group
459:01 - okay it's now saying there's an instance
459:03 - we don't have a status as of yet
459:07 - and so there are ways of doing uh status
459:10 - checks to for it to determine whether or
459:12 - not the server is working
459:14 - because if the server is unhealthy what
459:16 - it would do is it would actually kill it
459:18 - and then start up a new one right so if
459:20 - i go down below it's right now doing the
459:21 - ec2 health check and the ec2 health
459:23 - check just means that is the server
459:25 - working right is it running it doesn't
459:27 - necessarily mean like hey can i load
459:29 - this web app um but you know it's very
459:31 - simple so we'll give it a moment here
459:34 - to start up and just make sure that it's
459:35 - working
459:41 - okay and i think it's ready so if i take
459:42 - that public ip address here and paste it
459:44 - in there it is okay
459:47 - so if we were to
459:49 - tell it to increase the capacity to
459:50 - three then what it would do is it would
459:52 - launch three and then it should probably
459:54 - launch it all evenly to those other
459:57 - it should evenly launch it to all those
459:59 - other uh availability zones and then
460:01 - we'll have something that is highly
460:02 - available okay
460:04 - so that's pretty much it for this and
460:05 - then we'll move on to auto scaling
460:07 - groups
460:11 - [Music]
460:12 - all right so we have our ec2 instance
460:14 - now managed by an auto screen group and
460:16 - the great thing is that if we terminate
460:18 - this instance this auto discounting
460:20 - group will launch another uh instance to
460:22 - meet our particular capacity um the only
460:25 - thing though is that if we were to have
460:27 - multiple ec2 instances running like
460:29 - three of them
460:30 - um how would you distribute traffic to
460:33 - the mall right so you know you have an
460:35 - ip address coming in from the internet
460:37 - but let's say you want to evenly
460:39 - distribute it and that's where a load
460:40 - bouncer comes into play
460:42 - and even if you have a single server you
460:44 - should always have a load balancer
460:45 - because it just makes it a lot easier
460:46 - for you to scale when you need to
460:49 - and you it acts as an intermediate layer
460:51 - where you can attach a web application
460:52 - firewall you can attach an ssl
460:55 - certificate for free
460:56 - so there's a lot of reasons to have
460:58 - a load balancer so what we'll do is go
461:01 - down below on the left-hand side and
461:02 - we're going to make our way over to load
461:03 - bouncers and we're going to create
461:05 - ourselves a new load balancer so i'm
461:07 - going to hit create load balancer here
461:10 - and you're going to see we have a lot of
461:11 - options application load bouncer network
461:13 - load balancer gateway load balancer and
461:15 - then the classic load bouncer and so we
461:17 - are
461:18 - running an application so i'm going to
461:20 - create an application load balancer and
461:22 - here i'm going to say my alb
461:25 - for an application load balancer this is
461:26 - going to be internet facing it's going
461:28 - to be ipv4
461:30 - we're going to let it launch in the
461:31 - default
461:32 - subnet and we're going to choose the
461:34 - same
461:35 - the same
461:37 - azs
461:39 - right so that we get the same subnets as
461:41 - our
461:42 - that are in our auto scanning group and
461:43 - that's really important okay
461:45 - and then here um you know we need to
461:48 - have a security group and i just feel
461:51 - like selecting the same one here because
461:52 - that should work
461:53 - no problem there
461:55 - and we want to make sure that we can
461:57 - listen on port 80 and then it's going to
461:59 - forward it to a a target group it looks
462:02 - like i might have a target group there
462:04 - from before so just to reduce that
462:07 - confusion you won't have this problem
462:08 - i'm just going to double check if that's
462:10 - true
462:11 - so do i have a target group from there
462:12 - before before yes i do
462:15 - that came from
462:17 - i'm not sure it might have been created
462:18 - by um
462:20 - elastic bean stock and wasn't deleted
462:22 - okay so i'll go back over to here just
462:25 - so there's less confusion
462:26 - and
462:29 - we were selecting our target group so
462:31 - we're going to create a new target group
462:33 - so we'll go over here
462:34 - and here you can choose whether it's
462:36 - instance ip lambda application load
462:38 - balancer so you could point it
462:40 - specifically to an ip address and so if
462:42 - it was a static ip address that would
462:43 - make sense
462:44 - uh apparently you can port uh
462:47 - point it directly to instances i don't
462:48 - remember seeing that option before
462:51 - i guess that makes sense yeah no sorry
462:52 - that makes sense because i would go to
462:54 - uh vpcs okay or sorry uh asgs
462:57 - autoscaling groups it's just that
462:59 - you're pointing them to auto screen
463:00 - groups you're not pointing them to
463:01 - instances so that's why that's confusing
463:03 - so i'm going to say
463:04 - my
463:05 - target group it'll be for port 80 here
463:08 - protocol http 1 is fine we want to be in
463:12 - the same
463:13 - vpc so that's fine as well
463:15 - and down below we have our health check
463:17 - and so the forward slash means that it's
463:18 - going to hit the index.html page and so
463:21 - if it gets back
463:22 - um something healthy and that that
463:24 - something healthy is going to be um
463:27 - port 80 then it's going to be considered
463:29 - good
463:30 - and then we can say the threshold of
463:32 - check so i'm just going to reduce this
463:33 - so it's not so crazy so we'll say three
463:35 - uh two and then ten
463:38 - okay
463:40 - and then it expects back a 200 which i
463:42 - think that's what we'll get back so
463:44 - we'll go ahead and hit next and so now
463:46 - we have our target group and it should
463:49 - register instances so it's saying hey we
463:51 - detected this and this fits the
463:53 - requirements for this so this is now
463:55 - uh this is now in this target group okay
463:58 - so we can go back over here
464:00 - and we can now drop down and choose oops
464:03 - hit the refresh button
464:05 - and choose our target group
464:08 - so i'm not
464:10 - seeing it here so i'm gonna go back over
464:11 - here oh we didn't create it okay
464:15 - and now we can go back hit refresh and
464:18 - there it is
464:20 - and yeah that looks all good so we'll go
464:23 - ahead and hit create load bouncer
464:25 - we can view the load balancers and these
464:27 - crate really fast
464:29 - if we scroll on up
464:30 - what we can do is now access our server
464:33 - through this dns name okay so we copy
464:35 - that
464:36 - paste that on in there
464:38 - does it work
464:44 - not as of yet so if it's not working
464:46 - there because we did say look at these
464:48 - instances another way is to directly
464:50 - associate your auto scaling group with
464:52 - the load balancer
464:54 - so if i go into here and we hit uh
464:57 - edit
464:59 - there is a way
465:01 - aha a little bouncer so
465:04 - we want to associate this way and we
465:06 - want to say this target group here
465:09 - and also while we're here we might as
465:11 - well set it to elb so it's going to use
465:12 - the elb check so that makes it so the
465:14 - auto scaling group
465:15 - if it wants to restart server it's going
465:17 - to use the elbs check which is a lot
465:19 - more sophisticated
465:20 - and then what we'll do is go hit update
465:23 - okay
465:26 - and now if we go back over to our load
465:29 - balancer i'm just going to close some of
465:30 - these tabs so it's a little less
465:31 - confusing
465:34 - a little bouncer here
465:38 - i think we should be able to see through
465:40 - here whether it is seeing it
465:43 - let's go down below listeners monitoring
465:46 - integrated services no it's going to be
465:48 - through the target group
465:52 - okay
465:56 - i mean it already had it there so maybe
465:58 - it's just that it hasn't finished the
465:59 - check so over here it has a health
466:01 - status check oh now it's healthy okay so
466:04 - if it's healthy in the target group and
466:05 - the load bouncer is pointing to it then
466:07 - it should technically work so we're
466:09 - going to go ahead and
466:12 - copy the dns again here make a new tab
466:15 - paste it in
466:19 - and there it is okay so
466:22 - that's how you're gonna access
466:23 - all your all your instances that are
466:25 - within your auto scanning groups you're
466:26 - gonna always go through the dns and so
466:28 - if you had a row 53
466:31 - domain like your domain managed by aws
466:33 - you just point to the load balancer and
466:35 - that's how you hook it up so that's
466:37 - pretty much it so yeah there you go
466:43 - all right so there you go we learned
466:44 - everything we wanted to know about ec2
466:46 - so the the last thing to do is to tear
466:48 - everything down so we have a load
466:50 - balancer we have an auto scanning group
466:51 - um and those are the two things we'll
466:53 - have to
466:54 - pull on down so the first thing would be
466:56 - to take down the auto scaling group and
466:58 - when you delete another scaling group
466:59 - it's going to delete all the ec2
467:01 - instances so we'll do it that way if you
467:03 - tried to delete the ec2 it would just
467:06 - keep on spinning up so you have to
467:07 - delete that first and so as that's
467:09 - deleting then we'll be able to delete
467:10 - our load balancer i'm going to try
467:12 - anyway to see if i can delete it at the
467:13 - same time
467:16 - and so i'll go up here i'm going to go
467:18 - ahead and delete that load balancer
467:20 - actually it did work no problem
467:23 - i'm gonna make sure i don't have any
467:24 - elastic ips
467:26 - i'm gonna also make sure i don't have
467:28 - any key pairs
467:30 - you can keep your key pairs around but
467:31 - like i just want to kind of clean this
467:33 - up so
467:36 - okay
467:47 - okay and that instance should be
467:48 - terminating
467:51 - go back to the auto scan group here
467:57 - if we click into it we can check
467:59 - its activity here
468:03 - so it's just saying successful so it is
468:06 - waiting on elb connection draining which
468:08 - is kind of annoying because we deleted
468:10 - elb so there's nothing to drain
468:13 - um draining is just to make sure that uh
468:17 - you know there's no interruptions when
468:18 - terminating services so just trying to
468:20 - be smart about it
468:31 - and all i want to see is that it's just
468:32 - saying terminating over here and then i
468:34 - think we're done
468:36 - okay so we'll just have to wait a little
468:38 - while here okay
468:40 - and i'll see you back in a moment okay
468:42 - all right so after waiting a very long
468:44 - time it did destroy so if i go down over
468:47 - to my load balancer here we're gonna see
468:50 - that it does not exist so there was that
468:51 - connection draining thing which was kind
468:53 - of annoying it's probably because i
468:54 - deleted the load balancer first and then
468:56 - the um
468:58 - the uh
468:59 - the autoscaling group second and
469:01 - probably connection draining was turned
469:02 - on but it's not a big deal we just
469:03 - waited and it did eventually delete so
469:06 - we're pretty much all done here so there
469:07 - you go
469:08 - [Music]
469:12 - hey this is andrew brown from exam pro
469:14 - and we are taking a look at ec2 pricing
469:16 - models and there are five different ways
469:17 - to pay with ec2 remember each two are
469:19 - virtual machines so we have on-demand
469:21 - spot uh reserved dedicated and adamus
469:24 - savings plans so what we'll do is look
469:26 - at these in summary here and then we'll
469:28 - dive deep onto each of these different
469:30 - pricing models so for on demand you are
469:32 - paying the uh a low cost and also you
469:34 - have a lot of flexibility with this plan
469:37 - uh you are paying per hour so this is a
469:38 - pay-as-you-go model uh or you could be
469:41 - paying down to the second which we'll
469:43 - talk about uh the caveats there when we
469:45 - get to the on-demand section this is
469:46 - suitable for workloads that are going to
469:48 - be short-term spiky unpredictable
469:50 - workloads uh that cannot be interrupted
469:53 - and it's great for first-time
469:54 - applications and the on-demand pricing
469:56 - model is great when you need the least
469:58 - amount of commitment for spot pricing
470:00 - you can see we can save up to 90 percent
470:02 - which is the greatest savings of out of
470:03 - all these models here uh the idea here
470:05 - is you're requesting spare computing
470:07 - capacity that database is not using and
470:09 - that's where you're gonna get that
470:10 - savings you have flexible start and end
470:11 - times
470:12 - but your workloads have to be able to
470:14 - handle interruptions because these
470:16 - servers can be stopped at any time to be
470:18 - giving to more priority customers and
470:20 - this is great for non-critical
470:21 - background jobs very common for like
470:23 - scientific computing
470:25 - where jobs can be started and stopped at
470:27 - any given time this has the greatest
470:28 - amount of savings then you have reserve
470:30 - or reserved instances this allows you to
470:33 - save up to 75 percent this is great for
470:35 - steady state or predictable usage you're
470:37 - committing with aws for ec2 usage over a
470:41 - period of one or three year terms you
470:43 - can resell on
470:45 - unused reserve instances so you're not
470:47 - totally stuck with this if you buy them
470:49 - this is great for the best long term
470:51 - savings then you have dedicated so these
470:54 - are just dedicated servers and
470:55 - technically not a pricing model but more
470:57 - so that the fact that it can be utilized
470:59 - with pricing models um but the idea here
471:02 - is it can be used with on demand
471:03 - reserved or even spot this is great when
471:06 - you need to have a guarantee of isolate
471:08 - hardware for enterprise requirements and
471:10 - this is going to be the most expensive
471:12 - so yeah there you go and we'll dive deep
471:14 - here okay
471:16 - [Music]
471:21 - so the on-demand pricing model is a
471:23 - pay-as-you-go model where you consume
471:25 - compute and then you pay later so when
471:28 - you launch an ec2 instance by default
471:30 - you are using that on-demand pricing and
471:33 - on-demand has no upfront payment and no
471:36 - long-term commitment you are charged by
471:38 - the second up to a minimum of 60 seconds
471:41 - so technically a minute or the hour so
471:43 - let's just talk about the difference
471:45 - between those uh per second billing and
471:47 - those per hour billing so per second are
471:50 - for linux windows windows with sql
471:53 - enterprise windows with sql standard
471:55 - windows with sql web instances that do
471:57 - not have a separate hourly charge and
472:00 - then everything else is going to be um
472:02 - per hour and so
472:04 - you know when i'm launching ec2 instance
472:06 - i can't even tell when something's per
472:07 - second or per hour you just have to know
472:09 - that it has a separate hourly charge but
472:11 - generally you know if you're just
472:12 - launching things it's going to probably
472:13 - be the per second billing when you look
472:16 - up the hourly or the the pricing it's
472:19 - always shown in the hourly rate so even
472:20 - if it is using uh per second billing
472:23 - when you look up that pricing it's
472:25 - always going to show it to you like that
472:27 - but on your bill you'll see it down to
472:28 - the second okay up to the first 60
472:31 - seconds and on demand is great for
472:33 - workloads that are short-term spiky or
472:35 - unpredictable uh but when you have a new
472:38 - app development this is where you want
472:39 - to experiment and then when you're ready
472:41 - to uh start saving because you know
472:43 - exactly what that workload's going to be
472:45 - over the span of a year or three that's
472:47 - where we're going to get into reserved
472:48 - instances which we'll cover next
472:51 - [Music]
472:55 - hey this is andrew brown from exam pro
472:57 - and we are taking a look at reserved
472:58 - instances also known as ri and this is a
473:01 - bit of a complex topic but uh you know
473:04 - if we do get through it it's going to
473:05 - serve you well through
473:07 - multiple aw certifications so let's give
473:09 - it a bit of attention here so ri is
473:12 - designed for applications that have a
473:13 - steady state predictable usage or
473:15 - required reserve capacity so the idea is
473:18 - that you're saying to aws i'm going to
473:19 - make a guaranteed commitment
473:21 - saying this is what i'm going to use and
473:23 - i'm going to get savings because abuse
473:25 - knows that you're going to be spending
473:26 - that money okay so the idea here is that
473:29 - the reduced pricing is based on this
473:31 - kind of formula where we have term class
473:33 - offering the r a tributes and payment
473:35 - options technically the ra tributes
473:37 - don't exactly factor into it other the
473:39 - fact that they on our attribute could be
473:41 - like the instance type size
473:43 - but i'm going to put that in the formula
473:44 - there just because it is an important
473:45 - component so let's take a look at each
473:47 - of these components of the formula to
473:50 - understand how we're going to save so
473:51 - the first is the term so the term uh the
473:54 - idea here is the longer the term the
473:55 - greater the savings so you're committing
473:57 - to a one year or three year contract
474:00 - with aws
474:01 - um and one thing you need to know is
474:03 - that these do not renew so
474:06 - at the end of the year the idea is that
474:08 - you have to purchase again
474:09 - and when they do expire your instances
474:11 - are just going to flip back over to on
474:13 - demand with no interruptions to service
474:15 - then you have class offerings and so the
474:17 - idea here is the less flexible the
474:19 - offering the greater the savings so the
474:21 - first is standard and this is up to a 75
474:24 - reduction in the price compared to on
474:26 - demand and the idea here is you can
474:28 - modify some ra attributes which we'll
474:30 - we'll talk about when we get to the um
474:33 - ra tribute section there then you have
474:35 - convertible so you save up to 54 reduced
474:38 - pricing compared to on demand and you
474:39 - can exchange uh ris based on the ri
474:42 - tributes if the value is greater or
474:44 - equal in value and there used to be a
474:47 - third class called schedule but this no
474:48 - longer exists so if you do come across
474:50 - it just know that abuse is not planning
474:52 - on offering this again for whatever
474:54 - reason i'm not sure why
474:56 - then there are the payment options so
474:57 - the greater upfront the greater the
474:59 - savings so here we have all upfront so
475:02 - full payment is made at the start of the
475:03 - term partial front so a portion of the
475:06 - cost must be paid up front and the
475:08 - remaining hours in the terms are billed
475:10 - at a discounted rate and then there's no
475:12 - upfront so you are billed at a
475:13 - discounted hourly rate for every hour
475:15 - within the term regardless of whether
475:17 - the reservation is being used and this
475:19 - is really great this last option here
475:21 - because basically you're saying to aws
475:23 - you're saying like i'm just going to pay
475:24 - my bill as usual but i'm going to just
475:26 - tell you what it's going to be and i'm
475:27 - going to save money so if you know that
475:30 - you're going to be using a t2 medium for
475:32 - the next year uh you can do that and
475:34 - you're just going to save money okay so
475:36 - ris can be shared between multiple
475:38 - accounts within an organization and
475:40 - unused rise can be sold in the reserved
475:42 - instance marketplace but we'll talk
475:44 - about the limitations around that when
475:46 - we get a bit deeper in here just to kind
475:47 - of show you what it would look like at
475:48 - the end of this console and they updated
475:50 - it i love this new ui here the idea here
475:53 - is you're going to filter based on your
475:54 - requirements and that's going to show
475:55 - you ris that are available and then
475:57 - you'll just choose the desired quantity
475:59 - you can see the pricing stuff there
476:00 - you're going to add it to cart you're
476:02 - going to check out and that's how you're
476:03 - going to purchase it okay
476:05 - [Music]
476:09 - so another factor to that formula were
476:11 - ri attributes and sometimes the
476:13 - documentation calls them r attributes
476:14 - sometimes they call them instance
476:16 - attributes but these are limited based
476:18 - on class offering and can be
476:20 - uh can affect the final price of the r
476:23 - instance and there are four rh
476:25 - attributes so the first is the instance
476:27 - type so this could be like an m4 large
476:29 - and this is composed of an instance
476:31 - family so the m4 and the instant size so
476:34 - large okay then you have the region so
476:36 - this is where the reserved instance is
476:38 - purchased then you have the tendency
476:40 - whether your instance runs on shared so
476:42 - the default which would be multi-tenant
476:45 - or a single tenant which would be
476:46 - dedicated hardware and then you have the
476:48 - platform whether you're using windows or
476:50 - linux even if you're using on-demand of
476:52 - course this would just affect your
476:53 - pricing but there are some limitations
476:55 - around here which we'll get into as we
476:57 - dive a bit deeper here with our eye okay
476:59 - [Music]
477:04 - all right let's compare regional and
477:06 - zonal ri so when you purchase an ri you
477:08 - have to determine the scope
477:10 - for it okay so this is not gonna affect
477:12 - your price but it's gonna affect the
477:14 - flexibility of the instance uh so this
477:16 - is something you have to decide so we're
477:18 - gonna talk about regional ri which is
477:19 - when you purchase it for a regional and
477:21 - zonal ri when you purchase it for an
477:23 - availability zone so when you purchase
477:25 - it for a regional ri
477:27 - it does not reserve capacity meaning
477:29 - that there's no guarantee that those
477:30 - servers will be available so if anybody
477:32 - runs out of those servers uh you're just
477:34 - not going to have them but when it's
477:36 - zonal uh you are reserving capacity so
477:38 - there's a guarantee that those will be
477:40 - there when you need them
477:42 - in terms of az flexibility
477:45 - you can use the regional ri for any az
477:48 - within that region but for the zonal ri
477:50 - you can only use it for that particular
477:52 - region we're talking about instance
477:54 - flexibility
477:56 - you can apply the discount to uh any
477:59 - instance in the family regardless of the
478:01 - size uh but then when we're looking at a
478:03 - z there is no instance flexibility okay
478:05 - so you're just going to use it for
478:06 - exactly what you defined you can queue
478:09 - purchases for regional ri you cannot
478:11 - queue purchases for zonal ri so there
478:13 - you go
478:15 - [Music]
478:19 - let's talk about some ra limits here so
478:21 - there's a limit to the number of
478:22 - reserved instances that you can purchase
478:24 - per month and so the idea here is that
478:26 - you can purchase 20 regional reserve
478:28 - instances per region and then 20 zonal
478:32 - reserve instances per az so if you have
478:34 - a region that has three az's you can
478:37 - have uh 60
478:39 - zonal reserved instances in that region
478:41 - okay there are some other limitations
478:43 - here so for regional limits you cannot
478:45 - exceed the running on demand instance
478:48 - limit by purchasing regional reserve
478:49 - instances the default for on-demand
478:51 - limit is 20 so before purchasing your ri
478:55 - ensure on-demand limit is equal to or
478:57 - greater than your ri you intend to
478:59 - purchase you might even want to open up
479:01 - a service limit increase just to make
479:03 - sure you don't hit that wall for zonal
479:06 - limits you can exceed your running on
479:08 - demand instance limit by purchasing
479:10 - zonal reserve instances if you're
479:11 - already uh have 20 on-demand instances
479:14 - and you purchase 20 zone reserved
479:16 - instances you can launch a further 20
479:18 - on-demand instances that match the
479:20 - specification of your zonal reserved
479:21 - instances so there you go
479:23 - [Music]
479:28 - let's talk about capacity reservation so
479:30 - ec2 instances are backed by different
479:32 - kinds of hardware and so there is a
479:34 - finite amount of servers available
479:36 - within an availability zone per instance
479:38 - type of family remember an availability
479:39 - zone is just a data center or a
479:41 - collection of data centers and they only
479:43 - have so many servers in there so if they
479:45 - run out because the demand is too great
479:47 - you just cannot spin anything up and so
479:49 - that's what's happening you go to launch
479:50 - specific ec2 instant type but abs is
479:52 - like sorry we don't have any right now
479:54 - and so the solution to that is capacity
479:56 - reservation so it is a service of ec2
479:59 - that allows you to request
480:01 - a reserve of vcc instance type for a
480:03 - specific region and a z so here you
480:06 - would see that you just select the
480:07 - instance type platform a z tendency the
480:10 - quantity and then here you might
480:12 - manually do it specify time
480:14 - or you might say okay i can't get
480:16 - exactly what i want but can give me
480:17 - something generally around that kind of
480:19 - stuff or that type that i want so the
480:22 - reserve capacity is charged at the
480:23 - selected instance type on demand rate
480:25 - whether an instance is running in it or
480:27 - not and you can also use regional
480:29 - reserve instances with your capacity
480:31 - reservations to benefit from billing
480:34 - discounts so there you go
480:36 - [Music]
480:40 - so there are some key differences
480:42 - between standard and convertible ri so
480:44 - let's take a look at it here so the
480:46 - first is that with standard ri you can
480:48 - modify your attributes so you can change
480:50 - the az within the same region you can
480:52 - change the scope from a zonal ri to
480:55 - original ri or vice versa you can change
480:57 - the instant size
480:59 - as long as it's a linux and it has the
481:01 - default tendency you can change the
481:03 - network from ec2 classic to vpc and vice
481:05 - versa but where you're looking
481:07 - convertible you you don't modify ri
481:09 - tributes you perform in exchange okay
481:12 - and so standard rise cannot do exchanges
481:14 - where convertible ri you can uh exchange
481:18 - during the term for another convertible
481:20 - ri with new ra attributes and this
481:21 - includes the instance family instant
481:23 - type platform scope and tenancy um in
481:28 - terms of the marketplace you ca they can
481:30 - be bought in standard ri uh in the
481:32 - marketplace or you can sell your ri if
481:34 - you uh don't need them anymore but for
481:37 - convertible ri they cannot be sold or
481:39 - bought in the marketplace you're just
481:40 - dealing with aws directly okay
481:43 - [Music]
481:47 - hey this is andrew brown from exam pro
481:49 - and we are taking a look at the reserved
481:51 - instance marketplace we had mentioned a
481:53 - prior so let's give it a little more
481:54 - attention here so it allows you to sell
481:56 - your unused standard ri to recoup your
481:58 - spend for alright you do not intend or
482:01 - cannot use so reserved instances can be
482:04 - sold after they have been active for at
482:05 - least 30 days and once database has
482:07 - received the upfront payment you must
482:09 - have a u.s bank account to sell ri on
482:11 - the ra marketplace there must be at
482:13 - least one month remaining in the term
482:15 - for the ri you are listing you will
482:17 - retain the pricing and capacity benefit
482:19 - of your reservation until sold and the
482:21 - transaction is complete your company
482:23 - name and address upon request will be
482:25 - shared with the buyer for tax purposes
482:28 - a seller can set only the upfront price
482:30 - of an ri the usage price and other
482:32 - configurations such as instance type
482:34 - availability zone platform will remain
482:36 - the same as when the ri was initially
482:38 - purchased the term length will be
482:40 - rounded down to the nearest month for
482:41 - example a reservation with 9 months and
482:43 - 15 days remaining will appear as 9
482:45 - months on the rm market you can sell up
482:48 - to 20 000 usd in reserved instances per
482:51 - year if you need to sell more ri
482:53 - reserved instances in the govcloud uh
482:56 - region cannot be sold on the ra
482:58 - marketplace so there you go
483:02 - [Music]
483:04 - hey it's andrew brown from exam pro and
483:05 - we are taking a look at spot instances
483:07 - so a bus has unused compute capacity
483:10 - that they want to maximize the utility
483:12 - of their idle servers all right so the
483:15 - idea is just like when a hotel offers
483:17 - booking discounts to fill vacant suites
483:19 - or planes offer discounts to fill a
483:21 - vacant seats all right so spot instances
483:24 - provide a discount of 90 compared to
483:26 - on-demand pricing spot instances can be
483:29 - terminated if the computing capacity is
483:31 - needed by other on-demand customers but
483:33 - from what i hear rarely rarely does spot
483:36 - instances ever get terminated
483:38 - it's designed for applications that have
483:39 - flexible start and end times or
483:41 - applications that are only feasible at
483:43 - very low compute costs so you see some
483:45 - options here like load balancing
483:46 - workloads flexible workloads big data
483:48 - workloads things like that um there is
483:51 - another service called ada's batch which
483:53 - is for doing batch processing and this
483:54 - is very common what you use spot with
483:57 - and so you know if you find the spot
483:59 - interface too complicated you're doing
484:00 - batch processing you want to use this
484:01 - service instead um there are some
484:04 - termination conditions so instances can
484:06 - be terminated by aws at any time if your
484:08 - instance is terminated by a bus you
484:10 - don't get charged for a partial hour of
484:12 - usage if you terminate an instance you
484:14 - will be still charged for an hour that
484:17 - it ran so there you go
484:19 - [Music]
484:24 - hey this is andrew brown from exam pro
484:25 - and we are taking a look here at
484:27 - dedicated instances so dedicated
484:29 - instances is designed to help meet
484:31 - regulatory requirements innovas also has
484:33 - this concept called dedicated hosts and
484:35 - this is more for when you have strict
484:37 - server-bound licensing that won't
484:38 - support multi-tenancy or cloud
484:40 - deployments and we'll definitely
484:41 - distinguish that in this course but just
484:43 - not in this slide in particular um and
484:45 - so to understand uh dedicated instances
484:48 - or hosts we need to understand the
484:49 - difference between multi-tenancy and
484:50 - single tendency so multi-tenancy you can
484:53 - think of it like everyone living in the
484:54 - same apartment and single tendency you
484:56 - can think of it everyone having their
484:58 - own house so the idea here is that you
485:00 - have a server i'm just going to get my
485:02 - cursor or my pen out here to say server
485:05 - and you have multiple customers running
485:06 - workloads on the same hardware and the
485:09 - idea is that they are separated via
485:11 - virtual isolization so they're using the
485:12 - same server but it's just software that
485:14 - might be separating them okay
485:17 - and then we have the idea of single
485:18 - tenancy so we have a single customer
485:21 - that has dedicated hardware so the
485:23 - physical location is what separates
485:25 - customers
485:26 - um and the idea here is that dedicate
485:28 - can be offered via on-demand reserved
485:31 - and spot so that's what we're talking
485:33 - about dedicated here in the pricing
485:34 - model just so you know that you know
485:36 - even though these are a lot more
485:37 - expensive than on-demand uh you can
485:39 - still save by using reserve and also
485:41 - spot which i was very surprised about
485:43 - um
485:44 - and when you want to choose dedicated
485:46 - you're just going to launch your ec2 and
485:48 - you'll have a drop down where you have
485:49 - that shared so that's the default
485:51 - dedicated so you have dedicated instance
485:53 - and dedicated hosts and again we'll talk
485:54 - about dedicated hosts later when we need
485:57 - to here um and so again the reason why
486:00 - um you know enterprises or large
486:02 - organizations may want to use dedicated
486:04 - instances is because they have a
486:06 - security concern or obligation about
486:09 - against sharing the same hardware with
486:11 - other aws customers okay
486:16 - [Music]
486:18 - hey this is andrew brown from exam pro
486:20 - and we are taking a look at ava savings
486:21 - plans and this is similar to reserved
486:23 - instances but simplifies the purchasing
486:25 - process so it's going to look a lot like
486:27 - all right at the start here but i'll
486:29 - tell you how it's a bit different okay
486:30 - so there are three types of saving plans
486:32 - you have compute savings plan ec2
486:34 - instance saving plans and sage maker
486:36 - saving plans uh and so you just go ahead
486:39 - and choose that you can choose two
486:40 - different terms so one year three year
486:42 - so it'd be simple as that and then you
486:44 - choose the following payment options so
486:46 - you have all upfront partial payment and
486:48 - no upfront and then you're going to
486:49 - choose that hour of the commitment
486:51 - you're not having to think about
486:52 - standard versus convertible uh regional
486:55 - versus zonal ri attributes it's a lot
486:59 - simpler uh let's just talk about the
487:01 - three different saving plans or types in
487:03 - a bit more detail so you have compute so
487:05 - compute savings plans provides the most
487:07 - flexibility and helps to reduce your
487:09 - cost by 66 percent these plans
487:11 - automatically apply to ec2 instances
487:13 - usage aws fargate abuse lambda service
487:16 - uses regardless of the instance family
487:18 - size az region os or tenancy then you
487:21 - have ec2 instances so this provides the
487:23 - lowest prices offering saving up to 72
487:25 - percent in exchange for commitment to
487:27 - usage of instance uh individual instance
487:30 - families in a region so automatically
487:32 - reduce uh your costs on the selected
487:34 - instance family in the region regardless
487:36 - of az size os tenancy gives you the
487:38 - flexibility to change your usage between
487:41 - instances with a within a family in that
487:43 - region and the last is sagemaker so
487:45 - helps you reduce stage maker costs by up
487:47 - to 64 percent automatically apply to
487:50 - stage maker usage regardless of instance
487:52 - family size component aws region if you
487:55 - don't know what sagemaker is that's
487:56 - aws's ml service and it uses ec2
487:59 - instances or specifically ml ec2
488:02 - instances so everything's basically
488:03 - using ec2 here but there you go
488:09 - [Music]
488:10 - all right let's take a look at the xero
488:12 - truss model and the zero trust model is
488:14 - a security uh model which operates on
488:16 - the principle of trust no one and verify
488:18 - everything so what i mean by that is
488:20 - malicious actors being able to bypass
488:22 - conventional access controls
488:24 - demonstrates traditional security
488:25 - measures are no longer sufficient and
488:28 - that's where the zero trust model comes
488:29 - into play so with the zero trust model
488:32 - identity becomes the primary security
488:34 - perimeter
488:36 - and so you might be asking what do we
488:37 - mean by primary security perimeter the
488:39 - primary or new security perimeter
488:40 - defines the first line of defense and
488:43 - its security controls that protect a
488:45 - company's cloud resources and assets
488:48 - um if this still doesn't make sense we
488:49 - do cover a part of the defense in depth
488:52 - where you see the layers of defense from
488:55 - data all the way to physical and so you
488:57 - can kind of see you know what we're
488:58 - talking about in that model there
489:01 - but the old way that we used to do
489:02 - things is network-centric so we had
489:04 - traditional security focused on
489:05 - firewalls and vpn since there were few
489:07 - employees or workstations outside the
489:09 - office or they were in a specific remote
489:12 - office so we treated the network uh the
489:14 - network as kind of like the the boundary
489:16 - so if you're in in office there's
489:18 - nothing to worry about but we don't
489:20 - think like that anymore because
489:21 - everything is identity centric so
489:24 - this is where we have bring your own
489:25 - device remote workstations which are
489:27 - becoming more common uh we can't always
489:29 - trust that the employee is in a secure
489:30 - location we have uh identity-based
489:32 - security controls like mfa or providing
489:35 - provisional access based on the level of
489:36 - risk from where when and what a user
489:39 - wants to access and identity centric
489:41 - does not replace uh but it augments
489:43 - network-centric security so it's just an
489:45 - additional layer of consideration for uh
489:48 - security when we're thinking about our
489:50 - database cloud workloads okay
489:52 - [Music]
489:56 - all right so we just loosely defined
489:58 - what the zerotrust model is so let's
489:59 - talk about how we would do zero trust in
490:01 - aws and so zero trust has to do a lot
490:04 - with identity security controls uh so
490:06 - let's talk about what is at our disposal
490:08 - on aws so on database we have identity
490:11 - and access management i am this is where
490:12 - we create our users or groups or
490:14 - policies so time policy is a set of
490:16 - permissions that allow you to say okay
490:18 - this user is allowed to use
490:20 - these services with these particular
490:22 - actions
490:23 - then you have the concept of permission
490:25 - boundaries and so these are saying okay
490:28 - these aren't the permissions the user
490:29 - has currently but these are the
490:30 - boundaries to which we want them to have
490:32 - so they should never have access to
490:34 - um uh ml services and if someone's to
490:38 - apply them uh uh
490:40 - permissions it'll always be within these
490:42 - boundaries then you have service control
490:44 - policies and these are organization-wide
490:46 - policies so if you have a policy where
490:47 - you don't want anyone to run anything in
490:49 - the canada region you can apply that
490:51 - policy at the organizational level and
490:53 - it will be enforced
490:54 - then within an ion policy there are the
490:56 - concept of conditions and so these are
490:59 - all the kind of like uh little knobs you
491:01 - can tweak to say how do i control based
491:04 - on a bunch of different factors so
491:06 - there's source ip so restrict where the
491:08 - ip address is coming from a requested
491:10 - region so a restrict based on the region
491:12 - as we were just mentioned as an example
491:14 - uh multi-factor auth presence so
491:16 - restrict if mfa is turned off uh current
491:20 - time so restrict access based on time of
491:22 - day maybe your employees should never be
491:24 - really using things at night and so that
491:26 - could be an indicator that someone is
491:27 - doing something malicious so you know
491:29 - only give them access during a certain
491:31 - time of day and so that's where we're
491:33 - going to figure out you know based on
491:35 - all these type of control security
491:36 - controls uh to our aws resources we can
491:39 - kind of enforce the zero trust model aws
491:42 - itos does not have a ready-to-use
491:44 - identity controls that are intelligent
491:46 - which is why abuse is considered not to
491:47 - have a zero trust offering for customers
491:49 - and third-party services need to be used
491:51 - so what i'm saying is that technically
491:54 - you know this check box is this thing
491:56 - saying okay we can kind of do zero trust
491:58 - on aws but there's a lot of manual work
492:01 - and
492:02 - you know if i was to say okay
492:06 - i don't want anyone using this at
492:08 - nighttime that doesn't really detect you
492:10 - know what i'm saying it's not going to
492:11 - say oh i think this time is suspicious
492:14 - or malicious so then restrict access
492:16 - only to these core services and anything
492:18 - outside of the services can't be used it
492:20 - just can't exactly do that without a lot
492:22 - of work yourself and that's what i'm
492:23 - talking about here where we have a
492:25 - collection of services that can be set
492:27 - up in an intelligent intelligent ish
492:29 - detection way for identity concerns but
492:31 - requires expert knowledge so the way you
492:33 - might do that aws is that everything all
492:35 - the api calls go through awes cloudtrail
492:38 - and so what you could do is feed those
492:40 - into amazon guard duty and guard duty is
492:42 - an intrusion
492:43 - [Music]
492:44 - intrusion detection and protection
492:46 - system so it could detect suspicious
492:48 - from malicious activity on those
492:49 - cloudtrail logs and you could follow
492:51 - that up with remediation or you could
492:53 - pass that on to amazon detective that
492:55 - could analyze investigate and quickly
492:57 - identify security issues
492:59 - that it could ingest from guard duty but
493:01 - i'm telling you that this stuff here is
493:04 - not as easy
493:05 - for the consumer
493:07 - and so you of course you can do zero
493:09 - trust model but it's going to take a lot
493:10 - of work here and there are some
493:11 - limitations which we'll talk about next
493:13 - here
493:18 - so now let's see how we would do zero
493:19 - trust on a bus with third parties so it
493:21 - was just does technically implement a
493:23 - zero trust model but does not allow for
493:25 - intelligent identity security controls
493:27 - which
493:28 - you know you can do it but it's a lot of
493:30 - work so let's kind of compare it against
493:32 - kind of a third party where we would get
493:34 - the controls that we would not
493:36 - necessarily get with aws so for example
493:38 - azure active directory has a real-time
493:40 - and calculated risk detection based on
493:41 - data points than aws and this is based
493:44 - on device and application time of day
493:47 - location whether mfa is turned on what
493:49 - is being accessed and the security
493:51 - controls verification or logic
493:53 - restriction is much more robust so you
493:56 - know just as one particular example like
493:58 - device and application is not something
494:00 - that aws factors in uh with the
494:03 - existing controls or at least not in a
494:05 - way that is consumer friendly and you
494:07 - know i can't say on a bus okay when you
494:10 - think that this is the type of threat
494:12 - only allow them access to these things
494:13 - or if you think they're in a risky area
494:15 - or risky uh location only give them
494:18 - access to you know these things or where
494:20 - there's not sensitive data you can't
494:21 - exactly do that in a database very
494:23 - easily and so this is where third-party
494:24 - solutions are going to come into play so
494:26 - you have azure active directory google
494:28 - beyond corp jump cloud and all these
494:30 - have more intelligent security controls
494:32 - for real-time detection um and so the
494:34 - way you would use these is these would
494:36 - be your primary directories uh for
494:38 - google beyond corp is just a zero trust
494:40 - framework so i guess you'd use
494:43 - google's uh cloud directory but the idea
494:46 - anyway here is that you use single
494:47 - sign-on to connect those directories to
494:50 - your aws account and that's how you'd
494:52 - access access those uh aws resources and
494:55 - you get this more robust functionality
494:56 - okay
494:58 - [Music]
495:01 - hey it's andrew brown from exam pro and
495:03 - we're looking at identity now we need to
495:05 - know a bunch of concepts before we talk
495:07 - about identity on aws so let's jump into
495:09 - it the first is a directory service so
495:11 - what is directory service well it's a
495:13 - service that maps the names of network
495:15 - resources to network addresses and the
495:18 - directory services shared infrastructure
495:20 - or information
495:21 - infrastructure for locating managing
495:23 - administrating and organizing resources
495:25 - such as volumes folders files printers
495:28 - users groups devices telephone numbers
495:31 - and other objects a directory service is
495:33 - a critical component of a network
495:35 - operating system and a directory server
495:37 - or a name server is a server which
495:40 - provides a directory service so each
495:42 - resource on the network is considered an
495:44 - object by the directory server
495:46 - information about a particular resource
495:48 - is stored as a collection of attributes
495:50 - associated with that resource or object
495:53 - well-known directory services would be a
495:56 - domain name service
495:58 - so the directory service for the
495:59 - internet microsoft active directory and
496:03 - they have a
496:04 - cloud hosted one called azure active
496:06 - directory we have apache directory
496:08 - service oracle internet directory so oid
496:13 - uh open ldap uh cloud and identity and
496:16 - jump cloud okay
496:22 - hey this is andrew brown from exam pro
496:24 - and we're taking a look at active
496:25 - directory now you might say well we're
496:26 - doing a bus why are we looking at this
496:28 - well no matter what cloud provider
496:30 - you're using you should know what active
496:32 - directory is
496:33 - especially when it comes to identity
496:34 - because you can use it with aws
496:36 - so let's talk about it so microsoft
496:39 - introduced active directory domain
496:40 - services in windows 2000 to give
496:42 - organizations the ability to manage
496:44 - multiple on-premise infrastructure
496:45 - components and systems using a single
496:47 - identity per user and since then it's uh
496:50 - involved evolved obviously it's running
496:53 - beyond windows 2000 as of today and they
496:56 - even have a managed one called azure ad
496:58 - which is on microsoft azure but just to
497:01 - kind of give you an architectural
497:02 - diagram here the idea is that you would
497:04 - have your domain servers here
497:06 - and they might have child domains and
497:07 - the idea is that you have these running
497:09 - on multiple machines so that you have
497:11 - redundant ability to log in from various
497:13 - places when you have a bunch of domains
497:15 - it's called a forest and then within a
497:17 - domain you actually have organizational
497:19 - units and with them within
497:20 - organizational units you have all your
497:21 - objects like your users your printers
497:23 - your computers your servers
497:25 - all things like that okay
497:29 - [Music]
497:31 - hey it's andrew brown from exam pro and
497:32 - we're talking about identity providers
497:34 - or ipds so
498:03 - so
498:13 - hey this is andrew brown from exam pro
498:15 - and we are talking about identity
498:16 - providers also known as idps
498:20 - so an identity provider is a system
498:22 - entity that creates maintains and
498:23 - manages identity information for
498:25 - principles and also provides
498:26 - authentication services to applications
498:29 - with a federation or distributor network
498:31 - a trusted provider of your user identity
498:33 - that lets you use authent lets you
498:35 - authenticate to access other service
498:37 - identity providers so this could be like
498:38 - facebook amazon google twitter github
498:41 - linkedin
498:42 - uh federated identity is a method of
498:44 - linking a user's identity across
498:46 - multiple separate identity management
498:47 - systems and so some things that we can
498:50 - use for that is like open id so this is
498:52 - an open standard and decentralized
498:54 - authentication protocol allows you to be
498:56 - able to log in to different social media
498:58 - platforms using google or facebook
499:00 - account open ideas about providing who
499:02 - you are then we have oauth 2.0 this is
499:05 - an industry standard protocol for
499:06 - authorization oauth doesn't share
499:08 - password data but instead uses
499:11 - authorization tokens to prove an
499:13 - identity between consumers and service
499:15 - providers oauth is about granting access
499:17 - to functionality and then we have saml
499:20 - so security assertion markup language
499:23 - which is an open standard for exchanging
499:25 - authentication and authorization between
499:26 - an identity provider and a service
499:29 - provider and this is important to use
499:30 - for saml which we use for single sign-on
499:33 - via the web browser okay
499:36 - [Music]
499:41 - hey this is andrew brown from exam pro
499:43 - we're looking at the concept of single
499:44 - sign-on so sso is an authentication
499:46 - scheme that allows the user to log in
499:48 - with a single id and password to
499:50 - different systems and software sso
499:52 - allows it departments to administer a
499:53 - single identity that can access many
499:55 - machines and cloud services so the idea
499:57 - is you have azure active directory this
499:59 - is just an example of a very popular one
500:01 - you'd use saml to do sso you can connect
500:03 - to all things slackly the best google
500:05 - workspaces salesforce or your computer
500:08 - uh the idea here is uh once you
500:10 - log in
500:12 - you don't have to log in multiple times
500:14 - so you log into your primary directory
500:15 - and then after that you're not going to
500:16 - be presented with a login screen some
500:18 - services might show an intermediate
500:20 - screen but the idea is you're not
500:21 - entering your credentials in multiple
500:23 - times so it's seamless
500:25 - [Music]
500:28 - all right let's talk about ldap so
500:31 - lightweight directory access protocol is
500:33 - an open vendor neutral industry standard
500:35 - application protocol for accessing and
500:36 - maintaining distributed directory
500:38 - information services over ip network so
500:42 - a common use of ldap is to provide a
500:45 - central place to store usernames and
500:47 - passwords ldap enables for same sign-on
500:50 - so same sign-on allows users to
500:52 - use a single id and password but they
500:54 - have to enter it every single time they
500:56 - want to log in so maybe you have your
500:58 - on-premise active directory and then
501:00 - it's going to store it in that ldap
501:02 - directory and so the idea is that you
501:05 - know all these services like google
501:07 - kubernetes
501:08 - um jenkins is going to deal with that
501:11 - ldap server so why would you use ldap
501:14 - over sso which is more convenient or
501:16 - seamless so most sso systems are using
501:19 - ldap under the hood but ldap was not
501:22 - designed natively to work with web
501:23 - applications so some systems only
501:25 - support integration with ldp and not sso
501:28 - so you got to take what you can get okay
501:30 - [Music]
501:34 - let's take a look here at multi-factor
501:35 - authentication also known as mfa and
501:38 - this is a security control where after
501:39 - you fill in your user's name and email
501:42 - password you have to use a second device
501:44 - such as a phone to confirm that it's you
501:46 - that is logging in so mfa protects
501:48 - against people who have stolen your
501:50 - password mfa is an option in most cloud
501:53 - providers and even social media websites
501:55 - such as facebook so the idea is i have
501:57 - my username or email and password i'm
502:00 - going to try to log in this is the first
502:02 - factor and the second factor or
502:04 - multi-factor is i'm going to use a
502:06 - secondary device so maybe my phone we're
502:08 - going to enter in different codes or
502:09 - maybe it's password list so i just have
502:11 - to press a button to confirm that it's
502:12 - me and then i'll get access so in the
502:15 - context of aws
502:16 - it's strongly recommended that you turn
502:18 - on mfa for all your accounts especially
502:20 - the aws root account
502:22 - we'll see that when we do the follow
502:24 - alongs
502:25 - [Music]
502:29 - let's take a look at security keys so a
502:31 - security key is a second device used as
502:33 - a second step in authentication process
502:35 - to gain access to a device workstation
502:37 - or application a security key can
502:39 - resemble a memory stick and when your
502:41 - finger makes contact with a button of
502:42 - exposed metal on the device it will
502:44 - generate and autofill a security token a
502:46 - popular brand of security keys is the
502:48 - ubi key and this is the one i use and is
502:50 - looks exactly like the one that's right
502:52 - beside my desk it works out of the box
502:54 - with gmail facebook and hundreds more
502:56 - supports fido 2 web auth n uh u2f it's
503:01 - waterproof and crest resistance it has
503:03 - variations like usb a usb
503:06 - nfc dual connectors on a single key can
503:09 - do a variety of things so when you turn
503:11 - on mfa on your aws account you'll have
503:13 - virtual mfa device so that's when you're
503:15 - using something like a phone or using
503:17 - software on your phone to do that then
503:19 - there's the u2f security key so this is
503:22 - what we're talking about right now and
503:23 - there's even other kinds of hardware mfa
503:25 - devices
503:27 - which we're not really going to talk
503:28 - about but you know just security keys
503:31 - tie into mfa and this is a lot better
503:33 - way than using a phone because you know
503:36 - you can have it on your desk and press
503:37 - it um and you know you have to worry
503:38 - about your phone being not charged okay
503:41 - [Music]
503:45 - hey this is andrew brown from exam pro
503:47 - and we are taking a look at aws identity
503:49 - and access management also known as iem
503:51 - and you can use this service to create
503:53 - and manage database users groups uh use
503:55 - permissions to allow and deny their
503:57 - access to adab's resources so there's
504:00 - quite a few components here let's get to
504:01 - it so the first is i am policies so
504:04 - these are json documents which grant
504:05 - permissions for specific users groups or
504:08 - a role to access services and policies
504:10 - are attached to im identities then you
504:12 - have impermissions or permission and
504:15 - this is an api action that can or cannot
504:17 - be performed and they're represented in
504:20 - the i am policy document then there's
504:22 - the i am identity so we have i am users
504:25 - these are end users who log into the
504:27 - console or interact with aws resources
504:28 - pragmatically or via clicking ui
504:30 - interfaces you have im groups so these
504:33 - these group up your users so they all
504:36 - share the same permission levels so that
504:38 - maybe its admins developers or auditors
504:40 - then you have i am roles so these roles
504:42 - grant endless resources permissions to
504:44 - specific database api actions and
504:46 - associate policies to a role and then
504:48 - assign it to an aws resource so just
504:50 - understand that
504:51 - roles are when you're attaching these to
504:54 - resources so like if you have an ec2
504:56 - instance and you say it has to access s3
504:58 - you're going to be attaching a role not
505:00 - a policy directly okay
505:03 - [Music]
505:07 - hey this is andrew brown from exam pro
505:09 - and we are looking at iron policies a
505:11 - little bit closer here and they are
505:13 - written in json and contain the
505:14 - permissions which determine the api
505:16 - actions that are allowed or denied um
505:18 - and rarely do i write these out by hand
505:21 - because they have a little wizard that
505:23 - you can use to write out the code for
505:25 - you but if you want to you absolutely
505:27 - can write it out by hand but we should
505:29 - know the contents of it and how these
505:31 - json files work so the first thing is
505:32 - the version
505:34 - which is the policy language version and
505:36 - it's been 2012 for a very long time i
505:38 - don't see that changing anytime soon if
505:40 - they happen to change uh what or what
505:43 - the structure of the json is then you
505:45 - have the statements and these are for
505:47 - policy elements
505:48 - and you're allowed to have multiples of
505:50 - them so the idea is that this is the
505:52 - policies or permissions we should say
505:55 - that you uh plan on applying then you
505:58 - have the sid this is a way of labeling
505:59 - your statements um this is useful for
506:02 - like visualization or for referencing it
506:04 - for later on but a lot of times you
506:06 - don't have to have a sid then there's
506:08 - the effect it's either allow or deny
506:10 - then you have the action so here we're
506:12 - saying give access to s3 for all actions
506:16 - under it there's another action down
506:18 - below where it's saying give access and
506:21 - get my pen tool out here just to create
506:22 - a service link role so it's a cross
506:24 - account rule there
506:26 - then there's the principal so this is
506:27 - the account user role or federated user
506:29 - to which you would like to allow access
506:32 - or deny so we're specifically saying
506:34 - this user named barkley um in our aws
506:37 - account there
506:38 - uh then there are the resources so the
506:40 - resources to which the action applies um
506:43 - so in this one up here we are specifying
506:45 - a specific aws bucket here we're seeing
506:47 - all possible resources in enables
506:49 - account and then the condition so
506:51 - there's all sorts of different kinds of
506:53 - conditions so this is a string like one
506:54 - it's saying look at the service name and
506:56 - if it starts with this or that then
506:58 - they'll have access to that so this
506:59 - person even though it says all resources
507:01 - they're really only going to have access
507:02 - to rds okay
507:04 - [Music]
507:08 - so in this follow along we're going to
507:09 - take a closer look at im policy so go to
507:12 - the top and type in iam
507:14 - and what we'll do is make our way over
507:16 - here
507:17 - all the way over to policies and what i
507:19 - want to do is create a new policy that
507:21 - only has access to
507:23 - limited resources so
507:26 - let's say we want to create an amazon
507:28 - ec2 instance and that ec2 instance has
507:31 - access to a very particular s3 bucket
507:34 - so what i want you to do is make your
507:35 - way over to s3 and we're going to create
507:37 - ourselves a new bucket
507:41 - and i'm going to go ahead and create a
507:42 - bucket here
507:43 - we're going to call this um
507:46 - policy tutorial
507:49 - and i'm going to just put a bunch of
507:50 - numbers here
507:52 - you'll have to randomize it for your use
507:53 - case
507:55 - and so now that we have our bucket what
507:57 - we're going to do is go ahead and create
507:58 - a policy
508:01 - and the policy is going to choose a
508:04 - service we're going to say s3 and what i
508:06 - want to do is only be able to list out
508:08 - actions i'm going to expand this so i
508:10 - don't want everything so we're just
508:11 - going to say list buckets
508:13 - okay
508:14 - and then what we'll do is
508:16 - uh expand this here and i want to say
508:18 - for a particular bucket
508:20 - so we'll go back over here click into
508:22 - our bucket
508:24 - and
508:25 - we're going to go ahead and set those
508:27 - permissions
508:29 - by finding that iron
508:32 - we're going to paste that
508:34 - we're going to paste that iron up there
508:35 - sometimes it's a bit tricky it vanishes
508:37 - on you
508:38 - and we could set other conditions if we
508:40 - wanted to but this is pretty simple as
508:42 - it is
508:44 - and so that's our rule here right so
508:46 - we're saying
508:47 - this policy allows us to list this
508:48 - bucket for that okay
508:51 - so what we'll do is go ahead and hit
508:52 - next
508:53 - we'll hit review and we'll just say my
508:57 - bucket policy
509:00 - and we'll create that policy
509:04 - okay so
509:06 - there's a few other things i think that
509:08 - i'd like to do with this policy i'm
509:09 - going to pull it back up here so
509:11 - if we want to find it uh you used to be
509:13 - able to filter these based on the ones
509:15 - that you created
509:16 - but um
509:20 - yeah they should like the little icon so
509:21 - these are ones that i've created up here
509:24 - and so there's my bucket policy
509:27 - and i feel like i want to update this
509:29 - policy to have a
509:32 - bit of extra information here so i'm
509:33 - going to go edit this policy
509:36 - no you know what i think this is fine so
509:38 - what i want to do is now create a role
509:43 - and we're going to create a new role and
509:44 - i'm going to call this um
509:47 - well before i do i need to choose what
509:48 - it's for so it's going to be for ec2 so
509:50 - we're going to go ahead and hit next
509:52 - we're going to choose our policy so my
509:53 - bucket policy there it is
509:56 - and i want to add another one here
509:57 - because i want to be able to use
509:58 - sessions manager because i really don't
510:00 - want to use an ssh key to
510:02 - check that this works
510:04 - and
510:04 - so um for this
510:07 - i i need
510:09 - to use ssm so i'm going to type in ssm
510:11 - here
510:13 - and i'm going to just make sure this is
510:14 - the new one so this policy will soon be
510:16 - deprecated use amazon ssn managed core
510:19 - instance should always open these up and
510:20 - read them and see what they do
510:22 - and so that's the one that's going to
510:23 - allow us to access simpson's manager so
510:26 - we can use sessions manager okay
510:28 - and so we're going to say my ec2 roll4s3
510:34 - and we go ahead and create ourselves a
510:36 - roll
510:39 - and so now that we have our role i'm
510:41 - going to go over to ec2
510:44 - and i'm going to go ahead and launch
510:45 - myself a new instance
510:48 - we're going to choose amazon linux 2
510:51 - we're going to stick with t2 micro i'm
510:53 - going to go over to configuration here
510:55 - everything is fine here
510:57 - i'm fine with all that storage is fine
511:00 - we'll go to security group and i don't
511:02 - want any ports open because i'm not
511:04 - going to be using ssh
511:07 - we're going to launch this instance i
511:08 - don't even want a key pair
511:11 - okay
511:14 - and then we're going to go over here and
511:16 - so what we're waiting for is this
511:17 - instance to launch as that is going what
511:19 - i want to do is go over to my s3 bucket
511:22 - and i want to place something in this
511:24 - bucket so i do have some files here
511:27 - so what i'm going to do
511:29 - is create a new folder here
511:31 - whoops
511:33 - i'm going to go back and i'm just going
511:34 - to create a folder first create a folder
511:37 - enter prize d
511:43 - and i'm going to click into this and
511:44 - then i'm going to upload all my images
511:46 - here
511:47 - so you'll have to find your own images
511:49 - off the internet this is just the ones i
511:50 - have
511:52 - and we'll go ahead and upload those
511:55 - give that a moment
512:01 - okay and so we don't have access to read
512:03 - those files we'll adjust our policy as
512:06 - we go so that we can do that okay
512:09 - so this instance should be running um it
512:12 - has doesn't have the two status checks
512:13 - passed we should be able to uh connect
512:15 - to it so click on connect here and so we
512:17 - have options like ec2 instance connect
512:19 - sessions manager ssh client i want you
512:21 - to go to sessions manager
512:22 - it says we weren't able to connect your
512:24 - instance common reasons ssm agent was
512:26 - installed we absolutely have that
512:27 - installed
512:28 - the required item profile oh right so we
512:31 - were supposed to attach
512:33 - i forgot to do we were supposed to
512:34 - attach an iron profile right
512:36 - so an iron profile is the role
512:38 - uh
512:40 - it holds the role uh that's going to
512:42 - give the permissions to that instance
512:44 - and since we didn't add it we got to go
512:46 - retroactively at it after the fact
512:50 - and so i'm going to modify the i am roll
512:53 - and we're going to choose my ec2 roll
512:56 - for s3 and we're going to save that
512:59 - and actually when that happens you have
513:00 - to reboot the machine
513:02 - you only have to do that if you have no
513:04 - roll attached like prior no profile
513:06 - attached and they're attaching it for
513:07 - the first time
513:09 - but after that you never have to reboot
513:10 - the machine this is the only case where
513:11 - you'd have to do that
513:14 - that's why when i launch an ec2 instance
513:15 - i always at least have the ssm role
513:17 - attached the managed one that gets
513:19 - sessions manager so that i don't ever
513:20 - have to do a reboot in case i have to
513:22 - update the policy
513:25 - and so we will give that a moment there
513:29 - it says initializing so i'm going to try
513:31 - again to connect to it okay
513:37 - and we still don't have that option
513:38 - there um so i'm going to go back to my
513:40 - instances
513:42 - i'm going to check to see if the role
513:43 - the role or policy is attached
513:46 - or profile i should say
513:51 - so i'm just looking for it here
513:56 - there it is
513:58 - and so if i click into this
514:00 - into the role
514:01 - we can see that we have the amazon ssn
514:04 - managed instance core there so that's
514:06 - set up
514:07 - and the my
514:08 - bucket policy
514:10 - so this has everything
514:12 - that it should be able to do it no
514:14 - problem
514:18 - okay so i'm going to try that again
514:21 - okay so now the connection shows up aws
514:23 - is finicky like that you just have to
514:25 - have confidence in knowing what you're
514:27 - doing is correct okay
514:28 - we'll go ahead and hit connect
514:32 - i didn't have to use ssh keys or
514:33 - anything and this is a lot more secure
514:35 - way to connect your instances when it
514:37 - logs us in it's going to set us as the
514:39 - ssm user but we want to be
514:41 - the
514:43 - ec2 user
514:44 - okay
514:46 - that's uh aws always makes their uh am
514:49 - like their linux versions as the ec2
514:51 - user and that's what you're supposed to
514:52 - use
514:53 - but it's just
514:54 - you just that's how you have to get to
514:55 - that you have to type that sudo su
514:57 - hyphen ec2 user okay just once
515:01 - and if you type who am i that's who you
515:02 - are if you type exit you'll go back to
515:04 - that user so if i type exit and i type
515:06 - who am i and now this person so i'm
515:08 - going to go back hit up go back in there
515:10 - type clear
515:11 - so now i want to see if i have access to
515:14 - s3 so i have to do abs s3 ls
515:16 - let's see if i can list buckets
515:19 - it says axis denied
515:21 - so
515:23 - i mean that kind of makes sense because
515:25 - if you have list buckets and we're just
515:26 - saying only that bucket that might not
515:28 - make a whole lot of sense
515:30 - so i'm going to go back to my policy i
515:32 - might just written a crummy policy but
515:35 - we'll say i am here if we have that one
515:37 - open we should just go here
515:39 - and click
515:41 - on this policy here
515:45 - i'm going to edit that policy
515:47 - so what i'm going to do is i'm just
515:49 - going to change it i'm going to say all
515:50 - resources review the policy save changes
515:53 - and we'll see how fast that propagates
515:57 - okay
516:06 - because i'm pretty sure i don't have to
516:07 - do anything here it should just now give
516:09 - me full access to s3
516:12 - i'm just going to keep on hitting up
516:13 - here
516:15 - so i'm going to do is i'm just going to
516:16 - take like a three four minute break
516:18 - gonna get a drink i'm gonna come back
516:19 - here and see if this propagates i'm
516:21 - pretty sure i don't have to do anything
516:23 - for that to propagate
516:24 - and i think that i've attached
516:26 - everything correctly here okay
516:30 - okay so i haven't had much luck here
516:32 - it's still having the same issue so if
516:34 - that is happening what i'm going to do
516:37 - is i'm just going to reboot it because
516:38 - maybe i didn't give it a good
516:40 - opportunity to reboot there again i
516:42 - don't think we should have to reboot it
516:43 - every time we're changing um
516:46 - things there but we will give it another
516:48 - go here
516:49 - and see if that fixes that problem there
516:52 - so those sessions matter is going to
516:53 - time out here which is totally fine
516:57 - it's going to kill that session there
517:00 - and so what we'll have to do is close
517:02 - this out because there's not much we can
517:03 - do with that
517:05 - and we're going to go ahead and go back
517:06 - to connect and so we're waiting for this
517:08 - button to appear because it is rebooting
517:10 - so
517:12 - if we want to monitor that stuff usually
517:14 - there is an option
517:15 - here to monitor where it will show us
517:18 - the system logs of what it's doing
517:21 - so here it's just like restarting the
517:22 - machine
517:32 - i'm not sure if we expect to see
517:34 - something after this
517:36 - so i can click that there
517:42 - and uh yeah it's so easy to get turned
517:44 - around so i can connect to it again now
517:50 - we'll type in sudo su hyphen ec2 user
517:53 - aws s3 ls
517:57 - and we still have
518:00 - access deny for list buckets so if
518:02 - that's the case it could be that um
518:05 - sometimes you need other permissions
518:07 - when doing list policy like list buckets
518:10 - so if that's the case we're going to do
518:12 - a sanity check i'm just going to say all
518:13 - permissions here okay
518:15 - and this way there's no way that i've
518:16 - set this incorrectly
518:18 - um it just has to work now so type this
518:20 - in
518:23 - there we go okay so there has to be
518:25 - something more to it so just because you
518:26 - say list
518:27 - buckets you know like means there must
518:30 - be more to it right so if i go here to
518:31 - this
518:33 - right and i say whoops
518:36 - and i say uh list buckets here we'll say
518:38 - copy
518:40 - paste okay
518:49 - here it's saying maybe i need get object
518:51 - as well so
518:53 - i just know from using it about a long
518:56 - time that that's the case that it could
518:57 - be more than one thing so you know that
518:59 - was in the back of my mind that that
519:00 - could be happening and i guess that is
519:02 - but notice i didn't have to restart my
519:04 - uh my server boot my server to get those
519:06 - to work
519:07 - um
519:08 - but anyway let's go lock that down and
519:10 - see if we can just kind of make this uh
519:12 - more focused so let's say
519:15 - all resources i'm going to specify
519:18 - the condition
519:21 - so i might want to just say for
519:22 - particular buckets
519:25 - so we'll say specific
519:27 - when you checkbox everything then you
519:28 - have to do this so for storage accounts
519:30 - these are fine any
519:32 - for objects
519:36 - that could be something
519:38 - we'll say
519:39 - multi-region access bucket
519:41 - any bucket but what i'm going to say is
519:43 - i want to only allow them to access
519:44 - things in a particular bucket
519:47 - and so if i go to arn here
519:50 - um what is our bucket name
519:58 - our bucket name is policytutorial3414
520:03 - whatever
520:04 - right
520:05 - and so we can actually give it a wild
520:08 - card or we can say enterprise
520:12 - d
520:14 - and we learned this in the course that
520:15 - you can provide orange with randomized
520:19 - things there i don't know if i spelt it
520:20 - wrong over here so i should really
520:21 - double check
520:23 - i should probably just copy it
520:30 - oops
520:32 - i still want to type it wrong and so
520:34 - this
520:36 - okay
520:38 - means that we should only be able to get
520:40 - stuff from there i'm going to review the
520:41 - policy let's see if it takes save the
520:43 - changes
520:46 - and if i just view the json here
520:50 - notice it says anything from here right
520:53 - so allow s3 anything as long as it's
520:56 - within here and then it also broke it up
520:58 - into sub 1's 4 here okay
521:01 - so anyway what i want to see is what
521:03 - happens
521:04 - if i upload something into the loose
521:06 - area here so i'm going to say upload
521:09 - and i'm going to say add a file
521:13 - and we're just going to grab data here
521:15 - and upload
521:16 - it go back to our bucket
521:19 - there's our file we have that stuff in
521:21 - there and so if i go back over to my ec2
521:23 - instance which i'm still connected to
521:26 - uh who am i
521:27 - okay great
521:28 - clear
521:29 - so i'm going to say aws s3 ls see if
521:32 - that works still it does good and so
521:35 - what i want to do is see if i can copy a
521:36 - file locally so i'm going to do a bus s3
521:40 - copy
521:42 - i think it was s3 8 no it's just s3
521:44 - copy polis uh s3 forward slash forward
521:48 - slash
521:49 - policy
521:51 - tutorial
521:53 - three four
521:56 - one four one whoops three four
521:59 - tutorial
522:00 - hyphen
522:03 - three four one four one four slash
522:07 - enterprise d
522:10 - data dot jpg i think it's a jpg let's go
522:13 - double check
522:15 - yeah it is okay
522:16 - and then i just want to say data.jpg
522:20 - and it downloaded it right
522:22 - so i'm going to remove that one and so
522:24 - now what i'm going to do is i'm just
522:25 - going to see if my policy is working or
522:27 - maybe my permissions aren't exactly what
522:29 - i think they are
522:30 - and i was able to download it so
522:33 - it's these policies can get kind of
522:35 - tricky because like this one says allow
522:37 - all actions for these but then these say
522:39 - all actions and so
522:43 - that makes it hard because i want get
522:45 - object
522:51 - so another thing we can do
522:53 - and if that one doesn't work really well
522:54 - i'm just going to write one by hand
522:57 - it's not that scary to write these by
522:58 - hand you just get used to it so i'm
523:00 - going to say effect
523:03 - um
523:04 - is it disallow
523:06 - or maybe it's deny
523:10 - deny
523:12 - action
523:18 - s3
523:19 - get object
523:20 - i believe that's what it is
523:24 - resource
523:26 - and then i'm going to specify exactly
523:27 - the resource i don't want it to allow so
523:29 - we're going to say arn
523:31 - aws s3 3 colons
523:35 - policy tutorial
523:41 - 34141
523:44 - and just say data.jpg
523:48 - now if this is not valid it's going to
523:49 - complain and say hey you didn't write
523:50 - this right
523:52 - and it and it's fine okay so
523:55 - we'll save those changes
523:58 - and so that should deny access to that
524:01 - right
524:02 - hopefully i got the policy right
524:07 - okay so that one doesn't work which is
524:09 - fine
524:11 - and that one's fine so that worked we
524:13 - were able to deny that but you can see
524:15 - there's a little bit of an art to
524:16 - creating these policies
524:18 - as you make more of them it becomes a
524:20 - lot easier so hopefully it's not too
524:22 - scary but that's all there really is
524:24 - to it that i want to show you today so
524:26 - what we're going to do is clear out this
524:28 - bucket we're done with this bucket here
524:29 - so we'll say delete whoops we got to
524:31 - empty it first
524:35 - and we'll just say permanently delete
524:36 - here
524:38 - okay
524:40 - and we will exit that out we're gonna go
524:42 - ahead and delete that bucket
524:46 - grab its name here
524:50 - and uh we'll go back over here
524:53 - i think i forgot to delete this bucket
524:55 - from earlier i'm just going to delete
524:56 - that because i don't need that bucket so
524:58 - that's okay with you
524:59 - just going to go ahead and delete that
525:03 - and we have that ec2 instance running so
525:05 - we want to stop that
525:09 - so we go ahead and we're going to
525:11 - terminate that yes please
525:15 - and then we'll go to im and do some
525:16 - cleanup
525:20 - i have some custom rolls i've been
525:21 - creating um you know from prior things a
525:24 - lot of those
525:25 - usually there's a way to uh we've
525:27 - redesigned it okay where's the redesign
525:30 - this is the redesign that can't be it
525:32 - because it'll be like roles that ada
525:33 - best makes i think these are all roles
525:35 - that i've made
525:38 - um
525:39 - i don't want to delete service roles
525:43 - but i want to get rid of some of these
525:44 - because i just have too many you know
525:46 - it's getting out of hand for me
525:49 - and i'm going to just see if it will let
525:50 - me
525:53 - delete
525:57 - all of these let's delete those
526:07 - there we go just clean up a bit i still
526:09 - have a lot here but there's like service
526:11 - roles that aws creates once and you
526:12 - really don't want to delete those
526:14 - because
526:16 - you don't
526:18 - um and then i have a bunch of these like
526:19 - i'm never going to use these so i might
526:21 - as well detach them delete detach
526:27 - you really don't want to keep like rolls
526:29 - that you're never going to use around
526:32 - things like that like gauze we're going
526:34 - to be using that again
526:36 - delete
526:44 - there's that bucket we just created
526:51 - anyway you get the idea so yeah that's
526:53 - uh that's i am okay
526:55 - [Music]
527:00 - principle of least privilege pulp is the
527:02 - computer security concept of providing a
527:04 - user role or application the least
527:06 - amount of permissions to perform an
527:07 - operation
527:08 - or an action and the way we can look at
527:10 - it is that we have just enough access so
527:12 - jea permitting only the exact actions
527:15 - for the identity performer task and then
527:17 - we have just in time j-i-t permitting
527:20 - the smallest length of duration an
527:22 - identity can use permission so usually
527:24 - when we're talking about pulp it's
527:25 - usually a focus on here uh but now
527:28 - these days uh there's a larger focus on
527:30 - jit as well and so jit is the difference
527:33 - between having long lived
527:35 - permissions or access keys versus
527:37 - short-lived ones
527:38 - and the most progressive thing in polp
527:41 - is now risk-based adaptive policies so
527:43 - each attempt to access a resource
527:45 - generates a risk score of how likely the
527:47 - request is to be from a compromised
527:48 - source so the risk score could be based
527:50 - on many factors such as device user
527:52 - location ip address what services being
527:54 - accessed and when did they use mfa did
527:56 - they use biometrics things like that and
527:59 - right now at as of this time it was does
528:02 - not have a risk-based adaptive policies
528:04 - built into iam you can roll your own
528:07 - what's interesting is cognito has
528:10 - risk-based adaptive policies they call
528:11 - it like adaptive authentication but
528:13 - that's for user pools and not identity
528:15 - pools
528:16 - user pools is for getting access to an
528:18 - app
528:19 - uh that you built through an ipd where
528:22 - identity pools incognito
528:24 - is about getting access to itabus
528:26 - resources so
528:27 - uh you know maybe i'm sure about will
528:29 - get it eventually but they just don't
528:30 - have it right now and you have to rely
528:31 - on third-party
528:33 - identity solutions uh to get risk-based
528:36 - adaptive policies now talking about just
528:38 - enough access in just in time just in
528:40 - time is like you think how would you do
528:41 - that with aws you just add and remove
528:43 - permissions manually but one thing you
528:45 - could do is use something like console
528:46 - me so this is an open source netflix
528:48 - project to self-serve short-lived i am
528:50 - policies so an end user can access
528:53 - database resources while enforcing jea
528:55 - and jit and so there's a repo there as
528:57 - well
528:58 - but the idea is they have like this
528:59 - self-serve wizard so you say i want
529:01 - these things and then the machine
529:02 - decides okay you can have them or you
529:05 - you don't need them and it just frees
529:07 - you up asking people and worrying about
529:09 - the length and stuff like that okay
529:15 - hey this is andrew brown from exam pro
529:17 - and we are taking a look at the edibus
529:18 - root user uh and this gets confusing
529:21 - because there's energies account root
529:23 - user and regular users let's distinguish
529:25 - what those three things are so here we
529:27 - have an apes account and the account
529:29 - which holds all the aws resources
529:31 - including the different types of users
529:33 - then you have the root user this is a
529:34 - special account with full access that
529:36 - cannot be deleted and then you have just
529:38 - a user and this is a user for common
529:41 - tasks that is assigned permissions so
529:44 - just understand that sometimes people
529:45 - say it was account they're actually
529:46 - referring to the user and sometimes that
529:48 - when they're saying this account they're
529:49 - actually referring to the invoice
529:50 - account that holds the users i know it's
529:52 - confusing it just it's based on what
529:54 - people decide the context is when
529:56 - they're speaking so the in-apps account
529:58 - user is a special user who's created at
530:00 - the time of the invoice account creation
530:02 - and they can do uh they have a lot of
530:04 - conditions around them so the reuser
530:06 - account uses an email and password to
530:08 - log in as opposed to the regular user
530:11 - who's going to provide their account id
530:12 - alias username and password the root
530:15 - user account cannot be deleted the root
530:17 - user account has full permissions to the
530:19 - account and its permissions and cannot
530:20 - be limited and when we say cannot be
530:22 - limited we're saying that if you take an
530:24 - im policy to explicitly deny the user
530:26 - access the resources it's not something
530:28 - you can do however you can do it in the
530:30 - case of innovative organizations with
530:32 - service control policies because a
530:34 - service control policy applies to a
530:36 - bunch of accounts so it just it's one
530:38 - level above and so that is a way of
530:40 - limiting root users but generally you
530:42 - can't limit them within their own
530:43 - account
530:44 - there can only be one root user uh per
530:46 - aws account the real user is instead for
530:49 - very
530:50 - specific and specialized tasks that are
530:52 - infrequently or rarely performed and
530:54 - there's a big list and we'll get into
530:55 - that here in a moment and the abyss root
530:57 - account should uh not be used for daily
530:59 - or common tasks it's strongly
531:01 - recommended to never use the root users
531:04 - access keys because you can generate
531:05 - those and use them it's strongly
531:07 - recommended to turn on mfa for the root
531:09 - user and any of us will bug you to no
531:11 - ends to tell you to turn it on
531:13 - so let's talk about the
531:15 - tasks that you should be performing with
531:17 - the root user and only the user can
531:19 - perform so changing your account
531:21 - settings this includes account name
531:22 - email address root user password root
531:25 - user access keys other account settings
531:27 - such as contact information payment
531:28 - currency preference regions do not
531:31 - require the root user credentials so not
531:33 - everything
531:34 - restore im user permissions so if there
531:37 - is an i i am admin so just a user that
531:40 - has admin access who actually revokes
531:42 - their own permissions you can sign into
531:43 - the root user to edit policies and
531:45 - restore those permissions
531:47 - so you can also activate im access to
531:49 - the billing and cost management console
531:52 - you can view certain tax invoices you
531:54 - can close your aws account you can
531:56 - change or cancel your aws support plan
531:58 - register as a seller in the reserved
532:00 - instance marketplace enable mfa
532:02 - delete on s3 buckets
532:04 - edit or delete an amazon s3 bucket
532:07 - policy that includes an invalid vpc id
532:10 - or vpc endpoint id
532:12 - sign up for govcloud and something
532:14 - that's not in here which this i took
532:16 - this from the documentation but uh you
532:17 - can use the aws account user to create
532:20 - the organization you can't create that
532:22 - with any other user so um you know the
532:24 - ones i highlighted in red are very
532:26 - likely to show up your exam and that's
532:27 - uh why i highlighted them there for you
532:29 - but there you go
532:31 - [Music]
532:35 - hey this is andrew brown from exam pro
532:37 - and we are taking a look at adabus
532:38 - single sign-on also known as aws sso and
532:42 - so this is where you create or connect
532:43 - your workforce identities in aws once
532:45 - and manage access essentially across
532:47 - your items organization so the idea here
532:50 - is you're going to choose your identity
532:51 - source whether it's
532:52 - sso itself active directory saml 2.0 idp
532:56 - you're going to manage user permissions
532:58 - centrally to items accounts applications
533:01 - saml applications and it uses it can you
533:04 - get single click access to all these
533:06 - things so you know just to kind of zoom
533:07 - in on this graphic here
533:09 - you know you have your on premise active
533:12 - directory it's establishing a ad trust
533:16 - connection over to uh it will single
533:18 - sign-on you're going to be able to apply
533:20 - permissions to access resources within
533:22 - your abilities account so via aws
533:24 - organizations in your organizational
533:25 - units down to your resources you can
533:28 - also use aws sso to access custom saml
533:32 - based applications so you know if i
533:34 - build a web app and i like the example
533:37 - platform and i wanted to use saml based
533:40 - uh
533:40 - connections for single sign on there i
533:42 - could do that as well
533:44 - and even connect out sso access to your
533:46 - business cloud application so office 365
533:49 - dropbox slack things like that so there
533:51 - you go
533:55 - well let's take a look here at
533:56 - application integration so this is the
533:59 - process of letting two independent
534:01 - applications to communicate and work
534:02 - with each other commonly facilitated by
534:05 - an intermediate system
534:07 - so cloud workloads uh strongly encourage
534:09 - systems and services to be loosely
534:11 - coupled and so itabus has many services
534:13 - for the specific purpose of application
534:15 - integration and these are based around
534:18 - common systems or design patterns that
534:20 - utilize application integration and this
534:23 - would be things like queuing streaming
534:25 - pub sub api gateways state machines
534:28 - event buses and i'm sure there are more
534:31 - but that's what i could uh think about
534:33 - that are the most common ones okay
534:35 - [Music]
534:39 - so to understand queuing we need to know
534:41 - what is a messaging system so this is
534:44 - used to provide asynchronous
534:45 - communication and decouple processes via
534:47 - messages and events from a sender
534:49 - receiver or a producer and a consumer so
534:52 - a queuing system
534:54 - is a messaging system that generally
534:55 - will delete messages once they are
534:57 - consumed it's for simple communication
534:59 - it's not real time you have to pull the
535:01 - data it's not reactive and a good
535:04 - analogy would be imagining people that
535:06 - are queuing in a line to go do something
535:09 - so for aws it's called simple queuing
535:11 - service sqs it's a fully managed queuing
535:14 - service that enables you to decouple and
535:16 - scale microservices distributed systems
535:18 - and serverless applications so a very
535:21 - common use case in a web application
535:23 - would be to queue up transactional
535:24 - emails uh to be sent like sign up reset
535:27 - password and the reason why we have
535:29 - queuing to decouple those kind of
535:31 - actions is that if you had a
535:32 - long-running task
535:34 - and you had too many of them it could
535:36 - hang your applications so by decoupling
535:38 - them
535:39 - and letting a separate compute
535:41 - service take care of that
535:43 - that would be something that would be
535:44 - very useful okay
535:49 - let's take a look here at streaming and
535:51 - so this is a different kind of messaging
535:53 - system
535:55 - but the idea here is you have multiple
535:56 - consumers that can react to events and
535:59 - so in streaming we call messages events
536:02 - and then in a queuing system we just
536:03 - call them messages but events live in
536:05 - the stream for long periods of time so
536:07 - complex operations can be applied and
536:09 - generally streaming is used for real
536:11 - time stuff whereas cueing is not
536:13 - necessarily uh real time
536:16 - and so adabus's solution here is amazon
536:18 - kinesis you could also use kafka but
536:20 - we'll focus on kinesis here so amazon
536:22 - kinesis is the aws fully managed
536:24 - solution for collecting processing and
536:25 - analyzing streaming data in the cloud
536:28 - so the idea is that you have these
536:29 - producers so that are producing events
536:32 - could be ec2 instances mobile devices
536:35 - it could be a computer or traditional
536:36 - server
536:38 - they're going to go into the data stream
536:40 - there's a bunch of shards that scale and
536:41 - there's consumers on the other side so
536:43 - maybe redshift wants that data dynamodb
536:45 - s3 or emr okay but the thing you have to
536:48 - remember is that streaming is for
536:49 - real-time data and as you can imagine
536:52 - because it's real-time and it's doing a
536:54 - lot more work than
536:56 - a
536:56 - queueing system it's going to cost more
536:58 - okay
536:59 - [Music]
537:03 - so we have another type of messaging
537:05 - system known as pub sub so this stands
537:08 - for publish subscribe pattern commonly
537:11 - implemented in messaging systems and a
537:13 - pub sub system the sender of messages
537:15 - the publishers do not send their message
537:17 - directly to receivers they instead send
537:19 - their messages to an event bus the event
537:21 - bus categorizes their messages into
537:23 - groups then receivers of messages
537:25 - subscribers subscribe to these groups
537:27 - whenever new messages appear within
537:29 - their subscriptions the messages are
537:32 - immediately delivered to them so the
537:34 - idea is you have publishers event bus
537:35 - subscribers and event buses appear more
537:38 - than once so it actually appears in
537:39 - streaming appears in this pub sub model
537:42 - and then it can appear
537:43 - in other variations so you're going to
537:45 - hear it more than once the word event
537:46 - bus
537:47 - so the idea here is the publisher has no
537:49 - knowledge of who the subscribers are the
537:51 - subscribers do not pull for messages
537:52 - messages aren't said automatically
537:54 - immediately pushed to the subscribers
537:56 - and messages and events are
537:57 - interchangeable terms in pub
537:59 - sub all right
538:01 - and so you know
538:02 - the idea here with publisher subscribers
538:04 - just imagine getting like a um a
538:06 - magazine subscription right if you think
538:08 - of that you kind of think of the
538:09 - mechanisms that are going here in terms
538:11 - of practicality it's very common to use
538:13 - these as a real-time chat system or a
538:16 - web hook system so you know hopefully
538:18 - that gives you an idea there in terms of
538:20 - aws's solution we're using simple
538:22 - notification service sns this is a
538:24 - highly available durable secure fully
538:26 - managed pub sub messaging service
538:30 - that enables you to decouple
538:31 - microservices distributed systems and
538:33 - serverless applications
538:34 - so here
538:36 - we have a variety of publishers like the
538:38 - sdk the cli cloud watch aid with
538:40 - services
538:41 - you'll have your sns topic you can
538:43 - filter things fan them out and then you
538:45 - have your subscribers to lambda sqs
538:47 - emails https looks very similar to
538:50 - streaming but again you know um you know
538:53 - there's not a lot of communication going
538:54 - back between it it's just publishers and
538:56 - subscribers
538:57 - and it's limited to you know
539:00 - these things here so it's a very managed
539:02 - service right
539:04 - whereas uh kinesis you can do a lot more
539:06 - with it okay
539:08 - [Music]
539:12 - so what is api gateway well it is a
539:15 - program that sits between a single entry
539:17 - point and and multiple back-ends api
539:19 - gateway allows for throttling logging
539:21 - routing logic or formatting of the
539:23 - request and response when we say request
539:25 - a response we're talking about
539:27 - https
539:29 - requests and responses
539:30 - and so the service for aws is called
539:32 - amazon api gateway so api gateway is
539:34 - just a type of pattern
539:36 - and this is the few cases where aws has
539:39 - named the thing after what it is and so
539:42 - we have amazon api gateway which is a
539:45 - solution for creating secure apis in
539:47 - your cloud environment at any scale
539:49 - create apis that act as a front door for
539:51 - applications to access data business
539:52 - logic or functionality from back end
539:55 - services so the idea is that you have
539:57 - data coming in from
539:58 - mobile apps web apps iot devices and you
540:01 - actually define the api calls and then
540:04 - you say where do you want them to go so
540:06 - maybe tasks are going to go to your
540:07 - lambdas
540:08 - and then other routes are going to go to
540:10 - rds kinesis ec2
540:13 - or your web application
540:15 - and so these are really great for having
540:17 - um this uh being able to define your api
540:20 - routes and change them on the fly and
540:22 - then and always write them to the same
540:24 - place okay
540:25 - [Music]
540:30 - so what is a state machine it is an
540:32 - abstract model which decides how one
540:34 - state moves to another based on a series
540:36 - of conditions think of a state machine
540:38 - like a flow chart and for aws the
540:40 - solution here is itabus step function so
540:43 - coordinate multiple aw services into a
540:44 - serverless workflow a graphical console
540:47 - to visualize the components of your
540:48 - application as a series of steps
540:50 - automatically trigger and track each
540:52 - step and retries when there are errors
540:55 - so your application executes in order as
540:57 - expected every time
540:59 - logs the state of each step so when
541:01 - things go wrong you can diagnose and
541:03 - debug problems quickly and so here's
541:06 - example of using a bunch of
541:10 - steps together on the uh the abyss step
541:13 - functions service and so you know this
541:15 - is generally applied for service
541:17 - workflows but it is something that is
541:18 - very useful if in application
541:20 - integration okay
541:24 - [Music]
541:26 - so what is an event bus an event bus
541:28 - receives events from a source and routes
541:30 - events to a target based on rules i'll
541:32 - get my pen tool out here so we have an
541:34 - event it enters the event bus we have a
541:36 - rules tell it to go to the target it's
541:38 - that simple and we have been seeing
541:40 - event buses in other things like uh
541:43 - streaming
541:44 - and uh pub sub but aws has this kind of
541:47 - event bus offering that is kind of high
541:50 - level it's called eventbridge and it's a
541:52 - service event bus service that is used
541:54 - for application integration by streaming
541:55 - real-time data to your applications the
541:58 - service was formerly known as
541:59 - amazon cloudwatch events they gave it a
542:01 - renaming to give it a better
542:04 - opportunity for users to know that it's
542:07 - there to use and they also extended its
542:09 - capabilities
542:11 - and so the thing is is that a lot of aw
542:13 - services are always emitting events and
542:14 - they're already going into this bus and
542:16 - so if you utilize this service it's a
542:18 - lot easier than having to roll your own
542:20 - thing uh with other services
542:22 - so amazon event bridge will just define
542:24 - an event bus so there is an event bus
542:26 - holds event data defines the rules on an
542:27 - event bus to react to events you always
542:30 - get a default event for every single abs
542:32 - account you can create custom event
542:33 - buses scoped to multiple accounts or
542:35 - other abas accounts you have a sas event
542:37 - bus scope to third party sas providers
542:40 - you have producers these are aidable
542:41 - services that emit events you have
542:43 - events these are data emitted by
542:44 - services they're json objects that
542:46 - travel the stream within the event bus
542:48 - you have partnered sources these are
542:50 - third-party apps that can emit events to
542:52 - event buses you have rules these
542:54 - determine what events to capture and
542:56 - pass to targets and then targets which
542:58 - are aidable services that consume events
543:00 - so yeah it's all just this great
543:02 - built-in um uh uh stuff that's going on
543:05 - here and so you know there there might
543:07 - be a case where you can use eventbridge
543:09 - and save your time uh a lot of time and
543:11 - effort uh doing application integration
543:13 - okay
543:14 - [Music]
543:18 - hey this is andrew brown from exam pro
543:20 - and we are taking a look at application
543:22 - integration services at a glance here so
543:24 - let's get through them so the first is
543:26 - simple notification service sns this is
543:28 - a pub sub messaging system sends
543:30 - notifications via various formats such
543:33 - as plain text email https web hooks sms
543:36 - text messages sqs and lambda pushes
543:39 - messages which are then sent to
543:41 - subscribers you have sqs this is a
543:44 - queuing messaging system or service that
543:47 - sends events to a queue other
543:49 - applications pull the queue for messages
543:50 - commonly used for background jobs we
543:52 - have step functions this is a state
543:54 - machine service
543:55 - it is it coordinates multiple aimed
543:57 - services into a serverless workflow
543:59 - easily share data among lambdas have a
544:01 - group of lambdas wait for each other
544:03 - create logical steps also works with
544:05 - fargate tasks we have a vent bridge
544:07 - formerly known as cloudwatch events it
544:09 - is a service event bus that makes it
544:11 - easy to connect applications together
544:13 - from your own application third-party
544:14 - services and aws services then there's
544:17 - kinesis a real-time streaming data
544:18 - service creates producers which send
544:20 - data to a stream multiple consumers can
544:22 - consume data within a stream used for
544:25 - real-time analytics click streams
544:26 - ingesting data from a fleet of iot
544:28 - devices you have amazon mq this is a
544:31 - managed message broker service that uses
544:33 - apache
544:34 - active mq so if you want to use apache
544:37 - activemq there it is manage kafka
544:39 - service and this gets me every time
544:42 - because it says msk and that is the
544:44 - proper initialization but you think it'd
544:46 - be mks
544:48 - it is a fully managed apache kafka
544:50 - service kafka is an open source platform
544:53 - for building real-time streaming data
544:54 - pipelines and applications similar to
544:56 - kinesis but more robust very popular by
544:58 - the way we have api gateway a fully
545:00 - managed service for developers to create
545:02 - publish maintain monitor and secure apis
545:04 - you can create api endpoints and route
545:06 - them to ada services we have appsync
545:08 - this is a fully managed graphql service
545:11 - graphql is an open source agnostic query
545:13 - adapter that allows you to query data
545:15 - from many different data sources so
545:17 - there you go
545:22 - [Music]
545:23 - hey this is andrew brown from exam pro
545:25 - and we are comparing virtual machines to
545:27 - containers so i know we covered this
545:29 - prior but i just want to do it one more
545:31 - time just to make sure that we
545:32 - fundamentally understand the difference
545:34 - before we jump into containers so the
545:36 - idea is that if you were to request an
545:37 - ec2 instance it has a host operating
545:40 - system that we don't really know much
545:42 - about but we don't really need to know
545:44 - and then the idea is you have a
545:46 - hypervisor which allows you to deploy
545:48 - virtual machines
545:50 - and so when you launch an ec2 instance
545:51 - you're actually launching a vm on top of
545:54 - a hypervisor on a server uh with on uh
545:56 - within the aws
545:57 - data centers servers there and you're
545:59 - going to choose an operating system so
546:01 - like ubuntu and it might come with some
546:03 - pre-installed packages or you can
546:04 - install your own libraries packages and
546:06 - binaries and then you decide what kind
546:08 - of workloads you want to run on there so
546:09 - it could be django
546:10 - mongodb so your database and some kind
546:13 - of queueing system like rabbitmq the
546:16 - difficulties with virtual machines so
546:17 - you're always going to end up with some
546:19 - unused space because you're going to
546:20 - want to have some headroom uh to make
546:23 - sure that uh you know if you know django
546:25 - needs more memory or or mongodb needs
546:28 - more storage that you have that room
546:30 - that you can grow into
546:32 - but the idea is that you're always
546:33 - paying for that even when you're not
546:35 - utilizing it and so you know that can be
546:38 - uh not as cost effective as you'd like
546:40 - it to be so when we're looking at
546:42 - doing this again and we are
546:45 - using containers um instead of the
546:47 - hypervisor we have container
546:49 - virtualization a very common one would
546:50 - be called docker daemon for docker of
546:52 - course and so now you're launching
546:54 - containers and so maybe you have alpine
546:56 - and this is for your web app and then
546:58 - you install exactly the libraries
546:59 - packages and binaries you need for that
547:01 - and then for
547:02 - mongodb you want to have a different os
547:05 - different packages and same thing with
547:07 - rabbitmq maybe you want to run it on
547:09 - freebsd and the idea is that uh you know
547:11 - you're not going to have this waste
547:13 - because it it's kind of changed the
547:15 - sense that these containers are flexible
547:17 - so they can expand or decrease based on
547:20 - the the use case of what they need
547:22 - uh and you know if you use particular
547:24 - services like it was fargate
547:26 - you know you're paying like for running
547:28 - the containers not necessarily uh for uh
547:31 - over provisioning okay so vms do not
547:33 - make best use of space apps are not
547:35 - isolated which could cause uh config
547:38 - conflict security problems or resource
547:40 - hogging
547:41 - containers allow you to run multiple
547:42 - apps which are virtually isolated from
547:44 - each other launch new containers
547:45 - configure os dependencies per container
547:48 - okay
547:49 - [Music]
547:53 - hey this is andrew brown from exam pro
547:55 - and we are taking a look at the concept
547:56 - of microservices and to understand
547:58 - microservices we first need to
548:00 - understand monoliths or monolithic
548:02 - architecture and the idea here is that
548:04 - we have one app which is responsible for
548:06 - everything and the functionality is
548:08 - tightly coupled so i'm going to get my
548:10 - pen tool out here and just to highlight
548:11 - notice that there is a server and
548:13 - everything is running on a single server
548:15 - whether it's load balancing caching the
548:17 - database
548:19 - maybe the marketing website the
548:20 - front-end javascript framework the
548:22 - backend with its api uh the orm
548:26 - connected to background tasks things
548:28 - like that and that's the idea of a
548:29 - monolith and that's what
548:31 - a lot of people are used to doing but
548:33 - the idea with microservice architecture
548:35 - is that you have multiple apps which are
548:36 - responsible for one one thing and the
548:38 - functionality is isolate and stateless
548:41 - and so just by
548:42 - leveraging um various cloud services or
548:45 - bolting it onto your service
548:47 - you know you are technically using
548:49 - microservice architecture so maybe your
548:51 - web app is all hosted uh in containers
548:54 - so you have your apis your your orm your
548:57 - reports maybe you've abstracted out some
548:59 - particular functions into lambda
549:01 - functions you have your um marketing
549:04 - website hosted on s3 you have your
549:06 - front-end javascript hosted on that
549:07 - three
549:08 - you're now using elastic load balancer
549:11 - elasticash rds
549:14 - sqs and that's the idea between
549:16 - monoliths and microservices okay
549:19 - [Music]
549:23 - well let's take a look here at
549:24 - kubernetes which is an open source
549:26 - container orchestration system for
549:28 - automating deployment scaling and
549:30 - management of containers it was
549:31 - originally created by google and now
549:33 - maintained by the cloud native computing
549:35 - foundation so the cncf
549:37 - kubernetes is commonly called k-8 the
549:40 - eight represents the remaining letters
549:42 - for kubernetes which is odd because
549:44 - everyone calls it kubernetes with the s
549:46 - on there but that's just what it is the
549:48 - advantage of kubernetes over docker is
549:50 - the ability to run containers
549:51 - distributed across multiple vms a unique
549:54 - component of kubernetes are pods a pod
549:56 - is a group of one or more containers
549:58 - with with shared storage network
550:00 - resources and other shared settings
550:03 - so here is kind of an example where you
550:04 - have your kubernetes master it has a
550:06 - scheduler controller etcd you might be
550:08 - using
550:09 - it uses an api server to run nodes
550:12 - within the nodes we have pods and within
550:14 - the pods we have containers
550:16 - kubernetes is ideally for micro service
550:18 - architectures where company has tens to
550:21 - hundreds of services they need to manage
550:23 - i need to really emphasize that tens to
550:26 - hundreds of services all right so you
550:29 - know kubernetes is great but just
550:30 - understand that it is really designed uh
550:32 - to be used for
550:33 - massive amounts of microservices if you
550:36 - don't have that need you might want to
550:38 - look at something just easier to use
550:40 - okay
550:43 - [Music]
550:45 - all right let's take a look here at
550:46 - docker which is a set of platform as a
550:48 - service products that use os level
550:50 - virtualization to deliver software in
550:52 - packages called containers so docker was
550:55 - the earliest popularized open source
550:58 - container platform meaning there's lots
550:59 - of tutorials there's a lot of services
551:02 - that uh integrate with docker or make it
551:04 - really easy to use and so when people
551:06 - think of containers they generally think
551:07 - of docker there's of course a lot more
551:09 - options out there than docker to run
551:11 - containers but this is what people think
551:13 - of and so we said it's a suite of tools
551:15 - so the idea is you have this docker cli
551:17 - so these are cli commands to download
551:19 - upload build run and debug containers a
551:21 - docker file a configuration file on how
551:24 - to provision a container docker compose
551:26 - which is a tool and configuration file
551:28 - when working with multiple containers
551:31 - docker swarm an orchestration tool for
551:33 - managing deployed multi-container
551:34 - architectures docker hub a public online
551:37 - repository for containers published by
551:39 - the community for download and one
551:40 - really interesting thing that came out
551:42 - of docker was the open container
551:44 - initiative oci which is an open
551:46 - governance structure for creating open
551:48 - industry standards around container
551:50 - formats and runtimes so docker
551:52 - establishes oci and it is now maintained
551:55 - by the linux foundation and so the idea
551:58 - is that you can write a docker file or
552:00 - or do things very similarly and use
552:02 - different types of um
552:04 - technologies that can use containers as
552:07 - long as they're oci compatible you can
552:08 - use them so docker has been losing favor
552:11 - with developers due to their handling of
552:12 - introducing a paid open source model and
552:15 - alternatives like podman are growing and
552:17 - that's why we're going to talk about
552:18 - podman next okay
552:22 - [Music]
552:24 - so let's take a quick look here at
552:25 - podman which is a container engine that
552:27 - is oci compliant and is a drop-in
552:29 - replacement for docker i just want to
552:31 - get you exposure here because i want you
552:32 - to know about this um and that you can
552:35 - use it as opposed to using docker there
552:38 - are a few differences or advantages that
552:39 - podman has so podman is daemon-less
552:41 - where docker uses a container d daemon
552:44 - podman allows you to create pods like
552:45 - kubernetes where docker does not have
552:47 - pods uh podman only replaces one part of
552:50 - docker podman is is to be used alongside
552:52 - builda and uh scopio so you know docker
552:56 - is an all-in-one kind of tool
552:58 - everything is done via a single cli and
553:00 - everything is there but you know they
553:02 - just wanted to make it more module and
553:03 - so
553:04 - these other tools anytime you say podman
553:06 - it usually means we're talking about
553:07 - podman builda and scopio so builda is a
553:10 - tool used to build the oci images and
553:12 - scopio is a tool for moving container
553:14 - images between different types of
553:16 - container storages palm is not going to
553:17 - show up in your exam but you should
553:19 - practically know it
553:21 - just for your own benefit okay
553:22 - [Music]
553:27 - let's take a look here at the container
553:29 - services offered on aws
553:31 - so we have primary services that
553:32 - actually run containers provisioning and
553:34 - deployment on you know tooling around
553:36 - provisioning deployment and supporting
553:38 - services so the first here is elastic
553:40 - container service ecs
553:42 - and the advantage of this service is
553:44 - that it has no cold starts but it is a
553:46 - self-managed dc2 so that means that
553:49 - you're going to be always paying for the
553:50 - resource as it is running all right then
553:53 - he has aws fargate so this is more
553:55 - robust than using abus lambda it can
553:57 - scale to zero costs
554:00 - and it's being managed by adabus managed
554:02 - ec2 however it does have cold starts so
554:05 - you know if you need containers
554:06 - launching really fast you might be
554:08 - wanting to use ecs then you have elastic
554:10 - kubernetes service eks this is uh open
554:13 - source it runs kubernetes um and this is
554:16 - really useful if you want to avoid
554:18 - vendor lock-in um which is not really a
554:21 - problem but
554:23 - batteries just you want to run
554:24 - kubernetes then you have abs lambda so
554:26 - you only think about the code it's
554:28 - designed for short running tasks if you
554:30 - need something that runs longer you'd
554:32 - want to use that is serverless you'd use
554:34 - abus fargate which is serverless
554:36 - containers you can deploy custom
554:38 - containers so prior aws lambda just had
554:41 - pre-built runtimes which were containers
554:43 - but now you can create any kind of
554:44 - container and use that on it was lambda
554:48 - for provisioning deployment you can use
554:50 - elastic bean socks so
554:52 - it can uh deploy elastic container
554:54 - service for you um which is very useful
554:57 - there now there's app runner which kind
555:00 - of overlaps on what elastic beanstalk
555:01 - does but it specializes it specializes
555:04 - for containers um and i believe that it
555:07 - can actually i don't know what it uses
555:08 - underneath because it is a managed
555:09 - service so elastic bean stock is um open
555:13 - you can see what is running underneath
555:14 - an app runner i don't believe you can
555:15 - see what is running underneath is just
555:17 - taken care of by aws
555:19 - then there's abyss copilot cli so this
555:22 - allows you to build release operate
555:23 - production ready containerized
555:25 - applications on app runner ecs enables
555:27 - fargate for supporting services you have
555:29 - elastic container registry this is repo
555:31 - for your containers not necessarily just
555:33 - docker containers but containers in
555:35 - general probably oci compliant
555:36 - containers x-rays so analyze and debug
555:38 - between micro services so you know it's
555:42 - distributed tracing then you have step
555:43 - functions so stitch together lambdas and
555:45 - ecs tasks to create um
555:49 - a state machine and the only thing i
555:51 - don't have on here would be you know
555:53 - being able to launch an ec2 instance
555:54 - from the marketplace that has
555:56 - um a
555:58 - a container runtime installed like
556:00 - docker i just don't feel that that's
556:02 - very relevant for the exam but it is
556:04 - another option for containers not
556:06 - something that people do very often but
556:07 - there you go
556:08 - [Music]
556:12 - hey this is andrew brown from exam pro
556:14 - and we're taking a look here at
556:15 - organizations and accounts so aws
556:17 - organizations allow the creation of new
556:19 - aws accounts and allows you to centrally
556:22 - manage billing control access compliance
556:24 - security and share resources across your
556:26 - aws accounts so here's kind of a bit of
556:28 - a structure of
556:31 - the architecture of aws organizations
556:33 - and we'll just kind of walk through the
556:34 - components so the first thing you have
556:36 - is a root account user this is a single
556:39 - sign-in identity that has complete
556:40 - access to all eight of the services and
556:42 - resources in an account and each account
556:44 - has a root account user so generally you
556:47 - will have a master or root account and
556:49 - even within that you'll have a root
556:51 - account user and for every additional
556:53 - account that you have you'll notice over
556:55 - here we have a root account user
556:58 - then there's a concept of organizational
557:00 - units uh these are commonly abbreviated
557:02 - to ous so they are a group of aws
557:05 - accounts within an organization which
557:06 - can contain other organizational units
557:09 - creating a hierarchy so
557:11 - here is one where we have called
557:12 - starfleet and here's one called
557:14 - federation planets and underneath we
557:16 - have multiple
557:17 - accounts it was accounts within that
557:19 - organizational unit
557:21 - and even though it does not show it here
557:22 - you can create an organizational unit
557:24 - within an organizational unit
557:26 - then we have service control policies
557:28 - scps and these give uh central control
557:30 - over the allowed permissions for all aws
557:32 - accounts in your organization helping to
557:35 - ensure your accounts stay within your
557:37 - organizational guidelines what they're
557:39 - trying to say here is that um there's
557:41 - this concept of aws
557:43 - i am policies and all you're doing is
557:45 - you're creating a policy that's going to
557:47 - be uh organizational unit-wide or
557:50 - organizational-wide or for select
557:52 - accounts so it's just a way of applying
557:54 - iron policies across multiple accounts
557:56 - it was organizations must be turned on
557:58 - and once it's turned on it cannot be
558:00 - turned off it's generally recommended
558:02 - that you do turn it on because basically
558:05 - if you're gonna run any kind of serious
558:06 - workload you're gonna be using awesome
558:08 - organizations to isolate your abus
558:10 - accounts based on workloads you can
558:12 - create as many aws accounts as you like
558:14 - one account will be the master or root
558:16 - account
558:18 - and i say root account here because this
558:19 - is the new language here and some of the
558:21 - documentation still calls it master
558:22 - account so understand this is the root
558:24 - account not to be confused with the root
558:28 - account user so
558:30 - another clarification i want to make is
558:31 - an ito's account is not the same as a
558:34 - user account which is another thing that
558:36 - is confusing so when you sign up for aws
558:39 - you get an aws account and then it
558:41 - creates you a user account which happens
558:43 - to be a root user account so hopefully
558:45 - that is clear
558:46 - [Music]
558:51 - so aws control tower helps enterprises
558:53 - quickly set up a secure aws multi
558:55 - account it provides you with a baseline
558:57 - environment to get started with a
558:58 - multi-count architecture so it does this
559:01 - a few a few different ways the first
559:03 - thing is it provides you a landing zone
559:05 - this is a baseline environment following
559:07 - well architected and best practices to
559:09 - start launching production-ready
559:11 - workloads so imagine you wanted to go
559:13 - have um you know the perfect environment
559:15 - that you know is secure
559:17 - is correctly configured and has good
559:20 - logging in place that's what a landing
559:21 - zone is and so itabus's landing zone for
559:24 - control tower is going to have sso
559:26 - enabled by default so it's very easy to
559:27 - move between ips accounts it will have
559:30 - centralized logging for aws cloud trail
559:31 - so that you know they're going to be
559:33 - tamper evident or tamper proof away from
559:35 - your workloads where they can't be
559:37 - affected it'll have cross account
559:38 - security auditing
559:40 - um so yeah landing zones are really
559:42 - great to have then there's the account
559:43 - factory they used to call this um
559:46 - a vending machine but they changed it to
559:49 - account factory the idea is it automates
559:51 - provisioning of new accounts in your
559:52 - organization it standardizes the
559:54 - provisioning of new accounts with
559:55 - pre-approved
559:56 - account configuration you can configure
559:58 - account factory with pre-approved
560:00 - network configuration and region
560:01 - selections
560:02 - enable self-service for your builders to
560:05 - configure and provision to accounts
560:06 - using able service catalog able service
560:08 - catalog is just pre-approved uh
560:10 - workloads uh via cloud formation
560:12 - templates so you created to say okay
560:13 - you're allowed to launch this server or
560:15 - these resources
560:17 - and the third and most important thing
560:19 - that ava's control tower comes with is
560:20 - guard rails so these are pre-packaged
560:22 - governance rules for security operations
560:24 - compliance the customers can select and
560:26 - apply enterprise-wide or to specific
560:28 - groups of accounts
560:30 - so abus control tower is the replacement
560:33 - of the retired aws landing zone so if
560:36 - you remember abel's landing zones which
560:38 - was never a self-serve easy thing to
560:40 - sign up for it required a lot of money
560:42 - and
560:43 - stuff that go in there they just don't
560:44 - really have it anymore and it was
560:46 - control tower is the new offering um
560:48 - there okay
560:49 - [Music]
560:54 - hey this is andrew brown from exam pro
560:55 - and we are taking a look at abs config
560:57 - and to understand it was config we need
560:59 - to know what compliance as code is and
561:02 - to understand compliance as code we need
561:03 - to understand what change management is
561:06 - so change management in the context of
561:07 - cloud infrastructure is when we have a
561:10 - formal process to monitor changes
561:12 - enforce changes and remediate changes
561:15 - and compliance is code also known as cac
561:18 - is when we utilize programming to
561:20 - automate the monitoring enforcing and
561:22 - remediating changes to stay compliant
561:24 - with the compliance program or expected
561:27 - configuration so what is adabus config
561:30 - well it's a compliance code framework
561:32 - that allows us to manage change in your
561:34 - aws accounts on a per
561:36 - region basis meaning that you have to
561:38 - turn this on for every region that you
561:40 - need it for and so here is a very simple
561:43 - example where let's say we create a
561:45 - config rule and we have an ec2 instance
561:48 - and we expect it to be in a particular
561:49 - state
561:50 - and then in the other case we have a rds
561:54 - instance and it's in a state that we do
561:55 - not like so the idea is that we try to
561:57 - remediate it to put it in the state that
561:59 - we want it to be and those configurables
562:01 - are just powered by lambdas as you can
562:03 - see based on the lambda icon there
562:05 - so when should you use database config
562:08 - well this is when i want this resource
562:09 - to stay configured a specific way for
562:11 - compliance i want to keep track of
562:14 - configuration changes to resources i
562:16 - want a list of all resources within a
562:18 - region and i want to use
562:21 - analyze potential security weaknesses
562:23 - and you need detailed historical
562:25 - information so there you go
562:27 - [Music]
562:31 - hey this is andrew brown from exam pro
562:33 - and in this follow along we're going to
562:34 - take a look at aws config so itaps
562:36 - config is a tool that allows you to
562:38 - ensure that your services are configured
562:40 - as expected so i've already activated it
562:42 - in my north virginia region so what i'm
562:44 - going to do is just go over to ohio here
562:47 - because it is per region activated and
562:49 - i'll go over to config and then what
562:51 - we'll have to do is set it up
562:53 - so there is this one click setup and it
562:55 - did skip me to the review step because
562:57 - it's kind of piggybacking on the
562:59 - configuration of my original one here
563:01 - but the idea is that you'll just say uh
563:03 - record all resources in this region or
563:05 - things like that you'll have to create a
563:07 - service role link if you have not done
563:10 - so so this will look a little bit
563:11 - different but here it's using the
563:12 - existing one you'll have to choose a
563:14 - bucket so or create a bucket uh it's not
563:17 - super complicated so you get through
563:19 - there you hit confirm and basically
563:21 - you're going to end up with this so the
563:23 - inventory
563:24 - lets you see all the the resources that
563:27 - are not all of them but most resources
563:29 - that are in your aws account in this
563:31 - particular region it this will not
563:32 - populate right away so you will have to
563:35 - wait a little bit of time for that to
563:37 - appear one really nice thing are
563:39 - conformance packs i really love these
563:41 - things
563:42 - when nativists first brought these out
563:43 - there was only like a couple but now
563:44 - they have
563:45 - tons and tons and tons of performance
563:47 - packs so you can go deploy a conformance
563:49 - pack and you can open up the templates
563:52 - i just want to show you look at how many
563:53 - they have
563:55 - so there's some you might recognize like
563:56 - nist
563:58 - cis things like that well architected uh
564:00 - stuff and all these are
564:03 - um and i'm not sure if it's easy to open
564:05 - these up but all these are if we open
564:06 - them up they're on github is these are
564:08 - just cloud formation templates to set up
564:10 - configuration rules so there's a variety
564:13 - of suggested rules uh like around i am
564:16 - best practices and things like that that
564:17 - we can load in um but the idea is that
564:20 - you're just going to create rules so you
564:21 - go here and you add a rule and they have
564:23 - a bunch of managed rules here
564:25 - that we can look at but i think it might
564:27 - be fun to actually run a
564:30 - conformance pack i'll just show you what
564:31 - it looks like to add a rule first so
564:33 - let's say we wanted to do something for
564:34 - s3
564:36 - and it was making sure that we are
564:38 - blocking public access so we go next
564:40 - here generally you'll have a trigger
564:42 - type you can choose whether it's
564:44 - configured when it happens or it's
564:46 - periodic this is disabled in this case
564:48 - here and you just scroll on down
564:50 - and then once you've added the
564:52 - rule
564:54 - what you can do
564:56 - is
564:57 - also manage remediation so if this
565:00 - rule said hey this thing is
565:02 - non-compliant we want you to take a
565:04 - particular action you have all these aws
565:07 - actions that you can perform and you can
565:09 - notify the right people to correct it or
565:10 - have it auto correct if you choose to do
565:13 - so
565:14 - for rules you can also make your own
565:16 - custom ones so that's just you providing
565:18 - your own lambda functions you're
565:20 - providing that lambda iron and so
565:22 - basically you can have it do anything
565:23 - that you want whatever you want to put
565:25 - in a lambda you can make aws config
565:27 - check for
565:28 - okay so it's not super complicated here
565:31 - but
565:32 - this one here is just going to go ahead
565:33 - and check and so if we go and reevaluate
565:37 - we might just take some time to show up
565:38 - so they're gonna say that it's compliant
565:40 - or non-compliant okay and i it should be
565:42 - compliant but while we're waiting for
565:44 - that to happen let's just see how hard
565:45 - it is to deploy a conformance pack
565:47 - because i feel like that's something
565:48 - that's really important oh you just drop
565:50 - them down and choose them that's great
565:51 - so we might want to go to iam here
565:54 - oops identity and access management
565:57 - and hit next
565:58 - and say
565:59 - my
566:00 - [Music]
566:02 - im best practices
566:04 - and you might not want to do this
566:06 - because it does have spend and i want to
566:07 - say spend it's not going to happen
566:09 - instantly but the idea is that if you
566:10 - turn this on and forget to remove it
566:12 - you will see some kind of charges over
566:15 - time because it does check based on the
566:16 - rules it's not super expensive but it is
566:18 - something to consider about
566:20 - but anyway so it looks like we created
566:22 - that conformance pack so if i refresh
566:24 - it looks like it's in progress i wonder
566:25 - if that's going to set up a cloud
566:26 - formation template i'm kind of curious
566:28 - about that
566:30 - so make our way over to cloudformation
566:34 - and it is so that's really nice because
566:36 - once that is done what we can do is just
566:39 - tear it down by deleting the stack so
566:40 - i'm going to go back over to our
566:42 - conformance pack here
566:45 - let's take a look here
566:47 - and so it still says it's in progress
566:48 - but it is completed and we can click
566:50 - into it
566:52 - and we can see all the things that it's
566:55 - doing so it says item groups have user
566:56 - check performance pack
566:58 - and so it looks like there's a bunch of
567:00 - cool rules uh here so
567:03 - what we'll do
567:04 - is we'll just wait a little while and
567:06 - we'll come back here and then just see
567:08 - if um this updates and see how compliant
567:10 - we are from a uh
567:12 - a basic account okay
567:14 - all right so after waiting a little
567:15 - while there it looks like some of them
567:17 - are being set so i just gave it a hard
567:18 - refresh here uh and here you can see
567:21 - that it's saying is root account um oops
567:23 - we'll give it a moment here to refresh
567:25 - but uh is the root account mfa applied
567:27 - yes have we done a password policy no
567:30 - and actually i never did a password
567:31 - policy which is something i forgot to do
567:33 - but here they're just talking about the
567:34 - minimums and maximums of things that you
567:36 - can do
567:38 - okay so that's a conformance pack
567:40 - but if we go to rules actually i guess
567:42 - it's all the rules here
567:44 - i can't really tell the difference
567:45 - between the conformance pack rules and
567:46 - our plane rules it's kind of it's kind
567:48 - of all mixed
567:49 - together here i think
567:53 - yeah so it's a bit hard to see what's
567:54 - going on there
567:56 - if we go to the performance pack and
567:57 - clicking again it might show the rules
567:59 - yeah there we go so here's the rules
568:00 - there we're seeing a little bit more
568:02 - information so use a hardware mfa so you
568:05 - know how they're talking about using a
568:06 - security key like what i showed you that
568:08 - i had earlier in the course things like
568:10 - that
568:11 - um i am password policy things like that
568:14 - so you know
568:15 - not too complicated but um i think i'm
568:17 - all done here so what i'm going to do
568:19 - is i'm going to go over to
568:20 - cloudformation and tear that on down but
568:22 - you get the idea
568:24 - well i might want to show you uh drift
568:26 - so
568:27 - there used to be a way
568:29 - it's cause i keep changing things on me
568:30 - here but there's a way to see
568:33 - uh history over time
568:35 - and so that was something
568:38 - that they used to show
568:40 - and i'm just trying to like find where
568:42 - they put it because it is like somewhere
568:44 - else
568:46 - resources maybe
568:50 - ah resource timeline okay so they moved
568:52 - it over into the resource inventory
568:54 - and so if we were to take a look at
568:56 - something anything maybe this here
568:58 - resource timeline
569:00 - and there might not be much here but the
569:01 - idea is it will show you over time how
569:03 - things have changed so the idea is that
569:05 - not only can you say what about config
569:07 - is something compliant but when was it
569:09 - complying and that is something that is
569:10 - really important to know okay so very
569:12 - simple example maybe not the best but
569:14 - the idea is that we can see when it was
569:16 - and was not compliant based on changes
569:19 - to our stuff but anyway that looks all
569:22 - good to me here so i'm going to make my
569:23 - way over to cloudformation actually i
569:25 - already already have it open over here
569:27 - we can go ahead and delete that stack
569:29 - um
569:30 - termination protection is enabled you
569:32 - must first disable it so we'll edit it
569:35 - disable it
569:36 - whatever
569:38 - okay we'll hit delete there and as
569:39 - that's deleting i'm going to go look for
569:41 - and config my original
569:45 - rule there
569:46 - again i'm not really worried about it i
569:47 - don't think it's going to really cost me
569:48 - anything but i'm also just kind of clear
569:50 - the house here just so you're
569:52 - you're okay as well
569:54 - and so if we go over to our rules
569:57 - um the one that i spun up that was
569:59 - custom
570:00 - i think was this one here because these
570:01 - are all grayed out right so i can go
570:03 - ahead there delete that rule
570:05 - type in delete
570:07 - and we are good so there you go
570:10 - that is
570:12 - it all right
570:17 - [Music]
570:18 - aws quick starts are pre-built templates
570:20 - by ada best and ebay's partners to help
570:22 - deploy a wide range of stacks it reduces
570:25 - hundreds of manual procedures into just
570:27 - a few steps
570:28 - the quick start is composed of three
570:30 - parts it has a reference architecture
570:32 - for the deployment a database cloud
570:34 - formation templates that automate and
570:36 - configure the deployment a deployment
570:38 - guide explain the architecture
570:40 - implementation and detail so here's an
570:41 - example of one that you might want to
570:43 - launch like the adabus q a bot and then
570:45 - you will get an architectural diagram a
570:47 - lot of information about it and from
570:49 - there you can just go press the button
570:51 - and launch this infrastructure most
570:53 - quick start reference deployments enable
570:55 - you to spend up a fully functional
570:57 - architecture in less than an hour and
570:59 - there is a lot as we will see here when
571:01 - we take a look for ourselves
571:03 - [Music]
571:07 - all right so here is uh it was quick
571:10 - starts where we have a bunch of cloud
571:12 - formation templates uh built by aws or
571:14 - amazon or a best partner networks apn
571:17 - partners
571:18 - and there's a variety of different
571:20 - things here so i'm just going to try to
571:21 - find something like q and a bot
571:24 - q and a bot just type in bot here
571:27 - and i don't know why it was here the
571:28 - other day now it's not showing up which
571:30 - is
571:31 - totally fine but um you know i just want
571:33 - anything to deploy just to kind of show
571:34 - you what we can do with it
571:36 - so you scroll on down we have uh this
571:39 - graphic here that's representing what
571:40 - will get deployed so we have cloudfront
571:42 - s3 dynamodb
571:44 - systems manager lex paulie all these
571:46 - kind of fun stuff
571:49 - and there's some information about how
571:50 - it is architected and the idea is you
571:52 - can go ahead and launch in the console
571:54 - or view the implementation guide let's
571:56 - go take a look here
571:57 - um and there's a bunch of stuff so we
572:00 - have solutions and things like that
572:01 - conversational things like that
572:04 - but what i'm going to do is go ahead and
572:05 - see how far i can get to launching with
572:08 - this it doesn't really matter if we do
572:09 - launch it but it's just the fact that um
572:11 - i wanted to show you what you can do
572:13 - with it so if we go to the designer it's
572:15 - always fun to look at it in there
572:17 - because then we can kind of visualize
572:18 - all the resources that are available
572:21 - and i thought that that would populate
572:23 - over there but maybe
572:24 - we did the wrong things i'm just going
572:26 - to go back and click
572:29 - i'm just going to click out of this
572:32 - oops cancel let's close that
572:34 - leave yes
572:36 - and we will launch that again
572:40 - and so
572:41 - this oh view in the designer hit the
572:43 - wrong button okay
572:47 - so now this should show us the template
572:50 - it might just be loading
572:54 - there we go so this is what it's going
572:56 - to launch and you can see there's a lot
572:57 - going on here i'm just going to shrink
572:59 - that there uh and i don't know if you
573:01 - can make any sense of it but clearly
573:03 - it's doing a lot
573:04 - and so if we were happy with this and we
573:06 - wanted to launch it i know i keep
573:08 - backing out of this but we're going to
573:09 - go back into it one more time
573:12 - we can go here and we go next
573:14 - and then we would just fill in what we
573:16 - want so you name it put the language in
573:18 - and this is stuff that they set up so
573:19 - maybe you want a mail voice
573:21 - set the admin and stuff like that and
573:24 - it's that simple really
573:26 - um and every stack is going to be
573:27 - different so they're all going to have
573:28 - different configuration options but
573:30 - hopefully that gives you kind of an idea
573:32 - of what you can do with quick starts
573:34 - okay
573:35 - [Music]
573:40 - let's take a look at the concept of
573:42 - tagging within aws so a tag is a key and
573:45 - value pair that you can assign to any of
573:47 - this resource so as you are creating a
573:49 - resource is going to prompt you to say
573:51 - hey what tags do you want to add you're
573:53 - going to give a key you're going to give
573:54 - a value and so some examples could be
573:56 - something like based on department the
573:59 - status the team the environment uh the
574:02 - project as we have the example here the
574:04 - location
574:06 - and so tags allow you to organize your
574:07 - resources in the following way for
574:08 - resource management so specific
574:10 - workloads so you can say you know
574:12 - developer environments cost management
574:14 - and optimization so cost tracking
574:16 - budgets and alerts operations management
574:18 - so business commitments sla operations
574:21 - mission critical services security so
574:23 - classification of data security impact
574:25 - governance and regulatory compliance
574:27 - automation workload automation and so
574:30 - it's important to understand that
574:32 - tagging can be used in junction with i
574:34 - am policy so that you can restrict
574:36 - access or things like that based on
574:38 - those tags okay
574:43 - [Music]
574:44 - all right i just want to show you one
574:45 - interesting thing about tags um and it's
574:48 - just the fact that it's used as the name
574:51 - for some services so when you go to ec2
574:54 - and you launch an instance uh the way
574:56 - you set the name is by giving it a tag
574:58 - called name and i just want to prove
574:59 - that to you
575:01 - just like one of those little exceptions
575:02 - here so we choose an instance here
575:05 - we go to configure storage and then what
575:08 - we do is we add a tag and we say name
575:11 - and my server name okay and then we go
575:14 - ahead and review and launch
575:16 - we're going to launch this i don't need
575:18 - a key pair so we'll just say proceed
575:19 - without key pair
575:21 - i acknowledge
575:23 - okay
575:26 - and we will go view the instances and
575:28 - you'll see that is the name so um that's
575:30 - just like one of those exceptions or
575:33 - things that you can do with tags if
575:34 - there's other things with tags i have no
575:36 - idea that's just like a a basic one that
575:39 - everybody should know and that's why i'm
575:41 - shown to you with the tags but there you
575:43 - go
575:44 - [Music]
575:48 - so we just looked at tags now let's see
575:50 - what we can do with resource groups
575:51 - which are a collection of resources that
575:53 - share one or more tags or another way to
575:55 - look at it it's a way for you to take
575:58 - multiple tags and organize them
576:01 - into resource groups so it helps you
576:03 - organize and consolidate information
576:05 - based on your project and the resources
576:06 - that you use resource groups can display
576:09 - details about a group of resources based
576:11 - on metrics alarms configuration settings
576:14 - and at any time you can modify the
576:16 - settings of your resource groups to
576:18 - change what resources appear resource
576:20 - groups appear in the global console
576:22 - header
576:23 - which is over here and under the systems
576:26 - manager so technically it's part of aws
576:28 - simple systems manager or systems
576:30 - manager interface but it's also part of
576:32 - the global interface so sometimes that's
576:34 - a bit confusing but that's where you can
576:36 - find it okay
576:37 - [Music]
576:42 - all right so what i want to do is
576:43 - explore resource groups and also
576:46 - tagging so what i want you to do is type
576:48 - in resource groups at the top here and
576:51 - it used to be accessible
576:53 - not sure where they put it but it used
576:54 - to be accessible here at the top but
576:56 - they might have moved it over to systems
576:57 - manager so i'm going to go to ssm here
577:00 - not sure why i can't seem to find it
577:02 - today
577:03 - and on the left hand side we're going to
577:05 - look for
577:07 - resource groups
577:45 - you
578:55 - all right so what i want to do is take a
578:57 - look at resource groups and i'm really
578:59 - surprised because it used to be
579:00 - somewhere in the global now but
579:03 - i think they might have changed it um
579:06 - and what's also frustrating is if i go
579:07 - over to systems manager it was over here
579:10 - as well and so on the left-hand side i'm
579:13 - looking for resource groups it's not
579:15 - showing up so
579:17 - i don't really the best you keep moving
579:18 - things around on me and i'm i can only
579:20 - update things so quickly in my courses
579:22 - but if you type in resource groups and
579:24 - tag editor it's actually over here
579:27 - um i guess it's its own standalone
579:28 - service now why they keep changing
579:30 - things i don't know
579:32 - but uh
579:33 - the idea is we want to create a resource
579:34 - group so you can create unlimited single
579:36 - region groups in your abel's account use
579:39 - the group to view related insights
579:41 - things like that so i'm going to go
579:42 - ahead and create a resource group you
579:44 - can see it can be tag based or cloud
579:45 - formation based but i don't have any
579:47 - tags i don't really have anything tags
579:49 - so what i'm going to do
579:50 - is make my way over to s3 we're just
579:52 - going to create some resources or a
579:54 - couple resources here with some tags so
579:56 - that we can do some filtration so i can
579:58 - go ahead and create a bucket i'm going
579:59 - to say my
580:00 - bucket uh this like that whoops
580:05 - and then down below i'm going to go down
580:06 - to tags and we're going to say project
580:09 - and we're going to say um
580:12 - rg for resource group
580:15 - okay and then i can go back over here
580:17 - and then i'm going to just say
580:19 - i can say exactly what type i want i'm
580:21 - going to support all resource types
580:23 - and i'm going to say project
580:28 - rg see how it auto-completes
580:30 - and we'll go down below
580:32 - we'll just say
580:34 - my rg
580:36 - a test rg
580:39 - we'll create that
580:42 - and so now we have a resource group and
580:44 - we can see them all in one place
580:46 - resource groups are probably useful for
580:48 - using in
580:50 - policy so you can say say like resource
580:52 - group
580:53 - i am policies
580:56 - that's probably what they're used for
581:00 - okay so before i use i am managed to
581:01 - actually realize groups you should
581:02 - understand i am features things like
581:04 - that
581:06 - and so administrators can use json
581:09 - policies to specify who has access to
581:11 - what
581:12 - and so a policy action a resource group
581:14 - is used following the prefix resource
581:16 - groups
581:17 - so
581:18 - my thought process there is that if you
581:21 - want to say okay you have access to a
581:23 - resource you can just specify a resource
581:25 - group and it will include all the
581:27 - resources within there and so that might
581:30 - be
581:30 - a better way to apply permissions at a
581:33 - per project basis
581:35 - um and that could save you a lot of time
581:36 - writing out i am policies so
581:39 - that's basically all there really is to
581:41 - it also you kind of get an overview of
581:42 - of the resources that are there
581:46 - so that can be kind of useful as well
581:48 - there's the tag editor here i can't
581:50 - remember what you use this for you can
581:52 - set up tag policies
581:55 - tag policies help you standardize tags
581:57 - on resource groups and your accounts use
581:59 - to define tech policies and absorb to
582:01 - attach them to the entire organization
582:03 - um we're not in the org account so i'm
582:05 - not going to show you this and it's not
582:06 - that important
582:08 - but just understand that resource groups
582:09 - can be created and they are used within
582:11 - i am policies in order to um
582:14 - grant or deny access to stuff
582:16 - you go ahead and delete that resource
582:18 - group and really aws stop moving that on
582:20 - me if you move one more time i'm just
582:22 - never going to talk about resource
582:23 - groups again okay
582:28 - hey this is andrew brown from exam pro
582:30 - and we're taking a look at business
582:31 - centric services and you might say well
582:34 - why because an exam guide it explicitly
582:36 - says that these are not covered but the
582:38 - thing is is that when you're taking the
582:40 - exam some of the choices might be some
582:43 - of these services as distractors and if
582:45 - you know what they are it's going to
582:47 - help make sure that you um
582:49 - guess correctly and the thing is that
582:51 - some of these services are useful you
582:53 - should know about them so that's another
582:55 - reason why i'm talking about them here
582:57 - so the first one is amazon connect this
582:59 - is a virtual call center you can create
583:00 - workflows to write callers you can
583:02 - record phone calls manage a queue of
583:04 - callers based on the same proven system
583:06 - used by amazon customer service teams we
583:09 - have workspaces this is a virtual remote
583:10 - desktop service secure managed service
583:13 - for provisioning either windows or linux
583:14 - desktops in just a few minutes which
583:16 - quickly scales up to thousands of
583:18 - desktops we have workdocs which is a
583:20 - shared collaboration service a
583:22 - centralized storage to share content and
583:23 - files it is similar to microsoft
583:25 - sharepoint think of it as a shared
583:26 - folder where the company has ownership
583:29 - we have chime which is a video
583:30 - conference service it is similar to zoom
583:32 - or skype you can screen share have
583:34 - multiple people on the on the same call
583:36 - it is secure by default and can show you
583:38 - a calendar of upcoming calls we have
583:40 - work mail this is a managed business uh
583:42 - email contacts calendar service with
583:44 - support of existing desktop and mobile
583:46 - email client applications that can
583:49 - handle things like imap similar to gmail
583:51 - or exchange we have pinpoint this is a
583:53 - marketing campaign management service
583:55 - pinpoint is for sending targeted emails
583:58 - via sms push notifications voice
584:00 - messages so you can perform um a to b
584:03 - testing or create journey so complex
584:05 - email response workflows we have ses
584:08 - this is a transactional email service
584:10 - you can integrate ses into your
584:12 - application to send emails you can
584:14 - create common templates track open rates
584:16 - keep track of your reputation we have
584:18 - quicksite this is a business
584:19 - intelligence service connect multiple
584:21 - data sources and quickly visualize data
584:23 - in the form of graphs with little to no
584:25 - knowledge definitely you want to
584:26 - remember quicksite ses pinpoint
584:29 - because those definitely will show up in
584:31 - the exam the rest probably not but they
584:32 - could show up as distractors okay
584:35 - [Music]
584:39 - hey this is andrew brown from exam pro
584:41 - and we are taking a look at provisioning
584:42 - services so let's first define what is
584:44 - provisioning so provisioning is the
584:46 - allocation or creation of resources and
584:48 - services to a customer and its
584:50 - provisioning services are responsible
584:51 - for setting up and managing those awes
584:53 - services we have a lot of services that
584:55 - do provisioning most of them are just
584:57 - using cloud formation underneath which
584:58 - we'll mention here but let's get to it
585:00 - the first is elastic bean stock this is
585:02 - a platform as a service to easily deploy
585:04 - web apps eb will provision various
585:06 - adwords services like ec2 s3 sns cloud
585:09 - watch ec2 auto scaling groups load
585:11 - balancers
585:12 - and you can think of it as the heroku
585:14 - equivalent to aws then you have opsworks
585:17 - this is a configuration management
585:18 - service that also provides managed
585:20 - instances of open source configuration
585:22 - managed software such as chef and public
585:23 - puppet so you'll say i want to have a
585:26 - load balancer or i want to have servers
585:28 - and it will provision those for you
585:30 - indirectly then you have cloudformation
585:32 - itself this is an infrastructure
585:34 - modeling and provisioning service it
585:36 - automates the provisioning of aws
585:37 - services by writing cloud formation
585:38 - templates in either json or yaml and
585:40 - this is known as iac or infrastructures
585:43 - of code you have quick starts these are
585:45 - pre-made packages that can be launched
585:47 - and configure your abus compute network
585:49 - storage and other services required to
585:50 - deploy a workload on the bus we do cover
585:53 - this in this course but quick starts is
585:55 - basically just confirmation templates
585:57 - that are authored by the community or
585:59 - by um
586:01 - amazon partner network okay
586:02 - then we have abs marketplace this is a
586:04 - digital catalog for thousands of
586:05 - software listings of independent
586:07 - software vendors that you can use to
586:08 - find buy and test and deploy software so
586:10 - the idea is that you know you can go
586:12 - there and provision whatever kind of
586:13 - resource you want we have abs amplify
586:16 - this is a mobile web app framework that
586:18 - will provision multiple able services as
586:20 - your backend it's specifically for
586:22 - serverless services i don't know i
586:23 - didn't write that in there
586:25 - but you know like dynamodb um
586:28 - things like uh whatever the graphql
586:30 - service is called api gateway things
586:32 - like that
586:33 - then we have aws app runner this is a
586:35 - fully managed service that makes it easy
586:36 - for developers to quickly deploy
586:38 - containerized web apps and apis at scale
586:41 - with no prior information experience
586:42 - required it's basically a platform as a
586:44 - service but for containers
586:47 - we have abas copilot this is a command
586:49 - line interface that enables customers to
586:50 - quickly launch and manage containerized
586:52 - applications any bus it basically is a a
586:56 - cli tool that sets up a bunch of scripts
586:58 - to set up pipelines for you makes things
587:00 - super easy we have aws codestart this
587:02 - provides a unified user interface
587:03 - enabling you to manage your software
587:05 - development activities in one place
587:07 - usually launch common types of stacks
587:08 - like lamp then we have cdk and so this
587:11 - is infrastructure as a code tool allows
587:13 - you to use your favorite programming
587:14 - language generates that confirmation
587:16 - templates as a means of ic so there you
587:19 - go
587:19 - [Music]
587:23 - hey this is andrew brown from exam pro
587:25 - and we're taking a look at aws elastic
587:27 - beanstalk before we do let's just define
587:29 - what passes so platform as a service
587:31 - allows customers to develop run and
587:33 - manage applications without the
587:35 - complexity of building and maintaining
587:36 - the infrastructure typically associated
587:38 - with developing and launching an app and
587:41 - so elastic bean stock is a pass for
587:43 - deploying web apps with little to no
587:45 - knowledge of the underlying
587:46 - infrastructure so you can focus on
587:48 - writing application code instead of
587:50 - setting up an automated deployment
587:51 - pipeline or devops tasks the idea here
587:54 - is you choose a platform upload your
587:56 - code and it runs with little knowledge
587:58 - of the infrastructure and aws will say
588:00 - that it's generally not recommended for
588:02 - production apps but just understand that
588:03 - they are saying this for enterprises and
588:05 - large companies
588:06 - if you're a small to medium company you
588:08 - can run elastic beanstalk for quite a
588:10 - long time it'll work out great elastic
588:12 - being stock is powered by cloudformation
588:13 - templates and it sets up for you elastic
588:16 - load balancer asgs
588:18 - rds ec2 instances pre-configured for
588:21 - particular platforms uh monitoring
588:23 - integration with cloudwatch sns
588:26 - deployment strategies like in-place
588:28 - blue-green deployment has security built
588:30 - in so it could rotate out your passwords
588:32 - for your databases and it can run
588:34 - dockerized environments and so when we
588:36 - talk about platforms you can see we have
588:38 - docker multi-container docker
588:41 - go.net java node.js ruby php python
588:44 - tomcat go a bunch of stuff and just to
588:47 - kind of give you that architectural
588:48 - diagram to show you that it can launch
588:50 - of multiple things okay
588:52 - [Music]
588:56 - hey it's andrew brown from exam pro and
588:58 - in this follow along we're going to
588:59 - learn all about elastic bean stock maybe
589:02 - not everything but we're going to
589:03 - definitely know how to at least
589:06 - use the service so elastic beanstalk is
589:08 - a platform as a service and what it does
589:10 - is it allows you to uh deploy web
589:12 - applications very easily so here i've
589:15 - made my way over to elastic beanstalk
589:16 - open environment and app and then we set
589:19 - up our application
589:20 - we have two tiers a web server
589:22 - environment a worker environment worker
589:23 - environment's great for long running
589:25 - workloads
589:26 - performing background jobs and things
589:28 - like that and then you have your web
589:30 - server which is your web server and you
589:32 - can have both and it's generally
589:33 - recommended to do so um but anyway what
589:36 - we'll do is create a new application so
589:37 - let's say my app here and there's some
589:41 - tags we can do and then it will name
589:43 - based on the environment then we need to
589:45 - choose an environment name so let's say
589:46 - my environment and just put a bunch of
589:48 - numbers in there hit the check
589:50 - availability scroll on down and we have
589:51 - two options manage platform custom
589:54 - platform and i'm not sure why custom is
589:56 - blanked out but it would allow you to
589:59 - um it would allow you to i think use
590:01 - your own containers so i'm a big fan of
590:03 - ruby so i'm gonna drop down to ruby
590:06 - and here we have a bunch of different
590:07 - versions and so 2.7 is pretty pretty new
590:10 - which is pretty good
590:12 - and then there's the platform version
590:13 - which is fine and the great thing is it
590:14 - comes with a sample application now you
590:16 - could hit create environment but you'd
590:18 - be missing out on a lot if you don't hit
590:20 - this configure more options i don't know
590:21 - why they put it there it's a not very
590:23 - good ui but
590:25 - if you click here you actually get to
590:26 - see everything possible and so up here
590:29 - we have some presets where we can have a
590:30 - single instance so
590:32 - this is where it's literally running a
590:34 - single ec2 instance so it's very cost
590:35 - effective you can have it with spot spot
590:37 - pricing so you save money
590:39 - there's high availability so you know if
590:42 - you want it set up with a load balancer
590:43 - an auto scaling group it will scale very
590:45 - well or you can do custom configuration
590:48 - we scroll on down here
590:50 - you can enable amazon x-ray you can
590:52 - rotate out logs you can do log streaming
590:56 - um there's a lot of stuff here
590:58 - and basically it's just like it sets up
591:01 - most for you but you can pretty much
591:02 - configure what you want as well if we
591:04 - have the load bouncer set if i go here
591:06 - go to high availability now we'll be
591:08 - able to change our load balancer options
591:10 - you have different ways of deploying so
591:12 - you can go here and then change it from
591:14 - all at once rolling immutable traffic
591:16 - splitting depends on what your use case
591:18 - is
591:20 - we can set up a key pair to be able to
591:22 - log into the machine
591:25 - there's a whole variety of things you
591:27 - can connect your database as well so it
591:29 - can create the database alongside with
591:31 - it and then it can actually rotate out
591:33 - the key so you don't have to worry about
591:34 - it which is really nice what i'm going
591:36 - to do is go to the top here and just
591:37 - choose a single instance because i want
591:39 - this to be very cost effective we're
591:40 - going to go ahead and hit create
591:42 - environment
591:44 - and so we're just going to wait for that
591:46 - to start up and i'll see you back when
591:48 - it's done okay
591:51 - okay so it's been uh quite a while here
591:53 - and it says a few minutes so if it does
591:55 - do this what you can do is just give it
591:57 - a hard refresh i have a feeling that
591:58 - it's already done is it done
592:01 - yeah it's already done so and here it
592:03 - says on september 2020 elasticity so i
592:05 - can use etc default default i don't care
592:08 - but anyway so this application i guess
592:10 - it's an appending state
592:12 - i'm not sure why let's go take a look
592:14 - here causes instance has not sent any
592:17 - data since launch
592:19 - none of the instances are sending data
592:20 - so that's kind of interesting because
592:22 - um
592:24 - i shouldn't have any problems you know
592:25 - what i mean
592:26 - so what i'm going to do is just reboot
592:28 - this machine and see if that fixes the
592:29 - issue there but usually it's not that
592:31 - difficult because it's the sample
592:32 - application it's not up to me
592:34 - um as to how to fix this
592:37 - you know what i mean so
592:39 - i'm not sure but um what we'll do is
592:43 - we will let the machine reboot and see
592:45 - if that makes any difference okay
592:47 - all right so after rebooting that
592:48 - machine now it looks like the server is
592:49 - healthy so it's not all that bad right
592:52 - if you do run in issues that is
592:53 - something that you can do
592:55 - and so
592:56 - uh let's go see if this is actually
592:58 - working so the top here we have a link
593:00 - and so i can just right click here it
593:02 - says congratulations your first aws
593:04 - elastic beanstalk ruby application is
593:06 - now running so it's all in good shape
593:09 - there's a lot of stuff that's going on
593:11 - here in elastic beanstalk that we can do
593:13 - we can go back to our configuration and
593:15 - change any of our options here so
593:17 - there's a lot of stuff as you can see
593:19 - we get logging so click the request log
593:22 - so if we click on this and say last 100
593:24 - lines
593:26 - we should be able to get logging data we
593:29 - have to actually download it i wish it
593:30 - was kind of in line but here you can
593:32 - kind of see what's going on so we have
593:34 - sdo access logs error logs puma logs
593:36 - elastic bean stock engine so you could
593:39 - use that to debug very common to take
593:41 - that over to
593:42 - support if you do have issues
593:44 - for health it monitors the health of the
593:46 - instances which is great then we have
593:49 - some
593:50 - monitoring data here so it gives you
593:53 - like a built dashboard so that's kind of
593:55 - nice you can set up alarms um you have
593:57 - not defined any alarms you can add them
593:59 - via the monitoring dashboard so i guess
594:01 - you'd have to
594:03 - you'd have to somehow add them i don't
594:05 - think i've ever added alarms for um
594:08 - elastic beanstalk but it's nice to know
594:09 - that they have them
594:11 - you can set up schedules for managed
594:13 - events then this is event data so it's
594:15 - just kind of telling you it's kind of
594:16 - like logs it just tells you of things
594:18 - that have changed
594:20 - so there's stuff like that what i'm
594:22 - looking for is to see how i can download
594:24 - the existing application
594:27 - because there's a version uploaded here
594:29 - oh the source is over here okay
594:32 - so
594:34 - i think it's probably over here the one
594:35 - that's running
594:37 - so that's it
594:40 - if it was easy to find what i probably
594:42 - would do is just modify it and oh yeah
594:44 - it's over here so if we go here and
594:46 - download the zip
594:50 - i wonder if it'd be even worth um
594:52 - playing with us so let's i'm just going
594:53 - to see if we can go over to cloud9
594:56 - and give this a go quickly
594:59 - so if we go over and launch a cloud9
595:01 - environment maybe we can tweak it and
595:03 - upload a revised version so we'll say
595:06 - create new we'll say eb
595:08 - um
595:11 - environment for elastic beanstalk
595:13 - we'll set it all the defaults that's all
595:15 - fine it's all within the free tier we'll
595:16 - create that environment
595:18 - what i'm going to do is just take this
595:20 - ruby zip file and move it to my desktop
595:23 - and as that is loading we'll give it a
595:25 - moment here i'm just going to go back
595:27 - and i was just curious does it let you
595:28 - download it directly from here no
595:31 - the only thing is that you know if you
595:33 - download that application
595:35 - elastic beanstalk usually has a
595:36 - configuration file with it and so i
595:38 - don't know if they would have given that
595:40 - to us
595:41 - but if it did that would be really great
595:44 - but we just have to wait for that to
595:46 - launch there as well
595:48 - i guess you can save configurations and
595:50 - roll back on those as well
595:54 - um but we will just wait a moment here
595:58 - while it's going i might just peek
596:00 - inside of this file to see what it is
596:02 - this zip contains
596:04 - just going to go my desktop here open up
596:06 - that zip
596:08 - so it looks pretty simple it doesn't
596:10 - even look like a rails app it looks like
596:11 - maybe it's a sinatra app i thought
596:13 - before that they would it would have
596:14 - deployed a ruby on rails application but
596:16 - maybe they keep it really simple
596:18 - um
596:21 - i don't see
596:22 - usually it's like yaml files they use
596:24 - for configuration i don't see that there
596:28 - so
596:29 - it might be that the default settings
596:31 - will work fine there's a config.ru and
596:33 - stuff like that but once cloud9 is up
596:35 - here we will upload this and see what we
596:38 - can do with it okay so there we go
596:40 - cloud9 is ready to go and so if we right
596:42 - click here whoops right click here we
596:44 - should be up be able to upload a file if
596:46 - not we can go up here to the top
596:49 - or it's here or there
596:52 - where is the upload i've i've uploaded
596:54 - things in here so i absolutely know we
596:56 - can i just gotta find it
597:01 - is that the upload
597:06 - upload files cloud9
597:16 - oh boy that's not helpful
597:18 - that's not helpful at all
597:20 - so let me just click around a little bit
597:22 - here i mean worst case i can always just
597:24 - bring it in via a curl oh upload local
597:26 - files there it is
597:27 - i was just not um being patient okay so
597:30 - we'll drag that on in there
597:32 - and we will
597:34 - did it upload yep it's right there okay
597:36 - great and so we need to unzip it so what
597:38 - i'll do is just drag this on up here
597:40 - i'll do an ls and we'll say unzip
597:43 - ruby.zip
597:45 - and so that unzipped the contents there
597:47 - i think the readme was part of cloud9 so
597:51 - i'm going to go ahead and delete that
597:52 - out
597:53 - not that it's going to hurt anything
597:55 - and so now what we can do we'll delete
597:56 - the original
597:59 - original zip there
598:01 - um and let's see if we can make a change
598:04 - here so i'm just going to open up see
598:05 - what it is so it's yeah it's running
598:07 - sinatra so that's pretty clear there
598:09 - we have a profile to see how it runs we
598:11 - have a worker sample so that just tells
598:14 - how the requests go
598:15 - you don't need to know any of this i'm
598:16 - just kind of clicking through it because
598:17 - i know ruby very well we have a cron
598:20 - yaml file so that could be something
598:22 - that gets loaded in here so i think
598:24 - basically a sinatra app probably just
598:26 - works
598:27 - off the bat here but if we want to make
598:28 - a change we probably just mix up a
598:29 - change over to here
598:31 - so i'll go down here and this is your
598:35 - second
598:36 - aws elastic bean stock application so
598:38 - the next thing we need to do is actually
598:39 - zip the contents here
598:41 - i don't know if it would let us zip it
598:43 - within here but also look like zip the
598:46 - contents of a directory
598:50 - linux
598:51 - just goes to show
598:53 - google is everything
598:55 - so the easiest way to zip a folder
599:00 - um
599:07 - zip
599:08 - everything in the current directory
599:13 - linux
599:18 - okay that's easy so
599:20 - we'll go back over here
599:22 - and we will type in zip
599:25 - and it wants hyphen r for recursive
599:27 - which makes sense
599:29 - and then the name of the zip so
599:34 - ruby2.zip
599:37 - and we'll do period
599:42 - zip warning found is
599:44 - who is
599:45 - zip
599:48 - oh
599:49 - uh
599:50 - yum install zip
599:53 - maybe we have to install uh zip but
599:54 - maybe it's not installed
599:57 - pseudo yum install zip
600:00 - since amazon likes to uses yum
600:03 - and so package already installed so i'm
600:05 - gonna type zip again so zip is there now
600:07 - great oops don't need install twice
600:13 - zip warning ruby two zip not found or
600:16 - empty
600:25 - okay so install zip and use zip hyphen r
600:29 - you can use the flag to best
600:31 - compensate
600:34 - so if that's not working what i'm going
600:36 - to do is just go up a directory
600:42 - why is it saying not found or empty
600:54 - hmm
601:01 - maybe i need to use
601:07 - okay so i think the problem was i was
601:09 - using the wrong flag so i put f instead
601:11 - of r i don't know why i did that so i
601:13 - probably should have done this
601:15 - okay and so that should have copied all
601:16 - the contents of that file so what i'm
601:18 - going to do is go ahead whoops make sure
601:20 - i have that selected and download that
601:21 - file
601:23 - and once i have downloaded that file i'm
601:25 - going to just open the contents to make
601:26 - sure it is what i expect it to be
601:29 - so we're going to open that up and oops
601:32 - get out of here winrar and it looks like
601:34 - everything i want so
601:36 - what i'm going to do is go back over to
601:38 - here i'm going to make sure i have my
601:40 - ruby 2 on my desktop
601:43 - and we're going to see if we can upload
601:44 - another version here so upload deploy
601:46 - choose the file we're gonna go all the
601:48 - way to my desktop here and we're gonna
601:50 - choose ruby two
601:52 - and
601:53 - um like ruby two will be the version
601:55 - name or we'll just say two
601:57 - and we'll deploy and we'll see if that
601:59 - works
602:00 - okay but there are like uh elastic being
602:02 - stock configuration files like gamble
602:04 - files that can sit in the root directory
602:06 - and so generally you're used to seeing
602:07 - them there but you know i imagine that
602:09 - databus probably engineered these
602:11 - examples so that it uses all the default
602:12 - settings but once this is deployed i'll
602:15 - see you back here in a moment okay
602:17 - after a short little wait it looks like
602:18 - it has deployed so what i'm going to do
602:20 - is just close my other tabs here and
602:22 - open this up and see if it's worked it
602:24 - says your second awesome beanstalk ruby
602:27 - application so
602:29 - we were successful uh deploying that out
602:31 - which is really great so what we can do
602:33 - now is just close that tab there
602:35 - and since we have that cloud no
602:36 - environment it will shut down on its own
602:39 - but you know just for your benefit i
602:41 - think that we should shut it off for
602:42 - right now so go ahead and delete that
602:45 - i'm going to go back over to elastic
602:46 - bean stock here and i just want to
602:49 - destroy all of it so we'll see if we can
602:51 - just do that from here terminate the
602:53 - application
602:55 - enter the name
602:57 - so i think we probably have to enter
602:58 - that in there
603:01 - and so i think that
603:02 - oh a problem occurred right
603:05 - exceeded
603:07 - what
603:09 - let's say aws for you so it's not a big
603:11 - deal i would just go and check it again
603:14 - and maybe what we'll do is just delete
603:15 - the application first
603:21 - okay so that one is possibly deleting
603:28 - let's go in here is anything changing
603:33 - can't even tell
603:35 - we'll go ahead oh can't take that one
603:37 - out
603:50 - delete application again
603:53 - if it takes a couple times it's not a
603:54 - big deal
603:57 - it's aws for yes so
603:59 - there's a lot of moving parts so it
604:01 - looks like it is terminating the
604:02 - instance and so we just have to wait for
604:04 - that to complete
604:06 - uh once that is done we might have to
604:07 - just tear down the environment so i'll
604:08 - see you back here when it has finished
604:10 - tearing this down okay all right so
604:12 - after a short little wait here i think
604:13 - it's been destroyed we'll just double
604:15 - check by going to the applications going
604:16 - to the environments yeah and it's all
604:18 - gone probably because i initially
604:20 - deleted that environment and then took
604:21 - the application with it so i probably
604:22 - didn't have to delete the app separately
604:25 - um but uh yeah so there you go just make
604:27 - sure your cloud9 environment's gone and
604:29 - you are a-okay there'll probably be some
604:31 - like lingering s3 buckets so if you do
604:33 - want to get rid of those you can it's
604:34 - not going to hurt anything having those
604:36 - around
604:37 - but they do tend to stack up after a
604:39 - while which is kind of annoying so if
604:41 - you don't like them you can just empty
604:42 - them out
604:44 - as i am doing here whoops
604:47 - i'll just permanently delete
604:51 - copy that text there
604:53 - then go back
604:57 - to here and then just go
604:59 - take out that bucket
605:01 - let's delete that there
605:06 - oh if you get this this is kind of
605:08 - annoying but uh elastic beanstalk likes
605:10 - to put in an imp permission or policy in
605:13 - here so if you go down here there's a
605:14 - bucket policy you just have to delete it
605:16 - out it prevents it from being deleted
605:20 - and we'll go back over here
605:22 - and then we will delete it
605:26 - okay
605:27 - and yeah there we go that's it
605:32 - [Music]
605:34 - so let's take a look at several services
605:36 - on aws and this is not including all of
605:38 - them because we're looking at the most
605:40 - purely serverless services uh if we try
605:43 - to include all the server services it
605:45 - would just be too long of a list
605:47 - but let's take a look here so
605:49 - before we do let's just redefine what is
605:51 - serverless so when the underlying
605:52 - servers infrastructure operating system
605:54 - is taken care by the csp serverless is
605:56 - generally by default highly available
605:58 - scalable cost effective you pay for what
606:00 - you use the first one is dynamodb which
606:02 - is a serverless nosql key value and
606:04 - document database it's designed to scale
606:06 - to billions of records with guaranteed
606:08 - consistent data returned in at least a
606:10 - second you do not have to worry about
606:12 - managing charge you have simple storage
606:15 - service s3 which is a serverless object
606:17 - storage service you can upload very
606:19 - large and unlimited amounts of files you
606:21 - can pay for what you store you don't
606:23 - worry about the underlying file system
606:24 - we're upgrading the disk size we have
606:27 - ecs fargate which is a servless
606:28 - orchestration container service is the
606:31 - same as ecs except you pay on demand per
606:34 - running container with ecs you have to
606:36 - keep a ec2 server running even if you
606:39 - have no containers running where aws
606:41 - manages the underlying server so you
606:42 - don't have to scale or upgrade the ec2
606:45 - server
606:46 - we have aws lambda which is a serverless
606:48 - function service you can run code
606:50 - without provisioning or managing servers
606:52 - you upload a small piece of code choose
606:55 - how much memory you want how long you
606:57 - want the function is allowed to run
606:58 - before timing out your charge based on
607:00 - the runtime of the service function
607:02 - rounded to the nearest 100 milliseconds
607:04 - we have step functions this is the state
607:06 - machine service
607:08 - it coordinates multiple services into
607:11 - serverless workflows easily share data
607:13 - among lambdas
607:15 - have a group of lambdas wait for each
607:17 - other create logical steps also work
607:19 - with fargate tasks we have aurora
607:21 - serverless this is a serverless
607:22 - on-demand version of aurora so when you
607:24 - want most of the benefits of aurora but
607:27 - trade you have to trade off those cold
607:29 - starts or you don't have lots of traffic
607:31 - or demand so things several services
607:33 - that we could have put in here as well
607:34 - is like api gateway appsync it was
607:38 - amplify um and those are like the the
607:41 - first two were application integrations
607:43 - you could say sqs sns those are all
607:46 - serverless services but you know again
607:48 - we'd be here all day if i i listed them
607:50 - all right
607:54 - [Music]
607:55 - all right let's take a look at what is
607:57 - serverless and we did look at it from a
607:59 - server perspective earlier in the course
608:01 - but let's just try to abstractly define
608:03 - it and talk about the architecture so
608:05 - serverless architecture generally
608:07 - describes fully managed cloud services
608:09 - and the classification of a cloud
608:11 - service being serverless is not a
608:13 - boolean answer it's it's not a yes or no
608:16 - but an answer on a scale where a cloud
608:18 - service has a degree of serverless and i
608:20 - do have to point out that this
608:22 - definition might not be accepted by um
608:25 - everybody because serverless is one of
608:27 - those uh terms
608:29 - where
608:30 - we've had a bunch of different cloud
608:32 - service providers define it differently
608:34 - and then we have thought leaders that
608:35 - have
608:36 - a particular concept of what it is so
608:38 - you know i just do my best to try to
608:40 - make this practical here for you but a
608:42 - servless service could have all or most
608:45 - of the following characteristics and so
608:47 - it could be highly elastic and scalable
608:49 - highly available highly durable secure
608:52 - by default it abstracts away the
608:54 - underlying infrastructure and are built
608:56 - based on the execution of your business
608:58 - tasks a lot of times that that cost is
609:01 - not
609:02 - uh it's not always represented as
609:04 - something that is like i'm paying x for
609:07 - compute it could be abstracted out into
609:10 - some kind of um credit that doesn't
609:12 - necessarily map to something physical
609:15 - then we have serverless can scale to
609:16 - zero meaning when it's not in use the
609:19 - serverless resources cost nothing uh and
609:22 - these two last topics basically pull
609:24 - into pay for value so you don't pay for
609:26 - idle servers you're paying for the value
609:30 - that your service provides
609:32 - and my friend daniel who runs the
609:34 - serverless toronto group he likes to
609:36 - describe serverless as being similar to
609:38 - like energy efficient rating so an
609:41 - analogy of service could be similar to
609:42 - energy rating labels which allows
609:44 - consumers to compare the energy
609:46 - efficiency of a product so some services
609:48 - are more serverless than others and
609:50 - again you know some people might not
609:52 - agree with that where there's a
609:54 - definitive yes or no answer but i think
609:56 - that's the best way to look at it okay
609:58 - [Music]
610:02 - hey it's andrew brown from exam pro and
610:04 - we're taking a look at windows on
610:06 - database so abs has multiple cloud
610:08 - services and tools to make it easy for
610:09 - you to run window workloads on aws so
610:12 - let's get to it so the first is windows
610:14 - servers on dc2 so you can select from a
610:16 - number of windows server versions
610:17 - including the latest version
610:19 - like windows server 2019
610:22 - for
610:23 - databases we have sql server on rds you
610:25 - can select from a number of sql server
610:27 - database versions then we have aws
610:29 - directory service which lets you run
610:31 - microsoft active directory ad as a
610:33 - managed service we have aws license
610:36 - manager which makes it easier to manage
610:37 - your software licenses from software
610:40 - vendors such as microsoft we have amazon
610:42 - fsx for windows file server which is a
610:44 - fully managed scalable storage built for
610:47 - windows we have the aws sdk which allows
610:50 - you to write code in your favorite
610:51 - language to interact with a database api
610:53 - but it specifically has support for net
610:56 - a language favorite for windows
610:57 - developers we have amazon workspaces so
611:00 - this allows you to run a virtual desktop
611:02 - you can launch a windows 10 desktop to
611:04 - provide secure and durable workstations
611:07 - that is accessible from wherever you
611:09 - have an internet connection about lambda
611:11 - supports powershell as a programming
611:12 - language to write your serverless
611:14 - functions and we have abs migration
611:17 - acceleration program map for
611:19 - windows is a migration methodology for
611:21 - moving large enterprises items has
611:24 - amazon partners that specialize in
611:26 - providing professional services for map
611:28 - this is not just
611:29 - everything for windows on aws like if
611:32 - you want to move your sql server over to
611:35 - rds postgres i believe they've like
611:38 - created an adapter to do that
611:41 - but yeah hopefully that gives you an
611:42 - idea what you can do with windows on aws
611:44 - okay
611:44 - [Music]
611:49 - hey this is andrew brown from exam pro
611:50 - and i want to show you how you can
611:51 - launch a windows server on aws so what
611:55 - you're going to do is go to the top here
611:56 - and we are going to type in ec2 and from
611:59 - here uh what we'll do is we'll go ahead
612:02 - and launch ourselves a new ec2 instance
612:06 - and we are going to have
612:08 - a selection of instances that we can
612:10 - launch and so we're looking for the
612:12 - microsoft
612:13 - windows
612:14 - server and this is interesting there's
612:16 - actually a free tier
612:18 - eligible that is crazy because if you go
612:20 - over to azure they don't have a free
612:22 - tier windows server like any bus does
612:25 - so that's pretty crazy um and it runs on
612:28 - a t2 micro no that can't be right
612:31 - there's no way
612:33 - it can run a tt micro that seems like
612:34 - that's too small
612:36 - let's try it okay i just don't believe
612:38 - it because when you use azure you have
612:39 - to choose a particular size of instance
612:41 - by default
612:42 - and it's a lot more expensive and there
612:44 - is no free tier
612:45 - so we'll go here
612:47 - there are free tiers just not really for
612:48 - windows in particular so we'll go here
612:51 - this looks good security groups this
612:53 - opens up rdp so we can get into that
612:55 - machine we're gonna go next here
612:57 - and launch this machine
613:00 - says if you plan to use ami the benefits
613:03 - the microsoft license mobility check out
613:05 - this form
613:06 - that's not something we're worried about
613:07 - today
613:08 - and
613:10 - i mean i guess we can create a key pair
613:12 - i'm not sure what it we would use a key
613:14 - pair for here
613:16 - um for windows amis the private key file
613:18 - is required to obtain the password used
613:19 - to log into the instance okay so i guess
613:21 - we're going to need it so
613:23 - windows key
613:27 - great we'll launch that instance
613:30 - and uh i'll see you back here when it
613:32 - launches but i just don't believe that
613:34 - it would launch that fast you know
613:36 - all right so after a short little wait
613:38 - here the server is ready and so let's
613:40 - see if we can actually go ahead and
613:42 - connect to this so i'm going to hit
613:43 - connect here
613:44 - and we'll go over to rdb client so you
613:46 - connect to your windows instance using
613:48 - your remote desktop client of your
613:49 - choice and downloading and running the
613:51 - rdb shortcut below so i'm going to go
613:53 - ahead and download this and you're going
613:55 - to have to be
613:57 - on a
613:59 - windows machine to be able to do this or
614:00 - have an rdb client installed i think
614:02 - there's one for mac that you can get
614:03 - from the apple store
614:05 - but all i'm going to do is just double
614:07 - click the file so you probably can't see
614:11 - it here i'm just going to expand this
614:14 - trying to
614:15 - oh my computer is being silly but anyway
614:17 - there we go we moved it over there i'm
614:18 - just going to drag over here and just
614:20 - double click this image so you can see
614:22 - that i'm doing it i'm saying connect
614:25 - okay
614:26 - and that's going to ask for a password
614:28 - so i'm going to hope that i can just
614:30 - click that and get the password so to
614:32 - decrypt the password you will need your
614:34 - key pair instance you'll have to upload
614:36 - that
614:37 - and i don't know if i remember having to
614:38 - do that before but it's a great security
614:41 - measure so i'm fine with it i'm going to
614:42 - drag my key to my desktop so i can see
614:44 - what's going on there as well
614:48 - and we're going to go grab that and
614:49 - decrypt the password
614:51 - and so now
614:53 - um
614:55 - where's our password oh it's right here
614:56 - okay so we're going to grab that
614:58 - password there
615:00 - we will paste that in
615:03 - said okay
615:05 - say yes
615:06 - and see if we can connect to this
615:08 - instance if this is running on a t2
615:10 - micro i'm going to lose it because that
615:12 - is just cheap
615:18 - it just just doesn't seem possible to me
615:20 - because again on azure you have to
615:22 - launch an instance with a lot of stuff
615:24 - and
615:24 - it just uh seems uh crazy what's also
615:27 - interesting is that itabus uh on windows
615:29 - like launches so fast it's unbelievable
615:32 - how fast these servers
615:33 - spin up and it's just very unusual but
615:36 - yeah so we are in here
615:38 - um
615:41 - it's not asking me to activate or
615:42 - anything so i guess there's already a
615:44 - windows license here
615:48 - and
615:48 - i'm not sure if there's any kind of like
615:50 - games installed like do we have
615:52 - minesweeper can i play minesweeper on
615:54 - here
615:57 - it's a data center server so i'm
615:59 - assuming not
616:00 - but yeah so this is a windows server and
616:02 - it's pretty impressive that this works
616:04 - i'm not sure if this is going to have an
616:05 - outbound connection here um just because
616:07 - we probably would have to configure it
616:09 - let's just say okay i just i really
616:11 - don't think it's going to go out to the
616:12 - internet by default
616:18 - yeah so you'd probably have to
616:21 - do some stuff you know
616:26 - oh no there we go so yeah we got to the
616:27 - internet so it's totally possible but uh
616:30 - yeah that's about it that's all i really
616:32 - wanted to show you so what i'm going to
616:33 - do is just go back to ec2 and we're
616:35 - going to shut down the server here just
616:38 - expand that there
616:42 - and we will go here and we will
616:45 - terminate that instance
616:48 - good we'll give that a refresh that's
616:50 - shutting down and we are done
616:52 - [Music]
616:56 - hey this is andrew brown from exam pro
616:58 - and we are taking a look at abyss
617:00 - license manager and before we do let's
617:02 - talk about what byol
617:04 - or bring your own license means so this
617:06 - is the process of reusing an existing
617:08 - software license to run vendor software
617:10 - on a cloud vendor's computing service
617:12 - byol allows companies to save money
617:15 - since they may have purchased the
617:16 - license in bulk or a time that provided
617:18 - a greater discount than if purchased
617:20 - again and so an example of this could be
617:22 - the license mobility provided by
617:24 - microsoft's volume licensing to
617:25 - customers with eligible server
617:28 - applications covered by the microsoft
617:29 - software assurance program uh and i
617:32 - don't know what i was trying to do there
617:32 - i guess maybe it was just sa and i
617:34 - missed the parentheses there on the end
617:35 - no big big deal
617:37 - but aws license manager is a service
617:39 - that makes it easier for you to manage
617:41 - your software licenses from software
617:43 - vendors centrally across aws in your
617:45 - on-premise environments able's license
617:47 - manager software that is licensed based
617:50 - on virtual cores
617:52 - physical cores sockets or a number of
617:53 - machines this includes a variety of
617:55 - software products for microsoft ibm sap
617:58 - oracle and other vendors so that's the
618:00 - idea you say what is my license type
618:02 - it's it's bound to this amount of cpus
618:05 - items license manager works with ec2
618:07 - with dedicated instances dedicated hosts
618:09 - and even spot instances and for rds
618:12 - there's only for oracle databases so you
618:14 - can import that license for your oracle
618:16 - server
618:17 - just understand that
618:19 - if you're doing microsoft windows
618:21 - servers or microsoft sql server license
618:23 - you're generally going to need a
618:24 - dedicated host because of the assurance
618:27 - program
618:28 - and this can really show up on your exam
618:30 - so even though ava's license manager
618:31 - works on dedicated instances and spot
618:33 - instances
618:34 - just trying to gravitate towards
618:36 - dedicated hosts on the server or on the
618:39 - exam okay
618:40 - [Music]
618:44 - all right let's take a look at the
618:46 - logging services that we have available
618:48 - in aws so the first one here is
618:49 - cloudtrail and this logs all api calls
618:52 - whether it's sdk or the cli so if it's
618:55 - making a call to the api it's going to
618:56 - get tracked between aws services and
618:58 - this is really useful to say who can we
619:00 - blame who was the person that did this
619:02 - so who created this bucket who spent up
619:04 - that expensive ec2 instance who launched
619:06 - the sagemaker notebook
619:08 - and the idea here is you can detect
619:10 - developer misconfigurations detect
619:12 - malicious actors or automate responses
619:14 - through the system then you have
619:16 - cloudwatch which is a collection of
619:17 - multiple services i commonly say this is
619:19 - like an umbrella service because it has
619:20 - so many things underneath it so we have
619:22 - cloudwatch logs which is a centralized
619:24 - place to store your cloud services log
619:26 - data and application logs metrics which
619:28 - represents a time ordered set of data
619:30 - points a variable to monitor
619:33 - event bridge or previously known as
619:35 - cloudwatch events triggers an event
619:37 - based on a condition so every hour take
619:39 - a snapshot of the server alarms triggers
619:41 - notifications based on metrics
619:43 - dashboards creates visualizations based
619:45 - on metrics and that's not all of the
619:47 - things that are under cloud watch but
619:48 - those are the core five ones you should
619:50 - always know um absolutely there then we
619:53 - have aws x-ray this is for distributed
619:55 - tracing systems so you can use it to
619:56 - pinpoint issues within your services so
619:59 - you see how data moves from one app to
620:01 - another how long it took to move and if
620:03 - it failed uh to move forward okay
620:06 - [Music]
620:10 - let's take a closer look here at ibis
620:11 - cloud trail because it's a very
620:12 - important service so it's a service that
620:15 - enables governance compliance
620:16 - operational auditing and risk auditing
620:18 - of your adwords account and the idea is
620:20 - that every time you make an api call
620:21 - it's going to show up as some kind of
620:23 - structured data that you can interact
620:25 - with or read through so this cloud trail
620:27 - is used to monitor api calls and actions
620:29 - made on the database account easily
620:31 - identify which users and accounts made
620:33 - the call to aws so you might have the
620:35 - where so the source ip address the when
620:37 - the event time the who the user agent
620:40 - and the what the region resource in
620:42 - action so i'm just gonna get my pen tool
620:44 - out here for a moment and just notice
620:46 - you have the event time so when it
620:47 - happened the source the name the region
620:50 - the source ip address the user agent uh
620:52 - who was doing it so here was laforge of
620:55 - the response element so you know it's
620:56 - very clear what is going on here
620:58 - um and then you know cloudtrail is
621:00 - already logging by default and we'll
621:02 - collect logs for the for the last 90
621:04 - days via event history if you need more
621:06 - than 90 days you need to create a trail
621:08 - which is very common you'll go into aws
621:10 - and make one right away trails are
621:11 - outputted to s3 and do not have gui like
621:14 - event history to analyze the trail you
621:16 - have to use amazon athena and i'm sure
621:18 - there are other ways to analyze it
621:19 - within aws but here's just what the
621:22 - event history looks like so right off
621:23 - the bat you can already see that there
621:25 - are information there i'm not sure if
621:27 - they've updated the ui there they might
621:28 - have uh
621:29 - as even when i'm recording this i kind
621:31 - of feel like if we go into the follow
621:33 - along which we will um i bet they might
621:35 - have updated that the idea here is that
621:36 - you know you can browse the last 90 days
621:39 - but anything outside of that you're
621:41 - gonna have to do a little bit of work
621:41 - yourself okay
621:42 - [Music]
621:47 - so we're not going to cover all the
621:48 - cloudwatch services there's just too
621:49 - many but let's look at the most
621:50 - important ones and one of the those
621:52 - important ones is cloudwatch alarms so
621:54 - cloudwatch alarms monitors a cloudwatch
621:56 - metric based on a defined threshold uh
621:58 - so here you can see there's kind of a
622:00 - condition being set there so if the
622:01 - networking is greater than 300 for one
622:03 - data point within five minutes it's
622:05 - going to breach an alarm so
622:07 - that's when it goes outside it's defined
622:09 - threshold and so the state's going to
622:10 - either be something like okay so the
622:12 - metric or expression is within the
622:13 - defined threshold so do nothing alarm
622:16 - the metric or expression is outside of
622:17 - the defined threshold so do something or
622:19 - insufficient data the alarm has just
622:22 - started the metric is not available not
622:24 - enough data is available and so when the
622:26 - state has changed you can define actions
622:28 - that it should take and so that could be
622:30 - doing a notification auto scaling group
622:32 - or an ec2 action um so cloudwatch alarms
622:35 - are really useful for a variety of
622:36 - reasons the one that we will come across
622:38 - right away will be setting up a billing
622:39 - alarm
622:41 - [Music]
622:45 - so let's take a look here at the
622:46 - autonomy of an alarm and so i have this
622:48 - nice graphic here to kind of explain
622:50 - that there and so the first thing is we
622:51 - have our threshold condition
622:53 - and so here you can just set a value and
622:55 - say okay the value is a thousand or a
622:57 - hundred whatever you want it to be and
622:59 - this is going to be
623:00 - for a particular metric the actual data
623:02 - we are measuring so maybe in this case
623:04 - we're measuring network in so the volume
623:06 - of incoming network traffic measured in
623:08 - bytes so when using five-minute
623:09 - monitoring divide by 300 we get bytes
623:11 - per second if you're trying to figure
623:12 - out that calculation there you have data
623:14 - points so these represent the metrics
623:16 - measurement at a given point then you
623:18 - have the period how often it checks to
623:20 - evaluate the alarm so we could say every
623:22 - five minutes
623:23 - uh you have the evaluation period so the
623:25 - number of previous periods and the data
623:27 - points to alarm so you can say one data
623:29 - point is breached in evaluation period
623:31 - going back four periods so this is what
623:34 - triggers the alarm
623:35 - uh the thing i just want you to know is
623:37 - that you can set a value right and that
623:39 - it's based on a particular metric and
623:40 - there is a bit of logic here in terms of
623:43 - the alarm so it's not as simple as just
623:45 - it's breached but there's this period
623:47 - thing happening okay
623:48 - [Music]
623:52 - well let's take a look at cloudwatch
623:54 - logs so to understand that we have logs
623:56 - streams and log groups so a log stream
623:58 - is a stream that represents a sequence
624:01 - of events from an application or
624:02 - instance being monitored so imagine you
624:04 - have an ec2 instance running a web
624:06 - application and you want those logs to
624:08 - be streamed to cloudwatch logs that's
624:10 - we're talking about here so you can
624:11 - create log streams manually but
624:13 - generally this is automatically done by
624:14 - the service you are using
624:16 - unless you were collecting application
624:18 - logs on an ec2 instance as i just
624:19 - described here is a log group of a
624:21 - lambda function you can see the log
624:23 - streams are named after the running
624:25 - instance lambda's free frequency run on
624:27 - new instances so the stream contains
624:29 - timestamps so what i'm trying to say
624:31 - here is that there's a variety of
624:32 - different services lambda rds what have
624:35 - you and they already send their logs to
624:37 - cloudwatch logs and and they're going to
624:39 - vary okay so here's a log group of an
624:41 - application log running on ec2 you can
624:43 - see here the log streams are named after
624:45 - the running instance id here is the log
624:47 - group for aws glue you can see the log
624:49 - streams are named after the glue jobs
624:52 - and so you know we have the streams but
624:54 - let's talk about the actual data that's
624:55 - made up of it the log events so this
624:57 - represents a single event in a log file
624:59 - log events can be seen within the log
625:01 - stream and so here's an example of you
625:04 - would open this up in cloudwatch logs
625:06 - and you can actually see what what was
625:08 - being reported back by your server you
625:09 - can filter these events to filter out
625:11 - logs based on simple or pattern matching
625:14 - syntax so here i'm just typing in saying
625:16 - give me all those debug stuff and you
625:18 - know this is a very robust but awes does
625:20 - have a better way of analyzing your logs
625:22 - which is log insights which we'll look
625:23 - at here in a moment
625:25 - [Music]
625:29 - so we're just looking at cloudwatch log
625:31 - events and how those are collected but
625:32 - there's an easier way to analyze them
625:34 - and that's with login sites so you can
625:36 - interactively search and analyze your
625:37 - cloudwatch log data and it has the
625:39 - following advantages more robust
625:40 - filtering than using the simple filter
625:42 - in the in a log stream less burdensome
625:44 - than having to export logs to s3 and
625:46 - analyze them via athena cloudwatch login
625:49 - site supports all types of logs so
625:50 - cloudwatch log insights is commonly used
625:53 - via the console to do ad-hoc queries
625:55 - against log groups
625:56 - so that's just kind of an example of
625:58 - someone writing a query
626:00 - and cloudwatch log insights uses a query
626:02 - syntax so a single request can query up
626:05 - to 20 logs create timeout after 50
626:08 - minutes if not completed
626:10 - and queries results are available for
626:12 - seven days so abras provides sample
626:14 - queries that you can get started for
626:16 - common tasks and and ease the learning
626:18 - into the query syntax a good example is
626:20 - filtering vpc flow logs so you go there
626:22 - you click it and you start getting some
626:24 - data you can create and save your own
626:26 - queries
626:27 - to make future repetitive tasks easier
626:29 - on the certified cloud partitioner
626:30 - they're not going to ask you all these
626:31 - details about this stuff but i just
626:33 - conceptually want you to understand that
626:35 - in log insights you can use it to
626:37 - robustly filter your logs based on this
626:39 - query syntax language you get this kind
626:41 - of visual and it's really really useful
626:47 - let's take a look here at cloudwatch
626:48 - matrix which represents a time ordered
626:50 - set of data points it's a variable that
626:52 - is monitored over time so cloudwatch
626:54 - comes with many predefined metrics that
626:55 - are generally namespaced by aw services
626:58 - uh so the idea is that like if we were
627:00 - to look at the ec2 it has these
627:02 - particular matrixes so that we have cpu
627:04 - utilization discrete ops disk write ops
627:07 - disk read bytes disk write bytes network
627:10 - in network out network packet in network
627:13 - packets out and the idea is that you can
627:15 - just like click there into ec2 and then
627:17 - kind of get that data there and so cloud
627:20 - metrics are leveraged by other things
627:22 - like cloudwatch events cloudwatch alarms
627:24 - cloudwatch dashboards so just understand
627:26 - that okay
627:30 - [Music]
627:31 - all right so what i want to do in this
627:32 - follow along is show you a bit about
627:34 - cloudtrail so we're going to go to the
627:36 - top here and type in cloudtrail the
627:38 - great thing about cloudtrail is it's
627:40 - already turned on by default so it's
627:42 - already kind of collecting some
627:43 - information and so it's here it says now
627:45 - use i am access analyzer on cloud trail
627:48 - trails that sounds pretty cool to me
627:50 - but we shouldn't have to create a trail
627:52 - right off the bat because we'll have
627:53 - some event history and the event history
627:55 - allows us to see
627:56 - things that are happening within our
627:57 - account in the last 90 days
628:00 - but the thing is if you want something
628:01 - beyond 90 days you're going to have to
628:02 - create a trail
628:04 - but if we just take a look here we can
628:05 - kind of see
628:06 - as we've been doing a lot of things all
628:08 - the kind of actions that's been
628:09 - happening so here we have an instance
628:10 - that i terminated so if i go in here and
628:13 - and look at it i can kind of see
628:15 - more information about it so we can see
628:18 - when it terminated who had done that
628:20 - what access key they had used the event
628:22 - source the request id
628:25 - the source ip what whether it was read
628:28 - only what was the event type that was
628:30 - called the resource there and this is
628:32 - the actual raw record so this is
628:34 - generally how i would look at it or this
628:35 - is how you had to look at it back in the
628:37 - day but the idea is that you would have
628:39 - that
628:40 - user identity described the event time
628:42 - the source the event name the region the
628:43 - source ip the the agent
628:46 - all the information there okay
628:48 - and so this is a great way to kind of
628:50 - find stuff so you can go through here
628:52 - and try to debug things this way so you
628:54 - can go to the event name
628:55 - and so if you if you go here you can
628:57 - kind of get
628:58 - uh see a bit of stuff here so
629:01 - if i was just trying to say like maybe
629:02 - create
629:03 - i'm just trying to find something that i
629:04 - know that i've been doing like create
629:06 - access keys i can see the access keys
629:08 - that have been created within this
629:10 - sandbox account here for the user and
629:12 - things like that so it's a great way to
629:14 - kind of find things but generally you're
629:16 - going to always want to turn on
629:18 - uh or create your own trail so if you go
629:20 - here and hit create trail say my new
629:22 - trail
629:23 - and um you're gonna need an s3 bucket
629:26 - for that you'll probably want encryption
629:27 - turned on
629:29 - which sounds good to me you'll
629:30 - absolutely want log file validation and
629:33 - generally you don't want to store your
629:34 - your cloudtrail logs within the existing
629:37 - account you want to have a isolated
629:39 - hardened account that's that is
629:42 - infrequently accessed or only by your
629:45 - your cloud security engineers
629:47 - away from here because you don't want
629:48 - people tampering with it deleting it or
629:50 - changing stuff
629:51 - but let's take an existing one here
629:54 - i don't want a customer manager don't i
629:57 - have one that is managed by aws here
629:59 - new custom
630:03 - um let's choose that one i don't know
630:05 - which one that is we'll just hit next
630:06 - usually adamus gives you a managed key
630:08 - there so i was kind of surprised
630:10 - you can also include additional data so
630:12 - if you do data events this would collect
630:13 - information from s3 but the thing is you
630:16 - might not want to track everything
630:18 - because if you track to everything it
630:19 - can get very expensive very quickly
630:22 - but if you don't you just leave on
630:23 - management events it'll save you more
630:24 - money there's inside events uh this is
630:26 - new i haven't seen this yet so i didn't
630:28 - identify unusual activity errors users
630:30 - of behavior that sounds really good but
630:33 - these could come also at additional
630:34 - charges but i'm going to hit next anyway
630:36 - for fun i'm going to create that trail
630:39 - okay
630:41 - and uh the key policy does not grant
630:43 - sufficient access to etc etc so i'm
630:46 - gonna go turn that off even though i
630:47 - should really have it turned on but i
630:49 - just want to be able to show you this
630:52 - okay so we have this new trail
630:54 - and so this trail is being dumped to s3
630:57 - so we might not be able to see anything
630:59 - in here as of yet but i'm just going to
631:01 - pop over here and just see
631:03 - right
631:05 - i probably have one in my other account
631:07 - but it's not
631:09 - it's not that important we basically saw
631:10 - what the data would look like so we go
631:12 - into here
631:13 - there's a digest i don't remember there
631:14 - being a digest so that's nice
631:17 - so there's no data yet but when there is
631:18 - it will pop into there
631:21 - um i'm not sure if we're gonna be able
631:22 - to do anything with insights here at
631:23 - least not in this account
631:26 - insights are events that are showing
631:27 - usual api activity and things like that
631:29 - so that's kind of cool i don't know what
631:31 - cloudwatch insights looks like
631:35 - uh inside events are shown in the table
631:38 - for 90 days okay so i'm just curious if
631:41 - we can see kind of a screenshot of what
631:42 - that looks like
631:44 - whoops
631:46 - well at least on the article here
631:49 - so i guess you could kind of get like
631:50 - some kind of graphs or something saying
631:51 - like hey
631:53 - this looks unusual and they might select
631:54 - it so
631:55 - not pretty clear in terms of what that
631:57 - looks like but i mean sounds like a cool
631:59 - feature and i'm sure when i i'm working
632:01 - on my security certification course i
632:03 - will definitely include them there but
632:05 - that's pretty much all there is to it
632:07 - i'm going to go ahead and delete
632:09 - that trail because i i just don't really
632:11 - need it in this account
632:13 - but generally you always want to go in
632:15 - and create a trail
632:16 - and what you can do is if you're in your
632:18 - root account i'm not this is actually a
632:20 - an account that's part of an
632:21 - organization but if you're at that
632:23 - organization level you can create a
632:24 - trail that ex that spans all the regions
632:27 - that spans all the interest accounts
632:28 - with an organization and that's what you
632:30 - should be doing okay
632:31 - but that's about it
632:36 - [Music]
632:37 - hey this is andrew brown from exam pro
632:39 - we're looking at ml and ai services on
632:42 - aws but let's first just define what is
632:44 - aiml and deep learning so ai also known
632:48 - as artificial intelligence is when
632:49 - machines that perform jobs that mimic
632:51 - human behavior
632:53 - ml or machine learning are machines that
632:55 - get better at a task without explicit
632:56 - programming
632:58 - and deep learning or dl are machines
633:00 - that are have an artificial neural
633:02 - network inspired by the human brain to
633:04 - solve complex problems and a lot of
633:06 - times you'll see this kind of onion
633:07 - where they're showing you that
633:10 - you know ai
633:11 - can be using ml or deep learning and
633:13 - then deep learning is definitely using
633:15 - machine learning but it's using neural
633:16 - networks and so for aws their flagship
633:19 - product here is amazon sagemaker it is a
633:21 - fully managed service to build train
633:23 - deploy machine learning models at scale
633:26 - um and there's a bunch of different kind
633:27 - of open source frameworks you can use
633:29 - with it like apache mx net audios which
633:32 - is an open source deep learning
633:33 - framework that is the one that it has
633:34 - decided to say hey we are going to back
633:36 - this one and so you'll see a lot of
633:38 - example code for that one we have
633:40 - tensorflow that you can use pie torch
633:43 - hugging face other things as well okay
633:47 - and so there's a lot of services
633:49 - underneath some that might be of
633:51 - interest to mention right away is like
633:52 - amazon sagemaker ground truth which is a
633:55 - data labeling service where you have
633:57 - humans that label a data set that will
633:59 - be used to train machine learning models
634:00 - or maybe something like amazon uh
634:03 - augmented ai so human intervention
634:05 - review services when sagemaker uses
634:07 - machine learning to make a prediction
634:08 - that is not confident that it has the
634:11 - right answer queue up to predict for a
634:13 - human review and these are all about
634:14 - just labeling data um you know when
634:17 - you're using supervised um
634:20 - supervised learning but there are a lot
634:22 - of services under sagemaker itself and
634:24 - just ai services in general so we'll
634:26 - look at that next okay
634:30 - [Music]
634:32 - all right let's take a look at all the
634:33 - ml and ai services and there's a lot on
634:35 - aws so the first is amazon code guru
634:38 - this is a machine learning code analysis
634:40 - service and code guru performs code
634:41 - reviews and will suggest to improve the
634:44 - code quality of your code it can show
634:46 - visual code profiles to show the
634:47 - internals of your code to pinpoint
634:49 - performance next we have amazon lux this
634:52 - is a conversation interface service with
634:54 - lux you can build voice and text chat
634:56 - bots
634:57 - we have amazon personalized this is a
634:59 - real-time recommendation service it's
635:01 - the same technology used to make product
635:02 - recommendations to customers shopping on
635:04 - the amazon platform
635:06 - then we have amazon poly this is a
635:08 - text-to-speech service upload your text
635:10 - and an audio file spoken by synthetic
635:12 - synthesize voice
635:14 - and that will be generated you have
635:16 - amazon recognition this is an image and
635:19 - video recognition service
635:21 - uh analyze image and videos to detect
635:23 - and label objects peoples and
635:25 - celebrities
635:26 - then we have amazon transcribe this is a
635:28 - speech to text service so you upload
635:30 - your audio and it'll be converted into
635:32 - text we have amazon text extract this is
635:35 - an ocr tool so it extracts text from
635:38 - scanned documents when you have paper
635:41 - forms and you want to digitally extract
635:43 - that data
635:44 - you have amazon translate this is a
635:46 - neural machine learning translation
635:48 - service so use deep learning module
635:51 - models to deliver more accurate and
635:53 - natural sounding translations
635:55 - we have amazon comprehend this is an nlp
635:58 - so natural language processing service
636:00 - find relationships between text to
636:02 - produce insights looks at data such as
636:04 - customer email support tickets social
636:06 - media and makes predictions
636:09 - then we have amazon forecasts this is a
636:11 - time series forecasting service and it's
636:14 - you know uh i mean technically i guess
636:16 - it's a bit of a database but the idea
636:17 - here is that it can forecast business
636:19 - outcomes such as product demand resource
636:21 - needs or financial uh performance and
636:24 - it's powered by ml or ai if you want to
636:26 - call it
636:26 - we have aws deep learning amis so these
636:29 - are amazon ec2 instances they're
636:31 - pre-installed with popular deep learning
636:32 - frameworks and interfaces such as
636:34 - tensorflow pytorch apache mxnet chainer
636:38 - gluon uh horovod and kires
636:43 - we have adabus deep learning containers
636:45 - so docker images instances pre-installed
636:47 - with popular deep learning frameworks
636:49 - interfaces such as tensorflow
636:51 - pytorch apache mxnet
636:54 - we have aws deep composer this is
636:56 - machine learning enabled musical
636:57 - keyboard i don't know many people using
636:59 - this but it sounds like fun it was steep
637:01 - lens is a video camera that uses deep
637:03 - learning it's more of like a learning
637:04 - tool so again we don't see many people
637:06 - using this airbus deep racer is a toy
637:08 - race car that can be powered with
637:09 - machine learning to perform autonomous
637:11 - driving again this is another learning
637:12 - tool for learning ml they like to do
637:15 - these at re invent to have like these
637:16 - racing competitions
637:18 - amazon elastic interface so this allows
637:20 - you to attach low-cost gpu perform
637:22 - powered acceleration to ec2 instances to
637:25 - reduce the cost of running deep learning
637:26 - interfaces by 75 percent we have amazon
637:30 - fraud detector so this is a fully
637:31 - managed fraud detection as a service uh
637:34 - it identifies potentially fraudulent
637:36 - online activities such as online payment
637:38 - fraud and the creation of fake accounts
637:40 - amazon kendra so this is an enterprise
637:42 - machine learning search engine service
637:44 - it uses natural language to suggest
637:46 - answers to questions instead of just
637:48 - simple keyword matching so there you go
637:50 - [Music]
637:55 - hey it's andrew brown from exam pro and
637:56 - we're going to do a quick review here of
637:58 - the big data and analytic services that
638:00 - are on aws but before we do let's just
638:02 - define what big data is so it's a term
638:05 - used to describe massive volumes of
638:07 - structured or unstructured data that is
638:10 - so large it is difficult to move and
638:12 - process using traditional database and
638:14 - software techniques so the first tier we
638:17 - have is amazon athena this is a
638:19 - serverless interactive
638:20 - query service it can take a bunch of csv
638:24 - or json files in an s3 bucket and load
638:26 - them into a temporary sql table and so
638:29 - you can run sql queries so it's one you
638:31 - want to query
638:32 - csv or json files if you've ever heard
638:35 - of apache presto it's basically that
638:38 - okay
638:39 - then we have amazon cloud search so this
638:41 - is a fully managed full text search
638:42 - service so when you want to add search
638:45 - to your website
638:46 - we have amazon elastic search service
638:49 - commonly abbreviated to es
638:51 - and this is a manage elastic
638:52 - elasticsearch cluster and elasticsearch
638:55 - is an open source full-text search
638:57 - engine it is more robust than cloud
638:58 - search but requires more server and
639:00 - operational maintenance
639:02 - then we have amazon elastic mapreduce
639:04 - commonly known as emr
639:06 - and this is for data processing and
639:08 - analysis it can be used for creating
639:10 - reports just like redshift but is more
639:12 - suited when you need to transform
639:13 - unstructured data into structured data
639:15 - on the fly and it leverages open source
639:18 - um technology so like spark
639:20 - um hive pig things like that
639:24 - then we have kinesis data stream so this
639:26 - is a real time streaming data service it
639:28 - creates producers which sends data to a
639:30 - stream it has multiple consumers that
639:32 - can consume data within a stream and use
639:35 - it for real-time analytics click streams
639:37 - ingestion data from a fleet of iot
639:40 - devices
639:41 - then we have kinesis fire hose this is a
639:44 - serverless and a simple version of a
639:46 - data stream
639:47 - and you pay on demand based on how much
639:49 - data is consumed through the stream and
639:51 - you don't worry about the underlying
639:52 - servers
639:54 - then you have amazon kinesis data
639:56 - analytics
639:57 - this allows you to run queries against
639:59 - data that is flowing through your
640:00 - real-time stream so you can create
640:01 - reports and analysis on emerging data
640:04 - and last on the kinesis side here we
640:06 - have amazon kinesis video streams this
640:08 - allows you to analyze or apply
640:10 - processing on real-time streaming videos
640:12 - on the second page here we have managed
640:15 - kafka service msk
640:18 - and it might be mks
640:20 - now that i'm looking at it here so
640:22 - just be aware that that might be
640:23 - incorrect but a fully managed apache
640:26 - kafka service kafka is an open source
640:28 - platform for building real-time
640:29 - streaming data pipelines and
640:31 - applications it is similar to kinesis
640:33 - but with more robust functionality
640:35 - then we have redshift which is um
640:38 - it was this flagship
640:40 - big data tool it's a petabyte size data
640:43 - warehouse the data warehouses are for
640:45 - online
640:46 - uh online analytical processing olap so
640:49 - data warehouses can be expensive because
640:51 - they are keeping data hot meaning that
640:52 - we can run a very complex query and a
640:54 - large amount of data and get that data
640:56 - back very fast but this is great when
640:58 - you need to quickly generate analytics
640:59 - or reports from a large amount of data
641:01 - we have amazon quick site this is a
641:03 - business intelligence tool or business
641:05 - intelligence dashboard bi for short you
641:07 - can use it to create business dashboards
641:09 - to power business decisions it requires
641:11 - little to no programming and connect and
641:13 - adjust to many different types of
641:14 - databases have you ever heard of tableau
641:16 - or power bi this is just the aws
641:18 - equivalent
641:20 - we have aw data pipelines this automates
641:22 - the movement of data you can reliably
641:24 - move data between compute storage and
641:26 - services
641:27 - we have abs glue this is an etl service
641:30 - so it allows you to move data from one
641:32 - location another where you need to
641:33 - perform transformations before the final
641:35 - destination it's similar similar to dms
641:38 - but it's more robust
641:39 - we have abus lake formation this is a
641:42 - centralized curated and secured
641:44 - repository that stores all your data so
641:46 - it's a data lake it is a storage
641:47 - repository that holds a vast amount of
641:49 - raw data in its native format until it
641:51 - is needed and then last on here we have
641:53 - aws data exchange this is a catalog of
641:55 - third-party data sets you can download
641:57 - for free
641:58 - or subscribe or purchase data sets so
642:00 - they might have like the kovid 19 foot
642:02 - traffic data the imdb tv movie data
642:05 - historical weather data and sometimes
642:07 - this is really great if you're just
642:08 - trying to learn how to work with these
642:09 - tools okay
642:10 - [Music]
642:15 - hey this is andrew brown from exam pro
642:16 - and we are taking a look here at amazon
642:18 - quick site which is a business
642:19 - intelligence dashboard or bi dashboard
642:21 - that allows you to ingest data from
642:23 - various database storage or database
642:24 - services to quickly visualize business
642:26 - data with minimal programming or data
642:28 - formula knowledge so here's an example
642:31 - of a quick site dashboard
642:34 - and so the way quicksite is able to make
642:36 - these dashboards super fast is via spice
642:39 - the super fast parallel in memory
642:40 - calculation engine
642:42 - and the thing is you don't have to use
642:44 - spice but generally it is good to use it
642:47 - and there are some caveats when getting
642:49 - your data into quicksite sometimes it
642:51 - can't ingest it directly from a
642:53 - particular data store so you might have
642:55 - to dump it to s3 first but it's not too
642:57 - bad because you can use it with glue to
642:59 - transform that data over um there are
643:01 - additional features sometimes marketed
643:03 - services but we have quick site ml
643:05 - insights this detects anomalies perform
643:07 - accurate forecasting it can generate
643:09 - natural language narratives so basically
643:11 - like
643:12 - you know describe it as if you're going
643:13 - to read it out as a business report you
643:15 - know then there's amazon quick site
643:17 - queue this allows you to ask questions
643:19 - using natural language on all your data
643:20 - and receive answers in seconds so there
643:23 - you go
643:23 - [Music]
643:27 - hey this is andrew brown from exam pro
643:29 - and let's go take a look at amazon quick
643:31 - sites which is a or quick site which is
643:34 - a business intelligence tool so when you
643:36 - go here you have to sign up because it's
643:39 - kind of part of aws but on its own
643:42 - separate thing and then you have to
643:44 - choose what you want so we have
643:45 - enterprise and standard
643:46 - um i do not want to pay that much so i'm
643:50 - going to go to standard over here i'm
643:52 - not really sure what the difference is
643:53 - it's not really telling me what
643:56 - between standard and enterprise
643:59 - but i'm going to assume standard is more
644:01 - cost effective but here we it says
644:03 - user use i am federator identities which
644:06 - is fine use i am federal identities only
644:10 - um we can stick with the top one there
644:13 - that seems fine to me
644:14 - we need to enter a name so we'll just
644:16 - say
644:17 - my quick site
644:19 - account
644:21 - and we probably have to fill something
644:23 - in there so let's say andrew example co
644:25 - and these are the services that are
644:26 - going to integrate with athena s3 rds
644:29 - things like that i guess we could select
644:30 - some of those buckets i'm not too
644:32 - worried about doing that right now the
644:33 - provided account name is not available
644:35 - that is a terrible ui but that's
644:37 - aws for you so i'm just going to dump
644:39 - some numbers there
644:41 - i'm going to put my email in here again
644:44 - um we probably want some s3 buckets
644:47 - i'm going to
644:50 - make a new bucket
644:52 - because i think that's how we're going
644:53 - to do this we're going to have to make a
644:55 - bucket here and say
644:56 - quick cite
644:58 - data
645:00 - okay
645:02 - and we're gonna create ourselves a
645:03 - bucket here
645:04 - i'm gonna go back and hopefully that
645:06 - shows up
645:09 - uh
645:10 - it does not so what i'll have to do is
645:12 - just back out
645:15 - and i'm just gonna give it a hard
645:16 - refresh here and we'll hit quick sign up
645:18 - for quick site again
645:20 - and we'll choose standard
645:22 - and we'll say my quick site account a
645:25 - bunch of numbers there
645:27 - android example.co i don't really care
645:29 - about ingesting data from everywhere
645:30 - else i just want it from s3
645:33 - there's my data
645:35 - sure we'll give it right permissions
645:36 - even though i don't plan to do anything
645:38 - with athena here today
645:45 - and we'll give it a moment to load
645:50 - so what i'm thinking is
645:56 - so what i'm thinking is just making like
645:57 - an excel spreadsheet here
645:59 - and just filling in some data so
646:02 - oh it says our account is set up here so
646:04 - we'll go to quick site
646:05 - because i bet i can import like a csv or
646:07 - something
646:09 - um i'm more of a tableau or power bi
646:11 - kind of person um but uh you know for
646:14 - the purpose of the cloud practitioner i
646:15 - am going to show you this amazon quick
646:17 - set lets you easily visualize data and
646:19 - etc that sounds great next next next i
646:22 - know what i'm doing
646:23 - oh do we have some examples great so i
646:25 - don't even have to make a spreadsheet
646:26 - okay so what we'll do is just click on
646:28 - that
646:31 - and we have stuff it looks like they've
646:33 - really improved this since the last time
646:35 - i've seen it which is quite nice
646:39 - but i could try and make my own
646:44 - i'm just trying to think how do we do
646:46 - this again
646:49 - yeah we have the spice there so it's a
646:50 - lot easier from starting from scratch
646:51 - i'm just gonna say close
646:53 - and
646:54 - [Music]
646:55 - these are analysis we want data sets in
646:57 - here
646:58 - oh we already have some data sets these
647:00 - are coming from s3 i think that's the
647:02 - old s3 logo i'm not sure why they're
647:04 - using that one
647:05 - we can go here and create a new data set
647:06 - oh we can upload directly so i don't
647:08 - even have to use s3 that's great so what
647:10 - i'm going to do is just have some values
647:12 - in here so i'm going to just say um
647:18 - type
647:19 - value
647:20 - so we'll say banana
647:22 - 125 123 we'll say apple
647:26 - 11
647:28 - orange
647:30 - nobody likes oranges
647:32 - i shouldn't say i'm sure it's like lots
647:33 - of people like oranges
647:36 - oh we gotta put pears on there
647:39 - i actually really like paris people
647:40 - think i like bananas which is not true i
647:42 - actually like pears
647:44 - that's what i like so i'm going to go
647:46 - ahead and save this save as
647:50 - and i'm just going to save this to my
647:52 - desktop here so just give me a moment
647:53 - just doing this off screen
647:58 - and i'm going to save this uh data set
648:01 - quick site
648:03 - csv it can even take an xls so i don't
648:05 - have to save it as a uh
648:07 - i'll just save it as an xls
648:10 - okay and so we're going to just upload
648:11 - that so there is that data set
648:15 - it's going to scan that file it's going
648:17 - to see that sheet
648:19 - you can even preview it
648:22 - there's the information we're going to
648:23 - add that data
648:26 - i get added as a data data set
648:31 - well how do i
648:33 - where do i it's like it says add the
648:35 - data i just want to add it as a data set
648:37 - so they set up here maybe save and
648:38 - visualize
648:40 - up here and is it autographing it
648:44 - maybe if i drag in is it working is it
648:47 - thinking okay it's 100
648:49 - so i'm going to just drag that onto
648:51 - there
648:54 - and
648:54 - it says pear orange banana
648:59 - just kind of trying to make sense of
649:00 - this here is it taking in count the
649:02 - value maybe put the value down there
649:05 - wow that's so much easier i haven't used
649:07 - this for like a year and um i'm gonna
649:09 - tell you this has gotten a lot easier to
649:11 - use so i'm quite impressed with this but
649:13 - yeah i mean this is pretty much what
649:14 - quicksite is if you want to visualize
649:16 - things in different types you can drag
649:17 - them out you can probably like click on
649:20 - the wheel here and change it
649:22 - again i'm not sure
649:25 - exactly how all the
649:27 - uh the dials and knobs work here but i
649:29 - mean another thing we could do is just
649:31 - drag out like another object and do the
649:32 - same thing so maybe i'd want a pie chart
649:35 - um so
649:38 - add a visual
649:41 - yeah it's not as nice as power bi but
649:43 - like it's still great that it's here you
649:44 - know type
649:46 - value
649:48 - so we got a nice pie chart there
649:51 - uh let's try something weird
649:53 - let's give this one a go
649:56 - doesn't color it which is not very nice
649:58 - um there's probably some kind of way to
649:59 - color it but
650:01 - focus on banana only
650:03 - i don't know i don't know the point of
650:04 - there but anyway that's quick site so
650:07 - um i really don't want to pay for this
650:08 - so what i'm going to do
650:10 - is go up here
650:12 - um there's you have to deactivate i'm
650:14 - just trying to remember how
650:16 - because they change the interface again
650:18 - they change everything on you
650:21 - so there we go i'm on a trial for four
650:23 - days here maybe
650:25 - quantity four just the four 29 day trial
650:28 - so if i want to get out of this trial
650:30 - what do i do
650:32 - i don't
650:33 - want to use it anymore
650:35 - um so
650:38 - how to delete
650:39 - aws quicksite
650:43 - canceling your subscription
650:45 - so before you can unsubscribe uh you're
650:47 - assigned in the im account
650:49 - your quick site administrator you're the
650:51 - root i am administrator sure
650:53 - you deleted any secondary namespaces to
650:56 - find the existing namespace etc so
650:59 - choose your username in the application
651:00 - bar to quick site account settings
651:02 - unsubscribe
651:04 - so i was almost there i thought i was in
651:06 - the right place
651:10 - uh this one no
651:13 - i was just there
651:16 - manage quick site
651:18 - your subscriptions
651:22 - edit
651:24 - there's no unsubscribe option
651:26 - so i'm not sure
651:29 - can i cancel
651:35 - unsubscribe
651:40 - button does not
651:41 - appear in quick site
651:56 - okay just because we're on trial and so
651:58 - maybe after the end of the trial it will
651:59 - uh it will vanish there
652:03 - they are not making this easy for me
652:05 - account settings ah delete accounts this
652:07 - is what we probably want to do
652:08 - permanently delete the account yes
652:11 - i mean that has to get rid of the
652:12 - description because it gets rid of
652:13 - everything
652:15 - there we go
652:17 - we'll say confirm
652:20 - delete account
652:23 - unless you're using them in the services
652:25 - blah blah blah
652:27 - successful okay great so now i should go
652:29 - back
652:30 - to adress.amazon.com and just to confirm
652:33 - that it's gone
652:34 - i'm going to
652:36 - go to quicksite again and just see if
652:39 - it's trying to ask me to
652:41 - sign up again so it is so i've gotten
652:42 - rid of my account so we're all in good
652:44 - shape and uh yeah that is that is quick
652:46 - site
652:46 - [Music]
652:51 - hey this is andrew brown from exam pro
652:53 - and we are taking a look at the aws well
652:55 - architecture framework so this is a
652:57 - white paper created by aws to help
652:59 - customers build using best practices
653:01 - defined by aws you can find this at
653:04 - adabus.amazon.com forward slash
653:05 - architecture forward slash well
653:07 - architected this idea is not unique to
653:09 - aws the other providers have it but i
653:11 - believe aimbots was the first one to
653:13 - define this and they have a really good
653:15 - uh a good approach to this and this is
653:18 - pretty much essential knowledge that you
653:20 - have to have four certifications when
653:22 - we're looking at the cloud practitioner
653:24 - the system architect associate and
653:25 - professional
653:26 - because
653:27 - there's a lot of principles here best
653:29 - practices that adabus uses themselves to
653:31 - architect their infrastructure okay so
653:33 - the framework is divided into five
653:35 - sections called pillars which address
653:37 - different aspects or lenses that can be
653:39 - applied to a cloud workload so imagine
653:42 - you have your cloud workload you're
653:43 - going to want to adopt that as well
653:45 - architect framework some things that you
653:47 - know people don't consider outside the
653:48 - five pillars is that you need to know
653:50 - general definitions uh general design
653:52 - principles and the review process
653:55 - and then from there you have your five
653:56 - pillars so you have operational
653:57 - excellence security reliability
654:00 - performance efficiency and cost
654:02 - optimization and all these have major
654:04 - sections in this white paper but outside
654:07 - of just the main white paper each of
654:09 - these have their own white papers that
654:11 - go even into farther detail so if you
654:13 - really want to
654:14 - really focus on security and get a lot
654:16 - more information they have that as well
654:18 - okay
654:19 - [Music]
654:23 - let's take a look at the general
654:25 - definitions for the well architecture
654:26 - framework starting with the pillars so
654:28 - the operational excellent pillar is
654:30 - there to run and monitor systems the
654:32 - security pillar is to protect data and
654:34 - systems to mitigate risk the reliability
654:37 - pillar is to mitigate and recover from
654:40 - disruptions the performance efficiency
654:42 - pillar is about using computing
654:44 - resources efficiently or effectively and
654:47 - the cost optimization pillar is about
654:49 - getting the lowest price and this is
654:50 - where you're going to find all the
654:52 - business value and i put an asterisk
654:53 - there because
654:55 - you know you might obsess saying we need
654:57 - to meet the requirements for all these
654:58 - pillars and that's not the case you can
655:00 - trade off pillars based on the business
655:02 - context so you know don't take it as
655:05 - literally implement every single thing
655:07 - but just consider that uh you know you
655:09 - might have to adapt it based on your
655:11 - workloads then we have some general
655:12 - definitions that we will come across so
655:14 - there's components so code configuration
655:16 - it was resources against a requirement a
655:18 - workload so a set of components that
655:20 - work together to deliver business value
655:22 - milestones so key changes of your
655:24 - architecture through the product
655:26 - lifecycle
655:27 - then there's architecture itself so how
655:28 - components work together in a workload
655:31 - and then we have technology portfolio so
655:33 - a collection of workloads required for
655:35 - the business to operate okay
655:37 - [Music]
655:42 - so the well architected framework is
655:44 - designed around a different kind of team
655:46 - structure so when you're looking at
655:48 - enterprises they generally have a
655:50 - centralized team with specific roles
655:52 - where adabas structures their teams as
655:54 - being distributed with flexible roles
655:57 - and so this new kind of methodology of
655:59 - distributed teams uh has some major
656:01 - advantages but it does come with some
656:03 - risks and so aws has baked in some uh
656:05 - practices or uh things that they do to
656:08 - mitigate these issues okay so let's
656:09 - compare on-premise enterprise uh to what
656:12 - abuse is proposing for your team
656:14 - structure so on-premise what we'd see is
656:16 - a centralized team consisting of
656:18 - technical architects solution architects
656:21 - data architects network architects
656:23 - security architects and you kind of see
656:25 - that they all have a specialized
656:26 - vertical and they are usually managed by
656:29 - either
656:30 - togaf or
656:32 - zac
656:32 - uh man framework so those are just ways
656:34 - of structuring your teams those are very
656:36 - popular and so what a bus is proposing
656:38 - here is that you have a distributed team
656:40 - and the way you're going to make that
656:42 - team work because obviously just
656:43 - thinking about a distributed team
656:44 - they're going to be a lot more agile but
656:46 - to make sure that they effectively work
656:48 - you have practices like team experts who
656:50 - raise the bar
656:51 - making sure that you know in any areas
656:53 - we can always say how can we do this
656:54 - better
656:55 - then there are mechanisms in place for
656:57 - automated checks for standards so that's
656:59 - the great thing about cloud can all be
657:00 - automated to say hey does it meet our
657:02 - regulatory compliance or what have you
657:04 - and then there's the concept of the
657:06 - amazon leadership principles which we
657:08 - will cover on in the next slide in
657:10 - detail and so um you know itabus is not
657:12 - obviously using uh these other
657:14 - frameworks because it has its own which
657:16 - is this one here but the mechanism to
657:19 - which they stay organized and up to date
657:21 - is they are supported by a virtual
657:22 - community of subject matter experts
657:25 - principal engineers so that what they'll
657:26 - do is they'll engineer things like
657:27 - lunchtime talks and then recycle that
657:29 - into their onboarding material or into
657:31 - this framework itself okay
657:34 - [Music]
657:39 - so we're taking a look here at amazon's
657:40 - leadership principles and these are a
657:42 - set of principles used during the
657:43 - company's decision making problem
657:45 - solving simple brainstorming and hiring
657:48 - all right um and so i can't say that i
657:50 - like all of these but uh definitely some
657:52 - of them really stand out as being great
657:54 - especially the first one which is
657:55 - customer obsession so instead of
657:57 - worrying about what your competitors are
657:58 - doing think about what the customer
657:59 - wants work your way back and you know
658:02 - really focus on the customers needs then
658:04 - there's ownership so if you're going to
658:05 - go do something you know try to be your
658:07 - own mini boss uh and take responsibility
658:10 - for whatever it is you're building event
658:12 - and simplify so you know always look for
658:14 - the simplest solution don't try to
658:15 - engineer something super complicated if
658:17 - it's not necessary
658:19 - are right a lot so you know try to
658:21 - be right uh learn and be curious so
658:24 - that's pretty self-explanatory hire and
658:26 - develop the best insist on the high
658:28 - standards aws always refers to this as
658:30 - raising the bar think big bias for
658:32 - action frugality and abuse is really
658:35 - frugal if you didn't know that but not
658:37 - just for like themselves but also for
658:39 - their customers they want customers to
658:41 - spend the least amount of money possible
658:43 - when using their infrastructure earn
658:45 - trust
658:46 - dive deep have a backbone disagree and
658:48 - commit deliver results strive to be the
658:50 - earth's best employer success and scale
658:53 - bring broad responsibility and if you
658:55 - want to read these in detail because
658:56 - they have a big block of text for each
658:58 - of these
658:58 - you can go to amazon.jobs
659:01 - for en forward slash principles and read
659:03 - all about it okay
659:04 - [Music]
659:09 - all right let's talk about some general
659:11 - design principles that you should be
659:13 - considering when you are designing your
659:15 - infrastructure no matter what pillar
659:16 - that you are looking to adopt the first
659:18 - is stop guessing your capacity needs so
659:20 - the great thing with cloud computing is
659:22 - you use as little or much based on
659:23 - demand whereas on premise you would have
659:26 - to purchase a machine and you'd have to
659:28 - make sure you have additional capacity
659:30 - so that you could grow into it right and
659:32 - so here with uh cloud you do not have to
659:34 - worry about that
659:35 - test systems at production scale so be
659:37 - able to clone your production
659:38 - environment to testing tear down testing
659:40 - while not in use to save money so a lot
659:42 - of people will have a staging server
659:44 - that they run all the time but the great
659:46 - thing here is that with cloud you know
659:48 - it's you can just spin it up and have it
659:50 - right away and then tear it down and
659:51 - save money
659:52 - there's automating to make architectural
659:54 - experimentation easier this is talking
659:56 - about using infrastructure as a code so
659:58 - for aws it should be using cloud
659:59 - formation creating change sets which
660:01 - kind of um uh say exactly what is going
660:04 - to change stack updates drift detection
660:06 - to see if your stuff is
660:08 - being changed over time by developers
660:09 - through manual configuration things like
660:11 - that then we have allow for evolutionary
660:13 - architectures so this is about adapting
660:16 - ci cd um doing nightly releases or if
660:19 - you're using serverless if you adopted
660:21 - lambdas they deprecate over time forcing
660:23 - you to use the latest version
660:26 - and so that is evolutionary
660:28 - architectures then we have drive
660:29 - architectures using data so um when
660:32 - you're using cloud there's a lot of
660:34 - tooling in there to automatically start
660:35 - collecting data so cloudwatch will be
660:37 - collecting some things by default and
660:39 - cloudtrail will as well so you know that
660:42 - is another thing and then improving
660:44 - things through game day so this is about
660:46 - simulating traffic on production or
660:47 - purposely killing ec2 instances or or
660:50 - messing with your services to see how
660:51 - well they recover all right
660:54 - [Music]
660:59 - before we jump into each of the pillars
661:01 - let's go open them up and take a look at
661:03 - what structure we should expect to see
661:05 - so we have design principles definition
661:07 - best practices and resources all the
661:09 - pillars follow this to a t so let's just
661:12 - talk about what these are so the design
661:13 - principles are a list of design
661:15 - principles that needs to be considered
661:17 - during implementation and that's where
661:19 - we're going to focus a lot of our energy
661:21 - then you have definition so this is an
661:23 - overview of the best practice categories
661:25 - then you have the best practices
661:26 - themselves these are detailed
661:28 - information about each practice with
661:30 - various aws services and then you have
661:32 - resources these are additional
661:34 - documentation white papers
661:36 - and videos to implement this pillar and
661:38 - i just want to tell you that if you're
661:39 - doing the certified cloud practitioner
661:41 - we're really just going to cover the
661:42 - design principles but for the solutions
661:44 - architect associate or anything uh
661:47 - that's associated or above that's we're
661:48 - gonna actually dive deep into the
661:50 - implementation of the best practices
661:52 - because there is a lot of stuff there so
661:54 - yeah there we go
661:59 - [Music]
662:00 - let's take a look here at the design
662:01 - principles for operational excellence so
662:04 - the first here is perform operations as
662:06 - code supply the same engineering
662:07 - discipline you would to application code
662:10 - to your infrastructure so by training
662:12 - your operations as code you can limit
662:14 - human error and enable consistent
662:16 - responses to events generally we're
662:18 - talking about infrastructure
662:19 - infrastructure as a code here so this
662:20 - would probably be like things like cloud
662:21 - formation there's other things you could
662:22 - do like policy as a code and a bunch of
662:24 - other ones then we have make frequent
662:26 - small reversible changes so design your
662:28 - workloads to allow components to be
662:31 - updated regularly uh this could be
662:33 - talking about doing rollbacks
662:34 - incremental changes blue green
662:36 - deployments having a ci cd pipeline
662:38 - refined operations procedures frequently
662:40 - so look for continuous opportunities to
662:42 - improve your operations
662:44 - here you use game days to simulate
662:46 - traffic or event failure on your
662:47 - production workloads anticipate failure
662:49 - so perform post modems on system
662:51 - failures to better improve write test
662:53 - code kill production servers
662:56 - there's a small spelling mistake it
662:57 - should have an r here so servers to test
663:00 - recovery learn from all operational
663:02 - failures so share lessons learned in a
663:04 - knowledge base for operational events
663:06 - and failures across your entire
663:08 - organization but you know if you can
663:09 - just remember these headings here
663:11 - and be able to categorize what would be
663:13 - under operational excellence you'll be
663:14 - okay all right
663:15 - [Music]
663:20 - all right let's take a look at the
663:21 - design principles for the security
663:22 - pillar so the first here is implement a
663:25 - strong identity foundation so implement
663:27 - the principle of least privilege or polp
663:30 - that's a very popular concept meaning
663:33 - giving people only the permissions that
663:34 - they need use centralized identity so
663:36 - that would be using database iam avoid
663:39 - long link credentials then we have
663:41 - enable traceability so monitor alerts
663:43 - and audit actions and changes to your
663:45 - environment in real time integrate log
663:47 - and metric collection and automate
663:49 - investigations and remediation then we
663:52 - have apply security at all layers so
663:55 - take defense in depth approach with
663:57 - multiple security controls for
663:59 - everything from as networks vbcs load
664:01 - balancing instances os application code
664:04 - we might have a slide in this course on
664:06 - defense and uh depth where basically you
664:08 - see like a ring of things and you can
664:10 - kind of see how like there's layers that
664:12 - go from outward to inward and that's
664:14 - what they're talking about when they're
664:14 - listing out all these things here
664:17 - automate security best practices
664:19 - protect your data in transit at rest
664:22 - keep people away from your data
664:24 - the reason i don't have descriptions
664:25 - there is because those are pretty
664:26 - self-evident prepare for security events
664:28 - so incident management systems and
664:30 - investigation policies and processes
664:32 - tools to detect investigate and recovery
664:34 - from incidences and uh there are a lot
664:37 - of security tools out there and they all
664:38 - have funny initialisms i didn't put any
664:40 - of them in here but i'm sure there are
664:41 - some there
664:42 - but yeah there you go for security
664:47 - [Music]
664:48 - all right let's take a look at design
664:50 - principles for reliability and the first
664:52 - here is automatically recover from
664:53 - failure so monitor kpis and trigger
664:56 - automations when the threshold is breach
664:58 - test recovery procedures so test how
665:00 - your workload fails and you validate
665:02 - your recovery procedures you can use
665:04 - automation to simulate different
665:06 - failures or to recreate scenarios that
665:07 - led to failures before
665:09 - scale horizontally to increase aggregate
665:11 - system availability so replace one large
665:13 - resource with multiple small resources
665:14 - to reduce the impact of a single failure
665:16 - on the over overall workload distribute
665:20 - requests across multiple smaller
665:22 - resources to ensure that they don't
665:23 - share a common point of failure so we're
665:25 - talking about multi-az
665:26 - high availability okay stop guessing
665:28 - capacity we've seen this multiple times
665:30 - so in on-premise it takes a lot of
665:32 - guesswork to determine the elasticity of
665:33 - your workloads uh
665:35 - workload demands with cloud you don't
665:36 - need to guess how much you need because
665:38 - you can request the right size of
665:39 - resources on demand that's going to give
665:41 - you better reliability okay manage
665:43 - change and automation so making changes
665:45 - via infrastructure as a code will allow
665:47 - for a formal process to track and review
665:49 - infrastructure they're going to see iac
665:50 - show up a lot in this framework okay
665:52 - [Music]
665:57 - let's take a look at design principles
665:58 - for performance efficiency so the first
666:00 - here is democratize advanced technology
666:03 - so focus on product development rather
666:05 - than procurement provisioning and
666:06 - management of services because if you're
666:08 - on prem you'd have to order those
666:09 - machines set them up and so take
666:11 - advantage of advanced technologies
666:13 - specialize and optimize for your use
666:14 - case with on-demand cloud services
666:16 - because again if you're using on-prem uh
666:18 - you you know
666:19 - you might not have the option to have
666:20 - sage maker right it's just going to be a
666:22 - vm and you're going to do all the work
666:24 - yourselves whereas aws has all these
666:26 - specialized things so you can move
666:27 - quickly
666:28 - go global in minutes so deploying your
666:30 - workload in multiple abs regions around
666:32 - the world allows you to provide lower
666:33 - latency and a better experience for your
666:35 - customers at a minimal cost we have used
666:37 - serverless architecture so serverless
666:39 - architecture removes the need for you to
666:40 - run and maintain physical servers for
666:42 - traditional computing activities removes
666:44 - the operational burden of managing
666:45 - physical servers and can lower
666:47 - transactional costs because managed
666:48 - services operate at cloud scale and aws
666:50 - can be a lot better at
666:52 - running them
666:53 - efficiently then you will uh experiment
666:56 - more often so with virtual and
666:57 - automatable uh resources you can quickly
666:59 - carry out comparative testing using
667:01 - different types of instances storage or
667:03 - configurations to make the best choice
667:05 - we call this right sizing choosing the
667:06 - right size consider mechanical sympathy
667:10 - so understand how cloud services are
667:11 - consumed and always use technology
667:13 - approach that aligns best with your
667:15 - workload goals for example consider data
667:17 - access patterns when you select database
667:19 - or storage approaches
667:21 - [Music]
667:26 - let's take a look here at design
667:27 - principles for cost optimization so the
667:29 - first one here is implement cloud
667:31 - financial management so dedicate time
667:33 - and resources to build capacity via
667:35 - cloud financial management and cost
667:37 - optimization tooling statements is
667:38 - saying hey take advantage of all our
667:40 - tooling that makes it easy for you to
667:41 - know exactly what you're spending adopt
667:43 - a consumption model so pay only for
667:46 - computing resources that you require
667:48 - an increase or decrease using uh
667:50 - depending on the business requirements
667:51 - we're talking about on-demand pricing
667:53 - measure overall efficiency so measure
667:55 - the business output of the workload and
667:57 - the cost associated associated with
667:59 - delivering use this measure to know the
668:01 - gains you make from increasing output
668:03 - and reducing costs so stop spending
668:05 - money on undifferentiated that's a hard
668:08 - word to say
668:09 - undifferentiated heavy lifting so aws
668:12 - does the heavy lifting of the data
668:14 - center operations like racking stacking
668:15 - and power servers it also removes the
668:17 - operational burden of managing operating
668:19 - systems
668:20 - and applications with managed services
668:22 - this allows you to focus on your
668:23 - customers and business projects rather
668:25 - than your it infrastructure
668:28 - and the last one here is analyze and
668:30 - attribute expenditure so the cloud makes
668:32 - it easier to accurately identify the
668:34 - usage and cost of systems which then
668:36 - allow transparent attribution of it
668:39 - costs to individualize workload owners
668:41 - this helps measure return on investment
668:43 - and gives workload owners an opportunity
668:45 - to optimize the resources and reduce
668:47 - costs so there you go
668:48 - [Music]
668:53 - hey this is andrew brown from exam pro
668:54 - and we are taking a look at the aws well
668:56 - architected tool so this is an auditing
668:58 - tool to be used to assess your cloud
669:00 - workloads for alignment with the aws
669:02 - well architected framework and so what
669:05 - it is it's essentially a checklist
669:07 - but it also has nearby references so you
669:09 - know as you're reading through it it
669:11 - will show you information uh and
669:13 - resources so that it can help you with
669:15 - this checklist here and the idea is when
669:17 - you're done you can generate out a
669:19 - report and then you can provide that
669:21 - report to your executives and key
669:22 - stakeholders to prove uh you know how
669:25 - well architected your workload is on aws
669:27 - okay
669:28 - [Music]
669:32 - hey this is andrew brown from exam pro
669:34 - and in this video i want to show you two
669:35 - things the well architected framework
669:37 - and the well architected tool so first
669:39 - let's go look for the well architected
669:41 - framework
669:42 - so we're going to look up white papers
669:44 - aws
669:46 - and so if we go here to about amazon.com
669:48 - white papers we have a bunch of pages
669:50 - here and so i'm going to just check box
669:52 - on white paper so that we can kind of
669:54 - reduce the amount there and i'm going to
669:56 - check box well architected framework if
669:58 - we scroll all the way top here one of
670:00 - these you think it'd be right at the top
670:02 - but one of these is the well architected
670:04 - framework and here it is and so if we
670:06 - open it up i used to just directly open
670:09 - up as a pdf i'm sure you can still
670:10 - download it as is but generally you're
670:12 - going to open up as this html page and
670:15 - you can basically read through it see
670:16 - all the stuff see the multiple pillars
670:19 - we can click into here see the design
670:21 - principles read the definitions and then
670:25 - start reading about uh the best
670:27 - practices and they have these things at
670:28 - the bottom of each one
670:30 - uh very boring very very boring but um
670:32 - you know when you get to the solutions
670:34 - architect and things like that you're
670:35 - going to need to know this stuff inside
670:36 - and out it's going to really help you
670:38 - out this cloud practitioner we only need
670:40 - to know surface level information
670:42 - uh but that's a little arctic framework
670:44 - let's take a look at the well
670:46 - architected tool so we're going to type
670:48 - in well here we'll get the well
670:50 - architected tool and if we go here you
670:52 - can see that i've created a couple
670:54 - before probably demos
670:56 - for
670:57 - our videos and so i'm going to go define
670:59 - a new workload i'm going to say my
671:01 - my workload here
671:03 - my workload
671:07 - whoops my workload it is messing up
671:10 - because i probably have grammarly
671:11 - installed so it does not like grammarly
671:13 - so i'm just going to turn it off for now
671:17 - so my workload
671:20 - and it's still not typing correctly so i
671:22 - have to kill a kill of grammarly here
671:25 - which is kind of frustrating so that's a
671:26 - bug that that's not grammarly's fault
671:28 - that's adabus's fault for not playing
671:30 - well
671:31 - with grammarly and that's something i
671:33 - will definitely report to them because
671:35 - it's very annoying
671:37 - so i'm going to go ahead and refresh
671:38 - this page
671:41 - my workload my workload
671:45 - um
671:46 - and this is andrew brown
671:49 - production or pre-production doesn't
671:51 - matter pick your regions us east
671:54 - or usc's 2 sure
671:57 - i'm selecting it
672:00 - there we go uh optional optional
672:03 - optional optional you go to next
672:05 - and then you can choose your lens
672:06 - serverless lens ftr lens so that's the
672:09 - foundational technical review sas lens
672:11 - we can go with architected framework and
672:14 - then once that is there we can start
672:16 - reviewing
672:18 - okay and then we get this big checklist
672:20 - and so we can go through this and read
672:22 - each one so we say
672:23 - ops one how do you determine what your
672:25 - priorities are and all these things like
672:27 - ops and stuff like that these are all
672:28 - the summaries in each of the well
672:30 - architected framework sections
672:32 - so you pretty much don't need to really
672:33 - read the dock and just go through this
672:34 - so everyone needs to understand their
672:36 - part in enabling business success
672:39 - have shared goals in order to set
672:40 - priorities of resources this will
672:42 - maximize the benefit of your efforts
672:44 - so select from the following evaluate
672:46 - the customer's external needs
672:49 - external customer needs evaluate
672:50 - internal customer needs if you click
672:52 - info it's going to highlight each one
672:53 - here so of all key stakeholders
672:56 - including business development
672:57 - operations teams this will ensure etc
672:59 - and so you just go through this and uh
673:02 - you know once you have that
673:04 - and you save and exit
673:06 - okay
673:07 - you'll have
673:08 - the questions that are answered it'll
673:10 - say what's high risk what's not things
673:12 - like that
673:13 - very simplistic it's really just a way
673:15 - of making a very organized report or
673:17 - checklist and proving that you went
673:19 - through it uh to the executive level or
673:22 - to the management level there so
673:23 - hopefully that makes sense to you it's
673:25 - not too complicated but there you go
673:27 - [Music]
673:32 - hey it's andrew brown from exam pro and
673:33 - we are looking at the aws architecture
673:35 - center so the architecture center is a
673:37 - web portal that contains best practices
673:40 - and reference architectures for a
673:41 - variety of different workloads and you
673:43 - can find this at
673:44 - adabus.amazon.com for slash architecture
673:47 - so if you're looking for best practices
673:48 - in terms of security they have a huge
673:50 - section on that and they have it for
673:52 - pretty much every kind of category on
673:54 - aws or if you're looking for practical
673:57 - examples you can view the large library
674:00 - of reference architectures so here's one
674:02 - to make an aws q and a bot and it will
674:05 - have an architectural diagram but you
674:06 - can also
674:08 - deploy via cloud formation or possibly
674:10 - cdk
674:11 - and this way you can get a working
674:13 - example and then tweak it for your use
674:14 - case so this is a really great tool um
674:17 - when you are done the awesome
674:19 - architecture framework and you're saying
674:20 - okay how do we apply it can we get more
674:22 - concrete examples and i wouldn't be
674:23 - surprised if a lot of the resources
674:25 - within the well-architected framework
674:27 - white paper are just pointing to the
674:28 - center okay
674:29 - [Music]
674:34 - hey this is andrew brown from exam pro
674:36 - and we are taking a look at the concept
674:37 - of total cost of ownership also known as
674:39 - tco so what is tco well it is a
674:42 - financial estimate intended to help
674:44 - buyers and owners determine the direct
674:46 - and indirect costs of a product or
674:47 - service so here is an example of you
674:50 - know tco for maybe like a data center so
674:52 - we have hardware monitoring installation
674:54 - i.t personnel training software
674:57 - uh security licensing and taxes but
675:00 - that's not just the limit of it it's
675:01 - just kind of the examples we show here
675:03 - the idea of creating tco is useful when
675:06 - your company's looking to migrate from
675:07 - on-prem to cloud and we will have a
675:10 - better kind of visual here to kind of
675:12 - understand how you would contrast
675:14 - against on-premise to cloud but let's
675:15 - just talk about how it actually works in
675:17 - practicality which i think gets kind of
675:19 - overlooked when cloud service providers
675:21 - are selling you on tco so the idea is
675:24 - that gardner um you know they
675:26 - uh they were they wrote this article
675:28 - based on this research where an
675:30 - organization had moved uh 2500 virtual
675:33 - machines over to amazon dc2 and so what
675:36 - you're seeing here is that there is a an
675:39 - additional cost that we're not
675:40 - considering which is the migration cost
675:42 - see this bar up here um so the idea is
675:44 - that the company was paying around 400
675:46 - 000 and so they started to move over and
675:50 - as you see their costs initially went up
675:52 - for a short period of time here uh but
675:55 - then once that migration cost was over
675:57 - uh you can notice that they had a 55
675:59 - reduction so it's uh totally possible to
676:01 - save money uh and clearly there is great
676:03 - savings uh now is it exactly what aws
676:06 - promises probably not and that's that
676:08 - could be the reason why they update
676:10 - their tco calculator but let's now just
676:12 - do that contrast against the two so we
676:15 - have on-premise on the left and aws on
676:17 - the right or any cloud service provider
676:19 - and what i want to do is help you think
676:20 - about what costs do people generally
676:22 - think about because if we have like
676:24 - iceberg the idea here is that these are
676:26 - the costs that we always think about
676:27 - above the iceberg and then there's these
676:29 - hidden costs that we just don't consider
676:31 - when factoring in our move and that's
676:32 - the idea of tcos to consider all the
676:34 - costs not just the superficial ones and
676:37 - so people say these look like teeth and
676:38 - that's why i add penguins and a whale
676:40 - here um and so when we're talking about
676:43 - on-premise what we generally think are
676:44 - software license fees and subscription
676:46 - fees but when you compare those against
676:48 - each other they might look the same um
676:50 - aws might just look slightly cheaper or
676:52 - even more and so the idea is you need to
676:54 - then factor in everything so on on
676:56 - premise there's implementation
676:57 - configuration training physical security
677:00 - hardware id personnel maintenance and on
677:02 - the aws side you know you are you don't
677:04 - have to do as much of that stuff so you
677:06 - just have implementation configuration
677:08 - and training and so aws with their tco
677:10 - calculator their old one used to make a
677:12 - promise of 75 percent in savings um
677:15 - again you know
677:17 - this is going to really vary based on
677:19 - what your migration strategy looks like
677:21 - um but you know it's totally possible
677:22 - you could save 75 percent or you could
677:25 - save 50 percent over a third year
677:27 - three-year period and there's a initial
677:29 - spike so that's just something you have
677:31 - to consider but the nice thing though is
677:33 - that once you've moved over all the
677:34 - stuff over here on the left-hand side
677:36 - will be eight of us's responsibility
677:37 - okay
677:38 - [Music]
677:43 - all right so let's take a look at
677:44 - capital versus operational expenditure
677:46 - so there's capex and opex so on the
677:49 - catholic side the idea here is you're
677:51 - spending money upfront on physical
677:52 - infrastructure deducting that expenses
677:54 - from your tax bill over time
677:56 - a lot of companies that are running
677:57 - their own data centers uh or have a lot
678:00 - of on-premise stuff understand what
678:02 - capex is because
678:04 - it's something that a lot of times they
678:05 - get tax breaks is on and that's why we
678:07 - see a lot of people that have a hard
678:08 - time moving away from the cloud because
678:10 - you know they keep on thinking about
678:11 - that money they save from the government
678:13 - but capex costs would be things like
678:15 - server costs storage network costs
678:17 - backups and archives disaster recovery
678:20 - costs data center costs technical
678:22 - personnel so the idea is with capital
678:25 - expenses you have to guess up front what
678:27 - you plan to spend okay with operational
678:30 - expenditure the idea here is the cost
678:32 - associated with an on-premise data
678:33 - center that has shifted the cost to the
678:35 - service provider the customer only has
678:37 - to be concerned with non-physical costs
678:39 - so leasing software and customizing
678:41 - features
678:42 - training employees and cloud services
678:44 - paying for cloud support
678:46 - billing based on cloud metrics so
678:48 - compute usage storage usage and so the
678:50 - idea here is with operational expenses
678:53 - you can try a product or service without
678:55 - investing in equipment so basically apex
678:58 - is what we think about when we think of
678:59 - on-premise and then opex is what we
679:01 - think about
679:03 - you know we're thinking about cloud or
679:04 - aws okay
679:05 - [Music]
679:10 - all right let's ask a very important
679:12 - question about cloud migration so does
679:14 - cloud make it personnel redundant so a
679:16 - company is considering migrating their
679:18 - workloads from on-premise to the cloud
679:19 - to take advantage of the savings there
679:21 - is a concern among the staff that there
679:24 - will be mass layoffs does cloud make it
679:27 - personnel redundant and that's a very
679:29 - important question to to have an answer
679:31 - to and this all talks about shifting
679:33 - your i.t team into different
679:35 - responsibilities so a company needs i.t
679:38 - personnel during the migration phase as
679:39 - we saw with that gardner research report
679:41 - that there was a period
679:43 - at least like a year where they needed
679:45 - that for you know depending on the size
679:46 - your company so you're still going to
679:48 - need those people around a company can
679:50 - transition some roles to new cloud roles
679:52 - so a very traditional example would be
679:54 - you have your traditional networking
679:55 - roles or people have like their ccna and
679:58 - now they're moving over to cloud
679:59 - networking they have a reduced workload
680:02 - but there's other things that they could
680:04 - be doing in the cloud
680:06 - a company may decide to take a hybrid
680:07 - approach so they'll always need to have
680:09 - a traditional it team and a cloud it
680:12 - team
680:12 - um and the last one and this one you'd
680:15 - actually see on the exam which is a
680:17 - company can change employees activities
680:19 - from managing infrastructure to rev
680:20 - revenue generating activities okay so
680:23 - the idea is that you know if you're a
680:24 - company why would you get rid of all
680:26 - your staff and you just put them all
680:27 - into rev regeneration i suppose you know
680:30 - you could uh you know uh lay them off
680:32 - and some companies might do that um or
680:34 - you know you could just retrain them
680:36 - because
680:37 - if that it personal team has uh
680:39 - technical expertise i'm sure they can
680:40 - translate that to the cloud
680:42 - [Music]
680:46 - let's talk about the database pricing
680:47 - calculator and this is a free cost
680:49 - estimate tool that can be used within
680:51 - your web browser without the need of a
680:53 - database account to estimate the cost of
680:54 - a various items services and this is um
680:58 - available at calculator.aws
681:00 - and the reason we're bringing this up is
681:01 - because there used to be a tco
681:04 - calculator but now this is the
681:05 - calculator that you use
681:07 - so the abs pricing calculator contains
681:09 - 100 plus services that you can configure
681:10 - for cost estimate and so you can just
681:13 - click through a bunch of knobs and
681:15 - boxes to
681:17 - uh you know
681:18 - exactly figure out a very accurate cost
681:22 - so the idea here is that to calculate
681:24 - your tco an organization needs to
681:26 - compare that existing cost against their
681:28 - abuse costs and so the ibis prices
681:30 - calculator can be used to determine uh
681:32 - you know the aws costs and obviously the
681:34 - organization knows its cost so we can
681:35 - compare it against that
681:38 - and the way you can get data out of this
681:39 - is you can export it as a final estimate
681:42 - to a csv okay
681:44 - [Music]
681:48 - hey this is andrew brown from exam pro
681:49 - and we are taking a look at the aws
681:51 - pricing calculator so to get there it's
681:53 - calculated.aws what you're going to do
681:55 - is hit create estimate and then here you
681:57 - have a bunch of services so you just
681:58 - choose what you want so you type in ec2
682:00 - we're going to configure that
682:02 - and from there we can do a quick
682:04 - estimate or an advanced estimate so
682:05 - choose this option for fast and easy
682:07 - route to ballpark an estimate choose
682:09 - this option for detailed estimate for
682:11 - accounts workloads and stuff so notice
682:12 - down below very simplistic we hit
682:15 - advanced
682:17 - and we get all
682:18 - sorts of stuff okay so you know it's
682:20 - really up to you i'm very comfortable
682:22 - with the advanced options so i might be
682:24 - running a linux machine what is my usage
682:27 - it's going to
682:28 - have uh daily spikes of traffic because
682:31 - of the use cases you could say it's not
682:33 - busy on saturday and sunday that it has
682:35 - a baseline of one a peak of two eight
682:37 - things like that then you can choose
682:39 - what you're using um
682:42 - t4 g i don't even know what that is uh
682:44 - but let's just say like t
682:46 - uh
682:47 - t to uh micro which is not that big two
682:50 - three micro
682:52 - and you could say we're doing on demand
682:53 - because a lot of people would be doing
682:55 - that and
682:56 - you see like seven dollars a month it's
682:58 - not a lot of money then you're looking
683:00 - at your storage data in data out
683:04 - okay so we can add that
683:06 - another thing that we might see is
683:07 - something like rds
683:10 - so we go to rds and we add postgres and
683:14 - not all of them have the simple and
683:15 - complex sometimes they're simple so
683:17 - production database
683:20 - we'll have one here and we're just going
683:22 - to be
683:23 - say a db t2 micro
683:26 - t3 micro there we go
683:28 - a hundred that's fine we're not going to
683:30 - have multi-az we'll have single lazy on
683:33 - demand show the calculation 13 a month
683:36 - add that to our estimate
683:37 - so you're kind of getting the idea there
683:39 - right
683:41 - and so you know we have our summary
683:43 - that's our monthly 391 dollars
683:47 - um oh sorry over 12 months so our
683:48 - monthly cost is 32
683:51 - okay you can go back there clone the
683:52 - service edit it stuff like that you can
683:54 - export the estimate i think it goes out
683:57 - as a csv you can also hit share
684:01 - and then hit agree
684:02 - and so then you have a public link
684:04 - and if i have that link we can just see
684:07 - what happens if i paste it okay and it
684:10 - just brings them to the same estimate so
684:12 - there you go
684:13 - [Music]
684:17 - hey this is andrew brown from exam pro
684:19 - and we are taking a look at migration
684:21 - evaluators so it was formerly known as
684:23 - tcl logic and then abus acquired the
684:26 - company and it is an estimate tool used
684:28 - to determine an organization existing
684:30 - on-premise costs so it can compare it
684:32 - against its aws costs for planned cloud
684:34 - migration uh so the idea is that you can
684:37 - get a very very detailed information and
684:40 - the way it collects information is via
684:42 - an agentless collector to collect data
684:45 - from your on-premise infrastructure to
684:46 - extract from your own on-premise costs i
684:48 - don't know if you can see there but you
684:49 - can see that it works with a lot of
684:51 - different kinds of on-premise technology
684:53 - like vmware microsoft
684:56 - tsql all sorts of things okay
684:59 - [Music]
685:04 - one migration tool that we can use with
685:06 - aws is the vm import export and this
685:09 - allows us to import virtual machines
685:11 - into ec2 so itamus has import
685:13 - instructions for vmware citrix
685:16 - microsoft hyper-v
685:18 - windows vhd from azure and also linux
685:21 - vhd from azure and so the way this works
685:23 - is that you prepare your virtual image
685:25 - for upload and adabus has a bunch of
685:27 - instructions for that once it is ready
685:29 - you're going to upload that to an s3
685:31 - bucket and once it's uploaded to an s3
685:33 - bucket then what you can do is use the
685:36 - aw cli to import your image
685:38 - um and so that is the cli command down
685:41 - below
685:42 - and once it is produced it will generate
685:44 - out an amazon machine image and so from
685:46 - an ami you can then go launch your ec2
685:49 - okay
685:50 - [Music]
685:54 - hey this is andrew brown from exam pro
685:56 - and we are taking a look at the database
685:58 - migration service which allows you to
685:59 - quickly and securely migrate one
686:01 - database to another dms can be used to
686:03 - migrate your on-premise database to aws
686:05 - and that's why we're talking about it
686:07 - and so here's a general diagram where
686:08 - you have your source database which
686:10 - connects to a source endpoint goes
686:11 - through a replication instance so that's
686:13 - an ec2 instance that's going to
686:15 - replicate the data to the target
686:17 - endpoint onto the target database
686:20 - and so we have a bunch of possible
686:21 - sources so we have oracle database
686:23 - microsoft sql mysql mario db postgresql
686:28 - mongodb sap at asc
686:31 - imdb
686:32 - db2 azure sql database amazon rds
686:37 - amazon s3 and i'm assuming these are
686:39 - database dumps
686:40 - amazon aurora amazon document db and so
686:44 - for possible targets it's very similar
686:46 - we got oracle database microsoft sql
686:48 - mysql mario db post sql redis sap se
686:54 - amazon redshift amazon rds amazon
686:57 - dynamodb amazon s3 amazon aurora amazon
687:00 - open search service amazon elastic cache
687:03 - for redis amazon document db amazon
687:06 - neptune apache kafka i'm just showing
687:08 - you the list to give you an idea of how
687:10 - flexible this service is uh but you can
687:13 - tell that these are very different
687:14 - databases so how can it uh move them
687:17 - over right and so in not all cases can
687:19 - it easily do it like it's very easy to
687:21 - go from mysql to postgres um but you
687:23 - know for ones that are like relational
687:26 - to
687:26 - uh nosql uh this is where the innova
687:29 - schema conversion tool comes into play
687:31 - it's used in many cases to automatically
687:32 - convert a source database schema to a
687:34 - target database schema or semi-automate
687:37 - it so that you can kind of like you know
687:40 - figure out how to map the new schema
687:42 - each migration path requires a bit of
687:44 - research since not all combinations of
687:45 - sources and targets are possible and it
687:48 - really comes down to even versions of
687:50 - these things so but i just want you to
687:51 - know about that it's an option as a
687:53 - database migration service and i've
687:55 - migrated a very large database before
687:57 - and it's super fast so and it's not that
687:59 - hard to use so something you definitely
688:01 - want to remember when you're
688:03 - [Music]
688:06 - migrating hey this is andrew brown from
688:09 - exam pro and we are taking a look at the
688:11 - cloud adoption framework so this is a
688:12 - white paper to help you plan your
688:14 - migration from on premise to aws at the
688:16 - highest level the aws caf organizes
688:19 - guidance into six focus areas we've got
688:21 - business people governance platform
688:23 - security and operations and this white
688:26 - paper is pretty high level uh so you
688:29 - know it doesn't get into granular
688:30 - details on how that migration should
688:32 - work but gives you kind of a holistic
688:34 - approach and i believe that probably
688:36 - through the aws
688:38 - amazon partner network there's people
688:39 - that specialize in using this particular
688:41 - framework to help organizations move
688:43 - over and i believe that anybody has
688:45 - professional services through the apn
688:47 - but let's just kind of break down what
688:48 - these six categories are we're not going
688:50 - to go too deep into this but let's do it
688:52 - so the first is the business perspective
688:54 - so these are business managers finance
688:56 - managers budget owners strategy
688:58 - stakeholders so it's how to update the
689:01 - staff skills and organizational
689:02 - processes to optimize this value as they
689:04 - move ops to the cloud you have people
689:06 - perspective so human resources staffing
689:08 - people managers so how to update the
689:11 - staff skills and organizational
689:12 - processes to optimize and maintain the
689:14 - workforce and ensure competencies are in
689:16 - place at the appropriate time you have
689:18 - governance perspectives so cios program
689:21 - managers project managers enterprise
689:23 - architects business analysts so how to
689:25 - update the staff skills and
689:26 - organizational processes that are
689:28 - necessary to ensure business governance
689:30 - in the cloud and manage and measure
689:32 - cloud investments to evaluate the
689:34 - business outcomes we have platform
689:36 - perspectives so ctos it managers
689:37 - solution architects so how to update the
689:40 - staff skills and organizational
689:42 - processes that are necessary to deliver
689:43 - and optimize cloud solutions and
689:45 - services security perspectives so ciso
689:48 - i.t security managers i.t security
689:50 - analysts so how to update the staff
689:52 - skills and organizational processes that
689:53 - are necessary to ensure that the
689:55 - architecture deployed in in the cloud
689:58 - aligns to the organization's security
689:59 - control requirements resilience and
690:01 - compliance requirements we have
690:03 - operational or operations perspective so
690:05 - i t operations managers i t support
690:07 - managers so how to update the staff
690:09 - skills and organizational processes that
690:11 - are necessary to ensure system health
690:13 - and reliability during the move of
690:15 - operations to the cloud and then to
690:17 - operate operate using agile ongoing
690:19 - cloud computing best practices so this
690:22 - just
690:22 - taps the surface of what the caf is
690:25 - and i think for each of these they
690:27 - actually have a more detailed breakdown
690:28 - so you know business is going to break
690:30 - down to even more uh finite things there
690:33 - okay
690:34 - [Music]
690:38 - so aidabus has free services that are
690:41 - free forever unlike the free tier that
690:43 - are up to a point of usage or time
690:45 - um and so there are a lot here this is
690:47 - not even the full list there's
690:48 - definitely more and we have iem amazon
690:51 - vpc auto scaling cloud formation elastic
690:53 - bean stock ops works amplify appsync
690:56 - code star organizations consolidate
690:58 - billing it was cost explorer
691:00 - sagemaker systems manager there's a lot
691:03 - of them okay
691:04 - but the thing is is that these services
691:06 - are free but some of these
691:08 - can spin up other resources so the
691:10 - services are free themselves uh however
691:13 - ones that provision services may cost
691:15 - you money so cloudformation which is an
691:16 - infrastructure as a code tool could
691:18 - launch virtual machines those virtual
691:19 - machines will cost money right opsworks
691:22 - can launch servers that can cost money
691:23 - amplify can launch
691:25 - um lambdas that can cost money so that's
691:28 - something you just have to consider um
691:30 - but yeah there you go
691:32 - [Music]
691:37 - hey this is andrew brown from exam pro
691:38 - and we are taking a look at the aws
691:40 - support plans so we got basic developer
691:43 - business and enterprise and you
691:45 - absolutely absolutely need to know this
691:47 - stuff inside and out for exams they will
691:49 - ask you questions on this okay
691:51 - so basic is for email support only uh
691:54 - such as billing and account so if you
691:56 - think it got over billed and that's
691:58 - something you should do if you've uh
692:00 - misconfigured something and you end up
692:02 - with a big bill just go
692:04 - open up a support ticket under basic for
692:06 - billing and they're likely to refund you
692:08 - but if you do have questions about
692:10 - billing accounts that's we're going to
692:11 - be using for everything else that is for
692:12 - tech support um and so for developer
692:15 - business enterprise you're going to get
692:16 - email support which they'll
692:19 - roughly reply within 24 hours i believe
692:21 - this is business hours so if you message
692:24 - them on friday um or sorry saturday you
692:27 - might be waiting till monday for it okay
692:31 - in terms of third party support the only
692:33 - one that doesn't have third-party
692:35 - support is developer so if you are using
692:38 - something like ruby on rails or azure or
692:40 - something that has interoperability
692:42 - between
692:43 - aws and something else business
692:44 - enterprise will absolutely help you out
692:46 - with it same with enterprise but the
692:48 - developer one not so much uh if you like
692:51 - to use the phone or you like to chat
692:53 - with people um that's available the
692:55 - business enterprise tier this is the way
692:57 - i end up talking to people if you are um
693:00 - you know like if you're in north america
693:02 - and you're calling between nine to five
693:03 - on a monday friday you're likely to get
693:05 - somebody that is in within north america
693:08 - if not it'll be one of uh
693:11 - one of the supports from some other area
693:13 - so just be aware of that that can also
693:14 - affect the time they pick up uh
693:16 - sometimes it's five minutes sometimes
693:18 - it's 30 minutes to to an hour uh you
693:20 - know it just depends on what service
693:22 - you're asking for and you know what time
693:25 - of day okay
693:27 - in terms of
693:29 - responsiveness uh for general guidance
693:31 - everything is 24 hours or less for
693:33 - developer business enterprise if your
693:35 - system is impaired it's within 12 hours
693:38 - or less with production system impaired
693:40 - it's four hours or less with production
693:42 - system down it's one hour or less and if
693:44 - you're for enterprise um it's going to
693:46 - be business critical system down less
693:48 - than 15 minutes so just notice who has
693:51 - what for these things um i've definitely
693:54 - waited like
693:55 - three days on general guidance before so
693:57 - just take these with a grain of salt
693:59 - that they're not you know they don't
694:00 - really stick to these that or maybe i'm
694:02 - just not paying enough for them to care
694:04 - okay um in terms of uh getting actual
694:07 - people assigned to you this only happens
694:08 - at the enterprise level where they have
694:10 - their concierge team so they uh help
694:12 - your
694:13 - organization uh learn how to use adabask
694:16 - ask them any questions personally and
694:18 - then you have a tam a technical account
694:19 - manager that is somebody that knows um
694:22 - awsi inside and out and they'll help you
694:24 - architect things and make correct
694:25 - choices or they'll check your bill and
694:27 - help you try to reduce that bill things
694:29 - like that okay
694:30 - in terms of trusted advisory checks at
694:32 - the basic developer you get seven
694:34 - advisor checks once you're paying for
694:35 - business you get all the checks the cost
694:38 - here for business is zero um for
694:40 - developer it's starting at 29 a month
694:43 - for businesses starting at 100 a month
694:45 - and then for enterprise it's 15 000 a
694:48 - month so i said starting yet because
694:50 - it's dependent on your usage okay
694:53 - so let's just look at developer business
694:54 - enterprise here because basic's not
694:56 - going to be applicable here so for
694:58 - developers 29 usd a month or three
695:01 - percent of the monthly database usage
695:03 - which whichever is greater on the exam
695:05 - they're only going to ask you like is it
695:06 - 29 100 like generally do you know the
695:09 - tier of expensiveness but they're not
695:11 - going to ask you the percentage of usage
695:13 - okay there's not going to be formulas
695:14 - here
695:15 - when you get into business it's a little
695:16 - bit different where they have it in
695:18 - different brackets so it's going to be
695:20 - 10 for the first uh 10 000 and the next
695:23 - is going to be the next 7 000 stuff like
695:25 - that similar for enterprise as well so
695:27 - let's just do some math so we know that
695:30 - we understand how this works so
695:32 - if you if you had a monthly spend of
695:34 - 500. at the developer tier that's three
695:36 - percent of five hundred is fifteen
695:38 - dollars so they go okay what is greater
695:40 - twenty nine dollars or fifteen dollars
695:41 - so you're paying twenty nine dollars if
695:43 - your spend is a thousand dollars that
695:45 - comes up to thirty dollars uh so you're
695:47 - gonna end up paying thirty dollars
695:49 - because that's greater than 29 okay for
695:52 - business if your monthly spend is a
695:53 - thousand that's ten percent of a
695:54 - thousand that's a hundred dollars if
695:56 - your spend is five thousand then you're
695:58 - going to be paying 500 if your monthly
696:00 - spend is 12 000 then the first 10
696:03 - percent of a 10 000 is a thousand and
696:06 - then the next is seven percent of two
696:08 - thousand so your total bill is 140 usd
696:11 - we're not going to do a calculation for
696:12 - enterprise because the same for business
696:14 - but hopefully that gives you an idea
696:15 - there okay
696:19 - [Music]
696:21 - hey it's andy brown from exam pro and we
696:22 - are taking a look at a technical account
696:24 - manager also known as a tam and these
696:26 - provide both proactive guidance and
696:28 - reactive support to help you succeed
696:30 - with your aws journey so what does atam
696:34 - do and this is straight from a database
696:36 - job posting
696:38 - what they would do is build solutions
696:39 - provide technical guidance and advocate
696:41 - for the customer
696:42 - ensure aws environments remain
696:44 - operationally healthy while reducing
696:46 - costs and complexity
696:48 - develop trusting relationships with
696:50 - customers understanding their business
696:51 - needs and technical challenges
696:53 - using your technical uh acumen and
696:56 - customer obsession you'll drive
696:58 - technical discussions regarding
696:59 - incidents trade-offs risk management
697:01 - consult with a range of partners from
697:03 - developers through the c-suite
697:04 - executives collaborative with adwords
697:07 - solutions architect business developers
697:09 - professional service consultants and
697:11 - sales account managers proactively find
697:14 - opportunities for customers to gain
697:15 - additional value from aws
697:17 - provide detailed reviews of service
697:19 - disruptions metrics detailed pre-launch
697:21 - planning
697:22 - being a part of a wider enterprise
697:25 - support team providing post-scale con uh
697:28 - consolidative expertise
697:30 - solve a variety of problems across
697:32 - different customers as they migrate
697:34 - their workloads to the cloud
697:35 - uplift customer capabilities by running
697:37 - workshops brown bag sessions brown bag
697:40 - sessions being a sessions that occur at
697:42 - lunch time something you can learn in 30
697:44 - minutes an hour and so one thing that's
697:46 - really important to understand is that
697:47 - tams follow the amazon leadership
697:49 - principles especially about customer uh
697:52 - being customer obsessed and we do cover
697:54 - the amazon leadership principle
697:56 - somewhere in this course and tams are
697:58 - only available at the enterprise support
697:59 - tier so hopefully that gives you an idea
698:01 - what a tam does
698:02 - [Music]
698:06 - hey this is andrew brown from exam pro
698:08 - in this follow along i'm going to show
698:10 - you um database support and in order to
698:12 - use ada support or to change your level
698:15 - of support you're going to need to be
698:16 - logged into the root account i should
698:18 - say you can use support with im users
698:21 - but if you want to change the support
698:22 - plan you're going to have to be the root
698:24 - user so the top right corner i'm going
698:25 - to support
698:26 - and notice here on the left hand side
698:28 - right now i have a basic plan
698:31 - and so before we look at changing our
698:33 - plan i'm just going to go create a case
698:35 - and we're going to just take a look
698:38 - at some of the options that are open to
698:39 - us so we have account billing support
698:41 - service limit increase technical support
698:43 - notice this is grayed out so we cannot
698:46 - select anything here
698:48 - i can go to here and increase our
698:50 - service limit
698:51 - and this is something that you might
698:52 - have to do pretty soon earlier in your
698:54 - account you might say hey i need more of
698:56 - something like ec2 or a very common
698:59 - thing is ses
699:00 - so for ses you might say hey
699:02 - i need to have this amount of emails for
699:05 - etc okay
699:07 - so um if we go over to account and
699:09 - billing support uh we can go here and
699:11 - ask anything we want so if it's about
699:13 - the free tier i could say
699:15 - ask the general question getting started
699:17 - and saying
699:18 - uh what is free on aws
699:22 - um
699:24 - i want to know
699:26 - what is free on aws
699:28 - and you can attach three attachments
699:31 - there you can choose via web
699:33 - and phone which is really nice um but
699:35 - today i'm just going to do web here and
699:37 - submit that just to kind of show you
699:38 - that as an example and so what that is
699:40 - going to do is open a case and then we
699:42 - will see probably respond
699:44 - in 24 hours to 48 hours just depends on
699:48 - whether it's the weekend or not because
699:49 - it's based on business hours of course
699:52 - so now that we have an understanding of
699:54 - basic let's go take a look at what the
699:57 - other tiers look like so we have basic
699:58 - developer business and enterprise
700:00 - enterprise being extremely expensive
700:02 - developer being affordable and then
700:04 - business being um you know affordable
700:06 - for businesses so i would say developer
700:08 - is okay it gives you um
700:11 - it gives you better support but it's all
700:13 - via email and so you know if you really
700:16 - want good support you're gonna have to
700:17 - pay the business one and that's the one
700:19 - that i use quite a bit so if i change my
700:21 - plan i'm gonna go over to business and
700:23 - this is gonna cost me 93 bucks just to
700:25 - do to show you here today
700:27 - so i'm going to go ahead and click that
700:28 - and so it's now processing it
700:31 - and so what's going to happen is
700:33 - i'm going to have to wait for this basic
700:35 - to switch to business because if i go to
700:36 - the case here it hasn't happened as of
700:39 - yet
700:40 - so notice i cannot select this so i'm
700:42 - going to see you back here it may be
700:43 - like four or five minutes or however
700:45 - long it takes and we'll take a look then
700:47 - okay great so after a few minutes it
700:49 - says my plan is now business and what i
700:51 - can do is go ahead and create a new case
700:53 - and so i can go over to technical
700:54 - support and ask a question so if i was
700:56 - having issues with anything it doesn't
700:58 - matter what i could go over to ec2
701:00 - linux and then i could choose my
701:02 - category so i could say i'm having an
701:04 - issue with um
701:06 - systems manager
701:08 - and
701:09 - a lot of times they like you to provide
701:10 - the instance id it's going to change
701:12 - based on what service you choose here
701:14 - but you'll get different information
701:15 - i'll just say
701:16 - i need help
701:19 - with
701:19 - [Music]
701:21 - logging into my ec2 instance managed by
701:25 - ssm
701:26 - so i could say i created an ec2 instance
701:30 - and i am attempting to access
701:34 - the instance via
701:36 - sessions manager
701:39 - but it is not working
701:42 - i think i have a role issue and then i'm
701:44 - just going to go down here and say
701:46 - this is not a real question
701:50 - i
701:51 - am filming a demo video for a tutorial
701:55 - video
701:59 - on how to use support okay and so once
702:02 - we do that we have the option of web
702:04 - chat and phone so if you use phone
702:06 - you're going to enter your phone number
702:07 - in and they're going to call you back
702:10 - usually you'll be on hold for
702:12 - anywhere for five minutes to an hour it
702:14 - just depends usually it's within 15
702:16 - minutes so it's very good of course it
702:18 - depends on the time of day and your
702:19 - location things like that and the
702:21 - service because there's different
702:23 - support engineers for different types of
702:25 - services and the the balance of those
702:27 - are different but generally chat is
702:29 - pretty good so i can go here and i'm
702:31 - just going to hit submit and it's going
702:32 - to open a chat box and so you just wait
702:35 - okay
702:36 - and sometimes it's super fast and
702:39 - sometimes it takes uh minutes okay so
702:42 - we are going to just sit here for a bit
702:45 - and um you know i'll just pop back here
702:48 - when there is somebody to talk to okay
702:51 - okay so after waiting a little while
702:52 - looks like uh we've been connected here
702:54 - so it took a bit of time so we're just
702:56 - going to say hello hi umair
702:59 - this is andrew brown
703:02 - i am recording a video to teach people
703:06 - how to use aws
703:09 - and i wanted to show them
703:11 - how it was support works
703:17 - so i'm just showing them
703:20 - how the chat system works
703:24 - say hello
703:31 - and hopefully they'll appreciate or they
703:33 - won't it just doesn't really matter
703:44 - we'll give them a moment
703:58 - there we go
704:02 - that's it
704:05 - thanks for your help
704:08 - okay so that's pretty much it um so
704:10 - you know there's nothing really uh
704:12 - special about that but the idea is when
704:14 - you are typing with them it will appear
704:16 - in the correspondence there so i'm just
704:18 - going to end the chat
704:19 - okay
704:21 - and then i'm just going to mark that
704:22 - case as result sometimes they will ask
704:24 - you to resolve it
704:26 - if i go to cases i probably have some
704:27 - previous ones here um and i have a lot
704:30 - but i don't know why they don't all show
704:31 - up here so you can see this one is
704:33 - pending this one is resolved i go back
704:35 - to this one you can kind of see that the
704:37 - history of a conversation is kept and
704:39 - you can go back and forth
704:41 - with the people there
704:43 - um yeah that's pretty much it you can
704:45 - also do screen sharing so they might
704:47 - send you a request to go on zoom or
704:49 - download this piece of software that
704:51 - shares your screen and so that is
704:53 - another option as well so they can get
704:54 - pretty hands-on to help you
704:57 - with your problems there but that's
704:58 - pretty much all i wanted to show you
704:59 - with support i'm going to downgrade this
705:01 - and i'm not sure if they're going to
705:02 - give you back my money sometimes they'll
705:04 - prorate it for you but i'm going here
705:06 - and go back to basic
705:07 - um so we will also refine your credit
705:09 - card directly in the month's remaining
705:11 - fees on your old plan which you
705:13 - previously paid you're obligated to pay
705:15 - a minimum of 30 days of support each
705:17 - time you register so i'm not going to
705:19 - get any money back which is totally fine
705:20 - because i just wanted to show you how
705:22 - that works but business support is
705:23 - definitely worth it
705:25 - and uh you know that's it
705:27 - [Music]
705:32 - so the aws marketplace is a curated
705:34 - digital catalog with thousands of
705:36 - software listings from independent
705:37 - software vendors uh easily find buy test
705:40 - and deploy software that already runs on
705:42 - abs the product can be free to use or
705:44 - can have an associated charge the charge
705:47 - becomes part of your abs bill and once
705:49 - you pay database market pays the
705:51 - provider the sales channel for isv and
705:53 - consulting partners allow you to sell
705:55 - your solutions to other awes customers
705:57 - products can be offered such as amis it
706:00 - is confirmation templates software the
706:02 - service offerings web acls ableist laugh
706:04 - and rules so it sounds great um if you
706:07 - want to sell here i think you need like
706:09 - a u.s bank account to do it um and you
706:12 - know sometimes database marketplace is
706:13 - just part of aws so like when you're
706:16 - using the ec2 marketplace you are
706:17 - technically using the aws marketplace
706:20 - but they also have like a dedicated page
706:22 - for it so it's integrated with some
706:23 - services and it's also standalone okay
706:26 - [Music]
706:30 - hey this is andrew brown from exam pro
706:32 - in this follow along we're going to take
706:33 - a look at the adabus marketplace so what
706:35 - i want you to do is go to the top and
706:36 - type in marketplace and that will bring
706:39 - us over to here the marketplace can be
706:41 - found in a variety of different places
706:42 - on the platform here you can see that uh
706:44 - previously it was using something called
706:46 - guacamole bastion host to launch a
706:48 - server
706:49 - but the idea is that um you can discover
706:51 - products and subscriptions that you
706:53 - might want to utilize so if i go over
706:55 - here there's a variety of different
706:58 - things
706:59 - and so it could be like i want to have
707:02 - something like a firewall that might be
707:05 - something that we might be interested in
707:06 - so we can search there and there's like
707:08 - bring your own license firewall so maybe
707:09 - you have a license with this and you
707:11 - want to run it on an ec2 instance
707:14 - something like that
707:15 - again it's not like super complicated uh
707:17 - what's going on here but a lot of times
707:19 - you know when you're using services
707:20 - you're accessing the marketplace anyway
707:23 - so like when i'm launching an ec2
707:25 - instance
707:26 - noticeable on the left-hand side is
707:28 - 8-bit marketplace and so i don't have to
707:29 - go to the marketplace there i can just
707:31 - kind of like check out the thing i want
707:33 - and that's pretty much all there really
707:35 - is to it okay so you know hopefully that
707:37 - makes sense
707:38 - [Music]
707:41 - well let's take a look here at
707:42 - consolidated billing so this is a
707:44 - feature of abuse organizations that
707:46 - allows you to pay for multiple accounts
707:48 - via one bill
707:49 - so the idea here is we have a master
707:51 - account and we have member accounts and
707:53 - i'm pretty sure that we probably call
707:55 - this root account now i don't think
707:57 - account might be a dated term but it's
707:58 - still showing up in the documentation
708:00 - the idea is that if you have member
708:01 - accounts within your organization
708:03 - they're all going to be consolidated
708:04 - under the single account if you have an
708:06 - account outside of your organization
708:08 - you know this is not going to give you
708:10 - this is going to be basically a separate
708:12 - bill
708:13 - as if it's like a standalone
708:14 - organization or what have you okay
708:18 - so
708:18 - for billing aws treats all accounts in
708:20 - an organization as if they were one
708:22 - account you can designate one
708:25 - master or root account that pays the
708:27 - charges for all the other member
708:28 - accounts consolidate billing is offered
708:30 - at no additional cost you can use cost
708:33 - explorer to visualize usage for
708:35 - consolidated billing which we can see i
708:37 - have the icon here
708:39 - you can combine the usage across all
708:41 - accounts in the organization to
708:43 - to share the volume pricing discount
708:45 - which we did cover in this course
708:46 - separately if you want an account to be
708:48 - able to leave the organization you do
708:50 - have to attach it to a new payment
708:52 - method so if let's say you had an
708:54 - account and you want to give it to your
708:55 - friend or whatever they have to hook up
708:57 - their cred their credit card but you can
708:58 - totally have
709:00 - an account leave an organization but you
709:01 - have to deal with that billing aspect
709:03 - okay
709:07 - [Music]
709:09 - all right so there's a really cool way
709:11 - to save an aws and that's through volume
709:12 - discounts and it's available for many
709:14 - services the more you use the more you
709:16 - save is the idea behind it um and so
709:18 - consolidating billing lets you take
709:20 - advantage of volume discounts this is a
709:22 - particular feature of database
709:23 - organization so if you do not have the
709:25 - orgs turned on you're not going to be
709:26 - able to take advantage of that okay
709:28 - so one example would be something like
709:30 - data transfer where it is billed for the
709:33 - first 10 terabytes at 17 cents or sorry
709:36 - point 17 cents and then the next 40
709:40 - terabytes it will be at point 13 cents
709:42 - okay so if we had two accounts um such
709:45 - as odo and dax and they're not with an
709:48 - ableist organization we can calculate
709:50 - those and see what they are
709:51 - unconsolidated and just so you know one
709:53 - terabyte equals 1.024 gigabytes and
709:55 - that's what we're going to see in these
709:56 - calculations so for odo uh you know if
709:58 - he has four terabytes and that is uh
710:01 - we calculate the gigabytes there we
710:03 - times it by
710:04 - uh the um set value there we're going to
710:07 - get 696 dollars okay
710:09 - for dax we're going to end up with uh
710:12 - about 13.92 there and so if we were to
710:15 - add those up the bill would come out to
710:18 - 2088
710:20 - okay so the idea is that there's an
710:22 - organization and they like your company
710:24 - and they created two accounts but
710:26 - they're just not within an organization
710:28 - by having them in the organization
710:29 - you're gonna save um about almost eighty
710:32 - dollars there so
710:34 - um that is a reason why you'd want to
710:36 - use volume discounts okay
710:38 - [Music]
710:42 - hey this is andrew brown from example
710:44 - and we're taking a look at abyss trusted
710:45 - advisor so trusted advisor is a
710:47 - recommendation tool which automatically
710:49 - and actively monitors your aws accounts
710:51 - to provide actual recommendations across
710:54 - a series of categories so this is what
710:56 - it looks like i personally prefer the
710:58 - older dashboard but this is what they
711:00 - have now and you can see along the side
711:02 - we have a bunch of categories and then
711:03 - we have some checks here saying uh you
711:06 - know what are we meeting what are we not
711:07 - and you can go in and read each one and
711:10 - they'll tell you so much information
711:12 - they'll even show you like what things
711:14 - are not meeting that requirements in
711:16 - some case you can easily remediate by
711:17 - pressing a button not in all cases but
711:20 - the thing with the ambush trust advisor
711:21 - is think of its trusted advisor like an
711:23 - automated checklist of best practices on
711:25 - aws
711:26 - and they kind of map to
711:28 - the pillars of the well-architected
711:30 - framework not exactly but pretty close
711:32 - but there are five categories of aws
711:33 - trusted advisor
711:35 - so we have cost up to imagine station
711:37 - how much money can we save
711:39 - performance so how can we improve
711:41 - performance security how can we improve
711:43 - security fault tolerance how we can we
711:46 - prevent a disaster or data loss and
711:49 - service limits so are we going to hit
711:51 - the maximum limit for a service
711:54 - and so the next thing we need to discuss
711:56 - is
711:57 - um there is a variation of the amount of
711:59 - checks that are available to you based
712:01 - on your support plan so you know if
712:03 - you're using basic or developer you have
712:05 - seven trusted advisor checks and if you
712:07 - have business enterprise you have all
712:08 - the trusted advisor checks so
712:11 - if we're talking about just the ones
712:12 - that are available to you the ones that
712:13 - come for free is mfa on root account
712:16 - security groups specified ports of
712:17 - unrestricted amazon s3 bucket
712:19 - permissions amazon ebs public snapshots
712:22 - amazon rds public snapshots imu so
712:26 - this is just about alerting you about
712:27 - discouraging the use of the root account
712:29 - service limits so all service limits
712:31 - checks are free um it's weird because
712:34 - they call it the like seven security
712:36 - checks but if you counted all the
712:37 - service limits it'd obviously be too
712:39 - large of a number but notice that one
712:41 - through six are all security checks so
712:44 - you're not getting anything from the
712:45 - other tiers just the security tier and
712:47 - what i want to do is just go over
712:50 - a bunch of available checks out there
712:52 - it's probably not the full list because
712:54 - i couldn't even be bothered to update it
712:55 - if they've added more but it'll give you
712:57 - a general idea of what you could expect
712:58 - under each category so for cost
713:00 - optimization
713:02 - it could be things like looking at idle
713:03 - load bouncers so you know if you have
713:06 - load balancers you're not using you're
713:07 - paying for them so get rid of them
713:09 - unassociated elastic ip addresses so for
713:12 - every ip that's not associated you're
713:13 - paying for as well maybe under
713:15 - performance you have high utilization of
713:17 - amazon ec2 instances so maybe you can
713:20 - save money by switching to smaller
713:22 - instances under security we saw mfa on
713:24 - root account very popular one making
713:27 - sure you turn on key rotation could be
713:29 - something as well there
713:31 - under fault tolerance
713:33 - it could be making sure that you're
713:34 - using backups on your amazon rdes
713:36 - database maybe that's turned off uh for
713:38 - service limits there's just a ton of
713:40 - them and so uh one that that you know
713:43 - might be pertinent to use vpcs or ec2
713:45 - limits so there you go
713:50 - [Music]
713:51 - hey this is andrew brown from exam pro
713:53 - and we're going to take a look at
713:54 - trusted advisors so what i want you to
713:56 - do is go to the top and type in trusted
713:58 - advisor
713:59 - and once you're there you're going to
714:01 - notice on the left hand side we have
714:02 - cost optimization performance security
714:04 - fault tolerance and service limits right
714:06 - now there are no recommended actions
714:08 - because there's not much going on this
714:09 - account and when you uh have the uh free
714:12 - level of support the basic support
714:14 - you're not going to have all these
714:15 - checks but if we go in here we can still
714:17 - see kind of what they do
714:19 - so we have like performance security
714:22 - things like that so these are the ones
714:23 - that we actually can see and they
714:25 - generally work all the same way if you
714:27 - expand here it's going to say amazon ebs
714:30 - public snapshot so check the permission
714:31 - settings for the ebs volume snapshots
714:34 - and alert you if the any snapshots are
714:36 - marked as public
714:37 - and so if you scroll on down if there
714:40 - were ones that were an issue it would
714:41 - tell you right here
714:43 - okay
714:44 - then down below here we see like check
714:46 - buckets in amazon s3 that have open
714:49 - access permissions or allow access to
714:51 - authenticated database users
714:53 - so yellow the acl allows
714:56 - list access for everyone
714:58 - a bucket policy allows for any kind of
715:00 - open access bucket police statements
715:02 - have public grant access so maybe what
715:04 - we can do is to see if we can get this
715:06 - to trigger
715:07 - and so what i'm going to do here is go
715:10 - over to s3 and what we're going to do is
715:12 - make a
715:13 - bucket that has a full axis okay
715:16 - so i'm going to create a new bucket and
715:17 - we'll say my exposed bucket
715:21 - we'll scroll on down here and we'll just
715:23 - check box that off and create the bucket
715:26 - let's say i acknowledge that is totally
715:27 - fine
715:30 - okay so now i have a bucket that is 100
715:32 - exposed if we go back to trusted advisor
715:34 - give this a refresh
715:36 - i'm not sure how fast it will show up
715:38 - here but if i expand
715:40 - so it says the bucket acl allows upload
715:44 - delete for everyone the trusted advisor
715:46 - does not have permissions to check the
715:47 - policy
715:49 - uh bucket policy statements that grant
715:51 - public access
715:53 - so what we could try to do is make a
715:56 - policy
716:02 - and try to grant all access here so
716:05 - i'm not writing these every single day
716:06 - but i'm sure we could try to figure this
716:08 - out
716:10 - um
716:13 - we'll say s3 bucket policy public access
716:18 - public read
716:26 - and so that one might be a good example
716:28 - so i'm going to go ahead and copy this
716:30 - one granting read only permission to
716:31 - anonymous users
716:33 - i don't recommend you doing this i'm
716:35 - just doing this to show you to see if we
716:36 - can get the trusted advisor to check
716:38 - because i don't want you to
716:39 - do this and forget about it and then
716:41 - have a serious issue but the principle
716:43 - is set to anybody so anyone can read it
716:46 - here it's saying get object etc then
716:48 - it's saying what particular resource so
716:50 - this one is going to be for
716:52 - the bucket in question here which is my
716:54 - exposed
716:56 - bucket
716:58 - we're going to scroll on down save the
717:00 - changes
717:02 - okay so this bucket is publicly
717:03 - accessible we're going to go back over
717:05 - here refresh and see what we can see
717:11 - okay so checks buckets in s3 etc so it
717:15 - should appear under here
717:18 - and it could be that it's just going to
717:19 - take some time so what i'm going to do
717:21 - is i'm just going to hang tight for a
717:23 - little bit oh there we go okay
717:25 - so it's showing up and i guess it just
717:27 - took some time to populate and so here
717:29 - we can see we have a yellow symbol it's
717:31 - a warning saying hey there's a problem
717:33 - here if we go back to the dashboard i
717:35 - wonder if that shows up so this one's
717:36 - for investigation and recommendation
717:39 - so you know hopefully that kind of makes
717:41 - sense to you i think in some cases you
717:43 - can do remediation from
717:45 - from here or at least you can go and
717:47 - check box and say okay um
717:49 - excuse me ignore
717:51 - gonna swore there's remediation for some
717:53 - of these
717:55 - but in any case you know that's
717:57 - generally what trusted advisor does
718:00 - i think that you probably can have it so
718:02 - it gives you alerts
718:04 - so yeah you could set recipients for
718:05 - particular things like if there's a
718:06 - security issue that i could email a
718:09 - particular person on your team and they
718:10 - could deal with it but that's pretty
718:12 - much it so what i'm going to do is go
718:13 - ahead and delete this bucket i'm all
718:14 - done with it
718:17 - we'll go delete
718:19 - and say my delete my exposed bucket here
718:22 - to delete it
718:24 - and that is it okay
718:28 - [Music]
718:32 - let's cover the concepts of service
718:34 - level agreements also known as slas so
718:36 - an sla is a formal commitment about the
718:38 - expected level of service between a
718:40 - customer provider when a service level
718:42 - is not met and if customer meets its
718:44 - obligation under the sla customer will
718:46 - be eligible to receive compensation so
718:48 - financial or service credits and so when
718:50 - we talk about slas then we talk about
718:52 - sli so at sli service level indicator is
718:55 - a metric or measurement that indicates
718:57 - what measure performance a customer is
718:59 - receiving at a given time
719:01 - a sli metric could be uptime performance
719:03 - availability throughput latency error
719:05 - rate durability correctness
719:07 - and if we're talking about sli's then
719:08 - we're talking about slos service level
719:10 - objectives so the objective that that
719:12 - the provider has agreed to meet as wells
719:14 - are represented as a specific target
719:16 - percentage over a period of time
719:18 - and so an example of a target percentage
719:21 - would be something that says
719:23 - availability sla of 99.99
719:26 - in a period of three months all right
719:28 - and let's just talk about target
719:30 - percentages in the way they can be
719:31 - represented very common ones we will see
719:33 - is 99.95 percent 99.99
719:38 - uh then we have 99 followed by nine
719:42 - nines and so commonly we just say we
719:44 - call this nine nines okay and then
719:46 - there's one
719:47 - nine elevens so if somebody says we have
719:49 - an sla guaranteeing of of 911s it's
719:52 - going to be the 99 followed by 0.911s
719:55 - all right
719:56 - [Music]
720:01 - let's take a look at abus service level
720:03 - agreements and so there are a lot of
720:04 - them and i just wanted to show you a few
720:07 - services to give you an idea how they
720:09 - work
720:10 - on the exam they're not going to ask you
720:12 - like oh what's dynamodb's sla for global
720:14 - tables
720:16 - but generally we should just go through
720:17 - this because it's good practice so let's
720:18 - take a look at dynamodb sla so abyss
720:21 - will use commercially reasonable efforts
720:22 - to make dynamodb available with a
720:24 - monthly uptime percentage of each aws
720:27 - region during any monthly billing cycle
720:30 - so for a at least
720:31 - 99.999 percent if global tables sla
720:34 - supplies or 99.99 if the standard sla
720:39 - applies in the event dynamodb does not
720:41 - meet the service commitment you'll be
720:42 - eligible to receive service credits
720:44 - described below so we have monthly
720:46 - uptime percentage and the service credit
720:48 - percentage we get global tables standard
720:50 - tables so let's take a look here
720:53 - so if less than 99.999 but equal to or
720:57 - greater than
720:58 - 99.0 percent is met so if if the service
721:02 - ends up being this you'll get 10
721:04 - back of what you spent as service
721:05 - credits
721:06 - if it drops between
721:08 - 99.0 and 95.0 you get 25 percent back if
721:12 - it's less than 95
721:14 - percent
721:15 - um then it's a hundred percent back
721:18 - okay and you get the general idea here
721:19 - sla is going to be slightly different
721:21 - with their drops now let's take a look
721:23 - at um a compute so compute is going to
721:25 - apply across a bunch of compute services
721:29 - probably because they're all using ec2
721:30 - underneath so that's probably the reason
721:32 - for it so we have ec2 ebs ecs eks and
721:37 - abus makes two sla commitments for the
721:40 - included services so we have a region
721:42 - level sla that governs included services
721:45 - deployed across multiple azs or regions
721:47 - and an instance level sla that governs
721:49 - amazon ec2 instances individually
721:52 - and again we have our monthly up time
721:53 - percentage our service credit percentage
721:56 - region and instance level so you can
721:58 - just see the same thing it's like it's
722:00 - going to change based on uh what it can
722:02 - meet then we'll take a look at one more
722:04 - like rds so relational database uh
722:07 - service so abs will use commercially
722:10 - reasonable efforts to make multi-az
722:12 - instances available with monthly uptime
722:13 - percentage of 99.95 during any monthly
722:16 - billing cycle
722:18 - and again you know if if they don't meet
722:19 - those requirements you're gonna get
722:20 - service credits back which basically
722:22 - equal usc dollars on the platform and so
722:25 - for this it looks like that so just
722:26 - notice that you know with comp like
722:28 - compute it was for a a bunch of services
722:30 - for dynamodb it was based on uh
722:33 - particular features like global standard
722:34 - tables sla it's very straightforward uh
722:38 - we didn't do s3 because i just did not
722:40 - want to show you that one it's just too
722:41 - complicated but my point is is that it's
722:43 - going to vary so you have to look up per
722:45 - service okay
722:46 - [Music]
722:50 - hey this is andrew brown from exam pro
722:52 - and we're taking a look at amazon's
722:54 - service level agreements and so the way
722:56 - you find slas is you're pretty much just
722:59 - typing sla for whatever it is so if
723:00 - you're looking for compute you type in
723:02 - sla or you look for a particular service
723:04 - so maybe you say sage maker
723:07 - aws i don't think there's like a generic
723:09 - sla page at least i don't know where it
723:11 - is i always just type in sla to find
723:13 - what it is and through that you can just
723:14 - kind of read through and try to find out
723:17 - uh the things that that matter to you
723:19 - for your business okay
723:21 - [Music]
723:25 - let's take a look here at the service
723:27 - health dashboard and so the service
723:29 - health dashboard shows general status of
723:31 - aws services it's really simple the idea
723:34 - is that you can check based on
723:36 - the geographic area so you'd say north
723:38 - america europe etc and what you'll see
723:41 - is an icon that says whether the service
723:43 - is in in good standing and the details
723:45 - whether the service is operating
723:46 - normally etc notice they also have an
723:48 - rss feed the reason i'm talking about
723:50 - service health dashboards is because i
723:52 - want to talk about personal health
723:53 - dashboards and because they're both
723:55 - called health dashboards it's confusing
723:57 - so i wanted to tell you about this one
723:58 - first so now we'll jump into the aws
724:00 - personal health dashboard
724:06 - so we saw the service health dashboard
724:08 - now let's take a look at the adabus
724:10 - personal health dashboard so this is
724:12 - what it looks like and it provides
724:14 - alerts and guidance for it events that
724:16 - might affect your environment all airbus
724:18 - customers can access the personal health
724:20 - dashboard the personal health dashboard
724:21 - shows recent events to help you manage
724:23 - active events and show proactive
724:25 - notifications so that you can plan for
724:27 - scheduled activities you you can use
724:30 - these alerts to get notified about
724:31 - changes that can affect your invoice
724:33 - resources and then follow the guidance
724:34 - to diagnose and resolve the issue so
724:38 - this is very similar to the service
724:40 - health dashboard but it's personalized
724:41 - for you
724:42 - um and it's you know i i don't see it
724:45 - crop up very often but if you had to
724:48 - create alerts or be reactive to uh
724:50 - things that are happening within your
724:52 - bus this is where you do it okay
724:53 - [Music]
724:58 - so there's a team called aws trust and
725:00 - safety that specifically deals with
725:02 - abuses occurring on the abyss platform
725:04 - and so i'm going to just list of all the
725:06 - cases where you'd want to be contacting
725:08 - them as opposed to support so the first
725:10 - is spam so you're receiving unwanted
725:12 - emails from an abus owned ip address or
725:14 - abus resources are used to spam websites
725:16 - or forms port scanning your logs show
725:18 - that one or more aws owned ip addresses
725:20 - are sending packets to multiple ports on
725:22 - your server
725:24 - you also believe this is an attempt to
725:26 - discover unsecured ports uh dos attacks
725:28 - so your logs show that one or more
725:30 - italy's owned ip addresses are used to
725:32 - flood ports on your resources with
725:33 - packets you also believe this is an
725:35 - attempt to overwhelm or crash your
725:36 - server or the software running on your
725:38 - server intrusion attempts so your logs
725:41 - show that one or more adidas owned ip
725:42 - addresses are used to attempt to log
725:44 - into your resources
725:46 - hosting prohibited content so you have
725:47 - evidence that abyss resources are used
725:49 - to host or distribute prohibited content
725:51 - such as illegal content or copyrighted
725:53 - content without the consent of the
725:54 - copyright holder distributing malware so
725:57 - you have evidence that abus resources
725:59 - are used to distribute software that was
726:00 - knowingly created to compromise
726:02 - or cause harm to computers machines that
726:04 - it's installed on and so in any of these
726:07 - cases you're not going to it with
726:09 - support you're going to open up an abuse
726:11 - ticket and so you got to contact abuse
726:13 - at
726:14 - amazonatabus.com or fill out the
726:18 - amazon abuse form so and this is whether
726:21 - it's coming from
726:23 - an outside ableist account or even your
726:25 - internally if you think that somehow
726:27 - someone has a compromise your account
726:28 - and it's being used any of these ways
726:30 - this is what you're going to do okay
726:32 - [Music]
726:36 - hey this is andrew brown from exam pro
726:38 - and we're looking at awsw so uh we were
726:41 - saying that database has the itabus
726:42 - trust and safety team and what you'll
726:45 - want to do is if you find that there's
726:47 - an issue you're going to report it to
726:48 - this email at abuse at amazon.com or
726:51 - you're going to use this form which is
726:53 - the report amazon it was abuse so you'll
726:55 - go down here you'll sign in you'll put
726:57 - your email in your first name last name
726:58 - org phone number um the source ip the
727:01 - the details uh
727:03 - in here you can even select the type of
727:05 - abuse so you say if it's this kind or
727:07 - that kind things like that it's very
727:09 - straightforward and that's pretty much
727:11 - it okay
727:12 - [Music]
727:16 - hey this is andrew brown from exam pro
727:18 - and we are taking a look at the aws free
727:20 - tier and this allows you to use database
727:22 - at no cost um and when we say free tier
727:24 - there there there's the idea of the
727:27 - first 12 months of sign up there's going
727:28 - to be special offerings or it's free
727:31 - usage up to a certain monthly limit
727:32 - forever
727:34 - and then there's just services that are
727:35 - inherently free which we have a total
727:37 - separate slide on but let's talk about
727:39 - just the free tier stuff and this is
727:41 - absolutely not the full list but it's a
727:44 - good idea like it gives you a good
727:46 - overview of stuff that is free so for
727:48 - ec2 which you use for a web server you
727:50 - get a t2 micro for 750 hours per month
727:53 - for one year
727:54 - and so
727:55 - there's about 730 hours um in a month
727:59 - and so that means you could have a
728:00 - server running
728:03 - the entire month for free
728:05 - and an additional server for a bit as
728:08 - well
728:09 - so for rds which is a relational
728:11 - database service for either mysql or
728:13 - postgres we can do a t2db micro for 750
728:16 - hours for free so there we get our free
728:19 - database and you would be surprised how
728:21 - far you can get with a uh a t2 db micro
728:24 - um you know even for a medium sized
728:26 - startup you can run it on a t2 db micro
728:29 - with no problems then you have your
728:31 - elastic load balancer you get 75 hours
728:33 - per month for one year um so that is a
728:36 - really good thing uh load bouncers
728:37 - usually cost 50 a month so that's great
728:40 - actually all these pretty much cost 15 a
728:41 - month so that's about um 15 30 45
728:46 - month over month for a year that's uh
728:48 - free then you have amazon cloudfront
728:50 - this is where you'd have your home page
728:51 - caching your videos things like that so
728:53 - you get 50 gigabytes data transfer out
728:55 - for the total year then there's amazon
728:57 - connect you get your toll-free number
728:58 - there 90 minutes of a call time per
729:00 - month for one month or for one year
729:02 - sorry amazon elastic cash so you could
729:05 - launch a redis or um elastic cash server
729:07 - you get 70 hours on a cash d3 micro for
729:10 - a year um elastic search service so it's
729:13 - full text search so again 70 50 hours
729:16 - per month for one year pinpoint campaign
729:18 - marketing email so you can send out 5
729:20 - 000 targeted users per month for one
729:22 - year scs so simple email uh service so
729:26 - this is for um transactional emails um
729:29 - so that you send out from your web app
729:30 - so 62 000 emails per month forever
729:32 - airbus code pipeline so one pipeline
729:35 - free it was code build so this is for
729:38 - building out
729:39 - projects or things like that so 100
729:41 - build minutes per month forever it was
729:43 - lambda service compute 1 million free
729:46 - requests per month 3.2 million million
729:49 - seconds of compute time per month for
729:50 - free
729:52 - and you know i like to highlight these
729:53 - ones because for traditional
729:54 - architecture you're always going to have
729:56 - a web server a database a load balancer
729:59 - um and you might even have cloudfront in
730:01 - there as well but uh yeah again there's
730:03 - a huge list and this does not even tap
730:05 - the service of what's free on aws
730:07 - [Music]
730:12 - hey this is andrew brown from exam pro
730:13 - and we are taking a look at abyss
730:15 - promotional credits and these are the
730:16 - equivalent to usd dollars on the abyss
730:18 - platform abs credits can be earned
730:20 - several ways this could be joining the
730:22 - database activate startup program
730:23 - winning a hackathon participating
730:25 - surveys
730:26 - and any other reason that database wants
730:28 - to give credits out
730:29 - once you
730:30 - have
730:31 - a promotional code you click the redeem
730:33 - credit button in the billing console you
730:35 - enter it in and then your credits will
730:37 - be shown there you can monitor them via
730:38 - it was budgets or via cost explorer and
730:41 - probably even billing alarms it was
730:43 - credits generally have an expired day
730:45 - attached to them could be a few months
730:47 - to a year immense credits can be used
730:48 - for most services but there are
730:50 - exceptions where it is credits cannot be
730:51 - used like purchasing a domain via roe 53
730:54 - because uh that domain costs money
730:56 - outside of aws's cost like for their
730:58 - infrastructure and virtual stuff and so
731:00 - for things like that uh you know they're
731:03 - not gonna be you're not gonna be able to
731:04 - use credits for that okay
731:05 - [Music]
731:10 - the adams partner network also known as
731:12 - apn is a global partner program for aws
731:15 - so joining the apn will open your
731:17 - organization up to business
731:18 - opportunities and allow exclusive
731:20 - training and marketing events so when
731:22 - joining the apn you can either be a
731:24 - consulting partner so you help companies
731:25 - utilize database or a technology partner
731:28 - you build technology on top of abs as a
731:30 - service offering and a partner belongs
731:32 - to a specific tier so it's either going
731:34 - to be select advanced or premiere when
731:36 - you sign up it's free to sign up but
731:38 - you're not going to be able to do much
731:39 - until you start uh committing to an
731:42 - annual fee so that's it's like a certain
731:44 - amount of money to uh be able to be part
731:47 - of that tier and it starts in the
731:48 - thousands okay so i think the first tier
731:50 - is like something like a thousand or two
731:51 - thousand dollars and it gets uh more
731:53 - expensive as you go up as a tier and you
731:56 - also have to have particular knowledge
731:57 - requirements so this could be holding uh
732:00 - particular edible certifications at this
732:02 - at the foundational level at the
732:04 - associate level things like that um or
732:07 - it could be uh aws apn exclusive
732:09 - certification so training that um it's
732:11 - not in certifications but there's
732:13 - certifications that are only available
732:15 - to partners saying like how do you it
732:17 - could be like something like how do you
732:18 - uh talk to customers or communication
732:21 - things like that
732:22 - you can get back promotional database
732:24 - credits so you know if you say oh man i
732:27 - spent uh two thousand dollars on just
732:30 - being able to
732:31 - get into the apn at least the idea is
732:33 - that you can generally get back that uh
732:36 - that spend on aws so it's like you
732:38 - committing
732:39 - if you give like two thousand dollars
732:40 - like you're going to commit to keep
732:41 - using aws i'm not showing the annual fee
732:44 - commitments here and the promotional
732:46 - credits that you get back just because
732:48 - they've changed it a couple times on me
732:49 - and i just don't want this slide to go
732:51 - stale in case they happen to change it
732:52 - again so you'll have to look that up to
732:54 - find out what they actually are right
732:55 - now uh you can have unique speak
732:58 - speaking opportunities in the official
732:59 - awesome marketing channels like the
733:01 - blogs or webinars being part of the apn
733:03 - is a requirement to be a sponsor with a
733:05 - vendor booth enables events so when you
733:07 - s when you go to re invent or any aws
733:09 - event all the vendors are part of the
733:12 - apn all right so they've paid their fee
733:13 - and now they paid an additional fee to
733:15 - get their booth
733:16 - but um yeah the bus partner network is
733:19 - very good for
733:21 - helping you find new business and
733:22 - connecting with other people that are
733:23 - building workloads in aws but hopefully
733:25 - that gives you an idea of how that works
733:27 - okay
733:27 - [Music]
733:31 - hey this is andrew brown from exam pro
733:33 - and we are taking a look at ibis budgets
733:35 - so abs budgets gives you the ability to
733:37 - set up alerts if you exceed or
733:38 - approaching your defined budget create
733:41 - cost usage or reservation budgets it can
733:44 - be tracked at the monthly quarterly or
733:46 - yearly levels with customizable start
733:49 - and end dates alert support ec2 rds
733:52 - redshift elasticast reservations
733:54 - uh and so the idea here is you can
733:56 - choose your budget amount so it could be
733:58 - like a hundred dollars it'll even show
733:59 - you what was the last amount if you're
734:02 - resetting the budget
734:03 - it's something new you can choose based
734:06 - on a different kind of unit so if you
734:07 - wanted to be based on
734:09 - running hours on ec2 you could totally
734:11 - do that is budgets can be used to
734:13 - forecast costs but is limited compared
734:15 - to cost explorer or doing your own
734:17 - analysis whether it was costs and uses
734:19 - reports along with business intelligence
734:20 - tools budgets uh based on a fixed cost
734:23 - or or you can plan your cost up front
734:26 - based on your chosen level can be easily
734:28 - managed from the aws budgets dashboard
734:31 - via the aws budgets api get notified by
734:34 - providing email or chat bot and
734:36 - threshold uh how close to the current or
734:38 - forecasted budget um so you'd see a list
734:40 - of budgets here uh current versus
734:42 - forecasted the amount used things like
734:44 - that you can see your budget history you
734:47 - can download a csv uh it'll show you the
734:49 - cost history right in line there which i
734:51 - can't show you it's hard to see there
734:52 - you get the first two budgets are free
734:54 - so there's no reason not to set a budget
734:56 - when you first get into aws and each
734:58 - budget costs about 0.02 cents a day so
735:01 - it's like 60 cents
735:02 - um
735:03 - usd per month for a budget so they're
735:05 - very cheap to use and you've got a limit
735:06 - of 20 000 budgets they're going to be in
735:08 - good shape okay
735:09 - [Music]
735:13 - well let's take a look here at airbus
735:15 - budget reports which is used alongside
735:17 - abs budgets to create and send daily
735:19 - weekly or monthly reports to monitor the
735:21 - performance of your abus budgets that
735:22 - will be emailed to specific emails so
735:24 - it's not too complicated here you say
735:26 - create the report budget choose your
735:28 - frequency
735:29 - the emails you want um an administrative
735:31 - report serves as a more convenient way
735:33 - of staying on top of reports since
735:35 - they're delivered to your email instead
735:36 - of logging into the management console
735:38 - so it's just for those people that just
735:39 - can't be bothered to log in okay
735:45 - well let's take a look here at abyss
735:46 - costs and uses reports so generate a
735:48 - detailed spreadsheet enabling you to
735:50 - better analyze and understand your abs
735:52 - cost so this is kind of what it looks
735:53 - like and when you turn this feature on
735:55 - it will place it into an s3 bucket you
735:57 - could use something like athena to turn
735:59 - the report into a queryable database
736:00 - since it's very easy to consume s3 csvs
736:03 - into athena you could use quicksite to
736:06 - visualize your billing data as graphs so
736:08 - quicksite is a business intelligence
736:10 - tool similar to tableau or power bi you
736:14 - could also ingest this into redshift
736:17 - but the idea here is when you turn it on
736:19 - you can choose how granular you want the
736:21 - data to be hourly daily or monthly if
736:23 - you turn on daily you'll be able to even
736:24 - say spikes of uh of of
736:27 - of costs for ec2 instances which is kind
736:29 - of nice the report will contain cost
736:32 - allocation tags um which i think we have
736:34 - a separate slide on that type of tags
736:37 - and the data is stored in either as
736:39 - either a csv it will be zipped or it
736:41 - will be a par-cat format it just depends
736:43 - on how you want it
736:45 - for that okay
736:47 - [Music]
736:51 - let's talk about cost allocation tags so
736:53 - these are optional metadata that can be
736:55 - attached to aws resources so when you
736:58 - generate a cost and uses report you can
737:00 - use that data to better analyze your
737:02 - data so what you'd have to do is make
737:04 - your way over to cost allocation tags
737:06 - and need to activate the tags you want
737:08 - to show up there are two types of tags
737:10 - so we have user defined so whatever
737:12 - you've previously tagged will show up
737:14 - probably there
737:15 - you turn it on so if you made one with
737:16 - project you turn on project and there's
737:18 - a lot of aws generated ones
737:21 - that you can turn on so there's a huge
737:22 - list there
737:23 - but uh yeah that's particular with
737:26 - cost
737:27 - usage and reports if it says like cost
737:29 - allocation reports it's just that's what
737:31 - costs and usage reports used to be
737:32 - called
737:33 - and some of the documentation is a bit
737:34 - old there but yeah there you go
737:36 - [Music]
737:41 - so you can create your own alarms in
737:43 - cloudwatch alarms to monitor spend and
737:45 - they're commonly called building alarms
737:47 - and so it's just a regular alarm but
737:49 - it's just focused on spend but in order
737:51 - to do this you have to turn on building
737:52 - alerts first in order to be able to use
737:55 - it
737:56 - and then you'll go to cloudwatch alarms
737:58 - and you can choose billing as your
737:59 - metric and then you just set your alarm
738:01 - however you'd want build alarms are much
738:03 - more flexible than aba's budgets and are
738:05 - ideal for more complex use cases for
738:07 - monitoring spend and usage
738:09 - in terms of alerting
738:11 - so you just have to decide what you want
738:13 - to do uh before it was budgets this was
738:15 - the only way to do it and so this is the
738:17 - way i'm used to doing it and i still do
738:19 - it this way today but you know both
738:21 - options are valid and just have to
738:22 - decide what is your use case okay
738:24 - [Music]
738:28 - let's take a look at about cost explorer
738:30 - which lets you visualize understand and
738:31 - manage your aws costs and usage over
738:33 - time so here's a big graphic of aws cost
738:37 - explorer and you can specify time and
738:39 - range and aggregation it has a lot of
738:41 - robust filtering
738:43 - what's really nice is that they have a
738:44 - bunch of default reports for you so i'm
738:47 - just gonna get my pen tool just to show
738:48 - you where that button is it's over uh
738:50 - here
738:51 - uh if you can see my marker there but
738:53 - but you know you can look at things like
738:55 - monthly cost by service monthly cost by
738:57 - linked account daily cost savings
738:59 - marketplace r utilization so there's a
739:01 - bunch there you could also notice that
739:03 - you can create your own report so if you
739:05 - do find something that you like you can
739:06 - save it for later um you can you could
739:09 - have access to forecasting here so you
739:10 - get an idea of the future costs and
739:12 - whether it's been it's gone up or down
739:15 - just to kind of zoom in on some of those
739:16 - filtration options you can choose
739:18 - um either monthly or daily level of
739:21 - of how you want the data to be grouped
739:23 - together
739:24 - and you have a lot of filter control so
739:26 - if i want to just have ec2 instances for
739:29 - a particular region then i can get that
739:31 - filtered information over here and you
739:33 - can see you have a breakdown of the
739:34 - different types so it's very detailed
739:36 - and cost explorer shows up in us east
739:39 - one i'm pretty sure if you click on
739:40 - class explorer we'll just switch you
739:41 - over to that region but just understand
739:43 - that's where it lives okay
739:44 - [Music]
739:48 - hey this is andrew brown from exam pro
739:50 - and in this video i want to show you aws
739:52 - cost explorer so what we'll do is go to
739:55 - the top here and actually on the right
739:56 - hand side we're going to click on the
739:58 - right and go to my billing dashboard and
740:00 - from there on the left hand side we're
740:02 - going to look for cost explorer and then
740:04 - click launch cost explorer and this is
740:06 - where we're going to get to the aws cost
740:08 - management dashboard where this is where
740:10 - we find savings plans reservations
740:11 - things like that on the left hand side
740:13 - click on cost explorer and you can get
740:15 - this nice chart and so the idea is you
740:16 - can change it from monthly to daily if
740:18 - you if you uh prefer
740:21 - okay you can change the scope here maybe
740:23 - we don't need six months we can just go
740:25 - back
740:28 - three months here so there's less data
740:34 - it is a bit delayed when i'm clicking
740:36 - here so it also could be just because
740:37 - i'm doing the daily instead of monthly
740:40 - so you just have to be a little bit
740:41 - patient when uh using this interface
740:44 - you can change it to stack line graph
740:46 - you can kind of see the details there
740:48 - it's not always clear like what others
740:50 - is or things like that and so
740:52 - uh you can drill down and there's like
740:55 - ways of applying filters and things like
740:57 - that
740:59 - i always forget how to do this because
741:01 - it's bringing everything in so you have
741:03 - to hit clear all first i think
741:06 - and
741:10 - oh you have to click into it so like if
741:11 - you wanted to click into it and pick a
741:13 - particular service we could go here and
741:14 - type in ec2
741:17 - and say
741:18 - ec2 instances
741:20 - and then apply that filter so now we can
741:22 - just see exactly that cost or if we want
741:24 - to
741:25 - choose like maybe just rds
741:29 - okay
741:31 - so
741:32 - you know that could be useful for you to
741:33 - see but yeah sometimes it's not always
741:36 - clear and so what i recommend is just go
741:38 - back to your billing dashboard
741:40 - and from there just go to bills
741:42 - okay bills is really really useful
741:44 - because here it shows you exactly every
741:47 - single little service that you're being
741:48 - billed for you can expand it and see
741:50 - exactly where if there you have other
741:52 - accounts you can go into this side here
741:53 - as well and find spend that way
741:56 - but cost explorer is very useful just
741:58 - it's useful in a different way okay so
742:00 - there you go
742:05 - hey this is andrew brown from exam pro
742:07 - and we are taking a look at the database
742:09 - pricing api so with adabs you can
742:11 - programmatically access pricing
742:13 - information to get the latest pricing
742:15 - offerings for services this makes sense
742:17 - because database can change them at any
742:18 - time and so
742:20 - you know you might want to know exactly
742:21 - what the current price is there are two
742:23 - versions of this api so we have the
742:25 - query api known as the pricing service
742:27 - api and you access this via json and
742:30 - then there's the batch api also known as
742:32 - the price
742:34 - list api via html what's odd is that the
742:37 - batch api returns json but you're
742:39 - accessing it via html so
742:42 - you can literally paste those links in
742:43 - your browser for the query api you're
742:45 - actually sending an an application json
742:47 - request
742:49 - so you'd have to use something like
742:50 - postman or something uh you can also
742:52 - subscribe to sns uh notifications to get
742:54 - alerts when pricing for the services
742:56 - change database prices change
742:58 - periodically such as when aws cuts
743:00 - prices when new instance types are
743:01 - launched or when new services are
743:03 - introduced so there you go
743:04 - [Music]
743:09 - hey this is andrew brown from exam pro
743:10 - and what i want to do here is show you
743:12 - savings plans and savings plan is going
743:14 - to be found under the it was cost
743:16 - explorer so just type in cost explorer
743:18 - at the top here or if you want you can
743:19 - type in savings plan as well and once we
743:22 - are here on the left hand side we are
743:23 - going to have a savings plans option so
743:25 - we're going to go to the overview
743:28 - and here it just describes
743:30 - what our savings plans if you want to
743:31 - read through it but down below if you
743:32 - have already some spend happening it's
743:35 - going to make some suggestions and in
743:36 - this particular account it's saying that
743:38 - i could save some money on compute
743:40 - before we take a look here i'm just
743:41 - going to go to the form here and see
743:43 - what we can see so up here we can say
743:46 - commitment two three years by the way
743:48 - you have compute savings which applies
743:49 - to ec2 fargate or lambda then you have
743:52 - the ec2 specific one where
743:54 - uh we can select a very particular type
743:56 - of instance family and then there's the
743:58 - sagemaker savings plans um but if we go
744:00 - here and we just enter in like two
744:03 - dollars
744:05 - all up front
744:06 - i don't really understand it from here
744:08 - because it doesn't make it clear what
744:09 - the savings are
744:11 - um but uh what it does make it very easy
744:13 - is probably if we go over here and then
744:15 - click down on the compute
744:17 - so kind of feel like here would auto
744:19 - fill it in for you and so here i filled
744:20 - it in or sorry it's filled in for me and
744:23 - so here it's saying with a one-year plan
744:26 - all up front for based on the past 30
744:28 - days
744:29 - that it's going to see that i'm going to
744:31 - see a monthly savings of 25 and 36 cents
744:35 - and then i can add it to the cart that
744:36 - way and i kind of feel like that is the
744:38 - easiest way to
744:40 - um figure that out where with um
744:44 - with how it was going that form i just
744:46 - couldn't figure it out myself what the
744:47 - savings were
744:49 - there are some utilization reports and
744:50 - coverage reports honestly i've never
744:52 - really looked at these before
744:54 - um but uh i'm just curious like what
744:56 - we're looking at monthly daily
744:59 - the last
745:01 - let's go a few months here i've been
745:02 - running stuff in this account for a
745:04 - while so there should be something
745:06 - apply
745:09 - so
745:11 - nothing nothing of interest but um i
745:13 - mean i guess you have a lot of use and
745:14 - coverage report
745:16 - utilization report could be interesting
745:18 - but i imagine it's maybe you have to be
745:20 - using you have to have a savings plan
745:22 - before you can see this so that's
745:23 - probably the reason why
745:25 - um but yeah hopefully that gives you a
745:26 - clear idea that you know you can just go
745:28 - down to those recommendations and
745:30 - and see exactly what you can save and
745:32 - you just add it to your cart and then
745:33 - once you want to pay for it you just
745:35 - choose to submit that order and you're
745:37 - all good to go
745:38 - all right so that's savings plans
745:41 - [Music]
745:46 - let's take a look here at defense in
745:47 - depth to understand the layers of
745:49 - security aws has to consider uh for
745:51 - their data centers for their uh virtual
745:53 - workloads and things that you also have
745:55 - to consider when you are
745:57 - uh thinking about security for your
745:59 - cloud resources
746:00 - so in the most interior we have data so
746:04 - this is access to business and customer
746:05 - data and encryption to protect your data
746:08 - then we have applications so
746:10 - applications are secure and free of
746:11 - security vulnerabilities then you have
746:13 - compute so access to virtual machines
746:16 - ports on premise and cloud you have the
746:18 - network layers so this limits
746:20 - communication between resources using
746:22 - segmentation and access controls you
746:24 - have the perimeter itself so distributed
746:26 - denial of service protection to filter
746:28 - large-scale attacks before they can
746:30 - cause denial of service of users you
746:32 - could say that's part of the network
746:33 - layer and that's when i say there are
746:34 - variants on this but we're just
746:36 - separating it out
746:38 - explicitly there we have identity and
746:40 - access so controlling access to
746:41 - infrastructure and change control and
746:43 - then there's the physical layer so
746:46 - limiting access to data centers to only
746:48 - authorize personnel you'll notice i
746:50 - highlighted identity and access in
746:53 - yellow it's because that is considered
746:54 - the new primary um
746:57 - perimeter from the customer's
746:58 - perspective of course ida best has
747:00 - concern about the physical perimeter and
747:02 - things like that but as a customer
747:05 - that's what you're going to be thinking
747:06 - about especially with the zero trust
747:07 - model and when you see these depths the
747:10 - idea is that in order to get here you
747:12 - have to pass through all the stuff so if
747:14 - this um
747:15 - if this outward one is protected pretty
747:17 - well then you generally don't have to
747:19 - worry about the interiors but of course
747:20 - you should um but yeah there you go
747:27 - let's take a look here at
747:28 - confidentiality integrity and
747:30 - availability also known as the cia triad
747:34 - is a model describing the foundation to
747:36 - security principles and their trade-off
747:38 - relationships so here is our triad so we
747:41 - have confidentiality so confidentiality
747:44 - is a component of privacy that
747:45 - implements to protect our data from
747:47 - unauthorized viewers and practice this
747:49 - can be using cryptographic keys to
747:52 - encrypt our data and using keys to
747:54 - encrypt our keys so envelope encryption
747:56 - then we have integrity so maintaining
747:57 - and ensuring the accuracy and
747:59 - completeness of data over its entire
748:00 - lifecycle and practice utilizing
748:02 - acid-compliant databases for valid
748:04 - transactions utilizing tamper evident or
748:07 - tamper proof hardware security modules
748:09 - hsms availability so information needs
748:11 - to be available when needed in practice
748:13 - so high availability mitigating ddos
748:16 - decryption access so the cia triad was
748:19 - first mentioned in this publication in
748:21 - 1977 there have been efforts to expand
748:24 - and modernize or suggest alternatives
748:26 - the cia triad so one was in 1998 for the
748:29 - six atomic elements of information uh or
748:32 - in 2004 we have the engineering
748:34 - principles for uh for information
748:36 - technology security so it has 33
748:37 - security principles but this is still a
748:40 - very popular um
748:42 - model for security uh and it's just to
748:44 - kind of tell you like you know you don't
748:46 - always get everything you don't get all
748:48 - three of them sometimes you have to
748:49 - trade off in your scenario um you know
748:51 - and hopefully some of the terminology
748:53 - here will resonate as we go through more
748:55 - security content
748:56 - [Music]
749:00 - what i want to do here is just to find
749:01 - the term vulnerability so a
749:03 - vulnerability is a whole or weakness in
749:05 - an application which can be designed a
749:07 - design flaw or implementation bug that
749:08 - allows an attacker to cause harm to
749:10 - stakeholders or applications and uh
749:14 - there's a lot of great definitions of
749:16 - vulnerabilities but owasp has a ton of
749:18 - them and we talked about oats when we
749:20 - talk about abuse waff but it's an
749:22 - organization that creates security
749:23 - projects that help you know what you
749:25 - should protect uh or gives you a working
749:28 - example so that you can understand how
749:29 - to get better at security and so they
749:31 - have a lot of ones here but maybe you
749:34 - might notice some here like using a
749:36 - broken or risky cryptographic algorithm
749:39 - maybe there's a memory leak least
749:40 - privileged violation so that's um uh
749:44 - least privilege is something that is a
749:45 - thing that you're always worried about
749:46 - in security improper data validation
749:48 - buffer overflows so you know just to
749:51 - kind of set the tone of what a
749:53 - vulnerability is and things you should
749:54 - be thinking about okay
749:59 - [Music]
750:01 - let's understand what encryption is but
750:02 - before we do we need to understand what
750:04 - is cryptography so this is the practice
750:06 - and study of techniques for secure
750:07 - communication in the presence of third
750:09 - parties called adversaries and
750:11 - encryption is the process of encoding or
750:13 - scrambling information using a key and a
750:15 - cipher to store sensitive data in an
750:18 - unintelligible format as a means of
750:20 - protection
750:21 - an encryption takes in plain text and
750:23 - produces produces a cipher text so
750:26 - here's an example of a very old
750:28 - encryption machine this is the enigma
750:30 - machine used during world war ii
750:33 - and it has a different key for each day
750:34 - that it was used to set the position of
750:36 - the rotors and it relied on simple
750:38 - cipher substitution
750:40 - and so you might be asking what is a
750:42 - cipher and that's what we're going to
750:43 - look at next
750:44 - [Music]
750:48 - so what is a cipher it is an algorithm
750:51 - that performs encryption or decryption
750:53 - so cipher is synonymous with code
750:56 - and the idea is that you use the code to
750:58 - either unlock or or lock up the
751:00 - information that you have so what is a
751:02 - ciphertext a ciphertext is the result of
751:05 - encryption performed on plain text via
751:08 - an algorithm so you lock that up you
751:10 - scramble it it doesn't make sense and
751:12 - you need that code to unlock it to get
751:14 - the information so a good practical
751:17 - example back in the day was a code book
751:19 - and this was the type of document used
751:20 - for gathering and storing cryptographic
751:23 - codes or ciphers so the idea is if we
751:25 - zoomed up on here notice where we have
751:28 - cannot so
751:30 - and it would be zero zero and then there
751:32 - would be give them authority so the idea
751:35 - is
751:36 - zero zero
751:38 - or if you had the word cannot it would
751:39 - translate to zero zero and then you use
751:41 - zero zero to match that up to say what
751:43 - does that actually mean and so that is
751:45 - kind of a very practical example of
751:47 - ciphers in action
751:52 - so we just took a look at encryption but
751:54 - what are cryptographic keys so a a
751:58 - cryptographic key an easy way to think
751:59 - of it is a variable used in conjunction
752:02 - with an encryption algorithm in order to
752:04 - encrypt or decrypt data
752:07 - and there are different kinds of um ones
752:10 - we have so we have symmetric encryption
752:13 - so this is where we have the same key
752:15 - that is used for encoding and decoding
752:17 - and a very popular one and the one
752:19 - you'll see on aws is called advanced
752:21 - encryption standard aes so just take a
752:24 - look at that graphic very closely so we
752:26 - have one key
752:28 - and it is used to encrypt so it produces
752:30 - the cipher and then or cipher text we
752:34 - should say and then it will decrypt and
752:36 - we will get our plain text so one single
752:38 - key
752:39 - then we have asymmetric encryption so
752:42 - two keys are used one in code and one to
752:45 - decode and a very popular one here is
752:47 - rsa
752:49 - if you're wondering what those those
752:50 - words are it's three people's names put
752:52 - together who helped
752:54 - invent this type of algorithm and so
752:57 - here we have
752:58 - one key for encrypt and one key for
753:01 - decrypt and there are two different keys
753:03 - all right
753:04 - [Music]
753:08 - all right let's look at the concept of
753:10 - hashing and salting so for hashing we
753:12 - have a hashing function and this accepts
753:14 - arbitrary size values and maps it to a
753:16 - fixed size data structure hashing can
753:18 - reduce the size of a store value and
753:20 - hashing is a one-way process and is
753:22 - deterministic so a deterministic
753:24 - function always returns the same output
753:27 - output for the same input so if we have
753:29 - something like john smith and we pass it
753:31 - to the hash function it's going to
753:32 - create something that is not human
753:34 - readable but it'll say something like
753:35 - zero two f a e x x y whatever um and it
753:39 - will always produce the same thing if
753:41 - the same key or you know value is being
753:43 - inputted there so the reason we use
753:46 - hashing functions or hash in general is
753:47 - to hash passwords so hash functions are
753:50 - used to store passwords in a database so
753:52 - that the password does not reside in a
753:53 - plain text format so you've heard about
753:56 - all these data breaches where they've
753:57 - stored the password in plain text this
753:59 - is the thing that helps us avoid that
754:01 - issue
754:02 - and the thing again is because it's one
754:04 - way you can't take that hash and unhash
754:06 - it
754:08 - well there are some conditions to it but
754:09 - so to authenticate a user when a user
754:11 - inputs their password it is then hashed
754:13 - so the one that was inputted at the time
754:15 - of you know login and then that hash is
754:17 - compared to the stored hash in the
754:19 - database and if they match the user is
754:22 - successfully logged in so in that case
754:24 - we never ever had to know what the
754:26 - original password looked like uh popular
754:28 - hashing functions are md5 sha-256 or
754:31 - bcrypt
754:32 - if an attacker knows the function you
754:34 - are using
754:35 - and
754:36 - and stole your database they could
754:37 - enumerate a dictionary of passwords to
754:39 - determine the password so they'll never
754:41 - see it but they could just keep on going
754:43 - through that so that's why we salt our
754:45 - passwords so a salt is a random string
754:47 - not known to the attacker that the hash
754:49 - function accepts to mitigate the
754:51 - deterministic nature of a hashing
754:53 - function so there you go
754:55 - [Music]
754:59 - let's take a look here at digital
755:01 - signatures and signing so what is a
755:03 - digital signature is a mathematical
755:05 - scheme for verifying the authenticity of
755:06 - digital messages or documents and a
755:08 - digital signature gives us tamper
755:10 - evidence so did someone mess or modify
755:12 - the data is this data from someone we
755:15 - did not expect it to be is it from the
755:17 - actual sender and so we kind of have
755:18 - this diagram where we have a person who
755:20 - sends or is going to send a message so
755:22 - they sign it and then uh bob verifies
755:25 - that it was for the person who it's from
755:27 - so there are three algorithms to a
755:29 - digital signature the key generation so
755:31 - generates a public and private key
755:34 - then there is signing the process of
755:36 - generating a digital signature with a
755:39 - private key and the inputted value so
755:41 - signing which is what is happening up
755:43 - here signing verification verifies the
755:45 - authenticity of the message with a
755:46 - public key so remember the private key
755:48 - is used for signing and the public key
755:50 - is used for verifying
755:53 - ssh uses a public and private key to
755:55 - authorize remote access into a remote
755:57 - machine such as a virtual machine it is
755:59 - common to use rsa and we saw that rsa is
756:03 - a type of algorithm earlier
756:06 - and so ssh hyphen keygen is a well-known
756:08 - command to generate a public and private
756:10 - key on linux i know this one off the top
756:12 - of my head i always know to do this
756:15 - and so what is code signing so when you
756:17 - use a digital signature to ensure
756:19 - computer code has not been tampered
756:22 - and so that's just a like subset of
756:23 - digital signaturing so you can use this
756:26 - as a means to get into a virtual machine
756:28 - or you can use signing as a means to
756:30 - make sure that the code being committed
756:31 - to your repository is who you expect it
756:34 - to be from so there you go
756:35 - [Music]
756:40 - let's talk about in transit versus at
756:42 - rest encryption so encryption transit
756:44 - this is data that is secure when moving
756:46 - between locations and the algorithms
756:48 - here are tls and ssl then you have
756:51 - encryption at rest so this is data that
756:53 - is secure when residing on storage or
756:55 - within a database so we're looking at
756:57 - aes or rsa which we both covered
757:00 - previously these algorithms so ones that
757:03 - we did not cover was tls and ssl so
757:06 - we'll cover them now so tls transport
757:08 - layer security is an encryption protocol
757:10 - for data integrity between two or more
757:12 - communic communicating computer
757:14 - applications so 1.0 and 1.1 are no
757:17 - longer used but tls 1.2 and 1.3 is the
757:22 - current best practice then we have ssl
757:25 - secure socket layers so an encrypted
757:27 - protocol for date integrity between two
757:28 - or more communicating uh computer
757:30 - applications so
757:32 - 1.0 2.0 and 3.0 are deprecated um and
757:36 - honestly i always get these two mixed up
757:38 - and i always figure uh
757:41 - get confused which is being used but um
757:43 - you know they're always changing on us
757:45 - but just understand generally what these
757:46 - concepts are and that you're familiar
757:48 - with the terms okay
757:49 - [Music]
757:53 - hey this is andrew brown from exam pro
757:55 - and we are taking a look at common
757:56 - compliance programs so these are a set
757:58 - of internal policies and procedures for
758:00 - a company to comply with laws rules and
758:02 - regulations or to uphold business
758:05 - reputation so here we have a bunch of
758:07 - different compliance programs and so
758:09 - some popular ones are like hipaa or
758:12 - pci dss the question is should you know
758:15 - these yes you should generally know the
758:17 - most popular ones because you're going
758:18 - to see them throughout your cloud career
758:21 - and so just getting familiar now is a
758:23 - good time so let's jump into it okay so
758:26 - the first one i want to introduce you to
758:27 - is for ia iso and they have a bunch of
758:30 - different ones so iso is the
758:31 - international organization of
758:33 - standardization and their other one
758:35 - called iec which is the international
758:37 - electro technical commission one deals
758:40 - with uh you know like uh virtual things
758:42 - the other one deals with hardware things
758:43 - but they have a lot of overlapping
758:46 - compliance programs okay
758:48 - and so the most popular absolutely most
758:50 - popular one that i know of is the 270100
758:53 - i know a lot of organizations that are
758:55 - going for their 2701 so this is for
758:57 - control implementation guidance you have
758:59 - the 27017
759:01 - this is enhanced focus on cloud security
759:03 - the 27018 this is protection of personal
759:06 - data in the cloud then you have the
759:09 - 27701 this is privacy information
759:12 - management system so pims framework this
759:14 - outlines controls and processes to
759:16 - manage data privacy and protect pii so
759:19 - that's personally identifiable
759:20 - information then you have system and
759:22 - organization control sock and this is a
759:24 - very popular thing that organizations go
759:26 - for especially the sock too so sock one
759:28 - is 18 standards and report on the
759:30 - effectiveness of internal controls at
759:32 - the service organization relevant to the
759:34 - client's internal control over financial
759:36 - reporting we have stock 2 evaluates
759:38 - internal controls policies and
759:40 - procedures that directly relate to the
759:41 - security of the system at a service
759:43 - organization and stock 3 a report based
759:46 - on the trust
759:47 - service services criteria that can be
759:49 - freely distributed
759:51 - then we have pci dss a set of security
759:54 - standards designed to ensure that all
759:56 - companies that accept process store and
759:58 - transmit credit card information
760:00 - maintains in a secure
760:03 - environment we have a federal
760:05 - information procedure standards or fips
760:06 - so 140 hyphen 2. this is u.s and
760:09 - canadian government standard that
760:11 - specifies the security requirements for
760:13 - cryptographic modules that protect
760:15 - sensitive information then we have a ph
760:18 - ipa this is more relevant to me because
760:20 - i'm actually in ontario and canada but
760:22 - it's also very
760:24 - well known
760:25 - one out there outside of hipaa so this
760:27 - regulates patient protected health
760:29 - information then you actually have hipaa
760:31 - this is the u.s federal law that
760:34 - regulates patient procedure health
760:35 - information then we have a cloud
760:38 - security alliance so csa star
760:40 - certification independent third-party
760:42 - assessment of a cloud provider's
760:45 - security posture if you never heard of
760:46 - csa they have a very uh well-known
760:49 - fundamental uh security certification
760:51 - called the cssk or ccsk i always get
760:54 - that mixed up then we have
760:56 - fedramp which we covered earlier in this
760:58 - course or in the future depending on
760:59 - where we put it but fedramp stands for
761:02 - federal risk and authorization
761:03 - management program it's a us government
761:05 - standardization approach to security
761:06 - authorizations for cloud service
761:08 - offerings if you want to work with the
761:10 - u.s
761:11 - government or places that sell the us
761:13 - government you need fed ramp that
761:15 - similar to criminal justice information
761:17 - services any u.s state or local agency
761:19 - that wants to access the fbi's cgis
761:22 - database is required to adhere to the
761:24 - cgis security policy
761:26 - then we have gdpr
761:29 - the general data protection regulation
761:31 - everyone knows what this is in europe
761:32 - maybe not so much in north america or
761:34 - other places a european privacy law
761:36 - imposes new rules on companies
761:38 - governments agencies nonprofits and
761:40 - other organizations that offer goods and
761:42 - services to people in the european union
761:45 - or that collect analyze data try tied to
761:48 - eu's residents there's a lot of
761:50 - compliance programs out there one that's
761:51 - also very popular is fips but we'll get
761:53 - to that when we talk about kms
761:55 - but yeah there you go
761:57 - [Music]
762:01 - so i just wanted to quickly show you
762:03 - here the aws compliance programs page
762:05 - where they list out all the types of
762:07 - compliance programs that aws is uh
762:09 - working with and that it has different
762:11 - types of certification and attestments
762:13 - which we can use it was artifact or
762:15 - amazon artifact whichever
762:17 - prefix they decide to use for the name
762:18 - there to
762:20 - ensure that it was has in order to meet
762:23 - those regulatory compliance so you can
762:25 - see them all there and if you want to
762:27 - know a little bit more about any of
762:28 - these you just go ahead and click them
762:30 - and you can read and they have
762:31 - additional information so you have a
762:33 - better idea okay
762:37 - [Music]
762:41 - let's talk about pen testing so pen
762:43 - testing is an authorized simulated cyber
762:45 - attack on a computer system performed to
762:47 - evaluate the security of the system and
762:49 - on aws you are allowed to perform
762:51 - uh pen testing but there are some
762:53 - restrictions so permitted services or
762:55 - ec2 instances nat gateways elbs rds so
762:59 - that's relational database service
763:01 - cloudfront aurora api gateways lambda
763:04 - lambda edge functions light cell
763:06 - resources elastic bean stock
763:08 - environments things you cannot do or you
763:10 - should not be doing
763:12 - is dns zone walking via rough d3 hosted
763:14 - zones
763:15 - then there's ddos simulation testing so
763:17 - you should not be doing ddot or dos
763:19 - ddoses
763:21 - or simulated dos or simulated ddos is
763:24 - okay and that doesn't mean that you
763:26 - can't necessarily do them uh again
763:28 - there's a lot of exceptions to the pen
763:29 - testing they have a whole page on this
763:31 - but generally you're not allowed to do
763:32 - ddosing
763:34 - port flooding protocol flooding request
763:36 - flooding can't do any of those things
763:38 - for other simulated events you need to
763:40 - submit a request to a bus a reply could
763:42 - take up to seven days you know again
763:44 - there's a lot of
763:46 - little intricacies here so you'd have to
763:48 - really read up on it if you're
763:49 - interested in doing this okay
763:51 - [Music]
763:55 - hey this is andrew brown from exam pro
763:57 - and we are taking a look at pen testing
763:58 - on the aws platform so they have this
764:00 - page here that tells you what you're
764:02 - allowed to do what you're not allowed to
764:03 - do
764:04 - um and there's some additional things
764:05 - you can read into like the stress test
764:07 - policy the ddos simulate simulation
764:09 - testing policy which i didn't cover in
764:11 - detail
764:12 - in the course content but for whatever
764:14 - reason you're interested in it i just
764:16 - want you to be aware of that kind of
764:17 - stuff if you want to simulate events
764:20 - there is a simulate events form that you
764:21 - have to fill out so yeah open it up and
764:24 - you can kind of read about it and it
764:25 - gives it eight of us a heads up of what
764:28 - you're going to be doing stress test
764:29 - fishing malware analysis other so that
764:32 - way that if you are doing it you're not
764:33 - going to get in trouble they're aware of
764:35 - what you are doing okay so that's pretty
764:37 - much it
764:41 - [Music]
764:42 - hey this is andrew brown from exam pro
764:44 - and we are taking a look at ibis
764:45 - artifact which is a self-serve portal
764:47 - for on-demand access to itabus
764:49 - compliance reports so here's an example
764:51 - of a bunch of different compliance
764:53 - reports that aws could be meeting and
764:55 - the idea is that when you go to this
764:57 - portal within the database management
764:58 - console you'll have a huge list of
765:00 - reports that you can go and access so
765:02 - here i'm searching for canada to get the
765:05 - government of canada partner package and
765:07 - then i go ahead and i download that
765:09 - report as a pdf and then within the pdf
765:12 - we can click a link to get the
765:13 - downloadable excel and that's pretty
765:15 - much what it is it's just if you want to
765:17 - see that databus is being compliant for
765:19 - different programs
765:23 - [Music]
765:24 - hey this is andrew brown from exam pro
765:26 - and we're going to take a look at
765:27 - adobe's artifacts so at the top here
765:29 - we're going to type in artifact
765:31 - and not to be confused with code
765:33 - artifact which i guess is a new service
765:35 - there's just always releasing new
765:36 - services eh
765:37 - and so here we have a video and some
765:41 - things but it's not too hard all we got
765:42 - to do is go to view reports
765:45 - and from here we have all the types of
765:47 - compliance programs or regulatory
765:49 - compliance programs that aws is meeting
765:53 - and we can do is search for something so
765:55 - we type in canada
765:57 - and that's the government of canada
765:58 - partner package and i can go ahead and
766:00 - download that report so when you
766:02 - download it you really want to open this
766:04 - up in
766:06 - um you're going to really want to open
766:07 - this up in
766:10 - adobe acrobat because if you don't open
766:12 - it up in adobe acrobat you're not going
766:14 - to be able to access the downloadables
766:16 - within it i know that's kind of odd to
766:18 - say but that's just what it is you do
766:20 - have to install adobe acrobat reader and
766:23 - once you have it open
766:26 - and i'm just moving it over here this is
766:28 - what you're going to see and
766:30 - it's going to say like hey um oops
766:33 - no i don't want to do that so please
766:34 - scroll to the next page to view the
766:36 - artifact download and so i think that
766:40 - if we go here
766:41 - you know they say scroll to the next
766:43 - page but i'm pretty sure we can just go
766:44 - here on the left hand side and this is
766:46 - what we're looking for that excel
766:48 - spreadsheet so we're going to save that
766:50 - attachment
766:51 - or actually we just can open it up
766:54 - open this file
766:57 - okay and we'll give it a moment i have
766:58 - excel installed
767:00 - and there we go
767:03 - there it is okay so i know it's a little
767:06 - bit odd way to get to those um
767:08 - certificates or reports but that's just
767:11 - how it works um but yeah i mean that's
767:13 - the idea is like if you need to prove
767:15 - that database is meeting whatever those
767:16 - standards are you can just type them in
767:18 - whatever it is i mean like maybe there's
767:20 - like fedramp right whatever it is and
767:22 - download those certificate attestments
767:24 - whatever um and just double check that
767:26 - aws is meeting those standards okay
767:29 - [Music]
767:33 - hey this is andrew brown from exam pro
767:35 - and we are taking a look at abs
767:36 - inspector but before we can answer what
767:38 - it does let's talk about hardening so
767:40 - hardening is the act of eliminating as
767:42 - many security risks as possible
767:44 - hardening is common for virtual machines
767:45 - where you run a collection of certain
767:47 - security checks known as a security
767:49 - benchmark so aws inspector runs a
767:52 - security benchmark against specific ec2
767:54 - instances and you can run a variety of
767:56 - security benchmarks and you can perform
767:58 - network and host assessments and so
768:00 - here's an example of those two check
768:02 - boxes there which you'd say which
768:04 - assessments you want to do so the idea
768:06 - is you have to install the adobe station
768:07 - on your ec2 instance you run an
768:09 - assessment for your assessment target
768:11 - you review your findings and remediate
768:12 - security issues and one very popular
768:14 - benchmark you can run is the cis which
768:17 - has
768:18 - 699 checks so if you don't know what cis
768:21 - it stands for the center of internet
768:22 - security uh and so they are this
768:24 - organization that has a bunch of um
768:28 - uh security controls or check marks uh
768:30 - that are published that they suggest
768:31 - that you should check on your machine
768:33 - [Music]
768:37 - hey this is andrew brown from exam pro
768:39 - and we're looking at ddos so ddos is a
768:42 - type of malicious attack to disrupt
768:44 - normal traffic by flooding a website
768:45 - with a large amount of fake traffic so
768:47 - the idea is we have an attacker and the
768:49 - victim the victim is us and it could be
768:52 - our virtual machines our cloud services
768:55 - the idea is that it's some kind of
768:56 - resource which
768:58 - can take in uh incoming requests over
769:01 - the internet so the idea is the attacker
769:03 - is utilizing the internet and so they
769:05 - may control a bunch of virtual machines
769:07 - or servers they're loaded up with
769:09 - malicious software and the idea is that
769:11 - the attacker is going to tell them all
769:13 - to send a flood of traffic over the
769:16 - internet
769:17 - at your computing resource and this is
769:21 - where your website is going to either
769:23 - start to stall or it's going to become
769:25 - unavailable for your users and so the
769:28 - idea here is that you know if you want
769:29 - to protect against cdos you need some
769:31 - kind of ddos protection traditionally
769:33 - those used to be like third-party
769:35 - services that you uh would have to pay
769:37 - for and the and it would sit in front of
769:39 - your load bouncer or your n server but
769:43 - now the great thing with cloud service
769:44 - providers is that generally their
769:46 - networks have built-in ddos protection
769:48 - so the idea is just by having your
769:50 - compute or your resources on aws you're
769:53 - going to get
769:54 - built-in protection for free via aws
769:56 - shield and we'll talk about that next
769:59 - [Music]
770:03 - hey this is andrew brown from exam pro
770:05 - and we are taking a look at it with
770:06 - shield which is a managed ddos
770:08 - protection service that safeguards
770:10 - applications running on aws so when you
770:14 - route your traffic through refu3 or
770:16 - cloudfront you are using it with shield
770:18 - standard so here's a diagram to kind of
770:20 - show you that it's not just those
770:22 - services but these are the most common
770:23 - ones where you'll have a point of entry
770:25 - into aws
770:26 - so here we could also be including
770:28 - elastic ip it was global accelerator but
770:31 - the idea is that when you go through
770:33 - these services into the airbus network
770:35 - it has shield built in and so you're
770:37 - going to get that protection before
770:38 - those
770:39 - uh before that traffic reaches your
770:42 - cloud services and in this case we're
770:43 - showing uh ec2 instances so it was
770:46 - shield protects against layers three
770:48 - four and seven attacks
770:50 - uh layer three four
770:52 - and seven is based off the osi model
770:54 - which is a um a fundamental networking
770:57 - concept so
770:58 - seven is for the application layer four
771:02 - is the transport layer three is the
771:04 - network layer
771:05 - um there are two different types of
771:07 - plans ready with shield we have shield
771:09 - standard which is free
771:11 - and then shield advance which starts at
771:13 - 3000 usd per year plus some additional
771:16 - costs based on usage of the size of the
771:19 - attack or what services you're using how
771:21 - much traffic is moving in and out
771:23 - so protection against the most common
771:25 - ddos attacks is what shield standard
771:26 - does
771:27 - you have access to tools and best
771:29 - practices to build ddos brazilian
771:31 - architecture it's automatically
771:33 - available on all above services
771:35 - for additional protection against larger
771:37 - and more sophisticated attacks that's
771:38 - where she'll advance comes into play
771:40 - it's available for specific database
771:43 - services so refugee 3 cloud front elb
771:47 - able global accelerator elastic ip
771:51 - and some notable features here is
771:52 - visibility reporting on layer three four
771:54 - and seven you're only going to get seven
771:56 - if you are using it will swap with it uh
771:59 - you have access to team and support so
772:01 - these are ddos experts but you're only
772:02 - gonna get it if you're paying for
772:04 - business or enterprise support as you're
772:06 - paying for this as well uh you also get
772:09 - ddos cost protection just to ensure that
772:11 - you know your bills don't go crazy
772:13 - and it comes with an sla so you have a
772:15 - guarantee that it's going to work
772:17 - both plans integrate with aws web
772:19 - application firewall so waff to give you
772:22 - that layer 7 application protection so
772:24 - understand that if you're not using waff
772:26 - you're not going to be having that layer
772:28 - 7 production okay
772:29 - [Music]
772:33 - hey this is andrew brown from exam pro
772:35 - and we are looking at amazon guard duty
772:38 - so before we look at that we need to
772:39 - understand what is an ids ips
772:42 - so an intrusion detection system and
772:45 - intrusion protection system is used as a
772:48 - device or software application that
772:49 - monitors a network or systems for
772:52 - malicious activity or policy violations
772:55 - so guard duty is a threat detection
772:57 - service which is ids ips that
773:00 - continuously monitors for malicious and
773:02 - suspicious activity and unauthorized
773:04 - behavior it uses machine learning to
773:07 - analyze the following database logs your
773:09 - cloud trail logs your vpc flow logs your
773:12 - dns logs and what it will do is report
773:15 - back to you and say hey um there's this
773:18 - issue here and this is actually one
773:20 - that's very easy to replicate it's just
773:21 - saying somebody is using the root
773:23 - credentials and it's suggesting that you
773:26 - should not be doing that right because
773:27 - you're never supposed to be invoking api
773:29 - calls with the root credentials or you
773:31 - should be limiting that
773:33 - you might also notice that if you want
773:34 - to investigate you can kind of follow up
773:36 - that with uh amazon detective or aws
773:39 - detective which
773:40 - ever
773:41 - prefix they decided to put on that
773:42 - service it will alert you of findings
773:45 - which you can automate an incident
773:47 - response via cloudwatch events which
773:49 - this uh it's been renamed to eventbridge
773:52 - as you know or third party services so
773:53 - you can
773:54 - follow up a remediation action
773:57 - and here is a graphic of amazon guard
774:00 - duty just a bit up closer so you can see
774:02 - all the findings and you can just see
774:04 - you have a lot of detailed information
774:06 - there okay
774:07 - [Music]
774:11 - hey this is andrew brown from exam pro
774:13 - and we're going to take a look at guard
774:14 - duty so guard duty is
774:16 - an intrusion protection and detection
774:19 - service and so what i've done is i've
774:22 - done some bad practices purposely so
774:23 - that i can show you some information in
774:26 - there so i'm gonna go over to guard duty
774:28 - okay and you do have to turn guard duty
774:30 - on and so once scar duty is on you're
774:32 - going to start getting reports coming in
774:34 - so notice here that we have some
774:36 - anomalous behavior eight days ago and so
774:39 - uh that's bako he's my co-founder he's
774:41 - also named andrew as well and so we can
774:43 - kind of see some details here about
774:45 - who's accessing what and what they were
774:47 - doing he's not doing anything malicious
774:49 - but we can have an idea where they're
774:50 - from even shows uh generally where he is
774:52 - which he is near thunder bay and his his
774:55 - provider would be tbaytel
774:57 - um and you can see that he is making uh
774:59 - api calls the scribe account attributes
775:02 - and things like that then the other
775:03 - issue is the root account so there's mfa
775:06 - i turned it off so that we can or maybe
775:08 - it's just usage here i actually do have
775:09 - it turned on i suppose but here we see
775:11 - root credential usage and so it's saying
775:13 - hey you used it 77 times because
775:16 - sometimes i go in and and use the ruby
775:19 - account for tutorials but saying you're
775:21 - using this way too much you've got to
775:23 - stop doing that okay so that's something
775:25 - that is uh pretty interesting with guard
775:27 - duty and it's really cost effective and
775:29 - easy to turn on so you can turn it on
775:32 - looks like they have a new thing for s3
775:34 - have not looked at that as of yet but
775:36 - that's kind of cool kind of feels like
775:37 - that would overlap with amazon macy but
775:40 - whatever and here we get a breakdown of
775:42 - cost so we see cloudtrail vpc full logs
775:44 - dns logs and this is where it would be
775:46 - ingesting data if you want to use that
775:48 - s3 protection you'd have to probably be
775:50 - turning or creating a custom cloud watch
775:52 - trail that has data events to consume
775:54 - that information um
775:57 - you know so you know hopefully that
775:58 - gives you kind of an idea of things you
776:00 - can do and you can also centralize guard
776:02 - duty into one account so you can have
776:04 - one thing that takes care of everything
776:06 - and move all the data across all your
776:07 - accounts into a single place
776:09 - so that's kind of interesting and you
776:11 - can set up follow follow-ups um it's
776:14 - possible that
776:15 - i don't see this
776:17 - this here but generally it would show
776:19 - you
776:22 - uh it would show you a way of like
776:23 - triggering into cloud watch probably
776:25 - could do it pragmatically this is
776:26 - something interesting like the list
776:28 - management you can add trusted ips or
776:30 - threat list so if there's people that
776:31 - you know are fine you can just white
776:33 - list them or if there's people that you
776:34 - know that are bad make sure that they
776:36 - are never allowed to get through so
776:38 - that's pretty much it with guard duty
776:39 - okay
776:40 - [Music]
776:44 - let's take a look here at amazon macy so
776:46 - macy is a fully managed service that
776:48 - continuously monitors s3 data access
776:50 - activity for anomalies and generates
776:52 - detailed alerts when it detects risks of
776:55 - unauthorized access or inadvertent data
776:57 - leaks so macy works by using machine
776:59 - learning to analyze your cloudtrail logs
777:02 - and macy has a variety of alerts so we
777:04 - have anomalized access config compliance
777:07 - credential loss data compliance file
777:10 - hosting identity enumeration information
777:12 - loss
777:14 - location anomaly open permissions
777:16 - privilege escalation ransomware service
777:19 - disruption suspicious access and mac
777:22 - will identify your most at-risk users
777:24 - which could lead to compromise so here's
777:27 - just one little kind of tidbit from the
777:30 - app itself where
777:32 - you have the total users and they
777:33 - categorize them into different uh risks
777:35 - i can't remember which flag means what
777:37 - in here
777:38 - uh amazon macy is an okay service
777:41 - it's very important if you're storing
777:42 - things in s3
777:44 - but i don't i don't use it very often to
777:46 - be honest
777:47 - [Music]
777:52 - hey this is andrew brown from exam pro
777:54 - and we are taking a look at aws virtual
777:56 - private network also known as vpn so aws
777:59 - vpn lets you establish a secure
778:02 - and private tunnel from your network or
778:04 - device to the aws global network it's
778:06 - very important to emphasize the word
778:08 - secure here
778:09 - because when you're using direct connect
778:11 - that will establish a private connection
778:14 - but it's not using any kind of protocol
778:15 - to secure that data in transit whereas
778:18 - database vpn will be using a secure
778:21 - protocol there are two options here we
778:23 - have abyss site to site vpn so securely
778:26 - connect on-premise network or branch
778:28 - office site to vpc
778:30 - and it was client vpn so securely
778:32 - connect users to aws or on-premises
778:34 - networks
778:36 - one thing that we need to understand
778:38 - alongside vpns is ipsec
778:41 - this stands for internet protocol
778:42 - security and is a secure network
778:45 - protocol suite that authenticates and
778:46 - encrypts the packets of data to provide
778:48 - secure encrypted communication
778:50 - between two computers over an internet
778:53 - protocol network and it is used in vpns
778:56 - and it was definitely uses it okay
778:59 - [Music]
779:03 - hey this is andrew brown from exam pro
779:05 - and we are taking a look at aba's web
779:07 - application firewall also known as waff
779:10 - which protects you
779:11 - protects your web application from
779:13 - common web exploits so the idea here is
779:16 - you write your own rules to allow or
779:18 - deny traffic based on the contents of an
779:20 - http requests you use a rule set from a
779:23 - trusted image security partner in the
779:25 - abyss waff rule marketplace
779:27 - waft can be attached to either
779:29 - cloudfront or an application load
779:31 - balancer so here is that diagram the
779:33 - idea is you see cloudfront with the waf
779:36 - or alb with the laugh
779:39 - and what it does is it can protect uh
779:41 - web applications from attacks covered
779:43 - and the owasp10
779:45 - uh top 10 most dangerous attacks if you
779:47 - don't know owas they're the open web
779:49 - application security project and they
779:52 - basically have all these security
779:53 - projects which are things to say hey
779:56 - these are things that you should
779:58 - commonly protect against or they might
780:00 - have like example applications that
780:02 - serve as a means to learn security so we
780:05 - look at the top 10 it's injection broken
780:08 - authentication sensitive data exposure
780:10 - xml external entities so xxe broken
780:14 - access control security
780:16 - misconfigurations cross-site scripting
780:18 - so xss
780:20 - insecure deserialization using
780:22 - components with known vulnerabilities
780:25 - and insufficient logging and monitoring
780:27 - so there you go
780:33 - hey this is andrew brown from exam pro
780:34 - and we're going to take a quick look at
780:36 - adabus web application firewall also
780:38 - known as waff and so in this account i
780:41 - happen to have a waf running
780:43 - so we don't have to create one we
780:45 - already have something we can take a
780:46 - look here so i'm going to go to waff and
780:48 - shield and then on the left hand side
780:50 - you'll notice it's a global service but
780:52 - on the left hand side we're going to be
780:54 - looking for our web acls and so the idea
780:57 - is that when you want to waff you create
780:58 - a web acl and then when then within that
781:01 - web acl you have uh the overview and
781:04 - then you have you can kind of show you
781:06 - kind of the traffic that's going on here
781:08 - we can have our rules and so there's a
781:11 - lot of different kind of managed rule
781:13 - groups that you can use so these are
781:14 - ones that are provided by aws so
781:17 - and a lot of these some of these can be
781:18 - paid some of these are free so you see
781:20 - there's these free rule groups where
781:21 - you're like hey
781:23 - i don't want any anonymous ips you
781:25 - checkbox that on you know or i want to
781:28 - protect against sql injection now the
781:30 - interesting thing is that abyss has this
781:32 - capacity unit so
781:34 - you can't add all of these you can add a
781:36 - certain amount of capacity before you
781:38 - have to
781:41 - uh pay for more or something like that
781:42 - it's just kind of a way to
781:44 - um
781:45 - uh kind of cap the amount of stuff that
781:47 - you can put in in terms of rules um but
781:50 - there's a lot of other um rule groups
781:52 - from third party services like security
781:55 - companies that know what they're doing
781:56 - so if you like fortinet's os top 10 you
781:59 - can subscribe to that in the marketplace
782:01 - and be able to use it
782:03 - but uh yeah so that's how you apply
782:06 - rules
782:07 - there's something called bot control
782:09 - i've never used this before get
782:10 - real-time visibility into bot activity
782:12 - on your resource and controllers what
782:13 - bots allow and block from your resources
782:16 - that sounds really cool i cannot stand
782:18 - bots so i might turn that on myself or
782:21 - take a look at the cost there and see
782:23 - what we can find out but that's pretty
782:24 - much it with waff
782:26 - one thing i would say is that you can
782:28 - block out specific ip addresses or white
782:31 - list specific ip addresses
782:33 - and you might do that through rules i'm
782:35 - just going to see yeah like maybe the
782:36 - bypass here
782:38 - and so these ip addresses are some of
782:41 - our um
782:42 - cloud support engineers where they're
782:44 - using our mid panel and um
782:47 - waff is being too aggressive in terms of
782:50 - protection and so sometimes you have to
782:52 - say hey allow this ip address and let my
782:55 - um
782:56 - you know let my cloud
782:58 - support engineer be able to use the mid
782:59 - panel because they're not malicious okay
783:01 - so that's one little exception there but
783:02 - that's pretty much it okay
783:17 - [Music]
783:22 - hey this is andrew brown from exam pro
783:23 - and we are taking a look at hardware
783:25 - security modules also known as hsm and
783:27 - it's a piece of hardware designed to
783:29 - store encryption keys and it holds keys
783:32 - in memory and never writes on the disk
783:33 - so the idea is that if the hsm was shut
783:36 - down that key would be gone and that
783:38 - would be a guarantee of protection
783:40 - because nobody could you know take the
783:42 - drive and steal it so here is an example
783:44 - of an hsm these are extremely expensive
783:47 - so you definitely don't want to have to
783:49 - buy them yourselves uh they generally
783:51 - follow fips so fips is the federal
783:53 - information processing standard so it's
783:56 - a u.s and canadian government standard
783:58 - that specifies the security requirements
783:59 - for cryptographic modules that protect
784:01 - sensitive information fips is something
784:03 - you want to definitely remember
784:06 - and there are two
784:08 - different
784:09 - protocols here there's actually a bunch
784:10 - of different fips versions but we have
784:13 - fips
784:14 - 142 level 2 and then fips 143 level 3 so
784:19 - let's talk about the difference here so
784:20 - hsms that are multi-tenant are going to
784:23 - be using fips 142 hyphen 2 level 2
784:26 - compliant
784:28 - where you have multiple customers
784:29 - virtually isolated on the hsm
784:32 - and then there are hsms that are single
784:34 - tenant and so they're going to be
784:36 - utilizing fips 140 hyphen 2 level 3
784:39 - compliant so a single customer on a
784:41 - dedicated hsm
784:43 - and so the reason why we have these two
784:45 - levels is that when you have multiple
784:47 - tenants you can say oh right this thing
784:50 - is uh has temper evidence so we can see
784:53 - that somebody was trying to break into
784:54 - it but there's no guarantee of tamp it
784:58 - being tamper proof where level three is
785:00 - tamper proof there's also fips 140
785:04 - hyphen 3 which is the new
785:06 - uh the newer
785:07 - standard but not all cloud resources can
785:11 - meet that standard just because of how
785:13 - they offer the service
785:14 - so
785:15 - again fips 142 is really good but just
785:17 - understand that there are other ones out
785:19 - there and it's very easy to get fips 142
785:22 - level 3 mixed up with pips 140 iphone 3
785:25 - something that i always had
785:27 - a hard time remembering the
785:28 - distinguishing between those two so for
785:31 - multi-tenant this is where we're using
785:33 - adabus key management service and for
785:35 - single tenant we're using aws cloud hsm
785:38 - and the only time you're really using
785:39 - cloud hsm is if you're a large
785:41 - enterprise and you need that regulatory
785:43 - compliance of getting fips 140 heaven to
785:45 - level three okay
785:47 - [Music]
785:51 - hey this is andrew brown from exam pro
785:53 - and we are taking a look at key
785:55 - management service also known as kms and
785:57 - it is a managed service that makes it
785:59 - easy for you to create and control the
786:01 - encryption keys you use to encrypt your
786:02 - data so kms is a multi-tenant hsm so
786:06 - it's a hardware security module
786:08 - and many aws services are integrated to
786:10 - use kms to encrypt your data with a
786:12 - simple checkbox and kms uses envelope
786:15 - encryption so here's that example of a
786:18 - simple checkbox in this case it's for
786:20 - rds and what you'll do is choose a
786:22 - master key a lot of times aws will have
786:24 - a default
786:25 - key for you that's managed by them that
786:27 - is free to use which is really great
786:30 - so for kms it's using envelope
786:32 - encryption so when you encrypt your data
786:34 - your data is protected but you have to
786:36 - protect your encryption key when you
786:38 - encrypt your data key with a master key
786:40 - as an additional layer security so
786:41 - that's how it works so just to make this
786:43 - really clear i have my data i use this
786:46 - key to encrypt this data and now i need
786:48 - to protect this key so i use another key
786:51 - to encrypt
786:53 - this key which
786:55 - forms an envelope and then i store this
786:58 - master key in kms and this one's
787:00 - considered the data key all right
787:03 - [Music]
787:07 - hey this is andrew brown from exam pro
787:09 - and we're going to take a look at key
787:11 - management service also known as kms so
787:13 - type in kms on the top here
787:15 - and we'll pop over here and kms is a way
787:18 - for you to create your own keys or you
787:20 - can use abyss manage keys so up here and
787:23 - not all these appear right away but as
787:24 - you use services um you will itamas will
787:27 - generate out manage keys for you and
787:28 - these are free
787:30 - you can create your own keys um and
787:33 - these cost a dollar each so if i go
787:34 - ahead here and create a key i can choose
787:36 - whether it's symmetric or asymmetric
787:37 - which we definitely learned in the
787:38 - course which is nice for asymmetric you
787:40 - can make it encrypt and decrypt sign and
787:42 - verify and they're just kind of
787:43 - narrowing down the type of key you would
787:45 - use
787:46 - for this you know if i went to symmetric
787:49 - i go here
787:50 - i'm just kind of seeing if i can enter
787:52 - the actual material into the key here
787:56 - so i'm just going to keep clicking
787:57 - through here my custom key
788:00 - generally you don't really need to do
788:01 - this but you know if it's interesting
788:03 - you can set up administrators to say
788:05 - who's allowed to administer the key and
788:07 - then you have someone that
788:10 - is allowed to use the key and you
788:11 - usually want to keep those two accounts
788:13 - separate you don't want the same person
788:14 - administrating and using the key
788:16 - okay keep those two separate and so we
788:18 - would have a key policy so you can
788:20 - change this to say the rules that is
788:22 - allowed to use
788:24 - and then we can go here and hit finish
788:27 - and so there we now have our own custom
788:30 - key
788:31 - and
788:32 - one thing we can do
788:34 - is it's possible to rotate out these
788:36 - keys when you need to be
788:38 - um but anyway
788:40 - when we use kms it's built into
788:42 - basically everything and we've seen it
788:43 - multiple times throughout this course
788:45 - when we've gone over to ec2 we'll just
788:47 - go take a peek at a few different places
788:49 - here
788:50 - so when we've gone to go launch an ec2
788:53 - instance and we go over to storage so we
788:55 - say select
788:57 - and review
788:59 - or next
789:00 - and we go over to storage notice that
789:03 - here this is using encryption right so i
789:05 - can choose that or even my custom key if
789:08 - you're in dynamodb or anywhere else it's
789:09 - always something like a checkbox and you
789:11 - choose your key so that's pretty much
789:12 - all there really is to kms it's very
789:14 - easy to use and there you go
789:15 - [Music]
789:20 - hey this is andrew brown from exam pro
789:22 - and we are going to take a look here at
789:23 - cloud hsm it is a single tenant
789:26 - hsm as a service that automates hardware
789:29 - provisioning software patching high
789:31 - availability and backups so here's the
789:33 - idea is that you have your aws cloud hsm
789:36 - you have your developers interacting
789:38 - with it your application interacting
789:40 - with it you have an hsm client installed
789:42 - in your ec2 instance so that it can
789:45 - access uh the cloud hsm keys
789:48 - so aws cloud hsm enables you to generate
789:51 - and use your encryption keys on fips 140
789:53 - hyphen 2 level 3 validated hardware it's
789:56 - built on open hsm industry standards to
789:58 - integrate with things like pk
790:01 - cs 11 java cryptography extension so jce
790:06 - microsoft crypto and g libraries you can
790:10 - transfer your keys to other commercial
790:12 - commercials hsm solutions to make it
790:14 - easy for you to migrate keys on or off
790:16 - aws configure aws kms to use aws cloud
790:20 - hsm
790:21 - cluster as a custom
790:23 - key store rather than the default kms
790:26 - keystore uh so cloud hsm is
790:29 - way more expensive than kms kms is like
790:32 - free or a dollar per key where cloud hsm
790:35 - is a fixed cost
790:36 - per month because you are getting a
790:38 - dedicated piece of hardware
790:40 - um and there's not a lot of stuff around
790:43 - it so other than the aws kms integration
790:45 - a lot of times it can be really hard to
790:47 - use this as well so the only time you're
790:49 - really going to be using cloud hsm is if
790:51 - you're an enterprise and you need to
790:52 - meet
790:53 - fips 140 hyphen 2 level 3 compliancy
790:56 - okay
790:57 - [Music]
791:01 - hey this is andrew brown from exam pro
791:03 - and we are taking a look at know your
791:05 - initialism so a lot of aws services and
791:08 - concepts and cloud technologies use
791:11 - initialisms to just kind of shorten uh
791:14 - common things that we need to use on a
791:16 - frequent basis and it's going to really
791:18 - help if you learn these because then
791:20 - what you can do is substitute them when
791:22 - you are
791:23 - seeing a service name or something
791:25 - particular and that's going to get you
791:27 - through content a lot faster and in the
791:30 - wild you're going to see these all over
791:32 - the place because people aren't going to
791:33 - say the full name they're going to say
791:34 - the initialism so let's go through them
791:36 - so for iam it's identity and access
791:39 - management for s3 that's simple storage
791:42 - for swfs it's uh swf that's simple
791:46 - workflow service sns is simple
791:48 - notification service sqs is simple queue
791:51 - service scs a simple email service ssm
791:54 - is simple systems manager but uh you
791:57 - know when we see the name it's usually
791:59 - just systems manager but we still use
792:01 - the uh initialism ssm then there's rds
792:04 - relational database service vpc virtual
792:07 - private cloud vpn virtual private
792:09 - network cfn cloud formation waf web
792:13 - application firewall and that is a very
792:15 - common initialism uh not just databus
792:17 - but outside of it as well mq for amazon
792:21 - active mq asg for auto scaling groups
792:24 - tam for technical account manager elb
792:27 - for elastic load balancer alb for the
792:29 - application load balancer nlb for the
792:32 - network load balancer gwlb for the
792:34 - gateway load balancer clb for the
792:37 - classic load balancer ec2 for elastic
792:39 - cloud or cloud compute ecs for elastic
792:42 - container service ecr for elastic
792:44 - container repository ebs for elastic
792:46 - block storage emr for elastic mapreduce
792:50 - efs for elastic file store ebs or eb for
792:54 - elastic beanstalk es for elasticsearch
792:57 - eks for elastic kubernetes service
793:00 - msk for managed kafka service and if you
793:03 - think i got the s and k backwards i did
793:05 - not for whatever reason it's msk
793:08 - uh then uh there's resource manager
793:10 - which is known as ram acm for amazon
793:12 - certificate manager popl for principle
793:15 - of lease privilege which is a concept
793:17 - not a service iot internet of things
793:19 - this is not a service but is a tech
793:22 - concept or cloud concept ri for reserved
793:24 - instances and i'm sure there are more
793:26 - but these are the ones that i know off
793:28 - the top my head uh and they're in my
793:31 - usual use case uh for what i'm doing day
793:33 - to day but a lot of times you'll
793:34 - probably just end up needing to remember
793:36 - asg elb
793:38 - um ec2 s3 things like that okay
793:42 - [Music]
793:47 - all right let's compare aws config and
793:49 - app config which both have configured
793:51 - the name but there are two completely
793:53 - different services so aws config and app
793:56 - config so abs config is a governance
793:58 - tool for compliance as code you can
794:00 - create rules that will check to see if
794:02 - resources are configured the way you
794:03 - expect them to be if a resource drifts
794:05 - from the expected configuration you are
794:07 - notified or aws config can auto
794:09 - remediate correct the configuration back
794:11 - to the expected state for app config it
794:14 - is used to automate the process of
794:15 - deploying application configuration
794:17 - variable changes to your web application
794:19 - you can write a validator to ensure uh
794:22 - the changed variable will not break your
794:24 - web app you can monitor deployments on
794:26 - automate integrations to catch errors or
794:28 - rollbacks so config is for compliance
794:30 - governance app config is for uh config
794:33 - application configure configuration
794:35 - variables so there you go
794:36 - [Music]
794:41 - well let us take a look at sns versus
794:43 - sqs and these things have something in
794:46 - common and it's they both connect apps
794:48 - via messages
794:50 - uh so they're for application
794:52 - integration so let's take a look at sns
794:54 - so simple notification service and then
794:56 - simple queue service okay so sns is
794:59 - intended to pass along messages via a
795:02 - pub sub model whereas sqs queues up
795:04 - messages and has a guaranteed delivery
795:07 - so the idea with sns you send
795:08 - notifications to subscribers of topics
795:11 - via multiple protocols so it could be
795:13 - http email sqs sms and sns is generally
795:18 - used for sending plain text emails which
795:20 - is triggered via other services the best
795:23 - example here is building alarms i know
795:24 - we mentioned this but i like to repeat
795:25 - it so that you absolutely know
795:27 - it can retry sending in the case of
795:30 - failures of https so it does have a
795:32 - retry attempt but that doesn't mean
795:34 - there's a guarantee of delivery it's
795:36 - really good for web hooks simple
795:37 - internal emails triggering lambda
795:39 - functions if you had to compare these to
795:41 - third-party services it's similar to
795:42 - pusher or
795:44 - pub nub so sqs is uh the idea here is
795:47 - that messages are placed into a queue
795:49 - applications pull the queue using the
795:50 - itabus sdk you can
795:53 - retain a message for up to 14 days you
795:55 - can send them in sequential order a
795:57 - sequential order or in parallel you can
796:00 - ensure only one message is sent you can
796:03 - ensure messages are delivered at least
796:04 - once it's really good for delayed tasks
796:07 - queuing up emails um comparable uh stuff
796:10 - would be something like rabbit mq or
796:12 - uh ruby on rails sidekick okay
796:15 - [Music]
796:19 - hey this is danny brown from exam pro
796:20 - and we're doing variation study with sns
796:23 - versus ses versus pinpoint versus work
796:25 - mail and so sns and scs get confused
796:28 - quite often but all of these services uh
796:31 - have something common they all send
796:33 - emails but
796:35 - the utility of email is completely
796:36 - different for each one so the first one
796:39 - is simple notification service is for
796:41 - practical and internal emails so you
796:44 - send notifications to subscribers of
796:45 - topics via multiple protocols so it's
796:47 - not just for email it can handle http it
796:51 - can send to sqs it can send sns or sms
796:54 - messages so um messages to your phone
796:58 - but it does send emails and so sns is
797:01 - generally used for sending plain text
797:02 - emails which is triggered via other aws
797:05 - services the best example of this is a
797:07 - building alarm so most exam questions
797:10 - are going to be talking about sns
797:12 - because lots of services can trigger
797:15 - sns for notifications and so that's the
797:17 - idea it's like oh um you know
797:20 - did somebody spin up a server send off
797:22 - an email via sns uh did we spend too
797:25 - much money here you know all sorts of
797:27 - things can go through sns to send out
797:29 - emails and you need to know what are
797:31 - topics and subscriptions regarding sns
797:35 - then you have ses so simple email
797:37 - service and this is for transactional
797:40 - emails and
797:41 - when i say transaction emails i'm
797:43 - talking about emails that should be
797:44 - triggered based on in-app action so sign
797:46 - up reset password invoices
797:49 - so a cloud-based email service that is
797:52 - similar to this would be like send grid
797:54 - scs sends html emails
797:57 - sns cannot so that is the distinction is
797:59 - that scs can do html and plain text but
798:02 - sns just does plain text and you would
798:04 - not use sns for transactional emails sas
798:07 - can receive inbound emails
798:10 - scs can create email templates custom
798:14 - domain name emails so when you use sns
798:17 - it's whatever amazon gives you it's
798:19 - going to be some weird address but ses
798:21 - is whatever custom domain you want you
798:23 - can also monitor email reputation for
798:24 - scs
798:26 - then you have amazon pinpoint and so
798:28 - this is for promotional emails so these
798:31 - uh when we say promotional we're talking
798:33 - about emails for marketing so you can
798:34 - create email campaigns you can segment
798:36 - your contacts you can create customer
798:38 - journeys via emails um it can do a to b
798:41 - email testing and so scs and pinpoint
798:44 - get mixed up because a lot of people
798:46 - think well can i just use my transaction
798:48 - emails for promotion emails absolutely
798:50 - you can it's not recommended because um
798:53 - you know pinpoint has a lot more
798:55 - functionality around promotional emails
798:57 - they're built differently
798:59 - and so you know just understand that
799:01 - those two have overlapping
799:02 - responsibilities but generally you
799:04 - should use them for what they're for
799:05 - then you have amazon workmail and this
799:07 - is just an email web client so it's
799:09 - similar to gmail or outlook you can
799:11 - create company emails read write and
799:12 - send emails from a web client within the
799:14 - database management console so there you
799:16 - go
799:17 - [Music]
799:20 - let us compare amazon inspector versus
799:23 - adabus trusted advisor so both of these
799:25 - are security tools and they both perform
799:28 - audits but what they do is slightly
799:30 - different so amazon inspector audits a
799:33 - single ec2 instance that you've selected
799:35 - or i suppose you could select multiple
799:37 - ec2s it generates a report from a long
799:39 - list of security checks um and so
799:42 - trusted advisor has checks but uh the
799:44 - the key difference here is that it
799:46 - doesn't generate out a pdf report though
799:48 - i'm sure you could export csv data if
799:50 - you wanted to and then turn that into a
799:51 - report
799:52 - it gives you a holistic view of
799:54 - recommendations across multiple services
799:56 - and best practices so for example if you
799:58 - have an open port on the security groups
800:00 - that can tell you about about that you
800:01 - should enable mfa on your root account
800:03 - when using trusted advisor things like
800:05 - that
800:07 - one thing though is that trust advisor
800:09 - isn't just for security does checks
800:10 - across um
800:12 - five different things
800:14 - but they both use security and they both
800:15 - technically do checks okay
800:20 - [Music]
800:22 - so there are a few services that have
800:24 - connected the name you'd think they'd be
800:26 - related in some way but they absolutely
800:28 - are not and they don't even have similar
800:29 - functionality but let's take a look here
800:31 - so we know the difference the first is
800:33 - direct connect it is a dedicated fiber
800:35 - optics connection from your data center
800:37 - to aws it's intended for large
800:38 - enterprises with their own data center
800:41 - and they need an insanely fast and
800:43 - private connection directly
800:45 - to aws and you'll notice they give
800:46 - private and emphasis because if you need
800:48 - a secure connection you need to apply a
800:51 - database virtual private network
800:53 - connection on top of direct connect then
800:55 - you have amazon connect this is a call
800:57 - center as a service get a toll-free
800:59 - number accept inbound and outbound calls
801:01 - set up automated phone systems uh so if
801:03 - you ever heard of an interactive voice
801:05 - system at ibs this is basically what
801:07 - amazon connect is you have media connect
801:09 - this is the new version of elastic
801:11 - transcoder it converts videos to
801:13 - different video types so if you have
801:15 - let's say a thousand videos you need to
801:16 - transcode them into different video
801:18 - formats maybe you need to apply
801:19 - watermarks insert introduction videos in
801:22 - front of each one this is what you use
801:24 - media connect for okay
801:25 - [Music]
801:29 - just in case you see elastic transcoder
801:32 - as an option i just want you to know
801:34 - what it is compared to media connect so
801:35 - both these services are used for
801:37 - transcoding and technically elastic
801:39 - transcoder is the old way and it was
801:42 - elemental media convert or just media
801:44 - convert is the new way so elastic
801:46 - transcoder was the original transcoding
801:48 - service it may still have chromatic apis
801:51 - or workflows not available in media
801:53 - convert so this could be reasons why we
801:55 - see legacy customers still using it or
801:57 - you know it's just too much effort for
801:58 - them to
801:59 - upgrade to the new one it transcodes
802:01 - videos to streaming formats
802:03 - media convert is more robust transcoding
802:05 - service that can perform various
802:07 - operations during transcoding so it also
802:09 - translates videos to streaming different
802:11 - streaming formats but it overlays images
802:13 - it inserts video clips extracts captions
802:16 - data it has a robust ui so generally
802:19 - it's recommended to use the uh media
802:21 - convert in terms of costs they're
802:22 - basically the same so there's no reason
802:24 - not to use media convert okay
802:29 - [Music]
802:30 - so it was artifact versus amazon
802:32 - inspector get commonly mixed up all the
802:34 - time but both
802:36 - artifact inspector compiler pdf reports
802:38 - so that's where the confusion comes from
802:40 - but let's talk about what is different
802:41 - about the reports so abus artifact
802:43 - enables inspector so for artifact you're
802:46 - answering why should an enterprise trust
802:48 - aws it generates a security report
802:50 - that's based on global compliance
802:52 - frameworks such as sock or pci or a
802:55 - variety of others where amazon inspector
802:57 - is all about how do we know the cc2
802:59 - instance is secure can you prove it so
803:01 - it runs a script that analyzes your ec2
803:03 - instance then generates a pdf report
803:05 - telling you which security checks had
803:07 - passed
803:08 - so the idea here is it's an auto tool
803:09 - for security of ec2 instances so there
803:11 - you go
803:12 - [Music]
803:17 - so let's compare elb versus alb versus
803:20 - nlb versus jwlb versus clb uh because
803:24 - you know when i was first learning aws i
803:26 - was getting confused because there was
803:27 - elastic load balancer but there was
803:29 - these other ones so
803:30 - what gives right so what's happening
803:32 - here is that there is a main service
803:34 - called elastic load balancer elb and it
803:36 - has four different types
803:38 - of possible load balancers so we'll go
803:41 - through all the types so the first is
803:43 - application load bouncer commonly uh
803:46 - initializes alb and so this operates on
803:48 - layer seven for https so this makes
803:51 - sense because that is the application
803:53 - layer and it has some special powers in
803:55 - terms of routing rules so the idea here
803:58 - is you can create rules to change
803:59 - routing based on information found
804:01 - within the https request so let's say
804:03 - you wanted some
804:05 - routes to go that have a particular
804:07 - subdomain to this server and a different
804:10 - subdomain to another one you could do
804:11 - that
804:12 - and because it is an application load
804:14 - bouncer you can attach a web application
804:18 - firewall for protection you can't attach
804:20 - this on the nlb or other ones because
804:22 - they're not application based so that is
804:24 - just a little caveat there
804:26 - then you have network load bouncer uh
804:28 - commonly abbreviated to nlb this
804:30 - operates on layer three and four so
804:32 - we're talking tcp udp this is great for
804:35 - when you have extreme performance that
804:37 - requires tcp and tls traffic it's
804:40 - capable of handling millions of requests
804:42 - per seconds
804:44 - while maintaining ultra low latency it's
804:46 - optimized for sudden and volatile
804:47 - traffic patterns while using a single
804:50 - static ip address per availability zone
804:53 - if you're making video games this is
804:54 - what they like to use is the network
804:56 - load balancer but it has other utilities
804:58 - outside of that then you have gateway
805:01 - load bouncer gwlb this is where you need
805:03 - to deploy a fleet of third-party virtual
805:05 - appliances that support uh i don't know
805:08 - how to say that in abbreviation but i'll
805:09 - just say it's
805:11 - g-e-n-e-v-e
805:13 - um and there's not much we need to know
805:14 - outside of that okay then there is the
805:17 - classic load bouncer uh commonly
805:19 - initializes clb this operates on layer
805:21 - three four and seven it's intended for
805:23 - applications that were built within the
805:25 - ec2 classic network it doesn't support
805:27 - target groups so albs at nlbs
805:31 - use target groups which is just an
805:32 - easier way of grouping together
805:34 - a bunch of
805:35 - targeted resources like compute
805:37 - that we're going to load balance to and
805:39 - with classic load balancer you just
805:40 - directly assign ec2 instances
805:43 - and it's going to be retired on august
805:44 - 15th of 2022 so yeah it looks like it
805:47 - can do a lot of stuff but it also
805:49 - doesn't have any of the superpowers of
805:51 - these specialized ones and so
805:53 - there's no reason to keep it around and
805:55 - generally you should not be using it
805:57 - and so yeah that's about it

Cleaned transcript:

hey this is andrew brown your cloud instructor exam pro bringing you another complete study course and this time it's the aws certified cloud practitioner made available to you here on free code camp and if you think you've seen this course before that's because this is a major update from the very popular 201920 course that had over 2 million views and this time around we have three times more content so this course is designed to help you pass and achieve it was issued certification and the way we're going to do that is by going through lecture content doing labs in our own account utilizing a practice exam downloading the cheat sheets on the day of the exam and then once you pass you can improve on your resume and linkedin you have that either business knowledge to get that cloud job or to get that promotion to tell you a bit about me i was previously the cto of multiple edtech companies with 15 years industry experience five years specializing in the cloud i'm ava's community hero i publish multiple free cloud courses i love star trek and coconut water and i just want to take a moment to thank people like you because it's you that make these free courses possible and if you want to know how to support more free courses like this one the best way is to buy or extra study materials and so for this course it's at exam pro dot co four slash clf hyphen c01 this is where you'll get study notes flash cards quizlets downloadable lecture slides downloadable cheat sheets uh prax exams you can ask questions and get support and i also just want to tell you if you do sign up you're going to get additional stuff already so you'll get the free practice exam and cheat sheet there's no credit card required required and there's no trial limit so there's no reason not to sign up and if there are course updates check the description in the youtube to see if there are any updates okay so there might be corrections additions modifications and this is just going to ensure that you're using utilizing the latest version of this course and so to keep up to date with upcoming courses follow me on twitter at andrew brown and if you are over there i'd love to hear if you have passed your exam and what you'd like to see next so there you go hey this is andrew brown from exam pro and we're at the start of our journey asking the most important question first which is what is the aws certified cloud partitioner so the cloud partitioner is the entry level aida certification teaching cloud fundamentals such as cloud concepts architecture deployment models it will take a close look at database core services a quick look at the vast amount of data services and will cover topics like identity security governance billing pricing support of aws services the course code for this exam is the clf c01 but it's commonly referred to as the ccp and aws is the leading cloud service provider in the world and that makes the certified cloud petitioner the most common starting point for people breaking into the cloud industry no matter what their path is so who is this certification for well you should be considering the aws cloud partitioner if you are new to cloud and need to learn the fundamentals if you are in the executive management or sales level and you need to acquire strategic information about cloud for adoption or migration or you are a senior cloud engineer or solutions architect who needs to reset or refresh their aws knowledge after working for multiple years and just seeing how the landscape has changed so what value does this certification bring well the aws certified cloud practitioner provides the most expensive view possible of cloud architectures and advanced and when we're talking about that expansive view what you should be thinking about is it being a bird's eye view or a 50 000 foot view looking onto a panoramic landscape where you can see everything and the idea of this expansive view is to promote big picture thinking so the idea here is you're zooming out and assessing the cloud it was landscape for changes trends opportunities and being strategic about the approach and process for our cloud journey the innovations cloud practitioner is not a difficult exam it will not validate that you can build cloud workloads for technical implementation roles like a developer engineer devops role it will not be enough to obtain a cloud role but it can help shortlist your resumes for interviews the exam covers content not found in other certifications and it is recommended as an essential study for your aws journey so now let's take a look at the awesome certification roadmap to see where we would go after the cloud petitioner and what kind of uh cloud roles would be associated with those certifications so at the start you get your cloud practitioner which is at the fundamental level after that we have the associate level such as the sysop administrator the developer and the solutions architect followed by the professional level the devops engineer the solutions architect professional and then the specialties such as security advanced networking database machine learning data analytics and sap which just is not on here yet because it's such a new certification so after the cloud practitioner generally people will go for an associate and it's up to you to choose one of the three because they're all great routes but the most common one is the solutions architect associate because the most common role in the industry is a cloud engineer so even though it's called solution to architect they really should have named it cloud engineer because that is really what it is uh if you were to go the developer route you're basically becoming a cloud developer and then if you are going the sysops admin route you are becoming a junior devops engineer and it's not uncommon for people to obtain all three associates and a lot of times the order will be the solution architect first because it's the easiest and and has the broadest services followed by the developer um which adds uh practical programming skills and um life cycle stuff of like deployment for apps followed by the sysops administrator which is considered the hardest of the three in the associate tier from there you can go for the solutions architect professional and that would be associated with a solutions architect or cloud architect role that's basically like a harder version of the cloud engineer with a lot more responsibilities if you were going to devops route you'd go for the devops engineer professional and so this would open you up to roles such as the devops engineer or the site reliable reliability engineer an sre and some people like to get both of the professionals and that could be if you want to be a cloud architect or devops engineer because having adjacent skills and the professionals is always very useful now you don't have to go for a professional after the associate a lot of people will jump over to the specialties and so when we're looking at the solutions architect you basically have any pic after that but generally what i see are people going for data analytics or machine learning so for data analytics this would be if you want to be a data analyst or if you're doing machine learning this is where data scientists will go through the solutions architect route okay for the junior devops you could jump over to security and become a cloud security engineer if you want to go into devsecops so the automation of security operations you probably want to get the devops engineer um or you may be if you're after the devops engineering you might be transitioning to the advanced networking for roles like in netdevops where you're specializing in migration or hybrid engineer for architectures that both use onpremise and the cloud from the devops engineer position you can still go for the database or machine learning certification if you want to become either a data engineer or an ml ops engineer so there's a lot of opportunities here and there is no perfect route but just these are suggestions for you to decide on your own okay so how long is it going to take to pass this certification well it's going to really depend on your background but if we had to generalize it we can look at it uh as kind of a scale and so if you are at the beginner level you're looking at 30 hours of studying and when we say beginner we're saying someone that has never used aws or any cloud provider i have never written code or held a tech role and when we're looking at the other side of it someone that is experienced we're looking at a six hour study time and when i say that i'm talking about somebody that's watching on two times speed and are able to absorb this information uh very quickly so they have practical working experience with aws or they have equivalent experience in another cloud service provider like azure gcp where they can translate that knowledge or they have a very strong background in uh technology where they've worked in the industry for many years and so you know their study time is going to be a lot shorter and so on average most people are going to take about 24 hours to study for this course and when we talk about the kind of stuff that you'll be doing it's going to be 50 lectures and labs and we call our labs follow alongs where the idea is you follow along in your own account and then 50 is the practice exams so if you look at the length of the content which is around uh 12 hours then you know you should expect to spend as much time doing practice exams uh to pass okay and the rem recommended time to study is one to two hours a day for 14 days okay so what kind of effort are we going to have to put in to pass this exam well you have to watch the lecture videos and memorize key information you'll need to do handson labs and follow along with your own account and you will need paid online practice exams that simulate the real exam and the last two here were things that i used to never suggest because you could literally just watch the videos and pass however edibus has made this exam a lot more difficult and so for these last two points you do have to do these two things for the paid online practice exams uh that can be a hard for some people so i've made it easier for you by providing you a full free practice exam on exam pro at four slash clf c01 and so you just have to sign up no credit card required and you'll get a full set of 65 questions that simulate the real exam okay so for the contents of the exam it is composed of four domains and each domain has its own weighting which determines how many questions in the domain that will appear so for domain one which is cloud concepts we're looking at 26 percent for domain 2 security and compliance we should expect to see 25 percent of the questions from there for domain 3 which is technology and where we will see the most amount of questions that we're sitting at 33 percent for domain four billing and pricing we have 16 of the exam there so just to emphasize for domain 3 you need to know a wide range of services but you also need to know indepth the core services so where do you take the exam well at an inperson test center or online from the convenience of your own home aws is partnered with two different test center networks the first being psi and the second being pearson vue and they both offer inperson or online and these exams are proctored meaning there is somebody watching you to ensure that you are not cheating okay in order to pass this exam you have to score 700 points out of a thousand and so 700 generally equates to 70 percent but it's around 70 percent because aws uses scaled scoring meaning that they could adjust it based on how many people are passing or failing so always aim to uh get higher than 70 percent the exam contains 65 questions 50 scored and 15 unscored and you can afford to get about 15 questions wrong there is no penalty for wrong questions so you should always choose an answer and the questions come in two formats multiple choice and multiple answers for these unscored questions there are 15 on the exam they will not count towards your final score why is there unsword questions on the exam well unscored questions are used to evaluate the introduction of new questions they can determine if the exam is too easy and the passing score or question difficulty needs to be increased and they can discover users who are attempting to cheat the exam or steal dump exam questions so if you encounter questions you've never studied for that seem really hard keep your cool and remember they may be unscored questions the duration of this exam is 1.5 hours so you have about 1.5 minutes per question the exam time is 90 minutes but the seat time is 120 minutes seat time refers to the amount of time you should allocate for the exam so that means including things like time to review instructions show online proctor your workspace read and accept the nda and complete the exam and provide feedback and when you do pass this exam is valid for 36 months and that equates to three years before recertification hey this is andrew brown from exam pro and i'm on the aws certified cloud partitioner page because what i want to show you here is the exam guide if you're wondering how to book your exam you go to schedule exam there and that's the way you can do it but if you scroll on down there's this download exam guide and this will download a pdf that will tell you everything about the exam and so just make note of the course code this is the clf c01 because if this exam has a major major change they'll call it the co2 okay and then you'll know that this exam might not fit for the new uh the new exam guide okay so if we scroll on down there is a basic introduction they'll say you have to have six months which is totally not true you can get in the cloud with no experience uh and uh be passing this exam within two to three weeks so you can just kind of ignore that so it will just state that there is multiple choice multiple responses also known as multiple answer there are 50 questions of the exam with 15 unscored questions so you'll get 65 questions in total uh it's scored between 100 to 1000. the passing grade is 700 it explains about scaled scoring there then it goes onto the course our content outline where we have the four domains and it has a big breakdown of all the things that could appear on the exam and the thing about this is is that um you know there's only 65 questions but there if you break down all these points there's like three times more information than could possibly show up on the exam so just understand that you are going to be studying a lot of information but only one third of it's going to show up on your exam so what i did is i went through every single one of these things and i made sure that we are covering them some stuff i just never saw an exam and also other people i never saw were design principles um i mean they are generally covered in the well architected framework but it's unusual because some of the things in here i just feel they aren't actually on the exam and they just kind of cram this exam guide together but i was very thorough to make sure to add everything here um so for security and compliance it's just knowing a collection of um database security services and some security concepts for technology this is our largest section you need to know so much stuff but we spent a lot of time in the course just covering technology then you have your billing and pricing and you could also say support and so that covers a lot of interesting thing a lot of stuff around ec2 pricing and then they just have a big list of stuff so this is a bit a bunch of random technologies and concepts that might be covered and then they talk about services and so again we cover basically everything just in case for you but yeah there you go hey this is andrew brown from exam pro and what we're looking at here is a free practice exam that i provide with you uh for this course and all you have to do is sign up on exam pro you don't even need a credit card and you can redeem uh the free available content here and this is really up to date and very well simulates what you will see on the actual exam and it's a full set full 65 questions so you're getting a real simulation here but what i'm going to do is just start it off here we're not going to do the whole thing i'm just going to click through and show you a couple of them so you have an idea um the level of difficulty these questions are so the first question we got presented with here is which support plans provide access to the seven core trusted advisor checks and so that is a question that you might need to answer i don't want to spell this for you so i'm not going to tell you the answer i will go to the next one so a large accounting firm wants to utilize aws to store customer accounting information in archive storage and must store this information for seven years due to regulatory compliance which database service meets this requirement so the first one you'll notice this one is multiple choice or sorry multiple answers so you have to select multiples before you can submit your answer and the next one here is just a single choice so those are the two types of questions you will see on the exam they're not going to ask you anything about coding you're not going to see any kind of code in terms of length that's pretty much what we'll see in terms of the questions i think in many cases i wrote a little bit more more like um in the style the solutions architect associate to make it slightly more difficult just so that you're a little bit over prepared so if you do well on these practice exams you're going to do a well on the real exam okay so i just wanted to kind of get you that exposure there okay hey this is andrew brown from exam pro and we are at the start of our journey asking the most important questions first which is what is cloud computing so cloud computing is the practice of using a network of remote servers hosted on the internet to store manage and process data rather than a local server or personal computer and so when we're talking about onpremise you own the servers you hire the i.t people you pay or rent the real estate you take all the risks but with a cloud provider someone else owns the servers someone else hires the it people someone else pays or rents the real estate and you are responsible for configuring cloud services and code and someone takes care of the rest of it for you okay so to understand cloud computing we need to look at the evolution of cloud hosting going all the way back to 1995 where if you wanted to host your website or web app you'd have to get a dedicated server so that would be one physical machine dedicated to a single business running a single project a site or an app and as you can imagine these are expensive because you have to buy out write the hardware have a place to store it the network connection having a person to maintain it but it did give you a guarantee of high security and they still do as of today so this model hasn't gone away but it's been specialized for a particular use case then came along the virtual private server so the idea is we still had one physical machine but now we were able to subdivide our machine into submachines via virtualization and so essentially you're running a machine within a machine and so you had better utilization of that machine running multiple web apps as opposed to having a physical machine per project so you got better utilization and isolation of resources and so these two options still required you to purchase a machine a dedicated machine and so that was still kind of expensive but then came along shared hosting and so if you remember the mid2000s like with godaddy or hostgator or any of those sites where you had really cheap hosting the idea is that you had this one physical machine shared by hundreds of businesses and the way this worked it relied on tenants under utilizing their resources so you know you wouldn't have a sub machine in there but you'd have a folder with permissions that you could use um and so you would really share the cost and this was very very cheap but you were limited to whatever that machine could do and you were very restricted in terms of the functionality you had and there was this poor isolation meaning that you know if one person decided to utilize the server more they could hang up all the all the websites on that single server then came along cloud hosting and the idea is that you have multiple physical machines that act as one system so this is distributed computing and so the system is abstracted into multiple cloud services and the idea is that you basically get the advantages of a lot of the things above so it's flexible you can just add more servers um it's scalable it's very secure because you get that virtualized isolization you get it extremely at a low cost because you're sharing that cost with the users where in the shared hosting it might be hundreds of businesses we're looking at thousands of businesses and it was also highly configurable because it was a full virtual machine now cloud actually still includes all of these types of hosting they haven't gone away but it's just the idea that you now have more of a selection for your use case uh but hopefully that gives you an idea uh what cloud hosting looks like and it really has to come down to distributed computing okay hey this is andrew brown from exam pro and before we talk about aws we need to know what is amazon so amazon is an american multinational computer technology corporation headquartered in seattle washington and so this is the seattle skyline with the space needle and amazon was founded in 1994 by jeff bezos and the company started as an online store for books and expanded to other products so as you can see this is jeff bezos a long time ago and he has this interesting spray painted sign and his desk is held up by cinder blocks and it looks like his uh desk is like an old uh table or something and he's working really late and he used to be a millionaire at this time and he would be driving into work in his honda accord because you know he just his motivation was always to put all the money back into the company so it really shows that he worked really hard and it did pay off because amazon has expanded beyond just an online commerce store into a lot of different things such as cloud computing which is amazon web services digital streaming such as amazon prime video prime music they bought twitch.tv they own the whole foods market grocery store they have all this artificial intelligence they own low orbit satellites and a lot more stuff it's hard to list at all and so jeff bezos today is not the um the ceo it's actually andy jassy is the current ceo of amazon he was previously the ceo of aws so jeff bezos can focus on space travel so there you go hey this is andrew brown from exam pro and we are taking a look at amazon web services and this is the name that amazon calls their cloud provider service and it's commonly referred to just as aws so here is the old logo where we see the full name and here is the new logo but i like showing the old logo because it has these cubes which best represent what aws is and it is a collection of cloud services that can be used together under a single unified api to build a lot of different kinds of workloads so aws was launched in 2006 and is the leading cloud service provider in the world i put an asterisk there because technically aws existed before 2006 and a cloud service provider which is what aws is is often initialized as csp so if you hear me saying csp i'm just saying cloud service provider okay so just trying to look at the timeline of when services rolled out the first one came out in uh 2004 and was simple queue service sqs and this service still exists as of today but at the time it was the only service that was publicly available so it wasn't exactly a cloud service provider at this time and it was neither aws it was just sqs but then a couple years later we had simple storage service also known as s3 which was launched in march of 2006 and then a couple months later we had elastic compute cloud also known as ec2 and ec2 is still like the most used service within aws and is like the backbone for pretty much everything there then in 2010 it was reported that all of amazon.com's retail sites had migrated to aws so even amazon was using aws full steam and to support industrywide training and and skill standardization it was began offering a certification program for computer engineers on april 2013 and this is the type of certifications that we are doing as we speak um so i just want you to know that aws was the one leading uh cloud certifications and we just want to take a look here at the executive level as of today the ceo is adam he's the former cto of tableau and he spent a decade with aws as a vp of marketing sales and support so he was there he had left for a bit and now he is back then we have uh werner and he's the cto of aws he's been uh the cto for pretty much the entire time it was existed with the exception of some time of the first year he's famous for quoting everything fails all the time and then there's jeff barr who's the chief evangelist so um if you're ever wondering who is writing all the blog posts and talking about databus it's always jeff barr okay all right so what i want to do here is expand on what is a cloud service provider also known as a csp just because there's a lot of things out in the market there that might look like a csp but they actually are not so let's go through this list and see what makes a csp so this is a company which provides multiple cloud services ranging from tens to hundreds of services those cloud services can be chained together to create cloud architectures those cloud services are accessible via a single unified api so databases cases that is the aws api and from that you can access the cli the sdk the management console those cloud services utilize metered billing based on usage so this could be per second per hour vpcus memory storage things like that those cloud services have rich monitoring built in so you know every api action is tracked and you have access to that so in aws's case it's ableist cloudtrail and the idea here is those cloud services have infrastructure as a service offering so iaas that means they have networking compute storage databases things like that those cloud services offers automation via infrastructure as code so you can write code to set everything up and so here's just kind of an example of an architecture where we have a very simple uh web application running on ec2 behind the load bouncer with the domain with rough d3 but the idea is just to show you that you know you're chaining these things together if a company offers multiple cloud services under a single ui but do not meet most or all of these requirements it would just be referred to as a cloud platform so when you hear about twilio or hashicorp or databricks those are cloud platforms and aws azure gcp are cloud service providers okay let's take a look here at the landscape of cloud service providers and the industry likes to break these down into three tiers so we have tier one so this is top tier these were early to market they have a wide service offering they have strong centers used between services and they're well recognized in the industry and in the leading spot is amazon web services and there's no surprise to this because they were the first to develop the technology and so they pretty much dominated the market for multiple years before anyone entered and so it's going to be very hard for anyone to catch up or even overtake them but right behind them is microsoft azure then we have google cloud platform and these three are known as the big three because they're the most used around the world and we actually have a fourth one that's in the tier one and that's alibaba cloud you might not know about it just because it really is based in mainland china and in the asia region so it is really big but it's just the fact that there's that divide between mainland china and the rest of the world okay you have tier two so these are the midtiers so at one point you know they could have been topped here but um you know they were just slow to innovate and so they had to turn to specialization but they're all backed by wellknown tech companies have been around for a long time well before aws existed so we have ibm cloud oracle cloud or rackspace and so rackspace is offering is actually their software called openstack which allows you to run a cloud service providerlike environment uh on your onpremise okay and so you know these are still in use so oracle cloud what they usually do is they try to fight on price and ibm cloud they they fight on ai and ml uh solutions against the top tier then you have the uh tier three the light tier and so these were virtual private servers that turned to offer core iias infrastructure as a service offerings and so they're simple and cost effective and a lot of people that are getting into cloud or even just trying to deploy apps are probably using these and not realizing their cloud service providers so we have a vulture digital ocean and lynnoids so they started with a single offering just virtual machines then they added a load balancer and so they're starting to get more so like digitalocean i think is getting a serverless offering and then linoid or sorry vulture is getting a kubernetes managed service and so you know they kind of live in this realm of are they csps and i would classify them as they are i would say they are a tier three they're just a light tier and i'm sure they'll expand their services to have more of the core but they're just going to stay i think very small in general okay so how do we know who is the leader in the market well all comes down to the madric quadrant and this is a series of market research reports published by it consulting firm gardner that rely on proprietary qualitative data analysis methods to demonstrate market trends such as direction maturity and participants people take these graphs very seriously and so this is what it looks like and as you can see amazon web services is marked as the leader and the closer you are to this top corner here is the better you are off as you can see microsoft is not too far behind followed by google then followed by alibaba cloud then by oracle ibm tencent which we don't uh ever talk about and then there's the other ones that just don't show up because they're so small like digital ocean and linoid there so generally that gives you kind of an idea how the market is growing and stuff like that um but as you can see you know there's still a lot for the other ones to do to catch up to aws okay so a cloud service provider can have hundreds of cloud services that are grouped into various types of services but the four most common types of cloud services for infrastructure as a service uh and i call these the four core would be compute so imagine having a virtual computer that can run applications programs and code networking so imagine having virtual network defining internet connections or network isolation between services or outbound to the internet storage so imagine having a virtual hard drive that can store files databases so imagine a virtual database for storing reporting data or a database for general purpose web applications and aws in particular has 200 plus cloud services and i want to clarify what cloud computing means because notice that we have cloud computing cloud networking cloud storage cloud databases but the industry often just says cloud computing to refer to all categories even though it has computer in the name so just understand when someone says cloud computing they don't just generally mean the subcategory they're talking about all of cloud okay so awes has a lot of different cloud services and i just want to kind of go quickly over the types of categories that we can encounter here and just mention the four core so any csp that has ias will always have these four core service offerings we have compute so nato s this would be ec2 vms storage this could be something like ebs virtual hard drives database so that could be rds sql databases networking and content delivery but really it's networking uh and this would be vpc so private cloud network okay so uh let's just look at all the categories that are outside the four core so there could be analytics application integration ar vr it was cost management blockchain business application containers customer engagement developer tools and user computing game tech iot machine learning management governance media services migration and transfer mobile quantum technologies robotics satellites security identity and compliance if there was more i would not be surprised but you can see there's a lot of stuff that's going on here so let's take a look at all the services that are available to us so if you're on the marketing website which is adabus.amazon.com what you'll see in the top left corner is products and so these are all the categories and for whatever we want if it's like ec2 we can go into here and we can read all about it so usually we'll have our overview all right and that's not very useful and then we'll go over to features and so this is can be kind of useful to get some basic information and pricing which is something you'll do a lot in aws is you're always going to be going to a service and looking up its price and so you'll make your way over here every single one is different a very important page would be like getting started so this will give you basic information but what i do is i like to go all the way down to the bottom here and find my way over to the documentation so i'll go here to documentation to get that deeper knowledge about that service and as you can see things get pretty deep with aws in terms of the information they have so hopefully that gives you an idea of the scope also when you're logged into aws and this will be when we create our account you can explore all the services this way as well so these are all the awesome services but you just notice that there's two ways to explore them where this is actually you just actually utilizing the services and then the marketing website is you reading about them and learning all about them okay hey this is andrew brown from exam pro and we are looking at the evolution of computing your cloud service provider has all of these offerings and the idea is that you need to choose the one that meets your use case a lot of times this all has to come around the utilization of space that's what we're trying to illustrate here in this section here and the tradeoffs of why you might want to use some of these offerings okay for dedicated we're talking about a a physically a physical server wholly utilized by a single customer that's considered single tenant and uh for google cloud we're talking about single node clusters and bare metal machines where you have control of the virtualization so you can sell any kind of hypervisor or virtualization you want the system the tradeoff here though is that you have to guess up front what your capacity is going to be and you're never going to 100 utilize that machine because it's going to have to be a bit under in case the utilization goes up that's you choosing the cpus and the memories you're going to end up overpaying because you're uh you'll have under underutilized server uh it's not going to be easy to vertically scale it's not like you can just say resize it because the machine you have is what you have right you can't add more i mean i suppose they can insert more memory for you but that's a manual migration so it's very difficult um and replacing the server is also very difficult okay so you're limited by the host operating system it's not virtualized so whatever is on there is on there and that's what your apps are going to have access to if you decide to run more than one app which is not a good practice for these kind of machines you're going to end up with resource sharing where one machine might utilize more than the others technically with a dedicated machine you have a guarantee of security privacy and full utility of the underlying resources i put an asterisk there because yes it's more secure but but it's up to you to make sure that it's more secure so you have that's up to your skills of security right whereas if you had a virtual machine or anything above that there's more responsibility on the cloud service provider to just provide a circuit secure machine and they can do a better job than you so why would you use a dedicated machine well maybe you're doing high performance computing where you need these machines like very close together and you have to choose what kind of virtualization you need to have okay so then we're looking at virtual machines the idea here is you can run a machine within a machine the way that works is we have a hypervisor this is a software layer that lets you run the virtual machines uh the idea here is now it's a multitenant you can share the cost with multiple customers you're paying for a fraction of the server uh you'll still end up overpaying for the unrealized virtual machine because a virtual machine is just like you have to still say how many vcpus how much memory and your app is you know you don't want an app that uses 100 right you want to use exactly the amount you need but you can see here you know there's still going to be some underutilization uh you're limited by the guest operating system now but now it's virtualized so at least it's very easy to uh possibly migrate away if you choose to run more than one app on a virtual machine it can still run into resource sharing conflicts it's easier to export or import images for migration it's easier to vertically or horizontally scale okay and virtual machines are the most common and popular offering for compute because people are just very comfortable with those then you have containers and the idea is you have a virtual machine running these things called containers the way they do that is similar to a hypervisor but instead you have um like here is a docker demon so it's just a um a container uh software layer okay to run those containers there's different kinds docker is the most popular and the great thing is you can maximize the uh the capacity because you can easily add new containers resize those containers use up the rest of the space it's a lot more flexible okay your containers will share the same underlying os but they are more efficient than multiple vms multiple apps can run side by side without being limited by the same os requirements and not cause conflicts during resource sharing so containers are really good but you know the tradeoff is there a lot more work to maintain then you have functions functions go even step further and the idea is that you uh the the containers where we where we talked about that's a lot of work to maintain now the cloud service provider is taking care of those containers generally sometimes not it depends if it's serviced or not but the idea is that you don't even think about this is called service compute but you don't even think about uh the os or anything you just know that what your runtime is you run ruby or python or node and you just upload your code and you just say uh i want this to be able to run for this long and use this amount of memory okay you're only responsible for your code and data nothing else it's very cost effective you only pay for the time the code is running and vms only run when there is code to be executed but because of that there is this concept of cold starts and this is uh where the virtual machine has to spin up and so sometimes requests can be a bit slow so there's a bit of tradeoff there but functions or serverless compute is generally one of the best offerings as of today but most people are still getting kind of comfortable with that paradigm okay hey this is andrew brown from exam pro and we are taking a look at the types of cloud computing and the best way to represent this is a stacked pyramid and we'll start our way at the top with sas also known as software as a service so this is a product that is run and managed by the cloud service provider you don't have to worry about how the service is maintained it just works and remains available so examples of this and actually uh the first company to coin this was actually salesforce uh then there's things like gmail office 365 so i think microsoft word excel things like that and they run the cloud okay and sas is generally designed for customers in mind then came along platform as a service also known as pass and these focus on the development or sorry the deployment and management of your apps so you don't worry about provisioning configuring or understanding the hardware or operating system and so here we'd have things like elastic beanstalk heroku which is very popular among developers that just want to launch their code or google app engine and that is the old logo but that's the logo i like to use because i think it looks cool and so these are intended for developers the idea is that you just deploy your code and the platform does the rest then there is infrastructure as a service there's no way to say that like it's easy to say sas or pass but there's no easy way to say iaas so this is the basic building blocks for cloud it it provides access to networking features computers and data storage space and the idea here is you don't worry about the it staff data centers and hardware and so that would be like microsoft azure aws oracle cloud things like that and these are for administrators okay so there you go hey this is andrew brown from exam pro and we are taking a look at cloud computing deployment models starting with public cloud and the idea here is that everything when i say everything i'm talking about the workloads the projects the code is built on the cloud service provider so here is a diagram where we have a ec2 instance a virtual machine running our application and then we have our database in rds and we have the internet coming into our aws account and so everything is contained all of our infrastructure is within aws all right and so this is known as being cloud native or cloud first and i put an asterisk beside cloud native because that was a term uh that was uh used prior to classroom providers to refer to containers or open source um models being deployed and being mobile other places so just understand that it has two meanings but in the context of this cloud native just being like native to the cloud like using cloud to begin with okay then we have private cloud so everything built on a company's data center uh and being built on a data center is known as being on premise because that is where the data center resides near where you work and so here you could be using cloud but you'd be using openstack which would be a private cloud so here we have our onpremise data center and the internet's coming into our data center and we're running on openstack where we can launch virtual machines and a database okay then there's the concept of a hybrid cloud so using both onpremise and a cloud service provider together and so the idea here is we have our onpremise data center and then we have an established connection maybe it's a vpn connection maybe it is a direct connection um but the idea is that we're bridging that connection and utilizing both our private and our public uh stuff to uh create a cloud workload then there is a fourth one called crosscloud sometimes it's known as multicloud and sometimes it's erroneously referred to as hybrid cloud but it generally is not uh hybrid cloud okay the idea here is when you're using multiple cloud providers and so one example here could be using services like azure arc so azure arc allows you to extend your control plane uh so that you can deploy containers for kubernetes in azure within amazon eks within gcp kubernetes engine but you know being cross cloud doesn't necessarily mean that you're running a using a service that used works across the cloud and manages it it could just mean using multiple providers at the same time another service that is similar to azure arc but is for a google cloud platform is also known as anthos aws has traditionally not been um crosscloud friendly and so we haven't seen any kind of developments there where we see uh these other services that are or cloud service providers behind aws trying to promote it to grab more of the market share okay so let's talk about the different deployment models and what kind of companies or organizations are still utilizing uh for these particular categories so for cloud again this is where we're formally utilizing cloud computing hybrid is a combination of public cloud and onprem or private cloud and then onprem is deploying resources onpremise using virtualization resource management tools sometimes called private cloud or it could be utilizing something like openstack so for companies that are starting out today or are small enough to make the leap from a virtual private server to a cloud service provider this is where we're looking at cloud so we're looking at startups sas offerings new projects and companies so maybe this would be like base camp dropbox squarespace then for hybrid these are organizations that started with their own data center but can't fully move to cloud due to the effort or migration or security compliance so we're talking about banks fintech investment management large professional service providers legacy onprem so maybe cibc which is a bank deloitte the ccp or cpp investment board and then for onpremise these are organizations that cannot run on cloud due to strict regulatory compliance or the share size of the organization or they just have like an outdated idea of what cloud is so they just have a lot of difficulties in terms of politics adopting cloud so this would be public sector like government super sensitive data like hospitals large enterprise with heavy regulation insurance companies um so again hospitals maybe aig the government of canada and so i shouldn't say that they aren't using cloud but um you know because uh aws and all the cloud providers have um uh public sector offerings so um you know i'm just trying to stage as an example of things that could be still using onpremise so you know i know the government canada definitely uses uh cloud in a lot of ways same with aig and hospitals but you know generally these are the last holdouts of onprem because there really isn't a a good reason to be fully on premise anymore but again there are some things that are still doing that okay hey this is andrew brown from exam pro and we are at the start of our journey creating ourselves an aws account so what you need to do is go to aws.amazon.com if you don't have a lot of confidence how to get there just type in adabus into google and then click here on the link where it says adabusamazon.com it'll take you to the same place now notice we have a big orange button in the top right corner so it says sign into the adwords console it's the if it's the first time you've ever been to this website so if i go to adabus.amazon.com incognito it will have the create enables account button i don't know why they don't keep this consistent across the board but i wish they did but if you are on the screen you can click here or there um but if you do see something that doesn't say uh you know create an account or or et cetera you can just sign in okay and then down below you can hit create a new aws account so that's the way you're going to get in there and so you're going to put an email a password and create a database account name i've created this so many times and it's so hard to set up new emails i'm not going to do this again it's not complicated but one thing i need to tell you is that you do need to have a credit card you cannot create an account without a credit card um and for those who are in places where maybe you don't have a traditional credit card maybe you can get a prepaid one so up here in canada we have a company called coho and so coho is a visa debit card and so it's basically a virtual prepaid credit card and so these do work on the platform as well so if you have a traditional credit card or possibly could find one of these you still have to load up with money but it does give you a bit more flexibility to create that account so what i want you to do is go through that process yourself it's not complicated and i'll see you on the other end okay so once you've finished creating your account you should be within the adwords management console and this is the page you're always going to see when you log in it's always going to show the most recent services here and you'll notice in the top right corner that i have my account called exam pro if you're wondering how do you change that name what you do is to go to my accounts here and once there you'll have your account settings up here if you go to edit you can change that name here okay so you know sometimes when you create your account you don't like the account name that you gave it and so that's your opportunity to fix it but once we're in our account what i want you to do is immediately log out because i want you to get familiar with the way you log into aws because it is a bit um different than other providers and so i don't want you to get hung up later on with your account so i've logged out i'm going to go ahead and log back in so you can click the orange button or what i like to do is drop down my account and go to aws management console it's a lot more clear and you'll notice we're going to have two options root user and iam user so this is what i'm talking about for the confusion so when you log into your root user account you all are always using an email and when you're logging as an imuser you're actually going to be entering the account id or account alias but what we'll do is go to the root user and this is the email you use to sign up with the account so for me i called this one andrew plus sandbox at exam pro dot co i'm gonna go to next sometimes you get this character box it's very annoying but it happens time to time and so what i'm gonna do is just go ahead and type that in okay and hopefully it likes it and then i'm just going to enter in my password all right and i'll be back into my account and so notice it takes me back to about management console so the root account is not something we want to be generally using except for very particular use cases and we do cover that in the course but what i want you to do is go set yourself up with a proper account and so what we'll do is go to the top here and type in iam and this stands for identity and access management and we'll click on iem here and on the left hand side we're going to see a bunch of options here and so notice right away we get to the i am dashboard where it's going to start to make some recommendations for us the first one is always to add mfa multifactor authentication another thing you can do is set an account alias so you can see that i've set one here prior so if i just go ahead and remove it the way we'd have to log in is via the account alias which is the same as the account id and so i don't really like that so i can just rename it to deep space nine and these are unique so you have to pick something that is unique to you so it could be your company name or things like that it's gonna make it a lot easier to log in when we create our additional user here so we'll come back to mfa at some point here what i want you to do is go over to users and go ahead and make yourself a new user and so i'm going to call this one andrew brown and i'm going to enable programmatic access i'm going to enable aws management console so this one's going to allow me to use the apis to programmatically work with aws and this one here is going to allow me to just log into the console which is pretty fair here so now that i have this we can auto generate it or give it a custom password i'm just going to auto generate it for the time being and here it says you must create a new password at the next sign in which sounds fair to me and we go ahead and create ourselves a new group so it's pretty common to create a group called admin and notice here this is where we're going to have a bunch of different policies so the first one here which is admin and access provides full access to able services and resources and this pretty much gives you almost nearly almost the same capabilities as the um aws root user account and so that's going to be okay because we are an admin in our account so i'll check box that on but i just want to show you here if you drop down filter policies and you went to invest manage job functions these are a bunch of premade aws policies that you could apply to different users so what's really popular after the administrator access is to usually give the power user access and so this one allows a user to do basically anything they want with the exception of management of users and groups so you know it could be that that's something that you'd want to do for some of your users i just don't want to have any trouble so i'm going to give us admin access here and we're going to go ahead and create this group and so here is the group that we are creating we're going to go next we can apply our tags if we want i'm not going to bother we hit next review and then hit create user all right and so now what it's doing is it's showing us the access id and the access key secret that we can use to pragmatically access aws and then there's a password here so i'm going to go ahead and show it and what i'm going to do is just copy this into a clipboard anywhere and so i'm just copying that off screen here because i'm going to need it to log in and i'm just going to remember my username as well alright and so what we'll do is go ahead and hit close so what i'll do is go back to my dashboard here and remember i set my account alias as deep space 9 but we could also use the account id to log in i'm just going to grab my account id off screen here and what i want to do now is go ahead and log out and now log into this im user and this is the one that you should always be using within your aws account you shouldn't be using your root user account so what i'll do is go over to i am user here and notice now that it says account id so 12 digits or the account alias so here i can enter in these numbers here or i can enter in my alias which is deep space 9 and again you'll have to come up with your own creative uh one there for yourself and we'll go ahead and hit next and so notice what it's going to do is now ask me what my imuser name is so i defined mine as andrew brown and then we had an autogenerated password there so that we had saw and so i'm going to place that in there we'll go ahead and hit sign in and so now right away it's going to ask me to reset the password so i'm going to put the old password in there and so now i need a new password i strongly recommend that you generate out your passwords to be very strong i like to go to password generator and i'll drop this down and i'll do something really long like 48 characters and if you don't like uh weird characters you can take those out there sometimes it loads here so you gotta try it twice um and we're gonna go down to whoops 48 there we go and so that's pretty darn long so i'm going to copy that off screen here so i do not forget and you probably would want to put this in a password manager something like dashlane or some sort of thing like that and we'll go ahead and we will paste that in and we'll see whoops i don't want google to save it and we'll see if it takes it and so there we go so what i'll do is now log out and i'll make sure my new password works because you really don't want to have problems later so we'll type in deep space nine andrew brown again this is going to be based on what your what you have set and we'll go ahead and log in and there i am and so now notice that it doesn't say example or whatever it says andrew brown at deep space nine so it's using the account alias and showing the name and that's how i'm going to know whether i'm the root account user or whether i'm logged in as an iam user all right so there we go okay so now that we have the proper user account to log in i just want to point out about regions so in the top right corner you'll notice it says north virginia here it possibly will say something completely else for you but what you'll do is you'll click and drop that down and you'll see a big list of regions and so sometimes when i log into aws it likes to default me to u east uh us east ohio but i honestly like to launch all my stuff in u.s east north virginia even though i'm in canada i probably should be using the canada central region down here but the default region is going to be based on your locality okay so just understand that it might be different i strongly recommend for all of our follow alongs you run in u.s east one because usc swan is the original the original region and it also has the most access to aws services and some aws services um such as like billing and cost and things like that are only going to show up in u.s east north virginia so just to make our lives a lot easier we're going to set it there but i want you to understand that some services are global services meaning that it doesn't matter what region you're in it's going to default to global and one example could be cloudfront so if i jump over to cloudfront here for a moment and we do seem to have uh some cloudfront distributions here from a prior follow along but notice up here that it now says global so cleft front does not require a region selection let's make our way over to s3 all right and this one's also global so again this one does not require a region selection but if you go over to something like ec2 okay this has a region dependency so just be really careful about that because a lot of times you'll be doing a follow along and you'll be like why aren't these resources here or whatever and it's because this got switched on you and it can happen at any time so just be cautious or aware of that okay so one of the major advantages of using aws or any cloud service provider is that it utilizes metered billing so that is different from a fixed cost where you'd say okay i want a server for x amount of dollars every month but the way nms works is that it's going to bill you on the hour on the second based on a bunch of factors and so you're going to be able to get services at a lower cost however if you choose an expensive service and you forget about it or if there's misconfiguration where you thought you were launching something that was cost effective but turned out to be very expensive you could end up with a very large bill very very quickly and so that is a major concern for a lot of people utilizing cloud but there's a lot of great toolings built into aws to allow you to catch yourself if you happen to make that mistake and before we go ahead and learn how to do that i want to show you some place where you could end up having excessive spend without knowing it so one example and this is actually happened to me when i first started using aws before i even knew about all the billing tools is i wanted to launch a redis instance and so you just have to watch you don't have to do this but elasticash is a service that allows you to launch either a memcache or redis uh database and i just wanted to store a single value and so i went here and i scrolled down it looked all good and i hit create but i wasn't paying attention because apparently it was like the default the node type here to the cache r6g.large all right and you know you might think that a bus has your best interest in play and most services are pretty good they make sure that they're either free or very low spend but some of these and elastic is an older service where they just have these weird defaults so um you know if we were to go look up this the rg6 large all right and look at its spend all right and we would go over here whoops i think i went to the china one but if we were to go over here and look for that instance i'm just trying to find it here for cost this one down below um this doesn't say pricing does it say our pricing here here it is so this one cost um this one costs about two cents per hour it doesn't sound like a lot but if we go here and we do the math we say 7 30 7 30 is the amount of hours in a month that is 150 okay so if you don't know about that and forget about that that's gonna be 150 and i'm going to tell you that it used to be a lot higher i'm pretty sure they used to have it defaulted to something like like this or that because i remember i did this and i had a bill that came in that was like 3 000 usd dollars and i'm in canada so like 3 000 usd is like a million dollars up here and so i remember um it was a big concern and i freaked out but that was okay because all i had to do was go to support and what i had done is i went to the support center and i had opened a support case and i just said hey i have this really big bill so you go here right and you look for billing and you look for something like charging query or misspend and you say you know um you know like help my bill's too high and you just say like you explain the problem saying hey you know i was using elastic cash and it was set to a large default and i wasn't aware about it can you please give me back the money and the great thing is that aws is going to give you a free pass if it's your first time where you've had a misspending they generally will say okay you know don't do it again and if it happens again you will get billed but go ahead and learn how to set up billing alerts or things like that okay so just so you know don't freak out if you do have a really high bill you're going to get a single free pass but now that we know that let's go learn how to set up a budget okay all right so now that we've had a bit of a story about over span for misconfiguration let's learn how to protect ourselves against it and we're going to go ahead and set up a budget so go to the top here and type in budget and what that will do is bring us over to the billing dashboard another way to get here is to go click at the top here and go to my billing dashboard and then you'll see the lefthand menu here and so the great thing about budgets is that the first two are free it says there is no additional charge for any of his budgets you pay for configured use usage but i'm pretty sure that that's not true because it used to be abs budget reports okay so that costs something it used to be that aws budgets um after subscription enabled will occur 10 cents daily so in addition to budget monitor you can add actions to your budgets the first two actionenabled budgets are free okay so just be aware that just because it says there's no additional charge read into it because sometimes the fine line will tell you it does cost something but i know that the first two are free what we'll do is go ahead and create a budget i'm going to close these other tabs here since we have no need for them and we're going to be presented with a bunch of budget types we're concerned about cost today so we're going to go with a cost budget and notice we can change the period from monthly to daily to quarterly to annually if you change it to daily um you won't get forecasting so i don't want that today but a monthly is pretty good you can have a reoccurring which is strongly recommended and then you can put a fixed cost notice that i already have some spend on this account so it was like 25 bucks last month i'm going to set it my budget here to a hundred dollars and you can add filters here to um filter that cost out so if you want to say only for this region or things like that you could do that uh notice that this is my spend over here um so this is my budget and that's the actual cost notice my cost has been going up the last few months because i've been doing things with this account and so i'll do is say simple budget here we'll hit next and so now it's asking us if we want to configure alerts we probably do so you'd hit add alert and then you'd set a threshold like 80 percent or you could say an absolute value and then you put in your emails like andrew exam pro dot co and i want to point out that this is using um it was sns or it should be anyway so amazon sns has no upfront cost based on your stuff here so even though you're filling out an email you know and maybe it doesn't show it but i'm pretty sure that this would create an sns topic but what we'll do is hit next here we have an alert so we're just reviewing actually this is for attaching any action so maybe we want some kind of followup thing to happen here so we say add action and uh requires specific i am permissions on your behalf okay sure so i guess you could follow up actions that's no different than um on a building alarm but we're not really worried about that right now i'm not going to bother with an action we'll go ahead and create a budget and so here it's going to say that our budget is 100 it's going to show us the amount used forecast amount current budget sometimes this takes time to show up so i'm going to hit refresh and see if it shows up yet there we go so notice we have forecast amount 23 current budget etc forecasted budget uh forecasted versus budget so it's pretty straightforward on how that works i'm just curious if it actually created an sns event so i'm going to go over here because a lot of services utilize sns so if i go over here default cloud watch alarm um so i think this is something i had created before so i'm gonna go ahead and just delete it so default cloudwatch alarms actually i'm going to just click into here and see what i have confirmed so i think it might have used this when we created it but um the reason i'm bringing up sns is that there's a lot of services that allow you to email yourself for alerts and it always integrates with this service and so i just want kind of want to point that out so that you remember what sns is for but yeah so setting up a budget is not too hard so there you go all right so now that we've set a budget what i want to talk to you about is the free tier and the free tier is something that is available to you uh for the first 12 months of a new abs account and allows you to utilize the services without incurring any cost to you and so it's in your advantage to utilize this free tier as you are experimenting and learning cloud so if you want to learn about all the offerings what you do is go to google type in aws free tier and you'll get this page that explains all the sorts of things here so you can get 750 hours on ec2 rds things like that there are stipulations in terms of what it would be so here this is a t2 or t3 michael mic micro running linux red hat or other type of os's okay so there are details you have to read the fine print some services are only available for the first two months things like that so it's going to highly vary based on service but it's worth giving us a read in areas that you are interested in now the thing is is how do you know that you are still in the free tier or you go outside of it and that's what i want to talk to you about right now so i am actually in another aws accounts that knows in the top right corner says brown.lap or hyphen laptop exam pro dot co sometimes i will switch into different abs accounts during these follow along so i can best show you um you know these settings so if you make your way over to billing and actually i should show you up here if we go to my dealing dashboard just trying to be consistent here and you go to the lefthand side to billing preferences what you can do is enable receive free tier usage alerts and then put your email in there and save that and so turn on this feature to receive email alerts a when your abs service usage is approaching or exceeded database free tier usage limits if you wish to receive these alerts etc etc etc right and while you're there i want you to also check box receive billing alerts so i can show you how to set a billing a billing alert and adabas says you know budgets are a new thing but billing alerts are still something that we use as of today so if you checkbox that on we'll be able to see your cost if we go back here it should show you um it's because i'm out of the free tier on this account but it would show you in the alerts you know your usage there so example here is if we scroll down this is the documentation tracking your image free tier usage you would see like a box like this and would say hey your free tier usage limit is here and you're over it okay so that generally would show up on this panel here but again i'm outside of the free tier so i'm not seeing it here today okay so you know hopefully that is clear um but yeah there you go all right so we created ourselves a budget we're monitoring our free tier but there's another way that we can monitor our spend and that is through building alerts or alarms and it is the old way before we had it was budgets this was the only way you could do it but i still recommend it because there is a bit more flexibility here with this service and so i wanted to teach you early on so that you know what's available to you or if you want to play around with it in the future so what you'll do is go to the top here and type in cloudwatch and cloudwatch is one of those services where it's actually a collection of services so there's cloudwatch alarms cloudwatch logs cloudwatch metrics those are all individual services and animus loves to update their interface so sometimes you'll be presented this option to change the latest interface i'm going to try out the new interface here and that is one challenge with databases you always have to expect that they're going to change the ui on you and you're going to work through it so just understand that i try to keep my videos up to date as best i can but part of the challenge is getting used to that so this is what they have today i don't know if they're going to stick with this but this is what it looks like but what i want you to do is make your way over to alarms on the left hand side and notice that we actually have a section just for billing which is interesting i remember them having that before so it's new so uh here it says it was cloudwatch help can help you monitor the charges of the spill remember that we had to turn that on get 10 free alarms with a thousand free email notifications each month as part of the free tier so understand that if you create billing alarms they do cost money um as well if you go over that limit but you sure get a lot 10 free alarms is quite a bit but we'll do is go ahead here and create our sales alarm we're going to go and choose a metric and so here are the options we could choose from and so we i think would like um billing and so we can do by service or total estimated charge we're going to do a total estimated charge we can only select usd i've never seen any other currency ever there and so here we kind of get this little graph where we can see stuff but this is a lot more powerful than budgets because you can do anomaly detection uh so like here it will actually check base between a range as opposed to just going through a particular value but what i'll do is just set a value here like fifty dollars right so notice that it sets the line up here and this is my current spend here right and so back to anomaly detection this is a lot smarter so the idea is that if something is outside this band of a certain amounts then it would alert okay but i'm going to go back here i'm just going to set this to 50 and that looks okay to me you can change the period six hours is fine um there's additional configuration that's fine as well we're going to go ahead and hit next and so the idea is that you know if it passes that red line it will go to an in alarm state and then what it will do is uh we want to have it to trigger an sns topic so i would generally just create a new one here and we'll just say my billing alarm okay and then here we'll just set the email and your exam pro.co and we'll go ahead and create that topic and so that is now set i don't know if it would confirm it we might have to go to our email to confirm it so notice it says pending confirmation so what it has done is it sent me out an email and it wants me to click that link to confirm that i want to subscribe to it so i might just do that off screen to show you here okay so i'm just going to pull up my email here just give me a moment okay and so if i come back here this is the email that came in so i'm just going to confirm that subscription says i'm confirmed good and if i refresh this page we can now see that that is confirmed all right so we'll scroll down here so we can trigger an auto scaling action so maybe you know if you have too many servers you say hey the cost is too much shut down those servers there's ec2 actions things like that so these are kind of similar to budgets right they're system manager actions i imagine all these things are available in budgets as well but budgets just makes it a little bit easier to look at so i'm going to say my simple building alarm here we'll hit next all right we'll hit create alarm and there you go so billy alarms don't have like forecasting things like that um but you know they are they do have their own kind of special utility and so i utilize both okay so there we go let's go back to our management console move on to the next one so one of the strongest recommendations that abuse gives you is to say to set mfa on your database root user account so that's something we're going to do right now so make sure you're logged into the root user account so i'm going to go log out as my im user i'm going to go back and log in and i'm going to log in as my root user here so to do that no sometimes it will be expanded as the imuser click and sign into root user here we'll have root user i'm going to go ahead and enter my email that i used and if you do switch accounts frequently they will ask you these silly captchas which drive me crazy but uh you know it happens you probably won't encounter it as much as i do and so i'm going to go ahead and grab my password here and paste it on in and so now that i'm in what i want to do is make my way over to iam and i'm going to go and look for users actually sorry just right here add an mfa root user we're going to go ahead and hit add mfa all right so that's going to bring us to this screen and so here we can activate our mfa and so we have a few options here so we have virtual mfa device u2f security key other hardware like a gem gym gemalto token so you know i generally use this because i have a security key and i want to show you what i'm talking about so this is how i log into my machine or my aws account this is a security key an ubi key that sits on my desk i tape it so it doesn't fall fall off the cord but the idea is that when i log in i have to press this little button here to double confirm before i get into my account but if you don't have a security key you can just use a virtual mfa and all that means is you're going to use something on your phone to log in so we'll click continue here and so it says install a compatible app on your mobile phone or device and so if you click and open this what it will do is tell you about some things that you can use um so if we scroll down to virtual here this suggests uh if you have android iphone so authy dual mobile last path microsoft authenticator google authenticator so google authenticator microsoft authenticator and authy i have all those three installed um honestly authy has the the nicest simplest ui but i'm using microsoft authenticate authenticator quite a bit so anyway whichever you want to do it's fine but what we'll have to do is go back here and then it says use your virtual mfa app on your device camera to scan your qr code so once you have one of those apps installed like authy or whatever one you want what you're going to do is open up the application and i can't tell you exactly where it is but you'll have to hit add account in your in your app and then from there it will ask you to scan your qr code and so once you're ready you hit show the qr code you hit scan the qr code on your phone i'm holding my phone up to my my um my computer screen here and it's going to find it and i'm just going to take a moment here to rename the account so i can tell what it is so i'm just naming it aws sandbox because that's what i call this account and i'm gonna go ahead and save that and so now what i can do is enter uh two consecutive mfa codes now this always confused me what they wanted here but the idea is that you're gonna see one code right whatever's on the screen right now so i'm gonna type in it it says seven 734051 and i'm going to wait until the new code shows up so there's like a timer in all these apps and they go across the screen or they count down and so you have to wait for that to happen and so i'm just going to wait here a little bit and once i get the new number here this one is zero seven one five three zero i'm gonna hit assign mfa and there we go and i can't tell you how many times i like messed that up because i didn't understand the consecutive numbers but you're just waiting for uh the number that's on the screen it entered in and then entered the next one in to turn on mfa and so now your account is protected and every time you log in you're going to have to enter in mfa so let's log out and see what that looks like so we'll go ahead and sign in and again we'll put in our root user account here we'll type in 74m32t submit and i need to go grab my password so that's in my password managers just give me a moment here and now it wants the mfa code so this is in my phone and so i'm going to go enter it in so this one says four seven five eight four one all right we'll hit submit okay there we go so that's gonna happen every single time we want to log in i'm going to tell you that if you get one of these they're so much easier to use because you just press the button okay so that's why i have this because i cannot stand entering the code in time and time again but you know those are your options there okay hey this is andrew brown from exam pro and we're looking at the concept of innovation waves so when we're talking about innovative waves we're talking about chondrativia or k waves which are hypothesized cyclelike phenomena in the global world economy and the phenomenon is closely connected with technology life cycles so here is an example where each wave is irreversibly changes the society on a global scale and if you look across the top we can kind of see what they're talking about so we have steam engine cotton railway and steel electric engineering chemistry petrochemicals automobiles information technology and so the idea is that cloud technology is the latest wave and i'm not sure if you'd fit web 3 in there as well ml ai but maybe they're all part of the same wave or their separate waves but generally they're broken up based on this p r d e here where it says perspective recession depression and movement uh improvement sorry and so this is the common pattern of wave where we see a change of supply and demand and so if we're seeing this we know that we are in a wave in where we are in a wave okay hey this is andrew brown from exam pro and we are looking at the concept of a burning platform so burning platform is a term used when a company abandons old technology for new technology with the uncertainty of success and can be motivated by fear the organization's future survival hinges on digital transformation and just to kind of give you a visualization here is a literal burning platform so imagine you have to jump to it uh jump from it to make a change so um you know burning platform could be you know stop using onprem and start using cloud or maybe going from cloud to web 3 and that's generally the idea when we talk about a burning platform so i just want to quickly show you that digital transformation checklist that i mentioned and the way you can get to it is by typing in digital transformation aws and so it should bring you to the public sector page and here it is so we click there and all it is is a pdf uh so it's not news from 2017 but that doesn't mean that it's not valid anymore uh it's just that that's when it was made so we scroll on down and we can see transforming vision and so we have a checklist there so if we click into this uh we can see things like communicate a vision of what success looks like define a clear governance strategy including the framework of achieving goals uh build a crossfunctional team identify technical uh partners they talk about shifting the culture and then down below i assume that this one is related to that one it's unusual because you know they just have a checklist here but then they have a sub checklist which must be clear to that so reorganize staff into smaller teams things like that so it's not super complicated you'll see each category go go cloud native they'll have a checklist um you know and if you are at the executive level or the sales level or trying to convince your vps and stuff like that give this a read it might give you something useful in the end to help better communicate that transformation for you okay hey this is andrew brown from exam pro and we are looking at the evolution of computing power so what is computing power it's the throughput measured at which a computer can complete computational tasks and so uh what we're pretty much used to right as of these days is general computing so a good example here would be a zeon cpu processor uh that's more of a highend processor not something you'd find in your home computer but we're talking about data centers specifically uh um you know innovative data centers xeon cpu processors are what you're going to come across uh then came along a new type of compute which is gpu computing um when we're talking about google cloud they have tensor computing and so this is where i get the 50 times faster based on that metric and so i didn't have an exact metric here for aws as a solution for this mid tier of computing power so i just borrowed that 50 times there but the idea is that gpu computing or tensor computing is is 50 times faster than traditional cpu and generally that's going to be used for very specialized tasks when you're doing machine learning or ai so it's not something you're going to be doing for your regular web workloads but just understand that all these fit so we're not getting rid of general computing we're just adding new levels of compute then there's the latest which is uh quantum computing and so here we have an example of the rigid rig right getty 16q aspen 4 and so it literally looks like it's out of um science fiction and this thing is like a hundred million times faster it is super cutting edge and we don't even know exactly how it works and there's not even anything that's very applicable that we can use this for but the idea is that we're not done with the evolution of computing power things are going to get a lot faster once we solve this last one here and so above service offering here would be for general computing you're looking at elastic compute cloud ec2 so we have a variety of different uh instance types and they're all going to have different types of hardware with different types of general computing for gpu computing this is a specialized chip that aws has produced called the edibus and i don't know how to say it but we'll just abbreviate it to infer so aws infer chip and this was designed as a direct competitor to gcp's tensor computing uh unit the tpu um and so this is intended for ai ml workloads but it works with not just um tensorflow but it works with any machine learning framework so that is one advantage it has over uh tpus um and then the last one here is aws brackets so you can actually use quantum computing as a service on your bus you uh as of even today um the way aws is able to do this is they work with caltech so that's the california technology university or institute i'm not sure the name of it there so it's not exactly aws producing this but itabus is doing this as a partnership to give quantum computing accessible to you okay so i'm here in the aws console because i just want to prove to you that you can use quantum computing on aws it's that accessible so all you'd have to do is go to the top here type in bracket and then you make it over to amazon bracket and so here you can like set up quantum tasks the first time you set it up you've got to go through this process here i think i have to go through this onboarding to be able to show you the next step so i'm going to go ahead and enable bracket in this abs account okay and i'm not going to launch anything i'm just going to try to just kind of show you a little bit of what is accessible to you because it's not super exciting but the fact that you can do it is kind of interesting so here i am on the inside here and we have all these different types of quantum computing so d wave i know i i o n q righty things like that and then down below these are the quantum processing units the q q p u's and then down below you have the simulator so you can kind of simulate uh these things here um so i think that's kind of interesting but in terms of the cost like if you scroll on down here um so it was bracket is part of that it was free tier it gives you one free hour of quantum circuit simulation time per month during the first 12 months so it's free to do a circuit simulation but if you actually want to run it on the actual hardware you can see the cost there's the per task price the per shot price things like that what could you do with this i don't know there's things called like quad bits or something like that and i can't imagine that you're going to be doing anything useful but i think it's just more so like you are sending out quad bits or whatever they are and you're observing them but what you can do with them i have no idea but it's just exciting that you can do that i didn't have any spend just by activating that i'm just kind of just showing you there okay hey this is andrew brown from exam pro and we are looking at the benefits of cloud and this is a summary of reasons why an organization would uh consider adopting or migrating to utilizing public cloud and so we'll quickly go through the list here uh because in the followup slides we actually go into them a bit more detailed so we have agility page ago economy of scale global reach security reliability high availability scalability um and elasticity so the thing is is that eight of us had this before it was called the six advantages of cloud but they have reworked it to include additional items um and so where you see these uh sub bullets here those are the original six as you see one two three four five six and so i kind of just put them where they kind of fall under the new categories there and you'll notice that database has included high availability elasticity reliability and security as uh new ones here okay and so the thing is is that um i have always always even in my original uh i think my original cloud practitioner had cloud architecture as a separate section and included all these things in here so it's a great thing to see that ableist has included it but in terms of how i organize this course we're not going to cover them in this section because i have the cloud architecture section so just understand that we will come to those eventually and i would just say that aws is still missing something on this list which is fault tolerance so you know my list looks like this except i would add fault tolerance to it so you have everything there and disaster recovery okay so the benefits of cloud is a reworking expansion of the six advantages of the cloud and we will look at the original six advantages um and then look at another one that is more of a generalized one that i i've used across my courses so that we fully understand the benefits okay all right let's take a look here at the six advantages to cloud defined by aws and so these are still uh part of aws marketing pages um but you know it's interesting because you can't find the benefits of the cloud in a single page on any of this at least the time of making this so there's a bit of disconnect between the um exam guide and the actual marketing material but that's okay i fill it all in for you so you know i'm just again noting that the sixth advantage of cloud was the original description for cloud benefits and we'll go through them okay so the first is trade capital expense for variable variable expense so you can pay on demand meaning that there is no upfront cost and you pay for only what you consume or you pay by the hour minutes or seconds so instead of paying for upfront costs of data centers and servers the next is benefit from uh massive uh economies of scale so you are sharing the cost with other customers to get unbeatable savings hundreds of thousands of customers utilizing a fraction of the server stop guessing capacity so scale up or down to meet the current needs launch and destroy services whenever so instead of paying for idle or underutilized servers we have increased speed and agility so launch resources within a few clicks and minutes instead of waiting days or weeks of your it to implement the solution on premise we have stopped spending money on running and maintaining data centers so focus on your customers developing and configuring applications so instead of operations such as racking stacking and powering servers the last is go global in minutes so deploy your app in multiple regions around the world with a few clicks provide low latency and a better experience for your customers at minimal cost the six advantages of cloud still apply and i like to include them here because they just have a different kind of lens or or or angle when you're looking at this stuff and so we've looked at the six advantages of cloud and now let's take a look at the next slide my reworking of the sixth advantage of the cloud to be more generalized okay all right i just wanted to show you where that sixth advantage of cloud computing comes from it's part of it it was documentation so i typed it in here and you can see that it is still around and so it's unusual because this used to be part of the marketing website it had those nice little graphics but for whatever reason it's over here now in the overview of amazon web services and by the way if you're starting starting out with databus this is a very light read but it is a good read to get started with we obviously cover all this stuff in the course um but you know maybe you'll get something different here but the idea is that it was definitely expanded on this but for whatever reason this documentation hasn't changed so just understand that i've polyfilled that for you in this course okay all right so this is the seven advantages to cloud i said six but i meant to say seven and so um you know since i've created fundamental courses for all these cloud service providers i started to notice kind of a trend and so what i did is i normalized it into my own seven advantages and this actually maps up really well to the new benefits of the cloud so it looks like invoice was thinking the same as i was um with the exception of those cloud architect stuff which i keep in a separate section but let's go through it and see what is here so the first is cost effective you pay for what you consume no upfront costs on demand pricing so pay as you go payg with thousands of customers sharing the on uh sharing the cost of resources any of us used to refer to this always as ondemand pricing and azure always said pay as you go and so it looks like aws now uses both ondemand and payasyougo to describe them which is great um but there you go then we have global so launch workloads anywhere in the world just choose a region it's secure so cloud provider takes care of physical security cloud services can be secured by default or you have the ability to configure access down to a granular level uh it's reliable so data backup disaster recovery data replication fault tolerance it's scalable increase or decrease resources and services based on demand elastic so automate scaling during spikes and drop in demand current so the underlying hardware and and managed uh software is patched upgraded and replaced by the cloud provider without interruption to you so i think this is one that isn't on the benefits of the cloud which is a really good one um but uh yeah that's the seven hey this is andrew brown from exam pro and we are taking a look at what is able's global infrastructure so global infrastructure is globally distributed hardware and data centers that are physically networked together to act as one large resource for the end customers so if you see here on the right hand side we have a picture of a globe and the idea is that we have a bunch of these regions and these regions are containing a bunch of data centers and then you have those lines going in between them which kind of represents the network okay so the global infrastructure is made up of the following resources so they have regions availability zones direct connection locations point of presence so those are pops local zones wavelength zones and we're going to cover all of these in this section here and one thing i want to note is that airbus has millions of active customers and tens of thousands of global partners that are constantly using this infrastructure so you know that it is rock solid okay all right so i'm over here on the global infrastructure page if you type in aws global infrastructure you'll make your way here and so i just wanted to point out that aws is always updating their global infrastructure so these numbers are increasing all the time but if you're over here what you probably want to do is make your way to regions and azs so you can kind of see what's in your area so i'm in canada and we have canada central region here and it has three availability zones have launched in 2016. you'll notice that it has a couple asterisks if you scroll on down here explain that it's in the montreal metropolitan area so saying it's in the downtown it's in the city uh that could matter to you for whatever reason um but just kind of pointing out where that stuff is you can read about all this stuff but of course we cover this all in the course but there you go hey this is andrew brown from exam pro and we are taking a look at above regions and regions are geographically distinct locations consisting of one or more availability zone and so here is a world map showing you all the regions that abuse has in the world and the blue ones represent regions that are already available to you and the orange ones represent ones that ableis is planning to open so aws is always expanding their infrastructure uh in the world so always expect there to be more upcoming ones every region is physically isolated from independent of every other region in terms of location power and water supply and the most important region that you should give attention to is u.s east one uh in particular so this is northern virginia it was italy's first region where we saw the launch of sqs and s3 uh and there are a lot of special use cases where things only work in u.s east ones and we'll find that out here in a moment what i do want to show you is what it looks like for an architectural diagram when you are seeing a region so notice that we have this little flag here it says us east one us west one and inside of it we have an ec2 instance so that is going to represent a region in our architectural diagrams uh but let's look at some of the facts here and understand why u.s east or u.s east 1 is so important so each region generally has three availability zones and that is by intention and we will talk about that when we get to the availability zone section some new users are limited to two or uh to two uh but generally there's always three okay new services almost always become available first in u.s east and specifically u.s east one not all services are available in all regions all your billing information appears in u.s east one so that's a usc one particular thing uh the cost of aidable services vary per region and so if you're on the marketing website or uh for with global infrastructure you can see uh here in north america they'll say like when it launched how many availability zones and there might be some conditions so you'll notice there's like asterisks uh beside these things here or um in this one particular there's an asterisk saying hey there are three zones but generally you're limited to two okay when you choose a region there are four factors you need to consider uh what are the regulatory compliance does this region meet what is the cost of this enable service in this region what input services are available in this region and what is the distance distance or latency to my end users and those are those four factors that you should remember okay all right so we just talked about adabus regions now let's talk about uh how that affects our services versus regional and global services so regional services are scoped based on what is set in the database management console on the selected region so you have this drop down and that's what you'll do you'll say okay i want to have resources in canada or in europe so this will determine where a native service will be launched and what will be seen within the airbus services console you generally don't explicitly set the region for a service at the time of creation i explicitly mentioned this because when you use something like gcp or azure when you create the resource that's when you select the region but aws is it has this kind of global thing which is unique to their platform then there's the concept of global services so some aw services operate across multiple regions and the region will be fixed to the word global and for these that's services like s3 cloud front row 53 iam so the idea is if you were to go over to cloud cloudfront and go into the cloudfront console you'll notice that it will just say global and you can't switch out of that for these global services at the time of creation it's a bit different so we were saying up here for regional ones that you don't select the region but when you are clearing global services if you're using something like iam there is no concept of region because they're just globally available so you don't have to determine a subset of regions if you're using s3 bucket that has to be in one region so you actually do have to select a region at time of creation um and then there's something like cloud form distributions where you were choosing a group of regions so you either say all of the world or only north america which is more like geographic distribution so you don't say the region in particular but you know hopefully that gives you a distinction between regional services and global services hey this is andrew brown from exam pro and we are taking a look at availability zones so availability zones commonly abbreviated as a z and i'll frequently use b using the term a z is physical locations made up of one or more data centers so a data center is a secured building that contains hundreds or thousands of computers and this is one of my favorite graphics i like to show of course uh you know aws would never have a dog um in their data center but i just thought that would be fun a region will generally contain three availability zones and i say generally because there are some cases where we will see less than three so there might be two data centers within a region will be isolated from each other so there will be different buildings but they will be close enough to provide low latency and that is within the 10 milliseconds or less so it's very very low uh it's common practice to run workloads in at least three azs to ensure services remain available in case one or two data centers fail and this is known as high availability and this generally is driven based on regulatory compliance so a lot of companies uh you know they have to at least be running in three az's and that's why aws tries to always have at least three azs within a region uh azs are represented by a region code followed by a letter so here you know you'd have us east one which would be the region and then the a would represent the particular availability zone in that region um so a subnet which is related to availability zones is associated with two availability zones so you never choose an az when launching resources you always choose a subnet which is then associated uh two and a z a lot of services um you know don't even require you to choose a subnet because they're fully managed by aws but in the case of like virtual machines you're always choosing a subnet okay so here is a graphical uh representation or a diagram that's representing two availability zones so here we have the region usc 1 and us west 2 and then we have our 2az so here is 1a and 1b and so these are effectively the subnets okay and so within those subnets then you can see or availability zones you will see that we have two virtual machines okay so the usc s1 region has six azs and i thought that's just kind of like a fun fact because it is the most out of every single one um i don't think anyone comes close to usc 1 but of course it is the most popular it is the first uh region or so it's not a surprise that that one has that many a okay so we just covered regions and availability zones but i really want to make it clear what they look like so i kind of have a visual representation so let's say we have our aws region and in this particular one we have canada central which in particular is montreal so ca central one and the idea here is that a region has multiple availability zones so here you can see that we have uh one a one b and one d for some reason aws decided to uh not launch one c maybe it's haunted who knows you know and then within your um availability zones they are made up of one or more data centers so just understand that az is not a single data center but could be a collection of buildings and that these azs are interconnected with high bandwidth low latency networking they're fully redundant dedicated to metro fiber providing high throughput low latency networking between so just very fast connections in between and all traffic between azs is encrypted and these azs are within a hundred kilometers so about 60 miles of each other okay so what i want to do here is just show you uh how regions and availability zones work with some different database services so you have a general idea when you are selecting a region or a z and when you're not so within aws when you want to select a region you're going to go up here and change it and this is going to apply to regional services a very famous example of a regional service would be ec2 so we go over to ec2 which is elastic cloud computing or compute whatever let's forget the name of it and what we can do is go over to instances i'm going to launch an instance i'm not going to complete the process i just want to show you what would happen when you go select some things here so i'm going to go with amazon x2 we're going to just go to next here and so here is where we're going to select our availability zone so up here we have north virginia that's our region and when i say we're selecting our availability zone we're actually selecting the subnet so so here we are choosing a subnet and a subnet is associated to a availability zone and every single um region has a default vpc and that vpc has subnets set up and the subnets are defaulted to each of the availability zones available so usc 1 has six of them so this server is going to launch in u.s east 1b so this is a regional service okay then we have global services like s3 so we go over to s3 and it says it's global right and so we're going to go ahead and create our bucket and so here we choose the region so we go down we're going to say the region we want to be in but we don't choose the availability zone because there's nothing to um choose because aws is going to run these in multiple azs and it doesn't matter to you what it's doing there okay so there's that and then there's something like cloudfront so cloudfront's a little bit different here so we go over to cloudfront and we create ourselves a distribution um and so yeah if you don't have that option there because sometimes database has like a splash screen just click on the left hand side then go to distributions okay and so here well they changed it again on me they're always changing this ui but if we scroll on down it should allow us to change um change where this is going to launch it's like global stuff like that literally they just recently changed this and that's why i'm confused ah we'll scroll on down here it used to be maybe it's under legacy additional customized oh it's here sorry okay so notice here the price class that says use the edge locations for best performance north america and europe north america europe asia middle uh middle east and africa so we're not choosing a particular region we're picking a geographical area and so those are pretty much the major um uh examples of that uh then there's of course things like in iem where you don't even say where it is so you go to i am you know if i create something like a group over here a user group whoops here i say create group you know i'm not saying oh this is for this particular region or something like that okay so yeah hopefully that makes sense hey this is andrew brown from exam pro and let's take a look here at fault tolerance specifically for global infrastructure and so before we jump into that let's just define some fault terminology here uh so let's describe what a fault domain is so a fault domain is a section of a network that is vulnerable to damage if a critical device or system fails and the purpose of a fault domain is that if a failure occurs it will not cascade outside that domain limiting the possible damage and so there's this very popular meme called this is fine where there's obviously a serious problem but the person's not freaking out and i gave it some context to say well the reason they're not freaking out because they know that there's a fault domain and nothing outside of this room is going to be affected okay so you can have fault domains nested inside of other fault domains but generally they're grouped in something called fault level so a fault level is a collection of fault domains and the scoping of a fault domain could be something like a specific specific servers in a rack an entire rack in a data center an entire room in a data center the entire data center building and it's really up to the cloud service provider to define those boundaries of the domain it's abstracts it all away so you don't have to think about it but just to compare it against something else when you're using azure you actually define your fault domain so you might say like okay uh make sure that this workload is never running on the same vm on the same rack for these things uh and you know you might like to have this level of control but i really like the fact that it was just abstracts it away i'm not sure how they segment their uh their their fault domains but they definitely are some broader ones which we'll describe right now so when we're looking at an enables region this would be considered a fault level and then within that fault level you would have your availability zones and these would be considered fault domains and of course those data centers can have uh fault domains within them okay like maybe you know they have everything in a particular room and that room is secure so like if there's a fire in that room it's not gonna affect the other room things like that um so each amazon region is designed to be completely isolated from the other amazon region they achieved this with the greatest possible fault tolerance and stability uh each availability availability zone is also isolated but the availability zone in a region are connected through low latency links each availability zone is designed as an independent failure zone and so here we have some kind of different language that database is using i've never experienced this terminology in other any other cloud service providers so i kind of feel like it's something that it was made up but basically a failure zone they're just basically saying a fault domain but let's kind of expand on their fault failure zone terminology so availability zones are physically separated within a typical metropolitan region and are located in lower risk flood plains discrete uninterruptible power supply so ups and an onsite backup generation facilities uh data centers located in different azs are designed to be supplied by independent substations to reduce the risk of an event on the power grid impacting more than one availability zone availability zones are all redundantly connected to multiple tier one transit providers and we'll talk about what those are in an upcoming slide and just one thing i want to note here is that when you adopt multiaz you get high availability so if an application is partitioned across azs companies are better isolated and protected from issues such as power outages lightning strikes tornadoes earthquakes and more so that's the idea behind you know why we want to run in multiaz okay because of these fault domains hey this is andrew brown from exam pro and we're talking about the global network so the global network represents interconnections between aws global infrastructure and it's commonly referred to as the backbone of aws so is ec2 so just understand that that could be used in more than one way but think of it as a private expressway where things can move fast between data centers and uh one thing that is utilized a lot to get data in and out of aws very quickly is edge locations they can act as on and off ramps to the abs global network of course you can get to the network through pops which we'll talk about um you know in the upcoming slides here but let's just talk about edge locations and what services use them so uh when we're talking about things that are getting on to the database network we're looking at things like abus global accelerator aws s3 transfer acceleration and so these use edge locations as an onramp to quickly reach able's resources and other regions by traversing the fast away global network notice that the names in it's a accelerator acceleration so the idea is that they are moving really fast okay on the other side when we talk about like an offramp we're looking at amazon cloudfront which is a content distribution network this uses edge locations to as an offramp to provide an at the edge storage and compute near the end user and one other thing that is kind of always utilizing the global network are vpc endpoints now these aren't using edge locations but the idea here is that this ensures your resources stay within the aws network and do not traverse over the public internet so you know if you have uh you know a resource running in u.s east one and one in uh eu it would and they never have to go to the internet it would make sense to always enforce it to stay within the database network because it's going to be a lot faster so there you go hey this is andrew brown from exam pro and we are taking a look at point of presence also known as pop and this is an intermediate location between a database region and the end user and this location could be a data center or a collection of hardware so for aws a point of presence is a data center owned by aws or trusted partner that is utilized by itabus services related for content delivery or expedited upload so a pop resource could be something like an edge location or a regional edge cache so as an example over here we see an s3 bucket and it has to go through a regional edge cache and then cut to an edge location let's go define what those are so an edge location are data centers that hold cached copies on the most popular files so web pages images and videos so that the delivery of the distance to the end users are reduced then you have regional edge locations and these are data centers that hold much larger caches of less popular files to reduce a full round trip and also to reduce the cost of transfer fees so to kind of help put pops more in presence just in the general sense here is a uh diagram i got from wikipedia that kind of just shows a bunch of different networks and notice where the pop is it's on the edge or the intersection of uh two networks so here you know we have um you know tier three and then there's tier two and there's this pop that is in between them okay so tier one networks is a network that can reach every other network on the internet without purchasing iptransit or paying for peering and so the innovas availability zones or azs are all redundantly connected to multiple tier one transit providers okay all right so let's take a look at some state of the services that are utilizing pops or edge locations for content delivery or expedited uploads so amazon cloudfront is a content delivery network service and the idea here is you point your website to cloudfront so it will write requests to the nearest edge location cache it's going to allow you to choose an origin so that could be a web server or storage that'll be the source of the cache and caches the content of what origin would return to various edge locations around the world then you have amazon s3 transfer acceleration this allows you to generate a special url that can be used by the end users to upload files to a nearby edge location once a file is uploaded to an edge location it can move much faster within the aws network to reach s3 then at the end here you have aws global accelerator you can find the optimal path from the end user to your web servers so global accelerators are deployed within edge locations so you send user traffic to an edge location instead of directly to your web application this service is really really great for if let's say you're running a web server usc 1 and you just don't have the time to set up infrastructure in other regions you turn this on and you basically get a boost okay this is andrew brown from exam pro and let's take a look at it was direct connect so this is a private or dedicated connection between your data center office colocation and aws and so the idea here is imagine if you had a fiber optic cable running from your data center all the way to your aws so that it feels like when you're using your stuff on your data center like your local virtual machines that there's like next to no latency okay so direct connect has two very fast network connection options we have the lower bandwidth which is at 50 to 500 megabytes per second and then you have the higher bandwidth which is one gigabytes to 10 gigabytes per second so using direct connect helps reduce network costs increase bandwidth throughput so great for high traffic networks it provides a more consistent network experience than a typical internetbased connection so reliable and secure i do want to point out the term colocation if you never heard of that before a colocation or a carrier hotel is a data center where equipment space and bandwidth are available for rental uh to retail customers i do want to also point out that even though it says private up here and this is the language that aws used i usually just say dedicated but the connection is private but that doesn't necessarily mean it's secure okay so uh we'll talk about that when we reach above vpns and how we can use that with direct connect to make sure our connections are secure okay all right so let's take a look at what a direct connect location is so a direct connect location are trusted partner data centers that you can establish a dedicated highspeed lowlatency connection from your onpremise to aws so an example of a partner data center would be one like here in toronto the allied data center so you can tell that's right down in uh the toronto center and so you would use this uh uh as part of direct connect service to order and establish a connection okay hey this is andrew brown from exam pro and we're taking a look at local zones which are data centers located very close to densely populated areas to provide single digit millisecond low latency performance so thinks like seven milliseconds for that area so here is a map of uh local zones that exist and ones that are coming out i believe the orange ones are probably ones that are on their way and so to use a local zone you do need to opt in so you gotta go talk to aws probably open a support ticket to get access to it the first one to ever be launched was uh the la one uh and so um you know when you want to see it it looks just like a availability zone it's going to show up under whatever region that is because these are always tied to existing regions so the la1 is tied to u.s west uh region and the az would look like u.s west 2 hyphen la x hyphen 1a okay so only specific ab services have been made available so there's a particular ec2 types ebs amazon fsx application load balancer amazon vpc they probably have extended it to more services do you need to know that for the exam no but you know the point is is that there's a limited subset of things that are available the purpose of local zone is to support highly demanding applications sensitive to latency so media and entertainment electronic design and automation ad tech machine learning so it kind of makes sense like you look at la they're in the media entertainment and so they're dealing with lots of media content so it has to be really low for them okay hey this is andrew brown from exam pro and we are taking a look at abus wavelength zones and these allow for edge computing on the 5g networks and applications will have ultra low latency being as close as possible to the users so abus has partnered with various telecom companies to utilize their 5g networks so we're looking at verizon vodafone kddi sk telecom and so the idea here is that you will create a subnet tied to a wavelength zone and then and just think of it as like an availability zone but it's a wavelength zone and then you can launch your vms to the edge of the targeted 5g network so that's the network you're using aws to deploy an ec2 instance and then when users connect to you know those radio towers those cell towers they're going to be routed to you know nearby hardware that is running those virtual machines okay and that's all it is it's just it's just ec2 instances um but you know the advantage here is that it's like super super low latency okay hey this is andrew brown from exam pro and we are taking a look at data residency so this is the physical or geographical location of where an organization or cloud resources reside and then you have the concept of compliance boundaries so a regulatory compliance so legal requirement by government or organization that describes where data and cloud resources are allowed to reside and then you have the idea of data sovereignty so data sovereignty is the jurisdictional control or legal authority that can be asserted over data because its physical location is within a jurisdictional boundary and so the reason we care about this stuff is that if we want to work with the canadian government or the us government and they're like hey you got to make sure that you know if you want to work with us all the data has to stay in canada and you need to give them that guarantee so data residency is not a guarantee it just says where your data is right and compliance boundaries are those controls that are in place to say okay this is going to make sure that data stays where we want to be and date of sovereignty is just like the idea of the scope of the legal the legal stuff that ties in with compliance boundaries so how do we do that on aws well there's a few different ways but um let's just take a look at some ways that we can meet those compliance boundaries one which is very expensive but also very cool is aws outposts so this is a physical rack of servers that you can put in your data center and you'll know exactly where the data resides because you know it's physical if it's in your data center and you're in canada that's where it's going to be okay and i believe that you know there is only a subset of aws services that are available here but you know that is one option to you another is using like services for governance so like one could be abs config this is a policy as a code service so you can create rules to continuously check database resource configuration so if they deviate from your expectations you are alerted or image config can in some cases auto remediate so if you were expecting you know um you know you had an aws account and you're saying this account is only to be used for candid resources and somebody launches let's say something in another region then you could get an alert or to tell it was config to go delete that resource okay now if you want to prevent people from doing it all together that's where i am policies come into play so these can be written explicitly to deny access to specific aws regions and you know this is great if you're applying it to users or roles but if you wanted to have it organizational wide across all of your abus accounts you can use something called a service control policy that is just an i am policy that is used within its organizations that makes it organizational wide okay hey this is andrew brown from exam pro and we are looking at it for government so to answer that we first have to understand what is public sector so public sector includes public goods and government services such as military law enforcement infrastructure public transit public education health care and the government itself so abus can be utilized by the public sector or organizations developing cloud workloads for the public sector enables achieves this by meeting regulatory compliance programs along with specific governance and security controls so this could be i meet the requirements with hipaa fedramp um cjis and fips okay so amaz has a special regions or special regions for us regulation called govcloud which we'll talk about next okay hey this is andrew brown from exam pro and we are taking a look at govcloud and to understand what govcloud is we need to know what fedramp is so fedramp stands for federal risk and authorization management program it's a u.s governmentwide program that provides a standardized approach to security assessment authorization continuous monitoring for cloud products and services so that we know what fedramp is what is govcloud well and again it's not particular to aws because azure has govcloud as well but a cloud service provider like aws or azure general will offer an isolated region to run fedramp workloads and so in aws it's called govcloud and these are specialized regions that allow customers to host sensitive controlled unclassified information and other types of regulated workloads so govcloud regions are only operated by you by u.s citizens on u.s soil they are only accessible to u.s entries and root account holders who pass a screening process customers can architect secure cloud solutions that comply with fedramp uh do the doj's credible justice information systems uh security policy the u.s international traffic and arms regulation export administration regulations the department of defense cloud computing security requirements and guides so if you want to work with the us government you want to engineer and use govcloud okay hey this is andrew brown from exam pro and we're taking a look at uh running ada bus in china so eight of us china is the ito's cloud offering in mainland china enemies china is completely isolated intentionally from adamus global to meet regulatory compliance from mainland china so that means that if you make a workload on the awesome global you can't interact with it within the aws china one okay it's basically treated like a completely separate service like adabus has its own chinese version uh and so it was china is on its own domain at amazon aws.cn and for everybody else that's what's considered it is global so when i'm using adabus from canada or use it from the u.s or from india or from europe or wherever that is the adabus global okay so in order to operate in aws china regions you need to have a chinese business license so icp license not all services are available in china so you will not have the use of route 53 and you might say well why not just run in singapore and it was global and you could do that but the advantage of running in mainland china means that you would not have to traverse the great firewall okay so all your traffic is already within china so you don't have to deal with that airbus has two regions in mainland china so uh there's this one here which is the northwest region operated by nswc and then you have the one in beijing north one operated by uh synnet so you know itabus just could not meet the the compliance requirements so they had to partner with local providers uh or data centers and so that is how that works okay all right so i want to show you how you get over to the chinese database management console so this one is adabus.amazon.com that is the global one for everyone outside of mainland china but if you want to run resources uh on data centers within mainland china this is at amazon awesome.cn and so it looks very similar if you go to create a free account you're going to fill in this stuff but notice that you need to have your business registration certificate uh and additional information in order to run these data centers down below that aws is partnered with also notice that the logo doesn't say aws in it and there's a good reason for that if i type in aws trademark china inbus is actually banned from using the aws logo in china uh for whatever reason it's a weird reason if you ever want to read about it but that's why you don't see aws here all right so yeah there you go hey this is andrew brown from exam pro and we are looking at sustainability for aws global infrastructure and before we talk about that let's talk about the climate pledge so amazon cofounded the climate pledge to achieve net zero carbon emissions by 2040 across all of amazon's businesses which includes aws if you all want to find out more information go to sustainability.about amazon.com there's a lot of great information there and you'll learn exactly how uh aws is achieving this in particular like their data centers it's very interesting okay so aws cloud sustainability goals are composed of three parts the first is renewable energy so eight of us is working towards having their abs global infrastructure powered by 100 renewable energy by 2025 and abbas purchases and retires environmental attributes to cover the nonrenewable energy for abyss global infrastructure so they would purchase things like renewable energy credits also known as recs guarantees of origin so gos the second point here is cloud efficiency so abyss infrastructure is 3.6 times more energy efficient than the medium of u.s enterprises data centers surveyed so that's going to really rely on that survey surveys are not always that great so you know take that with a grain of salt okay then we have water stewardship so direct evaporative technology to cool our data centers use of nonuh potable water for cooling purposes so the recycling water onsite water treatment allows us to remove us them to remove scale forming minerals and reuse waters for more cycles water efficiency metrics to determine and monitor optimal water use for each adibus region and you'll find that water plays a large part on making these um these data centers very efficient okay so i just wanted to show you where you get to that sustainability information so i just went to itabus global infrastructure you click sustainability and that's going to bring us over to oops i have my twitter open there to the sustainability in the cloud so if you want to read a bunch of stuff here about things that are going on that database is up to see how they are progressing with renewable energy there's cloud efficiency up here so you know how they being efficient it's worth the read to really understand that there's a lot of water involved like reducing water and data centers i thought that was really interesting um i mean they have native podcasts but i don't think there's really much to it a biweekly podcast of bitesized stories about how tech makes the world better that's not necessarily a sustainability podcast it's just the endless podcast in general there's a download center um amazon's 2020 sustainability reports so i guess you can download the reports to see what is going on there so we could download the progress here and see what they've been up to okay so there's a bunch of numbers things like that okay very short reports but hey at least you can download them okay so just in case you're very interested in sustainability all right hey this is andrew brown from exam pro and we are taking a look at abus ground station so this is a fully managed service that lets you control satellite communications process data and scale your operations without having to worry about building or managing your own ground station infrastructure and so when we're talking about ground station a really good way to cement what the service is is just think of a big antennae dish that's pointing to the sky trying to communicate with satellites because that's essentially what the service is doing so the use cases here could be for weather forecasting surface imaging communications video broadcasts and to use ground station the idea is that you would schedule a contact so that's where you're selecting a satellite a start and end time in the ground location and then you use an abuse ground station ec2 ami and amazon machine image to launch ec2 instances that will uplink and downlink data during the contact or receive downlink data in an amazon s3 bucket a use case could be something like you are a company you've reached an agreement with a satellite image provider to use their satellites to take photos for a specific region or time or whatever and so the idea is that you are using aws ground station to communicate to that company satellite and download that as that image data to your s3 bucket okay hey this is andrew brown and we are looking at able's outposts and this is a fully managed service that offers the same ableist infrastructure services apis tools to virtually any data center colocation space or onpremise facility for a truly consistent hybrid experience and just to kind of summarize it it's a rack of servers running aws stuff on your physical location okay so before we jump into the service or technology itself uh let's talk about what is a rack server or just a rack so it's a frame designed to hold and organize it equipment so here's an example of a four to u rack and there's a concept of rack heights so the u stands for rack units or u spaces uh with it equal to 1.75 inches and the industry standard rack is a 4 8 u um so that is a seven foot rack so um a full uh size rack cage is commonly the four two high okay and uh in it you might have equipment that is of different sizes so there could be one u two u three u or four u high so here's an example of you know of an interior of a rack and notice that like one u two u for u they're all a little bit shaped differently uh but they give you kind of an idea of um you know what those are so it's outpost comes in three form factors the four to you the one you and the two you so the uh the first one here the four to you this is basically a full rack of servers provided by aws so you're not just getting the frame it actually comes with you know servers uh and so abs delivers it to your preferred physical site fully assembled and ready to be rolled into the final position it is installed by aws and the rack needs to be simply plugged in to the power and network and there's a lot of details about um the specs on this on the adabus website so you know i'm not going to go through them all here um then there are servers that you can just place into your existing racks so we have the 1u so this is suitable for 19 inch wide 24 inches deep cabinets it's using it with gravitron 2 cpus and you can have up to 64 virtual cpus we have 128 gigabytes 4 terabytes of local nvm storage um and then you have the u or sorry the 2u so suitable for 19 inch wide 36 inch deep intel processors up to 128 virtual cpus 256 gigabytes of memory eight terabytes of local nvme storage so there you go let's take a look at cloud architecture terminologies before we do let's talk about some of the roles that are around doing cloud architecture so the first is solutions architect this is a role in a technical organization that architects a technical solution using multiple systems via researching documentation and experimentation and then you have the cloud architect this is a solutions architect that is focused solely on architecting technical solutions using cloud services understand that in the actual marketplace a lot of times solutions architect is used to describe both a cloud architect and a solutions architect and you know these are going to highly vary based on your locality and how companies want to use these terms but this is just me broadly defining them here so just don't take them as a perfect word in terms of what they're representing so a cloud architect needs to understand the following terms and factors and factor them into their designed architecture based on the business requirements so we have the idea of availability your ability to ensure service remains available scalability your ability to grow rapidly or unimpeded elasticity your ability to shrink and grow to meet the demand fault tolerance your ability to prevent a failure disaster recovery your ability to recover from a failure and there are a couple other things that uh you that should be considered they're not terminologies but they're definitely important to a solutions architect or cloud architect and uh these are things you always need to consider as well and this is just me talking to my solutions architect friends where they'll always ask me these two questions after presentation they'll say how secure is the solution and how much is this going to cost all right and so for the terminologies up here we're going to define these right away and we're going to figure these out throughout the course we have two giant sections just on cost and security alone uh so there we go the first term we're looking at is high availability and this is your ability for your service to remain available by ensuring there is no single point of failure and or you ensure a certain level of performance so the way we're going to do that on aws is you'd want to run your workload across multiple availability zones to ensure that if one or two availability zones became unavailable your servers or applications remain available because those other those other servers are going to be there and the way we would accomplish that is via elastic load bouncer so a load balancer allows you to evenly distribute traffic to multiple servers in one or more data center if a data center or server becomes unavailable or unhealthy the load bouncer will route the traffic to only the available data centers within the server and understand that just because you have additional servers doesn't mean that you are you're available you have to you might need to meet a particular threshold of availability so you might need to have at least two servers always running to meet the demand so it's based on the the demand of traffic okay let's take a look here at high scale abilities so this is your ability to increase your capacity based on the increasing demand of traffic memory and computing power and we have the terms vertical scaling so scaling up this is where you upgrade to a bigger server and then there's horizontal scaling scaling out this is where you add more servers of the same size and the great thing about scaling out or adding additional servers is that you're also going to get high availability so if you do need two servers it's always better to you know add an additional server as opposed to having a larger server but it's going to be very dependent on a lot of factors okay so scalability and elasticity seem very similar but there is a crucial difference and this is your ability to automatically increase or decrease your capacity based on the current demand of traffic memory and computing power again it's the it's the fact that it happens automatically and you can go both ways increase or decrease so for horizontal scaling we have the concept of scaling out so add more servers of the same size and then scaling in removing underutilized servers of the same size and vertical scaling is generally hard for traditional architectures so you'll usually only see horizontal scaling described with elasticity and the way we would accomplish uh being highly elastic is using auto scaling groups asgs and this is a database feature that will automatically add or remove servers based on scaling rules you define based on those metrics okay let's talk about being highly fault tolerant so this is your ability for your service to ensure there is no single point of failure preventing the chance of failure and the way we could do that is with failovers so this is when you have a plan to shift traffic to a redundant system in case the primary system fails a very common example is having a copy or secondary uh of your database where all ongoing changes are synced the secondary system is not in use until a failover occurs and it becomes the primary database so when we're talking about databases on abs this is the concept of rds multiaz so this is when you run a duplicate standby database in another availability zone in the case your primary database fails and last here is high durability so this is your ability to recover from a disaster and to prevent the loss of data so solutions that recover a disaster uh from a disaster is known as disaster recovery so do you have a backup how fast can you restore the backup does your backup still work how do you ensure current live data is not corrupt and so maybe a solution aws would be using cloud endure which is a disaster recovery service which continuously replicates your machines in a low cost staging area in your target apes account and preferred region enabling fast and reliable recovery in the case of an i.t data center fails okay so to understand disaster recovery we need to know more about uh things around it like business continuity plans bcps and rtos and rpos so a bcp is a document that outlines how a business will continue operating during unplanned disruption in services so it's basically the plan that you're going to execute if that happens and so here we have a disaster and you can see that there's a chance of data loss and downtime and these two factors as rpo and rto are going to define the length of these durations so recovery point objective is the maximum acceptable amount of data loss after an unplanned data loss incident expressed this amount of time so how much data are you willing to lose and then recovery time objective so the maximum amount of downtime your business can tolerate without incurring a significant financial loss so how much time you're willing to go down okay so those are the two there and now let's go take a look at the disaster recovery options that we can use to define in our our bcp so let's take a look at our disaster recovery options uh and based on what you choose they're going to be a trade of cost versus time to recover based on the rpos your rtos of course and so sometimes this is re represented vertically like a a thermostat or you can do it horizontally here both are valid ways of displaying this information but i just have it horizontally here today and so we have low or high or you could say even though i don't have it written here this could be cold or this could be hot okay so um on the left hand side we got back up and restore pilot light warm standby multi active site notice we're using the like the words like pilot light warm things that are relating to temperature so again cold and hot all right so let's just walk through what each of these things us conceptually do uh in terms of architecture so when you're doing a backup restore you're back you basically back up your data and at the time of disaster recovery you're just going to restore it to new infrastructure for a pilot light the data is replicated to another region with the minimal services running to keep on replicating that data and so you might have some core services running a warm standby is a scaled down copy of your infrastructure so you basically have everything that you would absolutely need to run an application but the idea is it's not at scale and so at any time when there's an incident you're going to scale up to the capacity that you need and then you have multisite active active where you you have a scaled up copy of your infrastructure in other regions so basically everything you have identically in another region and so in terms of the rpos and the rtos for back and restore you're looking at hours uh with the pilot light you're looking at 10 minutes with a warm standby you're looking at minutes and multisite active active you're looking at real time so you know hopefully that gives you an idea of you know the difference in terms of scale but let's just look at more detail so for a backup and restore this is for low priority use cases restore data after event deploy resources after an event and it's very cost effective for pilot light you this is where you have less stringent rtos and rpos so that you're going to be just running your core services you're going to start and scale resources after the event and this is a little bit more expensive this is very good for warm standby is good for business critical services so you scale resources after the event uh and it's almost very it's very it's costly but it's not as expensive as a multisite active active so you get zero downtime near zero loss uh you have it's great for mission critical services and it's just as expensive as your original infrastructure so you're basically doubling the cost there okay so we already defined rto but let's redefine it again based on what aws describes in their white paper and just look at how it maps against the disaster recovery options so recovery time objective is the maximum acceptable delay between the interruption of service and restoration of service this objective determines the what is considered an acceptable time window when service is unavailable and is defined by the organization and so this is the diagram found in the white paper and so on the lefthand side we have cost and complexity here and then lengths of service interruption and what you can see here is that the cost and complexity for a multisite active active is very high but the length of service interruption is zero and then as we go down we have warm standby so it's significantly like at least half the complexity of that one then we have our pilot light down here and backup and restore but notice backup restore takes the longest amount of time and notice here we have a recovery time objective so in your bcp you kind of define where that is based on the cost of business impact so you might have to calculate that saying okay what is our cost over time based on the length of service interruption where do we want our rto to be what is the acceptable recovery cost and this is where you're going to decide uh what you want to do so here we have pilot light and backup and restoring so this company he has to decide whether they want to do a pilot light or they're going to do a back and restore but it sounds like this is where they're going to be which is at the pilot light for what is acceptable in their business use case okay let's do the same for rpo so recovery point objective is the maximum acceptable amount of time since the last data recovery point the objective determines what is considered an acceptable loss of data between the last recovery point and the interruption of service and is defined by the organization again we pulled this from the aws white paper for disaster recovery and uh we have cost and complexity but this time it's replaced with data loss before service interruption so uh for multisite again it's going to be very expensive and high up here as you notice it's not like a perfect um curve it's just it's a bit different in terms of what it looks like so here we have warm stand standby pilot light and so you'll see that the data loss is um not a big deal but for backup and restore it really juts out there so you can see that you can get pretty good results just with the pilot light and the cost and complexity is very low again we have to look at our cost and business impact so we got to follow that line and we need to see where our acceptable recovery cost is and so you're going to notice that we have a bit of an intersection here okay and so we need to determine you know like are we going to be doing a warm standby it looks like we have the cost to do it um but you know it just really depends you know do we want to be down here or down there okay so hopefully that helps and visualize that information for you hey this is andrew brown from exam pro and what i want to show you here is a real world architectural diagram i created this a while ago this is a previous version of the example or technically teacher see platform that powers the learning experience for my cloud certifications and so i'm hoping that by giving you some exposure you'll absorb some information here and that will carry through to really help you cement what these services do and how they work together now you might be asking how did i make this well i'm in adobe xd it's by photoshop or sorry adobe it's free to download but there's a lot of options out there and but the first thing you'll need is those aws architectural icons so these are free on aws you can download them in powerpoint download those assets as svgs and pngs which is what i have done and start using them in your um whatever software you like there's also third party providers out there so like there's lucidcharts i love lucidcharts but i don't use it to make architectural diagrams for aws um but you know you can drag drop and stuff and they already have the library there and there's a bunch of them that you can choose from so uh you know that's interesting but let's take a look at one that we can download maybe everyone's familiar with powerpoint so here is the aws architectural icons and the reason i'm showing you this is not because it just contains icons but it also suggests how you should build them so if i go through here they'll give you a definition of those system elements uh how they would look like here so we have our group icons our layer group our service icons resource icons where they should go and then they have some interesting guidelines of like do's and don'ts so here's like a simple example of a get to an s3 bucket here's an example of using vpc subnets and things like that on the inside um and then you can see kind of like all the groups that we have and they'll show all like the uh the arrows it's a big faux pas to make a diagonal arrows that's just something that it was defined but you'll see a lot of people do them anyway and then you'll see all the icons so do you have to make them like eight of us suggest no but you know if you like the way they look that is fine everyone just does whatever they want honestly so anyway now that we've seen you know how we can go get the resources to make our own i have adobe xd opened up here and so i just kind of want to walk you through what's going on here so again i said this is a traditional architecture meaning that it's powered by virtual machines and so what we need to look for uh is ec2 because that's where it's going to start that's our virtual machine and you'll notice we have one here so there's a t2 um that's running over here and then over here we have a t2 okay so we have a blue and a green environment so this is our running environment so i'm just going to zoom on in here okay so the web app would be running on this and and then on the outside here we have an auto scaling group and so auto scaling groups allow us to manage a group of ec2 instances and they will automatically scale if the demand increases or or decline so if this machine can't handle it it will just automatically provision a new one and so i've contained it in this environment here because i'm representing a blue green deploy meaning that when i deploy this will get this will be the environment that replaces things and so you can see i have a lot of lines being drawn around here so over here we have uh parameter store so parameter store is a place where we can store our environment variables um or application configuration variables and so i have this line going here and it's just saying we're going to take these environment variables and put them into the application okay and then there's also uh the database credentials so here we are using postgres over here so and then we need the database credentials so we're grabbing those database credentials those are stored in secrets manager and we're giving to the application so the app knows how to connect to the database and this one knows how to uh configure it okay then we have a bunch of uh buckets here for different organizations and so you know s3 is for storage so this is a way we're going to um store a variety of things so like user data assets artifacts cloudformation templates so some of this is for the app some of them is for the infrastructure so that's one thing there okay then over here we have a ci cd pipeline so we have code pipeline and so code pipeline is triggered by github so we put our code in github and when that happens it's going to do a code build so that's going to build out a server and then from there it's going to run another code build server and then from there it's going to then um use codeploy and so codeploy is going to trigger a deploy what it will do is create a new environment so it's going to create a copy of this um sorry it's going to create a cop this is actually the environment that's running so we'll copy that and that will be our new environment right okay and so when the deploy is done it will swap and that environment will become this new one um and so you know again this is actually really the the running server it's just kind of easy to get hung up on this one but the idea here is that um you know that's how deployment works but let's say you know we want to get uh traffic to this actual instance this is going to come through the internet and the internet is going to probably go to revit three so reference three is used for domain names so this would be like example teacherseek.com we pass that over to our elastic load bouncer which in this case is an application load bouncer that's why it's called alb and that's going to distribute the traffic there if we wanted to run the server in another um in another availability zone so that we make it highly available you know alb the elastic load balancer application load bouncer is going to have some traffic go here and some traffic go there so this is just the blue environment or whichever the current environment is over here now when we want to deploy new versions we're going to use launch templates and launch templates um uh are necessary when using auto scaling groups so um you know you do have to define launch template it just says like what is the shape of this instance type like what's this family what should it be and then we need an amazon machine image so our amazon machine image is custom built because we are installing all the stuff that we want on it and so in order to automate that process we are using um ssm automation documents so ssm stands for systems manager and automation allows you to automate that step so what it's going to do is launch an instance install ruby install postgres download the code base then it's going to create that ami and then um it will do a bunch of other stuff here as well and this is going to run weekly or actually at the time uh it was running nightly so we're doing nightly builds so that we would always get the latest um updates to our server because it's a virtual machine there could always be uh new updates for that linux version or amazon machine limit next version we were using and then there's a bunch of other stuff here so you know hopefully that kind of gives you an idea of like the complexity of it and you know this is how i like to make my architectural diagrams very in detail so that we can um look at them but yeah if that was too much that's fine but you know that's just the complexity of it if you build your own you'll start to really grasp the stuff pretty well okay so what i want to do is just show you how high availability is built into some aws services where in other cases you have to explicitly choose that you want something to be highly available so what i'm going to do is make my way over to s3 and so with s3 this is where you can create s3 buckets and this allows you to store things and so the great thing about s3 is that it's basically serverless storage so the idea is that you're just going to choose your region and by default it's going to replicate your data across multiple um uh data centers or azs and so this one's already highly available by default with the standard tier and so that is something that's really nice but other services uh you know like ec2 the idea is that you are going to launch yourself an ec2 instance so we would launch that one and the problem with this is that if you launch a single ec2 that is not highly available because it's a single server running in a single um a z so here you know we would choose our subnet our subnet is our availability zone but you'd have to launch at least two additional servers and then you'd have to route um you'd have to have something that would balance uh the traffic to the to the three which is a load bouncer and so in this case you have to construct your high availability then you have services like elastic bean stock this is a platform as a service um and we'll go to environments here i'm not sure it wasn't showing up there uh and so the idea is that with elastic bean stock i'm just going to click on the main service here you're going to go ahead and create your application or create your environment you probably want to create environment first here okay and so i would choose a web server and then the idea is i'll just name it so my application here my environment and then down below you go configure more options whoops wants me to choose everything that's totally fine and we say configure more options we're not going to create it because um we don't want to create one but the idea is that you'd you could choose whether you want this to be highly available or not so see it's a single instance of free tier and then if you choose this what it's going to do is set up a bunch of stuff for you so it's going to set up an application load balancer for you it's going to set up auto scaling groups for you to make it highly available it's going to run at least between one to four instances so this does everything that ec2 you'd have to do manually setting up so that's really nice okay so you know some options have that if we make it our way over to rds and again we're not creating anything we're just looking at the options it gives us when we start things these up here we'll make our way over to rds and it gives us a moment here and if we go ahead and create ourselves a new database and we look at something like a postgres database notice that we have a production option and a dev test option and so i i mean usually it shows us the price down here so even test dev is 118 which is not true it can get cheaper than that but the idea is that when you choose between these two options um it's going to set up a multiaz it's going to that means that it's going to run an additional database and another availability zone replicate that data over so that it stays highly available um you know it's going to have auto scaling uh part of it and so some services you just choose it abstractly so you just have to understand what highly availability is going to mean underneath so hopefully that kind of gives you a picture of high availability on aws hey this is andrew brown from exam pro and we are looking at abuse application programming interface also known as database api so before we talk about the api let's describe what application programming interface is so an api is software that allows two applications or services to talk to each other and the most common type of api is via http requests and so the aws api is actually an http api and you can interact with it by sending https requests using an application interacting with apis like postman and so here's kind of an example of what a request would be that would be sent out and so the way it works is that each database service generally has a service endpoint so see where it says monitoring that's going to be cloudwatch so sometimes they're named after the services sometimes the name is a bit obscure and of course you can't just call and uh call a api request without authenticating or authorizing and so you have to sign your request and so that's a process of making a separate request with your idioms credentials to get back a a temporary token in order to authorize that and i don't have room to show it but the thing is is that what you'd be also going along with those requests would be to provide an action so when you look at um the aws api it will show you a bunch of actions that you can call they're basically the same ones you'll see in the in policies so it could be like describe ec2 instances or list buckets and they can also be accompanied with parameters okay so you know we're probably not going to show you how to make an api request directly because that's not something that you would generally do um but what you would do is you probably use the aws management console which is powered by the api use the abyss sdk which is powered by the api or using the aws cli so we'll cover all those three okay all right so what i want to do is just point you to where you'd find the resources to use the api programmatically uh we're not going to actually use the api because there's a lot more to it uh than what i'm going to show you here but at least you'll be familiar with how the api works so i'm on the aws.amazon.com website if you type in docs the type top there it's going to bring you to the main documentation and what we're looking for if we scroll on down there should be a general reference area where we have service endpoints if we click into here it's going to talk about how a server's endpoint is structured and if we go down to ibis api we can see some additional information of course to use um the api you're going to have to sign api requests first which is not a super simple process but you have to use an authorization header and send along some credentials and things like that so if you want to know what service endpoints are available to you if you search service endpoints list for aws this is the big list and so if i was to go down here and look for ec2 uh might be a common example here it's going to tell us what the endpoints are and as you can see they are regional based but the idea here is that i could take something like this okay i could grab that and using something like postman i could go and create a new request and it's probably a post i'm not sure what it's supposed to be it's probably a post and then you'd set your authorization header there might even be one in here for aws see where it says adab signature so you can go here and put your access key and secret within here um so that's something nice about postman so it's going to do the signing requests for you so it makes your life a lot easier and then from there what you do is you go to your body and you'd want to enter in json so to do json would probably be raw you drop down the format json and then you'd send your payload whatever it is so again i haven't done this in a while because it's not a very common uh thing that i have to do like describe ec2 instances but there probably is like an action and some additional information that you would send along um so you know hopefully that gives you kind of an idea how the api works but you know you should never pro in practice ever have to really work with the api uh this way directly okay hey this is andrew brown from exam pro and we are looking at the database management console so the italo's management console is a webbased unified console to build manage and monitor everything from simple web apps to complex cloud deployments so when you create your apps account and you log in that is what you're using the aws management console and i would not be surprised if you're watching this video and they've already changed um the default page here since adobe's loves to change the ui on us all the time but uh the way you would access this is via console.ableis.amazon.com when you click sign in or go to the console that's the link that it's going to uh and so the idea here is that you can point and click to manually launch and configure aws resources with limited programming knowledge this is known as click ops since you can perform all your system operations via clicks okay let's talk about the aws management console in brief here so you know of course when you're on the home page you go to aws management console and you will end up logging in and from there we will make our way over to the edwards management console when i say [ __ ] management console i'm referring to this homepage but i'm also referring to anything that i'm doing in this web ui whether it's a sub service or not so you know a lot of times people just call this the dashboard uh or the home page but you know it is technically the us management console but everything is the aws management console you can drop down services here if there's some that you like you can favorite them on the left hand side i don't find that particularly useful you can see the most recent ones here they'll also show recently up here as well we have the search at the top notice that there's a hotkey for alt s i don't think i ever use it if i was to type in a service like ec2 it's going to get me the services and then down below it's the sub features of it so if i just click into that there into this use this is the main this is a service console so i would call this the ec2 console or the ec2 service console so if you ever hear me saying go to the ec2 console that's what i'm saying and you'll notice here like there is stuff on the left hand side so i come back here ec2 image builder you see two global views these are considered services but if you drop down it says top features or you go down here it says dashboard limits amis you go over here the ec2 dashboard limits amis are here and limits are somewhere here right there so okay so those kind of map over pretty well polls and documentation knowledge based articles marketplace i don't think i've ever touched those in my life this here is the cloud shell so if you click it it will launch a cloud shell we'll cover that when we get to that section here we have this little bell it tells us about open issues i think this is for the personal health dashboard yeah it says phd in the bottom left corner or left corner so if i open that up it'll bring up the phd the personal health dashboard all right our region selector our support so nothing super exciting here but just kind of giving you a bit of a tour so that you know there are some things you can do um can you change the look of this i don't think right now as of yet um there is any way i'm sure it was thinking about it because it's been a high request that's in demand but this is what it looks like as of today okay all right so i just want to describe what a service console is so an aws service each have their own customized console and you can access these consoles by searching the service name so you would go ahead and type in ec2 and then what we refer to this screen as as the ec2 console the reason i'm telling you this is that when you're going through a lot of labs or follow alongs you'll hear the instructor say go to the ec2 console go to the sagemaker console go to the rds console what they're telling you is to go type the the name of the service and go to that particular services console okay some interest service consoles will act as an umbrella console containing many aws services so uh you know vpc console ec2 console systems manager console sagemaker console uh cloudwatch console these all contain multiple services so you know for um for ec2 you might say okay well i need a security group there's no security group console it's under the ec2 console okay so just be aware of that so now i want to show you some of these service consoles to kind of distinguish how they might vary per per service okay so if we were to look up ec2 um and we just did look at this but the interesting thing is that some consoles the ec2 console is the home for other database services and you just have to learn this over time to know that so for instance elastic block store is its own service but it's tightly linked to ec2 instances so that's why they always have it here same thing with amis security group same thing with that so these are interesting because these are basically part of virtual networking and so you'd think they'd be under the vpc console but they're actually under here with ec2 and so load balancing auto scanning groups tightly coupled to to ec2 if we make our way over to vpc you know here it's going to contain all the new stuff does it have a new experience no i guess this is the newest one it looks a bit old and a little bit new here but you know we have a lot of different things here like firewalls vpns transit gateways traffic mirroring we make our way over to cloudwatch okay and cloudwatch has uh very uh focused services they're all actually named and this is more like a feels more like a single service where you have these very focused services where you have alarms logs metrics events insights right but you're going to notice that like the ui highly varies so we had looked at cloudwatch and then we had looked at vpc and it looks like this and then we looked at ec2 and it looked like that and so there is inconsistencies because each um service team like that work on per service or whatever they have full control over their ui and so some of them are in different states of updating so some people might have updated the lefthand column but this part is old or you might click around like under something else like the ec2 dashboard or maybe a better example might be amis i remember we're in here and something looked old here yeah see these are the old buttons and that's just how it is so everything is very uh modular and so they get updated over time so that is the challenge that you're dealing with you're always having like three different versions that are cobbled together in each uh ui one thing that i found really interesting is that um vpc has its own console management console but if you were to look up this in the uh the sdk so if i was to look up abs sdk ec2 okay i'm just looking up ruby here as an example because that's what i know how to do if you look under here let's say you want to pragmatically work with vpcs you'd think that it would have its own toplevel vpc because it has in the console its own its own management console but actually vpc is tightly coupled ec2 and so when you want to pragmatically use vpc you're going to be um using actually ec2 as as how it was built so the the the what i'm trying to get is the apis don't onetoone match with this kind of stuff and so it's just kind of interesting that there's those kind of uh differences uh but again it's not that big of a deal i'm just trying to say like you know keep your mind open when you look at the stuff okay so every aws account has a unique account id and the account id can be easily found by dropping down the current user in the global navigation so what i'm going to do is pull up my pen tool here and just show you it's right there uh the imbus account id is composed of 12 digits and so it could look like this or this or this the universal account id is used when logging in with a nonroot user account but generally a lot of people like to set their own alias because it's tiring to remember your account id the you use it when you're creating cross account roles so you'd have the target account e the source account id to gain access to resources in another's account when you're dealing with support cases awast will commonly ask you what your account id is so they can identify the account that they want to look at and it is generally good to keep your account id private as it is one of the many components used to identify an account for attack by malicious actor so you don't have to be overly sensitive with it but you know try to hide it when you can when it's easy okay all right so let's talk about the account id which appears up here in the top right corner uh where you can get the account id it also appears in im so if we go over to iam and you look on the right hand side it should show you the example here it keeps on trying to take us to the old dashboard that's fine but you'll notice that it's over here and i don't have mfa turned on because i'm in my imuser account but it should be turned on on everything that's a given but you know i just want to show you where it is and also where you might be using it so one example where you would use you would need to know your account id would be something like creating a cross account policy so i went here and went to policy and went create policy um and we went to maybe it's a role i think we actually sorry we want to cross account roles not the policy sorry we go here and we say i want to access something in another abs account what we have to do is specify the account id specify the accounts that can use this role so you give i think the the id of the other account okay and so that is one place where you'd use it another place would be when you're creating policies so if i go back to policies here i can create a policy here and i can just choose something like s3 okay and i'll just choose list and under the request conditions i might specify i think the account id it should be in here um i know i can limit based on account id principal account you could do principal account so if i just looked up this here address principal account and you just got to get used to google and things because that's always what's happening here and so we should be able to specify an account id yeah like that so that would be the principle there so if i just took that and it doesn't matter what it is we just put the value in here um string equals this add i should be able to go over here and now see the full statement nope sometimes that happens because we don't have it fully filled out but um yeah so that pretty much that's pretty much how we use it like it would normally show up as that so if i just go ahead and go next the policy contains an error you are required to choose a resource what do you mean the resource is this right oh down here okay sorry so we'll just say all resources then we flip over now it's valid and so here we can see our condition saying only from this account id that it is allowed um other places we're going to see account ids are in um arn's right so if we had an ec2 instance we don't have one launched right now but if i was to go ahead and oh maybe we have some prior ones yeah so if i was to check box this here and you might not have any prior ones so there might not be nothing for you to see but if you look for the arn where is our iron sometimes it doesn't show the iron in the services sometimes it does i wish that abuse always showed the iron to make our lives a bit easier but it could be because of other reasons why but even though we don't have the rn i think it shows it shows us the owner id and so that's the account the count id number you can tell because it's 12 digits so hopefully that gives you kind of a tour of the account id and what its purpose is in the account okay all right let's take a look at aws tools for powershell so what is powershell powershell is a task automation configuration management framework is a command like shell and a scripting language so here it is over here uh if you're a windows user you're used to seeing this because it has a big blue window so unlike most shells which accept and return text powershell is built on top of the dot net common language runtime clr accepts and returns the dotnet objects so aws has a thing called the interbus tools for powershell and this lets you interact with the aws api via powershell commandlets is a special type of command in powershell in the form of the capitalized verb and noun so in this case it'd be new uh hyphen s3 buckets so you know we looked at the awcli and that is generally for bash um you know shells and so powershell is just another type of shell that's very popular and i just wanted to highlight it for those people that are uh you know used to using microsoft workloads or azure workloads uh that this actually exists okay all right let's take a look at the powershell tools um i actually haven't used this one yet so i'm kind of curious i am on a windows machine so if i was to open cm or powershell and you probably can't see this but if i just bring this over here if i type in powershell on my computer you'll notice that i have it um so that's how you would launch it looks like a blue screen here okay um if you're on a mac you're not going to have that but that's totally fine we don't need to have a windows machine to use powershell because we can go ahead and use cloud shell so make sure you're in a region that supports cloud shell so i switch back to north virginia this is not important for the exam but it's just kind of fun for me to go through this with you if you just like want to watch uh here and so i want to change this over to powershell so i imagine that it must be over here um so how do we change to powershell so we'll type in advanced power or aws cloud shell power shell like how do we do it okay and so we're just going to scroll down here so the following shells are preinstalled uh the bash the powershell the zshell you can identify them by that yeah of course to switch to new shell enter the shell's program name in the command line prompt oh wow that's easy so um if we want pwsh do we just type pwsh let's find out give it a moment to think oh there we go okay so now we're using powershell and so i would think that databus would give this preinstalled for us so if we go over here to the instructions and we scroll on down there's probably like oh wait like i don't use powershell a lot it's very easy to install modules i've done it before but i never remember how to do it but let's just see what we can find here so i want the documentation for powershell here and i'm going to go to the um the maybe the reference here because i just want to see some examples for the commandlets and so we'll look for s3 again never done this before but i'm always great at jumping into these things and all i want to do is just list out the buckets so i'm going to just search for the word list and just see if i can find something very simple here and calls to get the list buckets api operation so i think that is what we're going to be doing here so i'm going to click into that okay and then from there what i'm going to do is just see if i can copy this command so we will go ahead and copy this and paste it in here and i like how we got this little shell here so we can tweak it so we need the bucket name but i don't want to return a list of all the buckets owned by the author so we don't have a bucket name that we want to explicitly set here so it's required false so we can remove that okay we'll look at the next one select required false use the select command to control the command line output the default is bucket specifying select will result in turning all the whole buckets for that specifying the name but it says it's not required so let's just take that out as well i don't think we need any of these actually let's just go and put that in there and i think that there must be something we need to put in front of that right well let's just see what happens uh the term is not recognized as the name of the command function script is operable so i think we're missing something in front of here we'll go to the user guide here quickly and we'll get to the getting started i just want a super simple example here new bucket get bucket well let's try this one here because they have it here and so it should just work right i'm going to change this to usc 1. the term new bucket is not recognized as the name of the commandlet function so i'm guessing that the commandlet's not installed i would have thought that they would have installed it by default so i guess what we'll do is look at how to install it so installing on linux i suppose so you can install the modulized version of the powershell on computers to install aws tools on linux pwsh to start powershell core session so i guess that's how you must start it on linux and then install the module this way so yeah i said it's easy to install these things we'll hit enter cross your fingers hope this works hope this is fast i'm just going to take a look here peek forward here if you are not if you're notified the repository is untrusted you're asked if you want to trust anyway just hit y so we're waiting for that here um you're installing this module from untrusted repository it's funny that it's untrusted by but it's by aws maybe that's some kind of drama between microsoft not letting a bus have an official module there but it looks like it should be installed now so if i type in get s3 buckets here um unless i typed it wrong that still doesn't seem to be working if i go up here and try to create a new bucket still does not recommend recognize the command command lit here so there must be more going on here if you are notified you can now install the module for each service okay what did we do you're installing the the modules from untrusted if you trust it change the uh change its installation policy value by running set policy command are you sure you want to install this module from the ps gallery so i said yes and i gave it a capital y and it didn't do anything else so oh hold on here so this is the installer and then here is the actual tool that we want to solve so it installed oh so we just installed this thing and now we use this thing to install s3 okay great not hard okay and so we'll just say yes to all and so that's going to install i guess everything oh we said ec2 and s3 well we didn't need both but that's fine and so what i'm going to do is go get bucket and so now recognize it it lists out the items here we can go and create ourselves a new bucket so we'll do that okay we'll make our way back over the database management console we'll go to s3 just because i don't need all these buckets lying around here and i'm going to go ahead and delete some of these buckets here so we'll say delete my bucket great and we'll go to this one here and say delete my bucket excellent all right so we have an idea how to use powershell and so powershell is just really popular because it's the way you do inputs it's very standardized and the outputs that come so it's very popular um and a very powerful scripting tool that's our cli tool as well so you know hopefully that's that was interesting for you but what we'll do is just close these off here and go back to our home page always just clicking that logo there and there we go so amazon resource names uniquely identify aws resources and arms are required to specify resource and ambiguously across all of all of aws so the iron has the following format variations so there's a few different things here but just notice here that sometimes it has a resource id or it has a path so with the resource type or could be separated by a colon so the partition can either be aws china or gov cloud because this is basically the aws portal or url that are completely separated from each other as we talked about those earlier in the course then there's the service identifier so ec2 s3 iam pretty much every service has their own service that name here that would be identified so the region would be pretty obvious usc 1 ca central 1 you'd have a count id which would be 12 digits the resource id could be a name or a path so like for imusers we have user bob this is an ec2 instance and most of the irons are accessible via the airbus management console and you can usually click the rn to copy to your clipboard so here is it is for an s3 bucket and notice that it's a little bit different because it is a global service aws there's no reason to specify the region or the account id or anything else there like the resource type so straight away we already know it's a bucket so we can just say my bucket so that one's really short but in other cases it's really long so here it is for a load bouncer and it has all the information there and notice that like this as it passes load bouncer app my server will be and then it has the id okay for paths and arms they can also include a wildcard asterisk and we'll see these like with im policies or or paths these are really useful when you are doing um policies where you have to specify an army you want to say a group of things and things like that so there you go all right so now let's take a look at amazon resource name or also known as arn and so arns are used to reference objects they're very commonly used when you're using the cli or the sdk to reference to something um the easiest example is s3 right so if we go over to s3 here and we create ourselves a new bucket um so i'll go ahead and create ourselves a new one here we'll say my new bucket i'm just going to put a bunch of numbers in here it doesn't matter we'll hit create bucket and what we will see if we click into this is the orange should be under properties and there it is okay so there are many cases where you might want to use the iron and a lot of times you'll just copy it and a very common example would be again with i am policy so we go over to i am policies right and i want to get to policies here to save myself some trouble and we create a policy you know i might want to restrict someone to use only that bucket so let's say s3 okay and then i'm going to say i want to be able to read and write from a particular bucket we go drop down these resources here and so here we have a lot of options maybe i'll just get rid of the read option and i'm going to actually expand right because it's just creating too much work for me here and i just want to have put put object that's that's what we use to put something into a bucket so we expand the resource here and notice it says add the iron so we go here and we could type the bucket name so do that or we just paste it on in here at the top so it's probably easier just to grab it sometimes but if you don't know an iron a lot of times you can just expand this and then fill it in and that's how you get an iron so put that there let's list oh you could also do it that way which is easier too and so now if i go to json is it valid there we go so here it's saying um this policy allows somebody to put an object into this particular bucket and so that would be an example where we would use an iron okay or if you're doing uh if you're using uh itabus support you might have to use an arm to um to get help from support saying hey look at this particular resource exactly here and then the the cloud support engineer can help you okay hey this is andrew brown from exam pro and we are looking at the abs command line interface before we do that we got to define some terms so what is a cli so a command line interface processes commands to a computer program in the form of lines of text operating system implement a command line interface in a shell okay so we have a terminal say terminal is a text only interface so it has input output environment then you have a console this is the physical computer to physically input information into a terminal then you have the shell a shell is the command line program that users interact uh with uh to input commands popular shell programs or bash zsh powershell and uh you might remember this one ms dos prompt so this has been around for obviously a very long time so maybe this kind of primes your mind for what is a shell and just so you know people commonly erroneously use terminal shell or console generally describe interacting with the shell so if we say shell or console or terminal we're just talking about the same thing but there is technically a difference between these three things but most people do not care and i wouldn't worry about it too much okay so now let's take a look at the database command line interface which allows you to pragmatically interact with the adobe's api via entering single or multiline commands into a shell and then here i say or terminal but really it's just the shell okay so uh here is an example of one so we're trying to describe uh ec2 instances and then we're getting the output because we asked to have it back in this table like view so the abcli is a python executable program so python is required to install the awcli the awcli can be installed on windows mac linux unix the name of the cli program is aws you'll notice that up here in the top left corner there's a lot more to this but this is all we need for now okay hey this is andrew brown from exam pro and we are taking a look at the abyss cli and the easiest way to get started with this is actually via the cloud shell so you'll notice this little icon here in the top right corner that is cloud shell and it's going to allow us to um uh pragmatically do things without having to set up our own environments so if i just click that there okay uh and i say do not show again close and by the way if you don't see cloud shell here it could be your region so like if i go to canada central it doesn't have it there and so if i was to search cloud shell here okay it's going to say it's only supported in those regions so that's a bit annoying but once cloud shell loads it already has our credentials loaded within our account and so this is going to save us a lot of time in terms of you know trying to get set up with the exception that you have to wait for this environment to create so it takes a little bit of time but it's not that bad um and while that is waiting what i'll do is show you actually how you'd install the cli yourself so if we typed in about cli install all right and we went here the way you install i believe it's a python library but if we went to version 2 and we just said linux you go down here they'll have instructions so you just curl it unzip it and do that um so you know it's if it's this and then once it's installed you'll have the 8 of cli commands this is still going so you know maybe i can show you what it would be like to install the cli by hand so if we wanted to do that one easy way to do this is if we just go to github it doesn't matter what repository i'm just looking for anything here and if i open up git pods so if we go on the top here and type in gitpod.com maybe that i just want to see whoops maybe it's get pods like that oh get pod you're not giving me oh you know what it's dot io that's why okay so if we go back here sorry and we type in dot io what this will do is launch me a temporary environment and so this is outside of aws so i'd actually have to install the cli so this would be a great opportunity to show you how to install the cli i'm just doing it this way because git pod is free to use and um you know it's going to set up an environment and how let us simulate installing the cli so here is the cli here i'm going to see if i can bump up the font let's make the font as large as we can go light or dark dark sounds good to me and so if we type in aws and give it a moment we can see that we have uh the command here so if i say abs s3 ls whoops that should be able to list things out in a bucket so this is what's currently in the bucket if you're wondering how do i know what these commands are i can just type in able cli commands okay and we go here and we go to the cli ref reference then we have um anything we want here right so we go down here and i just want to see what's running in s3 and i go here and i scroll on down it's going to show me commands like copy move remove sync uh mbrb list right and if you're looking for a particular command you go down say okay i'll look at ls here and it will explain to me all the little options that we can do with it and then it will always give me examples right so i can see examples like that so if i wanted to move something into an s3 bucket so let's say i want to create a new s3 bucket um we'll type in aws s3 and just hit enter and it should tell us um the sub commands maybe if i do like help like this and if we scroll on down so i guess it just pulls up documentation let's open it we give us like a tiny summary okay so what we can do here because i want to create a bucket type in like buckets if you don't know something you just go about s3 cli create bucket we'll go here and then what i do is i always just go to examples here so we have aws s3 api create bucket and i know it's unusual there's an s3 and there's an s3 api i don't know why that is but it's always been that way and i just don't question it anymore and so here i can go ahead and create a new bucket so i'll just go ahead and paste that command in i do want to change it up a bit here because this name could be that has to be unique so just to make sure i get what i want i'm putting random numbers in here we're going to choose the region as us east one if i wanted to do other things here i could scroll up and look at some flags here so uh it looks all fine to me so i think i'll go back here and just hit paste okay and so it created that bucket for me if i go over to s3 and we'll wait here a moment we can see that bucket now exists if i wanted to place something in that bucket what i can do is just like touch a file so i'll just say touch touches a linux command to make just an empty file so we'll say hello.txt and then it would be a bus s3 um it would be sp to copy it and i'm going to give it the local path hello dot txt and then i need to give it the bucket address so it'd be s3 colon forward slash forward slash the bucket name so we named it this i'm not even going to try to type that in by hand because it's too hard and then i want to say where i want to put this file so i'm going to say hello.txt and if i'm right that should work as expected and so it says i uploaded that file i make my way back over to s3 i refresh there is the file if i want to copy this file back locally all i have to do i'm just going to remove i'm going to delete the original hello txt file ls to show you that there's nothing there and what i need to do oops is just revert this so instead of saying the address here we can go and type in hello.txt and if i do ls there's the file if you don't know what the address is of the bucket um a lot of times you can go here and find it so it should be because they're always changing this ui on me but we'll go to properties here and there that's the iron uh usually a good way to find it is if you go into an actual object so if you go here it will give you the full url so i could have grabbed that and i could have just pasted that in there um but you know you learn after time it's not hard to remember this s3 colon forward slash forward slash the unique name i do want to show you how to install it by hand so here i'm in get pods i'm not sure how i can change this to a dark theme because i really don't like this on my eyes we'll go down below here to color theme and we'll say get dark there we go and so this is a temporary workspace so when i close it it'll be gone so i'll be totally fine and so i'm going to type in abs to see that it's not installed we're going to go over here this runs linux by default so i already know that i'm going to use linux we want to use version 2 here so for the latest version use this command for a specific version no we just want the generic one so i'm going to go ahead and copy this whoops yes allow we'll paste that in we'll hit enter okay then we'll take the next command paste that in hit enter we'll go take the next command here we'll hit enter you can now run uh aws so we type aws and there's a command so uh the only thing is that if we do a bus s3 ls it's not going to work because we don't have any credentials set so we'll give it a moment to think so it says unable to locate credentials you can configure credentials by running it was configured so we type in ito's configure and by the way if this font is too small i believe i can bump it up like this not a great way to do it but it works and so it says databus access key id so what we can do is go over to iam and what i'm looking for is my particular user over here and if you remember when we first created our account it generated out access key so i go to security credentials and so we have a key here but i need the secret so this key is useless to me so i'm going to go ahead and deactivate it just because i don't even want this key and i'm going to create myself a new key so i'm going to have an access id and secret whenever you generate these out never ever ever ever ever show anyone what these are these are your yours and yours alone okay so this is cloud shell we're fine we're just gonna close that for now and i'm gonna go back over to get pods here and hit enter so that's the id i'm gonna go grab the secret hit enter paste and i want it to go to us east 1 to save myself some trouble you can change the output from json to tables i'm going to leave it as the default here and so now if i type a bus s3 ls i get a list and so if i want to grab that file there and grab that s3 uri and we type in aws s3 api or sorry it's just ls sorry or sorry cp and we're going to paste that link in and we're going to say hello.txt and i must have done the command wrong it's because we're missing s3 here i just hit up on the keyboard to get that command back and so i type in ls for list and i mean i have some other code here so you know again any repo you want on github it doesn't really matter but you'll see there is that file probably shouldn't use this one because it makes a bit of a mess um but yeah it's pretty straightforward just to one thing to show you is where those credentials are stored so by default they're going to be stored in um it's going to be in the hidden directory in your root or your home directory called above stock credentials so if i just do like ls here you can see there's a config file and a credentials file cat lets me print out the contents of that file so i go here and it's saying the default region is usc 1. this is a tombl file even though it doesn't have a dot tom along the end of it i just know by looking at it that's what it is config lets you set defaults that are going to apply to all of your credentials and then within the credential file here is the actual credentials so if you wanted to just set them you could go in here and just set them in here you can also set multiple credentials so if i go here and i'm going to open up and buy because i'm not sure how to open it up here in the main one but if you wanted multiple accounts you would do like exam pro and then you just repeat these with different keys right and then when you wanted to use a cli command actually i'm going to go back here for a second okay and if you want to um and by the way i'm using vi if you never use vim it's it's a bit tricky to use uh you might want to use nano instead if you're if you're kind of new to this because this will use like regular key key cuts and then down below it shows you what it is so this is like control x or alt x alt text no control x there we go um but anyway so if i go into this file and i delete the original one right and now i try to do um this command here even though we already have that file it should either hang or complain i could just kill that by doing control c if i do a bus s3 ls just notice that it's hanging so unable to locate credentials because there's no default one but if i go and i put profile and i say exam pro all right it'll now use that profile so that's the way we do it but hopefully that gives you kind of a crash course into the cli so yeah there you go okay so i'm just going to go ahead and close these off you can delete this bucket if you don't want it it's probably a good idea to delete this here and i'm just going to say permanently delete okay very very good okay close that off and yeah that's the introduction to the cli so yeah there you go hey this is andrew brown from exam pro and we are taking a look at software development kits uh so a software development kit or sdk is a collection of software development tools and one installable package so you can use the aws sdk to programmatically create modify delete or interact with aws resources so the innovas sdk is offered in a variety of programming languages so we have java python node.js ruby go.net php javascript c plus and so here would be an example of some ruby code where we are creating ourselves um an s3 bucket so we're just uploading a file there okay okay so now what i'm going to do is show you how to use the abyss sdk and so uh to do that uh we're going to need some kind of ide a a basically code editor and so we had looked at get pods which is a third party service and that's fine but let's take a look at cloud9 because that is built into aws so if i just type in cloud9 here and go over to ide i'm going to launch myself a new environment so i'll hit create i'm going to say my sdk environment env if you if you have our timetable environment like me and we have some options so create an ec2 instance for direct access create it via systems manager run a remote with ssh i'm going to leave it as the default then we have the option to choose what size i want to leave it on t2 micro because that is the free tier then we're going to scroll on down we have amazon x2 linux ami i'm going to stick with uh amazon linux 2 and we can have it turn off after 30 minutes a great option for us here we'll go ahead and hit next and we'll hit create environment and so we're going to have to wait a little bit for this to launch it'll take a few minutes as that is going let's go to google type in inbus sdk to get to the main page and so the idea here is that there are a bunch of different languages you can use c plus plus go java javascript.net node.js php python and ruby uh and so i'm a really big fan of ruby i've been using ruby since 2005 and so that's what we're going to do it in it's also really easy to use and it's a really great language so um you know down below it's just showing you that there's all these different things if we go down to the sdk here and we click on ruby we'll we have examples where you have the developer guide the api reference and so this tells you how to get started even here it's saying like hey go get started with cloud nine which is great as well i suppose um and so here might show you how to install it um and when we open up the api references this is what it looks like so a lot of times when i want to do something i know it's like i want to do something with s3 so i scroll on down here and i look for s3 right and then i just kind of like uh scroll around and look you know what i mean sometimes you have to expand it go into the client every api is slightly different so you do have to kind of figure out how to navigate that i'm actually under s3 right now so i'm looking for the client and i just know this from memory that this is where it is so first you create yourself a client and then you can do api operations so if i wanted to like list buckets i just searched the word list and i just scroll on down and there it is i click into that and i have an example of how to list a bucket so i'm going to go back to cloud9 and it is ready and it started in dark mode if yours is not in dark mode which really honestly why wouldn't you want dark mode if we go up to i think it's like file where is it uh preferences here gotta click the cloud9 option and i'm just seeing if it like remembers my settings i really like two two soft tabs here but uh there should be something for themes down below and so um that doesn't seem like that's it it used to be like a oh here it is if you go here and just choose like whatever you want i'm on jet dark here and so if it's on classic light or something you don't like you can fix that there but i'm just going to go here and just fiddle with my settings because i really like to use vim keys i don't recommend this if you are to change this if you are not a programmer but i'm just going to change it so that i can type here efficiently so i'm just looking for the option here and they moved it on me where did they move it it'd probably be like key bindings ah bin mode there we go again don't do that this is just for me so i can move around in a different way so what i want to do and by the way it looks like this default screen we could have just changed it here i just clicked through all that for nothing was here the entire time but what we need is we need to make sure that we have our credentials so if you type in aws s3 ls that's like my sanity check that i always like to do to make sure i have credentials notice that we didn't have to set up any credentials it was already on this machine which was really nice and so i'm going to create a new file here and it's okay if you don't know anything about ruby we're just going to have fun here and just follow along so i'm going to do example.rb i'm going to make sure ruby's installed by doing ruby hyphen v so it is installed which is great uh you need a gem file so say new gem file here and if we go back to the installation guide we need the gem sdk here actually i'm going to look at how to generate a gemfile gem file because there's some stuff that goes to the top of those files like this here i think we just need this line here so i'm just going to grab that whoops paste that in allow good and i you can do gem aws sdk that will install everything but uh we only want to work with s3 and so this is going to vary based on each language but i know that if we type in s3 we'll just get s3 and that's all we really need and so once we have that what we'll need to do is use a bundle install so we're going to make sure we're in the correct directory i'm going to type in ls down below notice the gem file is there and by the way if the fonts are too small i should probably bump those up let's see how we can do that uh editor size font user settings good luck trying to find today um project no you think it'd have to be under user settings right ah here it is okay so this is for probably the editor so we'll go to 18 here co code editor here i'm trying to find the one for the terminal probably over here there we go much easier okay so notice we have example.rb and gemfile so we're in the correct directory make sure i save that i'm going to type in bundle install and that's going to install the gems give it a moment there it's going to fetch notice that it installed the aws sdk s3 and everything that it was dependent on and so now if we go over to our example.rb file really when you're coding for the cloud you can pretty much copy paste everything so over here we found this code here for s3 list buckets and so i'm going to go ahead and paste that on in okay and i know it looks really complicated but we can quickly simplify this so i know that this is just the output so i don't need that okay and in ruby you don't need parentheses or curlies if uh if you don't have any things there and so all i need to do is define a client so if i click uh if i go to the top here of this file i think we're in the client right now all the way the top all the way the top here that's what we need okay and so i'm going to paste that in now we can set the region here so i'm going to say us east one right and then you'd have your credentials because the credentials are on the machine in the credentials file they're going to auto load here i believe so i don't think i need to set them so i'm just going to take that out here for a second okay and i can do this if i want this is just slightly different syntax it might be easier to read if i do it this way for you okay and i don't need double client there so we have the client i like to name this like s3 so i know what it is and i put puts for the response i'm gonna do inspect and so puts is like print okay and so now if i type in bundle exect let's just make sure that it's in the context of our bundler file ruby example.rb um we have a syntax error on this line here unexpected thing here oh it's because of this it's because i commented it out so i'm just going to do curly parentheses comment out here okay actually to make it a bit easier i'm just going to bring this down like this okay and we'll paste that there okay and we'll try this again initialize constants a to bus oh yeah we have to require it so we have to require abs sdk s3 i think we'll hit up and uh we got a struck back so it is working we are getting an object back if we want to play around with this a bit more i'm just going to install another gem called pry pry allows us to um inspect code so we're going to do bundle install and i'm going to go back to ruby here i'm going to put a binding pry in here and then if i hit up and i do bundle exec ruby example.rb um i installed it right bundle install yes undefined method pry oh because i have to require it again bad habit here okay we'll hit up and so now i have an interactive shell and i can kind of analyze that object so we have a response if i type in rsp here i have the structure object i can type in buckets here okay and it's showing me a bucket i can give it get its name um oh i think it's an array so i think i'd say like i'd say like zero here or i could say first this is just how the ruby language works we say name i get the name creation date okay so you get the idea whatever you want to do you know you search for it you just say i want to delete a bucket i want to create a bucket right and you look for it so i say create bucket here i click on this and i can see the options and they are always really good about giving me an example and then down below they always tell you all the parameters that you have there so that's how the sdk works uh but yeah the credentials were soft loaded here but you could easily provide them yourself i should just show you that before anything else just because there's some variations there and i'm just trying to look for it because it is separate code so you could do this this is one way of doing it so you could do it separate from the code so if you only wanted to configure it once right because you could you could have a lot of clients you wouldn't want to keep on like for each client you wouldn't want to put region in every time so i could take this and put this right here okay and this is the file here where we have the credentials so this would be our um our access key and our id and so you never want to put your code directly just in here so if i open up if you go cat you would never want to do this but i'm just going to show as an example here credentials oops i got to get out of this exit address credentials oh did they not even show it on this machine which would be smart we wouldn't really want to see our credentials here uh hit up say ls oh no it's there okay cat whoops cru credentials there it is okay so you know if we look here we can see that there are credentials set it's a little bit different we have this like session token i guess it's to make sure that this expires over time but if i was to take these okay and i was just to paste them in here that's one way you would do it you never ever want to do this ever ever ever ever you never want to do this because you'll end up committing that to your code so this is really dirty to do so i don't ever recommend to do it if you wanted to have this applied to everything you could put it up here and so now when we call the clients we don't have to do it um of course if the they're loaded on the machine you don't have to do it the other thing is like if you if you want you could load them in via environment variables that's usually what you want to do so you say a bus access key right and then you say environment databus access secret and so you'd set those by doing i think it's like an export environment variables set in linux you think i know after like 15 years of doing this but i never remember so you type in export so you go down into whoops here you type in export and you just say something like i'm going to show an example to see if it works so i'm going to say hello world okay and if i do hello like that echo see it prints it out so that's how you would set it you'd set those there's but there's actually very specific ones that aws uses for the api and it's these ones here so you always want to use those okay so you put that in there and then there but of course you know like if they're already set in your machine you don't have to even specify those because it would auto load those environment variables i don't think they're set right now if we type in echo just take a look here is are they going to get auto loaded here no so but anyway so we could go here just as an example and well actually they just show them right here so you see your access key but we go and we type in export and i'm going to paste the key in there and i'm going to go to the front of it we're going to type a bus access key id equals enter and so now if i did echo on this aws access key id okay shows up but i just want to show you how it can kind of vary and those conditions around it so yeah that is the abuse sdk um and yeah a lot of times you're just copying pasting code and just kind of tweaking it you're not really writing real programming okay so hopefully that is less intimidating so i'm just going to close these off and i want to close down this cloud9 environment um i might have to reopen this up in another tab and go to the management console here and then go over to cloud9 and just close this tab and then i'll go ahead and delete this environment oops i'll just type delete here even if you didn't it would turn off after 30 minutes and you have that free tier so it's not that big of a deal it's up to you whether you want to use cloud9 or git pods cloud9 is really good because it allows you to it allows you to uh use it runs on a virtual machine right so you have a a container runtime there and so it's very easy to run containers on it um whereas in like i've had some issues with git pods but um yeah those are the two okay well let's take a look at adam's cloud shell which is a browserbased shell built into the database management console and so cloud shell is scoped per region it has the same credentials as the logged in user and it's a free service so this is what it looks like and the great thing about this is that you know if you have a hard time setting up your own shell or terminal on your computer or maybe you just don't have access or privilege to do so it's just great that abuse makes this uh available to you and so what you can do is click the shell icon up at the top and that will expand this here some things to note about cloud shell is that it has some preinstalled tools so it has the cli python node.js kit make pip pseudo tar tmux vmwget vim and more it includes one gigabyte of storage free per aws region it will save your files in a home directory available for future sessions for the same in this region and it can support more than a single shell environment so it has bash powershell and zsh um and so enemies cloud shell is available in select regions so when i was in my canada region i was like where's the little shell icon but i realized it's limited for some areas okay hey this is andrew brown from exam pro and we're taking a look at infrastructure as code also known as iac and this allows you to write a configuration script to automate creating updating or destroying your cloud infrastructure the way you can think of isc it's a blueprint of your infrastructure and it allows you to easily share version or inventory your cloud infrastructure so aws has two different offerings for iac the first is cloud formation uh commonly abbreviated to cfn and this is a declarative iec tool and then you have abs cloud development kit commonly known as cdk which is an imperative iac tool so let's just talk about the difference between declarative and imperative and then we'll look at these tools a little bit closer uh each okay so declarative means what you see is what you get it's explicit it's more verbose but there's zero chance of misconfiguration unless the file's so big that you're missing something uh commonly declarative files are written in things like json yaml xml so for cloud formation it's just json and yaml and so that's that side there so for imperative you say what you want and the rest is filled in so it's implicit uh it's less verbose you could end up with some misconfiguration that's totally possible uh but it does more than declarative and you get to use your favorite programming language maybe python javascript actually cdk does not support ruby right now but i just have that in there just as a general description of what imperative is okay all right so just a quick look at cloudformation so cloudformation allows you to write infrastructure as code as either json or yaml the reason why was aws started with json and then everybody got sick of writing json and so they introduced yaml which is a lot more concise which you see on the right hand side so cloud formation is simple but it can lead to large files or is limited in some regards to creating dynamic or repeatable infrastructure compared to cdk a confirmation can be easier for devops engineers who do not have a background in web programming languages a lot of times they just know scripting and this basically is scripting since cdk generates out cloudformation it's still important to be able to read and understand cloud information in order to debug iac stacks knowing cloudformation is kind of a cloud essential when you go into the other tiers of aws like solutions architect associate professional or any of the associates you need to know cloud information inside and out okay okay so what i want to do now is introduce you to infrastructure as code and so we're going to take a look at cloud formation and so we were just using cloud9 for the sdk so we're going to go back and create ourselves a new cloud9 environment because we do have to write some code so i'll go ahead and hit create here and i'm going to just say uh cfn that's sort for cloudformation example and we'll hit next step and we'll create ourselves a new environment t2 micro amazon x2 is totally fine we'll hit next it'll delete after 30 minutes we'll be fine we're within the free tier we're going to give this a moment to load up and remember you can set your theme your your keyboard mode whatever you want as that loads and as that's going we're going to look up cloud formation and so cloud formation is very intimidating at first but once you get through the motions of it it's not too bad um so we'll go to the user guide here as we always do if you go to getting started it's going to just tell us some things it's going to read about yaml files um i don't think i really need to read much about this here so i think we'll just go start looking up some codes so something that might be interesting to launch is an ec2 instance cloudformation so that's what i'll do is i'll type in what i want so an ec2 instance and i'll just start pasting in code so if we scroll on down below here i'm going to go to examples because i want a small example here this is something that i might want to do and we're going to give that a moment here it's almost done you can do a database come on as that is going i'm going to open a new tab i'm going to make my way over to cloudformation okay and you can see i have some older stacks here notice cloud9 when we create an environment actually creates a cloudformation stack which is kind of interesting um but if we go here we can create a stack and we can create a file and upload it here so okay this is good i'm going to go ahead and make a new file we're going to call it template dot yaml just so you know yaml can be yml or yamml there's a big debate as to which one you use um i think that adabus likes it when you use the full version so i just stick with yaml i'm going to double click into that and so in the cc2 example i'm just going to copy this okay and i'm going to paste this in here and i'm going to type in resources oops capital okay so that's a resource i want to create um when you create cloud formation you always have a template version so i just need a basic example here at the top i guess that's a simple one is like a hello world bucket maybe we should do a bucket because it'll be a lot easier we don't have to make our lives super hard here okay um but what i'm looking for is the version because that's the first thing that you specify i'm just trying to find it within an example here oh for frick's eggs cloudformation version so they don't have the format version it's going to complain there it is okay so we'll copy that we'll go back over here we'll paste that in there it might be fun to do like an output here so i'm gonna do like an output outputs and uh maybe instead of doing this we'll type in a bus s3 confirmation because what i'm looking for is what we can set as output so we'll say return values here um maybe we just want returns the domain name so we'll just say uh value ref that that's going to get the reference for it and we have to say hello bucket uh type string i'll say outputs confirmation example and even though i've written tons of cloud information it's just like if you're not doing it on day in day out you start to forget what it is so here for outputs we need a logical id description value and export so um that is what i want so i'm going to go ahead and copy that back here this is just so that when we run it we're going to be able to observe an output from the cloud formation file okay so the logical id is whatever we want so hello bucket domain it's funny because this is how you do do kind of that would be the format for terraform i was getting that mixed up so the domain of the bucket the value here is going to be ref hello bucket domain name that's the output export value to export uh can i get an example here else name oh you know what export is for uh cross stacks we don't need to do that okay so that's fine so what we'll do is set that and we'll take out our old one and so this should create us an s3 bucket so with cloudformation you can provide a template here by providing a url or you can upload a file directly so i'm just trying to decide here how i want to do this you can also use a sample file or create a template in the designer i'm going to go over to the designer because then we can just like paste in what we want so if i go over to yaml here and we go back over here i copy this i'm just going to paste this in here and we're going to hit the refresh button nobody ever uses the designer but this is just kind of an easy example for me to place this in here it's not really working maybe i go to template dude here refresh there we go so there's our bucket it's nice to have a little visualization and i believe this is going to work as expected so now that we have our designer template i think if we hit close what's this button say validate template probably good idea validating the template template contains errors unresolved resource dependency in the output block of the template hello domain bucket seems like it should be fine let's go oops let's go back over here that's what i did i said reference that value oh uh maybe it's get a trib okay it's get att sorry get a trib cloud formation i can't remember if there's an r on the end of it oh it's just att this is if you're trying to get a return intrinsic value so a reference is like what the default one is but every time we do like a logical name and attribute that's how we get that there so what i'm going to do here is just hit refresh and i'm going to validate that one more time now it's valid if i hover over this is it going to upload it create the stack we could save this save it but we can save it in s3 bucket so we'll say hello bucket and so now we have this url so i'm going to copy it honestly i never use this editor so it's kind of interesting i'm going to leave and we're probably going to hit create stack but i just find it a bit easier if we just kind of do it through this here so go back create the stack we're going to paste in the url we're going to say next and we're going to say my new stack and i didn't see what the name of the bucket was oh there's no name so it's going to randomize that's perfect so we'll go next we have a bunch of options here we'll hit next we'll give it a moment here i guess we have to review it create the stack and this is the part where we watch so it says create in progress and we wait and we hit refresh and we can see what's happening it's trying to create a bucket and if we go to resources this is this is a lot easier to track because you can see all the resources that are being created if you notice that when you use the cl uh when you're using the abs management calls in korean s3 bucket it's instantaneous but like with cloud formation there's a bit of delay because there's some communication going on board but here it is and notice if we go to our outputs this is the the value of the bucket domain name if we were to make it with uh selfhosting which is not what we're doing with it we could also have an export name which would be used for crossreferencing stacks which is not something we care to do but yeah that's how you create a stack that way um but you know we can also do it via the sdk here so what i can do um is look up what is the inves cli cloud formation because they have their own commands here if i go here there's a new one and there's an old one so if we go create stack yeah there's things like this like create stack update um so if we wanted to do it this way okay and i copied this here i'm just gonna put this in my readme here for a second uh so here what you do is you say my new stack and you can provide the template url or you could specify the local path here so we have like a template body so i'm gonna go ahead and grab that okay this would be like yaml and um i need to specify this file here so template.yaml and i'm just gonna go pwd here to get the full path okay and i'm going to just paste that in there oops okay i'm going to do ls okay so that gives us the full path of the file you can also specify the template url and so this should work as well if i take this and paste that on as a command it's unable to locate parameter file there's three three triple slashes there we'll just fix that there paste unable to load param file no such file of directory and there's a t missing okay be like don't be like me and make sure you don't have spell any mistakes okay i can type clear down here so i can see what i'm doing we'll hit enter whoops unable to load the parameter file notice file or directory home well i you didn't want the forward slash so another thing we can try to do i think it will take it relative so if i do this it should work i don't ever remember having to specify the entire path an error occurred while calling the crate stack my new stack name already exists if i go back over here give this a refresh oh that's what we named our stack the the one that we did so i'm going to say stack2 okay format unsupported structure when calling the create stack operation are you kidding me i do this all the time template body yaml file cloudformation unsupported structure take a look here oh you know what i think uh this one's out of date that's why so what we can do is go to our old stack here and we can actually see the template i can go ahead and copy this whoops and we can go ahead and paste that in there and then now what i can do so you know that's that's the reason why it wasn't working okay so we'll hit enter um unsupported structure it should be supported let's see if cloudformation can help us out um apparently there was very unhelpful error message formatting so try the validate template option i wonder if we could just do this maybe if that would help here i'm just heading up to try to run it again nope i guess we can try to validate it here it's like i'm not having much luck here today so we'll just say this here maybe it's not even loading that file where it is i so there's no errors i'm just going to make this one line okay created so for whatever reason i must have had a bug there and so sometimes putting on one line helps that out because i must have had an obvious mistake there and now we can see the stack is creating it's doing the exact same thing it's creating a different bucket though if we go over to our s3 here again you know you don't need to be able to do this yourself to pass the exam it's just so i'm just trying to show you like what it is so you kind of absorb any kind of knowledge about what's going on here notice down below it uses the stack name followed by uh the read the logical name of the resource there okay and what we'll do is wait for that to create once that's created we can go ahead and delete these stacks we could also use the aws cloud formation to say like delete stack but i don't want to bore you with that today and so we'll hit refresh here wait for those stacks to vanish okay those are gone uh what i'm going to do is kill this cloud9 environment if there's a way to do it from here i have never known how to do it go back to your dashboard well that's nice to know we'll go ahead and just delete this okay we'll close that tab and so now we are all in good shape and so that was our introduction to cloudformation okay let's take a look here at cdk so ctk allows you to use your favorite programming language to write infrastructure as code and technically that's not true because they don't have ruby and that's my favorite but anyway some of the languages include node.js typescript python java.net and so here's an example of typescript typescript was the first language that was introduced for cdk it's usually the most uptodate so not always does cdk reflect exactly what's in cloud formation but i think they're getting better at that okay so cdk is powered by cloudformation it generates outcloud formation templates so there is an intermediate step uh it does sometimes feel a bit slow so i don't really like that but you know it's up to you cdk has a large library of reusable cloud components called cdk constructs at constructs.dev this is kind of the concept of terraform modules it is really really useful uh and they're really well written and they can just reduce a lot of your effort there ct cdk comes with its own cli um and i didn't mention this before but cloud formation also has its own uh cli okay cdk pipelines uh are allow you to quickly set up ci cd pipelines for cdk projects that is a big pain point for cloud formation where you have to write a lot of code to do this whereas the cdk has that off the bat makes it really easy for you cdk also has a testing framework for unit and integration testing i think this might be only limited to typescript because i didn't see any for the rest of the languages but um you know i wasn't 100 sure there this one thing about cdk is that it can be easily confused with sdk because they both allow you to pragmatically work with aws uh using your favorite language but the key difference is that cdk ensures uh it opponents of your infrastructure so what that means that's such a hard word to say but what that means is that um you know if you use this cdk to say give me a virtual machine you'll always have a single virtual machine uh because it's trying to manage the state of the file whereas uh when you use sdk if you run it every time you'll end up with more and more servers uh and it's not really managing state so hopefully that is clear between the difference there okay so we looked at cloud formation but now let's take a look at cdk cloud formation or confirmation cloud development kit it's just like cloud formation but you use a programming language in order to implement your infrastructure as a code i don't use it very often i don't particularly like it but you know if you are a developer and you don't like writing cloud formation files and you want to have something that's more pragmatic you might be used to that um this i think should be deleting because we were deleting the last one here and notice how it's grayed out i can't select it so don't worry about that create a new one it will say example we'll hit next t2 micro ec2 instance amazon x2 you know the drill it's all fine here we'll go ahead and create ourselves a new environment we're going to let that spin up there and as that's going we're going to look up adabus cdk so it was cdk and we probably want to go to github for this okay because it is open source and so i want to go to getting started and i have used this before but i never can remember how to use it probably the easiest way to use this is by using typescript so here's an example initialize a project make directory cdk oh first we gotta install it right so give that a moment so this is node you know how we did like bundle install this is like the same thing but for uh typescript installer update the it was cdkcli from npm we recommend using this version etc etc so again we're just waiting for that to launch but as we wait for that it's very simple we're just going to install it create a directory go into that directory initialize the example here it's setting up an sqsq which is um that's quite a complex example but you can see it's code right and then we run cdk deploy and we'll deploy it and then hopefully we'll have that resource so again we're just waiting for cloud nine there we go so cloud nine is more or less ready uh terminal seems like it's still thinking and we have a javascript one which i do not care about there we go there's our environment we're going to make sure we have npm so we can type in npm great it says version 8.1.0 and so this is asking for 10. okay i don't know if this gives us like nvm installed mvm it does so what we can do is do mvm list that stands for node version manager ruby has one as well and so it's telling us what version we're on i want to update um looks like we have a pretty uh pretty new version but what i want is the latest version of oh but that's node version that's not necessarily npm so we'll do node version oh 17 okay we're well well in the uh range of the new stuff so what i'm going to do is scroll on down we're going to grab this link here or this code here hit enter and that's going to install the adabus cdk so it says file already exists oh so maybe it's already installed on the machine um cdk let's type in cdk because of course aws wants to make it very easy for us this software has not been tested with what was that warning with node 1701 you may encounter runtime issues great aws you're like the one that installed this stuff here so we get a bunch of the commands which is great and so what we'll do is follow their simple instructions we'll say hello cdk we will cd into this and now what we can do is run cdk init and this language here and so that's going to do a bunch of stuff creates tons of files it's going to vary based on what you're using like which language because cdk comes available in a variety of languages so if you type in aws cdk documentation here notice up here python java.net so i think it has more than just those three languages but um you know i wish it supported more like yeah i see here is csharp java but i really wish there was a ruby so we'll give this a moment here to get installed and i will see you back here when it is done okay okay uh it turns out i only had to wait like a second there but it says there's a newer version of the cdk you probably should install it but i just want to get going here so as long as i don't run into any issues i do not care um but anyway so looking at this and again i rarely ever look at this but i'm a developer so it's not too hard for me to figure out but under the lib this is our stack that we're creating and here is it is loading in sqs it's loading in sns and then the core library it's creating an sqsq and it's setting the visibility of that timeout it's also creating an sns topic so those are two resources that we expect to be created if we scroll on down to the getting started it just says cdk deploy so what we'll do is go ahead and hit enter and let that do whatever it wants to do and it is thinking there we go so here we have i am statement changes so it's saying this deployment will potentially make potential sensitive changes according to your current security approval options there is there may be security related changes not in this list do you want to deploy sure we'll hit y deploying creating cloud information change that so cdk is using cloudformation underneath it's not complicated and as that is going what we'll do is we'll make our way over to our aws amazon.com console and if we go over to cloudformation we'll see if we see anything yet so it's creating a stack here we can click into it we can go over to our events see that things are being created this is always confusing so i always go to resources to see what is individually being created and they're all done so we go over here and they exist so here it says that we have a queue called this right sometimes they have links you can link through it so notice here i can click on the topic and get to that resource in sns which is nice for sqs i'm just going to type in sqs enter and there it is okay so we don't really understand what those are we could delete the stack this way there's probably a cdk way to delete the stack so cdk destroy i assume that's what it is destroy okay so we'll type in cdk destroy given a moment we're going to say yes okay it's deleting in progress we can even go back here and double check still thinking again you know if we deleted these for real it would take like a second but you know sometimes they're just slow sometimes it's because a resource can get hung as well but uh i don't think anything is a problem so here we can see what the problem is not necessarily a problem but it's just the sqs is taking a long uh longer time to delete where the s subscription is a lot faster so i'll just see you back here in a moment okay okay so after a short little wait there it finally finished uh i just kept on hitting refresh until i saw it deleted and so it's out of there and so we'll get rid of our cloud9 environment since we are done with it so type in cloud9 up at the top and we'll go ahead and delete and we will go ahead and delete this here thank you and we will go back to our aws amazon.console here just so we can get our bearings straight here and there we go all right let's take a look here at the aws toolkit for vs code so aws toolkit is an open source plugin for vs code to create debug deploy it was resources since vs code is such a popular editor these days i use vim but it's very popular um i figured i should make sure you're aware of this um plugin so it can do four things you get the abyss explorer this allows you to explore a wide range of database resources linked to your aws account uh and sometimes you can view them sometimes you can delete them it's going to vary per service and what's available there then you have the aws cdk explorer this allows you to explore your stacks defined by cdk then you have amazon elastic container service ecs this provides intellisense for ecs task definition files intellisense means that when you type and you you'll get like autocompletion but you'll also get a description as to what it is that you're typing out then there is serverless applications and this is pretty much the main reason to have database toolkit it allows you to create debug deploy service applications via sam and cfn and so there you can see the command palette and you can kind of access stuff there okay let's take a look here at access keys so an access key is a key and secret required to have pragmatic access to database resources when interacting with the awps api outside of the aws management console so uh access key is commonly referred to as aws credentials so if someone says database credentials so you generally are talking about the access key not necessarily your username and password to log in so a user must be granted access to use access key so when you're creating a user you can just check box access key um you can always do this after the fact but it's good to do that as you're creating the user and then you can generate an access key and secret so you should never share your access keys with anyone they are yours if you give them to someone else it's like giving them the keys to your house it's dangerous never commit access keys to a code base because that is a good place for it to get leaked at some point you can have two active keys at any given time you can deactivate access keys obviously delete them as well access keys have whatever access a user has to aims resources so you know you can do the database management console so can the key so access keys are to be stored in the aws.aws credentials file so um and if you're not familiar with linux this tilde here this actually represents your home folder so whether you're on windows or a linux that's going to be your home folder and then you have this period aws that means that it's a hidden folder but you can obviously access it and so in the it's just a tommel like file i think it's tommel um but i never uh 100 verified that it's tommle it looks just like tarmal and so what you'll have here is your uh default profile and so this is what you would use um or this is what any of your tools you use like the cli or anything else would automatically use if um if you did not specify a profile you can of course store multiple access keys and then give it a profile name um so if you are doing this for the first time you might just want to type in aws config and it'll prompt you and you'll just enter them in there as well i think that sets the default one when you're using the sdk you would rather probably use environment variables because this is the safest way to access them when you are writing code all right so there you go all right let's talk about access keys access keys are very important to your account um and so what we'll do is go to im if you are the root user you can go in and you can uh generate access keys for people um but uh generally you're doing it yourself for your own account so i go to users i'm going to click into mine here and we'll go over to security credentials and here you're going to notice access keys and one thing that is interesting is that you can only ever have two access keys at a time so hit create i'm just going to close that notice that the button is grayed out i can deactivate them if i feel that i haven't used them in a while and i can make them active again so i can bring them back into access or what i can do is make them inactive right and then i can delete them and so what i recommend right even if you do not want to pragmatically be using your account for anything you always want to fill up both these and the reason why and this is for security reasons is that if somebody wanted to come in and uh uh get into your account what they would do is they would try to find a user where they have access to them and then they would try to generate out a key so if both these keys are taken up so if you generate both these keys okay and this is the one you want to use you deactivate the other one okay we're not going to use that one and so now there's no way for them to fill up that other slot okay and so that is my strong recommendation to you but there's again only ever two here i'm just going to uh delete both of these so that when we want to uh do whatever next in a tutorial we'll go generate that out okay so go ahead and clear that out so hopefully that is enough for you to understand what to do with these axis keys okay so i'm gonna go back here there you go let's take a look here at aws documentation which is a large collection of technical documentation on how to use aws services which we can find at doc.abs.amazon.com and so this is kind of like the landing page where you can see all the guides and api references if you expand them in there into ec2 and you click on the user guide you can see html in pdf format kindle and you'll notice there's a link to github and that's because all of these docs are open source and you can contribute to them if you choose to do so i've done so multiple times in the past it's quite fun so aws is very good about providing detailed information about every individ service and the basis of this course and any aws certification will derive mostly from uh the adabus documentation so i like to say that i'm not really coming up with new information i'm just taking what's in the docs and trying to make it more digestible and i think that's the thing is like the docs are really good you can read them end to end but they are very dense and so it can be a bit hard to figure out what you should read and what you should not um but they are a really great resource and you should spend some time in there okay so i just want to quickly show you the aws documentation like give you a bit of a tour of it so if we go to about.amazon.com and type in docs i'm sure you might have seen this through other tutorials but the idea is that you have basically documentation for basically any possible service that you want and a lot of times you'll click into it and what you'll get are these little boxes and they'll show you different guides and it's going to vary based on service but a lot of times there's a user guide there's an api reference those are the two that you'll see there maybe go to something simpler like s3 that might be a simple example yeah user guide api api reference and so all of these are on github right if you open these up the documentation is here if you find something you don't like you can submit issues and uh and correct things you can even submit your own examples i have um i have committed uh example code to the docs specifically for ai services so you might be looking examples that i implemented or even ruby examples since i really like to promote ruby on aws you can download it as a pdf or you can take it as html a lot of times you're going to the user guide and the way i build the courses here is i actually go through and i read these end to end so you know if you wanted to do that you want to be like me you can do that or you can just watch my courses and save yourself the trouble and not worry about everything that is here but generally the documentation is extremely extremely good there are some exceptions like amazon cognito where the content is good but it's just not well organized so i would say aws out of every other provider they have the most complete documentation they generally don't keep their examples or like tutorials within here it's usually pretty light they'll have some examples um but like they like to have items labs separately so you type if it's labs github right you go here and a lot of stuff is in here instead so you have a lot of great tutorials and examples over there okay um but yeah pretty much that's all there is to it is there consistency between documentations no they kind of vary um you know but uh it's all there is my point and they're always keeping up to date so that's all you need to know about the aws documentation hey this is andrew brown from exam pro and we are taking a look at the shared responsibility model which is a cloud security framework that defines the security obligations of the customer versus the cloud service provider in this case we're talking about aws and they have their own shared responsibility model it's this big ugly blob here and the thing is is that every single csp has their own variant on the model so they're generally all the same but some visualizations make it a little bit easier to understand or they kind of include a little bit more information at different parts of it and so just to get make sure that you have wellrounded knowledge i'm going to go beyond the aws's shared responsibility model and just show you some variants uh there's also variance not just per uh csp but also the type of cloud deployment model and sometimes these are also scoped based on a cloud service category like compute or machine learning and these can result in specialized share responsibility models so that's what we'll look at in this section okay all right so let's take a look at the adab shared responsibility model and so i've reworked the graphic because it is a bit hard to uh digest and so i'm hoping that this way will be a little bit easier for you i cannot include the in and of here just because we're limited for space but don't worry we'll follow that up with the next slide here so there are two people that are responsible or two organizations that are responsible the customer and aws and on investors side they're going to be responsible for anything that is physical so we're talking about hardware global infrastructure so the regions the availability zones the edge locations the physical security so think of all that hardware that's there those data centers um everything like that then there's also software the services that they're offering and so you know this extends to all their services but generally it breaks down to the four core and so we're talking about compute storage database and networking okay and when we say networking we're talking about like physically setting up the wires and also you know the software to set up the routing and all that kind of stuff there now looking at the customer side of it they're responsible for configuration of managed services or thirdparty software so the platforms they use so whether they choose to use a particular type of os the applications so if they want to use like ruby on rails uh iam so identity and access management so if you uh create a user and you grant them permissions if you give them things they're not supposed to have access to that's on you right then there's configuration of virtual infrastructure and systems so that would be choosing your os that would be the networking so there could be networking on the um the virtual machines themselves or we could be talking about cloud networking in this case then there are firewalls so we're talking about virtual firewalls again they could be on the virtual machine or it could be configuring like knuckles or security groups on aws then there's security configuration of data uh and so there is clientside data encryption so if you're moving something from s3 from your local machine to s3 you might need to encrypt that first before you send it over then there's server side encryption so that might be turning on serverside encryption within s3 or turning it encryption on your ebs volume then there's networking traffic protection so you know that's turning on vpc flow logs so you can monitor them turning on aws guard duties so that it can detect anomalies with your traffic or or activities within your aws account and then there's customer data so that's the data that you upload on the behalf of your customers or yourself and what you decide to um you know like what levels of sensitivity that you want to lock it down do you want to use amazon macy to see if there's any public facing uh personally identifiable information that's up to you so there's a lot here and honestly it's a lot easier than you think um instead of thinking about this big diagram what i do is i break it down into this and so we have the in and the oven that's what i said i could not fit on the previous slide there but the idea is customers are responsible for the security in the cloud so that's your data and configuration so if it's data that's residing on there or is this something you can configure you are responsible for it on the adaba side they are responsible for the security of the cloud so if it's anything physical or hardware the operation of managed services or global infrastructure that's going to be on them and this in and of thing is very important for the exam so you should absolutely know the difference between the two this is kind of an aws concept i don't see any other cloud service provider talking about in and of uh so you definitely need to know it okay so one variant we might see for the uh shared responsibility model would be on the types of cloud computing this could also be applicable to the types of deployment models but we're doing types of cloud computing here and so we have the customer's responsibility and then the cloud service provider's responsibility so we're seeing onpremise infrastructure as a service platform as a service and software as a service and so when you are onprem you're basically responsible for everything apps data runtime middleware os virtualization servers storage networking basically everything and just by adopting the cloud you're almost cutting your responsibilities in half here so now the cloud service provider is going to be responsible for the physical networking uh the physical storage those physical servers and because they're offering virtual machines to you they're setting up a hypervisor on your behalf so virtualization is taken care for you and so um you know if you launch an ec2 instance you know you're going to have to choose the os that's why you're responsible whatever middleware there the run time so whatever kind of programs you install on it the data that resides on it and any kind of like major applications okay then we have platform as a service uh and so you know the cloud service provider is gonna take even more responsibility there so when we're talking about this we're thinking like abos elastic bean stock right so you know the you just choose what you want and it's all managed so you might say i want a ruby on a rail server but you're not saying what os you need um you're not saying exactly you might say what version of ruby you want but you don't have to manage it if it breaks or it might be managed updates and things like that the last thing here is like software as a service and this is something where the csp is responsible for everything so if you're thinking of a software as a service think of like microsoft word where you're just writing uh you know writing stuff in there and you know you you are responsible for where you might choose to store your data but the data is like still handled by the cloud service fighter because you know it's on the cloud so on their servers right so yeah hopefully that gives you kind of an idea across types of cloud computing responsibilities all right so what i want to do here is just shift the lens a bit and look at the shared responsibility model if we were just observing a subset of cloud services such as compute and so we're going to see infrastructure as a service platform as a service software as a service and now we have function as a service and so that's what i mean when we shift the lens we get new information and so you can just see that you really don't want to look at this from one perspective okay so starting at the top here we have bare metal uh and so abs's offering is called the ec2 bare metal instance and this is where you basically get the whole machine uh you can configure the entire machine with with the exception of the physical machine itself so as the customer you can install the host os the host os so the operating system that runs on the physical machine and then you can install your own hypervisor um and then awesome is going to be responsible for the rest the physical machine now normally the next step up would be dedicated but dedicated doesn't exactly give you more responsibility it gives you more assurance because it's a single tenant virtual machine and that's why i kind of left it out here but we'll see it in the next slide that it is kind of on the model and shares the same spot as uh ec2 but ec2 is a virtual machine and so um here the customer is responsible for the guest os so that means that you can choose what os you want whether it is ubuntu or debian or windows but that's not the actual os that is running on the physical machine and so you're not going to have control of that aws is going to take care of that then there's the container runtime so you know you you can install docker on this or any kind of container layer that you want um so that's another thing that you can do so aws is going to be responsible for the hypervisor uh the physical machine and the host os all right then looking at containers it just has more than one offering for containers but we'll just look at ecs here and so this is where you are going to have uh you don't you don't install the guest os right the guest os is already there for you what you are going to do is choose your configuration of containers you're going to deploy your containers you're going to determine where you need to access storage for your containers or attach storage to your containers and databus is going to be responsible for the guest os the there might not even be a guest os but they're the host os the guest os the hypervisor the container runtime and you're just responsible for your containers okay then going to the next level here we have platform as a service and so this one also is a little bit odd where it fits um because the thing is is that this could be using anything underneath it could be using containers it could be using virtual machines and so that's where it doesn't exactly fit well on a linear graph but let's just take a look at some things here so this is where you're just uploading your code you have some configuration of the environment you have options of deployment strategies the configuration of the associated services and then a bus is going to be responsible for the servers the os the networking the storage the security so it is taking on more responsibility than infrastructure as a service um uh whereas you know aws is just gonna be responsible for that so if it's a virtual machine that's being under uh under the use their business is going to be responsible for this customer stuff okay you're not if it's containers that abuse is going to be responsible for this but it just depends on how that platform as a service is set up actually the way elastic bean stock is set up is that you actually have access to all that infrastructure and you can fiddle with it and so in that case uh whereas like if you were to use heroku which is a a thirdparty provider you know they would take care of all this stuff up here um and so you would not have to worry about it but on aws you actually are responsible for uh the underlying infrastructure because you can you can configure it you can touch it so that's where you know again these do not fit perfectly and you can't look at platform as a service meaning that um you're not responsible for certain things it really comes down to the service offering okay then we're taking a look at software as a service so on aws this is going to be something like um amazon work docs which is i believe a competitor not a very popular competitor but a competitor to microsoft sharepoint and this is for content collaboration says the customer you're responsible for the contents of the document management of the files configuration of sharing access controls and the database is responsible for the servers the os networking the the storage the security and everything else so you know if you use the microsoft word doc and you type stuff in it you say where to save it that's what you're responsible for okay the last one here on the list is our uh functions here and so aws's offer is it was lambda and so as the customer all you're doing is you're uploading your code and database is going to take care of the rest so deployment container runtime networking storage security physical machine basically everything um and so you're really just left to develop okay so you know hopefully that gives you kind of an idea and again you know we could have thrown in a few other services like what we could not fit on this slide here was um it was fargate which is a serverless container as a function or sorry serverless serverless container as a service or container as a service so you know that has its own unique properties in the model as well okay so let's just have kind of a visualization on a linear graph here so we have the customer's responsibility on the lefthand side and it was a responsibility on the right and we'll look at our broad category so we got bare metal dedicated virtual machines containers and functions and so no matter which type of compute you're using you're always responsible for your code for containers you know if uh you know like uh the functions when you're using functions there are prebuilt containers so you'd say i want to use ruby and there's a ruby container and you don't have to configure it but obviously um you know when you're using container service you are configuring that container you are responsible for it for virtual machines you know you're responsible for the run time so you can install a container runtime on there or install a bunch of different packages like ruby and stuff like that the operating system you have control over in the virtual machines for the dedicated and we saw with bare metal you have both uh controls of the host os and the guest os and then only bare metal allows you to have control of the virtualization where you can install that hypervisor so hopefully that gives you an idea of compute and databases offering there and also kind of how there's a lot of little caveats when we're looking at the shared responsibility model okay all right so i have one more variant of the share responsibility model and this one is actually what is used by google so um we're going to apply to aws and uh see how it works so let's just kind of redefine shared responsibility model or just in a slightly different way so we fully understand it so the share responsibility model is a simple visualization that helps determine what the customer is responsible for and what the csp is responsible for related to aws and so across the top we have infrastructure service platform as a service software as a service but remember there's other ones out there like function as a service it's just not going to fit on here okay so and then along the side here we have content access policies usage deployment web application security identity operations access and authentication network security remember that's cloud networking security the guest os data and content audit logging now we have the actual traditional networking or physical networking storage and encryption and here we're probably talking about the physical storage hardened kernel ipc uh the boot the hardware and so then here we have our bars so we have the csp's responsibility and the customers responsibility so when we're looking at a sas software as a service uh the customer is gonna be responsible for the content remember like think of like a word processor you're writing the content the access policies like say i want to share this document with someone the usage like how you utilize it can you upgrade your plan things like that then next on our list here is platform as a service so generally uh you know platform is a services for developers to develop and deploy applications and so they will generally have more than one deploy strategy and uh you know there might be some cost saving measures to choose like uh you might have to pay additional for security uh or you or it's up to you to configure in a particular way or you might have to integrate it with other services uh and you know we saw that pass is not a perfect uh definition or fit because you know when we look at elastic bean stock if you have access to those resources and you can change them underneath then you might have more responsibility there than you think that you would okay the next one here is infrastructure as a service and so this is extending to identity so who's allowed to uh you know log into your aws account operations the things that they're allowed to do in the account access and authentication do they have to use mfa things like that network security obviously you can configure the security of your cloud infrastructure or cloud network um you know so you know do you isolate everything a single vpc how do you set up your security groups things like that we know with virtual machines you can set up the guest os there's data and content but remember that bare metal is part of the uh infrastructure service offering and so that's where we'd see hardware or not hardware but you'd have the host of the host os or virtualization and so this again is not a perfect representation but it generally works okay and then last and list there or just looking at what the aws is responsible for auto logging so of course database has cloudtrail which is for uh logging api um events but auto logging could be things that are internally happening with those physical servers then the networking the physical storage hardening the kernel airbus has i think what's called the nitro system where they have like a security chip that's uh installed on all their servers then it's the the boot os uh and then the hardware itself okay so just remember the customer is responsible for the data and configuration of access controls that reside in aws so if you can configure it or you can put data on it you're responsible for it okay the customer is responsible for the configuration of cloud services and granting access to users via permissions right so if you give one of your employees access to do it you know even if it's their fault it's your fault so remember that again the csp is generally responsible for the underlying infrastructure we say generally because you know there's edge cases like bare metal and coming back to aws is in the cloud and of the cloud so in the cloud so if you configure it or store it then you the customer are responsible for it and of the cloud if you cannot configure it then the csp is probably responsible for it okay hey this is andrew brown from exam pro and we are looking at the shared responsibility model from the perspective of architecture and if you're getting sick of share responsibility model don't worry i think this will be the last uh slide in this section but let's take a look here so uh we have uh less responsibility more responsible at the bottom so what we have down here is traditional or virtual machine architecture so global workforce is most familiar with this kind of architecture and there's lots of documentation frameworks and support so maybe this would be using elastic beanstalk with platform as a service or using ec2 instances alongside with auto scaling groups code deploy load balancers things like that the next level here is micro services or containers this is where you mix and match languages better utilization of resources so maybe you're using fargate which is serverless containers or elastic container service or elastic kubernetes service for containers on the top here we have serverless or commonly with functions as a service so there are no more servers you just worry about the data and the code right so literally just functions of code and so you could be using the amplify serverless framework or maybe able lambda for creating serverless architecture so there you go hey this is andrew brown from exam pro and we're looking computing services and before we jump into uh the entire suite of computing services database have let's just talk about ec2 for a moment which allows you to launch virtual machines so what is a virtual machine well a virtual machine or vm is an emulation of a physical computer using software server virtualization allows you to easily create copy resize or migrate your server multiple vms can run on the same physical server so you can share the cost with other customers so imagine if your server or computer was an executable file on your computer okay so that's the kind of way you want to think about it when we launch a vm we call it an instance and so ec2 is highly configurable server where you can choose the ami so the amazon machine image that affects options such as amount of cpus or vcpus virtual cpus amount of memory so ram the amount of network bandwidth the operating system so whether it's windows ubuntu amazon s2 the ability to attach multiple virtual hard drives for storage so elastic block store um and so the amazon machine image is a predefined configuration for a vm so just remember that and so ec2 is also considered the backbone of aws because the majority of services are using ec2 as the underlying servers whether it's s3 rds dynamodb or lambdas that is what it's using so um what i say also is just because when we talk about the aws network that is the backbone for global infrastructure and the networking at large and so ec2 is for the services okay hey this is andrew brown from exam pro so we just looked at what ec2 is but let's look at more of the broader services for computing and these are the more common ones that you'll come across there's definitely more than just what we're going to see on the single slide here so break this down with virtual machines containers and then serverless for virtual machines remember that's an emulation of a physical computer using software and ec2 is the main one but for our vm category we have amazon light sale this is a managed virtual server service it is the friendly version of ec2 virtual machines so when you need to launch a linux or windows server but you don't have much invoice knowledge you could launch a wordpress here and you could hook up your domain and stuff like that so this is a very good option for beginners we have containers so virtualizing an operating system or os to run multiple workloads on a single os instance so containers are generally used in microservice architecture when you divide your application into smaller applications that talk to each other so here we would have ecs elastic container service this is a container orchestration service that supports docker containers launches a cluster of servers on these two instances with docker installed so when you need docker as a service or you need to run containers we have elastic container registry ecr this is a repository of container images so in order to launch a container you need an image an image just means a safe copy a repository just means a storage that has version control we have ecs fargate or just fargate now people are kind of forgetting that it's it runs on ecs these days that's why i have it in there it is a service orchestration container service is the same as ecs accept you pay on demand per running container so with ecs you have to keep a ec2 server running even if you have no containers running so it is manages the underlying server so you don't have to scale or upgrade the ec2 server so there's the advantage over ecs okay then we have elastic kubernetes service eks this is a fully managed community service criminal or so kubernetes commonly abbreviated to k8 is an open source orchestration software that was created by google as generally the standard for managing microservices so when you need to run kubernetes as a service then we have serverless categories so when the underlying servers are managed by device you don't worry or configure servers soybes lambda is a servless function service you can run code without provisioning or managing servers you upload small pieces of code choose much uh how much memory how long you want the function to run is allowed to run before timing out and you are charged based on the runtime of the service function rounded to the nearest 100 milliseconds so there you go hey this is andrew brown from exam pro and what i want to do is just show you a variety of different computing services on aws so i'm going to try to launch them and we're not going to do anything with them i'm just going to simply launch them okay so the first i want to show you is ec2 and by the way we will go more in depth and ec2 later on in this course here but what i'm going to do is go ahead and launch the instance don't worry about all this stuff but just choose the amazon linux 2 so it's in the free tier all right we're going to choose an instance type of a t2 micro so that's part of the free tier it's going to be set as one all these options are fine i want you to go ahead and review and launch we're going to launch and i don't want to generate any key pair i'm going to proceed without a key pair i'm going to acknowledge that because i don't want it and that's all there is to launching an ec2 instance and so i can go here and view my instances and what you'll see is it's pending okay and usually it has like a little spinning icon maybe they've updated it since then so i go here it's hard to see because there's all these terminated ones but i don't need to do anything with it i just wanted to show you the actions that you'd have to do to launch it actually we'll leave it alone maybe we'll see it when it's launched the next one i want to show you is elastic container service um and wow this this is all let's go let's get the new experience please that's so old okay check box that on and we'll hit get started and we'll say create a cluster and we have some options here networking only ec2 linux plus networking uh for use with either aws fargate or external windows um this is if you're doing fargate which we're not doing right now fargate is part of elastic container service it used it well it used to be it is called ecs fargate but it was markets it as a separate service we'll go to next we'll say my ecs cluster um we can create an empty cluster but that would make it a fargate cluster which we don't want there's an ondemand server look it's m6i large if you're very afraid of a lot of spend here you don't have to do this you can just watch me do it and just learn well what i'm going to do is try to find something super cheap so i want a t2 micro or a t3 micro t2 micro is part of the free tier i don't know if we get to choose t2 anymore in here they might not let you there it is but you know t3 micro is great too i just whatever says it's free that's what i'm going to go for number of instances one the amazon linux version is fine i don't care about a key pair use existing vpc i don't want to have to make a new one select the existing ones okay let it create a new security group that's totally fine allow those to be fine create a new role that's fine create okay and so that's going to create ourselves a cluster i'm going to just make a new tab here let's just check on our ec2 instance and so if we look at our ec2 instance it is running okay great so it has a private ip address it has a public ip address all right there's not much we can do with it i can't even log into it because we didn't generate it out of key pair a lot of times you want to name these things so let's go here name it my server okay go back to our ecs instance and the cluster is ready so we'll go here and oh nice we got a new ui and so if we wanted to deploy something as a service or a task um we would need to create a template like a task definition file uh they don't have a new ui for this you're being redirected to the previous version console because this isn't available in the new experience yet of course it isn't so we can create a new task definition file that's what's used to run it it's basically like a docker file compose file whatever you want um we have fargate or ec2 we are doing ecs so we're going to have to do ec2 so we'll say my ecs task def file um task role optional i am role i don't need one network mode i don't care and then this is the idea is that because a container allows you to use up a particular amount of the thing we don't have to use all of the memory so we should look up what a t2 micro is because i don't even remember what size it is okay t2 micro aws so we go here we look at the instance types and we're gonna flip over to t2 and it says that it's one vcpu one gigabyte of memory so what i'll do one yeah one okay that's fine so what we want and this is in megabytes so we'll say 500 megabytes and um i don't know if we can do less than one but i'm going to do one here the task cpu must be an integer greater than or equal to 128 okay fine 128. oh i guess it's 1024 would utilize the whole thing so i could say 512 okay and this is where we would add our container so i don't do this every day so i don't remember how to do this we'll say my container and i need a repository here so i need like docker hub hello world okay i don't care what it is i just need a image that's simple and i'm looking for the address here um i'm hoping that's just this docker hub url so it'd be something like this right docker io probably docker io docker image docker hub url in ecs okay it goes to show how often i'm launching these things so repository url docker image so i think that what we're going to do here hmm i would really just like the url please reviews tags where is it where is it it's somewhere here right uh uh well let's just try it we'll go and we'll type in says image and tag so docker dot io hello world i really need an image id image url hello world docker hub they're not making my life easy here today anything i just want to see like a single example docker dot io docker io url examples ecs this is what it's like you know this is what you're going to be doing if you are um you know a cloud engineer you're going to be googling a lot and just trying to find examples here so here it says docker io the name the hostname okay so we'll just try it okay so i think that the the the name here is underscore and then it's hello world and that's what's throwing me off here right docker io just hold on here repository url and then there's the tag i don't know if like is the tag gonna be like latest view available tags latest okay so what i'll do here and that's the thing you have to have a lot of confidence too so hard limit soft limit do i have to set it do i have to set any of these things can i just go to the bottom and hit add looks like i can okay so we'll scroll on down create we create our task definition file which is fine we're going to go back to our cluster it's going to bring us back to the new experience we're going to click into this cluster holy smokes uh we're going to hit deploy and we're going to choose service that means it's going to continuously run task means that when it's done running it ends we're going to choose our family or version that's the task definition file there it's not compatible with the selected compute strategy my task file what if i just choose task take that okay so maybe some you have to like code it so that it continuously runs i don't care we don't need to run a service here the selected task definition is not compatible with the selected compute strategy okay let's see why can you double check if you're using fargate strategy instead of the ec2 uh blog design for the ec2 strategy so probably what it's suggesting is that the strategy file i made is not for the right one here task definitions go back over here well what's wrong with it taskroll none my container so what i'm going to do because i don't trust this i'm going to go ahead and delete this can i delete this how do i delete this oh boy actions deregister deregister we'll create a new one and so it was has tools like it was copilot cli to make this a lot easier because you can see this is very frustrating but i chose this so my task def requires compatibility of ec2 default 512 512 add container we're going to uh was it docker dot io underscore what's it called hello world i will just say hello world here and we'll just say 512 which is fine i don't care about any port mappings i'm just reading it carefully here to see what it wants we'll say 512 maybe because i didn't specify them it's complaining this looks fine we'll hit add okay constraints type this all looks fine so we'll try this again and so we now have our file let's see we can just run this task from here ec2 this is just another way to do it so we just choose the cluster this is actually a lot easier to do it this is old old old eh this is ugly and so now it launches so you know if you have trouble one way then just do it another way and sometimes it'll work here so i don't expect this task to really work in any particular way if it's pending that's fine if it fails it's fine if it's successful that's fine i don't care i just want to go through the motion so it was successful it ran and then it stopped i don't know if we could see like the output anywhere probably what it would do is it would log out something like into somewhere and so i don't know if like there's logs turned on for this if i go over to like cloud watch logs maybe i could see something a lot of these services will automatically create cloud watch logs so sometimes you can just go look at them there so we'll drop down we'll go to log groups here there is some stuff here um there's a couple that i created from before just go ahead delete those and so what i'm looking for is like ecs so no there's no logging happening here which is totally fine so that is ecs um for fargate it's pretty much the same the difference is that fargate is like it has to start up and run so it's a lot slower to watch okay and now let's go take a look at lambda okay so this is our serverless compute so go ahead and create ourselves a function uh we can start from a blueprint that doesn't sound too bad and i personally like ruby so no i'm not getting much here but we can do is look for something like hello do we have like a hello world there we go hello world and we'll click that we'll say my hello world uh it's going to create those permissions that's fine it's showing us the code it's very simple okay it's going to console log out these values not a very good hello world function doesn't even say hello world how can you call it a hello world function if it doesn't say hello world i don't understand so i'm going to go ahead and create this function usually doesn't take this long okay so uh here is our function here is our code notice that this is cloud9 okay and you can even move that over to cloud9 they didn't have this button here before that's kind of cool i hit test they used to have it up here but i guess they wanted to make it more obvious so they moved it down here which is nice so what i can do is hit this oops my test it's going to send a payload here to the actual function and it's going to tell us if it worked okay so can i run my test go over here to test it changed a bit so i guess i created there it succeeded so i have my logs okay so it's going to output those values there so there are the three values which basically is nothing maybe you were supposed to set those an environment variable but you can see you're just uploading uh some code right it's just a bit of code it's not like a full app or anything so we launched an ec2 container we did a a um sorry an ec2 instance a container we did a serverless function there's other things like eks but that is really really hard to set up okay because you'd have to use like kubernetes commands and stuff like that and my kubernetes knowledge is always very poor um i'm just taking a peek here to see if they've updated it so yeah you create the cluster but like deploying it is forget it i'm just trying to think of there's anything else i kind of want to show you um no those are the main three i would say so i'm pretty happy with that um what i'm gonna do is go and kill all these things so we're gonna go over to lambda okay and i'm going to go ahead and delete this as you saw ecs was the hardest and no matter how many times i've built things in ecs and i've deployed full things on ecs i can't remember i always have so much trouble with task definition files it's unbelievable we'll go over to our cluster here and ecs cluster up here make sure you're not in the fargate cluster i know i'm clicking really fast but there's just so many things to click and i'm going to click into this cluster we're going to hit edit because this is running an ec2 instance right i need to destroy it um it just took me back to the old one here i want to delete no i want to delete the cluster click back here where do i delete it up here here i can't checkbox anything uh how do i delete this do i have to delete the task first maybe so we'll go here i mean it's already stopped there's nothing to do edit uh account settings wow this is confusing okay how to delete ecs cluster you gotta be kidding me i have to actually look this up so open the ses console from navigation in the navigation choose clusters and the new turn off the turn off new ecs experience and choose the old console the delete cluster workflow is not supported in the ec ecs console are you serious then why why do you have it like why even let people use the new experience if that you don't have all the functionality there um oh i was gonna give it feedback but it didn't let me here's it says uh i need to delete an ecs cluster no okay so i'm here there's my big ugly cluster delete cluster okay so yeah it's a struggle okay like things are always changing on me but you just have to have confidence and if you've done it a few times you know that you can do it right um and that's one of the biggest hangups to cloud i would say so it's going to take a few minutes apparently to delete the cluster as that is going let's go over to ec2 i didn't close it i kept this tab open and uh there's our ec2 instance we can go ahead and terminate that instance terminate okay and if this says it's terminating then we're in good shape terminator shutting down that's fine and notice here that's the ecs instance just make sure you shut down the my server not the um the ecs instance because that's going to stop and so this has already terminated but if we go back here notice that it says that it's not done but clearly clearly has shut down okay so i'm going to wait here for a bit even though i know it's been deleted maybe it's deleting things like the auto scaling group so we go down below here right so that's probably what it's doing it's probably trying to destroy the auto scaling group but it doesn't show any here so it must have already destroyed it yeah so task services delete so i'll be back here in a bit but i know it's safe it's already deleted but i'll see you back here in a bit okay so i waited literally a second and it's now deleted so we deleted our lambda we deleted our oh did we delete our lambda good question now i'm not really worried about the lambda because i guess we did but i'm not really worried about it because um you know when it rests at idle it's not costing us anything where the ecs and the ec2 are backed by ec2 instances so we do have to shut those down okay and again remember you make sure you're in the correct region sometimes that gets flipped over and then you think those resources are gone but they're actually not they're just running in another region so there you go hey this is andrew brown from exam pro and we're taking a look at higher performance computing services on aws so before we do we've got to talk about the nitro system so this is a combination of dedicated hardware and lightweight hypervisor enabling faster innovation and enhanced security all new ec2 instant types use the nitro system and the nitrous system is designed by aws okay so this is made up of a few things we have nitro cards these are specialized cards for vpcs ebs instant storage and controller cards you have nitro security chips these are integrated into the motherboard protects hardware resources and we have the nitro hypervisor this is the lightweight hyper visor memory and cpu allocation bare metal like performance there's also nitro enclaves but that's a bit out of scope here but that has to do with like ec2 isolation okay then we have bare metal instances so you can launch ec2 instances that have no hypervisor so you can run workloads directly on the hardware for maximum performance and control we have the m5 the r5 ec2 instances that can run bare metal there's other ones i believe i've seen as well but you know if you are running bare metal you can just go investigate at the time of okay we have bottle rocket this is a linux based open source operating system that is purpose built by adabus for running containers on vms or bare metal hosts then let's just define what hbc is so it's a cluster of 100 of thousands of servers with fast connections between each of them with the purpose of boosting computing capacity so when you need a super computer to perform computational problems too large to run on a standard computer or computers or would take too long this is where you know hbc comes into play one solution here is abs parallel cluster which is an ada supported open source cluster management tool that makes it easy for you to deploy and manage high performance computing hpc clusters and aws so hopefully that gives you an idea of this stuff okay all right so let's take a look at hpc or high performance computing on aws so hpc is for uh running large complex simulations and deep learning workloads in the cloud with a complete suite of high performance computing product services gains insight faster and quickly move from idea to market blah blah blah it's for ml or very complex scientific computing stuff these run at least on c5 ends okay and the way it works is that you use this cli called p cluster variable's parallel compute or it was parallel cluster stuff and so let's see if we can get this installed very easily um so what i'm going to do is see how hard it is to install now i don't recommend you running this because i don't know what it's going to cost me and if i make a misconfiguration i don't want you to have that spent here but i don't think it's that dangerous so i'm going to go back over to usc 1 here i'm going to open up cloud shell and i'm going to give it a moment to load and so as that is loading let's take a look at how we would go ahead and install this so install the current parallel it is parallel i think we just copy that line okay and so we have to wait for environment to spin up alright so once it has spun up we will install it and then we will jump over to this tutorial here okay so we'll give this a moment and after waiting a little while here it looks like our shell is ready it looks like it's in bash um i'm just going to type in aws s3 ls that's a sanity check okay and it works that's great so go back over here and i'm going to go back up to install for linux and what i need is that single command where is it so i'm certain that we already have linux or python installed but i just want the command to install it we saw it a moment ago here i'm just going to back out until i can find it one more there it is so it's under oh it's this link here and that's what i talk about the documentations being tricky sometimes you have to click these uh headings here to find stuff so this is the first time installing it so we'll grab that usually you're supposed to create in virtual environments with python i don't care this is my cloud shell it doesn't matter to me so we're going to go ahead and download that and hopefully it is fast and it was super fast which was really nice and so what we'll do is go check out the p cluster version okay and that looks fine to me i'm going to go down below here to run our first job the returns the it gives outputs i don't think we need to configure it because we already have our cli so what i'm going to do is go ahead and create ourselves a new cluster um beginning cluster creation configuration file config not found so i guess we do have to configure this configure and it's asking what region do we want to be in um if i have usc 1 i would choose it for some reasons all the way for number 13 that is not a lucky number but i'm going to choose it anyway anyway no key pair found in us east 1 region please create one of the following so create an ec2 key pairs no options found for ec2 key pairs that's fine so what i'll do is go over here and we'll go over to ec2 and we will go over to key pairs key pairs key pairs key pairs we'll create ourselves a new one here so say hpc key pair or just my hpc so we know what it is for we have putty or pem we're going to do pem because we're on linux we'll create that and notice that it downloaded the pen down down here and we're going to need that for later um and so what i'll do is i'll type in p cluster here again configure we'll choose 13 we'll choose number one here allowed values for the scheduler i have no idea what these are uh let's choose the number one allowed values for the operating system amazon linux 2. i know what that is minimum cluster size one maximum cluster size two head notice instance oh t2 micro you can do that yeah let's do it i didn't know we could do that enter compute type uh t2 micro sure so i thought that we'd have to use a c5n but i guess apparently not automate vpn vpc creation yes of course network configuration so allowed values for the network configuration a head node in a public subnet and can and compute fleet in a private subnet a head node and compute that will do in the both just to make our lives easier i don't care first one sounds more secure of course and so oh it's creating cloud information sack wow this is easy i thought this was going to be super painful okay so we'll go over here we'll go take a look at what cloudformation's doing all right now i don't care if we actually run a task on here but it was just interesting to go through the process to see how hard it was and we will go look at what resources are being created so it's creating an internet gateway so it's literally creating a isolate vpc for it which is totally fine i guess it's creating a subnet it's creating a route table refresh here um i'm not sure how much it wants to create here it just looks like vpc that's all it's creating i thought maybe the ec2 instances would show up here but maybe it's going to launch that at on a need be basis okay so that's all created oh now it's doing a vpc gateway i think vpc gateways cost money let's go take a look here people say pricing yeah there's a transfer fee so just be careful about that you know again you just can just watch along here you don't have to do it default route depends on public so now it's creating ec2 route i don't know what an aws ec2 route is i've never seen that before sometimes what we can do is go into ec2 and then take a look on the left hand side you see anything in here we don't know what it is we just type in ec2 route cloud formation sometimes cloudformation is great for figuring out what a component is not all components are represented in the um um management console so specify route in the row table oh it's just a route okay and we'll go back here we'll refresh so that is done is the stack done created complete good we'll go back to our cloud shell it says you can edit your configuration file or simply do etc so now let's see if we can create the cluster i assume this would create ec2 instances so the job scheduler you are using is sge this is deprecated in future use parallel cluster well should have told me okay there is a new version of three zero one parallel available i don't understand because i just installed it right we'll go back to cloudformation we're just gonna probably create nested stacks which that's what i thought it would do nessa stacks means that it's reliant so there's one main one and then there's uh children stack so go here see what resources it's creating a whole bunch of stuff wow so many things that sqsq sns a network interface a dynamodb table yeah you probably don't want to run this you just want to watch me do it and then we go into here it's creating an ec2 volume so that's going to be ebs and then here we have a log group i don't know why they separated those out it seemed very necessary we are waiting on the elastic ip that always takes forever creating elastic ip root instance profile that is the item role for it that didn't take too long these these take a long time i never know why you create a role it's really easy but attaching an iron policy you're always waiting for those um so i'm gonna just stop it here i'll be back in a second because i don't want to have to make you watch me stare at the screen here okay all right so after a really really long wait um and it always takes some time there it finally created i'm not sure what it's made i mean we generally saw over here in the outputs but usually the cost that i'm worried about is whatever it's launching under ec2 it might not even have launched any servers here we're going to take a look here and see if there's anything so we have a master and a compute and they're t2 micro so seems pretty safe here this compute is not running yet so i'm assuming that this is like the machine that does the computing and maybe if you had multiple machines here like that would be the cluster like would manage multiple computes i'm not particularly sure but let's just keep going through the tutorial and see what we can do the next step is we need to get this pen key in our cloud shell here so this i don't know where this is but what i'm going to do is i'm going to move it to my desktop i'm doing this off screen by the way so i'm moving it to my desktop and then i'm just going to go and upload the file okay and there it is so we'll say open and we'll say upload and it's going to upload it here onto this machine and i believe this is on like uh i think this uses an efs instance like if you're wondering where the storage for cloud shell is if we go over here i think it's efs is it uh i don't know where it is okay maybe it's just uh maybe it's somewhere else okay i can't remember where it is but anyway um so now it's created the cluster can i hit enter here okay can i create a tab like if i quit this is it going to kill it it exited it which is i think it's fine i don't think it stopped running and so now if i do an ls there's my key and so we can go back to our instructions we just have too many tabs open here drag this all the way to the left here and so we can try to use our key here to log in so what i'm going to do is go here and we'll say my hpc pem and see if that works we'll say yes and permission denied it is required your private key is not accessible that's because we have to mod it um i never remember the command anymore because i rarely ssh into machines but if we go to connect and we go to ssh client it will tell us that we need to run chamod 400 okay so that's what we need to do is we need to do a chamat400 just wanted to grab that code there okay and now if we hit up we should ssh into the machine there we are we are in the instance we'll type it exit and so now we want to run our job on this machine and if we go back over to here i guess we can go create our first job so i'm just doing this in vi and i'm gonna paste that in yep and i don't want the first line oh okay that's perfect oh great right quit oh there's no file name hold on here so i need to name this file something so i'm going to say job.sh and we're going to paste that again here we'll say paste and i don't know if that's cut off yeah it is okay great is that one okay i don't trust that the first line is there so what i'm gonna do is go back to our tutorial here it's shebang forward slash bin forward slash bash uh this then that forward slash bin forward slash bash just double check it looks good to me we're going to quit that i'm just going to make sure that it is what we said it is so job.sh looks correct to me good and so we'll try to run our job here so i'm going to say q um job.sh ls and i guess it really depends on what we decided to use when we set up that thing i can't remember what we choose as our queue we do qstat oh you okay okay okay so i think the thing is like you see how we have sg i think that that's what we use to queue up jobs and so we have to have that installed probably so install configure sun grid engine sg install linux oh boy that looks like a lot of work so i don't think we need to do anything further here but as far as i understand the idea is that you're choosing some kind of way to manage these and so i'm not sure what q q sub is let's just go look at what that is what is q sub oh that is the sun grid engine okay so how do we install that um i'm just gonna see if we can install it so i'm gonna do i think this is using yum so if i do clear here clear yum install q sub let's see if i can do it sudo yum install qsum no package available amazon linux 2 q sub because that's probably what we're running in cloud shell q sub doesn't tell us how to install it that's great so that's probably what it is and so in order to use this we would have to install that sun whatever whatever and then we go through we do q sub it would cue it up um you could do q stat cat hello destroy it that's pretty much all we really need to know to understand this it would have been nice to queue up a job and see it work but you know we're getting kind of into a hairy territory here and i think that we fundamentally understand how this does work so what i'm going to do is i'm going to go here i'm going to remove the job.sh here and i want to destroy this cluster so i'm going to do p cluster commands to figure out what all the commands are and there's probably a delete command so we'll go back up here be cluster where is our credit so we'll say delete okay and so what that's going to do is just tear down all the stuff now so if we go over to cloudformation okay and it looks like it's destroying so yeah i'll see you here uh back in a bit when it's all destroyed okay all right so after a short little wait there it has destroyed it has been so long that i uh my connection vanished but just make sure if you did follow along for whatever reason uh you know make sure that the stuff is deleted and it looks like it did not destroy uh this so i'm going to go ahead and delete that that's just vpc stuff so i'm not too worried about it i know that's going to roll back no problem and so i'm going to consider this done so i'm going to make my way back to the management console close this stuff up and we are good to go for our next thing hey this is andrew brown from exam pro and we're taking a look at edge and hybrid computing services so what is edge computing when you push your computing workloads outside of your network to run close to the destination location so an example would be pushing computing to run on phones iot devices external servers not within your cloud network what is hybrid computing when you're able to run workloads on both your onpremise data center and the above vpc okay so we have a few services here starting with abus outposts this is a physical rack of servers that you can put into your data center invoice outputs allows you to use aws api and services uh such as ec2 right in your data center then we have abs wavelength this allows you to build and launch your applications in a telecom data center by doing this your applications will have ultra low latency since they will be pushed over the 5g network and be closest as possible to the end user so they've partnered with things like verizon vodafone business and a few others but those are the two noticeable ones okay we have vmware cloud on aws so this allows you to manage onpremise virtual machines using vmware within ec2 instances the data center must be using vmware for virtualization for this to work okay then we have abs local zones which are edged data centers located outside of the database region so you can use it as closer to the edge destination when you need faster computing storage databases in populated areas that are outside of aws region you could do this there's some other edge offerings on aws that aren't listed here like sagemaker has was called like neo stage maker unless you do edge computing with ml but i mean this is good enough okay all right so i wanted just to show an example of edge computing because we didn't cover it in our generic compute and so there's a variety of services that allow you to do edge computing like wavelength and so i've never actually launched wavelength before and i think that you have to request it so if i go over to support here again i've never done this before but i'm sure we can figure it out pretty easily i feel that if we create a case um maybe it's like service limit we type in wavelength here well nope not there so how do we get wavelength wavelength request so that's what i'm looking for here okay how do i use wavelength aws whoops and sometimes what i'll do is go to the docs here opt into wavelength zones before you specify wavelength zone for resource or service you must opt into it to opt in go to the aws console okay so we'll go to ec2 and then it's going to say use the region selector in the navigation bar to select the region which supports your wavelength so i know that there's stuff in uh us west because of las vegas right or not las vegas but los angeles right so if we go over here there's definitely that over there on the navigation pane of the ec2 dashboard under account attribute select zones okay do we see zones here zones oh ec2 dashboard zones let's go check here again on the navigation pane choose ec2 dashboard we are there right and under account attributes settings account attributes oh over here okay oh it's here zones and so there we have two zones and we see switch regions to make zones a different region okay so under zone groups turn on wavelengths zone groups okay nothing there so i'm just going to switch over to another one here oh maybe oregon maybe cs west 2. oh look at all the stuff we have here i've never seen these before okay so here is the wavelength one so that is the los angeles one we can go ahead and enable this before december the zone group i'm not sure what zone groups cost so wavelength zone pricing again you might just want to watch me do this because it might cost money and so you might not want to have to spend for that pricing provides mobile networks wavelengths are available across whatever learn about the data transfers enterprise about ec2 instances okay so what's the price we're going to here alright so what i'm going to suggest to you is don't do this but i'm going to do it and we're just going to see what the experience is like okay so i'm going to update my zone so now i have this one so we'll say enable i'm going to assume that it has to do with like data transfer costs okay and uh we're going to go over to ec2 and we're going to go over to instances here we're going to launch an instance and we're going to see if we have that available now i don't know if we're restricted to particular to particular uh instances i assume we can launch a linux machine it'd be really weird if we couldn't you know we'll go over to configuration and what we want to do is choose the zone so how do we do it so once it's turned on confirmation confirm it configure your network so create a vpc create a carrier gateway so you can connect your resources into the vpc to the telecommunication network holy smokes this is complicated but it's just kind of interesting to see like the process right you know it's not for our use case but uh carrier gateway right and as i do this i always check up all the costs here so i say carrier gateway pricing aws because maybe that's where the price is okay if you don't get a pricing page then usually that's hard to say logically isolated virtual networks again it's not telling me what um to use carrier you need to opt into at least one wavelength zone but i did right and sometimes what happens is that it just takes time for the optin to to go so go here manage the zone settings that was a lot easier way so we have one it's we're opted in right here okay and okay we'll go here again if that one didn't work um we can try so i guess these are all the regions denver things like that can i opt into this one optin it's not super exciting like all we're going to do is launch an ec2 instance but you know we'll go through the process here a bit and i don't know why i can't create one so we'll go back over to the instructions here credit so you can connect so create a route table using the vpc to the route table so i think that's as far as we're going to get here because i'm not seeing any options here but the idea was that we would have to create a carrier gateway we'd update our route tables and all we would be doing is launching an ec2 instance so you know it's no different than launching it you just choose a different subnet so i think you'd have to create a subnet for that zone and launch it in there and that would be edge computing another example of edge computing would be something like via cloudfront which we have these edge functions or not edge functions have functions here and so these are functions that are deployed to cloudfront so my cloudfront function and these would be deployed to um edge locations right and all you can use here is javascript so here's an example of one and um i'm fine with this development live this function is not published we'll go to test test the function it's good publish publish that function and so the advantage of this is that you know if you have functions that are in it with lambda there's a chance of cold start um whereas if they're deployed on the edge here there's still probably a cold start but it's going to be a lot faster because it's a lot closer to the edge location so um you know it's just the different uh different cases but yeah there was one where we're launching ec2 workload into wavelengths which we couldn't complete which is totally fine and then we have these functions on the edge there's other edge computing services like within sagemaker you can deploy i think it's called like neo sagemaker and then for iot devices those are obviously on the edge so you can deploy those as well but generally that gives you an idea of edge computing okay hey it's andrew brown from exam pro and we're looking at cost and capacity management computing services so before we talk about them let's define what is cost management so this is how do we save money and we have capacity management how do we meet the demand of traffic and usages through adding or upgrading servers so let's get to it the first are the different types of ect pricing models so you got spot instances reserved instances saving plans these are ways to save on computing by paying up in full or partially or by committing to a yearly contract or multiyear contract or by being flexible about the availability interruption to computing services we have it was batch so this plan schedules and executes your batch compute workloads across the full range of aws computing services which can utilize spot instances to save money we have abyss compute optimizer so suggest how to reduce costs and improve performance by using machine learning to analyze your previous usage history we have ec2 auto scan groups so asgs these automatically add or remove ec2 servers to meet the current demand all of traffic they will save you money and meet capacity since you only run the amount of servers you need then we have elb so elastic load bouncer so this distributes traffic to multiple instances we can reroute traffic from unhealthy instances to healthy instances and can route traffic to ec2 instances running in different availability zones and then we have elastic beanstalk here which is easy for deploying web applications without developers having to worry about setting up and understanding the io underlying aweso services similar to heroku it's a platform as a service so not all these are about cost some of them are about capacity management like elb but yeah there you go hey this is andrew brown from exam pro and we are looking at the types of storage services and no matter what cloud service provider using they're usually broken down into these three where we have blocks file and um uh object okay so let's take a look at the first so this is going to be for block storage so for aws this is called elastic block store data is split into evenly split blocks directly accessed by the operating system and supports only a single right volume so imagine you have an application over here and that application is using a virtual machine that has a specific operating system and then it has a drive mounted to it uh it could be using fc or scuzzy here but the idea here is when you need a virtual drive attached to your vm is when you're going to be using block okay the next one here is for um file or it's just basically a file system so this is about elastic file storage so the file is stored with data and metadata multiple connections via a network share supports multiple reads writes locks the file so over here we could have an application but it doesn't necessarily have to be an application and so it's using nasa exports as the means to communicate and so the protocols here can be nfs or smb which are very common uh file system protocols and so the idea here is when you need a file share where multiple users or vms need to access the same drive so this is pretty common where you might have multiple virtual machines and you just want to act as like one drive one example that could be like let's say you're running a minecraft server you're only allowed to have one world on a particular single drive but you want to be able to have multiple virtual machines to maximize that compute that'd be a case for that um so there you go then the last one here is like object storage and so for aws this is called amazon simple storage service or also known as s3 so object is stored with data metadata any unique id scales with limited uh with limited no file limit or storage limit so there's really very there's very little limit to this it just basically scales up supports multiple reasons right so there are no locks and so the protocol here we're going to be using https and api so when you just want to upload files and not have to worry about the underlying infrastructure not intended for high iops so input and outputs per seconds okay so depending on how fast you have to do your read and writes are going to determine uh you know whether you're going uh this direction or the other way um or you know how many need to actually connect at the same time and whether it has to be connected as a mount drive to the virtual machine okay hey it's andrew brown from exam pro and we're going to do a short introduction into s3 because on the certified cloud partitioner they ask you a little bit more than they used to and so we need to be a bit familiar with s3 because it is um at least i think that abel's considers its flagship uh storage uh service and it really is one of the earliest services is the second one ever launched okay so what is object storage or objectbased storage so data storage architecture that manages data as objects as opposed to other storage architectures so file systems where these are others right so which manages data as files and a hierarchy and block storage which manages data as blocks within sectors and tracks that get stored on an actual uh drive and so uh the idea here is we have s3 which provides basically unlimited storage you don't need to think about the underlying infrastructure the s3 console provides interface for you to upload and access your data okay so we have the concept of an s3 object so objects contain your data they are like files but objects may consist of a key this is the name of the object a value the data itself made up of a sequence of bytes the version id when versioning enabled the version of the object metadata additional information attached to the object and then you have your s3 buckets the buckets hold objects buckets can also have folders which in turn hold objects s3 is a universal namespace so bucket names must be unique it's like having a domain name okay and one other interesting thing is an individual object can be between zero bytes and up to five terabytes so you have unlimited storage but you can't have uh files of uh incredible size uh i mean five terabytes is a lot but nothing beyond that for a single file but just understand that you can actually have a zero byte file uh and for like associate certifications that can be a an actual question so that's why it's there all right let's take a look at s3 storage glasses um and so for the certified cloud partitioner we need to know generally what these are for associated levels we need more detail than we have here but let's get through it so adabus offers a range of s3 storage classes the trade retrieval time accessibility durability for cheaper storage and so the farther down we go here the more cost effective it should get pending uh you know certain conditions okay so when you put something to s3 it's going to go into the standard uh tier the default tier here and this is uh incredibly fast it has 99.99 availability 11 9's durability and it's replicated across three azs and so uh you know we have this cheaper meter here here on the lefthand side that would apply this is very expensive and it's not actually expensive but it is expensive at scale when you can uh better optimize it with these other tiers so just understand that then you have the s3 intelligent tiering so this uses ml to analyze objects and usage and determine the appropriate storage class it is moved to the most cost effective access tier without any performance impact or added overhead then you have s3 standard ia which stands for infrequent access this is just as fast as s3 standard but it's cheaper if you access the files less than once a month there's going to be an additional retrieval fee applied so if you do try to retrieve data as frequently as s3 standard it's going to actually end up costing you more so you don't want to do that okay then you have s3 one zone ia so as it says it's running in a single zone so it's as fast as s3 standard but it's going to have lowered availability but you're going to save money okay there is one caveat though your data could get destroyed because it's remaining in a single uh a z so if that a z or data centers um suffer a catastrophe you're not going to have a duplicate of your data to retrieve it okay and then you have s3 glacier so for longterm cloud storage retrieval of data can take minutes to hours but it's very very very cheap and then you have esri glacier deep archive which is the lowest cost storage class but the data retrieval is 12 hours and so you know um all of these here to here these are all going to be in the same abyss s3 console or amazon s3 console s2 glacier is basically like its own service but it's part of s3 so kind of lives in this weird state there's one here that we didn't have a list here which is s3 outputs because it has its own storage class it doesn't exactly fit well into this kind of leaner cheaper thing here okay hey it's andrew brown from exam pro and we're taking a look at the aws snow family so this is storage and compute devices used to physically move data in or out of the cloud when moving data over the internet or provide private connection that is too slow difficult or costly so we have snow cone snow ball edge and snowmobile and so there originally was just snowball and then they came out with snowball edge and edge introduced edge computing that's why there's edge in the name but pretty much all of these devices have edge computing uh and they do individually come with some variants so with the snowball snow cone it comes in two sizes where it has eight terabytes of usable storage and then there's one with 14 terabytes of usable storage for snowball edge it technically has like four versions but i'm going to break it down to two for you we have storage optimize where we have 80 terabytes of use of usable storage there and then compute optimize 30.9.5 terabytes and even though it's not here you get a lot of vcpus and increased memory which could be very important if you need to do edge computing before you send that over to aws and then last here we have snowmobile which can store up to 100 petabytes of storage um in the associates i cover these in a lot more detail because there's so much more about these like the security of them how they're tamper proof something like how they have networking built in the the connection to them but you know for this exam that's just too much information um you just need to know that there are three uh three ones in the family and generally what the sizes are and that they're going to be all placed into amazon s3 uh what's interesting is that you know snowmobile only does a hundred petabytes but adabus markets it as you can move exabytes of of um content because you can order more than one of these devices so uh they'll market it saying like snowball edge is when you want to move uh petabytes of data and snowball mobile is when you want to move exabytes but you can see that a single thing isn't in the exabytes just in the petabyte okay hey this is andrew brown from exam pro and we are taking a look at all the innova storage services in brief here so let's get to it so the first is simple storage service s3 this is a serverless object storage service you can upload very large files and an unlimited amount of files you pay for what you store you don't worry about the underlying file system or upgrading the disk size you have s3 glacier this is a cold storage service it's designed as a lowcost storage solution for archiving and longterm backup it uses previous generation hdd drives to get that low cost it's highly secure and durable we have elastic block store ebs this is a persistent block storage service it is a virtual hard drive in the cloud and you attach to ec2 instances you can choose different kinds of hard drives so ssd iops ssd throughput hdd and a cold hhd okay we have elastic file storage so efs it is a cloud native nfs file system service so file storage uh you can mount to multiple ec2 instances at the same time when you need to share files between multiple servers we have storage gateway this is a hybrid cloud storage service that extends your onpremise storage to the cloud we've got three offerings here file gateway so extend your local storage to amazon s3 volume gateway cache is your local drive to s3 so you have a continuous backup of the local files in the cloud tape gateway so stores files onto virtual tapes for backing up your files on very costeffective longterm storage we got warmer page here because there's a lot of services here we have eight of us snow family so these are storage devices used to physically migrate large amounts of data to the cloud and so we have snowball and snowball edge these are briefcase size data storage devices between 50 to 80 terabytes i don't believe snowball is available anymore it's just snowball edge but it's good to have all of them in here so we can see what's going on we have snowmobile this is a cargo container filled with racks of storage a compute that is transported via a semitrailer tractor truck to transfer up to 100 petabytes of data per trailer i don't think we're going to be ordering that anytime soon because that's pretty darn expensive but that's cool we have snow cone this is a very small version of snowball that can transfer eight terabytes of data we have aws backup a fully managed backup service that makes it easy to centralize and automate the backup of data across multiple services so ec2 ebs rds dynamodb efs storage gateway you create the backup plans we have cloud endure disaster recovery so continuously replicates your machine in a low cost staging area in your target able's account and preferred region enabling fast and reliable recovery in case of i.t data center failures we have amazon fsx this is a feature rich and highly performant file system that can be used for windows so that would be using smb or linux which uses luster and so there we have the amazon fsx for windows file server so use smb protocol and allow you to mount fsx to windows servers and then the luster one which uses a linux luster file system and allows you to mount ffsx linux servers are there any storage services missing here not really i mean you could count elastic container repositories one but that's kind of something else or you could also count maybe um code commit but you know i kind of put those in a separate category where we those are in our developer tools or our containers okay all right so what i want to do is show you around s3 so we'll make our way up here and type in s3 and we'll let it load here and what we're going to do is create a new bucket if you do not see the screen just click on the side here go to buckets and we'll create ourselves a new bucket so bucket names are unique so let's say my buckets and we'll just pound in a bunch of numbers i'm sure you're getting used to making buckets in this in this course so far so if we scroll on down notice that it says block public access settings for this bucket and this is turned on uh like the blocking is turned on by default because s3 buckets are the number one thing that are a point of entry for malicious actors where people leave their buckets open so if we want to grant access to this bucket for people to see this publicly we'd have to turn this off okay but for now we're going to leave that on you can version things in buckets which is pretty cool you can turn on encryption which you should turn on by default and use the amazon s3 key on the certified cloud partitioner it's going to ask you about clientside encryption and serverside encryption so you definitely want to know what these are i'm going to turn it off for the time being so we can kind of explore uh here by ourself here then there's object lock so we can lock files so that um you know there you know people aren't writing to them multiple times so go ahead and create a bucket and it's very quick so here's the new bucket we made and you'll notice we have nothing here which is totally fine if i go to properties um you know we can see that we can turn on bucket versioning turn on encryption but what i'm going to do is i'm going to go grab some files i remember i saved some files recently here i'm just going to make a new folder called star trek i just have some graphics you can pull anything off the internet you want to do this yourself but i'm just going to prepare a folder here it'll take me a moment okay just a moment okay great so now i have my folder prepared and so what i want to do is upload my first file so i can go here and upload and actually i can upload multiple files you can add a folder which is nice and so in here if i want to upload these files here whoops i'll just select multiples i'll hit open it'll queue them up which is really nice we can see the destination details here if we want to turn it versioning on we could there we could apply permissions for outside access but we have uh things turned on but what's really important is the properties where we have these different tiers and so based on the tier that you use the the lower you go at least it should be the cheaper it's going to get but it's going to have some tradeoffs let me cover that through the course then there's that server side encryption um and i'm going to hit upload we'll just individually turn it on so you're going to see this progress go across the top these have all been uploaded i'm going to click click on my destination bucket and so we can do is we can open these if they're images they'll show us right here in the browser we can download them so if we need to get them again all right we can create a folder here and just say star trek or enterprise d enter prize d here okay but it's not really easy it's not like i can drag this into there um i might be able there's no move option so you'd actually have to copy it into the destination and then delete the old one it's not like using a file system you know there's a lot more work involved but you know it's a great storage solution um so let's look at encryption so i have this selected here if i click into it i can go to permissions i can go to versions see that i'm looking for encryption here we go so if i turn it on i can enable encryption and i can choose whether i want to use an amazon s3 key so ss e s3 so an encryption key that amazon s3 creates manages and uses for you then you have itabus ssc kms and i believe this uses aes up here which is totally fine then you have kms down here and it's interesting because they're like database will manage the key for you and then this one abyss will manage the key for you it's just slightly different this one of course is a lot simpler it's not many reasons not to turn on encryption but i'm going to go turn this one so that it is encrypted here and just because it's encrypted doesn't mean we can't access the file i can still download it i can still view it because aws is going to decrypt it right so if i go i click on this one and i say open okay even though it's encrypted i can still view it right it just means that it's encrypted on the storage right so if somebody were to steal that hard drive or whatever hard drive it's sitting on on a bus if they can't even figure it out it's encrypted they're not going to be able to open up the file right so that is the logic there but through here i can get it something that's really interesting with um s3 is the ability to um have life cycle events so i'm just kind of looking where that is it's usually in the bucket so if i go to management up here i can set up a lifecycle rule and what i can do is say like move this to deep storage okay and then i can say what it is that i want to filter so maybe it's like data.jpg or i can say apply to all objects in the bucket i acknowledge that and we say move current versions of objects between storage classes and i check box that on and i can say move them to glacier after 30 days i think if i go lower it'll complain probably when i save there and so the idea is that we can move things into storage so maybe you have files coming in down below it's showing you here right so a file is uploaded and then after 30 days then move them into glacier so we save money okay that's a big advantage of s3 there's a lot of things going on in s3 here like you can turn on um uh wherever it is you can turn on web hosting so you can turn this into like a website down below here there's a whole a whole bunch of things that you can do okay so we're not going to get into that because that's just too much work but you know we learned the basics of s3 so what i want to do to delete this i have to empty it first watch it'll be like you cannot delete it you need to empty the bucket first so go ahead and empty it and i'll save my bucket empty or sorry i guess i have to type in permanently delete perm delete no they used to oh yeah i can copy it okay great and so once the bucket is emptied i can go back to the bucket and i'll go back one layer and then i'll go ahead and delete my bucket and you can only have so many buckets i think it's like a hundred you get like 100 buckets how many buckets can you have in aws 100 buckets yeah i was right and i think if you wanted to know how many you pro there's probably like a service limits page service limits service quotas so you go here you say aw services s3 how many buckets 100 right there okay so you know that gives you kind of an idea what's going on there but there you go that's s3 all right so let's go take a look at elastic block store which is uh virtual hard drives for ec2 so what i'm going to do is make my way over to the ec2 console because that is where it's at and on the left hand side if we scroll on down you'll see elastic block volumes or elastic block store volumes and so we can go here and the idea is we can go ahead and create ourselves a volume and what you'll notice is that we have a few different options here we have general purpose provisioned iops cold hdd throughput optimized magnetic magnetic beam basically like physical tape that you can use to back up like the old school stuff and so you have all these options here and you can choose the size so when you change these options you're going to notice that some things are going to change like the through throughput or iops so notice that general purpose is fixed at between 300 to 3000 and notice that it goes from one gigabyte to how many ever that is that's a lot there and so it's not too complicated but in practicality i don't really create volumes this way what i do is i'll just go launch an ec2 instance so i'll say launch ec2 instance and we'll choose amazon linux 2 and again you know if we haven't done the ec2 follow along we'll cover all this stuff in more detail don't worry about it we go to configure instance then we go to add storage and this is what you're going to be doing when adding ebs volumes um to your ec2 instances and you'll notice we always have a root volume that's attached to the ec2 instance that we cannot remove we can change the size up here i believe the oh it shows us right here that we have up to 30 gigabytes so sometimes you might want to max that out to take advantage of the free tier you notice we can also change this there might be some limitations in terms of the root volume so notice that we have a few more options here we can't have a cold hdd or hdd as our root volume uh notice we have a delete on termination so ebs volume persists independently from the running life so you can choose to automatically delete ebs volume when the associated instance is terminated so if you take this off if the ec2 instance is deleted the volume will still remain which could be something that's important to you uh for encryption here um you might want to turn it on and so generally aws always has a kms managed key which is free so you check box that on it will be encrypted you can turn it on later but you can never turn encryption off but you should always uh turn encryption on and so just be aware to turn that on you can also add file systems down below here but maybe we'll talk about that later because i think that gets into um efs okay so that is a different type of file storage there but that's pretty much all there is to it uh you just go ahead and create your volume there and then it would show up under ebs we could take snapshots of them to back them up that goes to s3 but that's all we really need to know here okay all right let's take a look at elastic file system or efs storage manage file storage what is efs stand for efs system elastic file system okay sorry and so what we can do is go ahead and create a file system here so i'm going to say my efs and the great thing is that it basically is serverless so it's only going to be what you consume right so what you store and what you consume and i think that's what's going to be based on we have to choose a vpc i want to launch it in my default vpc and we have the choice of regional or one zone i guess this is going to be based on what gets backed up to s3 possibly so one zone probably is more cost effective but i'm going to choose regional and that's a new option i never noticed before i just opened it up to see a few more things here we have general max io bursting provision things like that we'll hit next we'll choose our azs and uh then you might have to set up a policy so i'm going to hit next here you'll go ahead and hit create so you know this is really interesting but the trick to it is really mounting it to a dc2 instance and that's kind of the pain okay so if we go into this um you have to mount it and there are commands for it so like efs mounting linux commands okay i've done this in my solutions architect associate uh but you know again i'm not doing on a regular basis so i don't remember and so if we go here i'm just trying to see if we can see some code that tells us how to mount it so mounting on an ec2 ec2 linux instance with the efs mount helper um so i don't know if they had that before but that sounds interesting so pseudo mount hyphen t the file system the efs mounting point yeah this looks a lot easier than what we had before okay so before i had to enter a bunch of weird commands but now it looks like they've boiled it down to a single command but once you have your efs instance um i'm going to assume that there is an entry point here just clicking around here seeing what we can see i would imagine we have to create an access point so my access point sure i don't know if it's going to let me just do that it did and so i would imagine that you'd probably use an access point let's go back here if that's mount point i think that's the same thing i think the mount point and the access point you create access points and that's what you use uh we can go here we can attach it so oh yeah here's the command so um mount via dns or mount via ip address so it doesn't look too hard we can try to give it a go i haven't done it in a while it looks like they've made it easier so maybe we'll try it out okay so go to ec2 here and i'm going to launch an instance i'm going to choose amazon linux2 okay we're going to go and choose that and then we want to choose a file system and so it's going to mount to here okay and storage is fine all this is fine and i'm going to go ahead and launch this and i need a new key pair so create a new key pair this will be for efs example okay we're going to download that key pair there we're going to launch this instance okay and then we're going to go view this and as that is launching what i'm going to do is open up my cloud shell and i'm going to want to upload this pen so again like before i'm going to drag it to my desktop off screen and then what i'm going to do is upload this file so i have it efs example okay we're going to upload it i just want to see if we can access that efs volume and so if i do ls that's our old one which i can delete by the way i'm never going to use that anytime soon yes ls and i'm just delete the hello text there so it's a bit cleaner for what we're doing and so we need to mod that 400 uh efs example and we saw that's how like if you want to try to connect to a server remotely that's what you do right so i believe that the drive is mounted if i go to storage does it show up here it doesn't show up under here but what we're waiting for are these two status checks to pass and then we can ssh into this machine and i'm just going to go back here and take a look here so using the efs mount helper so sudo mount hyphen t efs tls this volume to efs and so i imagine it's going to mount it to efs here using the nfs client so i guess it just depends on what we're going to have available to us even if the sas checks haven't passed i'm going to try to get into this anyway so what we can do is click on this grab the public ip address we'll type in ssh ec2 hyphen user at sign paste this in hyphen i efs example pem i usually don't log in via ssh um but you know just for this example i will and so i want to see if this drive exists it usually be under mount right there it is okay so it already mounted for us so i can do touch hello world dot text say sudo here i can say sudo vi i'm going to open up the file and say hello from another computer okay and so i've saved that file and what i want to do now oops oh okay sorry i'm in the cloud shell here but what i want to do now is i want to kill this machine okay and what i'm going to do is spin up another ec2 instance i'm going to see if when i mount that if that file is there if it actually worked but wow that is so much easier than before i can't tell you how hard it was to attach an efs volume the last time i did it um so we'll go ahead we'll add that and the storage is fine we're gonna go to review here we're gonna say launch and i'm just gonna stick with the same key pair there we're going to give that moment to launch and we're going to go to view instances and so now this one is launching as that's launching let's just go peek around and see what we can see so you know i imagine if we didn't add that file system during the the boot um and we were we're adding it after the fact we probably could just ran that line and added it really easily i'm not going to bother testing that because i just don't want to go through that trouble to do that i still can't remember what these access points are for um but uh that's okay it's kind of out of the scope for the certified cloud partitioner and so i'm just curious so we have some nice monitoring here right so that's kind of nice um i guess they're trying to suggest here like aws backup data sync transfer so that would just be backing up simplify automates accelerates moving data okay that's pretty straightforward transfer family fully managed sftp okay so nothing exciting there and we're going to refresh that there and this is initializing so let's go see if we can connect to this one so i'm going to go ahead and grab that public ip address i'm going to hit up okay i'm going to swap out that ip address and we're going to see if we can connect to that machine yet so we'll say yes and we got into it so that's great and so what i'm going to do is go again into the mount directory efs fs1 ls and there it is i'm going to do cat hello world and so it works and so that's the cool thing about dfs is that you have a file system that you can share among other ec2 instances i'm sure users could connect to it using the nfs protocol i'm not the best at like networking or storage networking so i'm not going to show that here to you today but that gives you a general idea how efs works again you only pay for what you store it is serverless so we'll go here and type delete because i'm done with this i'll probably destroy the instance first it doesn't get mixed up and just so we clean up a little bit better here i'm going to delete these keys here delete okay and we'll go ahead and delete this one as well delete so i'm done with that uh we'll make sure that that is tearing down that is good and we'll make our way back over here and it says enter probably the id's name in so we'll enter that in and hit confirm and we'll see is it deleting i'm not confident with it i'm going to do it one more time confirm that by entering the the file system's id so we'll put it in again is it destroying i cannot tell there we go so it's destroying we are in good shape it is gone our data is gone um but yeah that is efs all right let's take a look at um the snow family in aws so if we type in snow up here and we click into into the snow family this is where we can probably order ourselves a device i might not be able to order them at least when i originally looked at this like way back in the day it wasn't available in canada so i'm kind of curious to see what there is but the idea is that you're going to go here in order and you have some options so you can import into s3 or export from s3 and then down below we have local compute storage so perform local compute storage workloads without transferring data you can order multiple devices and clusters for increased durability and storage capacity so it sounds like you're not you're not um transferring data you're just using it locally on to um it's like basically buying renting temporary computers which is kind of interesting i never saw that option before but we're going to choose import into aws s3 and we're just going to read through this stuff and it's not my expectation that we're going to be able to submit a job here and you probably don't want to because it's going to cost money but i just want to show you the process so we can see what there is here so snow job assistance if you're new to snow family run a pilot of one to two devices so batch file smaller than one megabyte benchmark and optimize deploy staging workstations discover remediate environmental uh issues early files and folders name must conform to amazon s3 prepare your ami once the pilot is completed confirm the number of snow family devices that you can copy devices to simultaneously follow the best practices use the following resources to manage your snow devices so we have aws open hub and then there's the edge client cli so open hub is a graphical user interface you can use to manage snow devices so that's kind of cool and then we have the cli which i imagine is something that's very useful to use so just close those off here and then we have other things so i'm going to say i acknowledge i know what i'm doing which i don't really but that's okay and then here we are going to enter in our address so we say andrew brown and i'm not gonna i'm not gonna enter this in for real just whatever so it'll be toronto exam pro um canada oh see so there's there's the thing you can only ship it to the us and so that's as far as i can get okay um and that's the thing is like if you really want to know aws inside and out you got to be in the us but let's pretend that we do have an address in the states what's a very famous address so what is the address of the white house okay there it is so i'm just going to copy that in because again we're not going to submit this for real i just want to see what's farther down the line here okay uh what's nw is that the state it's in washington right is is this part of it nw northwest is that a thing i'm from canada so i couldn't tell you um so we'll go down here and we have washington do we have a second address line it doesn't look like it um we have a zip code i believe this is the zip code and do we need a phone number looks like we do four one six uh one one one one one one one okay we have one day or two day shipping why not just have one right and so then we can choose our type of device so we have snow cone snow cone ssd snow cone optimized i'm surprised i never took a screenshot of this earlier um compute optimized things like that so you can choose which one you want it looks like we're going to see some different options but we'll go with snow cone my snow cone and snow cones do not ship with a power supply or ethernet cable snow cone devices are powered by 45 watt cb c usb c power supply i'll provide my own power supply and cable do not ship with a power supply or ethernet cable that's fine uh snow cone wireless no can connect your wireless connection connect the buckets you want there's a bucket we created earlier computing use compute using ec2 instances use a device as a mobile data center by loading ec2 ami so here's an ami that i might want to use uh aws iot green grass validated ami not interested remote device management you can use opshub or etc to monitor reboot your device that's fine and so then we need to choose our security key i don't know if i'll have to set the service role we'll see what happens here and we'll let it update that's fine and so then i guess we just hit create job and so i don't really want to order one um so i'm not going to hit that button and also it's going to go to the white house and they're going to be like andrew brown why did you do that so that's not something i feel like doing today but at least that gives you an idea of that process there and i imagine that uh if you go the other way it's gonna be pretty similar yeah it's just like same stuff i think so it saved that address that it's not a real address and the the options are a little bit uh limited here and it's like nfs space s3 based so it's slightly different but it's basically the same process just curious we'll take a look at the last one there since there are three options just curious okay similar thing okay so yeah that's pretty much all i want to know about um the snow family and that's about it okay hey this is andrew brown from exam pro and we are taking a look at what is a database so a database is a data store that stores semistructured and structured data and just to emphasize a bit more a database stores more complex data stores because it requires using formal design and modeling techniques so databases can generally be categorized as either being relational so structured data that strongly represents tabular data so we're talking about tables rows and columns so there's a concept of row oriented or columner oriented and then we have nonrelational databases so these are semistructured that may or may not distinctly resemble tabular data so here is a very simple example the idea is that you might use some kind of language like sql put in your database and you'll get back out tables for relational databases let's just talk about some of the functionality that these databases have so they can be using a special specialized language to query so retrieve data so in this case sql specialized modeling strategies to optimize retrieval for different use cases more finetuned control over the transformation of the data into useful data structures or reports and normally a database infers someone is usually using a relational roworiented data store so you know just understand that when people say database that's usually what they're talking about like postgres mysql relational row store is usually the default but obviously there's a lot more broader terms there okay hey this is andrew brown from exam pro and we are taking a look at what is a data warehouse so it's a relational data store designed for analytical workloads which is generally column oriented data store okay so companies will have terabytes and millions of rows of data and they'll need a fast way to be able to produce analytics reports so data warehouses generally perform aggregation so aggregation is the idea of grouping data together so find a total or an average and data warehouses are optimized around columns since they need to quickly aggregate column data and so here's kind of a diagram of a data warehouse and so the idea is that it could be ingesting data from a regular database here i'm just getting out my pen tool so it could be a regular database or it'd be coming from a different data source that isn't compatible in terms of the schema and you use like etl or elt or etl to get that data into that data warehouse so data warehouses are generally designed to be hot so hot means that they can return queries very fast even though they have vast amounts of data data warehouses are infrequently accessed meaning they aren't intended for realtime reporting but maybe once or twice a day or once a week to generate business and user reports of course it's going to vary based on the the service that is offering the data warehouse a data warehouse needs to consume data from a relational database on a regular basis and again it can consume it from other places but you'll have to transform it to get it in there okay hey this is andrew brown from exam pro and we're taking a look at a key value store so a key value store or database is a type of nonrelational database or nosql that uses a simple key value method to store data and so key value stores are dumb and fast but they generally lack features like relationships indexes aggregation of course there are going to be providers out there have managed solutions that might polyfill some of those uh issues there but i want to show you the underlying way that key value stores work to kind to kind of distinguish them between document stores so a key value stores literally a unique key alongside a value and the reason i'm representing that is zeros and ones is because i want you to understand that that's what it is it's basically just some kind of of data there and how the key value store interprets it is going to determine what it is so when you look at a document database that is just a key value store that interprets the value as being documents right and so key value stores can and do commonly store um multiple uh like an associative array that's pretty common so even for dynamodb that's how it does it and so that's why when you look at a key value store it looks like it uh a a table but it's not actually a table it's schemaless because underneath it's really just um you know that associative array and so that's why you can have columns or sorry rows that have uh different amounts of columns okay so due to the design they are able to scale very well beyond a relational database and they can kind of work like a relational database without all the bells and whistles so hopefully you know that makes sense okay all right let's take a look at document stores so a document store is a nosql database that stores documents as its primary data structure and a document could be an xml type of structure but it also could be something like json or jsonlike document stores are subclasses of key value stores and the components of a document store are very uh comparable to relational databases so just kind of an example here where in a relational database they'd be called tables now you have collections they were called rows now they're called documents you had columns they had fields they may have indexes and then joins might be called embedding and linking so you can translate that knowledge over uh you know they they're not as they don't have the same kind of feature set as a relational database but you have better scalability and honestly document stores are just key value stores with some additional features built on top of it okay hey it's andrew brown from exam pro and we're going to take a look at the girl database services that are available on aws so we have dynamodb which is a serverless noaa skill key value and document database it is designed to scale to billions of records with guaranteed consistent data returned in at least a second you do not have to worry about managing shards and dynamodb is adabus's flagship database service meaning whatever we think of a database service that just scales is cost effective and very fast we should think of dynamodb and in 2019 amazon the online shopping retail uh shut down their last oracle database and completed their migration to dynamodb so they had 7500 oracle databases with 75 petabytes of data and with dynamodb they reduced that cost by 60 and reduce the latency by 40 percent so that's kind of to be like a testimonial between relational and a no school database so when we want a massively scalable database that is what we want dynamodb for and i really just want to put that there because if you remember that you're going to always be able to pass or get those questions right on the exam okay then we have documentdb so this is a no scroll document database that is mongodb compatible so mongodb is very popular nosco among developers there were open source licensing issues around using open source mongodb so aws got around it by just building their own mongodb database basically so when do you want a mongodb like database you're going to be using documentdb we have amazon key spaces this is a fully managed apache cassandra database so cassandra is an open source nosql key value database similar to dynamodb that is columnar store database but has some additional functionality so when you want to use apache cassandra you're using amazon key spaces hey this is andrew brown from exam pro and we are taking a look at relational database services starting with relational database service rds and this is a relational database service that supports multiple sql engines so relational is synonymous with sql and online transactional processing oltp and relational databases are the most commonly used type of database among tech companies and startups just because they're so easy to use i use them i love them rds supports the following sql engines we first have mysql so this is the most popular open source sql database and it was purchased and is now owned by oracle uh and there's an interesting story there because when oracle purchased it they weren't supposed to have it um mario db was or sorry my squad was sold to oracle sun systems and then within the year um uh oracle purchased it from them and the original creators never wanted it to go to oracle just because of their uh the way they do licensing and things like that and so the original creators came back and they decided to fork mysql and then maintain it as mariodb just so that you know oracle never kind of pushed away the most popular database so that everyone had to go to a paid solution then you have postgres so psql as it's commonly known is the most popular open source sql database among developers this is the one i like to use because it has so many rich features over mysql but but it does come with added complexity then oracle has its own sql proprietary database which is well used by enterprise companies but you have to buy a license to use it then you have microsoft sql so microsoft's proprietary sql database and with this one you have to buy a license to use it then you have aurora so this is a fully managed database uh and there's a lot more to going on here with aurora so we'll talk about it almost acts as a separate service but it is powered by rds so aurora is a fully managed database of either mysql so five times faster or postgres sql three times faster database so when you want a highly available durable and scalable and secure relational database for postcode to mysql you want to use aurora then you have aurora serverless so this is a serverless ondemand version of aurora so when you want the most of the benefits of aurora but you can trade off to have cold starts or you don't have lots of traffic or demand this is a way you can use aurora in a serverless way then you have rds on vmware so this allows you to deploy rds supported engines to onpremise data centers the data center must be using vmware for server virtualization so when you want databases managed by rds on your own database center uh and yeah i realize that this is a small spelling mistake should say just on here but yeah there you go hey this is andrew brown from exam pro and we're looking at the other database services that abuse has because there's just a few loose ones here so let's talk about redshift so it is a petabyte size data warehouse and data warehouses are for online analytical processing olap and data warehouses can be expensive because they are keeping data hot meaning that they can run a very complex query and a large amount of data and get that data back very fast so when you need to quickly generate analytics or reports from a large amount of data you're going to be using redshift then you have elastic cache so this is a managed database of an inmemory and caching open source databases such as redis or memcache so when you need to improve the performance of an application by adding a caching layer in front of your web servers or database you're going to be using elastic cache then you have neptune this is a managed graph database the data is represented as interconnected nodes i believe that it uses gremlin as the way to interface with it which is no surprise because that's what it looks like most clusters providers are using so when you need to understand the connections between data so mapping fraud rings or social media relationships very relational database heavy information you're gonna want to use neptune we have amazon time streams it's a fully managed time series database so think of devices that send lots of data that are time sensitive such as iot devices so when you need to measure how things change over time we have amazon quantum ledger database this is a fully managed ledger database that provides transparent immutable cryptographically variable transaction logs so when you need to record a history of financial activities that can be trusted and the last one here is database migration service dms it's not a database per say but it's a migration service so you can migrate from onpremise database to aws from two databases in different or same database accounts using different sql engines and from an esque wall to a nosql database and i'm pretty sure we cover this in a bit greater detail in this course okay all right let's go take a look at dynamodb which is awesome's nosql database so we'll go over to dynamodb and what we'll do is create ourselves a new table and we'll just say my dynamodb table and you always have to choose a partition key you don't necessarily have to have a sort key but it could be something like um like it you want to be really unique so it could be like email and this one could be created at right and so we have string binary notice that the the types are very sim simple then for settings we have default settings or customized settings so the default is use provision capacity mode rewrite five rules etc custom no secondary indexes use kms so i'm gonna just expand that to see what i'm looking at we have two options here on demand so simplify billing by paying the actual reads and rights that you use or provisioned which is this is where you get a guarantee of performance so if you want to be able to do you know whatever it is a thousand i don't know what it goes up to but like a thousand read writes per second then that's what you're paying for okay you're paying for being able having a guarantee of that um of that capacity okay i'm not going to create any secondary indexes but that's just like another way to look at data notice down below that we have a cost of two dollars and ninetyone cents uh then we have encryption at rest so you can do owned by amazon dynamodb that's pretty much the same as like a bus has or s3 has ssc s3 there you could use uh actually i guess most of these are probably kms i would imagine we'll go ahead and create the table here and that's going to create the table this is usually really really fast we'll go here and what we can do is insert some data so as it's just starting up here we can go over to our tables they recently changed this ui so that's why i look a bit confused view items up here okay and then from here we can create an item so i can add something say so andrew at exam pro dot co and 2021 uh well we'll just do the future so let's say 20 25 0.505 i don't want to have to think too hard here but we can add additional information so i can say like uh today true we could say um make it like a list you know food and then i could go here and then add a string it is not working oh there we go there we are so we could say like um banana and then we could say pizza right we can go ahead and create that item so now that item is in our database uh we can do a scan that will return all items we can query we can actually have some limitations of what we're choosing there's the party cue editor so we can use sql to select it um i have not used this before party q aws or partyq dynamodb examples i'm hoping i can just find like an example of some of the language getting started here i don't need to i don't need an explanation just show me an example query here and i will i'll get to it here okay so here's some examples right so maybe we can give this a go um so we have our table here so my dynamo db table and i just want the email back we don't need a where we'll run this see if it works there we go i'm not sure if we could select additional data there so i know that we had some other things like food there it is okay so that's really nice um addition to it dynamodb can stream things into a dynamodb stream to go to kinesis and do a lot of fun things so there's all sorts of things you can do with dynamodb but i'm pretty much done with this so i'm going to go ahead and delete this table and notice that it also creates some cloudwatch alarms so we want to delete this as well create a backup no we do not care go ahead and delete that and that is dynamodb okay so now i want to show you uh rds or relational database service so go to the top here type in rds and we'll make our way over there and so rds is great because it allows us to launch relational databases sometimes the ui is slow i'm not sure why it's taking so long to load today but every day is a bit different and so what we're going to do is go ahead and create a new database you're going to notice that we're going to have the option between creating a standard or easy i stick with standard just because i don't like how easy hides a lot of stuff from us even here like it says two cents per hour but it's not giving us the full cost so i really don't trust it because if you go down here and you chose their dev test here look it's like a hundred dollars it's not showing the the cost preview right now maybe because we didn't choose the database type sorry i wanted to choose postgres but before we do that let's look at the engine types we have amazon aurora so we have between mysql and postgres mysql maritab postgres oracle microsoft sql notice for microsoft sql it comes with a license you don't have to do anything with that it might change based on the addition here nope comes with a license for all of them which is great if you want to bring your own license that's where you need a dedicated host right running microsoft sql for oracle uh you have to bring your own license that's going to be based on um importing with the abs license manager but we go over to postgres which is what i like to use we're going to set it to dev test to try to get the cheapest cost scroll down look 118 dollars we can get it cheaper we get super cheap so here the password is going to be testing one two three capital on the t so an explanation mark on the end okay because it has a bunch of requirements of what it wants here i want a t2 micro so i'm just going to scroll down here what is going on here standard oh look m classes i don't want an m class i want a burstable class that's the cheap ones and so we go here can we still do a t2 micro or is it now t3 so i don't see t2 so i imagine a t3 micro must be the new it was free tier so we go it was three tier here right and if i go to databases um rds on the t2 micro 750 hours but i can't select it so i'm going to assume that the t3 micro must be the new tier if it's not there right let's just say include previous generations and then maybe i can see it then okay so i don't see it there i really don't like how they've changed this on me okay so the oldest i can choose is a t3 micro which is fine i just i just know t2 being the free tier that's all uh this is fine we don't want auto scaling turned on for our example here we do not want a multiaz so do not create a standby that's going to really jump up our cost we don't need public access it will create a vpc that's fine password authentication is fine we have to go in here which i don't know why they just don't keep that expanded because you always have to come in here name your database so my database we choose our postgres version here i'm going to turn backups off because if we don't if we don't it's going to take forever to launch this thing encryption is turned on you can turn it off but generally it's not recommended we can have performance insights turned on i'm going to turn the retention i will leave it to seven days because we can't turn that off we don't need enhanced monitoring so i'm just going to turn that off and uh that's fine we're not going to enable delete protection here and so we are good we can now go ahead and create our database and what we'll do here is wait for that database to be created so the thing is is like if we're doing the solutions architect or the developer associate stuff i'd actually show you how to connect to the database um it's not that hard to do like you just have to connect uh grab all the database information so it's going to have an endpoint a port stuff like that and you'd use something like table plus or something to connect to the database but that's out of scope of the certified cloud partitioner i'm just going through the motions to show you that you can create an rds database very easily but not how to connect to it and actually utilize it okay and so that would spin up and we would have a server and after that we can just go ahead and delete the server here so just say delete me okay and that's all there really is to it there is the special type of database like aurora doesn't have its own like console page it's part of rds so if you want to spin up aurora you just choose the compatibility you want you can choose between provisioned or serverless the serverless is supposed to be really good for scaling to zero cost so that's something there so you'd fill that all out but the initial cost is a lot more expensive you can't choose a t2 micro here um unless it lets you now it is for provisioned it's uh oh t2 t3 medium is the smallest you can go okay so if you reach the point we're using a mediumsized database then you might consider moving over to aurora just because it's going to be highly scalable et cetera like that um so that's a consideration there there's also something called babelfish um that it was announced last year when i when i shot this or when i'm shooting this as of now and the idea was to make it compatible with mysql sql server to migrate over to aurora post sql which is kind of interesting um but that's about it so if our database is destroying i think it is just going to go back over here to rds it's taken a long time to load today and i think it's already deleted maybe we go to databases here it's deleting so i'm confident it's going to delete so there we go all right let's take a look at redshift so redshift is a data warehouse and it's generally really expensive so it's not something that you're going to want to launch uh day to day here but let's see how far we can get with it um just by running through it so what we'll do is go ahead and create a cluster and again you can just watch me do this you don't have to create you don't have to create one yourself so free trial configure for learning that sounds good to me it's free for a limited time if your organization has never created a cluster well i rarely ever create these so when the trial ends delete your cluster to avoid the charges of ondemand okay that sounds fair so here we're going to have two v3 cu's it's going to launch a dc a dc too large so let's look that up for pricing me prices please please please um i think it's loading right here okay so i don't know how much it is but i know it is not cheap and down below we have sample data is loaded into your redshift cluster that sounds good to me ticket is the sample data okay ticket sample data redshift i just imagine they probably have like a tutorial for it here they do right here and so because i want to know what we need to do to query it right if we can even query it via the interface here so the admin user is adabus user and the password is going to be capital t testing one two three four five six exclamation and we'll hit create cluster oh cool we can query the data right in here so that's what i wasn't sure about whether we would be able to just query it in line because before you'd have to use java with jdbc or an odbc driver and download the jar and it's not as fun as it sounds of course but looks like we can query data once the data is loaded so that looks really good i guess we can pull data in from the marketplace so that's looks pretty nice too and i guess we could probably integrate it into other things like quicksite because you probably want to adjust your data over there again i usually don't spend a lot of time in redshift but it looks like it's a lot easier to use i'm very impressed with this so i don't know how long it takes to launch a redshift cluster i mean it is 160 gigabytes of storage there it's even at the smallest it's pretty large so what i'm going to do is to stop the video and i'll be back when this is done okay okay so after a short little wait here um it was a lot faster than i was expecting but uh it's available and so looks like here it says to query the sample data use redshift version 2. so i'm going to click that and i'm sure there's tons of buttons to get here and it'd be great if it just populated the query for me it doesn't but this looks really nice really nice ui i wonder if it has like some existing queries no that's okay so what i'm going to do here is i'm going to go ahead and pull out this query and see if we can get this to work here never found out what those prices were though okay and what we'll do is hit run i like how there's like a limit of 100 but here it has that so we'll go ahead and hit run and see what data we get so relation sales does not exist okay so what's going on here um we'll go up here so most of the examples in the redshift documentation uses a sample database called ticket the sample the small database consists of seven tables you can load the ticket data set by following the this here okay so to load the sample data from amazon s3 okay so i would have thought it already had the data in there i could have swore it would have dev public tables zero tables okay so i don't think there's any data in here and so we're going to have to load it ourselves i really thought it would have added it for us let's go ahead and create these tables and see if this is as easy as we think so run that create that table cool okay we got it down here we'll run that we'll just run each at a time i think there's seven of them so date already exists okay that's fine event already exists saying all these tables exist maybe i just wasn't patient hmm okay um interesting all right so maybe we'll go back and uh run that query maybe we just had to wait a little while for that data to load run okay so you know what i think it was doing this for us if if it did not create it for us we would have to go through all these steps which is fine because we're learning a little bit about redshift but looks like we just had to wait there so it looks like you would run those you download that you use the copy command to bring it over there it looks like you can do all of this via the uh this interface here and we've done a query so that's kind of cool um i imagine you probably could like save it or export it what if we chart it what happens okay you can chart it it's kind of fun can we export it out to just we can save it i thought maybe it could export out to quicksite but i i suppose you'd rebuild it in quickside a but yeah i guess that's it right there so that's pretty darn simple so what i'm going to do is make my way back over to redshift because we are done for this example and we will go over to clusters here and i'm going to go ahead and delete my cluster delete create file snapshot nope delete delete the cluster there we go so i'm pretty sure that will succeed no problem there and we are done with redshift and redshift is super expensive so just make sure that thing deletes okay hey this is andrew brown from exam pro and we are taking a look here at cloud native networking services um and so i have this architectural diagram i created which has a lot of networking components uh when people create networking diagrams for aws they don't always include all these things here even though they're there so we're just being a little bit verbose so you can see okay the first thing is our vpc our virtual private cloud this is a logically isolated section of the database cloud where you can launch database resources that's where your resources are going to reside not all services uh require you to select a vpc uh because they're managed by aws but i wouldn't be surprised if under the hood they are in their own vpc okay then if you want the internet to reach your services you're gonna need an internet gateway um then you need to figure out a way to route things to your various subnets and that's where route tables come in then we need to define a region that it's going to be which is a geographical location on your network then you have your availability zones which are basically your data centers where your resources are going to reside then you have subnets which is a logical partition of an ip network into multiple smaller network segments and these pretty much map to your availability zones if you're making one per a z and then we have knuckles these act as a firewall at the subnet level then we have security groups that act as a firewall at the instance level so hopefully that gives you a good overview okay all right so now let's take a look at enterprise or hybrid networking so we have our onpremise uh environments or your private cloud and then we have our aws account or our public cloud so there's a couple services here that we can bridge them together the first is aws virtual private network vpn it's a secure connection between onpremise remote offices and mobile employees then you have direct connect this is a dedicated gigabit connection from onpremise data center to aws so it's a very fast connection a lot of times a direct we say it's a private connection but that doesn't necessarily mean secure it's not encrypting uh the data in transit so very commonly these services are used together not just singular okay and then uh we have private links and so this is where you already uh are using aws but you want to keep it all within databus never going out to the internet okay so these are generally called vpc interface endpoints and then the marketing pages call them private links which is a bit confusing but you know it just keeps traffic within the database network so it does not transverse out to the internet okay hey this is andrew brown from exam pro and we are taking a look at vpcs and subnets so a vpc is a logically isolated section of the database network where you launch your aws resources and you choose a range of ips using a cider range so cider range is an ip address followed by this netmaster subnet sub mask that's going to determine how many ip addresses there are and there's a bunch of math behind that which we're not going to get into but anyway so here is an architectural diagram just showing a vpc with a couple subnets so subnets is a logical partition of an ip network into multiple uh smaller network segments and so you're essentially breaking up your ip ranges for vpcs into smaller networks so just thinking about cutting up a pi okay so subnets need to have a smaller cider range to the vpcs represent for their portion so uh four slash 24 is actually smaller which is interesting the the higher the number gets the smaller it gets and so this would allocate 256 ip addresses and so that's well smaller than 16 okay we have the concept of a public subnet so this is one that can reach the internet and a private subnet the one that cannot reach the internet and um these are not strictly enforced by aws so the idea is that when you have a subnet you can just say don't by default assign publicly assignable ip addresses but it's totally possible to launch an ec2 instance into your private subnet and then turn on um the ip address so you've got to do other things to ensure that they stay private or public okay hey it's andrew brown from exam pro and we are comparing security groups versus knackles so i have this nice architectural diagram that has both knuckles and security groups in them and we'll just kind of talk about these two so knackles stand for network access control lists and they act as a virtual firewall at the subnet level and so here you can create and allow uh and deny rules and this is really useful if you want to block a specific ip address known for abuse and i'm going to just kind of compare that against security groups because that's going to be a very important difference okay so security groups act as a firewall at the instance level and they implicitly deny all traffic so you create only allow rules so you can allow an ec2 instance to access port on uh port 22 for ssh but you cannot block a single ip address and the reason i say that is because in order for you to block a single ip address and secure group you would literally have to block or you'd literally have to allow everything but that ip address and that's just not feasible okay and so if you can remember that one particular example you'll always be able to remember the difference between these two one other thing that aws likes to do is is ask which ones are stateless which ones are stateful but at the cloud particular level they're not going to be asking you that okay all right let's learn a bit about networking with aws so what i want you to do is go to the top and type in vpc which stands for virtual private cloud and what we'll do is set up our own vbc it's not so important that you remember all the little bit of details but you get through this so that you can remember the major components so what i'll do is create a new vpc i'm going to call this my vpc uh tutorial and here i'm going to say forward slash 10.0.0.06 the reason you're wondering why i'm doing that if we go to x y x y z here this tells you the size of it so i go here i put 16 so you can see we have a lot of room if we do 24 it takes up it it's smaller see so this is basically the size of it right the empty blocks over here so we're gonna have a lot of room so we do ten zero zero zero sixteen we don't need ipv6 we're gonna go ahead and create that and once we have that we can go ahead and create a subnet which we will need so we're going to choose our vpc we'll go down here and say my subnet tutorial and we'll choose the first say z you can leave it blank it'll choose it random and then we need to choose a block that is smaller than the current one so 16 would be definitely um well 16 is the size that we have now so we can match that size but 10.0.0.0 forward slash 24 would be absolutely smaller okay so go ahead and create that subnet and so that is all set up now um let's see if our route table is hooked up so our route table says where it links to and it says to local so it's not going anywhere and that's because we need to attach a internet gateway that allows us to reach the internet so if we go over here and create a new internet gateway we'll say my igw and we'll go ahead and create that and what we'll do is associate that with our vpc we created here okay and so now that we have the internet gateway attached we want that subnet to make its way out to the internet so if we go to the route table we can edit the route table association here i like how it keeps on showing me this as if i don't know what i'm doing but i do and so this would change that particular association but i want to add to that route table so i thought when i clicked that it would allow me to add more but apparently i got to go to route tables over here and i'm looking for the one that is ours we can see that it's over here you could even name it if we wanted to like my route table notice that we apply names it's actually just applying a tag see over here it's always what that is so go over to routes and we want to edit the routes and we want to add a route and we want this to go to zero zero zero and we're gonna choose the internet gateway okay we're gonna say save changes and what that's going to allow us to do is to reach the internet um and so what i want to do is go back to subnet i was just curious about this i've never used this before um so it looks like we could just choose some options here i'm not too concerned about that but i assume like that's used for debugging azure's had those kind of services for a long time and so it was been starting to add those so you can easily debug your network which is nice so we have a subnet the subnet can reach the internet because there's a there's a internet gateway and it's hooked up via the route table one thing that matters is will it assign a public ip address so that is something that we might want to look into it's not the default subnet which is totally fine so it says auto assign is no so that might be something that you might want to change so here we would go to edit the rope table association no it's not there they changed it on me it used to be part of the setup instructions you should just checkbox it now and they moved it modify the autoassign so we'll say enable so that means it's always going to give it a public ip address on launch and while we're here i'm just going to double check if i have any elastic ips i did not release okay just double checking here and so this is all set up and we should be able to launch a ec2 now within our our new vpc so i'll go over here to ec2 okay and i'm going to launch a new instance let's say amazon links 2 we're going to choose this tier here and now what we should be able to do is select that and that is our subnet there okay go ahead and launch that i don't care if we use a key whatsoever so i'm gonna go ahead and launch that there okay we'll go back and so there you go it is launching so we created our vpc and we launched uh in it no problem whatsoever so hopefully that is pretty darn clear um so yeah what i'm going to do is i'm going to let that launch because i want to show you security groups so within aws you can set security groups and knackles and that's going to allow or deny access based on stuff and when we launched this ecto instance it has a default security group that was assigned we could have created a new one but what i might want to do is create myself a new security group here okay and you can end up with a lot really fast like here's a bunch and i can't even tell what's what so like this bunch for load balancers and things like that and so i might just go ahead and delete a bunch of these because i cannot tell what is going on here and we'll delete these security groups and sometimes they won't let you delete them because they're associated with something like a network interface or something all right but we need to find out which one we're using right now so the one that we are using is the launch wizard4 so we'll go into here and i don't know if you can rename them after they've been created i don't think so which is kind of frustrating because if you want to rename it it's like i don't want that to be the name so what's interesting is you can go here and you can edit the routes the rules sorry the inbound rules and the outbound rules and so here it's open on port 22 so that allows us to ssh in we could drop this down and choose different things so if we want people to access a website we go port 80 and we save from anywhere ipv46 so now anyone can access it you might want to do something like give it access to postgres that runs on port 5432 things like that um could be something else like maybe you need to connect a redshift that's on that port you can go ahead and save those rules we're just going to say uh from anywhere it can even say my ip so maybe only i'm allowed to connect to it right so you added inbound rules you don't really ever have to touch outbound rules it's set for all traffic so it's stuff that's leaving uh the that there one interesting thing to note about security groups is that you don't have a deny option right so let's say you only wanted a particular ip address you only wanted let's say what's my ip my ip address so that is my ip address and let's say i wanted to block it right so i go here and i say okay i want to block on all tcp i want to block this number right but i can't do that all i can say is i allow this number so in order to do it i would have to enter everything but this number in here and you can enter ranges in with like these forward slashes and stuff like that but you would imagine that'd be really hard because you have to starting to like you'd have to start and go through every single ip address in the world to get it out of here and that's almost impossible and that's the key thing i want to remember about security groups so that's security groups and there's also knackles knackles they're associated with subnets so they probably show up under vpc i rarely touch knackles rarely ever have to um i mean they're great tools but you know for me i just don't ever need them so knackles are associated with subnets so we can go here and try to see my subnet tutorial so we created our subnet we got a knackle for free and we can set inbound and outbound rules and so here here is where we could say okay i want to add a new rule and i want to and i want to make the rule number 150 you always do these in the hundreds okay or the power of tens so that you can move them around easily and i can say all traffic that comes from this ip address i'm gonna put the forward slash zero that just means a single ip address and i say deny right and so now uh this at my address i can't access that ec2 instance okay if i try to go there's nothing running on the server but if i was to try to use it i wouldn't be able to do it and and this applies to anything for that subnet it's not for a particular instance it's for anything in that subnet so hopefully that is is pretty clear there but that's pretty much all you really need to know i mean there's lots of other stuff like network firewalls all these other things it gets pretty complicated it's well beyond what we need to learn here but what we'll do is tear down that ec2 instance okay we'll terminate that and once that instance is destroyed we can get rid of our security group and a bunch of other stuff and there's always a bunch of these darn things so we'll say delete one security group associated so we go here this is the one we are using but i wanna get rid of all these other ones okay if i go here it could be because like of inbound rules so see this one because you can reference another security group within a security group so i'm just going to go save that there see any my ip there oops it's set to nfs so that might have been set up for our access point i could just delete it that would probably be easier okay so that's one that's kind of of a pain so i'm just looking for rules that might be referencing other security groups to get rid of them okay let's try this again we'll go ahead and delete i'm leaving the um i'm leaving the uh the defaults alone because those come with your vpcs and you don't want to get rid of those it won't let me delete this one so i'm going to go edit that rule delete it save it you might not have this kind of cleanup to do it's just might be me here you know um outbound inbound let's try this again here delete and i'll open this one up must be this one that is referencing the other one i'm just going to delete the rule and this is something that's just kind of frustrating with aws but it's just how it is where sometimes it's hard to get rid of resources because you have to click through stuff so it's not always a clean you might have like lingering resources and this isn't going to cost us anything but it's just the fact that um that it just makes things harder to see what you're doing you know this last one really doesn't want to go away so i'm just trying to delete all the rules out of here get rid of it can i delete this one now one group associated it will not show me what it's talking about okay here it is um ah okay this is referencing it it was the one there was an old one i don't know what this is we'll go down here and we'll go here and delete that and while i've been cleaning all these up now we can go over to our instance make sure that it's terminated it is good because if our instance is not terminated we cannot destroy the vpc uh prior the vpc could not be destroyed unless you detach the internet gateway i wonder if it's going to still complain about that we'll say yes it actually looks like it includes it in the cleanup type delete here there we go so we're all good we're all cleaned up there you are hey this is andrew brown from exam pro and in this video i just want to show you cloud front so let's make our way over to cloudfront and cloudfront is a content delivery network and it's used to cache your data all over the place as you can see i have some older ones here if you have a splash screen what you can do is just look for the lefthand side there might be a hamburger menu open that up and then click on distributions and what we're going to do is create a new distribution if you don't want to create one because these do take forever to create you can just kind of watch along i don't even feel like i'm going to hit the um the create distribution button because i just hate waiting for so long but the idea is that you have to choose an origin and so the origin could be something like an s3 bucket a load bouncer media store this is where um the the content distribution network is going to source its content right so if i say this bucket here and i just it will probably default to the root path the idea is that it's going to be able to pull content from there and then cache it everywhere and then down below you can say okay set the type of protocol redirect to here you can set up caching rules or like how often do you want it to cache like cache a lot don't cache a lot but the great thing is like you have these edge or these um lambda edge functions so you can read and modify the requests and response to the cdn which is very powerful but what i'm going to do is i'm just going to go look at what we already have because again i said they take forever to spin up and we're not going to see too much if we do so once it's spun up this is what it looks like so you'll have an origin it says where it's pointing to you can create multiple origins group them uh you can modify your behavior so that was basically what we're looking at before as you can see we have our behavior there nothing super exciting we can set up error pages you can restrict based on geographical locations so if you're for whatever reason if you if you're not allowed to serve content in uk you could say exclude this geographical region right so you have an allow list or a block list saying like okay we can't do uk because like let's say you just don't want to do um england you don't want to do um uh gdpr for whatever reason you could block out i don't know i'm having a hard time here britain england it's england right united kingdom there we go so you just say okay forget united kingdom i don't have to do gdpr now uh for invalidations the idea is that you know it is a cache so things can get stale or just persist and so here you can just type in say i want to get rid of image.jpg and then you create that invalidation and then it will go delete it out of the cache and so the next time someone requests they'll get the fresh content this usually doesn't take that long but that's pretty much cloudfront in a nutshell okay hey this is andrew brown from exam pro and we are taking a look at ec2 also known as elastic compute cloud and so this is a highly configurable virtual server or it's also known as a virtual machine and that's what we're going to generally refer to it uh ec2 is resizable compute capacity it takes minutes to launch new instances and anything and everything on database uses ec2 instances underneath that's why we generally call it the backbone to all the eight of the services and uh you're gonna just have to choose a few options here so the first thing you'll need to do is choose your os via your amazon machine image so that's where you get red hat ubuntu windows amazon linux zeus it might also come with preinstalled libraries and things like that then you can choose your instance type that's going to determine things like your vcpus your memory so here you can see how many there are and you'll have like a monthly cost and that's the name of the instance type then you have to add storage so very commonly you're attaching elastic block storage or elastic files system or service and so you know if you do choose your ebs you are going to have to determine what type it is so whether it's a solid state drive a hard disk drive a virtual magnetic tape or even attaching multiple volumes not just a single one and the last thing is configuring your instance so this is configuring the security groups the key pairs user data im rolls placement groups all sorts of things so we will experience in that because we will show you how to launch an ec2 instance and it'll make a lot of sense if it does not make sense right now okay all right let's take a look here at ec2 instance families so what are instance families well instant families are different combinations of cpu memory storage and networking capacity and instance families allow you to choose the appropriate combination of capacity to meet your application's unique requirements different instance families are different because of the varying hardware used to give them their unique properties and we do talk about this thing about uh capacity reservation where aws can actually run out of a particular type of instance family because they just don't have enough hardware in that data center so you have to reserve it but let's go through the different types of instance families the first is general purpose and these are the names of the different families uh very popular ones is the t2 the t2 and one that's really interesting is the mac which actually allows you to run a mac server so these are great balance of compute memory and network resources so you're going to be using these most of the time the use cases here would be web servers code repositories things like that then you have compute optimize so um they all start with c uh no surprise there they're ideal for compute bound applications that benefit from high performance processor thread cases here are scientific modeling dedicated gaming servers ad server engines things like that then you have memory optimized um and so there's a variety here these are fast performance for workloads that process large data sets and memory they're great for inmemory caches and memory databases realtime big data analytics then you have accelerated optimize so this is your p2 p3 p4 things like that these are hardware accelerators or coprocessors these are great for machine learning computational finance seismic analysis speech recognition if you're doing um uh ml on aws you'll start coming across these types aws technically has a separate page on sagemaker ml machines but they're all pulling from these instance families okay then we have storage optimized so i3 i3en things like that these are highly high sequential read and write access to very large data sets on local storage the use cases here would be nosql in memory or transactional databases data warehousing for the certified cloud partitioner you just need to generally know these five categories not the names of the instance families if you're doing associates or above you definitely want to know these things in a bit more detail and i want to say that commonly instant families are called instance types but an instance type is a combination of size and family but even aws documentation doesn't make this family distinction clear but i know this because you know in azure they make that very clear and and gcp and so i'm bringing that language over here to just kind of normalize it for you okay let's take a look at what ec2 instance types are so an instance type is a particular instant size and instance family and a common pattern for instance sizes you'll see is things like nano micro small medium large x large 2x large 4x large 8x large and you know generally they're you to the power of twos but sometimes it'll be like 12 14 16 where it's even uh and so when you go to launch your ec2 instance you're going to have to choose that instance type and so here you can see you know here's our ttmicro and then we have um the small the bdm the large the x large okay but there are exceptions to this pattern for sizes so you know there is one particular one called uh dot metal and so that's going to indicate that this is a bare metal machine and then sometimes you get these oddball ones like 9x large so you know the rule of power of two or even numbers is not always the case uh but generally it'll be pretty even for you know the start here okay uh just talking about instant sizes so the easy two instance sizes generally double in price and attributes so uh just bringing up these numbers a little bit closer starting at the small here you're gonna notice one two it doesn't maybe double there but four and here we see twelve twenty four uh almost doubles there almost doubles there but i want to show you that the price is generally almost double so 16 33 67 135 and so a lot of times like you always have the option to say okay do i want to go to the next instance size up or have an additional distance of the same size and sometimes it's a better approach to get an additional instance because then you can distribute it across another az but then you also meet additional capacity so there you go so we talked about dedicated instances and hosts a little bit but let's just make that distinction very clear so dedicated hosts are single tenant ec2 instances designed to let you bring your own license so byol based on machine characteristics and so we'll compare the dedicated instance to the dedicated host across isolation billing physical characteristics visibility affinity between a host and instance targeted instance placement automatic instance placement and add capacity using allocation request so for isolation for dedicated instance you're going to get instance isolation so you can have the same customer on the same physical machine but there is virtualization there for them and there's a guarantee of that for a dedicated host you have physical server isolation so you get the whole server for billing uh on a dedicated instance it's per instance billing and it's gonna have an additional fee of two dollars per region and for dedicated hosts it's per host billing so it's a lot more expensive but you get the whole machine uh for visibility of physical characteristics you're not going to get any of that information for a dedicated instance for dedicated hosts you are such as sockets core host host id and this is really important when you have a bring your own license and they're saying this license is for x amount of cores or x amount of sockets then we have affinity so there's no affinity for dedicated instance for dedicated hosts you'll have consistency with deploys to the same instance the same physical server there's no control of target instance placement for dedicated instance you do have control on a dedicated host for automatic instance placements you have it for both and to add capacity using allocation requests it's a no for dedicated instance and it's a yes for dedicated host so i want to come back to the main point that's what's highlighted here is that on a dedicated host you have visibility of sockets core host id and this is really really important when you're bringing your own license byol such as you know microsoft sql servers where you have to specify the mana cores and things like that okay so we've been talking about uh tendency and i just wanted to make it very clear uh the difference between the different levels of tendency on aws so we have three okay so we got dedicated hosts so your server lives here and you have control of the physical attribute so basically the whole server okay then we have dedicated instances so your server is on the same uh physical machine as other customers but the actual slot that you have the dedicated instance will always be the same and then we have the default so your instance will live somewhere on the server uh and when you reboot it's going to be somewhere else so there's no guarantee that it's going to be in the same place every single time okay hey this is andrew brown from exam pro and in this follow along we're going to be looking at ec2 and also um services that are adjacent to it so like auto scaling groups load bouncers elastic ips things like that so we fully understand ec2 you don't have to know tons for the exam but you should be able to go through the motions of this with me so you can cement that knowledge for some of those deeper concepts like working with key pairs and things like that so let's make our way over to the ec2 console and learn what we can learn um and generally when you go over the ec2 console it'll bring it to the dashboard for whatever reason to bring me there and then the idea here is that on the left hand side we can make our way over to instances okay and this is where we can launch our first instance so we go here and launch our instance the first thing we're going to be presented with is to choose our mi or amazon machine image and so that is a template that contains the software configuration so the operating system applications and other binaries that would be installed on that os by default all right and so we have a variety that we can choose from in the quick starts and generally the ones that you're going to see first are the ones that it'll support so there are amis or operating systems that aws will support when you contact them and then there's ones that are outside that where they'll still help you with but they might not have the knowledge on so just understand that if you pick from these core ones you're going to be in good shape the most popular is the amazon linux 2 because it's part of the free tier and it is very minimal and well hardened by aws so it's a very good choice there but you can see you can install a bunch of things so like if you want to launch a mac os server you can absolutely do that a red hat susie ubuntu a windows server you name it they have it if you wanted something more farther out there you can go to the marketplace and subscribe to one that is managed by company basically everything exists under the sun here or you can get a community ami so these are ones that are contributed by the community for free but we're going to go back to quickstart here and what i want you to notice is that there is this ami id that's how we can uniquely identify what we're using if we were to change region even with the same amazon x2 instance this thing will change so just understand that it is regional based and it comes in a 64bit variant and a arm variant and so we're going to be using the x86 here you can notice here you can change it on the right hand side we're going to stick with x86 i'm going to go ahead and hit next so now we're going to choose our instance type and so this is going to decide um greatly how much we're going to be spending because the larger it is the more we're going to spend so see this t2 micro if we wandered into the pricing for that we go to ec2 pricing aws and once we get to ec2 pricing we want to go to on demand and from here this will load and so down below we can kind of go find our price it should show us it should show us the list ah here it is okay so i can say a t2 micro and we can see the on demand is this so it seems really cheap what you got to do is do the math so if you do time 730 that's how many hours there are in a month if we launch a ttmicro and let's say we didn't have the free tier we you do if you first made your account you're going to have 700 750 hours for free for the free tier but if you didn't it would only cost you eight dollars and and 46 cents usd okay so just be aware of that if you ever need to figure something out go there copy it do the math 730 it's pretty easy so here we have a t2 micro in the t2 family it's going to have one v v cpu notice it has a v for virtual so there could be more than a single cpu on the underlying hardware but we're only going to have access to one virtual cpu we have one gigabyte of memory it's for low to moderate network performance so that's a factor that can change if you need like gigabit stuff like really fast connections for onprem hybrid connections and you have specialized servers for that but for this this is fine the ct micro is great uh if you want you can also search this way to see all the instance families and things like that you can filter for current generations all generations so this is fine okay so from there we're going to go to configure our instance type you can say let's launch multiples of these instances let's turn on spot to save money and try to bid for a particular price we can change our vpc it's going to default to the default vpc um if you have no subnets just going to pick one at random here which is fine whether to auto assign a public ip address if you do not have an ip address you cannot reach the internet so generally you want this to be enabled this is dependent on the subnet whether it will default to enabled but it doesn't matter if you have an ec2 instance in a private or public subnet you can always override this and give it a public ip address you have placement groups which allows you to place servers together closely not something for the certified cloud partitioner there's capacity reservations so if you're worried about database running out of this you can reserve capacity so that's kind of interesting domain join directory this isn't something that i've done much with but i imagine that has something to do with um direct active directory or something like that to join information then you need to uh have an im role and we absolutely do need an item rule here so what i want you to do is create a new role it's going to close off these other tabs here and we will go wait a moment create a new role here and we want to do this for ec2 so we say ec2 is what we're creating the rule for we'll hit next and i don't know if i have a policy but i'm gonna go ahead and um oh well i don't need to make a new policy but i just want ssm and the reason i want ssm is so that i can um use sessions manager to log in so we don't have to use key pairs we will use key pairs but if we didn't want to use it that's what we could do and this used to be the old roll it'll tell you hey go use this new one here so i just want to make sure i know which one it is and so we'll just checkbox that on we'll hit next we can add tags right here it'd be well actually we don't need to add any tags here so that's fine we'll sit next and then i'll just say my ssm ec2 role okay and we'll create that role and now that we have created that role we can go back to our first tab here and give this a refresh and then drop down and it should show up here if we go down here a little bit we could turn on extra monitoring there is monitoring built in but if you wanted to monitor it to a lower uh like it more frequently you could do that as well we want share tenancy right this is where you change the dedicated instance or dedicated host obviously these costs more but we're gonna stick with shared elastic conference so this is for um uh attaching a a fractional gpu great for ml not something that we want there's credit specification i don't remember seeing this before selecting unlimited for credit specification allows for uh to burst beyond the baseline so it's for bursting here you can attach an uh efs so if you need a file system that you want to mount or attach um then there's the enclave option so nitro enclave enables you to create isolated compute environments to further protect your and securely process highly sensitive data so it might be something you might want to checkbox on um based on your use case and then down below are we have the ability to enter our user data and this is something we want to do because we want to install apache so that we have something to work with here so what i'm going to do is make a shebang so that is a pound and an exclamation mark i know that's really small so i'll try to bump up my font here so you can see what i'm doing and we're going to do a forward slash bin and a forward slash bash on the next line here we're going to do yum install hyphen y httpd that's going to install apache and why it's not called apache i don't know why but they call it httpd there's no apache in the name there and so we'll say systemctl start httpd system ctl enable httpd so we're saying startup apache and then make sure that it stays running if we restart our machine very simple so from there we will go to our storage we'll say add or storage and this is at 8 gigabytes by default we could turn that up to 30 if we like so you can go all the way up to 30 if you like and you might want to do that but i'm going to leave it at 8. we could change our volume type i'm fine with gp2 because that's very cost effective and if we want to turn encryption and you should always turn on encryption there's no reason not to and so we'll turn that on it's not like it's going to cost you more it's going to be the same cost it's just your choice there if we want to add a tag yes we're going to add a name and we're going to say my ec2 instance okay and so that's going to give us a name which is something we would really like to have then we have a security group i'm going to just create a new query book called my ec2sg here and you'll say my ec2 sg something you cannot do is rename a security group once you've made it so make sure you don't make a spelling mistake up here and we want to be uh accessing that http http or it's going to launch a website so in order to do that we need to make sure we have http as the type with the port 80 open and we want it from anywhere so we'll say anywhere and that will be 0.0.0.0.0 and that's for the ipv4 this is for the ipv6 okay so we'll just say internet and this is for ssh right and for this i would probably suggest to say my ip but since we might be using a cloud shell to do that we're going to leave it as anywhere so that we don't have any issues connecting so from here we'll review and launch and you can review what it is that's going on here it's going to say here hey you have an open port that's okay we we want the internet to see our website because that's the whole point there and we'll go ahead and launch it it's going to ask for a key pair we can go down and say proceed without key pair but what i'm going to do is i'm going to create a new key pair because i want to show you how those work and i'm sure we've already done in this course once but we'll do it again and so i'm going to just name this as my ec2 instance here and then we're going to go download that key pair it's going to download a pem file there and so now we can go ahead and launch that instance and while that is launching so i'm going to just close this other tab here we're going to click on the view instances and so here is that instance that's why we put the tag so we can have a name there we're going to wait for that to start but as that's going i'm going to make a new tab by just right clicking here on the logo click anywhere pretty much to do that and once we do that we'll click on cloud shell and as that is going what i want to do is take this pim down below i'm going to move it to my desktop to make it easier for me to upload i'm doing this off screen okay and uh once this environment's running i'm going to go ahead and upload that okay so we'll just give it a moment to do that we're also waiting for the server to spin up as you'll notice there is a public ip address here it says it's running so if we want we can copy it we're looking for those two checks to pass so the server could be available but generally you want to wait for those two system checks because one says hey the hardware is fine the network's fine things like that okay but if i take that ip address paste it on and up here we have the web page so that is working uh no problem there so that's great and we'll go over to cloud shell and that is still starting uh it's not the fastest but that's just how it is and um you know we'll get going here in a second as soon as this decides to load there we go so it's loaded i can type clear here just to clear that screen out and so what i want to do is upload that pem file so i'm going to go and upload that file we're going to go ahead and select it i'm going to go to my desktop here whoops my desktop and we are going to choose my ec2 instance pem all right and from there we'll hit upload that's going to upload that pem file once that is uploaded we're going to do ls okay and so this is from a previous tutorial so i'm going to go ahead and just delete that other one there we'll say remove efs example pem yes okay we'll type clear and then what we can do here is type in chamod and i believe it's 400 and what do we call this my ec2 instance pem if you hit tab it will auto complete which is nice and if you do ls hyphen la we can take a look at that file and see it should look like this should have only one r here so the idea is you're locking it down so it's not writable or executable it's just readable because that's what you have to have it if you want to ssh and so if we want ssh what we'll do is hit the connect button here and we have four options they just give you too many options it's gonna be a fifth one for sure soon but right now we're talking about ssh so for ssh um we had the chamod or file which we did and then we need to use this dns to connect to it and so this is the full line here if you click on this copy that over and paste it in that should be everything and notice we're doing ec2 user followed by this you could put the ip address in here it said if you preferred so if you were over here you could go and take that ip address which is i think shorter nicer but um you know if you just click that one button it works that's fine you always have to accept the fingerprint then you'll be inside the instance you can type who am i to see which user you are you're the ec2 user that's the user that aws creates for their amazon linux instances it's going to vary per ami so not all amis have an ec2 user it might be something else but that's generally the ones that adas uses for their supported ones and so if we do um an ls again we're in the server right now we can tell because it says right here or if we do a pwd we can kind of just kind of look around so i think it's going to be at var ww that's where ht httpd or apache always puts their files here so i go in here whoops i'm just looking for the index file so i thought the index file was in cd bar www hmm html well where the heck is it so i'm going to just touch a file here and see if it overrides it oh i don't care i'll just type sudo and what we can do is just try to restart this system ctl um there's a very similar command that's like uh service and so i always forget the order of it so i think it'd be i'm just checking probably restart httpd and so fail to restart the policy was not provided as the name service service uh maybe sudo there we go and so if we go back here i'm gonna see if it changed because it will take whatever is in the index.html file so if there's no file there it's going to show that there and so what i can do is i can edit this file i'm going to type vi index html and um i'm going to hit i for insert mode oh it says it's read only so what we have to do cue colon cue quit oops clear ls and so what we need to do is do sudo vi index html and so vim every single key is a hot key okay um i'm not teaching vim here but i'm going to teach you the basics but the idea is that when you're here notice that the cursor is blinking when i hit i it enters insert mode now i can type normally so i'd say hello uh hello cloud okay and i'm gonna hit escape to go back to um navigation mode whatever you wanna call it i'm gonna hit colon so it brings up the command i'm gonna type in uh write and quit okay and hit enter and so i'll type clear and so oops clear and so we'll hit up till we get that command sudo systemctl restart httpd we'll hit that hit enter okay and it should restart pretty fast there it is this is hello cloud i probably didn't even have to restart it to do that but anyway so now that instance uh you can see how we're updating that so what i want to do is just do a sanity check and make sure that if we restart this instance that we're going to be able to have apache running that's something you should always do if you have an app and you or anything you install it restart your server make sure that everything works so what i'm going to do is uh just hit exit here so we go back to the top level cloud shell type clear i'm going to go back over to my ec2 instance i'm gonna have to click around to find it here and what i want to do is reboot it okay and if i reboot the machine the ip address is going to stay the same okay so if we reboot it the ip address is going to stay the same and the reboot's going to happen really fast if we want to observe that reboot we could go over to um here on the right hand side go to the system log and it would show us that it it had rebooted i think so yeah it does cloud in it there i think it rebooted not sure but anyway if it's rebooted then we can go ahead and connect and make sure everything's fine so let's just go here and hit enter and let's see if the what the webpage is here notice that it's hanging right so it's probably because it's still restarting even though it doesn't look like it is and that's something that you have to understand about the cloud is that you have to think about what you're doing and have confidence that it is happening and also just double check it but uh that's something that can be kind of frustrating because these are globally available services uh they're massively scalable and so one of the tradeoffs is that you don't always have the most uh responsive uh uis aws has one of the most responsive uis out of all the major providers but even still like sometimes i have to secondguess myself but the page right now is not working now it is so it's fine so it just took time for that to reboot and so um what i want to do is connect a different way so we're going to go here and we're going to hit um we're going to checkbox that on we're going to hit connect and instead of using ssh client we're just going to go to sessions manager and hit connect and this is the preferred way of connecting because you don't have to have this this ssh key and that's a lot more secure because if someone has that key and you you know you hand it to someone they could hand it to somebody else and then you have a big problem on your hands so here this looks very similar but if you type who am i it actually logs in as the ssm user which is kind of annoying so i type in sudo su i have to do this hyphen here and then i'm going to say the user i want to be which is ec2 user and then if i type umi we are the correct user you can't do anything in that ssm hyphen user or ssm user so you got to switch that over and i can bump this up to make it a bit larger so this is obviously not as nice as working over here or even in your own terminal but it's a lot more secure and it's tracked and all these other things so we really should be using it okay and um i really don't like having to bump this up with my html i'm just go back to zero there there's probably a way to configure that but anyway let's just go and take a look at our file i'm gonna type buy again and we're gonna do var www.html index html i could put pseudo in front of there and again remember you have to hit i to go into insert mode and what i'm going to do is just capitalize that hello cloud give that exclamation mark colon wq to quit right quit i'm going to go back here refresh okay so we don't have to restart our server which is nice all right so um that's that that's pretty clear so i'll hit terminate here and i don't think we need cloud shell for anything so i'm just gonna close that and so that's pretty much it when it when it comes to working with an ec2 instance and so the next thing i want to show you is elastic ip okay okay so now i want to show you elastic ip commonly abbreviated to eip and so all that is it's just a a static ip and ip that does not change because this ec2 instance here notice that it's 54 163 4 104 and what would happen if we were to stop this instance not reboot it but stop it because for whatever reason we had to or or um for whatever reason and if we were to stop this instance and we were to restart it okay and we have to wait for it to stop but that ip address is going to change okay so 54 163 4104 hopefully we can observe that i'm just going to write that down so we do not forget so i can prove to you that it does change and now that it it's still stopping here so as that's stopping we're just going to go ahead and get our elastic ip and i will prove that as we go here so i'm going to go over to here and so what i want to do is reserve or allocate an elastic ip address and so i'm going to say us east 1 and it's going to say from the amazon pool of ipv4 addresses so eight of us has a bunch of ip addresses they're holding on to and so you can just allocate one and once you've allocated that's your ip address so coming back to here okay this has stopped notice there is no public ip address we're going to start it again okay and then we'll just checkbox it on and we just have to wait a little while to see what the ip address is going to be i'm going to tell you it's going to be something else so if i go back here this is 54 235 12 110 and our original one was 54 163 for 104. so the reason why it's important to have the same address is that if uh you have a load balancer well not a load bouncer but if you have a domain pointing to your i your server and you reboot then the route you have a dangling um a path or route where revenue 3 which is going to be pointing to nothing and so it was does have things to mitigate that like aliases and things like that but in general you know there's cases where you just have to have a static ip address and so we had allocated one over here and if we want to assign it we're going to associate that elastic ip address we're going to drop it down choose the cc2 instance um i suppose the private ips as well and then we're going to go ahead and hit allocate or associate and once it's associated it should now have 34 199 121 116. so we go over here and we're going to take a look here and that's its ip address we can pull it up okay and that's that so yeah that's elastic ip okay so now that we have our elastic ip we have our ec2 instance running let's say um you know we lose the server we terminate it so we would lose all of our configuration so if we wanted to bake this ami to save it for later what we'd have to do is go and create an image so to do that we go to the top here and we go to images and templates and we can create an image or we can create a a template which is a lot better but for the time being we're going to go ahead and create an image and when you create an image you're basically creating an ami and so here i'm just going to say my ec2 and i'm going to 000 to just kind of like number it so that's a very common numbering just do three zeros and then increment by one and so here i'm going to say my apache server and so it's going to save some settings like the fact that there is a volume you could save some tags there and so i might go ahead and add a tag and it'll say name and we'll just say my ec2 server or so that it remembers that okay and then what we'll do is go ahead and create our image and so this can take a little bit of time if we go over to uh images here it's going to be spinning for a while and we'll just wait until it's done okay all right so after waiting a little while here our ami is ready so we're just waiting for it to go available if you do not see it just make sure you hit the refresh because sometimes aws will just spin forever and so that's just something you'll have to do so you know hopefully that makes sense what we'll do is go make our way back over to instances here and we can launch one this way well actually we can do it over from the ami page so what i'm going to do is just terminate this instance we're all done with it okay and we'll hit terminate it's totally fine and it had a message about elastic ips about releasing them so when it does that the elastic ip is still over here so it did not release it so what we're going to do is go ahead and disassociate the elastic ip okay and then we're also going to release the ip address because if we don't we're going to have this ip addresses sticking around that we're not using it this is going to charge us a dollar a month over month so just be aware of those because that's just kind of like a hidden cost there but what we're going to do is go over to ami and we're going to select it here we're going to go to actions we're going to go ahead and launch and what it's going to do is make us fill all this other stuff again so if you had made a launch template we wouldn't have to fill out all the stuff it'd be part of it but that's what i'm trying to show you with this ami stuff so instead of filling out all this what i'm going to do is now go create a launch template just to kind of show you that that would be a much easier way to work so we go over to ec2 instances and then on the lefthand side we're looking for a launch template launch launch configurations is the old thing um launch templates here we go so what we'll do is create ourselves a launch template we'll just say my apache server and then down below we need to choose our ami so we're going to go here and we need to type it in so what did we call it my ec2 i really don't like this search here it's very slow and frustrating but once we find it whoops that's why i don't like it because a lot of times you'll be loading and you'll end up clicking the wrong thing okay so uh i don't like this okay we'll type in my give it a second there it is and just wait because it will keep loading and then once it's loaded hit enter and so it has that instance selected and then from there uh don't include in the launch template so here we could be explicit i would say i want this to be t2 micro but we could exclude it if we wanted to we could specify the key pair here um not that we really want to use key pairs we'll say my ec2 instance then down down here for the networking we can specify that security group we created so we created one here called myec2sg um storage is fine it's going to be encrypted network interface is fine advanced details what i want to do is set the i am instance profile that's really important because we don't want to have to figure out that role every single time so put that there and that should be everything and we could put user data in there but it's already baked into our ami so we don't have to worry about anything so what i'm going to do here is go ahead and create this launch template and then we're going to view this launch template and so now what we can do is then use it to launch an instance okay and so we're going to look here and it's very similar to dc tube except it's vertical so we're going to have one instance it's going to use that ami that instance type so you can see how you can override them which is nice we're going to check the advanced details and make sure that iom profile is set and we'll go ahead and launch this from a template so from there we can go ahead and click the instance value there and just be aware that when you do click through links like that you'll end up with a search so i was just check box that off so i can see what i'm doing and so we're just waiting for this instance to show up and the only thing i noticed is it didn't set our darn tags so i wanted the name in there and i think it's because we set it in the ami but it didn't carry over to the launch template so i'd have to go back to the launch template and update it probably so if i go into here into the launch template we can probably modify create a new version and then add tags there so we say name my apache server i realize i'm changing between them and so that should allow us to have a version two so we'll create that and but anyway that will be for the next time we launch it okay and so this instance is running i'm gonna go grab the ip address the server may or may not be ready we'll take a look here and so it's just spinning if it's spinning it's either the server is not ready or um our port's not open so it was just getting ready to work there so it is working now so that is our launch template so now you know we don't have to worry about losing our stuff and if we need to make new versions we can just bake new amis and increment them and attach them as new versions of the launch template okay all right so what i want to show you in this follow along is to set up an auto scaling group for our ec2 instance and the idea behind this is that we'll be able to always ensure that a single server is running or increase the capacity if the demand requires it so in order to create an auto scaling group we can go all the way down below to here and so you know i really don't like the autoscaling group form but it's okay we'll work our way through it so the first thing is we'll have to create our or name our auto screen group so let's just say my asg and then we'll have to select a launch template which is great because we already have one and then we'll have to select the version i'm going to select version two so that it applies that tag name and we'll go to next and so here it's going to need to select a vpc and then we need some subnets so we're going to choose three just because to have high availability you have to be running at least three different availability zones so that's why we have three different subnets and then down below we have the instance type requirements so uh t2 micro launch template looks good to me so we'll go ahead and hit next and then from here we can choose to do a load balancer and so i want to do the load balancer separate so we won't do it as of yet but very often if you're going to have an auto selling group you're going to usually have a load balancer but we'll talk about that when we get to that point there so we'll just go to the bottom here and hit next and so this is what's important so how many do you want to be always running and so we always want to have one and maybe the maximum capacity is two and you want the desired cast capacity to be around a particular number so if you had three and you said the desired is two there are things that could try to work to always make sure there's two but we just want to have one for this example we can set up scaling policy so i do target tracking scaling policy and so here we could do it based on a bunch of different things so if the cpu utilization went over 50 percent it would launch another server so that might be something we might want to set so we're not going to try to trigger the scaling policy but we might as well just apply because it's not too hard and you can also do a scaling scale in protection policy so if you want to make sure it does not reduce the amount of servers that's something you could do we could add a notification to say hey there's a scaling policy happening here which is fine we don't have to worry about that and there's tags so add text to help you search filter etc so i'm going to put a tag here i'm going to say name i'm just wondering if this is going to attach to the ec2 instance or this is for the auto scanning group you can optionally choose to add tags to instances by specifying tags in your launch template so we already did that so i don't need to put a tag here and so we can review our auto scaling group and go ahead and create that auto scaling group okay and so that auto scaling group expects there to be a single instance so what's going to do is it's going to start a launching an instance and so what i'm going to do is just get rid of this old server because we don't need it anymore this old one here okay and you can already see okay that the load balancer is launching this new one here and remember we updated our version two to have that name so that's how we know that it is so if we go back over to our auto scaling group okay it's now saying there's an instance we don't have a status as of yet and so there are ways of doing uh status checks to for it to determine whether or not the server is working because if the server is unhealthy what it would do is it would actually kill it and then start up a new one right so if i go down below it's right now doing the ec2 health check and the ec2 health check just means that is the server working right is it running it doesn't necessarily mean like hey can i load this web app um but you know it's very simple so we'll give it a moment here to start up and just make sure that it's working okay and i think it's ready so if i take that public ip address here and paste it in there it is okay so if we were to tell it to increase the capacity to three then what it would do is it would launch three and then it should probably launch it all evenly to those other it should evenly launch it to all those other uh availability zones and then we'll have something that is highly available okay so that's pretty much it for this and then we'll move on to auto scaling groups all right so we have our ec2 instance now managed by an auto screen group and the great thing is that if we terminate this instance this auto discounting group will launch another uh instance to meet our particular capacity um the only thing though is that if we were to have multiple ec2 instances running like three of them um how would you distribute traffic to the mall right so you know you have an ip address coming in from the internet but let's say you want to evenly distribute it and that's where a load bouncer comes into play and even if you have a single server you should always have a load balancer because it just makes it a lot easier for you to scale when you need to and you it acts as an intermediate layer where you can attach a web application firewall you can attach an ssl certificate for free so there's a lot of reasons to have a load balancer so what we'll do is go down below on the lefthand side and we're going to make our way over to load bouncers and we're going to create ourselves a new load balancer so i'm going to hit create load balancer here and you're going to see we have a lot of options application load bouncer network load balancer gateway load balancer and then the classic load bouncer and so we are running an application so i'm going to create an application load balancer and here i'm going to say my alb for an application load balancer this is going to be internet facing it's going to be ipv4 we're going to let it launch in the default subnet and we're going to choose the same the same azs right so that we get the same subnets as our that are in our auto scanning group and that's really important okay and then here um you know we need to have a security group and i just feel like selecting the same one here because that should work no problem there and we want to make sure that we can listen on port 80 and then it's going to forward it to a a target group it looks like i might have a target group there from before so just to reduce that confusion you won't have this problem i'm just going to double check if that's true so do i have a target group from there before before yes i do that came from i'm not sure it might have been created by um elastic bean stock and wasn't deleted okay so i'll go back over to here just so there's less confusion and we were selecting our target group so we're going to create a new target group so we'll go over here and here you can choose whether it's instance ip lambda application load balancer so you could point it specifically to an ip address and so if it was a static ip address that would make sense uh apparently you can port uh point it directly to instances i don't remember seeing that option before i guess that makes sense yeah no sorry that makes sense because i would go to uh vpcs okay or sorry uh asgs autoscaling groups it's just that you're pointing them to auto screen groups you're not pointing them to instances so that's why that's confusing so i'm going to say my target group it'll be for port 80 here protocol http 1 is fine we want to be in the same vpc so that's fine as well and down below we have our health check and so the forward slash means that it's going to hit the index.html page and so if it gets back um something healthy and that that something healthy is going to be um port 80 then it's going to be considered good and then we can say the threshold of check so i'm just going to reduce this so it's not so crazy so we'll say three uh two and then ten okay and then it expects back a 200 which i think that's what we'll get back so we'll go ahead and hit next and so now we have our target group and it should register instances so it's saying hey we detected this and this fits the requirements for this so this is now uh this is now in this target group okay so we can go back over here and we can now drop down and choose oops hit the refresh button and choose our target group so i'm not seeing it here so i'm gonna go back over here oh we didn't create it okay and now we can go back hit refresh and there it is and yeah that looks all good so we'll go ahead and hit create load bouncer we can view the load balancers and these crate really fast if we scroll on up what we can do is now access our server through this dns name okay so we copy that paste that on in there does it work not as of yet so if it's not working there because we did say look at these instances another way is to directly associate your auto scaling group with the load balancer so if i go into here and we hit uh edit there is a way aha a little bouncer so we want to associate this way and we want to say this target group here and also while we're here we might as well set it to elb so it's going to use the elb check so that makes it so the auto scaling group if it wants to restart server it's going to use the elbs check which is a lot more sophisticated and then what we'll do is go hit update okay and now if we go back over to our load balancer i'm just going to close some of these tabs so it's a little less confusing a little bouncer here i think we should be able to see through here whether it is seeing it let's go down below listeners monitoring integrated services no it's going to be through the target group okay i mean it already had it there so maybe it's just that it hasn't finished the check so over here it has a health status check oh now it's healthy okay so if it's healthy in the target group and the load bouncer is pointing to it then it should technically work so we're going to go ahead and copy the dns again here make a new tab paste it in and there it is okay so that's how you're gonna access all your all your instances that are within your auto scanning groups you're gonna always go through the dns and so if you had a row 53 domain like your domain managed by aws you just point to the load balancer and that's how you hook it up so that's pretty much it so yeah there you go all right so there you go we learned everything we wanted to know about ec2 so the the last thing to do is to tear everything down so we have a load balancer we have an auto scanning group um and those are the two things we'll have to pull on down so the first thing would be to take down the auto scaling group and when you delete another scaling group it's going to delete all the ec2 instances so we'll do it that way if you tried to delete the ec2 it would just keep on spinning up so you have to delete that first and so as that's deleting then we'll be able to delete our load balancer i'm going to try anyway to see if i can delete it at the same time and so i'll go up here i'm going to go ahead and delete that load balancer actually it did work no problem i'm gonna make sure i don't have any elastic ips i'm gonna also make sure i don't have any key pairs you can keep your key pairs around but like i just want to kind of clean this up so okay okay and that instance should be terminating go back to the auto scan group here if we click into it we can check its activity here so it's just saying successful so it is waiting on elb connection draining which is kind of annoying because we deleted elb so there's nothing to drain um draining is just to make sure that uh you know there's no interruptions when terminating services so just trying to be smart about it and all i want to see is that it's just saying terminating over here and then i think we're done okay so we'll just have to wait a little while here okay and i'll see you back in a moment okay all right so after waiting a very long time it did destroy so if i go down over to my load balancer here we're gonna see that it does not exist so there was that connection draining thing which was kind of annoying it's probably because i deleted the load balancer first and then the um the uh the autoscaling group second and probably connection draining was turned on but it's not a big deal we just waited and it did eventually delete so we're pretty much all done here so there you go hey this is andrew brown from exam pro and we are taking a look at ec2 pricing models and there are five different ways to pay with ec2 remember each two are virtual machines so we have ondemand spot uh reserved dedicated and adamus savings plans so what we'll do is look at these in summary here and then we'll dive deep onto each of these different pricing models so for on demand you are paying the uh a low cost and also you have a lot of flexibility with this plan uh you are paying per hour so this is a payasyougo model uh or you could be paying down to the second which we'll talk about uh the caveats there when we get to the ondemand section this is suitable for workloads that are going to be shortterm spiky unpredictable workloads uh that cannot be interrupted and it's great for firsttime applications and the ondemand pricing model is great when you need the least amount of commitment for spot pricing you can see we can save up to 90 percent which is the greatest savings of out of all these models here uh the idea here is you're requesting spare computing capacity that database is not using and that's where you're gonna get that savings you have flexible start and end times but your workloads have to be able to handle interruptions because these servers can be stopped at any time to be giving to more priority customers and this is great for noncritical background jobs very common for like scientific computing where jobs can be started and stopped at any given time this has the greatest amount of savings then you have reserve or reserved instances this allows you to save up to 75 percent this is great for steady state or predictable usage you're committing with aws for ec2 usage over a period of one or three year terms you can resell on unused reserve instances so you're not totally stuck with this if you buy them this is great for the best long term savings then you have dedicated so these are just dedicated servers and technically not a pricing model but more so that the fact that it can be utilized with pricing models um but the idea here is it can be used with on demand reserved or even spot this is great when you need to have a guarantee of isolate hardware for enterprise requirements and this is going to be the most expensive so yeah there you go and we'll dive deep here okay so the ondemand pricing model is a payasyougo model where you consume compute and then you pay later so when you launch an ec2 instance by default you are using that ondemand pricing and ondemand has no upfront payment and no longterm commitment you are charged by the second up to a minimum of 60 seconds so technically a minute or the hour so let's just talk about the difference between those uh per second billing and those per hour billing so per second are for linux windows windows with sql enterprise windows with sql standard windows with sql web instances that do not have a separate hourly charge and then everything else is going to be um per hour and so you know when i'm launching ec2 instance i can't even tell when something's per second or per hour you just have to know that it has a separate hourly charge but generally you know if you're just launching things it's going to probably be the per second billing when you look up the hourly or the the pricing it's always shown in the hourly rate so even if it is using uh per second billing when you look up that pricing it's always going to show it to you like that but on your bill you'll see it down to the second okay up to the first 60 seconds and on demand is great for workloads that are shortterm spiky or unpredictable uh but when you have a new app development this is where you want to experiment and then when you're ready to uh start saving because you know exactly what that workload's going to be over the span of a year or three that's where we're going to get into reserved instances which we'll cover next hey this is andrew brown from exam pro and we are taking a look at reserved instances also known as ri and this is a bit of a complex topic but uh you know if we do get through it it's going to serve you well through multiple aw certifications so let's give it a bit of attention here so ri is designed for applications that have a steady state predictable usage or required reserve capacity so the idea is that you're saying to aws i'm going to make a guaranteed commitment saying this is what i'm going to use and i'm going to get savings because abuse knows that you're going to be spending that money okay so the idea here is that the reduced pricing is based on this kind of formula where we have term class offering the r a tributes and payment options technically the ra tributes don't exactly factor into it other the fact that they on our attribute could be like the instance type size but i'm going to put that in the formula there just because it is an important component so let's take a look at each of these components of the formula to understand how we're going to save so the first is the term so the term uh the idea here is the longer the term the greater the savings so you're committing to a one year or three year contract with aws um and one thing you need to know is that these do not renew so at the end of the year the idea is that you have to purchase again and when they do expire your instances are just going to flip back over to on demand with no interruptions to service then you have class offerings and so the idea here is the less flexible the offering the greater the savings so the first is standard and this is up to a 75 reduction in the price compared to on demand and the idea here is you can modify some ra attributes which we'll we'll talk about when we get to the um ra tribute section there then you have convertible so you save up to 54 reduced pricing compared to on demand and you can exchange uh ris based on the ri tributes if the value is greater or equal in value and there used to be a third class called schedule but this no longer exists so if you do come across it just know that abuse is not planning on offering this again for whatever reason i'm not sure why then there are the payment options so the greater upfront the greater the savings so here we have all upfront so full payment is made at the start of the term partial front so a portion of the cost must be paid up front and the remaining hours in the terms are billed at a discounted rate and then there's no upfront so you are billed at a discounted hourly rate for every hour within the term regardless of whether the reservation is being used and this is really great this last option here because basically you're saying to aws you're saying like i'm just going to pay my bill as usual but i'm going to just tell you what it's going to be and i'm going to save money so if you know that you're going to be using a t2 medium for the next year uh you can do that and you're just going to save money okay so ris can be shared between multiple accounts within an organization and unused rise can be sold in the reserved instance marketplace but we'll talk about the limitations around that when we get a bit deeper in here just to kind of show you what it would look like at the end of this console and they updated it i love this new ui here the idea here is you're going to filter based on your requirements and that's going to show you ris that are available and then you'll just choose the desired quantity you can see the pricing stuff there you're going to add it to cart you're going to check out and that's how you're going to purchase it okay so another factor to that formula were ri attributes and sometimes the documentation calls them r attributes sometimes they call them instance attributes but these are limited based on class offering and can be uh can affect the final price of the r instance and there are four rh attributes so the first is the instance type so this could be like an m4 large and this is composed of an instance family so the m4 and the instant size so large okay then you have the region so this is where the reserved instance is purchased then you have the tendency whether your instance runs on shared so the default which would be multitenant or a single tenant which would be dedicated hardware and then you have the platform whether you're using windows or linux even if you're using ondemand of course this would just affect your pricing but there are some limitations around here which we'll get into as we dive a bit deeper here with our eye okay all right let's compare regional and zonal ri so when you purchase an ri you have to determine the scope for it okay so this is not gonna affect your price but it's gonna affect the flexibility of the instance uh so this is something you have to decide so we're gonna talk about regional ri which is when you purchase it for a regional and zonal ri when you purchase it for an availability zone so when you purchase it for a regional ri it does not reserve capacity meaning that there's no guarantee that those servers will be available so if anybody runs out of those servers uh you're just not going to have them but when it's zonal uh you are reserving capacity so there's a guarantee that those will be there when you need them in terms of az flexibility you can use the regional ri for any az within that region but for the zonal ri you can only use it for that particular region we're talking about instance flexibility you can apply the discount to uh any instance in the family regardless of the size uh but then when we're looking at a z there is no instance flexibility okay so you're just going to use it for exactly what you defined you can queue purchases for regional ri you cannot queue purchases for zonal ri so there you go let's talk about some ra limits here so there's a limit to the number of reserved instances that you can purchase per month and so the idea here is that you can purchase 20 regional reserve instances per region and then 20 zonal reserve instances per az so if you have a region that has three az's you can have uh 60 zonal reserved instances in that region okay there are some other limitations here so for regional limits you cannot exceed the running on demand instance limit by purchasing regional reserve instances the default for ondemand limit is 20 so before purchasing your ri ensure ondemand limit is equal to or greater than your ri you intend to purchase you might even want to open up a service limit increase just to make sure you don't hit that wall for zonal limits you can exceed your running on demand instance limit by purchasing zonal reserve instances if you're already uh have 20 ondemand instances and you purchase 20 zone reserved instances you can launch a further 20 ondemand instances that match the specification of your zonal reserved instances so there you go let's talk about capacity reservation so ec2 instances are backed by different kinds of hardware and so there is a finite amount of servers available within an availability zone per instance type of family remember an availability zone is just a data center or a collection of data centers and they only have so many servers in there so if they run out because the demand is too great you just cannot spin anything up and so that's what's happening you go to launch specific ec2 instant type but abs is like sorry we don't have any right now and so the solution to that is capacity reservation so it is a service of ec2 that allows you to request a reserve of vcc instance type for a specific region and a z so here you would see that you just select the instance type platform a z tendency the quantity and then here you might manually do it specify time or you might say okay i can't get exactly what i want but can give me something generally around that kind of stuff or that type that i want so the reserve capacity is charged at the selected instance type on demand rate whether an instance is running in it or not and you can also use regional reserve instances with your capacity reservations to benefit from billing discounts so there you go so there are some key differences between standard and convertible ri so let's take a look at it here so the first is that with standard ri you can modify your attributes so you can change the az within the same region you can change the scope from a zonal ri to original ri or vice versa you can change the instant size as long as it's a linux and it has the default tendency you can change the network from ec2 classic to vpc and vice versa but where you're looking convertible you you don't modify ri tributes you perform in exchange okay and so standard rise cannot do exchanges where convertible ri you can uh exchange during the term for another convertible ri with new ra attributes and this includes the instance family instant type platform scope and tenancy um in terms of the marketplace you ca they can be bought in standard ri uh in the marketplace or you can sell your ri if you uh don't need them anymore but for convertible ri they cannot be sold or bought in the marketplace you're just dealing with aws directly okay hey this is andrew brown from exam pro and we are taking a look at the reserved instance marketplace we had mentioned a prior so let's give it a little more attention here so it allows you to sell your unused standard ri to recoup your spend for alright you do not intend or cannot use so reserved instances can be sold after they have been active for at least 30 days and once database has received the upfront payment you must have a u.s bank account to sell ri on the ra marketplace there must be at least one month remaining in the term for the ri you are listing you will retain the pricing and capacity benefit of your reservation until sold and the transaction is complete your company name and address upon request will be shared with the buyer for tax purposes a seller can set only the upfront price of an ri the usage price and other configurations such as instance type availability zone platform will remain the same as when the ri was initially purchased the term length will be rounded down to the nearest month for example a reservation with 9 months and 15 days remaining will appear as 9 months on the rm market you can sell up to 20 000 usd in reserved instances per year if you need to sell more ri reserved instances in the govcloud uh region cannot be sold on the ra marketplace so there you go hey it's andrew brown from exam pro and we are taking a look at spot instances so a bus has unused compute capacity that they want to maximize the utility of their idle servers all right so the idea is just like when a hotel offers booking discounts to fill vacant suites or planes offer discounts to fill a vacant seats all right so spot instances provide a discount of 90 compared to ondemand pricing spot instances can be terminated if the computing capacity is needed by other ondemand customers but from what i hear rarely rarely does spot instances ever get terminated it's designed for applications that have flexible start and end times or applications that are only feasible at very low compute costs so you see some options here like load balancing workloads flexible workloads big data workloads things like that um there is another service called ada's batch which is for doing batch processing and this is very common what you use spot with and so you know if you find the spot interface too complicated you're doing batch processing you want to use this service instead um there are some termination conditions so instances can be terminated by aws at any time if your instance is terminated by a bus you don't get charged for a partial hour of usage if you terminate an instance you will be still charged for an hour that it ran so there you go hey this is andrew brown from exam pro and we are taking a look here at dedicated instances so dedicated instances is designed to help meet regulatory requirements innovas also has this concept called dedicated hosts and this is more for when you have strict serverbound licensing that won't support multitenancy or cloud deployments and we'll definitely distinguish that in this course but just not in this slide in particular um and so to understand uh dedicated instances or hosts we need to understand the difference between multitenancy and single tendency so multitenancy you can think of it like everyone living in the same apartment and single tendency you can think of it everyone having their own house so the idea here is that you have a server i'm just going to get my cursor or my pen out here to say server and you have multiple customers running workloads on the same hardware and the idea is that they are separated via virtual isolization so they're using the same server but it's just software that might be separating them okay and then we have the idea of single tenancy so we have a single customer that has dedicated hardware so the physical location is what separates customers um and the idea here is that dedicate can be offered via ondemand reserved and spot so that's what we're talking about dedicated here in the pricing model just so you know that you know even though these are a lot more expensive than ondemand uh you can still save by using reserve and also spot which i was very surprised about um and when you want to choose dedicated you're just going to launch your ec2 and you'll have a drop down where you have that shared so that's the default dedicated so you have dedicated instance and dedicated hosts and again we'll talk about dedicated hosts later when we need to here um and so again the reason why um you know enterprises or large organizations may want to use dedicated instances is because they have a security concern or obligation about against sharing the same hardware with other aws customers okay hey this is andrew brown from exam pro and we are taking a look at ava savings plans and this is similar to reserved instances but simplifies the purchasing process so it's going to look a lot like all right at the start here but i'll tell you how it's a bit different okay so there are three types of saving plans you have compute savings plan ec2 instance saving plans and sage maker saving plans uh and so you just go ahead and choose that you can choose two different terms so one year three year so it'd be simple as that and then you choose the following payment options so you have all upfront partial payment and no upfront and then you're going to choose that hour of the commitment you're not having to think about standard versus convertible uh regional versus zonal ri attributes it's a lot simpler uh let's just talk about the three different saving plans or types in a bit more detail so you have compute so compute savings plans provides the most flexibility and helps to reduce your cost by 66 percent these plans automatically apply to ec2 instances usage aws fargate abuse lambda service uses regardless of the instance family size az region os or tenancy then you have ec2 instances so this provides the lowest prices offering saving up to 72 percent in exchange for commitment to usage of instance uh individual instance families in a region so automatically reduce uh your costs on the selected instance family in the region regardless of az size os tenancy gives you the flexibility to change your usage between instances with a within a family in that region and the last is sagemaker so helps you reduce stage maker costs by up to 64 percent automatically apply to stage maker usage regardless of instance family size component aws region if you don't know what sagemaker is that's aws's ml service and it uses ec2 instances or specifically ml ec2 instances so everything's basically using ec2 here but there you go all right let's take a look at the xero truss model and the zero trust model is a security uh model which operates on the principle of trust no one and verify everything so what i mean by that is malicious actors being able to bypass conventional access controls demonstrates traditional security measures are no longer sufficient and that's where the zero trust model comes into play so with the zero trust model identity becomes the primary security perimeter and so you might be asking what do we mean by primary security perimeter the primary or new security perimeter defines the first line of defense and its security controls that protect a company's cloud resources and assets um if this still doesn't make sense we do cover a part of the defense in depth where you see the layers of defense from data all the way to physical and so you can kind of see you know what we're talking about in that model there but the old way that we used to do things is networkcentric so we had traditional security focused on firewalls and vpn since there were few employees or workstations outside the office or they were in a specific remote office so we treated the network uh the network as kind of like the the boundary so if you're in in office there's nothing to worry about but we don't think like that anymore because everything is identity centric so this is where we have bring your own device remote workstations which are becoming more common uh we can't always trust that the employee is in a secure location we have uh identitybased security controls like mfa or providing provisional access based on the level of risk from where when and what a user wants to access and identity centric does not replace uh but it augments networkcentric security so it's just an additional layer of consideration for uh security when we're thinking about our database cloud workloads okay all right so we just loosely defined what the zerotrust model is so let's talk about how we would do zero trust in aws and so zero trust has to do a lot with identity security controls uh so let's talk about what is at our disposal on aws so on database we have identity and access management i am this is where we create our users or groups or policies so time policy is a set of permissions that allow you to say okay this user is allowed to use these services with these particular actions then you have the concept of permission boundaries and so these are saying okay these aren't the permissions the user has currently but these are the boundaries to which we want them to have so they should never have access to um uh ml services and if someone's to apply them uh uh permissions it'll always be within these boundaries then you have service control policies and these are organizationwide policies so if you have a policy where you don't want anyone to run anything in the canada region you can apply that policy at the organizational level and it will be enforced then within an ion policy there are the concept of conditions and so these are all the kind of like uh little knobs you can tweak to say how do i control based on a bunch of different factors so there's source ip so restrict where the ip address is coming from a requested region so a restrict based on the region as we were just mentioned as an example uh multifactor auth presence so restrict if mfa is turned off uh current time so restrict access based on time of day maybe your employees should never be really using things at night and so that could be an indicator that someone is doing something malicious so you know only give them access during a certain time of day and so that's where we're going to figure out you know based on all these type of control security controls uh to our aws resources we can kind of enforce the zero trust model aws itos does not have a readytouse identity controls that are intelligent which is why abuse is considered not to have a zero trust offering for customers and thirdparty services need to be used so what i'm saying is that technically you know this check box is this thing saying okay we can kind of do zero trust on aws but there's a lot of manual work and you know if i was to say okay i don't want anyone using this at nighttime that doesn't really detect you know what i'm saying it's not going to say oh i think this time is suspicious or malicious so then restrict access only to these core services and anything outside of the services can't be used it just can't exactly do that without a lot of work yourself and that's what i'm talking about here where we have a collection of services that can be set up in an intelligent intelligent ish detection way for identity concerns but requires expert knowledge so the way you might do that aws is that everything all the api calls go through awes cloudtrail and so what you could do is feed those into amazon guard duty and guard duty is an intrusion intrusion detection and protection system so it could detect suspicious from malicious activity on those cloudtrail logs and you could follow that up with remediation or you could pass that on to amazon detective that could analyze investigate and quickly identify security issues that it could ingest from guard duty but i'm telling you that this stuff here is not as easy for the consumer and so you of course you can do zero trust model but it's going to take a lot of work here and there are some limitations which we'll talk about next here so now let's see how we would do zero trust on a bus with third parties so it was just does technically implement a zero trust model but does not allow for intelligent identity security controls which you know you can do it but it's a lot of work so let's kind of compare it against kind of a third party where we would get the controls that we would not necessarily get with aws so for example azure active directory has a realtime and calculated risk detection based on data points than aws and this is based on device and application time of day location whether mfa is turned on what is being accessed and the security controls verification or logic restriction is much more robust so you know just as one particular example like device and application is not something that aws factors in uh with the existing controls or at least not in a way that is consumer friendly and you know i can't say on a bus okay when you think that this is the type of threat only allow them access to these things or if you think they're in a risky area or risky uh location only give them access to you know these things or where there's not sensitive data you can't exactly do that in a database very easily and so this is where thirdparty solutions are going to come into play so you have azure active directory google beyond corp jump cloud and all these have more intelligent security controls for realtime detection um and so the way you would use these is these would be your primary directories uh for google beyond corp is just a zero trust framework so i guess you'd use google's uh cloud directory but the idea anyway here is that you use single signon to connect those directories to your aws account and that's how you'd access access those uh aws resources and you get this more robust functionality okay hey it's andrew brown from exam pro and we're looking at identity now we need to know a bunch of concepts before we talk about identity on aws so let's jump into it the first is a directory service so what is directory service well it's a service that maps the names of network resources to network addresses and the directory services shared infrastructure or information infrastructure for locating managing administrating and organizing resources such as volumes folders files printers users groups devices telephone numbers and other objects a directory service is a critical component of a network operating system and a directory server or a name server is a server which provides a directory service so each resource on the network is considered an object by the directory server information about a particular resource is stored as a collection of attributes associated with that resource or object wellknown directory services would be a domain name service so the directory service for the internet microsoft active directory and they have a cloud hosted one called azure active directory we have apache directory service oracle internet directory so oid uh open ldap uh cloud and identity and jump cloud okay hey this is andrew brown from exam pro and we're taking a look at active directory now you might say well we're doing a bus why are we looking at this well no matter what cloud provider you're using you should know what active directory is especially when it comes to identity because you can use it with aws so let's talk about it so microsoft introduced active directory domain services in windows 2000 to give organizations the ability to manage multiple onpremise infrastructure components and systems using a single identity per user and since then it's uh involved evolved obviously it's running beyond windows 2000 as of today and they even have a managed one called azure ad which is on microsoft azure but just to kind of give you an architectural diagram here the idea is that you would have your domain servers here and they might have child domains and the idea is that you have these running on multiple machines so that you have redundant ability to log in from various places when you have a bunch of domains it's called a forest and then within a domain you actually have organizational units and with them within organizational units you have all your objects like your users your printers your computers your servers all things like that okay hey it's andrew brown from exam pro and we're talking about identity providers or ipds so so hey this is andrew brown from exam pro and we are talking about identity providers also known as idps so an identity provider is a system entity that creates maintains and manages identity information for principles and also provides authentication services to applications with a federation or distributor network a trusted provider of your user identity that lets you use authent lets you authenticate to access other service identity providers so this could be like facebook amazon google twitter github linkedin uh federated identity is a method of linking a user's identity across multiple separate identity management systems and so some things that we can use for that is like open id so this is an open standard and decentralized authentication protocol allows you to be able to log in to different social media platforms using google or facebook account open ideas about providing who you are then we have oauth 2.0 this is an industry standard protocol for authorization oauth doesn't share password data but instead uses authorization tokens to prove an identity between consumers and service providers oauth is about granting access to functionality and then we have saml so security assertion markup language which is an open standard for exchanging authentication and authorization between an identity provider and a service provider and this is important to use for saml which we use for single signon via the web browser okay hey this is andrew brown from exam pro we're looking at the concept of single signon so sso is an authentication scheme that allows the user to log in with a single id and password to different systems and software sso allows it departments to administer a single identity that can access many machines and cloud services so the idea is you have azure active directory this is just an example of a very popular one you'd use saml to do sso you can connect to all things slackly the best google workspaces salesforce or your computer uh the idea here is uh once you log in you don't have to log in multiple times so you log into your primary directory and then after that you're not going to be presented with a login screen some services might show an intermediate screen but the idea is you're not entering your credentials in multiple times so it's seamless all right let's talk about ldap so lightweight directory access protocol is an open vendor neutral industry standard application protocol for accessing and maintaining distributed directory information services over ip network so a common use of ldap is to provide a central place to store usernames and passwords ldap enables for same signon so same signon allows users to use a single id and password but they have to enter it every single time they want to log in so maybe you have your onpremise active directory and then it's going to store it in that ldap directory and so the idea is that you know all these services like google kubernetes um jenkins is going to deal with that ldap server so why would you use ldap over sso which is more convenient or seamless so most sso systems are using ldap under the hood but ldap was not designed natively to work with web applications so some systems only support integration with ldp and not sso so you got to take what you can get okay let's take a look here at multifactor authentication also known as mfa and this is a security control where after you fill in your user's name and email password you have to use a second device such as a phone to confirm that it's you that is logging in so mfa protects against people who have stolen your password mfa is an option in most cloud providers and even social media websites such as facebook so the idea is i have my username or email and password i'm going to try to log in this is the first factor and the second factor or multifactor is i'm going to use a secondary device so maybe my phone we're going to enter in different codes or maybe it's password list so i just have to press a button to confirm that it's me and then i'll get access so in the context of aws it's strongly recommended that you turn on mfa for all your accounts especially the aws root account we'll see that when we do the follow alongs let's take a look at security keys so a security key is a second device used as a second step in authentication process to gain access to a device workstation or application a security key can resemble a memory stick and when your finger makes contact with a button of exposed metal on the device it will generate and autofill a security token a popular brand of security keys is the ubi key and this is the one i use and is looks exactly like the one that's right beside my desk it works out of the box with gmail facebook and hundreds more supports fido 2 web auth n uh u2f it's waterproof and crest resistance it has variations like usb a usb nfc dual connectors on a single key can do a variety of things so when you turn on mfa on your aws account you'll have virtual mfa device so that's when you're using something like a phone or using software on your phone to do that then there's the u2f security key so this is what we're talking about right now and there's even other kinds of hardware mfa devices which we're not really going to talk about but you know just security keys tie into mfa and this is a lot better way than using a phone because you know you can have it on your desk and press it um and you know you have to worry about your phone being not charged okay hey this is andrew brown from exam pro and we are taking a look at aws identity and access management also known as iem and you can use this service to create and manage database users groups uh use permissions to allow and deny their access to adab's resources so there's quite a few components here let's get to it so the first is i am policies so these are json documents which grant permissions for specific users groups or a role to access services and policies are attached to im identities then you have impermissions or permission and this is an api action that can or cannot be performed and they're represented in the i am policy document then there's the i am identity so we have i am users these are end users who log into the console or interact with aws resources pragmatically or via clicking ui interfaces you have im groups so these these group up your users so they all share the same permission levels so that maybe its admins developers or auditors then you have i am roles so these roles grant endless resources permissions to specific database api actions and associate policies to a role and then assign it to an aws resource so just understand that roles are when you're attaching these to resources so like if you have an ec2 instance and you say it has to access s3 you're going to be attaching a role not a policy directly okay hey this is andrew brown from exam pro and we are looking at iron policies a little bit closer here and they are written in json and contain the permissions which determine the api actions that are allowed or denied um and rarely do i write these out by hand because they have a little wizard that you can use to write out the code for you but if you want to you absolutely can write it out by hand but we should know the contents of it and how these json files work so the first thing is the version which is the policy language version and it's been 2012 for a very long time i don't see that changing anytime soon if they happen to change uh what or what the structure of the json is then you have the statements and these are for policy elements and you're allowed to have multiples of them so the idea is that this is the policies or permissions we should say that you uh plan on applying then you have the sid this is a way of labeling your statements um this is useful for like visualization or for referencing it for later on but a lot of times you don't have to have a sid then there's the effect it's either allow or deny then you have the action so here we're saying give access to s3 for all actions under it there's another action down below where it's saying give access and get my pen tool out here just to create a service link role so it's a cross account rule there then there's the principal so this is the account user role or federated user to which you would like to allow access or deny so we're specifically saying this user named barkley um in our aws account there uh then there are the resources so the resources to which the action applies um so in this one up here we are specifying a specific aws bucket here we're seeing all possible resources in enables account and then the condition so there's all sorts of different kinds of conditions so this is a string like one it's saying look at the service name and if it starts with this or that then they'll have access to that so this person even though it says all resources they're really only going to have access to rds okay so in this follow along we're going to take a closer look at im policy so go to the top and type in iam and what we'll do is make our way over here all the way over to policies and what i want to do is create a new policy that only has access to limited resources so let's say we want to create an amazon ec2 instance and that ec2 instance has access to a very particular s3 bucket so what i want you to do is make your way over to s3 and we're going to create ourselves a new bucket and i'm going to go ahead and create a bucket here we're going to call this um policy tutorial and i'm going to just put a bunch of numbers here you'll have to randomize it for your use case and so now that we have our bucket what we're going to do is go ahead and create a policy and the policy is going to choose a service we're going to say s3 and what i want to do is only be able to list out actions i'm going to expand this so i don't want everything so we're just going to say list buckets okay and then what we'll do is uh expand this here and i want to say for a particular bucket so we'll go back over here click into our bucket and we're going to go ahead and set those permissions by finding that iron we're going to paste that we're going to paste that iron up there sometimes it's a bit tricky it vanishes on you and we could set other conditions if we wanted to but this is pretty simple as it is and so that's our rule here right so we're saying this policy allows us to list this bucket for that okay so what we'll do is go ahead and hit next we'll hit review and we'll just say my bucket policy and we'll create that policy okay so there's a few other things i think that i'd like to do with this policy i'm going to pull it back up here so if we want to find it uh you used to be able to filter these based on the ones that you created but um yeah they should like the little icon so these are ones that i've created up here and so there's my bucket policy and i feel like i want to update this policy to have a bit of extra information here so i'm going to go edit this policy no you know what i think this is fine so what i want to do is now create a role and we're going to create a new role and i'm going to call this um well before i do i need to choose what it's for so it's going to be for ec2 so we're going to go ahead and hit next we're going to choose our policy so my bucket policy there it is and i want to add another one here because i want to be able to use sessions manager because i really don't want to use an ssh key to check that this works and so um for this i i need to use ssm so i'm going to type in ssm here and i'm going to just make sure this is the new one so this policy will soon be deprecated use amazon ssn managed core instance should always open these up and read them and see what they do and so that's the one that's going to allow us to access simpson's manager so we can use sessions manager okay and so we're going to say my ec2 roll4s3 and we go ahead and create ourselves a roll and so now that we have our role i'm going to go over to ec2 and i'm going to go ahead and launch myself a new instance we're going to choose amazon linux 2 we're going to stick with t2 micro i'm going to go over to configuration here everything is fine here i'm fine with all that storage is fine we'll go to security group and i don't want any ports open because i'm not going to be using ssh we're going to launch this instance i don't even want a key pair okay and then we're going to go over here and so what we're waiting for is this instance to launch as that is going what i want to do is go over to my s3 bucket and i want to place something in this bucket so i do have some files here so what i'm going to do is create a new folder here whoops i'm going to go back and i'm just going to create a folder first create a folder enter prize d and i'm going to click into this and then i'm going to upload all my images here so you'll have to find your own images off the internet this is just the ones i have and we'll go ahead and upload those give that a moment okay and so we don't have access to read those files we'll adjust our policy as we go so that we can do that okay so this instance should be running um it has doesn't have the two status checks passed we should be able to uh connect to it so click on connect here and so we have options like ec2 instance connect sessions manager ssh client i want you to go to sessions manager it says we weren't able to connect your instance common reasons ssm agent was installed we absolutely have that installed the required item profile oh right so we were supposed to attach i forgot to do we were supposed to attach an iron profile right so an iron profile is the role uh it holds the role uh that's going to give the permissions to that instance and since we didn't add it we got to go retroactively at it after the fact and so i'm going to modify the i am roll and we're going to choose my ec2 roll for s3 and we're going to save that and actually when that happens you have to reboot the machine you only have to do that if you have no roll attached like prior no profile attached and they're attaching it for the first time but after that you never have to reboot the machine this is the only case where you'd have to do that that's why when i launch an ec2 instance i always at least have the ssm role attached the managed one that gets sessions manager so that i don't ever have to do a reboot in case i have to update the policy and so we will give that a moment there it says initializing so i'm going to try again to connect to it okay and we still don't have that option there um so i'm going to go back to my instances i'm going to check to see if the role the role or policy is attached or profile i should say so i'm just looking for it here there it is and so if i click into this into the role we can see that we have the amazon ssn managed instance core there so that's set up and the my bucket policy so this has everything that it should be able to do it no problem okay so i'm going to try that again okay so now the connection shows up aws is finicky like that you just have to have confidence in knowing what you're doing is correct okay we'll go ahead and hit connect i didn't have to use ssh keys or anything and this is a lot more secure way to connect your instances when it logs us in it's going to set us as the ssm user but we want to be the ec2 user okay that's uh aws always makes their uh am like their linux versions as the ec2 user and that's what you're supposed to use but it's just you just that's how you have to get to that you have to type that sudo su hyphen ec2 user okay just once and if you type who am i that's who you are if you type exit you'll go back to that user so if i type exit and i type who am i and now this person so i'm going to go back hit up go back in there type clear so now i want to see if i have access to s3 so i have to do abs s3 ls let's see if i can list buckets it says axis denied so i mean that kind of makes sense because if you have list buckets and we're just saying only that bucket that might not make a whole lot of sense so i'm going to go back to my policy i might just written a crummy policy but we'll say i am here if we have that one open we should just go here and click on this policy here i'm going to edit that policy so what i'm going to do is i'm just going to change it i'm going to say all resources review the policy save changes and we'll see how fast that propagates okay because i'm pretty sure i don't have to do anything here it should just now give me full access to s3 i'm just going to keep on hitting up here so i'm going to do is i'm just going to take like a three four minute break gonna get a drink i'm gonna come back here and see if this propagates i'm pretty sure i don't have to do anything for that to propagate and i think that i've attached everything correctly here okay okay so i haven't had much luck here it's still having the same issue so if that is happening what i'm going to do is i'm just going to reboot it because maybe i didn't give it a good opportunity to reboot there again i don't think we should have to reboot it every time we're changing um things there but we will give it another go here and see if that fixes that problem there so those sessions matter is going to time out here which is totally fine it's going to kill that session there and so what we'll have to do is close this out because there's not much we can do with that and we're going to go ahead and go back to connect and so we're waiting for this button to appear because it is rebooting so if we want to monitor that stuff usually there is an option here to monitor where it will show us the system logs of what it's doing so here it's just like restarting the machine i'm not sure if we expect to see something after this so i can click that there and uh yeah it's so easy to get turned around so i can connect to it again now we'll type in sudo su hyphen ec2 user aws s3 ls and we still have access deny for list buckets so if that's the case it could be that um sometimes you need other permissions when doing list policy like list buckets so if that's the case we're going to do a sanity check i'm just going to say all permissions here okay and this way there's no way that i've set this incorrectly um it just has to work now so type this in there we go okay so there has to be something more to it so just because you say list buckets you know like means there must be more to it right so if i go here to this right and i say whoops and i say uh list buckets here we'll say copy paste okay here it's saying maybe i need get object as well so i just know from using it about a long time that that's the case that it could be more than one thing so you know that was in the back of my mind that that could be happening and i guess that is but notice i didn't have to restart my uh my server boot my server to get those to work um but anyway let's go lock that down and see if we can just kind of make this uh more focused so let's say all resources i'm going to specify the condition so i might want to just say for particular buckets so we'll say specific when you checkbox everything then you have to do this so for storage accounts these are fine any for objects that could be something we'll say multiregion access bucket any bucket but what i'm going to say is i want to only allow them to access things in a particular bucket and so if i go to arn here um what is our bucket name our bucket name is policytutorial3414 whatever right and so we can actually give it a wild card or we can say enterprise d and we learned this in the course that you can provide orange with randomized things there i don't know if i spelt it wrong over here so i should really double check i should probably just copy it oops i still want to type it wrong and so this okay means that we should only be able to get stuff from there i'm going to review the policy let's see if it takes save the changes and if i just view the json here notice it says anything from here right so allow s3 anything as long as it's within here and then it also broke it up into sub 1's 4 here okay so anyway what i want to see is what happens if i upload something into the loose area here so i'm going to say upload and i'm going to say add a file and we're just going to grab data here and upload it go back to our bucket there's our file we have that stuff in there and so if i go back over to my ec2 instance which i'm still connected to uh who am i okay great clear so i'm going to say aws s3 ls see if that works still it does good and so what i want to do is see if i can copy a file locally so i'm going to do a bus s3 copy i think it was s3 8 no it's just s3 copy polis uh s3 forward slash forward slash policy tutorial three four one four one whoops three four tutorial hyphen three four one four one four slash enterprise d data dot jpg i think it's a jpg let's go double check yeah it is okay and then i just want to say data.jpg and it downloaded it right so i'm going to remove that one and so now what i'm going to do is i'm just going to see if my policy is working or maybe my permissions aren't exactly what i think they are and i was able to download it so it's these policies can get kind of tricky because like this one says allow all actions for these but then these say all actions and so that makes it hard because i want get object so another thing we can do and if that one doesn't work really well i'm just going to write one by hand it's not that scary to write these by hand you just get used to it so i'm going to say effect um is it disallow or maybe it's deny deny action s3 get object i believe that's what it is resource and then i'm going to specify exactly the resource i don't want it to allow so we're going to say arn aws s3 3 colons policy tutorial 34141 and just say data.jpg now if this is not valid it's going to complain and say hey you didn't write this right and it and it's fine okay so we'll save those changes and so that should deny access to that right hopefully i got the policy right okay so that one doesn't work which is fine and that one's fine so that worked we were able to deny that but you can see there's a little bit of an art to creating these policies as you make more of them it becomes a lot easier so hopefully it's not too scary but that's all there really is to it that i want to show you today so what we're going to do is clear out this bucket we're done with this bucket here so we'll say delete whoops we got to empty it first and we'll just say permanently delete here okay and we will exit that out we're gonna go ahead and delete that bucket grab its name here and uh we'll go back over here i think i forgot to delete this bucket from earlier i'm just going to delete that because i don't need that bucket so that's okay with you just going to go ahead and delete that and we have that ec2 instance running so we want to stop that so we go ahead and we're going to terminate that yes please and then we'll go to im and do some cleanup i have some custom rolls i've been creating um you know from prior things a lot of those usually there's a way to uh we've redesigned it okay where's the redesign this is the redesign that can't be it because it'll be like roles that ada best makes i think these are all roles that i've made um i don't want to delete service roles but i want to get rid of some of these because i just have too many you know it's getting out of hand for me and i'm going to just see if it will let me delete all of these let's delete those there we go just clean up a bit i still have a lot here but there's like service roles that aws creates once and you really don't want to delete those because you don't um and then i have a bunch of these like i'm never going to use these so i might as well detach them delete detach you really don't want to keep like rolls that you're never going to use around things like that like gauze we're going to be using that again delete there's that bucket we just created anyway you get the idea so yeah that's uh that's i am okay principle of least privilege pulp is the computer security concept of providing a user role or application the least amount of permissions to perform an operation or an action and the way we can look at it is that we have just enough access so jea permitting only the exact actions for the identity performer task and then we have just in time jit permitting the smallest length of duration an identity can use permission so usually when we're talking about pulp it's usually a focus on here uh but now these days uh there's a larger focus on jit as well and so jit is the difference between having long lived permissions or access keys versus shortlived ones and the most progressive thing in polp is now riskbased adaptive policies so each attempt to access a resource generates a risk score of how likely the request is to be from a compromised source so the risk score could be based on many factors such as device user location ip address what services being accessed and when did they use mfa did they use biometrics things like that and right now at as of this time it was does not have a riskbased adaptive policies built into iam you can roll your own what's interesting is cognito has riskbased adaptive policies they call it like adaptive authentication but that's for user pools and not identity pools user pools is for getting access to an app uh that you built through an ipd where identity pools incognito is about getting access to itabus resources so uh you know maybe i'm sure about will get it eventually but they just don't have it right now and you have to rely on thirdparty identity solutions uh to get riskbased adaptive policies now talking about just enough access in just in time just in time is like you think how would you do that with aws you just add and remove permissions manually but one thing you could do is use something like console me so this is an open source netflix project to selfserve shortlived i am policies so an end user can access database resources while enforcing jea and jit and so there's a repo there as well but the idea is they have like this selfserve wizard so you say i want these things and then the machine decides okay you can have them or you you don't need them and it just frees you up asking people and worrying about the length and stuff like that okay hey this is andrew brown from exam pro and we are taking a look at the edibus root user uh and this gets confusing because there's energies account root user and regular users let's distinguish what those three things are so here we have an apes account and the account which holds all the aws resources including the different types of users then you have the root user this is a special account with full access that cannot be deleted and then you have just a user and this is a user for common tasks that is assigned permissions so just understand that sometimes people say it was account they're actually referring to the user and sometimes that when they're saying this account they're actually referring to the invoice account that holds the users i know it's confusing it just it's based on what people decide the context is when they're speaking so the inapps account user is a special user who's created at the time of the invoice account creation and they can do uh they have a lot of conditions around them so the reuser account uses an email and password to log in as opposed to the regular user who's going to provide their account id alias username and password the root user account cannot be deleted the root user account has full permissions to the account and its permissions and cannot be limited and when we say cannot be limited we're saying that if you take an im policy to explicitly deny the user access the resources it's not something you can do however you can do it in the case of innovative organizations with service control policies because a service control policy applies to a bunch of accounts so it just it's one level above and so that is a way of limiting root users but generally you can't limit them within their own account there can only be one root user uh per aws account the real user is instead for very specific and specialized tasks that are infrequently or rarely performed and there's a big list and we'll get into that here in a moment and the abyss root account should uh not be used for daily or common tasks it's strongly recommended to never use the root users access keys because you can generate those and use them it's strongly recommended to turn on mfa for the root user and any of us will bug you to no ends to tell you to turn it on so let's talk about the tasks that you should be performing with the root user and only the user can perform so changing your account settings this includes account name email address root user password root user access keys other account settings such as contact information payment currency preference regions do not require the root user credentials so not everything restore im user permissions so if there is an i i am admin so just a user that has admin access who actually revokes their own permissions you can sign into the root user to edit policies and restore those permissions so you can also activate im access to the billing and cost management console you can view certain tax invoices you can close your aws account you can change or cancel your aws support plan register as a seller in the reserved instance marketplace enable mfa delete on s3 buckets edit or delete an amazon s3 bucket policy that includes an invalid vpc id or vpc endpoint id sign up for govcloud and something that's not in here which this i took this from the documentation but uh you can use the aws account user to create the organization you can't create that with any other user so um you know the ones i highlighted in red are very likely to show up your exam and that's uh why i highlighted them there for you but there you go hey this is andrew brown from exam pro and we are taking a look at adabus single signon also known as aws sso and so this is where you create or connect your workforce identities in aws once and manage access essentially across your items organization so the idea here is you're going to choose your identity source whether it's sso itself active directory saml 2.0 idp you're going to manage user permissions centrally to items accounts applications saml applications and it uses it can you get single click access to all these things so you know just to kind of zoom in on this graphic here you know you have your on premise active directory it's establishing a ad trust connection over to uh it will single signon you're going to be able to apply permissions to access resources within your abilities account so via aws organizations in your organizational units down to your resources you can also use aws sso to access custom saml based applications so you know if i build a web app and i like the example platform and i wanted to use saml based uh connections for single sign on there i could do that as well and even connect out sso access to your business cloud application so office 365 dropbox slack things like that so there you go well let's take a look here at application integration so this is the process of letting two independent applications to communicate and work with each other commonly facilitated by an intermediate system so cloud workloads uh strongly encourage systems and services to be loosely coupled and so itabus has many services for the specific purpose of application integration and these are based around common systems or design patterns that utilize application integration and this would be things like queuing streaming pub sub api gateways state machines event buses and i'm sure there are more but that's what i could uh think about that are the most common ones okay so to understand queuing we need to know what is a messaging system so this is used to provide asynchronous communication and decouple processes via messages and events from a sender receiver or a producer and a consumer so a queuing system is a messaging system that generally will delete messages once they are consumed it's for simple communication it's not real time you have to pull the data it's not reactive and a good analogy would be imagining people that are queuing in a line to go do something so for aws it's called simple queuing service sqs it's a fully managed queuing service that enables you to decouple and scale microservices distributed systems and serverless applications so a very common use case in a web application would be to queue up transactional emails uh to be sent like sign up reset password and the reason why we have queuing to decouple those kind of actions is that if you had a longrunning task and you had too many of them it could hang your applications so by decoupling them and letting a separate compute service take care of that that would be something that would be very useful okay let's take a look here at streaming and so this is a different kind of messaging system but the idea here is you have multiple consumers that can react to events and so in streaming we call messages events and then in a queuing system we just call them messages but events live in the stream for long periods of time so complex operations can be applied and generally streaming is used for real time stuff whereas cueing is not necessarily uh real time and so adabus's solution here is amazon kinesis you could also use kafka but we'll focus on kinesis here so amazon kinesis is the aws fully managed solution for collecting processing and analyzing streaming data in the cloud so the idea is that you have these producers so that are producing events could be ec2 instances mobile devices it could be a computer or traditional server they're going to go into the data stream there's a bunch of shards that scale and there's consumers on the other side so maybe redshift wants that data dynamodb s3 or emr okay but the thing you have to remember is that streaming is for realtime data and as you can imagine because it's realtime and it's doing a lot more work than a queueing system it's going to cost more okay so we have another type of messaging system known as pub sub so this stands for publish subscribe pattern commonly implemented in messaging systems and a pub sub system the sender of messages the publishers do not send their message directly to receivers they instead send their messages to an event bus the event bus categorizes their messages into groups then receivers of messages subscribers subscribe to these groups whenever new messages appear within their subscriptions the messages are immediately delivered to them so the idea is you have publishers event bus subscribers and event buses appear more than once so it actually appears in streaming appears in this pub sub model and then it can appear in other variations so you're going to hear it more than once the word event bus so the idea here is the publisher has no knowledge of who the subscribers are the subscribers do not pull for messages messages aren't said automatically immediately pushed to the subscribers and messages and events are interchangeable terms in pub sub all right and so you know the idea here with publisher subscribers just imagine getting like a um a magazine subscription right if you think of that you kind of think of the mechanisms that are going here in terms of practicality it's very common to use these as a realtime chat system or a web hook system so you know hopefully that gives you an idea there in terms of aws's solution we're using simple notification service sns this is a highly available durable secure fully managed pub sub messaging service that enables you to decouple microservices distributed systems and serverless applications so here we have a variety of publishers like the sdk the cli cloud watch aid with services you'll have your sns topic you can filter things fan them out and then you have your subscribers to lambda sqs emails https looks very similar to streaming but again you know um you know there's not a lot of communication going back between it it's just publishers and subscribers and it's limited to you know these things here so it's a very managed service right whereas uh kinesis you can do a lot more with it okay so what is api gateway well it is a program that sits between a single entry point and and multiple backends api gateway allows for throttling logging routing logic or formatting of the request and response when we say request a response we're talking about https requests and responses and so the service for aws is called amazon api gateway so api gateway is just a type of pattern and this is the few cases where aws has named the thing after what it is and so we have amazon api gateway which is a solution for creating secure apis in your cloud environment at any scale create apis that act as a front door for applications to access data business logic or functionality from back end services so the idea is that you have data coming in from mobile apps web apps iot devices and you actually define the api calls and then you say where do you want them to go so maybe tasks are going to go to your lambdas and then other routes are going to go to rds kinesis ec2 or your web application and so these are really great for having um this uh being able to define your api routes and change them on the fly and then and always write them to the same place okay so what is a state machine it is an abstract model which decides how one state moves to another based on a series of conditions think of a state machine like a flow chart and for aws the solution here is itabus step function so coordinate multiple aw services into a serverless workflow a graphical console to visualize the components of your application as a series of steps automatically trigger and track each step and retries when there are errors so your application executes in order as expected every time logs the state of each step so when things go wrong you can diagnose and debug problems quickly and so here's example of using a bunch of steps together on the uh the abyss step functions service and so you know this is generally applied for service workflows but it is something that is very useful if in application integration okay so what is an event bus an event bus receives events from a source and routes events to a target based on rules i'll get my pen tool out here so we have an event it enters the event bus we have a rules tell it to go to the target it's that simple and we have been seeing event buses in other things like uh streaming and uh pub sub but aws has this kind of event bus offering that is kind of high level it's called eventbridge and it's a service event bus service that is used for application integration by streaming realtime data to your applications the service was formerly known as amazon cloudwatch events they gave it a renaming to give it a better opportunity for users to know that it's there to use and they also extended its capabilities and so the thing is is that a lot of aw services are always emitting events and they're already going into this bus and so if you utilize this service it's a lot easier than having to roll your own thing uh with other services so amazon event bridge will just define an event bus so there is an event bus holds event data defines the rules on an event bus to react to events you always get a default event for every single abs account you can create custom event buses scoped to multiple accounts or other abas accounts you have a sas event bus scope to third party sas providers you have producers these are aidable services that emit events you have events these are data emitted by services they're json objects that travel the stream within the event bus you have partnered sources these are thirdparty apps that can emit events to event buses you have rules these determine what events to capture and pass to targets and then targets which are aidable services that consume events so yeah it's all just this great builtin um uh uh stuff that's going on here and so you know there there might be a case where you can use eventbridge and save your time uh a lot of time and effort uh doing application integration okay hey this is andrew brown from exam pro and we are taking a look at application integration services at a glance here so let's get through them so the first is simple notification service sns this is a pub sub messaging system sends notifications via various formats such as plain text email https web hooks sms text messages sqs and lambda pushes messages which are then sent to subscribers you have sqs this is a queuing messaging system or service that sends events to a queue other applications pull the queue for messages commonly used for background jobs we have step functions this is a state machine service it is it coordinates multiple aimed services into a serverless workflow easily share data among lambdas have a group of lambdas wait for each other create logical steps also works with fargate tasks we have a vent bridge formerly known as cloudwatch events it is a service event bus that makes it easy to connect applications together from your own application thirdparty services and aws services then there's kinesis a realtime streaming data service creates producers which send data to a stream multiple consumers can consume data within a stream used for realtime analytics click streams ingesting data from a fleet of iot devices you have amazon mq this is a managed message broker service that uses apache active mq so if you want to use apache activemq there it is manage kafka service and this gets me every time because it says msk and that is the proper initialization but you think it'd be mks it is a fully managed apache kafka service kafka is an open source platform for building realtime streaming data pipelines and applications similar to kinesis but more robust very popular by the way we have api gateway a fully managed service for developers to create publish maintain monitor and secure apis you can create api endpoints and route them to ada services we have appsync this is a fully managed graphql service graphql is an open source agnostic query adapter that allows you to query data from many different data sources so there you go hey this is andrew brown from exam pro and we are comparing virtual machines to containers so i know we covered this prior but i just want to do it one more time just to make sure that we fundamentally understand the difference before we jump into containers so the idea is that if you were to request an ec2 instance it has a host operating system that we don't really know much about but we don't really need to know and then the idea is you have a hypervisor which allows you to deploy virtual machines and so when you launch an ec2 instance you're actually launching a vm on top of a hypervisor on a server uh with on uh within the aws data centers servers there and you're going to choose an operating system so like ubuntu and it might come with some preinstalled packages or you can install your own libraries packages and binaries and then you decide what kind of workloads you want to run on there so it could be django mongodb so your database and some kind of queueing system like rabbitmq the difficulties with virtual machines so you're always going to end up with some unused space because you're going to want to have some headroom uh to make sure that uh you know if you know django needs more memory or or mongodb needs more storage that you have that room that you can grow into but the idea is that you're always paying for that even when you're not utilizing it and so you know that can be uh not as cost effective as you'd like it to be so when we're looking at doing this again and we are using containers um instead of the hypervisor we have container virtualization a very common one would be called docker daemon for docker of course and so now you're launching containers and so maybe you have alpine and this is for your web app and then you install exactly the libraries packages and binaries you need for that and then for mongodb you want to have a different os different packages and same thing with rabbitmq maybe you want to run it on freebsd and the idea is that uh you know you're not going to have this waste because it it's kind of changed the sense that these containers are flexible so they can expand or decrease based on the the use case of what they need uh and you know if you use particular services like it was fargate you know you're paying like for running the containers not necessarily uh for uh over provisioning okay so vms do not make best use of space apps are not isolated which could cause uh config conflict security problems or resource hogging containers allow you to run multiple apps which are virtually isolated from each other launch new containers configure os dependencies per container okay hey this is andrew brown from exam pro and we are taking a look at the concept of microservices and to understand microservices we first need to understand monoliths or monolithic architecture and the idea here is that we have one app which is responsible for everything and the functionality is tightly coupled so i'm going to get my pen tool out here and just to highlight notice that there is a server and everything is running on a single server whether it's load balancing caching the database maybe the marketing website the frontend javascript framework the backend with its api uh the orm connected to background tasks things like that and that's the idea of a monolith and that's what a lot of people are used to doing but the idea with microservice architecture is that you have multiple apps which are responsible for one one thing and the functionality is isolate and stateless and so just by leveraging um various cloud services or bolting it onto your service you know you are technically using microservice architecture so maybe your web app is all hosted uh in containers so you have your apis your your orm your reports maybe you've abstracted out some particular functions into lambda functions you have your um marketing website hosted on s3 you have your frontend javascript hosted on that three you're now using elastic load balancer elasticash rds sqs and that's the idea between monoliths and microservices okay well let's take a look here at kubernetes which is an open source container orchestration system for automating deployment scaling and management of containers it was originally created by google and now maintained by the cloud native computing foundation so the cncf kubernetes is commonly called k8 the eight represents the remaining letters for kubernetes which is odd because everyone calls it kubernetes with the s on there but that's just what it is the advantage of kubernetes over docker is the ability to run containers distributed across multiple vms a unique component of kubernetes are pods a pod is a group of one or more containers with with shared storage network resources and other shared settings so here is kind of an example where you have your kubernetes master it has a scheduler controller etcd you might be using it uses an api server to run nodes within the nodes we have pods and within the pods we have containers kubernetes is ideally for micro service architectures where company has tens to hundreds of services they need to manage i need to really emphasize that tens to hundreds of services all right so you know kubernetes is great but just understand that it is really designed uh to be used for massive amounts of microservices if you don't have that need you might want to look at something just easier to use okay all right let's take a look here at docker which is a set of platform as a service products that use os level virtualization to deliver software in packages called containers so docker was the earliest popularized open source container platform meaning there's lots of tutorials there's a lot of services that uh integrate with docker or make it really easy to use and so when people think of containers they generally think of docker there's of course a lot more options out there than docker to run containers but this is what people think of and so we said it's a suite of tools so the idea is you have this docker cli so these are cli commands to download upload build run and debug containers a docker file a configuration file on how to provision a container docker compose which is a tool and configuration file when working with multiple containers docker swarm an orchestration tool for managing deployed multicontainer architectures docker hub a public online repository for containers published by the community for download and one really interesting thing that came out of docker was the open container initiative oci which is an open governance structure for creating open industry standards around container formats and runtimes so docker establishes oci and it is now maintained by the linux foundation and so the idea is that you can write a docker file or or do things very similarly and use different types of um technologies that can use containers as long as they're oci compatible you can use them so docker has been losing favor with developers due to their handling of introducing a paid open source model and alternatives like podman are growing and that's why we're going to talk about podman next okay so let's take a quick look here at podman which is a container engine that is oci compliant and is a dropin replacement for docker i just want to get you exposure here because i want you to know about this um and that you can use it as opposed to using docker there are a few differences or advantages that podman has so podman is daemonless where docker uses a container d daemon podman allows you to create pods like kubernetes where docker does not have pods uh podman only replaces one part of docker podman is is to be used alongside builda and uh scopio so you know docker is an allinone kind of tool everything is done via a single cli and everything is there but you know they just wanted to make it more module and so these other tools anytime you say podman it usually means we're talking about podman builda and scopio so builda is a tool used to build the oci images and scopio is a tool for moving container images between different types of container storages palm is not going to show up in your exam but you should practically know it just for your own benefit okay let's take a look here at the container services offered on aws so we have primary services that actually run containers provisioning and deployment on you know tooling around provisioning deployment and supporting services so the first here is elastic container service ecs and the advantage of this service is that it has no cold starts but it is a selfmanaged dc2 so that means that you're going to be always paying for the resource as it is running all right then he has aws fargate so this is more robust than using abus lambda it can scale to zero costs and it's being managed by adabus managed ec2 however it does have cold starts so you know if you need containers launching really fast you might be wanting to use ecs then you have elastic kubernetes service eks this is uh open source it runs kubernetes um and this is really useful if you want to avoid vendor lockin um which is not really a problem but batteries just you want to run kubernetes then you have abs lambda so you only think about the code it's designed for short running tasks if you need something that runs longer you'd want to use that is serverless you'd use abus fargate which is serverless containers you can deploy custom containers so prior aws lambda just had prebuilt runtimes which were containers but now you can create any kind of container and use that on it was lambda for provisioning deployment you can use elastic bean socks so it can uh deploy elastic container service for you um which is very useful there now there's app runner which kind of overlaps on what elastic beanstalk does but it specializes it specializes for containers um and i believe that it can actually i don't know what it uses underneath because it is a managed service so elastic bean stock is um open you can see what is running underneath an app runner i don't believe you can see what is running underneath is just taken care of by aws then there's abyss copilot cli so this allows you to build release operate production ready containerized applications on app runner ecs enables fargate for supporting services you have elastic container registry this is repo for your containers not necessarily just docker containers but containers in general probably oci compliant containers xrays so analyze and debug between micro services so you know it's distributed tracing then you have step functions so stitch together lambdas and ecs tasks to create um a state machine and the only thing i don't have on here would be you know being able to launch an ec2 instance from the marketplace that has um a a container runtime installed like docker i just don't feel that that's very relevant for the exam but it is another option for containers not something that people do very often but there you go hey this is andrew brown from exam pro and we're taking a look here at organizations and accounts so aws organizations allow the creation of new aws accounts and allows you to centrally manage billing control access compliance security and share resources across your aws accounts so here's kind of a bit of a structure of the architecture of aws organizations and we'll just kind of walk through the components so the first thing you have is a root account user this is a single signin identity that has complete access to all eight of the services and resources in an account and each account has a root account user so generally you will have a master or root account and even within that you'll have a root account user and for every additional account that you have you'll notice over here we have a root account user then there's a concept of organizational units uh these are commonly abbreviated to ous so they are a group of aws accounts within an organization which can contain other organizational units creating a hierarchy so here is one where we have called starfleet and here's one called federation planets and underneath we have multiple accounts it was accounts within that organizational unit and even though it does not show it here you can create an organizational unit within an organizational unit then we have service control policies scps and these give uh central control over the allowed permissions for all aws accounts in your organization helping to ensure your accounts stay within your organizational guidelines what they're trying to say here is that um there's this concept of aws i am policies and all you're doing is you're creating a policy that's going to be uh organizational unitwide or organizationalwide or for select accounts so it's just a way of applying iron policies across multiple accounts it was organizations must be turned on and once it's turned on it cannot be turned off it's generally recommended that you do turn it on because basically if you're gonna run any kind of serious workload you're gonna be using awesome organizations to isolate your abus accounts based on workloads you can create as many aws accounts as you like one account will be the master or root account and i say root account here because this is the new language here and some of the documentation still calls it master account so understand this is the root account not to be confused with the root account user so another clarification i want to make is an ito's account is not the same as a user account which is another thing that is confusing so when you sign up for aws you get an aws account and then it creates you a user account which happens to be a root user account so hopefully that is clear so aws control tower helps enterprises quickly set up a secure aws multi account it provides you with a baseline environment to get started with a multicount architecture so it does this a few a few different ways the first thing is it provides you a landing zone this is a baseline environment following well architected and best practices to start launching productionready workloads so imagine you wanted to go have um you know the perfect environment that you know is secure is correctly configured and has good logging in place that's what a landing zone is and so itabus's landing zone for control tower is going to have sso enabled by default so it's very easy to move between ips accounts it will have centralized logging for aws cloud trail so that you know they're going to be tamper evident or tamper proof away from your workloads where they can't be affected it'll have cross account security auditing um so yeah landing zones are really great to have then there's the account factory they used to call this um a vending machine but they changed it to account factory the idea is it automates provisioning of new accounts in your organization it standardizes the provisioning of new accounts with preapproved account configuration you can configure account factory with preapproved network configuration and region selections enable selfservice for your builders to configure and provision to accounts using able service catalog able service catalog is just preapproved uh workloads uh via cloud formation templates so you created to say okay you're allowed to launch this server or these resources and the third and most important thing that ava's control tower comes with is guard rails so these are prepackaged governance rules for security operations compliance the customers can select and apply enterprisewide or to specific groups of accounts so abus control tower is the replacement of the retired aws landing zone so if you remember abel's landing zones which was never a selfserve easy thing to sign up for it required a lot of money and stuff that go in there they just don't really have it anymore and it was control tower is the new offering um there okay hey this is andrew brown from exam pro and we are taking a look at abs config and to understand it was config we need to know what compliance as code is and to understand compliance as code we need to understand what change management is so change management in the context of cloud infrastructure is when we have a formal process to monitor changes enforce changes and remediate changes and compliance is code also known as cac is when we utilize programming to automate the monitoring enforcing and remediating changes to stay compliant with the compliance program or expected configuration so what is adabus config well it's a compliance code framework that allows us to manage change in your aws accounts on a per region basis meaning that you have to turn this on for every region that you need it for and so here is a very simple example where let's say we create a config rule and we have an ec2 instance and we expect it to be in a particular state and then in the other case we have a rds instance and it's in a state that we do not like so the idea is that we try to remediate it to put it in the state that we want it to be and those configurables are just powered by lambdas as you can see based on the lambda icon there so when should you use database config well this is when i want this resource to stay configured a specific way for compliance i want to keep track of configuration changes to resources i want a list of all resources within a region and i want to use analyze potential security weaknesses and you need detailed historical information so there you go hey this is andrew brown from exam pro and in this follow along we're going to take a look at aws config so itaps config is a tool that allows you to ensure that your services are configured as expected so i've already activated it in my north virginia region so what i'm going to do is just go over to ohio here because it is per region activated and i'll go over to config and then what we'll have to do is set it up so there is this one click setup and it did skip me to the review step because it's kind of piggybacking on the configuration of my original one here but the idea is that you'll just say uh record all resources in this region or things like that you'll have to create a service role link if you have not done so so this will look a little bit different but here it's using the existing one you'll have to choose a bucket so or create a bucket uh it's not super complicated so you get through there you hit confirm and basically you're going to end up with this so the inventory lets you see all the the resources that are not all of them but most resources that are in your aws account in this particular region it this will not populate right away so you will have to wait a little bit of time for that to appear one really nice thing are conformance packs i really love these things when nativists first brought these out there was only like a couple but now they have tons and tons and tons of performance packs so you can go deploy a conformance pack and you can open up the templates i just want to show you look at how many they have so there's some you might recognize like nist cis things like that well architected uh stuff and all these are um and i'm not sure if it's easy to open these up but all these are if we open them up they're on github is these are just cloud formation templates to set up configuration rules so there's a variety of suggested rules uh like around i am best practices and things like that that we can load in um but the idea is that you're just going to create rules so you go here and you add a rule and they have a bunch of managed rules here that we can look at but i think it might be fun to actually run a conformance pack i'll just show you what it looks like to add a rule first so let's say we wanted to do something for s3 and it was making sure that we are blocking public access so we go next here generally you'll have a trigger type you can choose whether it's configured when it happens or it's periodic this is disabled in this case here and you just scroll on down and then once you've added the rule what you can do is also manage remediation so if this rule said hey this thing is noncompliant we want you to take a particular action you have all these aws actions that you can perform and you can notify the right people to correct it or have it auto correct if you choose to do so for rules you can also make your own custom ones so that's just you providing your own lambda functions you're providing that lambda iron and so basically you can have it do anything that you want whatever you want to put in a lambda you can make aws config check for okay so it's not super complicated here but this one here is just going to go ahead and check and so if we go and reevaluate we might just take some time to show up so they're gonna say that it's compliant or noncompliant okay and i it should be compliant but while we're waiting for that to happen let's just see how hard it is to deploy a conformance pack because i feel like that's something that's really important oh you just drop them down and choose them that's great so we might want to go to iam here oops identity and access management and hit next and say my im best practices and you might not want to do this because it does have spend and i want to say spend it's not going to happen instantly but the idea is that if you turn this on and forget to remove it you will see some kind of charges over time because it does check based on the rules it's not super expensive but it is something to consider about but anyway so it looks like we created that conformance pack so if i refresh it looks like it's in progress i wonder if that's going to set up a cloud formation template i'm kind of curious about that so make our way over to cloudformation and it is so that's really nice because once that is done what we can do is just tear it down by deleting the stack so i'm going to go back over to our conformance pack here let's take a look here and so it still says it's in progress but it is completed and we can click into it and we can see all the things that it's doing so it says item groups have user check performance pack and so it looks like there's a bunch of cool rules uh here so what we'll do is we'll just wait a little while and we'll come back here and then just see if um this updates and see how compliant we are from a uh a basic account okay all right so after waiting a little while there it looks like some of them are being set so i just gave it a hard refresh here uh and here you can see that it's saying is root account um oops we'll give it a moment here to refresh but uh is the root account mfa applied yes have we done a password policy no and actually i never did a password policy which is something i forgot to do but here they're just talking about the minimums and maximums of things that you can do okay so that's a conformance pack but if we go to rules actually i guess it's all the rules here i can't really tell the difference between the conformance pack rules and our plane rules it's kind of it's kind of all mixed together here i think yeah so it's a bit hard to see what's going on there if we go to the performance pack and clicking again it might show the rules yeah there we go so here's the rules there we're seeing a little bit more information so use a hardware mfa so you know how they're talking about using a security key like what i showed you that i had earlier in the course things like that um i am password policy things like that so you know not too complicated but um i think i'm all done here so what i'm going to do is i'm going to go over to cloudformation and tear that on down but you get the idea well i might want to show you uh drift so there used to be a way it's cause i keep changing things on me here but there's a way to see uh history over time and so that was something that they used to show and i'm just trying to like find where they put it because it is like somewhere else resources maybe ah resource timeline okay so they moved it over into the resource inventory and so if we were to take a look at something anything maybe this here resource timeline and there might not be much here but the idea is it will show you over time how things have changed so the idea is that not only can you say what about config is something compliant but when was it complying and that is something that is really important to know okay so very simple example maybe not the best but the idea is that we can see when it was and was not compliant based on changes to our stuff but anyway that looks all good to me here so i'm going to make my way over to cloudformation actually i already already have it open over here we can go ahead and delete that stack um termination protection is enabled you must first disable it so we'll edit it disable it whatever okay we'll hit delete there and as that's deleting i'm going to go look for and config my original rule there again i'm not really worried about it i don't think it's going to really cost me anything but i'm also just kind of clear the house here just so you're you're okay as well and so if we go over to our rules um the one that i spun up that was custom i think was this one here because these are all grayed out right so i can go ahead there delete that rule type in delete and we are good so there you go that is it all right aws quick starts are prebuilt templates by ada best and ebay's partners to help deploy a wide range of stacks it reduces hundreds of manual procedures into just a few steps the quick start is composed of three parts it has a reference architecture for the deployment a database cloud formation templates that automate and configure the deployment a deployment guide explain the architecture implementation and detail so here's an example of one that you might want to launch like the adabus q a bot and then you will get an architectural diagram a lot of information about it and from there you can just go press the button and launch this infrastructure most quick start reference deployments enable you to spend up a fully functional architecture in less than an hour and there is a lot as we will see here when we take a look for ourselves all right so here is uh it was quick starts where we have a bunch of cloud formation templates uh built by aws or amazon or a best partner networks apn partners and there's a variety of different things here so i'm just going to try to find something like q and a bot q and a bot just type in bot here and i don't know why it was here the other day now it's not showing up which is totally fine but um you know i just want anything to deploy just to kind of show you what we can do with it so you scroll on down we have uh this graphic here that's representing what will get deployed so we have cloudfront s3 dynamodb systems manager lex paulie all these kind of fun stuff and there's some information about how it is architected and the idea is you can go ahead and launch in the console or view the implementation guide let's go take a look here um and there's a bunch of stuff so we have solutions and things like that conversational things like that but what i'm going to do is go ahead and see how far i can get to launching with this it doesn't really matter if we do launch it but it's just the fact that um i wanted to show you what you can do with it so if we go to the designer it's always fun to look at it in there because then we can kind of visualize all the resources that are available and i thought that that would populate over there but maybe we did the wrong things i'm just going to go back and click i'm just going to click out of this oops cancel let's close that leave yes and we will launch that again and so this oh view in the designer hit the wrong button okay so now this should show us the template it might just be loading there we go so this is what it's going to launch and you can see there's a lot going on here i'm just going to shrink that there uh and i don't know if you can make any sense of it but clearly it's doing a lot and so if we were happy with this and we wanted to launch it i know i keep backing out of this but we're going to go back into it one more time we can go here and we go next and then we would just fill in what we want so you name it put the language in and this is stuff that they set up so maybe you want a mail voice set the admin and stuff like that and it's that simple really um and every stack is going to be different so they're all going to have different configuration options but hopefully that gives you kind of an idea of what you can do with quick starts okay let's take a look at the concept of tagging within aws so a tag is a key and value pair that you can assign to any of this resource so as you are creating a resource is going to prompt you to say hey what tags do you want to add you're going to give a key you're going to give a value and so some examples could be something like based on department the status the team the environment uh the project as we have the example here the location and so tags allow you to organize your resources in the following way for resource management so specific workloads so you can say you know developer environments cost management and optimization so cost tracking budgets and alerts operations management so business commitments sla operations mission critical services security so classification of data security impact governance and regulatory compliance automation workload automation and so it's important to understand that tagging can be used in junction with i am policy so that you can restrict access or things like that based on those tags okay all right i just want to show you one interesting thing about tags um and it's just the fact that it's used as the name for some services so when you go to ec2 and you launch an instance uh the way you set the name is by giving it a tag called name and i just want to prove that to you just like one of those little exceptions here so we choose an instance here we go to configure storage and then what we do is we add a tag and we say name and my server name okay and then we go ahead and review and launch we're going to launch this i don't need a key pair so we'll just say proceed without key pair i acknowledge okay and we will go view the instances and you'll see that is the name so um that's just like one of those exceptions or things that you can do with tags if there's other things with tags i have no idea that's just like a a basic one that everybody should know and that's why i'm shown to you with the tags but there you go so we just looked at tags now let's see what we can do with resource groups which are a collection of resources that share one or more tags or another way to look at it it's a way for you to take multiple tags and organize them into resource groups so it helps you organize and consolidate information based on your project and the resources that you use resource groups can display details about a group of resources based on metrics alarms configuration settings and at any time you can modify the settings of your resource groups to change what resources appear resource groups appear in the global console header which is over here and under the systems manager so technically it's part of aws simple systems manager or systems manager interface but it's also part of the global interface so sometimes that's a bit confusing but that's where you can find it okay all right so what i want to do is explore resource groups and also tagging so what i want you to do is type in resource groups at the top here and it used to be accessible not sure where they put it but it used to be accessible here at the top but they might have moved it over to systems manager so i'm going to go to ssm here not sure why i can't seem to find it today and on the left hand side we're going to look for resource groups you all right so what i want to do is take a look at resource groups and i'm really surprised because it used to be somewhere in the global now but i think they might have changed it um and what's also frustrating is if i go over to systems manager it was over here as well and so on the lefthand side i'm looking for resource groups it's not showing up so i don't really the best you keep moving things around on me and i'm i can only update things so quickly in my courses but if you type in resource groups and tag editor it's actually over here um i guess it's its own standalone service now why they keep changing things i don't know but uh the idea is we want to create a resource group so you can create unlimited single region groups in your abel's account use the group to view related insights things like that so i'm going to go ahead and create a resource group you can see it can be tag based or cloud formation based but i don't have any tags i don't really have anything tags so what i'm going to do is make my way over to s3 we're just going to create some resources or a couple resources here with some tags so that we can do some filtration so i can go ahead and create a bucket i'm going to say my bucket uh this like that whoops and then down below i'm going to go down to tags and we're going to say project and we're going to say um rg for resource group okay and then i can go back over here and then i'm going to just say i can say exactly what type i want i'm going to support all resource types and i'm going to say project rg see how it autocompletes and we'll go down below we'll just say my rg a test rg we'll create that and so now we have a resource group and we can see them all in one place resource groups are probably useful for using in policy so you can say say like resource group i am policies that's probably what they're used for okay so before i use i am managed to actually realize groups you should understand i am features things like that and so administrators can use json policies to specify who has access to what and so a policy action a resource group is used following the prefix resource groups so my thought process there is that if you want to say okay you have access to a resource you can just specify a resource group and it will include all the resources within there and so that might be a better way to apply permissions at a per project basis um and that could save you a lot of time writing out i am policies so that's basically all there really is to it also you kind of get an overview of of the resources that are there so that can be kind of useful as well there's the tag editor here i can't remember what you use this for you can set up tag policies tag policies help you standardize tags on resource groups and your accounts use to define tech policies and absorb to attach them to the entire organization um we're not in the org account so i'm not going to show you this and it's not that important but just understand that resource groups can be created and they are used within i am policies in order to um grant or deny access to stuff you go ahead and delete that resource group and really aws stop moving that on me if you move one more time i'm just never going to talk about resource groups again okay hey this is andrew brown from exam pro and we're taking a look at business centric services and you might say well why because an exam guide it explicitly says that these are not covered but the thing is is that when you're taking the exam some of the choices might be some of these services as distractors and if you know what they are it's going to help make sure that you um guess correctly and the thing is that some of these services are useful you should know about them so that's another reason why i'm talking about them here so the first one is amazon connect this is a virtual call center you can create workflows to write callers you can record phone calls manage a queue of callers based on the same proven system used by amazon customer service teams we have workspaces this is a virtual remote desktop service secure managed service for provisioning either windows or linux desktops in just a few minutes which quickly scales up to thousands of desktops we have workdocs which is a shared collaboration service a centralized storage to share content and files it is similar to microsoft sharepoint think of it as a shared folder where the company has ownership we have chime which is a video conference service it is similar to zoom or skype you can screen share have multiple people on the on the same call it is secure by default and can show you a calendar of upcoming calls we have work mail this is a managed business uh email contacts calendar service with support of existing desktop and mobile email client applications that can handle things like imap similar to gmail or exchange we have pinpoint this is a marketing campaign management service pinpoint is for sending targeted emails via sms push notifications voice messages so you can perform um a to b testing or create journey so complex email response workflows we have ses this is a transactional email service you can integrate ses into your application to send emails you can create common templates track open rates keep track of your reputation we have quicksite this is a business intelligence service connect multiple data sources and quickly visualize data in the form of graphs with little to no knowledge definitely you want to remember quicksite ses pinpoint because those definitely will show up in the exam the rest probably not but they could show up as distractors okay hey this is andrew brown from exam pro and we are taking a look at provisioning services so let's first define what is provisioning so provisioning is the allocation or creation of resources and services to a customer and its provisioning services are responsible for setting up and managing those awes services we have a lot of services that do provisioning most of them are just using cloud formation underneath which we'll mention here but let's get to it the first is elastic bean stock this is a platform as a service to easily deploy web apps eb will provision various adwords services like ec2 s3 sns cloud watch ec2 auto scaling groups load balancers and you can think of it as the heroku equivalent to aws then you have opsworks this is a configuration management service that also provides managed instances of open source configuration managed software such as chef and public puppet so you'll say i want to have a load balancer or i want to have servers and it will provision those for you indirectly then you have cloudformation itself this is an infrastructure modeling and provisioning service it automates the provisioning of aws services by writing cloud formation templates in either json or yaml and this is known as iac or infrastructures of code you have quick starts these are premade packages that can be launched and configure your abus compute network storage and other services required to deploy a workload on the bus we do cover this in this course but quick starts is basically just confirmation templates that are authored by the community or by um amazon partner network okay then we have abs marketplace this is a digital catalog for thousands of software listings of independent software vendors that you can use to find buy and test and deploy software so the idea is that you know you can go there and provision whatever kind of resource you want we have abs amplify this is a mobile web app framework that will provision multiple able services as your backend it's specifically for serverless services i don't know i didn't write that in there but you know like dynamodb um things like uh whatever the graphql service is called api gateway things like that then we have aws app runner this is a fully managed service that makes it easy for developers to quickly deploy containerized web apps and apis at scale with no prior information experience required it's basically a platform as a service but for containers we have abas copilot this is a command line interface that enables customers to quickly launch and manage containerized applications any bus it basically is a a cli tool that sets up a bunch of scripts to set up pipelines for you makes things super easy we have aws codestart this provides a unified user interface enabling you to manage your software development activities in one place usually launch common types of stacks like lamp then we have cdk and so this is infrastructure as a code tool allows you to use your favorite programming language generates that confirmation templates as a means of ic so there you go hey this is andrew brown from exam pro and we're taking a look at aws elastic beanstalk before we do let's just define what passes so platform as a service allows customers to develop run and manage applications without the complexity of building and maintaining the infrastructure typically associated with developing and launching an app and so elastic bean stock is a pass for deploying web apps with little to no knowledge of the underlying infrastructure so you can focus on writing application code instead of setting up an automated deployment pipeline or devops tasks the idea here is you choose a platform upload your code and it runs with little knowledge of the infrastructure and aws will say that it's generally not recommended for production apps but just understand that they are saying this for enterprises and large companies if you're a small to medium company you can run elastic beanstalk for quite a long time it'll work out great elastic being stock is powered by cloudformation templates and it sets up for you elastic load balancer asgs rds ec2 instances preconfigured for particular platforms uh monitoring integration with cloudwatch sns deployment strategies like inplace bluegreen deployment has security built in so it could rotate out your passwords for your databases and it can run dockerized environments and so when we talk about platforms you can see we have docker multicontainer docker go.net java node.js ruby php python tomcat go a bunch of stuff and just to kind of give you that architectural diagram to show you that it can launch of multiple things okay hey it's andrew brown from exam pro and in this follow along we're going to learn all about elastic bean stock maybe not everything but we're going to definitely know how to at least use the service so elastic beanstalk is a platform as a service and what it does is it allows you to uh deploy web applications very easily so here i've made my way over to elastic beanstalk open environment and app and then we set up our application we have two tiers a web server environment a worker environment worker environment's great for long running workloads performing background jobs and things like that and then you have your web server which is your web server and you can have both and it's generally recommended to do so um but anyway what we'll do is create a new application so let's say my app here and there's some tags we can do and then it will name based on the environment then we need to choose an environment name so let's say my environment and just put a bunch of numbers in there hit the check availability scroll on down and we have two options manage platform custom platform and i'm not sure why custom is blanked out but it would allow you to um it would allow you to i think use your own containers so i'm a big fan of ruby so i'm gonna drop down to ruby and here we have a bunch of different versions and so 2.7 is pretty pretty new which is pretty good and then there's the platform version which is fine and the great thing is it comes with a sample application now you could hit create environment but you'd be missing out on a lot if you don't hit this configure more options i don't know why they put it there it's a not very good ui but if you click here you actually get to see everything possible and so up here we have some presets where we can have a single instance so this is where it's literally running a single ec2 instance so it's very cost effective you can have it with spot spot pricing so you save money there's high availability so you know if you want it set up with a load balancer an auto scaling group it will scale very well or you can do custom configuration we scroll on down here you can enable amazon xray you can rotate out logs you can do log streaming um there's a lot of stuff here and basically it's just like it sets up most for you but you can pretty much configure what you want as well if we have the load bouncer set if i go here go to high availability now we'll be able to change our load balancer options you have different ways of deploying so you can go here and then change it from all at once rolling immutable traffic splitting depends on what your use case is we can set up a key pair to be able to log into the machine there's a whole variety of things you can connect your database as well so it can create the database alongside with it and then it can actually rotate out the key so you don't have to worry about it which is really nice what i'm going to do is go to the top here and just choose a single instance because i want this to be very cost effective we're going to go ahead and hit create environment and so we're just going to wait for that to start up and i'll see you back when it's done okay okay so it's been uh quite a while here and it says a few minutes so if it does do this what you can do is just give it a hard refresh i have a feeling that it's already done is it done yeah it's already done so and here it says on september 2020 elasticity so i can use etc default default i don't care but anyway so this application i guess it's an appending state i'm not sure why let's go take a look here causes instance has not sent any data since launch none of the instances are sending data so that's kind of interesting because um i shouldn't have any problems you know what i mean so what i'm going to do is just reboot this machine and see if that fixes the issue there but usually it's not that difficult because it's the sample application it's not up to me um as to how to fix this you know what i mean so i'm not sure but um what we'll do is we will let the machine reboot and see if that makes any difference okay all right so after rebooting that machine now it looks like the server is healthy so it's not all that bad right if you do run in issues that is something that you can do and so uh let's go see if this is actually working so the top here we have a link and so i can just right click here it says congratulations your first aws elastic beanstalk ruby application is now running so it's all in good shape there's a lot of stuff that's going on here in elastic beanstalk that we can do we can go back to our configuration and change any of our options here so there's a lot of stuff as you can see we get logging so click the request log so if we click on this and say last 100 lines we should be able to get logging data we have to actually download it i wish it was kind of in line but here you can kind of see what's going on so we have sdo access logs error logs puma logs elastic bean stock engine so you could use that to debug very common to take that over to support if you do have issues for health it monitors the health of the instances which is great then we have some monitoring data here so it gives you like a built dashboard so that's kind of nice you can set up alarms um you have not defined any alarms you can add them via the monitoring dashboard so i guess you'd have to you'd have to somehow add them i don't think i've ever added alarms for um elastic beanstalk but it's nice to know that they have them you can set up schedules for managed events then this is event data so it's just kind of telling you it's kind of like logs it just tells you of things that have changed so there's stuff like that what i'm looking for is to see how i can download the existing application because there's a version uploaded here oh the source is over here okay so i think it's probably over here the one that's running so that's it if it was easy to find what i probably would do is just modify it and oh yeah it's over here so if we go here and download the zip i wonder if it'd be even worth um playing with us so let's i'm just going to see if we can go over to cloud9 and give this a go quickly so if we go over and launch a cloud9 environment maybe we can tweak it and upload a revised version so we'll say create new we'll say eb um environment for elastic beanstalk we'll set it all the defaults that's all fine it's all within the free tier we'll create that environment what i'm going to do is just take this ruby zip file and move it to my desktop and as that is loading we'll give it a moment here i'm just going to go back and i was just curious does it let you download it directly from here no the only thing is that you know if you download that application elastic beanstalk usually has a configuration file with it and so i don't know if they would have given that to us but if it did that would be really great but we just have to wait for that to launch there as well i guess you can save configurations and roll back on those as well um but we will just wait a moment here while it's going i might just peek inside of this file to see what it is this zip contains just going to go my desktop here open up that zip so it looks pretty simple it doesn't even look like a rails app it looks like maybe it's a sinatra app i thought before that they would it would have deployed a ruby on rails application but maybe they keep it really simple um i don't see usually it's like yaml files they use for configuration i don't see that there so it might be that the default settings will work fine there's a config.ru and stuff like that but once cloud9 is up here we will upload this and see what we can do with it okay so there we go cloud9 is ready to go and so if we right click here whoops right click here we should be up be able to upload a file if not we can go up here to the top or it's here or there where is the upload i've i've uploaded things in here so i absolutely know we can i just gotta find it is that the upload upload files cloud9 oh boy that's not helpful that's not helpful at all so let me just click around a little bit here i mean worst case i can always just bring it in via a curl oh upload local files there it is i was just not um being patient okay so we'll drag that on in there and we will did it upload yep it's right there okay great and so we need to unzip it so what i'll do is just drag this on up here i'll do an ls and we'll say unzip ruby.zip and so that unzipped the contents there i think the readme was part of cloud9 so i'm going to go ahead and delete that out not that it's going to hurt anything and so now what we can do we'll delete the original original zip there um and let's see if we can make a change here so i'm just going to open up see what it is so it's yeah it's running sinatra so that's pretty clear there we have a profile to see how it runs we have a worker sample so that just tells how the requests go you don't need to know any of this i'm just kind of clicking through it because i know ruby very well we have a cron yaml file so that could be something that gets loaded in here so i think basically a sinatra app probably just works off the bat here but if we want to make a change we probably just mix up a change over to here so i'll go down here and this is your second aws elastic bean stock application so the next thing we need to do is actually zip the contents here i don't know if it would let us zip it within here but also look like zip the contents of a directory linux just goes to show google is everything so the easiest way to zip a folder um zip everything in the current directory linux okay that's easy so we'll go back over here and we will type in zip and it wants hyphen r for recursive which makes sense and then the name of the zip so ruby2.zip and we'll do period zip warning found is who is zip oh uh yum install zip maybe we have to install uh zip but maybe it's not installed pseudo yum install zip since amazon likes to uses yum and so package already installed so i'm gonna type zip again so zip is there now great oops don't need install twice zip warning ruby two zip not found or empty okay so install zip and use zip hyphen r you can use the flag to best compensate so if that's not working what i'm going to do is just go up a directory why is it saying not found or empty hmm maybe i need to use okay so i think the problem was i was using the wrong flag so i put f instead of r i don't know why i did that so i probably should have done this okay and so that should have copied all the contents of that file so what i'm going to do is go ahead whoops make sure i have that selected and download that file and once i have downloaded that file i'm going to just open the contents to make sure it is what i expect it to be so we're going to open that up and oops get out of here winrar and it looks like everything i want so what i'm going to do is go back over to here i'm going to make sure i have my ruby 2 on my desktop and we're going to see if we can upload another version here so upload deploy choose the file we're gonna go all the way to my desktop here and we're gonna choose ruby two and um like ruby two will be the version name or we'll just say two and we'll deploy and we'll see if that works okay but there are like uh elastic being stock configuration files like gamble files that can sit in the root directory and so generally you're used to seeing them there but you know i imagine that databus probably engineered these examples so that it uses all the default settings but once this is deployed i'll see you back here in a moment okay after a short little wait it looks like it has deployed so what i'm going to do is just close my other tabs here and open this up and see if it's worked it says your second awesome beanstalk ruby application so we were successful uh deploying that out which is really great so what we can do now is just close that tab there and since we have that cloud no environment it will shut down on its own but you know just for your benefit i think that we should shut it off for right now so go ahead and delete that i'm going to go back over to elastic bean stock here and i just want to destroy all of it so we'll see if we can just do that from here terminate the application enter the name so i think we probably have to enter that in there and so i think that oh a problem occurred right exceeded what let's say aws for you so it's not a big deal i would just go and check it again and maybe what we'll do is just delete the application first okay so that one is possibly deleting let's go in here is anything changing can't even tell we'll go ahead oh can't take that one out delete application again if it takes a couple times it's not a big deal it's aws for yes so there's a lot of moving parts so it looks like it is terminating the instance and so we just have to wait for that to complete uh once that is done we might have to just tear down the environment so i'll see you back here when it has finished tearing this down okay all right so after a short little wait here i think it's been destroyed we'll just double check by going to the applications going to the environments yeah and it's all gone probably because i initially deleted that environment and then took the application with it so i probably didn't have to delete the app separately um but uh yeah so there you go just make sure your cloud9 environment's gone and you are aokay there'll probably be some like lingering s3 buckets so if you do want to get rid of those you can it's not going to hurt anything having those around but they do tend to stack up after a while which is kind of annoying so if you don't like them you can just empty them out as i am doing here whoops i'll just permanently delete copy that text there then go back to here and then just go take out that bucket let's delete that there oh if you get this this is kind of annoying but uh elastic beanstalk likes to put in an imp permission or policy in here so if you go down here there's a bucket policy you just have to delete it out it prevents it from being deleted and we'll go back over here and then we will delete it okay and yeah there we go that's it so let's take a look at several services on aws and this is not including all of them because we're looking at the most purely serverless services uh if we try to include all the server services it would just be too long of a list but let's take a look here so before we do let's just redefine what is serverless so when the underlying servers infrastructure operating system is taken care by the csp serverless is generally by default highly available scalable cost effective you pay for what you use the first one is dynamodb which is a serverless nosql key value and document database it's designed to scale to billions of records with guaranteed consistent data returned in at least a second you do not have to worry about managing charge you have simple storage service s3 which is a serverless object storage service you can upload very large and unlimited amounts of files you can pay for what you store you don't worry about the underlying file system we're upgrading the disk size we have ecs fargate which is a servless orchestration container service is the same as ecs except you pay on demand per running container with ecs you have to keep a ec2 server running even if you have no containers running where aws manages the underlying server so you don't have to scale or upgrade the ec2 server we have aws lambda which is a serverless function service you can run code without provisioning or managing servers you upload a small piece of code choose how much memory you want how long you want the function is allowed to run before timing out your charge based on the runtime of the service function rounded to the nearest 100 milliseconds we have step functions this is the state machine service it coordinates multiple services into serverless workflows easily share data among lambdas have a group of lambdas wait for each other create logical steps also work with fargate tasks we have aurora serverless this is a serverless ondemand version of aurora so when you want most of the benefits of aurora but trade you have to trade off those cold starts or you don't have lots of traffic or demand so things several services that we could have put in here as well is like api gateway appsync it was amplify um and those are like the the first two were application integrations you could say sqs sns those are all serverless services but you know again we'd be here all day if i i listed them all right all right let's take a look at what is serverless and we did look at it from a server perspective earlier in the course but let's just try to abstractly define it and talk about the architecture so serverless architecture generally describes fully managed cloud services and the classification of a cloud service being serverless is not a boolean answer it's it's not a yes or no but an answer on a scale where a cloud service has a degree of serverless and i do have to point out that this definition might not be accepted by um everybody because serverless is one of those uh terms where we've had a bunch of different cloud service providers define it differently and then we have thought leaders that have a particular concept of what it is so you know i just do my best to try to make this practical here for you but a servless service could have all or most of the following characteristics and so it could be highly elastic and scalable highly available highly durable secure by default it abstracts away the underlying infrastructure and are built based on the execution of your business tasks a lot of times that that cost is not uh it's not always represented as something that is like i'm paying x for compute it could be abstracted out into some kind of um credit that doesn't necessarily map to something physical then we have serverless can scale to zero meaning when it's not in use the serverless resources cost nothing uh and these two last topics basically pull into pay for value so you don't pay for idle servers you're paying for the value that your service provides and my friend daniel who runs the serverless toronto group he likes to describe serverless as being similar to like energy efficient rating so an analogy of service could be similar to energy rating labels which allows consumers to compare the energy efficiency of a product so some services are more serverless than others and again you know some people might not agree with that where there's a definitive yes or no answer but i think that's the best way to look at it okay hey it's andrew brown from exam pro and we're taking a look at windows on database so abs has multiple cloud services and tools to make it easy for you to run window workloads on aws so let's get to it so the first is windows servers on dc2 so you can select from a number of windows server versions including the latest version like windows server 2019 for databases we have sql server on rds you can select from a number of sql server database versions then we have aws directory service which lets you run microsoft active directory ad as a managed service we have aws license manager which makes it easier to manage your software licenses from software vendors such as microsoft we have amazon fsx for windows file server which is a fully managed scalable storage built for windows we have the aws sdk which allows you to write code in your favorite language to interact with a database api but it specifically has support for net a language favorite for windows developers we have amazon workspaces so this allows you to run a virtual desktop you can launch a windows 10 desktop to provide secure and durable workstations that is accessible from wherever you have an internet connection about lambda supports powershell as a programming language to write your serverless functions and we have abs migration acceleration program map for windows is a migration methodology for moving large enterprises items has amazon partners that specialize in providing professional services for map this is not just everything for windows on aws like if you want to move your sql server over to rds postgres i believe they've like created an adapter to do that but yeah hopefully that gives you an idea what you can do with windows on aws okay hey this is andrew brown from exam pro and i want to show you how you can launch a windows server on aws so what you're going to do is go to the top here and we are going to type in ec2 and from here uh what we'll do is we'll go ahead and launch ourselves a new ec2 instance and we are going to have a selection of instances that we can launch and so we're looking for the microsoft windows server and this is interesting there's actually a free tier eligible that is crazy because if you go over to azure they don't have a free tier windows server like any bus does so that's pretty crazy um and it runs on a t2 micro no that can't be right there's no way it can run a tt micro that seems like that's too small let's try it okay i just don't believe it because when you use azure you have to choose a particular size of instance by default and it's a lot more expensive and there is no free tier so we'll go here there are free tiers just not really for windows in particular so we'll go here this looks good security groups this opens up rdp so we can get into that machine we're gonna go next here and launch this machine says if you plan to use ami the benefits the microsoft license mobility check out this form that's not something we're worried about today and i mean i guess we can create a key pair i'm not sure what it we would use a key pair for here um for windows amis the private key file is required to obtain the password used to log into the instance okay so i guess we're going to need it so windows key great we'll launch that instance and uh i'll see you back here when it launches but i just don't believe that it would launch that fast you know all right so after a short little wait here the server is ready and so let's see if we can actually go ahead and connect to this so i'm going to hit connect here and we'll go over to rdb client so you connect to your windows instance using your remote desktop client of your choice and downloading and running the rdb shortcut below so i'm going to go ahead and download this and you're going to have to be on a windows machine to be able to do this or have an rdb client installed i think there's one for mac that you can get from the apple store but all i'm going to do is just double click the file so you probably can't see it here i'm just going to expand this trying to oh my computer is being silly but anyway there we go we moved it over there i'm just going to drag over here and just double click this image so you can see that i'm doing it i'm saying connect okay and that's going to ask for a password so i'm going to hope that i can just click that and get the password so to decrypt the password you will need your key pair instance you'll have to upload that and i don't know if i remember having to do that before but it's a great security measure so i'm fine with it i'm going to drag my key to my desktop so i can see what's going on there as well and we're going to go grab that and decrypt the password and so now um where's our password oh it's right here okay so we're going to grab that password there we will paste that in said okay say yes and see if we can connect to this instance if this is running on a t2 micro i'm going to lose it because that is just cheap it just just doesn't seem possible to me because again on azure you have to launch an instance with a lot of stuff and it just uh seems uh crazy what's also interesting is that itabus uh on windows like launches so fast it's unbelievable how fast these servers spin up and it's just very unusual but yeah so we are in here um it's not asking me to activate or anything so i guess there's already a windows license here and i'm not sure if there's any kind of like games installed like do we have minesweeper can i play minesweeper on here it's a data center server so i'm assuming not but yeah so this is a windows server and it's pretty impressive that this works i'm not sure if this is going to have an outbound connection here um just because we probably would have to configure it let's just say okay i just i really don't think it's going to go out to the internet by default yeah so you'd probably have to do some stuff you know oh no there we go so yeah we got to the internet so it's totally possible but uh yeah that's about it that's all i really wanted to show you so what i'm going to do is just go back to ec2 and we're going to shut down the server here just expand that there and we will go here and we will terminate that instance good we'll give that a refresh that's shutting down and we are done hey this is andrew brown from exam pro and we are taking a look at abyss license manager and before we do let's talk about what byol or bring your own license means so this is the process of reusing an existing software license to run vendor software on a cloud vendor's computing service byol allows companies to save money since they may have purchased the license in bulk or a time that provided a greater discount than if purchased again and so an example of this could be the license mobility provided by microsoft's volume licensing to customers with eligible server applications covered by the microsoft software assurance program uh and i don't know what i was trying to do there i guess maybe it was just sa and i missed the parentheses there on the end no big big deal but aws license manager is a service that makes it easier for you to manage your software licenses from software vendors centrally across aws in your onpremise environments able's license manager software that is licensed based on virtual cores physical cores sockets or a number of machines this includes a variety of software products for microsoft ibm sap oracle and other vendors so that's the idea you say what is my license type it's it's bound to this amount of cpus items license manager works with ec2 with dedicated instances dedicated hosts and even spot instances and for rds there's only for oracle databases so you can import that license for your oracle server just understand that if you're doing microsoft windows servers or microsoft sql server license you're generally going to need a dedicated host because of the assurance program and this can really show up on your exam so even though ava's license manager works on dedicated instances and spot instances just trying to gravitate towards dedicated hosts on the server or on the exam okay all right let's take a look at the logging services that we have available in aws so the first one here is cloudtrail and this logs all api calls whether it's sdk or the cli so if it's making a call to the api it's going to get tracked between aws services and this is really useful to say who can we blame who was the person that did this so who created this bucket who spent up that expensive ec2 instance who launched the sagemaker notebook and the idea here is you can detect developer misconfigurations detect malicious actors or automate responses through the system then you have cloudwatch which is a collection of multiple services i commonly say this is like an umbrella service because it has so many things underneath it so we have cloudwatch logs which is a centralized place to store your cloud services log data and application logs metrics which represents a time ordered set of data points a variable to monitor event bridge or previously known as cloudwatch events triggers an event based on a condition so every hour take a snapshot of the server alarms triggers notifications based on metrics dashboards creates visualizations based on metrics and that's not all of the things that are under cloud watch but those are the core five ones you should always know um absolutely there then we have aws xray this is for distributed tracing systems so you can use it to pinpoint issues within your services so you see how data moves from one app to another how long it took to move and if it failed uh to move forward okay let's take a closer look here at ibis cloud trail because it's a very important service so it's a service that enables governance compliance operational auditing and risk auditing of your adwords account and the idea is that every time you make an api call it's going to show up as some kind of structured data that you can interact with or read through so this cloud trail is used to monitor api calls and actions made on the database account easily identify which users and accounts made the call to aws so you might have the where so the source ip address the when the event time the who the user agent and the what the region resource in action so i'm just gonna get my pen tool out here for a moment and just notice you have the event time so when it happened the source the name the region the source ip address the user agent uh who was doing it so here was laforge of the response element so you know it's very clear what is going on here um and then you know cloudtrail is already logging by default and we'll collect logs for the for the last 90 days via event history if you need more than 90 days you need to create a trail which is very common you'll go into aws and make one right away trails are outputted to s3 and do not have gui like event history to analyze the trail you have to use amazon athena and i'm sure there are other ways to analyze it within aws but here's just what the event history looks like so right off the bat you can already see that there are information there i'm not sure if they've updated the ui there they might have uh as even when i'm recording this i kind of feel like if we go into the follow along which we will um i bet they might have updated that the idea here is that you know you can browse the last 90 days but anything outside of that you're gonna have to do a little bit of work yourself okay so we're not going to cover all the cloudwatch services there's just too many but let's look at the most important ones and one of the those important ones is cloudwatch alarms so cloudwatch alarms monitors a cloudwatch metric based on a defined threshold uh so here you can see there's kind of a condition being set there so if the networking is greater than 300 for one data point within five minutes it's going to breach an alarm so that's when it goes outside it's defined threshold and so the state's going to either be something like okay so the metric or expression is within the defined threshold so do nothing alarm the metric or expression is outside of the defined threshold so do something or insufficient data the alarm has just started the metric is not available not enough data is available and so when the state has changed you can define actions that it should take and so that could be doing a notification auto scaling group or an ec2 action um so cloudwatch alarms are really useful for a variety of reasons the one that we will come across right away will be setting up a billing alarm so let's take a look here at the autonomy of an alarm and so i have this nice graphic here to kind of explain that there and so the first thing is we have our threshold condition and so here you can just set a value and say okay the value is a thousand or a hundred whatever you want it to be and this is going to be for a particular metric the actual data we are measuring so maybe in this case we're measuring network in so the volume of incoming network traffic measured in bytes so when using fiveminute monitoring divide by 300 we get bytes per second if you're trying to figure out that calculation there you have data points so these represent the metrics measurement at a given point then you have the period how often it checks to evaluate the alarm so we could say every five minutes uh you have the evaluation period so the number of previous periods and the data points to alarm so you can say one data point is breached in evaluation period going back four periods so this is what triggers the alarm uh the thing i just want you to know is that you can set a value right and that it's based on a particular metric and there is a bit of logic here in terms of the alarm so it's not as simple as just it's breached but there's this period thing happening okay well let's take a look at cloudwatch logs so to understand that we have logs streams and log groups so a log stream is a stream that represents a sequence of events from an application or instance being monitored so imagine you have an ec2 instance running a web application and you want those logs to be streamed to cloudwatch logs that's we're talking about here so you can create log streams manually but generally this is automatically done by the service you are using unless you were collecting application logs on an ec2 instance as i just described here is a log group of a lambda function you can see the log streams are named after the running instance lambda's free frequency run on new instances so the stream contains timestamps so what i'm trying to say here is that there's a variety of different services lambda rds what have you and they already send their logs to cloudwatch logs and and they're going to vary okay so here's a log group of an application log running on ec2 you can see here the log streams are named after the running instance id here is the log group for aws glue you can see the log streams are named after the glue jobs and so you know we have the streams but let's talk about the actual data that's made up of it the log events so this represents a single event in a log file log events can be seen within the log stream and so here's an example of you would open this up in cloudwatch logs and you can actually see what what was being reported back by your server you can filter these events to filter out logs based on simple or pattern matching syntax so here i'm just typing in saying give me all those debug stuff and you know this is a very robust but awes does have a better way of analyzing your logs which is log insights which we'll look at here in a moment so we're just looking at cloudwatch log events and how those are collected but there's an easier way to analyze them and that's with login sites so you can interactively search and analyze your cloudwatch log data and it has the following advantages more robust filtering than using the simple filter in the in a log stream less burdensome than having to export logs to s3 and analyze them via athena cloudwatch login site supports all types of logs so cloudwatch log insights is commonly used via the console to do adhoc queries against log groups so that's just kind of an example of someone writing a query and cloudwatch log insights uses a query syntax so a single request can query up to 20 logs create timeout after 50 minutes if not completed and queries results are available for seven days so abras provides sample queries that you can get started for common tasks and and ease the learning into the query syntax a good example is filtering vpc flow logs so you go there you click it and you start getting some data you can create and save your own queries to make future repetitive tasks easier on the certified cloud partitioner they're not going to ask you all these details about this stuff but i just conceptually want you to understand that in log insights you can use it to robustly filter your logs based on this query syntax language you get this kind of visual and it's really really useful let's take a look here at cloudwatch matrix which represents a time ordered set of data points it's a variable that is monitored over time so cloudwatch comes with many predefined metrics that are generally namespaced by aw services uh so the idea is that like if we were to look at the ec2 it has these particular matrixes so that we have cpu utilization discrete ops disk write ops disk read bytes disk write bytes network in network out network packet in network packets out and the idea is that you can just like click there into ec2 and then kind of get that data there and so cloud metrics are leveraged by other things like cloudwatch events cloudwatch alarms cloudwatch dashboards so just understand that okay all right so what i want to do in this follow along is show you a bit about cloudtrail so we're going to go to the top here and type in cloudtrail the great thing about cloudtrail is it's already turned on by default so it's already kind of collecting some information and so it's here it says now use i am access analyzer on cloud trail trails that sounds pretty cool to me but we shouldn't have to create a trail right off the bat because we'll have some event history and the event history allows us to see things that are happening within our account in the last 90 days but the thing is if you want something beyond 90 days you're going to have to create a trail but if we just take a look here we can kind of see as we've been doing a lot of things all the kind of actions that's been happening so here we have an instance that i terminated so if i go in here and and look at it i can kind of see more information about it so we can see when it terminated who had done that what access key they had used the event source the request id the source ip what whether it was read only what was the event type that was called the resource there and this is the actual raw record so this is generally how i would look at it or this is how you had to look at it back in the day but the idea is that you would have that user identity described the event time the source the event name the region the source ip the the agent all the information there okay and so this is a great way to kind of find stuff so you can go through here and try to debug things this way so you can go to the event name and so if you if you go here you can kind of get uh see a bit of stuff here so if i was just trying to say like maybe create i'm just trying to find something that i know that i've been doing like create access keys i can see the access keys that have been created within this sandbox account here for the user and things like that so it's a great way to kind of find things but generally you're going to always want to turn on uh or create your own trail so if you go here and hit create trail say my new trail and um you're gonna need an s3 bucket for that you'll probably want encryption turned on which sounds good to me you'll absolutely want log file validation and generally you don't want to store your your cloudtrail logs within the existing account you want to have a isolated hardened account that's that is infrequently accessed or only by your your cloud security engineers away from here because you don't want people tampering with it deleting it or changing stuff but let's take an existing one here i don't want a customer manager don't i have one that is managed by aws here new custom um let's choose that one i don't know which one that is we'll just hit next usually adamus gives you a managed key there so i was kind of surprised you can also include additional data so if you do data events this would collect information from s3 but the thing is you might not want to track everything because if you track to everything it can get very expensive very quickly but if you don't you just leave on management events it'll save you more money there's inside events uh this is new i haven't seen this yet so i didn't identify unusual activity errors users of behavior that sounds really good but these could come also at additional charges but i'm going to hit next anyway for fun i'm going to create that trail okay and uh the key policy does not grant sufficient access to etc etc so i'm gonna go turn that off even though i should really have it turned on but i just want to be able to show you this okay so we have this new trail and so this trail is being dumped to s3 so we might not be able to see anything in here as of yet but i'm just going to pop over here and just see right i probably have one in my other account but it's not it's not that important we basically saw what the data would look like so we go into here there's a digest i don't remember there being a digest so that's nice so there's no data yet but when there is it will pop into there um i'm not sure if we're gonna be able to do anything with insights here at least not in this account insights are events that are showing usual api activity and things like that so that's kind of cool i don't know what cloudwatch insights looks like uh inside events are shown in the table for 90 days okay so i'm just curious if we can see kind of a screenshot of what that looks like whoops well at least on the article here so i guess you could kind of get like some kind of graphs or something saying like hey this looks unusual and they might select it so not pretty clear in terms of what that looks like but i mean sounds like a cool feature and i'm sure when i i'm working on my security certification course i will definitely include them there but that's pretty much all there is to it i'm going to go ahead and delete that trail because i i just don't really need it in this account but generally you always want to go in and create a trail and what you can do is if you're in your root account i'm not this is actually a an account that's part of an organization but if you're at that organization level you can create a trail that ex that spans all the regions that spans all the interest accounts with an organization and that's what you should be doing okay but that's about it hey this is andrew brown from exam pro we're looking at ml and ai services on aws but let's first just define what is aiml and deep learning so ai also known as artificial intelligence is when machines that perform jobs that mimic human behavior ml or machine learning are machines that get better at a task without explicit programming and deep learning or dl are machines that are have an artificial neural network inspired by the human brain to solve complex problems and a lot of times you'll see this kind of onion where they're showing you that you know ai can be using ml or deep learning and then deep learning is definitely using machine learning but it's using neural networks and so for aws their flagship product here is amazon sagemaker it is a fully managed service to build train deploy machine learning models at scale um and there's a bunch of different kind of open source frameworks you can use with it like apache mx net audios which is an open source deep learning framework that is the one that it has decided to say hey we are going to back this one and so you'll see a lot of example code for that one we have tensorflow that you can use pie torch hugging face other things as well okay and so there's a lot of services underneath some that might be of interest to mention right away is like amazon sagemaker ground truth which is a data labeling service where you have humans that label a data set that will be used to train machine learning models or maybe something like amazon uh augmented ai so human intervention review services when sagemaker uses machine learning to make a prediction that is not confident that it has the right answer queue up to predict for a human review and these are all about just labeling data um you know when you're using supervised um supervised learning but there are a lot of services under sagemaker itself and just ai services in general so we'll look at that next okay all right let's take a look at all the ml and ai services and there's a lot on aws so the first is amazon code guru this is a machine learning code analysis service and code guru performs code reviews and will suggest to improve the code quality of your code it can show visual code profiles to show the internals of your code to pinpoint performance next we have amazon lux this is a conversation interface service with lux you can build voice and text chat bots we have amazon personalized this is a realtime recommendation service it's the same technology used to make product recommendations to customers shopping on the amazon platform then we have amazon poly this is a texttospeech service upload your text and an audio file spoken by synthetic synthesize voice and that will be generated you have amazon recognition this is an image and video recognition service uh analyze image and videos to detect and label objects peoples and celebrities then we have amazon transcribe this is a speech to text service so you upload your audio and it'll be converted into text we have amazon text extract this is an ocr tool so it extracts text from scanned documents when you have paper forms and you want to digitally extract that data you have amazon translate this is a neural machine learning translation service so use deep learning module models to deliver more accurate and natural sounding translations we have amazon comprehend this is an nlp so natural language processing service find relationships between text to produce insights looks at data such as customer email support tickets social media and makes predictions then we have amazon forecasts this is a time series forecasting service and it's you know uh i mean technically i guess it's a bit of a database but the idea here is that it can forecast business outcomes such as product demand resource needs or financial uh performance and it's powered by ml or ai if you want to call it we have aws deep learning amis so these are amazon ec2 instances they're preinstalled with popular deep learning frameworks and interfaces such as tensorflow pytorch apache mxnet chainer gluon uh horovod and kires we have adabus deep learning containers so docker images instances preinstalled with popular deep learning frameworks interfaces such as tensorflow pytorch apache mxnet we have aws deep composer this is machine learning enabled musical keyboard i don't know many people using this but it sounds like fun it was steep lens is a video camera that uses deep learning it's more of like a learning tool so again we don't see many people using this airbus deep racer is a toy race car that can be powered with machine learning to perform autonomous driving again this is another learning tool for learning ml they like to do these at re invent to have like these racing competitions amazon elastic interface so this allows you to attach lowcost gpu perform powered acceleration to ec2 instances to reduce the cost of running deep learning interfaces by 75 percent we have amazon fraud detector so this is a fully managed fraud detection as a service uh it identifies potentially fraudulent online activities such as online payment fraud and the creation of fake accounts amazon kendra so this is an enterprise machine learning search engine service it uses natural language to suggest answers to questions instead of just simple keyword matching so there you go hey it's andrew brown from exam pro and we're going to do a quick review here of the big data and analytic services that are on aws but before we do let's just define what big data is so it's a term used to describe massive volumes of structured or unstructured data that is so large it is difficult to move and process using traditional database and software techniques so the first tier we have is amazon athena this is a serverless interactive query service it can take a bunch of csv or json files in an s3 bucket and load them into a temporary sql table and so you can run sql queries so it's one you want to query csv or json files if you've ever heard of apache presto it's basically that okay then we have amazon cloud search so this is a fully managed full text search service so when you want to add search to your website we have amazon elastic search service commonly abbreviated to es and this is a manage elastic elasticsearch cluster and elasticsearch is an open source fulltext search engine it is more robust than cloud search but requires more server and operational maintenance then we have amazon elastic mapreduce commonly known as emr and this is for data processing and analysis it can be used for creating reports just like redshift but is more suited when you need to transform unstructured data into structured data on the fly and it leverages open source um technology so like spark um hive pig things like that then we have kinesis data stream so this is a real time streaming data service it creates producers which sends data to a stream it has multiple consumers that can consume data within a stream and use it for realtime analytics click streams ingestion data from a fleet of iot devices then we have kinesis fire hose this is a serverless and a simple version of a data stream and you pay on demand based on how much data is consumed through the stream and you don't worry about the underlying servers then you have amazon kinesis data analytics this allows you to run queries against data that is flowing through your realtime stream so you can create reports and analysis on emerging data and last on the kinesis side here we have amazon kinesis video streams this allows you to analyze or apply processing on realtime streaming videos on the second page here we have managed kafka service msk and it might be mks now that i'm looking at it here so just be aware that that might be incorrect but a fully managed apache kafka service kafka is an open source platform for building realtime streaming data pipelines and applications it is similar to kinesis but with more robust functionality then we have redshift which is um it was this flagship big data tool it's a petabyte size data warehouse the data warehouses are for online uh online analytical processing olap so data warehouses can be expensive because they are keeping data hot meaning that we can run a very complex query and a large amount of data and get that data back very fast but this is great when you need to quickly generate analytics or reports from a large amount of data we have amazon quick site this is a business intelligence tool or business intelligence dashboard bi for short you can use it to create business dashboards to power business decisions it requires little to no programming and connect and adjust to many different types of databases have you ever heard of tableau or power bi this is just the aws equivalent we have aw data pipelines this automates the movement of data you can reliably move data between compute storage and services we have abs glue this is an etl service so it allows you to move data from one location another where you need to perform transformations before the final destination it's similar similar to dms but it's more robust we have abus lake formation this is a centralized curated and secured repository that stores all your data so it's a data lake it is a storage repository that holds a vast amount of raw data in its native format until it is needed and then last on here we have aws data exchange this is a catalog of thirdparty data sets you can download for free or subscribe or purchase data sets so they might have like the kovid 19 foot traffic data the imdb tv movie data historical weather data and sometimes this is really great if you're just trying to learn how to work with these tools okay hey this is andrew brown from exam pro and we are taking a look here at amazon quick site which is a business intelligence dashboard or bi dashboard that allows you to ingest data from various database storage or database services to quickly visualize business data with minimal programming or data formula knowledge so here's an example of a quick site dashboard and so the way quicksite is able to make these dashboards super fast is via spice the super fast parallel in memory calculation engine and the thing is you don't have to use spice but generally it is good to use it and there are some caveats when getting your data into quicksite sometimes it can't ingest it directly from a particular data store so you might have to dump it to s3 first but it's not too bad because you can use it with glue to transform that data over um there are additional features sometimes marketed services but we have quick site ml insights this detects anomalies perform accurate forecasting it can generate natural language narratives so basically like you know describe it as if you're going to read it out as a business report you know then there's amazon quick site queue this allows you to ask questions using natural language on all your data and receive answers in seconds so there you go hey this is andrew brown from exam pro and let's go take a look at amazon quick sites which is a or quick site which is a business intelligence tool so when you go here you have to sign up because it's kind of part of aws but on its own separate thing and then you have to choose what you want so we have enterprise and standard um i do not want to pay that much so i'm going to go to standard over here i'm not really sure what the difference is it's not really telling me what between standard and enterprise but i'm going to assume standard is more cost effective but here we it says user use i am federator identities which is fine use i am federal identities only um we can stick with the top one there that seems fine to me we need to enter a name so we'll just say my quick site account and we probably have to fill something in there so let's say andrew example co and these are the services that are going to integrate with athena s3 rds things like that i guess we could select some of those buckets i'm not too worried about doing that right now the provided account name is not available that is a terrible ui but that's aws for you so i'm just going to dump some numbers there i'm going to put my email in here again um we probably want some s3 buckets i'm going to make a new bucket because i think that's how we're going to do this we're going to have to make a bucket here and say quick cite data okay and we're gonna create ourselves a bucket here i'm gonna go back and hopefully that shows up uh it does not so what i'll have to do is just back out and i'm just gonna give it a hard refresh here and we'll hit quick sign up for quick site again and we'll choose standard and we'll say my quick site account a bunch of numbers there android example.co i don't really care about ingesting data from everywhere else i just want it from s3 there's my data sure we'll give it right permissions even though i don't plan to do anything with athena here today and we'll give it a moment to load so what i'm thinking is so what i'm thinking is just making like an excel spreadsheet here and just filling in some data so oh it says our account is set up here so we'll go to quick site because i bet i can import like a csv or something um i'm more of a tableau or power bi kind of person um but uh you know for the purpose of the cloud practitioner i am going to show you this amazon quick set lets you easily visualize data and etc that sounds great next next next i know what i'm doing oh do we have some examples great so i don't even have to make a spreadsheet okay so what we'll do is just click on that and we have stuff it looks like they've really improved this since the last time i've seen it which is quite nice but i could try and make my own i'm just trying to think how do we do this again yeah we have the spice there so it's a lot easier from starting from scratch i'm just gonna say close and these are analysis we want data sets in here oh we already have some data sets these are coming from s3 i think that's the old s3 logo i'm not sure why they're using that one we can go here and create a new data set oh we can upload directly so i don't even have to use s3 that's great so what i'm going to do is just have some values in here so i'm going to just say um type value so we'll say banana 125 123 we'll say apple 11 orange nobody likes oranges i shouldn't say i'm sure it's like lots of people like oranges oh we gotta put pears on there i actually really like paris people think i like bananas which is not true i actually like pears that's what i like so i'm going to go ahead and save this save as and i'm just going to save this to my desktop here so just give me a moment just doing this off screen and i'm going to save this uh data set quick site csv it can even take an xls so i don't have to save it as a uh i'll just save it as an xls okay and so we're going to just upload that so there is that data set it's going to scan that file it's going to see that sheet you can even preview it there's the information we're going to add that data i get added as a data data set well how do i where do i it's like it says add the data i just want to add it as a data set so they set up here maybe save and visualize up here and is it autographing it maybe if i drag in is it working is it thinking okay it's 100 so i'm going to just drag that onto there and it says pear orange banana just kind of trying to make sense of this here is it taking in count the value maybe put the value down there wow that's so much easier i haven't used this for like a year and um i'm gonna tell you this has gotten a lot easier to use so i'm quite impressed with this but yeah i mean this is pretty much what quicksite is if you want to visualize things in different types you can drag them out you can probably like click on the wheel here and change it again i'm not sure exactly how all the uh the dials and knobs work here but i mean another thing we could do is just drag out like another object and do the same thing so maybe i'd want a pie chart um so add a visual yeah it's not as nice as power bi but like it's still great that it's here you know type value so we got a nice pie chart there uh let's try something weird let's give this one a go doesn't color it which is not very nice um there's probably some kind of way to color it but focus on banana only i don't know i don't know the point of there but anyway that's quick site so um i really don't want to pay for this so what i'm going to do is go up here um there's you have to deactivate i'm just trying to remember how because they change the interface again they change everything on you so there we go i'm on a trial for four days here maybe quantity four just the four 29 day trial so if i want to get out of this trial what do i do i don't want to use it anymore um so how to delete aws quicksite canceling your subscription so before you can unsubscribe uh you're assigned in the im account your quick site administrator you're the root i am administrator sure you deleted any secondary namespaces to find the existing namespace etc so choose your username in the application bar to quick site account settings unsubscribe so i was almost there i thought i was in the right place uh this one no i was just there manage quick site your subscriptions edit there's no unsubscribe option so i'm not sure can i cancel unsubscribe button does not appear in quick site okay just because we're on trial and so maybe after the end of the trial it will uh it will vanish there they are not making this easy for me account settings ah delete accounts this is what we probably want to do permanently delete the account yes i mean that has to get rid of the description because it gets rid of everything there we go we'll say confirm delete account unless you're using them in the services blah blah blah successful okay great so now i should go back to adress.amazon.com and just to confirm that it's gone i'm going to go to quicksite again and just see if it's trying to ask me to sign up again so it is so i've gotten rid of my account so we're all in good shape and uh yeah that is that is quick site hey this is andrew brown from exam pro and we are taking a look at the aws well architecture framework so this is a white paper created by aws to help customers build using best practices defined by aws you can find this at adabus.amazon.com forward slash architecture forward slash well architected this idea is not unique to aws the other providers have it but i believe aimbots was the first one to define this and they have a really good uh a good approach to this and this is pretty much essential knowledge that you have to have four certifications when we're looking at the cloud practitioner the system architect associate and professional because there's a lot of principles here best practices that adabus uses themselves to architect their infrastructure okay so the framework is divided into five sections called pillars which address different aspects or lenses that can be applied to a cloud workload so imagine you have your cloud workload you're going to want to adopt that as well architect framework some things that you know people don't consider outside the five pillars is that you need to know general definitions uh general design principles and the review process and then from there you have your five pillars so you have operational excellence security reliability performance efficiency and cost optimization and all these have major sections in this white paper but outside of just the main white paper each of these have their own white papers that go even into farther detail so if you really want to really focus on security and get a lot more information they have that as well okay let's take a look at the general definitions for the well architecture framework starting with the pillars so the operational excellent pillar is there to run and monitor systems the security pillar is to protect data and systems to mitigate risk the reliability pillar is to mitigate and recover from disruptions the performance efficiency pillar is about using computing resources efficiently or effectively and the cost optimization pillar is about getting the lowest price and this is where you're going to find all the business value and i put an asterisk there because you know you might obsess saying we need to meet the requirements for all these pillars and that's not the case you can trade off pillars based on the business context so you know don't take it as literally implement every single thing but just consider that uh you know you might have to adapt it based on your workloads then we have some general definitions that we will come across so there's components so code configuration it was resources against a requirement a workload so a set of components that work together to deliver business value milestones so key changes of your architecture through the product lifecycle then there's architecture itself so how components work together in a workload and then we have technology portfolio so a collection of workloads required for the business to operate okay so the well architected framework is designed around a different kind of team structure so when you're looking at enterprises they generally have a centralized team with specific roles where adabas structures their teams as being distributed with flexible roles and so this new kind of methodology of distributed teams uh has some major advantages but it does come with some risks and so aws has baked in some uh practices or uh things that they do to mitigate these issues okay so let's compare onpremise enterprise uh to what abuse is proposing for your team structure so onpremise what we'd see is a centralized team consisting of technical architects solution architects data architects network architects security architects and you kind of see that they all have a specialized vertical and they are usually managed by either togaf or zac uh man framework so those are just ways of structuring your teams those are very popular and so what a bus is proposing here is that you have a distributed team and the way you're going to make that team work because obviously just thinking about a distributed team they're going to be a lot more agile but to make sure that they effectively work you have practices like team experts who raise the bar making sure that you know in any areas we can always say how can we do this better then there are mechanisms in place for automated checks for standards so that's the great thing about cloud can all be automated to say hey does it meet our regulatory compliance or what have you and then there's the concept of the amazon leadership principles which we will cover on in the next slide in detail and so um you know itabus is not obviously using uh these other frameworks because it has its own which is this one here but the mechanism to which they stay organized and up to date is they are supported by a virtual community of subject matter experts principal engineers so that what they'll do is they'll engineer things like lunchtime talks and then recycle that into their onboarding material or into this framework itself okay so we're taking a look here at amazon's leadership principles and these are a set of principles used during the company's decision making problem solving simple brainstorming and hiring all right um and so i can't say that i like all of these but uh definitely some of them really stand out as being great especially the first one which is customer obsession so instead of worrying about what your competitors are doing think about what the customer wants work your way back and you know really focus on the customers needs then there's ownership so if you're going to go do something you know try to be your own mini boss uh and take responsibility for whatever it is you're building event and simplify so you know always look for the simplest solution don't try to engineer something super complicated if it's not necessary are right a lot so you know try to be right uh learn and be curious so that's pretty selfexplanatory hire and develop the best insist on the high standards aws always refers to this as raising the bar think big bias for action frugality and abuse is really frugal if you didn't know that but not just for like themselves but also for their customers they want customers to spend the least amount of money possible when using their infrastructure earn trust dive deep have a backbone disagree and commit deliver results strive to be the earth's best employer success and scale bring broad responsibility and if you want to read these in detail because they have a big block of text for each of these you can go to amazon.jobs for en forward slash principles and read all about it okay all right let's talk about some general design principles that you should be considering when you are designing your infrastructure no matter what pillar that you are looking to adopt the first is stop guessing your capacity needs so the great thing with cloud computing is you use as little or much based on demand whereas on premise you would have to purchase a machine and you'd have to make sure you have additional capacity so that you could grow into it right and so here with uh cloud you do not have to worry about that test systems at production scale so be able to clone your production environment to testing tear down testing while not in use to save money so a lot of people will have a staging server that they run all the time but the great thing here is that with cloud you know it's you can just spin it up and have it right away and then tear it down and save money there's automating to make architectural experimentation easier this is talking about using infrastructure as a code so for aws it should be using cloud formation creating change sets which kind of um uh say exactly what is going to change stack updates drift detection to see if your stuff is being changed over time by developers through manual configuration things like that then we have allow for evolutionary architectures so this is about adapting ci cd um doing nightly releases or if you're using serverless if you adopted lambdas they deprecate over time forcing you to use the latest version and so that is evolutionary architectures then we have drive architectures using data so um when you're using cloud there's a lot of tooling in there to automatically start collecting data so cloudwatch will be collecting some things by default and cloudtrail will as well so you know that is another thing and then improving things through game day so this is about simulating traffic on production or purposely killing ec2 instances or or messing with your services to see how well they recover all right before we jump into each of the pillars let's go open them up and take a look at what structure we should expect to see so we have design principles definition best practices and resources all the pillars follow this to a t so let's just talk about what these are so the design principles are a list of design principles that needs to be considered during implementation and that's where we're going to focus a lot of our energy then you have definition so this is an overview of the best practice categories then you have the best practices themselves these are detailed information about each practice with various aws services and then you have resources these are additional documentation white papers and videos to implement this pillar and i just want to tell you that if you're doing the certified cloud practitioner we're really just going to cover the design principles but for the solutions architect associate or anything uh that's associated or above that's we're gonna actually dive deep into the implementation of the best practices because there is a lot of stuff there so yeah there we go let's take a look here at the design principles for operational excellence so the first here is perform operations as code supply the same engineering discipline you would to application code to your infrastructure so by training your operations as code you can limit human error and enable consistent responses to events generally we're talking about infrastructure infrastructure as a code here so this would probably be like things like cloud formation there's other things you could do like policy as a code and a bunch of other ones then we have make frequent small reversible changes so design your workloads to allow components to be updated regularly uh this could be talking about doing rollbacks incremental changes blue green deployments having a ci cd pipeline refined operations procedures frequently so look for continuous opportunities to improve your operations here you use game days to simulate traffic or event failure on your production workloads anticipate failure so perform post modems on system failures to better improve write test code kill production servers there's a small spelling mistake it should have an r here so servers to test recovery learn from all operational failures so share lessons learned in a knowledge base for operational events and failures across your entire organization but you know if you can just remember these headings here and be able to categorize what would be under operational excellence you'll be okay all right all right let's take a look at the design principles for the security pillar so the first here is implement a strong identity foundation so implement the principle of least privilege or polp that's a very popular concept meaning giving people only the permissions that they need use centralized identity so that would be using database iam avoid long link credentials then we have enable traceability so monitor alerts and audit actions and changes to your environment in real time integrate log and metric collection and automate investigations and remediation then we have apply security at all layers so take defense in depth approach with multiple security controls for everything from as networks vbcs load balancing instances os application code we might have a slide in this course on defense and uh depth where basically you see like a ring of things and you can kind of see how like there's layers that go from outward to inward and that's what they're talking about when they're listing out all these things here automate security best practices protect your data in transit at rest keep people away from your data the reason i don't have descriptions there is because those are pretty selfevident prepare for security events so incident management systems and investigation policies and processes tools to detect investigate and recovery from incidences and uh there are a lot of security tools out there and they all have funny initialisms i didn't put any of them in here but i'm sure there are some there but yeah there you go for security all right let's take a look at design principles for reliability and the first here is automatically recover from failure so monitor kpis and trigger automations when the threshold is breach test recovery procedures so test how your workload fails and you validate your recovery procedures you can use automation to simulate different failures or to recreate scenarios that led to failures before scale horizontally to increase aggregate system availability so replace one large resource with multiple small resources to reduce the impact of a single failure on the over overall workload distribute requests across multiple smaller resources to ensure that they don't share a common point of failure so we're talking about multiaz high availability okay stop guessing capacity we've seen this multiple times so in onpremise it takes a lot of guesswork to determine the elasticity of your workloads uh workload demands with cloud you don't need to guess how much you need because you can request the right size of resources on demand that's going to give you better reliability okay manage change and automation so making changes via infrastructure as a code will allow for a formal process to track and review infrastructure they're going to see iac show up a lot in this framework okay let's take a look at design principles for performance efficiency so the first here is democratize advanced technology so focus on product development rather than procurement provisioning and management of services because if you're on prem you'd have to order those machines set them up and so take advantage of advanced technologies specialize and optimize for your use case with ondemand cloud services because again if you're using onprem uh you you know you might not have the option to have sage maker right it's just going to be a vm and you're going to do all the work yourselves whereas aws has all these specialized things so you can move quickly go global in minutes so deploying your workload in multiple abs regions around the world allows you to provide lower latency and a better experience for your customers at a minimal cost we have used serverless architecture so serverless architecture removes the need for you to run and maintain physical servers for traditional computing activities removes the operational burden of managing physical servers and can lower transactional costs because managed services operate at cloud scale and aws can be a lot better at running them efficiently then you will uh experiment more often so with virtual and automatable uh resources you can quickly carry out comparative testing using different types of instances storage or configurations to make the best choice we call this right sizing choosing the right size consider mechanical sympathy so understand how cloud services are consumed and always use technology approach that aligns best with your workload goals for example consider data access patterns when you select database or storage approaches let's take a look here at design principles for cost optimization so the first one here is implement cloud financial management so dedicate time and resources to build capacity via cloud financial management and cost optimization tooling statements is saying hey take advantage of all our tooling that makes it easy for you to know exactly what you're spending adopt a consumption model so pay only for computing resources that you require an increase or decrease using uh depending on the business requirements we're talking about ondemand pricing measure overall efficiency so measure the business output of the workload and the cost associated associated with delivering use this measure to know the gains you make from increasing output and reducing costs so stop spending money on undifferentiated that's a hard word to say undifferentiated heavy lifting so aws does the heavy lifting of the data center operations like racking stacking and power servers it also removes the operational burden of managing operating systems and applications with managed services this allows you to focus on your customers and business projects rather than your it infrastructure and the last one here is analyze and attribute expenditure so the cloud makes it easier to accurately identify the usage and cost of systems which then allow transparent attribution of it costs to individualize workload owners this helps measure return on investment and gives workload owners an opportunity to optimize the resources and reduce costs so there you go hey this is andrew brown from exam pro and we are taking a look at the aws well architected tool so this is an auditing tool to be used to assess your cloud workloads for alignment with the aws well architected framework and so what it is it's essentially a checklist but it also has nearby references so you know as you're reading through it it will show you information uh and resources so that it can help you with this checklist here and the idea is when you're done you can generate out a report and then you can provide that report to your executives and key stakeholders to prove uh you know how well architected your workload is on aws okay hey this is andrew brown from exam pro and in this video i want to show you two things the well architected framework and the well architected tool so first let's go look for the well architected framework so we're going to look up white papers aws and so if we go here to about amazon.com white papers we have a bunch of pages here and so i'm going to just check box on white paper so that we can kind of reduce the amount there and i'm going to check box well architected framework if we scroll all the way top here one of these you think it'd be right at the top but one of these is the well architected framework and here it is and so if we open it up i used to just directly open up as a pdf i'm sure you can still download it as is but generally you're going to open up as this html page and you can basically read through it see all the stuff see the multiple pillars we can click into here see the design principles read the definitions and then start reading about uh the best practices and they have these things at the bottom of each one uh very boring very very boring but um you know when you get to the solutions architect and things like that you're going to need to know this stuff inside and out it's going to really help you out this cloud practitioner we only need to know surface level information uh but that's a little arctic framework let's take a look at the well architected tool so we're going to type in well here we'll get the well architected tool and if we go here you can see that i've created a couple before probably demos for our videos and so i'm going to go define a new workload i'm going to say my my workload here my workload whoops my workload it is messing up because i probably have grammarly installed so it does not like grammarly so i'm just going to turn it off for now so my workload and it's still not typing correctly so i have to kill a kill of grammarly here which is kind of frustrating so that's a bug that that's not grammarly's fault that's adabus's fault for not playing well with grammarly and that's something i will definitely report to them because it's very annoying so i'm going to go ahead and refresh this page my workload my workload um and this is andrew brown production or preproduction doesn't matter pick your regions us east or usc's 2 sure i'm selecting it there we go uh optional optional optional optional you go to next and then you can choose your lens serverless lens ftr lens so that's the foundational technical review sas lens we can go with architected framework and then once that is there we can start reviewing okay and then we get this big checklist and so we can go through this and read each one so we say ops one how do you determine what your priorities are and all these things like ops and stuff like that these are all the summaries in each of the well architected framework sections so you pretty much don't need to really read the dock and just go through this so everyone needs to understand their part in enabling business success have shared goals in order to set priorities of resources this will maximize the benefit of your efforts so select from the following evaluate the customer's external needs external customer needs evaluate internal customer needs if you click info it's going to highlight each one here so of all key stakeholders including business development operations teams this will ensure etc and so you just go through this and uh you know once you have that and you save and exit okay you'll have the questions that are answered it'll say what's high risk what's not things like that very simplistic it's really just a way of making a very organized report or checklist and proving that you went through it uh to the executive level or to the management level there so hopefully that makes sense to you it's not too complicated but there you go hey it's andrew brown from exam pro and we are looking at the aws architecture center so the architecture center is a web portal that contains best practices and reference architectures for a variety of different workloads and you can find this at adabus.amazon.com for slash architecture so if you're looking for best practices in terms of security they have a huge section on that and they have it for pretty much every kind of category on aws or if you're looking for practical examples you can view the large library of reference architectures so here's one to make an aws q and a bot and it will have an architectural diagram but you can also deploy via cloud formation or possibly cdk and this way you can get a working example and then tweak it for your use case so this is a really great tool um when you are done the awesome architecture framework and you're saying okay how do we apply it can we get more concrete examples and i wouldn't be surprised if a lot of the resources within the wellarchitected framework white paper are just pointing to the center okay hey this is andrew brown from exam pro and we are taking a look at the concept of total cost of ownership also known as tco so what is tco well it is a financial estimate intended to help buyers and owners determine the direct and indirect costs of a product or service so here is an example of you know tco for maybe like a data center so we have hardware monitoring installation i.t personnel training software uh security licensing and taxes but that's not just the limit of it it's just kind of the examples we show here the idea of creating tco is useful when your company's looking to migrate from onprem to cloud and we will have a better kind of visual here to kind of understand how you would contrast against onpremise to cloud but let's just talk about how it actually works in practicality which i think gets kind of overlooked when cloud service providers are selling you on tco so the idea is that gardner um you know they uh they were they wrote this article based on this research where an organization had moved uh 2500 virtual machines over to amazon dc2 and so what you're seeing here is that there is a an additional cost that we're not considering which is the migration cost see this bar up here um so the idea is that the company was paying around 400 000 and so they started to move over and as you see their costs initially went up for a short period of time here uh but then once that migration cost was over uh you can notice that they had a 55 reduction so it's uh totally possible to save money uh and clearly there is great savings uh now is it exactly what aws promises probably not and that's that could be the reason why they update their tco calculator but let's now just do that contrast against the two so we have onpremise on the left and aws on the right or any cloud service provider and what i want to do is help you think about what costs do people generally think about because if we have like iceberg the idea here is that these are the costs that we always think about above the iceberg and then there's these hidden costs that we just don't consider when factoring in our move and that's the idea of tcos to consider all the costs not just the superficial ones and so people say these look like teeth and that's why i add penguins and a whale here um and so when we're talking about onpremise what we generally think are software license fees and subscription fees but when you compare those against each other they might look the same um aws might just look slightly cheaper or even more and so the idea is you need to then factor in everything so on on premise there's implementation configuration training physical security hardware id personnel maintenance and on the aws side you know you are you don't have to do as much of that stuff so you just have implementation configuration and training and so aws with their tco calculator their old one used to make a promise of 75 percent in savings um again you know this is going to really vary based on what your migration strategy looks like um but you know it's totally possible you could save 75 percent or you could save 50 percent over a third year threeyear period and there's a initial spike so that's just something you have to consider but the nice thing though is that once you've moved over all the stuff over here on the lefthand side will be eight of us's responsibility okay all right so let's take a look at capital versus operational expenditure so there's capex and opex so on the catholic side the idea here is you're spending money upfront on physical infrastructure deducting that expenses from your tax bill over time a lot of companies that are running their own data centers uh or have a lot of onpremise stuff understand what capex is because it's something that a lot of times they get tax breaks is on and that's why we see a lot of people that have a hard time moving away from the cloud because you know they keep on thinking about that money they save from the government but capex costs would be things like server costs storage network costs backups and archives disaster recovery costs data center costs technical personnel so the idea is with capital expenses you have to guess up front what you plan to spend okay with operational expenditure the idea here is the cost associated with an onpremise data center that has shifted the cost to the service provider the customer only has to be concerned with nonphysical costs so leasing software and customizing features training employees and cloud services paying for cloud support billing based on cloud metrics so compute usage storage usage and so the idea here is with operational expenses you can try a product or service without investing in equipment so basically apex is what we think about when we think of onpremise and then opex is what we think about you know we're thinking about cloud or aws okay all right let's ask a very important question about cloud migration so does cloud make it personnel redundant so a company is considering migrating their workloads from onpremise to the cloud to take advantage of the savings there is a concern among the staff that there will be mass layoffs does cloud make it personnel redundant and that's a very important question to to have an answer to and this all talks about shifting your i.t team into different responsibilities so a company needs i.t personnel during the migration phase as we saw with that gardner research report that there was a period at least like a year where they needed that for you know depending on the size your company so you're still going to need those people around a company can transition some roles to new cloud roles so a very traditional example would be you have your traditional networking roles or people have like their ccna and now they're moving over to cloud networking they have a reduced workload but there's other things that they could be doing in the cloud a company may decide to take a hybrid approach so they'll always need to have a traditional it team and a cloud it team um and the last one and this one you'd actually see on the exam which is a company can change employees activities from managing infrastructure to rev revenue generating activities okay so the idea is that you know if you're a company why would you get rid of all your staff and you just put them all into rev regeneration i suppose you know you could uh you know uh lay them off and some companies might do that um or you know you could just retrain them because if that it personal team has uh technical expertise i'm sure they can translate that to the cloud let's talk about the database pricing calculator and this is a free cost estimate tool that can be used within your web browser without the need of a database account to estimate the cost of a various items services and this is um available at calculator.aws and the reason we're bringing this up is because there used to be a tco calculator but now this is the calculator that you use so the abs pricing calculator contains 100 plus services that you can configure for cost estimate and so you can just click through a bunch of knobs and boxes to uh you know exactly figure out a very accurate cost so the idea here is that to calculate your tco an organization needs to compare that existing cost against their abuse costs and so the ibis prices calculator can be used to determine uh you know the aws costs and obviously the organization knows its cost so we can compare it against that and the way you can get data out of this is you can export it as a final estimate to a csv okay hey this is andrew brown from exam pro and we are taking a look at the aws pricing calculator so to get there it's calculated.aws what you're going to do is hit create estimate and then here you have a bunch of services so you just choose what you want so you type in ec2 we're going to configure that and from there we can do a quick estimate or an advanced estimate so choose this option for fast and easy route to ballpark an estimate choose this option for detailed estimate for accounts workloads and stuff so notice down below very simplistic we hit advanced and we get all sorts of stuff okay so you know it's really up to you i'm very comfortable with the advanced options so i might be running a linux machine what is my usage it's going to have uh daily spikes of traffic because of the use cases you could say it's not busy on saturday and sunday that it has a baseline of one a peak of two eight things like that then you can choose what you're using um t4 g i don't even know what that is uh but let's just say like t uh t to uh micro which is not that big two three micro and you could say we're doing on demand because a lot of people would be doing that and you see like seven dollars a month it's not a lot of money then you're looking at your storage data in data out okay so we can add that another thing that we might see is something like rds so we go to rds and we add postgres and not all of them have the simple and complex sometimes they're simple so production database we'll have one here and we're just going to be say a db t2 micro t3 micro there we go a hundred that's fine we're not going to have multiaz we'll have single lazy on demand show the calculation 13 a month add that to our estimate so you're kind of getting the idea there right and so you know we have our summary that's our monthly 391 dollars um oh sorry over 12 months so our monthly cost is 32 okay you can go back there clone the service edit it stuff like that you can export the estimate i think it goes out as a csv you can also hit share and then hit agree and so then you have a public link and if i have that link we can just see what happens if i paste it okay and it just brings them to the same estimate so there you go hey this is andrew brown from exam pro and we are taking a look at migration evaluators so it was formerly known as tcl logic and then abus acquired the company and it is an estimate tool used to determine an organization existing onpremise costs so it can compare it against its aws costs for planned cloud migration uh so the idea is that you can get a very very detailed information and the way it collects information is via an agentless collector to collect data from your onpremise infrastructure to extract from your own onpremise costs i don't know if you can see there but you can see that it works with a lot of different kinds of onpremise technology like vmware microsoft tsql all sorts of things okay one migration tool that we can use with aws is the vm import export and this allows us to import virtual machines into ec2 so itamus has import instructions for vmware citrix microsoft hyperv windows vhd from azure and also linux vhd from azure and so the way this works is that you prepare your virtual image for upload and adabus has a bunch of instructions for that once it is ready you're going to upload that to an s3 bucket and once it's uploaded to an s3 bucket then what you can do is use the aw cli to import your image um and so that is the cli command down below and once it is produced it will generate out an amazon machine image and so from an ami you can then go launch your ec2 okay hey this is andrew brown from exam pro and we are taking a look at the database migration service which allows you to quickly and securely migrate one database to another dms can be used to migrate your onpremise database to aws and that's why we're talking about it and so here's a general diagram where you have your source database which connects to a source endpoint goes through a replication instance so that's an ec2 instance that's going to replicate the data to the target endpoint onto the target database and so we have a bunch of possible sources so we have oracle database microsoft sql mysql mario db postgresql mongodb sap at asc imdb db2 azure sql database amazon rds amazon s3 and i'm assuming these are database dumps amazon aurora amazon document db and so for possible targets it's very similar we got oracle database microsoft sql mysql mario db post sql redis sap se amazon redshift amazon rds amazon dynamodb amazon s3 amazon aurora amazon open search service amazon elastic cache for redis amazon document db amazon neptune apache kafka i'm just showing you the list to give you an idea of how flexible this service is uh but you can tell that these are very different databases so how can it uh move them over right and so in not all cases can it easily do it like it's very easy to go from mysql to postgres um but you know for ones that are like relational to uh nosql uh this is where the innova schema conversion tool comes into play it's used in many cases to automatically convert a source database schema to a target database schema or semiautomate it so that you can kind of like you know figure out how to map the new schema each migration path requires a bit of research since not all combinations of sources and targets are possible and it really comes down to even versions of these things so but i just want you to know about that it's an option as a database migration service and i've migrated a very large database before and it's super fast so and it's not that hard to use so something you definitely want to remember when you're migrating hey this is andrew brown from exam pro and we are taking a look at the cloud adoption framework so this is a white paper to help you plan your migration from on premise to aws at the highest level the aws caf organizes guidance into six focus areas we've got business people governance platform security and operations and this white paper is pretty high level uh so you know it doesn't get into granular details on how that migration should work but gives you kind of a holistic approach and i believe that probably through the aws amazon partner network there's people that specialize in using this particular framework to help organizations move over and i believe that anybody has professional services through the apn but let's just kind of break down what these six categories are we're not going to go too deep into this but let's do it so the first is the business perspective so these are business managers finance managers budget owners strategy stakeholders so it's how to update the staff skills and organizational processes to optimize this value as they move ops to the cloud you have people perspective so human resources staffing people managers so how to update the staff skills and organizational processes to optimize and maintain the workforce and ensure competencies are in place at the appropriate time you have governance perspectives so cios program managers project managers enterprise architects business analysts so how to update the staff skills and organizational processes that are necessary to ensure business governance in the cloud and manage and measure cloud investments to evaluate the business outcomes we have platform perspectives so ctos it managers solution architects so how to update the staff skills and organizational processes that are necessary to deliver and optimize cloud solutions and services security perspectives so ciso i.t security managers i.t security analysts so how to update the staff skills and organizational processes that are necessary to ensure that the architecture deployed in in the cloud aligns to the organization's security control requirements resilience and compliance requirements we have operational or operations perspective so i t operations managers i t support managers so how to update the staff skills and organizational processes that are necessary to ensure system health and reliability during the move of operations to the cloud and then to operate operate using agile ongoing cloud computing best practices so this just taps the surface of what the caf is and i think for each of these they actually have a more detailed breakdown so you know business is going to break down to even more uh finite things there okay so aidabus has free services that are free forever unlike the free tier that are up to a point of usage or time um and so there are a lot here this is not even the full list there's definitely more and we have iem amazon vpc auto scaling cloud formation elastic bean stock ops works amplify appsync code star organizations consolidate billing it was cost explorer sagemaker systems manager there's a lot of them okay but the thing is is that these services are free but some of these can spin up other resources so the services are free themselves uh however ones that provision services may cost you money so cloudformation which is an infrastructure as a code tool could launch virtual machines those virtual machines will cost money right opsworks can launch servers that can cost money amplify can launch um lambdas that can cost money so that's something you just have to consider um but yeah there you go hey this is andrew brown from exam pro and we are taking a look at the aws support plans so we got basic developer business and enterprise and you absolutely absolutely need to know this stuff inside and out for exams they will ask you questions on this okay so basic is for email support only uh such as billing and account so if you think it got over billed and that's something you should do if you've uh misconfigured something and you end up with a big bill just go open up a support ticket under basic for billing and they're likely to refund you but if you do have questions about billing accounts that's we're going to be using for everything else that is for tech support um and so for developer business enterprise you're going to get email support which they'll roughly reply within 24 hours i believe this is business hours so if you message them on friday um or sorry saturday you might be waiting till monday for it okay in terms of third party support the only one that doesn't have thirdparty support is developer so if you are using something like ruby on rails or azure or something that has interoperability between aws and something else business enterprise will absolutely help you out with it same with enterprise but the developer one not so much uh if you like to use the phone or you like to chat with people um that's available the business enterprise tier this is the way i end up talking to people if you are um you know like if you're in north america and you're calling between nine to five on a monday friday you're likely to get somebody that is in within north america if not it'll be one of uh one of the supports from some other area so just be aware of that that can also affect the time they pick up uh sometimes it's five minutes sometimes it's 30 minutes to to an hour uh you know it just depends on what service you're asking for and you know what time of day okay in terms of responsiveness uh for general guidance everything is 24 hours or less for developer business enterprise if your system is impaired it's within 12 hours or less with production system impaired it's four hours or less with production system down it's one hour or less and if you're for enterprise um it's going to be business critical system down less than 15 minutes so just notice who has what for these things um i've definitely waited like three days on general guidance before so just take these with a grain of salt that they're not you know they don't really stick to these that or maybe i'm just not paying enough for them to care okay um in terms of uh getting actual people assigned to you this only happens at the enterprise level where they have their concierge team so they uh help your organization uh learn how to use adabask ask them any questions personally and then you have a tam a technical account manager that is somebody that knows um awsi inside and out and they'll help you architect things and make correct choices or they'll check your bill and help you try to reduce that bill things like that okay in terms of trusted advisory checks at the basic developer you get seven advisor checks once you're paying for business you get all the checks the cost here for business is zero um for developer it's starting at 29 a month for businesses starting at 100 a month and then for enterprise it's 15 000 a month so i said starting yet because it's dependent on your usage okay so let's just look at developer business enterprise here because basic's not going to be applicable here so for developers 29 usd a month or three percent of the monthly database usage which whichever is greater on the exam they're only going to ask you like is it 29 100 like generally do you know the tier of expensiveness but they're not going to ask you the percentage of usage okay there's not going to be formulas here when you get into business it's a little bit different where they have it in different brackets so it's going to be 10 for the first uh 10 000 and the next is going to be the next 7 000 stuff like that similar for enterprise as well so let's just do some math so we know that we understand how this works so if you if you had a monthly spend of 500. at the developer tier that's three percent of five hundred is fifteen dollars so they go okay what is greater twenty nine dollars or fifteen dollars so you're paying twenty nine dollars if your spend is a thousand dollars that comes up to thirty dollars uh so you're gonna end up paying thirty dollars because that's greater than 29 okay for business if your monthly spend is a thousand that's ten percent of a thousand that's a hundred dollars if your spend is five thousand then you're going to be paying 500 if your monthly spend is 12 000 then the first 10 percent of a 10 000 is a thousand and then the next is seven percent of two thousand so your total bill is 140 usd we're not going to do a calculation for enterprise because the same for business but hopefully that gives you an idea there okay hey it's andy brown from exam pro and we are taking a look at a technical account manager also known as a tam and these provide both proactive guidance and reactive support to help you succeed with your aws journey so what does atam do and this is straight from a database job posting what they would do is build solutions provide technical guidance and advocate for the customer ensure aws environments remain operationally healthy while reducing costs and complexity develop trusting relationships with customers understanding their business needs and technical challenges using your technical uh acumen and customer obsession you'll drive technical discussions regarding incidents tradeoffs risk management consult with a range of partners from developers through the csuite executives collaborative with adwords solutions architect business developers professional service consultants and sales account managers proactively find opportunities for customers to gain additional value from aws provide detailed reviews of service disruptions metrics detailed prelaunch planning being a part of a wider enterprise support team providing postscale con uh consolidative expertise solve a variety of problems across different customers as they migrate their workloads to the cloud uplift customer capabilities by running workshops brown bag sessions brown bag sessions being a sessions that occur at lunch time something you can learn in 30 minutes an hour and so one thing that's really important to understand is that tams follow the amazon leadership principles especially about customer uh being customer obsessed and we do cover the amazon leadership principle somewhere in this course and tams are only available at the enterprise support tier so hopefully that gives you an idea what a tam does hey this is andrew brown from exam pro in this follow along i'm going to show you um database support and in order to use ada support or to change your level of support you're going to need to be logged into the root account i should say you can use support with im users but if you want to change the support plan you're going to have to be the root user so the top right corner i'm going to support and notice here on the left hand side right now i have a basic plan and so before we look at changing our plan i'm just going to go create a case and we're going to just take a look at some of the options that are open to us so we have account billing support service limit increase technical support notice this is grayed out so we cannot select anything here i can go to here and increase our service limit and this is something that you might have to do pretty soon earlier in your account you might say hey i need more of something like ec2 or a very common thing is ses so for ses you might say hey i need to have this amount of emails for etc okay so um if we go over to account and billing support uh we can go here and ask anything we want so if it's about the free tier i could say ask the general question getting started and saying uh what is free on aws um i want to know what is free on aws and you can attach three attachments there you can choose via web and phone which is really nice um but today i'm just going to do web here and submit that just to kind of show you that as an example and so what that is going to do is open a case and then we will see probably respond in 24 hours to 48 hours just depends on whether it's the weekend or not because it's based on business hours of course so now that we have an understanding of basic let's go take a look at what the other tiers look like so we have basic developer business and enterprise enterprise being extremely expensive developer being affordable and then business being um you know affordable for businesses so i would say developer is okay it gives you um it gives you better support but it's all via email and so you know if you really want good support you're gonna have to pay the business one and that's the one that i use quite a bit so if i change my plan i'm gonna go over to business and this is gonna cost me 93 bucks just to do to show you here today so i'm going to go ahead and click that and so it's now processing it and so what's going to happen is i'm going to have to wait for this basic to switch to business because if i go to the case here it hasn't happened as of yet so notice i cannot select this so i'm going to see you back here it may be like four or five minutes or however long it takes and we'll take a look then okay great so after a few minutes it says my plan is now business and what i can do is go ahead and create a new case and so i can go over to technical support and ask a question so if i was having issues with anything it doesn't matter what i could go over to ec2 linux and then i could choose my category so i could say i'm having an issue with um systems manager and a lot of times they like you to provide the instance id it's going to change based on what service you choose here but you'll get different information i'll just say i need help with logging into my ec2 instance managed by ssm so i could say i created an ec2 instance and i am attempting to access the instance via sessions manager but it is not working i think i have a role issue and then i'm just going to go down here and say this is not a real question i am filming a demo video for a tutorial video on how to use support okay and so once we do that we have the option of web chat and phone so if you use phone you're going to enter your phone number in and they're going to call you back usually you'll be on hold for anywhere for five minutes to an hour it just depends usually it's within 15 minutes so it's very good of course it depends on the time of day and your location things like that and the service because there's different support engineers for different types of services and the the balance of those are different but generally chat is pretty good so i can go here and i'm just going to hit submit and it's going to open a chat box and so you just wait okay and sometimes it's super fast and sometimes it takes uh minutes okay so we are going to just sit here for a bit and um you know i'll just pop back here when there is somebody to talk to okay okay so after waiting a little while looks like uh we've been connected here so it took a bit of time so we're just going to say hello hi umair this is andrew brown i am recording a video to teach people how to use aws and i wanted to show them how it was support works so i'm just showing them how the chat system works say hello and hopefully they'll appreciate or they won't it just doesn't really matter we'll give them a moment there we go that's it thanks for your help okay so that's pretty much it um so you know there's nothing really uh special about that but the idea is when you are typing with them it will appear in the correspondence there so i'm just going to end the chat okay and then i'm just going to mark that case as result sometimes they will ask you to resolve it if i go to cases i probably have some previous ones here um and i have a lot but i don't know why they don't all show up here so you can see this one is pending this one is resolved i go back to this one you can kind of see that the history of a conversation is kept and you can go back and forth with the people there um yeah that's pretty much it you can also do screen sharing so they might send you a request to go on zoom or download this piece of software that shares your screen and so that is another option as well so they can get pretty handson to help you with your problems there but that's pretty much all i wanted to show you with support i'm going to downgrade this and i'm not sure if they're going to give you back my money sometimes they'll prorate it for you but i'm going here and go back to basic um so we will also refine your credit card directly in the month's remaining fees on your old plan which you previously paid you're obligated to pay a minimum of 30 days of support each time you register so i'm not going to get any money back which is totally fine because i just wanted to show you how that works but business support is definitely worth it and uh you know that's it so the aws marketplace is a curated digital catalog with thousands of software listings from independent software vendors uh easily find buy test and deploy software that already runs on abs the product can be free to use or can have an associated charge the charge becomes part of your abs bill and once you pay database market pays the provider the sales channel for isv and consulting partners allow you to sell your solutions to other awes customers products can be offered such as amis it is confirmation templates software the service offerings web acls ableist laugh and rules so it sounds great um if you want to sell here i think you need like a u.s bank account to do it um and you know sometimes database marketplace is just part of aws so like when you're using the ec2 marketplace you are technically using the aws marketplace but they also have like a dedicated page for it so it's integrated with some services and it's also standalone okay hey this is andrew brown from exam pro in this follow along we're going to take a look at the adabus marketplace so what i want you to do is go to the top and type in marketplace and that will bring us over to here the marketplace can be found in a variety of different places on the platform here you can see that uh previously it was using something called guacamole bastion host to launch a server but the idea is that um you can discover products and subscriptions that you might want to utilize so if i go over here there's a variety of different things and so it could be like i want to have something like a firewall that might be something that we might be interested in so we can search there and there's like bring your own license firewall so maybe you have a license with this and you want to run it on an ec2 instance something like that again it's not like super complicated uh what's going on here but a lot of times you know when you're using services you're accessing the marketplace anyway so like when i'm launching an ec2 instance noticeable on the lefthand side is 8bit marketplace and so i don't have to go to the marketplace there i can just kind of like check out the thing i want and that's pretty much all there really is to it okay so you know hopefully that makes sense well let's take a look here at consolidated billing so this is a feature of abuse organizations that allows you to pay for multiple accounts via one bill so the idea here is we have a master account and we have member accounts and i'm pretty sure that we probably call this root account now i don't think account might be a dated term but it's still showing up in the documentation the idea is that if you have member accounts within your organization they're all going to be consolidated under the single account if you have an account outside of your organization you know this is not going to give you this is going to be basically a separate bill as if it's like a standalone organization or what have you okay so for billing aws treats all accounts in an organization as if they were one account you can designate one master or root account that pays the charges for all the other member accounts consolidate billing is offered at no additional cost you can use cost explorer to visualize usage for consolidated billing which we can see i have the icon here you can combine the usage across all accounts in the organization to to share the volume pricing discount which we did cover in this course separately if you want an account to be able to leave the organization you do have to attach it to a new payment method so if let's say you had an account and you want to give it to your friend or whatever they have to hook up their cred their credit card but you can totally have an account leave an organization but you have to deal with that billing aspect okay all right so there's a really cool way to save an aws and that's through volume discounts and it's available for many services the more you use the more you save is the idea behind it um and so consolidating billing lets you take advantage of volume discounts this is a particular feature of database organization so if you do not have the orgs turned on you're not going to be able to take advantage of that okay so one example would be something like data transfer where it is billed for the first 10 terabytes at 17 cents or sorry point 17 cents and then the next 40 terabytes it will be at point 13 cents okay so if we had two accounts um such as odo and dax and they're not with an ableist organization we can calculate those and see what they are unconsolidated and just so you know one terabyte equals 1.024 gigabytes and that's what we're going to see in these calculations so for odo uh you know if he has four terabytes and that is uh we calculate the gigabytes there we times it by uh the um set value there we're going to get 696 dollars okay for dax we're going to end up with uh about 13.92 there and so if we were to add those up the bill would come out to 2088 okay so the idea is that there's an organization and they like your company and they created two accounts but they're just not within an organization by having them in the organization you're gonna save um about almost eighty dollars there so um that is a reason why you'd want to use volume discounts okay hey this is andrew brown from example and we're taking a look at abyss trusted advisor so trusted advisor is a recommendation tool which automatically and actively monitors your aws accounts to provide actual recommendations across a series of categories so this is what it looks like i personally prefer the older dashboard but this is what they have now and you can see along the side we have a bunch of categories and then we have some checks here saying uh you know what are we meeting what are we not and you can go in and read each one and they'll tell you so much information they'll even show you like what things are not meeting that requirements in some case you can easily remediate by pressing a button not in all cases but the thing with the ambush trust advisor is think of its trusted advisor like an automated checklist of best practices on aws and they kind of map to the pillars of the wellarchitected framework not exactly but pretty close but there are five categories of aws trusted advisor so we have cost up to imagine station how much money can we save performance so how can we improve performance security how can we improve security fault tolerance how we can we prevent a disaster or data loss and service limits so are we going to hit the maximum limit for a service and so the next thing we need to discuss is um there is a variation of the amount of checks that are available to you based on your support plan so you know if you're using basic or developer you have seven trusted advisor checks and if you have business enterprise you have all the trusted advisor checks so if we're talking about just the ones that are available to you the ones that come for free is mfa on root account security groups specified ports of unrestricted amazon s3 bucket permissions amazon ebs public snapshots amazon rds public snapshots imu so this is just about alerting you about discouraging the use of the root account service limits so all service limits checks are free um it's weird because they call it the like seven security checks but if you counted all the service limits it'd obviously be too large of a number but notice that one through six are all security checks so you're not getting anything from the other tiers just the security tier and what i want to do is just go over a bunch of available checks out there it's probably not the full list because i couldn't even be bothered to update it if they've added more but it'll give you a general idea of what you could expect under each category so for cost optimization it could be things like looking at idle load bouncers so you know if you have load balancers you're not using you're paying for them so get rid of them unassociated elastic ip addresses so for every ip that's not associated you're paying for as well maybe under performance you have high utilization of amazon ec2 instances so maybe you can save money by switching to smaller instances under security we saw mfa on root account very popular one making sure you turn on key rotation could be something as well there under fault tolerance it could be making sure that you're using backups on your amazon rdes database maybe that's turned off uh for service limits there's just a ton of them and so uh one that that you know might be pertinent to use vpcs or ec2 limits so there you go hey this is andrew brown from exam pro and we're going to take a look at trusted advisors so what i want you to do is go to the top and type in trusted advisor and once you're there you're going to notice on the left hand side we have cost optimization performance security fault tolerance and service limits right now there are no recommended actions because there's not much going on this account and when you uh have the uh free level of support the basic support you're not going to have all these checks but if we go in here we can still see kind of what they do so we have like performance security things like that so these are the ones that we actually can see and they generally work all the same way if you expand here it's going to say amazon ebs public snapshot so check the permission settings for the ebs volume snapshots and alert you if the any snapshots are marked as public and so if you scroll on down if there were ones that were an issue it would tell you right here okay then down below here we see like check buckets in amazon s3 that have open access permissions or allow access to authenticated database users so yellow the acl allows list access for everyone a bucket policy allows for any kind of open access bucket police statements have public grant access so maybe what we can do is to see if we can get this to trigger and so what i'm going to do here is go over to s3 and what we're going to do is make a bucket that has a full axis okay so i'm going to create a new bucket and we'll say my exposed bucket we'll scroll on down here and we'll just check box that off and create the bucket let's say i acknowledge that is totally fine okay so now i have a bucket that is 100 exposed if we go back to trusted advisor give this a refresh i'm not sure how fast it will show up here but if i expand so it says the bucket acl allows upload delete for everyone the trusted advisor does not have permissions to check the policy uh bucket policy statements that grant public access so what we could try to do is make a policy and try to grant all access here so i'm not writing these every single day but i'm sure we could try to figure this out um we'll say s3 bucket policy public access public read and so that one might be a good example so i'm going to go ahead and copy this one granting read only permission to anonymous users i don't recommend you doing this i'm just doing this to show you to see if we can get the trusted advisor to check because i don't want you to do this and forget about it and then have a serious issue but the principle is set to anybody so anyone can read it here it's saying get object etc then it's saying what particular resource so this one is going to be for the bucket in question here which is my exposed bucket we're going to scroll on down save the changes okay so this bucket is publicly accessible we're going to go back over here refresh and see what we can see okay so checks buckets in s3 etc so it should appear under here and it could be that it's just going to take some time so what i'm going to do is i'm just going to hang tight for a little bit oh there we go okay so it's showing up and i guess it just took some time to populate and so here we can see we have a yellow symbol it's a warning saying hey there's a problem here if we go back to the dashboard i wonder if that shows up so this one's for investigation and recommendation so you know hopefully that kind of makes sense to you i think in some cases you can do remediation from from here or at least you can go and check box and say okay um excuse me ignore gonna swore there's remediation for some of these but in any case you know that's generally what trusted advisor does i think that you probably can have it so it gives you alerts so yeah you could set recipients for particular things like if there's a security issue that i could email a particular person on your team and they could deal with it but that's pretty much it so what i'm going to do is go ahead and delete this bucket i'm all done with it we'll go delete and say my delete my exposed bucket here to delete it and that is it okay let's cover the concepts of service level agreements also known as slas so an sla is a formal commitment about the expected level of service between a customer provider when a service level is not met and if customer meets its obligation under the sla customer will be eligible to receive compensation so financial or service credits and so when we talk about slas then we talk about sli so at sli service level indicator is a metric or measurement that indicates what measure performance a customer is receiving at a given time a sli metric could be uptime performance availability throughput latency error rate durability correctness and if we're talking about sli's then we're talking about slos service level objectives so the objective that that the provider has agreed to meet as wells are represented as a specific target percentage over a period of time and so an example of a target percentage would be something that says availability sla of 99.99 in a period of three months all right and let's just talk about target percentages in the way they can be represented very common ones we will see is 99.95 percent 99.99 uh then we have 99 followed by nine nines and so commonly we just say we call this nine nines okay and then there's one nine elevens so if somebody says we have an sla guaranteeing of of 911s it's going to be the 99 followed by 0.911s all right let's take a look at abus service level agreements and so there are a lot of them and i just wanted to show you a few services to give you an idea how they work on the exam they're not going to ask you like oh what's dynamodb's sla for global tables but generally we should just go through this because it's good practice so let's take a look at dynamodb sla so abyss will use commercially reasonable efforts to make dynamodb available with a monthly uptime percentage of each aws region during any monthly billing cycle so for a at least 99.999 percent if global tables sla supplies or 99.99 if the standard sla applies in the event dynamodb does not meet the service commitment you'll be eligible to receive service credits described below so we have monthly uptime percentage and the service credit percentage we get global tables standard tables so let's take a look here so if less than 99.999 but equal to or greater than 99.0 percent is met so if if the service ends up being this you'll get 10 back of what you spent as service credits if it drops between 99.0 and 95.0 you get 25 percent back if it's less than 95 percent um then it's a hundred percent back okay and you get the general idea here sla is going to be slightly different with their drops now let's take a look at um a compute so compute is going to apply across a bunch of compute services probably because they're all using ec2 underneath so that's probably the reason for it so we have ec2 ebs ecs eks and abus makes two sla commitments for the included services so we have a region level sla that governs included services deployed across multiple azs or regions and an instance level sla that governs amazon ec2 instances individually and again we have our monthly up time percentage our service credit percentage region and instance level so you can just see the same thing it's like it's going to change based on uh what it can meet then we'll take a look at one more like rds so relational database uh service so abs will use commercially reasonable efforts to make multiaz instances available with monthly uptime percentage of 99.95 during any monthly billing cycle and again you know if if they don't meet those requirements you're gonna get service credits back which basically equal usc dollars on the platform and so for this it looks like that so just notice that you know with comp like compute it was for a a bunch of services for dynamodb it was based on uh particular features like global standard tables sla it's very straightforward uh we didn't do s3 because i just did not want to show you that one it's just too complicated but my point is is that it's going to vary so you have to look up per service okay hey this is andrew brown from exam pro and we're taking a look at amazon's service level agreements and so the way you find slas is you're pretty much just typing sla for whatever it is so if you're looking for compute you type in sla or you look for a particular service so maybe you say sage maker aws i don't think there's like a generic sla page at least i don't know where it is i always just type in sla to find what it is and through that you can just kind of read through and try to find out uh the things that that matter to you for your business okay let's take a look here at the service health dashboard and so the service health dashboard shows general status of aws services it's really simple the idea is that you can check based on the geographic area so you'd say north america europe etc and what you'll see is an icon that says whether the service is in in good standing and the details whether the service is operating normally etc notice they also have an rss feed the reason i'm talking about service health dashboards is because i want to talk about personal health dashboards and because they're both called health dashboards it's confusing so i wanted to tell you about this one first so now we'll jump into the aws personal health dashboard so we saw the service health dashboard now let's take a look at the adabus personal health dashboard so this is what it looks like and it provides alerts and guidance for it events that might affect your environment all airbus customers can access the personal health dashboard the personal health dashboard shows recent events to help you manage active events and show proactive notifications so that you can plan for scheduled activities you you can use these alerts to get notified about changes that can affect your invoice resources and then follow the guidance to diagnose and resolve the issue so this is very similar to the service health dashboard but it's personalized for you um and it's you know i i don't see it crop up very often but if you had to create alerts or be reactive to uh things that are happening within your bus this is where you do it okay so there's a team called aws trust and safety that specifically deals with abuses occurring on the abyss platform and so i'm going to just list of all the cases where you'd want to be contacting them as opposed to support so the first is spam so you're receiving unwanted emails from an abus owned ip address or abus resources are used to spam websites or forms port scanning your logs show that one or more aws owned ip addresses are sending packets to multiple ports on your server you also believe this is an attempt to discover unsecured ports uh dos attacks so your logs show that one or more italy's owned ip addresses are used to flood ports on your resources with packets you also believe this is an attempt to overwhelm or crash your server or the software running on your server intrusion attempts so your logs show that one or more adidas owned ip addresses are used to attempt to log into your resources hosting prohibited content so you have evidence that abyss resources are used to host or distribute prohibited content such as illegal content or copyrighted content without the consent of the copyright holder distributing malware so you have evidence that abus resources are used to distribute software that was knowingly created to compromise or cause harm to computers machines that it's installed on and so in any of these cases you're not going to it with support you're going to open up an abuse ticket and so you got to contact abuse at amazonatabus.com or fill out the amazon abuse form so and this is whether it's coming from an outside ableist account or even your internally if you think that somehow someone has a compromise your account and it's being used any of these ways this is what you're going to do okay hey this is andrew brown from exam pro and we're looking at awsw so uh we were saying that database has the itabus trust and safety team and what you'll want to do is if you find that there's an issue you're going to report it to this email at abuse at amazon.com or you're going to use this form which is the report amazon it was abuse so you'll go down here you'll sign in you'll put your email in your first name last name org phone number um the source ip the the details uh in here you can even select the type of abuse so you say if it's this kind or that kind things like that it's very straightforward and that's pretty much it okay hey this is andrew brown from exam pro and we are taking a look at the aws free tier and this allows you to use database at no cost um and when we say free tier there there there's the idea of the first 12 months of sign up there's going to be special offerings or it's free usage up to a certain monthly limit forever and then there's just services that are inherently free which we have a total separate slide on but let's talk about just the free tier stuff and this is absolutely not the full list but it's a good idea like it gives you a good overview of stuff that is free so for ec2 which you use for a web server you get a t2 micro for 750 hours per month for one year and so there's about 730 hours um in a month and so that means you could have a server running the entire month for free and an additional server for a bit as well so for rds which is a relational database service for either mysql or postgres we can do a t2db micro for 750 hours for free so there we get our free database and you would be surprised how far you can get with a uh a t2 db micro um you know even for a medium sized startup you can run it on a t2 db micro with no problems then you have your elastic load balancer you get 75 hours per month for one year um so that is a really good thing uh load bouncers usually cost 50 a month so that's great actually all these pretty much cost 15 a month so that's about um 15 30 45 month over month for a year that's uh free then you have amazon cloudfront this is where you'd have your home page caching your videos things like that so you get 50 gigabytes data transfer out for the total year then there's amazon connect you get your tollfree number there 90 minutes of a call time per month for one month or for one year sorry amazon elastic cash so you could launch a redis or um elastic cash server you get 70 hours on a cash d3 micro for a year um elastic search service so it's full text search so again 70 50 hours per month for one year pinpoint campaign marketing email so you can send out 5 000 targeted users per month for one year scs so simple email uh service so this is for um transactional emails um so that you send out from your web app so 62 000 emails per month forever airbus code pipeline so one pipeline free it was code build so this is for building out projects or things like that so 100 build minutes per month forever it was lambda service compute 1 million free requests per month 3.2 million million seconds of compute time per month for free and you know i like to highlight these ones because for traditional architecture you're always going to have a web server a database a load balancer um and you might even have cloudfront in there as well but uh yeah again there's a huge list and this does not even tap the service of what's free on aws hey this is andrew brown from exam pro and we are taking a look at abyss promotional credits and these are the equivalent to usd dollars on the abyss platform abs credits can be earned several ways this could be joining the database activate startup program winning a hackathon participating surveys and any other reason that database wants to give credits out once you have a promotional code you click the redeem credit button in the billing console you enter it in and then your credits will be shown there you can monitor them via it was budgets or via cost explorer and probably even billing alarms it was credits generally have an expired day attached to them could be a few months to a year immense credits can be used for most services but there are exceptions where it is credits cannot be used like purchasing a domain via roe 53 because uh that domain costs money outside of aws's cost like for their infrastructure and virtual stuff and so for things like that uh you know they're not gonna be you're not gonna be able to use credits for that okay the adams partner network also known as apn is a global partner program for aws so joining the apn will open your organization up to business opportunities and allow exclusive training and marketing events so when joining the apn you can either be a consulting partner so you help companies utilize database or a technology partner you build technology on top of abs as a service offering and a partner belongs to a specific tier so it's either going to be select advanced or premiere when you sign up it's free to sign up but you're not going to be able to do much until you start uh committing to an annual fee so that's it's like a certain amount of money to uh be able to be part of that tier and it starts in the thousands okay so i think the first tier is like something like a thousand or two thousand dollars and it gets uh more expensive as you go up as a tier and you also have to have particular knowledge requirements so this could be holding uh particular edible certifications at this at the foundational level at the associate level things like that um or it could be uh aws apn exclusive certification so training that um it's not in certifications but there's certifications that are only available to partners saying like how do you it could be like something like how do you uh talk to customers or communication things like that you can get back promotional database credits so you know if you say oh man i spent uh two thousand dollars on just being able to get into the apn at least the idea is that you can generally get back that uh that spend on aws so it's like you committing if you give like two thousand dollars like you're going to commit to keep using aws i'm not showing the annual fee commitments here and the promotional credits that you get back just because they've changed it a couple times on me and i just don't want this slide to go stale in case they happen to change it again so you'll have to look that up to find out what they actually are right now uh you can have unique speak speaking opportunities in the official awesome marketing channels like the blogs or webinars being part of the apn is a requirement to be a sponsor with a vendor booth enables events so when you s when you go to re invent or any aws event all the vendors are part of the apn all right so they've paid their fee and now they paid an additional fee to get their booth but um yeah the bus partner network is very good for helping you find new business and connecting with other people that are building workloads in aws but hopefully that gives you an idea of how that works okay hey this is andrew brown from exam pro and we are taking a look at ibis budgets so abs budgets gives you the ability to set up alerts if you exceed or approaching your defined budget create cost usage or reservation budgets it can be tracked at the monthly quarterly or yearly levels with customizable start and end dates alert support ec2 rds redshift elasticast reservations uh and so the idea here is you can choose your budget amount so it could be like a hundred dollars it'll even show you what was the last amount if you're resetting the budget it's something new you can choose based on a different kind of unit so if you wanted to be based on running hours on ec2 you could totally do that is budgets can be used to forecast costs but is limited compared to cost explorer or doing your own analysis whether it was costs and uses reports along with business intelligence tools budgets uh based on a fixed cost or or you can plan your cost up front based on your chosen level can be easily managed from the aws budgets dashboard via the aws budgets api get notified by providing email or chat bot and threshold uh how close to the current or forecasted budget um so you'd see a list of budgets here uh current versus forecasted the amount used things like that you can see your budget history you can download a csv uh it'll show you the cost history right in line there which i can't show you it's hard to see there you get the first two budgets are free so there's no reason not to set a budget when you first get into aws and each budget costs about 0.02 cents a day so it's like 60 cents um usd per month for a budget so they're very cheap to use and you've got a limit of 20 000 budgets they're going to be in good shape okay well let's take a look here at airbus budget reports which is used alongside abs budgets to create and send daily weekly or monthly reports to monitor the performance of your abus budgets that will be emailed to specific emails so it's not too complicated here you say create the report budget choose your frequency the emails you want um an administrative report serves as a more convenient way of staying on top of reports since they're delivered to your email instead of logging into the management console so it's just for those people that just can't be bothered to log in okay well let's take a look here at abyss costs and uses reports so generate a detailed spreadsheet enabling you to better analyze and understand your abs cost so this is kind of what it looks like and when you turn this feature on it will place it into an s3 bucket you could use something like athena to turn the report into a queryable database since it's very easy to consume s3 csvs into athena you could use quicksite to visualize your billing data as graphs so quicksite is a business intelligence tool similar to tableau or power bi you could also ingest this into redshift but the idea here is when you turn it on you can choose how granular you want the data to be hourly daily or monthly if you turn on daily you'll be able to even say spikes of uh of of of costs for ec2 instances which is kind of nice the report will contain cost allocation tags um which i think we have a separate slide on that type of tags and the data is stored in either as either a csv it will be zipped or it will be a parcat format it just depends on how you want it for that okay let's talk about cost allocation tags so these are optional metadata that can be attached to aws resources so when you generate a cost and uses report you can use that data to better analyze your data so what you'd have to do is make your way over to cost allocation tags and need to activate the tags you want to show up there are two types of tags so we have user defined so whatever you've previously tagged will show up probably there you turn it on so if you made one with project you turn on project and there's a lot of aws generated ones that you can turn on so there's a huge list there but uh yeah that's particular with cost usage and reports if it says like cost allocation reports it's just that's what costs and usage reports used to be called and some of the documentation is a bit old there but yeah there you go so you can create your own alarms in cloudwatch alarms to monitor spend and they're commonly called building alarms and so it's just a regular alarm but it's just focused on spend but in order to do this you have to turn on building alerts first in order to be able to use it and then you'll go to cloudwatch alarms and you can choose billing as your metric and then you just set your alarm however you'd want build alarms are much more flexible than aba's budgets and are ideal for more complex use cases for monitoring spend and usage in terms of alerting so you just have to decide what you want to do uh before it was budgets this was the only way to do it and so this is the way i'm used to doing it and i still do it this way today but you know both options are valid and just have to decide what is your use case okay let's take a look at about cost explorer which lets you visualize understand and manage your aws costs and usage over time so here's a big graphic of aws cost explorer and you can specify time and range and aggregation it has a lot of robust filtering what's really nice is that they have a bunch of default reports for you so i'm just gonna get my pen tool just to show you where that button is it's over uh here uh if you can see my marker there but but you know you can look at things like monthly cost by service monthly cost by linked account daily cost savings marketplace r utilization so there's a bunch there you could also notice that you can create your own report so if you do find something that you like you can save it for later um you can you could have access to forecasting here so you get an idea of the future costs and whether it's been it's gone up or down just to kind of zoom in on some of those filtration options you can choose um either monthly or daily level of of how you want the data to be grouped together and you have a lot of filter control so if i want to just have ec2 instances for a particular region then i can get that filtered information over here and you can see you have a breakdown of the different types so it's very detailed and cost explorer shows up in us east one i'm pretty sure if you click on class explorer we'll just switch you over to that region but just understand that's where it lives okay hey this is andrew brown from exam pro and in this video i want to show you aws cost explorer so what we'll do is go to the top here and actually on the right hand side we're going to click on the right and go to my billing dashboard and from there on the left hand side we're going to look for cost explorer and then click launch cost explorer and this is where we're going to get to the aws cost management dashboard where this is where we find savings plans reservations things like that on the left hand side click on cost explorer and you can get this nice chart and so the idea is you can change it from monthly to daily if you if you uh prefer okay you can change the scope here maybe we don't need six months we can just go back three months here so there's less data it is a bit delayed when i'm clicking here so it also could be just because i'm doing the daily instead of monthly so you just have to be a little bit patient when uh using this interface you can change it to stack line graph you can kind of see the details there it's not always clear like what others is or things like that and so uh you can drill down and there's like ways of applying filters and things like that i always forget how to do this because it's bringing everything in so you have to hit clear all first i think and oh you have to click into it so like if you wanted to click into it and pick a particular service we could go here and type in ec2 and say ec2 instances and then apply that filter so now we can just see exactly that cost or if we want to choose like maybe just rds okay so you know that could be useful for you to see but yeah sometimes it's not always clear and so what i recommend is just go back to your billing dashboard and from there just go to bills okay bills is really really useful because here it shows you exactly every single little service that you're being billed for you can expand it and see exactly where if there you have other accounts you can go into this side here as well and find spend that way but cost explorer is very useful just it's useful in a different way okay so there you go hey this is andrew brown from exam pro and we are taking a look at the database pricing api so with adabs you can programmatically access pricing information to get the latest pricing offerings for services this makes sense because database can change them at any time and so you know you might want to know exactly what the current price is there are two versions of this api so we have the query api known as the pricing service api and you access this via json and then there's the batch api also known as the price list api via html what's odd is that the batch api returns json but you're accessing it via html so you can literally paste those links in your browser for the query api you're actually sending an an application json request so you'd have to use something like postman or something uh you can also subscribe to sns uh notifications to get alerts when pricing for the services change database prices change periodically such as when aws cuts prices when new instance types are launched or when new services are introduced so there you go hey this is andrew brown from exam pro and what i want to do here is show you savings plans and savings plan is going to be found under the it was cost explorer so just type in cost explorer at the top here or if you want you can type in savings plan as well and once we are here on the left hand side we are going to have a savings plans option so we're going to go to the overview and here it just describes what our savings plans if you want to read through it but down below if you have already some spend happening it's going to make some suggestions and in this particular account it's saying that i could save some money on compute before we take a look here i'm just going to go to the form here and see what we can see so up here we can say commitment two three years by the way you have compute savings which applies to ec2 fargate or lambda then you have the ec2 specific one where uh we can select a very particular type of instance family and then there's the sagemaker savings plans um but if we go here and we just enter in like two dollars all up front i don't really understand it from here because it doesn't make it clear what the savings are um but uh what it does make it very easy is probably if we go over here and then click down on the compute so kind of feel like here would auto fill it in for you and so here i filled it in or sorry it's filled in for me and so here it's saying with a oneyear plan all up front for based on the past 30 days that it's going to see that i'm going to see a monthly savings of 25 and 36 cents and then i can add it to the cart that way and i kind of feel like that is the easiest way to um figure that out where with um with how it was going that form i just couldn't figure it out myself what the savings were there are some utilization reports and coverage reports honestly i've never really looked at these before um but uh i'm just curious like what we're looking at monthly daily the last let's go a few months here i've been running stuff in this account for a while so there should be something apply so nothing nothing of interest but um i mean i guess you have a lot of use and coverage report utilization report could be interesting but i imagine it's maybe you have to be using you have to have a savings plan before you can see this so that's probably the reason why um but yeah hopefully that gives you a clear idea that you know you can just go down to those recommendations and and see exactly what you can save and you just add it to your cart and then once you want to pay for it you just choose to submit that order and you're all good to go all right so that's savings plans let's take a look here at defense in depth to understand the layers of security aws has to consider uh for their data centers for their uh virtual workloads and things that you also have to consider when you are uh thinking about security for your cloud resources so in the most interior we have data so this is access to business and customer data and encryption to protect your data then we have applications so applications are secure and free of security vulnerabilities then you have compute so access to virtual machines ports on premise and cloud you have the network layers so this limits communication between resources using segmentation and access controls you have the perimeter itself so distributed denial of service protection to filter largescale attacks before they can cause denial of service of users you could say that's part of the network layer and that's when i say there are variants on this but we're just separating it out explicitly there we have identity and access so controlling access to infrastructure and change control and then there's the physical layer so limiting access to data centers to only authorize personnel you'll notice i highlighted identity and access in yellow it's because that is considered the new primary um perimeter from the customer's perspective of course ida best has concern about the physical perimeter and things like that but as a customer that's what you're going to be thinking about especially with the zero trust model and when you see these depths the idea is that in order to get here you have to pass through all the stuff so if this um if this outward one is protected pretty well then you generally don't have to worry about the interiors but of course you should um but yeah there you go let's take a look here at confidentiality integrity and availability also known as the cia triad is a model describing the foundation to security principles and their tradeoff relationships so here is our triad so we have confidentiality so confidentiality is a component of privacy that implements to protect our data from unauthorized viewers and practice this can be using cryptographic keys to encrypt our data and using keys to encrypt our keys so envelope encryption then we have integrity so maintaining and ensuring the accuracy and completeness of data over its entire lifecycle and practice utilizing acidcompliant databases for valid transactions utilizing tamper evident or tamper proof hardware security modules hsms availability so information needs to be available when needed in practice so high availability mitigating ddos decryption access so the cia triad was first mentioned in this publication in 1977 there have been efforts to expand and modernize or suggest alternatives the cia triad so one was in 1998 for the six atomic elements of information uh or in 2004 we have the engineering principles for uh for information technology security so it has 33 security principles but this is still a very popular um model for security uh and it's just to kind of tell you like you know you don't always get everything you don't get all three of them sometimes you have to trade off in your scenario um you know and hopefully some of the terminology here will resonate as we go through more security content what i want to do here is just to find the term vulnerability so a vulnerability is a whole or weakness in an application which can be designed a design flaw or implementation bug that allows an attacker to cause harm to stakeholders or applications and uh there's a lot of great definitions of vulnerabilities but owasp has a ton of them and we talked about oats when we talk about abuse waff but it's an organization that creates security projects that help you know what you should protect uh or gives you a working example so that you can understand how to get better at security and so they have a lot of ones here but maybe you might notice some here like using a broken or risky cryptographic algorithm maybe there's a memory leak least privileged violation so that's um uh least privilege is something that is a thing that you're always worried about in security improper data validation buffer overflows so you know just to kind of set the tone of what a vulnerability is and things you should be thinking about okay let's understand what encryption is but before we do we need to understand what is cryptography so this is the practice and study of techniques for secure communication in the presence of third parties called adversaries and encryption is the process of encoding or scrambling information using a key and a cipher to store sensitive data in an unintelligible format as a means of protection an encryption takes in plain text and produces produces a cipher text so here's an example of a very old encryption machine this is the enigma machine used during world war ii and it has a different key for each day that it was used to set the position of the rotors and it relied on simple cipher substitution and so you might be asking what is a cipher and that's what we're going to look at next so what is a cipher it is an algorithm that performs encryption or decryption so cipher is synonymous with code and the idea is that you use the code to either unlock or or lock up the information that you have so what is a ciphertext a ciphertext is the result of encryption performed on plain text via an algorithm so you lock that up you scramble it it doesn't make sense and you need that code to unlock it to get the information so a good practical example back in the day was a code book and this was the type of document used for gathering and storing cryptographic codes or ciphers so the idea is if we zoomed up on here notice where we have cannot so and it would be zero zero and then there would be give them authority so the idea is zero zero or if you had the word cannot it would translate to zero zero and then you use zero zero to match that up to say what does that actually mean and so that is kind of a very practical example of ciphers in action so we just took a look at encryption but what are cryptographic keys so a a cryptographic key an easy way to think of it is a variable used in conjunction with an encryption algorithm in order to encrypt or decrypt data and there are different kinds of um ones we have so we have symmetric encryption so this is where we have the same key that is used for encoding and decoding and a very popular one and the one you'll see on aws is called advanced encryption standard aes so just take a look at that graphic very closely so we have one key and it is used to encrypt so it produces the cipher and then or cipher text we should say and then it will decrypt and we will get our plain text so one single key then we have asymmetric encryption so two keys are used one in code and one to decode and a very popular one here is rsa if you're wondering what those those words are it's three people's names put together who helped invent this type of algorithm and so here we have one key for encrypt and one key for decrypt and there are two different keys all right all right let's look at the concept of hashing and salting so for hashing we have a hashing function and this accepts arbitrary size values and maps it to a fixed size data structure hashing can reduce the size of a store value and hashing is a oneway process and is deterministic so a deterministic function always returns the same output output for the same input so if we have something like john smith and we pass it to the hash function it's going to create something that is not human readable but it'll say something like zero two f a e x x y whatever um and it will always produce the same thing if the same key or you know value is being inputted there so the reason we use hashing functions or hash in general is to hash passwords so hash functions are used to store passwords in a database so that the password does not reside in a plain text format so you've heard about all these data breaches where they've stored the password in plain text this is the thing that helps us avoid that issue and the thing again is because it's one way you can't take that hash and unhash it well there are some conditions to it but so to authenticate a user when a user inputs their password it is then hashed so the one that was inputted at the time of you know login and then that hash is compared to the stored hash in the database and if they match the user is successfully logged in so in that case we never ever had to know what the original password looked like uh popular hashing functions are md5 sha256 or bcrypt if an attacker knows the function you are using and and stole your database they could enumerate a dictionary of passwords to determine the password so they'll never see it but they could just keep on going through that so that's why we salt our passwords so a salt is a random string not known to the attacker that the hash function accepts to mitigate the deterministic nature of a hashing function so there you go let's take a look here at digital signatures and signing so what is a digital signature is a mathematical scheme for verifying the authenticity of digital messages or documents and a digital signature gives us tamper evidence so did someone mess or modify the data is this data from someone we did not expect it to be is it from the actual sender and so we kind of have this diagram where we have a person who sends or is going to send a message so they sign it and then uh bob verifies that it was for the person who it's from so there are three algorithms to a digital signature the key generation so generates a public and private key then there is signing the process of generating a digital signature with a private key and the inputted value so signing which is what is happening up here signing verification verifies the authenticity of the message with a public key so remember the private key is used for signing and the public key is used for verifying ssh uses a public and private key to authorize remote access into a remote machine such as a virtual machine it is common to use rsa and we saw that rsa is a type of algorithm earlier and so ssh hyphen keygen is a wellknown command to generate a public and private key on linux i know this one off the top of my head i always know to do this and so what is code signing so when you use a digital signature to ensure computer code has not been tampered and so that's just a like subset of digital signaturing so you can use this as a means to get into a virtual machine or you can use signing as a means to make sure that the code being committed to your repository is who you expect it to be from so there you go let's talk about in transit versus at rest encryption so encryption transit this is data that is secure when moving between locations and the algorithms here are tls and ssl then you have encryption at rest so this is data that is secure when residing on storage or within a database so we're looking at aes or rsa which we both covered previously these algorithms so ones that we did not cover was tls and ssl so we'll cover them now so tls transport layer security is an encryption protocol for data integrity between two or more communic communicating computer applications so 1.0 and 1.1 are no longer used but tls 1.2 and 1.3 is the current best practice then we have ssl secure socket layers so an encrypted protocol for date integrity between two or more communicating uh computer applications so 1.0 2.0 and 3.0 are deprecated um and honestly i always get these two mixed up and i always figure uh get confused which is being used but um you know they're always changing on us but just understand generally what these concepts are and that you're familiar with the terms okay hey this is andrew brown from exam pro and we are taking a look at common compliance programs so these are a set of internal policies and procedures for a company to comply with laws rules and regulations or to uphold business reputation so here we have a bunch of different compliance programs and so some popular ones are like hipaa or pci dss the question is should you know these yes you should generally know the most popular ones because you're going to see them throughout your cloud career and so just getting familiar now is a good time so let's jump into it okay so the first one i want to introduce you to is for ia iso and they have a bunch of different ones so iso is the international organization of standardization and their other one called iec which is the international electro technical commission one deals with uh you know like uh virtual things the other one deals with hardware things but they have a lot of overlapping compliance programs okay and so the most popular absolutely most popular one that i know of is the 270100 i know a lot of organizations that are going for their 2701 so this is for control implementation guidance you have the 27017 this is enhanced focus on cloud security the 27018 this is protection of personal data in the cloud then you have the 27701 this is privacy information management system so pims framework this outlines controls and processes to manage data privacy and protect pii so that's personally identifiable information then you have system and organization control sock and this is a very popular thing that organizations go for especially the sock too so sock one is 18 standards and report on the effectiveness of internal controls at the service organization relevant to the client's internal control over financial reporting we have stock 2 evaluates internal controls policies and procedures that directly relate to the security of the system at a service organization and stock 3 a report based on the trust service services criteria that can be freely distributed then we have pci dss a set of security standards designed to ensure that all companies that accept process store and transmit credit card information maintains in a secure environment we have a federal information procedure standards or fips so 140 hyphen 2. this is u.s and canadian government standard that specifies the security requirements for cryptographic modules that protect sensitive information then we have a ph ipa this is more relevant to me because i'm actually in ontario and canada but it's also very well known one out there outside of hipaa so this regulates patient protected health information then you actually have hipaa this is the u.s federal law that regulates patient procedure health information then we have a cloud security alliance so csa star certification independent thirdparty assessment of a cloud provider's security posture if you never heard of csa they have a very uh wellknown fundamental uh security certification called the cssk or ccsk i always get that mixed up then we have fedramp which we covered earlier in this course or in the future depending on where we put it but fedramp stands for federal risk and authorization management program it's a us government standardization approach to security authorizations for cloud service offerings if you want to work with the u.s government or places that sell the us government you need fed ramp that similar to criminal justice information services any u.s state or local agency that wants to access the fbi's cgis database is required to adhere to the cgis security policy then we have gdpr the general data protection regulation everyone knows what this is in europe maybe not so much in north america or other places a european privacy law imposes new rules on companies governments agencies nonprofits and other organizations that offer goods and services to people in the european union or that collect analyze data try tied to eu's residents there's a lot of compliance programs out there one that's also very popular is fips but we'll get to that when we talk about kms but yeah there you go so i just wanted to quickly show you here the aws compliance programs page where they list out all the types of compliance programs that aws is uh working with and that it has different types of certification and attestments which we can use it was artifact or amazon artifact whichever prefix they decide to use for the name there to ensure that it was has in order to meet those regulatory compliance so you can see them all there and if you want to know a little bit more about any of these you just go ahead and click them and you can read and they have additional information so you have a better idea okay let's talk about pen testing so pen testing is an authorized simulated cyber attack on a computer system performed to evaluate the security of the system and on aws you are allowed to perform uh pen testing but there are some restrictions so permitted services or ec2 instances nat gateways elbs rds so that's relational database service cloudfront aurora api gateways lambda lambda edge functions light cell resources elastic bean stock environments things you cannot do or you should not be doing is dns zone walking via rough d3 hosted zones then there's ddos simulation testing so you should not be doing ddot or dos ddoses or simulated dos or simulated ddos is okay and that doesn't mean that you can't necessarily do them uh again there's a lot of exceptions to the pen testing they have a whole page on this but generally you're not allowed to do ddosing port flooding protocol flooding request flooding can't do any of those things for other simulated events you need to submit a request to a bus a reply could take up to seven days you know again there's a lot of little intricacies here so you'd have to really read up on it if you're interested in doing this okay hey this is andrew brown from exam pro and we are taking a look at pen testing on the aws platform so they have this page here that tells you what you're allowed to do what you're not allowed to do um and there's some additional things you can read into like the stress test policy the ddos simulate simulation testing policy which i didn't cover in detail in the course content but for whatever reason you're interested in it i just want you to be aware of that kind of stuff if you want to simulate events there is a simulate events form that you have to fill out so yeah open it up and you can kind of read about it and it gives it eight of us a heads up of what you're going to be doing stress test fishing malware analysis other so that way that if you are doing it you're not going to get in trouble they're aware of what you are doing okay so that's pretty much it hey this is andrew brown from exam pro and we are taking a look at ibis artifact which is a selfserve portal for ondemand access to itabus compliance reports so here's an example of a bunch of different compliance reports that aws could be meeting and the idea is that when you go to this portal within the database management console you'll have a huge list of reports that you can go and access so here i'm searching for canada to get the government of canada partner package and then i go ahead and i download that report as a pdf and then within the pdf we can click a link to get the downloadable excel and that's pretty much what it is it's just if you want to see that databus is being compliant for different programs hey this is andrew brown from exam pro and we're going to take a look at adobe's artifacts so at the top here we're going to type in artifact and not to be confused with code artifact which i guess is a new service there's just always releasing new services eh and so here we have a video and some things but it's not too hard all we got to do is go to view reports and from here we have all the types of compliance programs or regulatory compliance programs that aws is meeting and we can do is search for something so we type in canada and that's the government of canada partner package and i can go ahead and download that report so when you download it you really want to open this up in um you're going to really want to open this up in adobe acrobat because if you don't open it up in adobe acrobat you're not going to be able to access the downloadables within it i know that's kind of odd to say but that's just what it is you do have to install adobe acrobat reader and once you have it open and i'm just moving it over here this is what you're going to see and it's going to say like hey um oops no i don't want to do that so please scroll to the next page to view the artifact download and so i think that if we go here you know they say scroll to the next page but i'm pretty sure we can just go here on the left hand side and this is what we're looking for that excel spreadsheet so we're going to save that attachment or actually we just can open it up open this file okay and we'll give it a moment i have excel installed and there we go there it is okay so i know it's a little bit odd way to get to those um certificates or reports but that's just how it works um but yeah i mean that's the idea is like if you need to prove that database is meeting whatever those standards are you can just type them in whatever it is i mean like maybe there's like fedramp right whatever it is and download those certificate attestments whatever um and just double check that aws is meeting those standards okay hey this is andrew brown from exam pro and we are taking a look at abs inspector but before we can answer what it does let's talk about hardening so hardening is the act of eliminating as many security risks as possible hardening is common for virtual machines where you run a collection of certain security checks known as a security benchmark so aws inspector runs a security benchmark against specific ec2 instances and you can run a variety of security benchmarks and you can perform network and host assessments and so here's an example of those two check boxes there which you'd say which assessments you want to do so the idea is you have to install the adobe station on your ec2 instance you run an assessment for your assessment target you review your findings and remediate security issues and one very popular benchmark you can run is the cis which has 699 checks so if you don't know what cis it stands for the center of internet security uh and so they are this organization that has a bunch of um uh security controls or check marks uh that are published that they suggest that you should check on your machine hey this is andrew brown from exam pro and we're looking at ddos so ddos is a type of malicious attack to disrupt normal traffic by flooding a website with a large amount of fake traffic so the idea is we have an attacker and the victim the victim is us and it could be our virtual machines our cloud services the idea is that it's some kind of resource which can take in uh incoming requests over the internet so the idea is the attacker is utilizing the internet and so they may control a bunch of virtual machines or servers they're loaded up with malicious software and the idea is that the attacker is going to tell them all to send a flood of traffic over the internet at your computing resource and this is where your website is going to either start to stall or it's going to become unavailable for your users and so the idea here is that you know if you want to protect against cdos you need some kind of ddos protection traditionally those used to be like thirdparty services that you uh would have to pay for and the and it would sit in front of your load bouncer or your n server but now the great thing with cloud service providers is that generally their networks have builtin ddos protection so the idea is just by having your compute or your resources on aws you're going to get builtin protection for free via aws shield and we'll talk about that next hey this is andrew brown from exam pro and we are taking a look at it with shield which is a managed ddos protection service that safeguards applications running on aws so when you route your traffic through refu3 or cloudfront you are using it with shield standard so here's a diagram to kind of show you that it's not just those services but these are the most common ones where you'll have a point of entry into aws so here we could also be including elastic ip it was global accelerator but the idea is that when you go through these services into the airbus network it has shield built in and so you're going to get that protection before those uh before that traffic reaches your cloud services and in this case we're showing uh ec2 instances so it was shield protects against layers three four and seven attacks uh layer three four and seven is based off the osi model which is a um a fundamental networking concept so seven is for the application layer four is the transport layer three is the network layer um there are two different types of plans ready with shield we have shield standard which is free and then shield advance which starts at 3000 usd per year plus some additional costs based on usage of the size of the attack or what services you're using how much traffic is moving in and out so protection against the most common ddos attacks is what shield standard does you have access to tools and best practices to build ddos brazilian architecture it's automatically available on all above services for additional protection against larger and more sophisticated attacks that's where she'll advance comes into play it's available for specific database services so refugee 3 cloud front elb able global accelerator elastic ip and some notable features here is visibility reporting on layer three four and seven you're only going to get seven if you are using it will swap with it uh you have access to team and support so these are ddos experts but you're only gonna get it if you're paying for business or enterprise support as you're paying for this as well uh you also get ddos cost protection just to ensure that you know your bills don't go crazy and it comes with an sla so you have a guarantee that it's going to work both plans integrate with aws web application firewall so waff to give you that layer 7 application protection so understand that if you're not using waff you're not going to be having that layer 7 production okay hey this is andrew brown from exam pro and we are looking at amazon guard duty so before we look at that we need to understand what is an ids ips so an intrusion detection system and intrusion protection system is used as a device or software application that monitors a network or systems for malicious activity or policy violations so guard duty is a threat detection service which is ids ips that continuously monitors for malicious and suspicious activity and unauthorized behavior it uses machine learning to analyze the following database logs your cloud trail logs your vpc flow logs your dns logs and what it will do is report back to you and say hey um there's this issue here and this is actually one that's very easy to replicate it's just saying somebody is using the root credentials and it's suggesting that you should not be doing that right because you're never supposed to be invoking api calls with the root credentials or you should be limiting that you might also notice that if you want to investigate you can kind of follow up that with uh amazon detective or aws detective which ever prefix they decided to put on that service it will alert you of findings which you can automate an incident response via cloudwatch events which this uh it's been renamed to eventbridge as you know or third party services so you can follow up a remediation action and here is a graphic of amazon guard duty just a bit up closer so you can see all the findings and you can just see you have a lot of detailed information there okay hey this is andrew brown from exam pro and we're going to take a look at guard duty so guard duty is an intrusion protection and detection service and so what i've done is i've done some bad practices purposely so that i can show you some information in there so i'm gonna go over to guard duty okay and you do have to turn guard duty on and so once scar duty is on you're going to start getting reports coming in so notice here that we have some anomalous behavior eight days ago and so uh that's bako he's my cofounder he's also named andrew as well and so we can kind of see some details here about who's accessing what and what they were doing he's not doing anything malicious but we can have an idea where they're from even shows uh generally where he is which he is near thunder bay and his his provider would be tbaytel um and you can see that he is making uh api calls the scribe account attributes and things like that then the other issue is the root account so there's mfa i turned it off so that we can or maybe it's just usage here i actually do have it turned on i suppose but here we see root credential usage and so it's saying hey you used it 77 times because sometimes i go in and and use the ruby account for tutorials but saying you're using this way too much you've got to stop doing that okay so that's something that is uh pretty interesting with guard duty and it's really cost effective and easy to turn on so you can turn it on looks like they have a new thing for s3 have not looked at that as of yet but that's kind of cool kind of feels like that would overlap with amazon macy but whatever and here we get a breakdown of cost so we see cloudtrail vpc full logs dns logs and this is where it would be ingesting data if you want to use that s3 protection you'd have to probably be turning or creating a custom cloud watch trail that has data events to consume that information um you know so you know hopefully that gives you kind of an idea of things you can do and you can also centralize guard duty into one account so you can have one thing that takes care of everything and move all the data across all your accounts into a single place so that's kind of interesting and you can set up follow followups um it's possible that i don't see this this here but generally it would show you uh it would show you a way of like triggering into cloud watch probably could do it pragmatically this is something interesting like the list management you can add trusted ips or threat list so if there's people that you know are fine you can just white list them or if there's people that you know that are bad make sure that they are never allowed to get through so that's pretty much it with guard duty okay let's take a look here at amazon macy so macy is a fully managed service that continuously monitors s3 data access activity for anomalies and generates detailed alerts when it detects risks of unauthorized access or inadvertent data leaks so macy works by using machine learning to analyze your cloudtrail logs and macy has a variety of alerts so we have anomalized access config compliance credential loss data compliance file hosting identity enumeration information loss location anomaly open permissions privilege escalation ransomware service disruption suspicious access and mac will identify your most atrisk users which could lead to compromise so here's just one little kind of tidbit from the app itself where you have the total users and they categorize them into different uh risks i can't remember which flag means what in here uh amazon macy is an okay service it's very important if you're storing things in s3 but i don't i don't use it very often to be honest hey this is andrew brown from exam pro and we are taking a look at aws virtual private network also known as vpn so aws vpn lets you establish a secure and private tunnel from your network or device to the aws global network it's very important to emphasize the word secure here because when you're using direct connect that will establish a private connection but it's not using any kind of protocol to secure that data in transit whereas database vpn will be using a secure protocol there are two options here we have abyss site to site vpn so securely connect onpremise network or branch office site to vpc and it was client vpn so securely connect users to aws or onpremises networks one thing that we need to understand alongside vpns is ipsec this stands for internet protocol security and is a secure network protocol suite that authenticates and encrypts the packets of data to provide secure encrypted communication between two computers over an internet protocol network and it is used in vpns and it was definitely uses it okay hey this is andrew brown from exam pro and we are taking a look at aba's web application firewall also known as waff which protects you protects your web application from common web exploits so the idea here is you write your own rules to allow or deny traffic based on the contents of an http requests you use a rule set from a trusted image security partner in the abyss waff rule marketplace waft can be attached to either cloudfront or an application load balancer so here is that diagram the idea is you see cloudfront with the waf or alb with the laugh and what it does is it can protect uh web applications from attacks covered and the owasp10 uh top 10 most dangerous attacks if you don't know owas they're the open web application security project and they basically have all these security projects which are things to say hey these are things that you should commonly protect against or they might have like example applications that serve as a means to learn security so we look at the top 10 it's injection broken authentication sensitive data exposure xml external entities so xxe broken access control security misconfigurations crosssite scripting so xss insecure deserialization using components with known vulnerabilities and insufficient logging and monitoring so there you go hey this is andrew brown from exam pro and we're going to take a quick look at adabus web application firewall also known as waff and so in this account i happen to have a waf running so we don't have to create one we already have something we can take a look here so i'm going to go to waff and shield and then on the left hand side you'll notice it's a global service but on the left hand side we're going to be looking for our web acls and so the idea is that when you want to waff you create a web acl and then when then within that web acl you have uh the overview and then you have you can kind of show you kind of the traffic that's going on here we can have our rules and so there's a lot of different kind of managed rule groups that you can use so these are ones that are provided by aws so and a lot of these some of these can be paid some of these are free so you see there's these free rule groups where you're like hey i don't want any anonymous ips you checkbox that on you know or i want to protect against sql injection now the interesting thing is that abyss has this capacity unit so you can't add all of these you can add a certain amount of capacity before you have to uh pay for more or something like that it's just kind of a way to um uh kind of cap the amount of stuff that you can put in in terms of rules um but there's a lot of other um rule groups from third party services like security companies that know what they're doing so if you like fortinet's os top 10 you can subscribe to that in the marketplace and be able to use it but uh yeah so that's how you apply rules there's something called bot control i've never used this before get realtime visibility into bot activity on your resource and controllers what bots allow and block from your resources that sounds really cool i cannot stand bots so i might turn that on myself or take a look at the cost there and see what we can find out but that's pretty much it with waff one thing i would say is that you can block out specific ip addresses or white list specific ip addresses and you might do that through rules i'm just going to see yeah like maybe the bypass here and so these ip addresses are some of our um cloud support engineers where they're using our mid panel and um waff is being too aggressive in terms of protection and so sometimes you have to say hey allow this ip address and let my um you know let my cloud support engineer be able to use the mid panel because they're not malicious okay so that's one little exception there but that's pretty much it okay hey this is andrew brown from exam pro and we are taking a look at hardware security modules also known as hsm and it's a piece of hardware designed to store encryption keys and it holds keys in memory and never writes on the disk so the idea is that if the hsm was shut down that key would be gone and that would be a guarantee of protection because nobody could you know take the drive and steal it so here is an example of an hsm these are extremely expensive so you definitely don't want to have to buy them yourselves uh they generally follow fips so fips is the federal information processing standard so it's a u.s and canadian government standard that specifies the security requirements for cryptographic modules that protect sensitive information fips is something you want to definitely remember and there are two different protocols here there's actually a bunch of different fips versions but we have fips 142 level 2 and then fips 143 level 3 so let's talk about the difference here so hsms that are multitenant are going to be using fips 142 hyphen 2 level 2 compliant where you have multiple customers virtually isolated on the hsm and then there are hsms that are single tenant and so they're going to be utilizing fips 140 hyphen 2 level 3 compliant so a single customer on a dedicated hsm and so the reason why we have these two levels is that when you have multiple tenants you can say oh right this thing is uh has temper evidence so we can see that somebody was trying to break into it but there's no guarantee of tamp it being tamper proof where level three is tamper proof there's also fips 140 hyphen 3 which is the new uh the newer standard but not all cloud resources can meet that standard just because of how they offer the service so again fips 142 is really good but just understand that there are other ones out there and it's very easy to get fips 142 level 3 mixed up with pips 140 iphone 3 something that i always had a hard time remembering the distinguishing between those two so for multitenant this is where we're using adabus key management service and for single tenant we're using aws cloud hsm and the only time you're really using cloud hsm is if you're a large enterprise and you need that regulatory compliance of getting fips 140 heaven to level three okay hey this is andrew brown from exam pro and we are taking a look at key management service also known as kms and it is a managed service that makes it easy for you to create and control the encryption keys you use to encrypt your data so kms is a multitenant hsm so it's a hardware security module and many aws services are integrated to use kms to encrypt your data with a simple checkbox and kms uses envelope encryption so here's that example of a simple checkbox in this case it's for rds and what you'll do is choose a master key a lot of times aws will have a default key for you that's managed by them that is free to use which is really great so for kms it's using envelope encryption so when you encrypt your data your data is protected but you have to protect your encryption key when you encrypt your data key with a master key as an additional layer security so that's how it works so just to make this really clear i have my data i use this key to encrypt this data and now i need to protect this key so i use another key to encrypt this key which forms an envelope and then i store this master key in kms and this one's considered the data key all right hey this is andrew brown from exam pro and we're going to take a look at key management service also known as kms so type in kms on the top here and we'll pop over here and kms is a way for you to create your own keys or you can use abyss manage keys so up here and not all these appear right away but as you use services um you will itamas will generate out manage keys for you and these are free you can create your own keys um and these cost a dollar each so if i go ahead here and create a key i can choose whether it's symmetric or asymmetric which we definitely learned in the course which is nice for asymmetric you can make it encrypt and decrypt sign and verify and they're just kind of narrowing down the type of key you would use for this you know if i went to symmetric i go here i'm just kind of seeing if i can enter the actual material into the key here so i'm just going to keep clicking through here my custom key generally you don't really need to do this but you know if it's interesting you can set up administrators to say who's allowed to administer the key and then you have someone that is allowed to use the key and you usually want to keep those two accounts separate you don't want the same person administrating and using the key okay keep those two separate and so we would have a key policy so you can change this to say the rules that is allowed to use and then we can go here and hit finish and so there we now have our own custom key and one thing we can do is it's possible to rotate out these keys when you need to be um but anyway when we use kms it's built into basically everything and we've seen it multiple times throughout this course when we've gone over to ec2 we'll just go take a peek at a few different places here so when we've gone to go launch an ec2 instance and we go over to storage so we say select and review or next and we go over to storage notice that here this is using encryption right so i can choose that or even my custom key if you're in dynamodb or anywhere else it's always something like a checkbox and you choose your key so that's pretty much all there really is to kms it's very easy to use and there you go hey this is andrew brown from exam pro and we are going to take a look here at cloud hsm it is a single tenant hsm as a service that automates hardware provisioning software patching high availability and backups so here's the idea is that you have your aws cloud hsm you have your developers interacting with it your application interacting with it you have an hsm client installed in your ec2 instance so that it can access uh the cloud hsm keys so aws cloud hsm enables you to generate and use your encryption keys on fips 140 hyphen 2 level 3 validated hardware it's built on open hsm industry standards to integrate with things like pk cs 11 java cryptography extension so jce microsoft crypto and g libraries you can transfer your keys to other commercial commercials hsm solutions to make it easy for you to migrate keys on or off aws configure aws kms to use aws cloud hsm cluster as a custom key store rather than the default kms keystore uh so cloud hsm is way more expensive than kms kms is like free or a dollar per key where cloud hsm is a fixed cost per month because you are getting a dedicated piece of hardware um and there's not a lot of stuff around it so other than the aws kms integration a lot of times it can be really hard to use this as well so the only time you're really going to be using cloud hsm is if you're an enterprise and you need to meet fips 140 hyphen 2 level 3 compliancy okay hey this is andrew brown from exam pro and we are taking a look at know your initialism so a lot of aws services and concepts and cloud technologies use initialisms to just kind of shorten uh common things that we need to use on a frequent basis and it's going to really help if you learn these because then what you can do is substitute them when you are seeing a service name or something particular and that's going to get you through content a lot faster and in the wild you're going to see these all over the place because people aren't going to say the full name they're going to say the initialism so let's go through them so for iam it's identity and access management for s3 that's simple storage for swfs it's uh swf that's simple workflow service sns is simple notification service sqs is simple queue service scs a simple email service ssm is simple systems manager but uh you know when we see the name it's usually just systems manager but we still use the uh initialism ssm then there's rds relational database service vpc virtual private cloud vpn virtual private network cfn cloud formation waf web application firewall and that is a very common initialism uh not just databus but outside of it as well mq for amazon active mq asg for auto scaling groups tam for technical account manager elb for elastic load balancer alb for the application load balancer nlb for the network load balancer gwlb for the gateway load balancer clb for the classic load balancer ec2 for elastic cloud or cloud compute ecs for elastic container service ecr for elastic container repository ebs for elastic block storage emr for elastic mapreduce efs for elastic file store ebs or eb for elastic beanstalk es for elasticsearch eks for elastic kubernetes service msk for managed kafka service and if you think i got the s and k backwards i did not for whatever reason it's msk uh then uh there's resource manager which is known as ram acm for amazon certificate manager popl for principle of lease privilege which is a concept not a service iot internet of things this is not a service but is a tech concept or cloud concept ri for reserved instances and i'm sure there are more but these are the ones that i know off the top my head uh and they're in my usual use case uh for what i'm doing day to day but a lot of times you'll probably just end up needing to remember asg elb um ec2 s3 things like that okay all right let's compare aws config and app config which both have configured the name but there are two completely different services so aws config and app config so abs config is a governance tool for compliance as code you can create rules that will check to see if resources are configured the way you expect them to be if a resource drifts from the expected configuration you are notified or aws config can auto remediate correct the configuration back to the expected state for app config it is used to automate the process of deploying application configuration variable changes to your web application you can write a validator to ensure uh the changed variable will not break your web app you can monitor deployments on automate integrations to catch errors or rollbacks so config is for compliance governance app config is for uh config application configure configuration variables so there you go well let us take a look at sns versus sqs and these things have something in common and it's they both connect apps via messages uh so they're for application integration so let's take a look at sns so simple notification service and then simple queue service okay so sns is intended to pass along messages via a pub sub model whereas sqs queues up messages and has a guaranteed delivery so the idea with sns you send notifications to subscribers of topics via multiple protocols so it could be http email sqs sms and sns is generally used for sending plain text emails which is triggered via other services the best example here is building alarms i know we mentioned this but i like to repeat it so that you absolutely know it can retry sending in the case of failures of https so it does have a retry attempt but that doesn't mean there's a guarantee of delivery it's really good for web hooks simple internal emails triggering lambda functions if you had to compare these to thirdparty services it's similar to pusher or pub nub so sqs is uh the idea here is that messages are placed into a queue applications pull the queue using the itabus sdk you can retain a message for up to 14 days you can send them in sequential order a sequential order or in parallel you can ensure only one message is sent you can ensure messages are delivered at least once it's really good for delayed tasks queuing up emails um comparable uh stuff would be something like rabbit mq or uh ruby on rails sidekick okay hey this is danny brown from exam pro and we're doing variation study with sns versus ses versus pinpoint versus work mail and so sns and scs get confused quite often but all of these services uh have something common they all send emails but the utility of email is completely different for each one so the first one is simple notification service is for practical and internal emails so you send notifications to subscribers of topics via multiple protocols so it's not just for email it can handle http it can send to sqs it can send sns or sms messages so um messages to your phone but it does send emails and so sns is generally used for sending plain text emails which is triggered via other aws services the best example of this is a building alarm so most exam questions are going to be talking about sns because lots of services can trigger sns for notifications and so that's the idea it's like oh um you know did somebody spin up a server send off an email via sns uh did we spend too much money here you know all sorts of things can go through sns to send out emails and you need to know what are topics and subscriptions regarding sns then you have ses so simple email service and this is for transactional emails and when i say transaction emails i'm talking about emails that should be triggered based on inapp action so sign up reset password invoices so a cloudbased email service that is similar to this would be like send grid scs sends html emails sns cannot so that is the distinction is that scs can do html and plain text but sns just does plain text and you would not use sns for transactional emails sas can receive inbound emails scs can create email templates custom domain name emails so when you use sns it's whatever amazon gives you it's going to be some weird address but ses is whatever custom domain you want you can also monitor email reputation for scs then you have amazon pinpoint and so this is for promotional emails so these uh when we say promotional we're talking about emails for marketing so you can create email campaigns you can segment your contacts you can create customer journeys via emails um it can do a to b email testing and so scs and pinpoint get mixed up because a lot of people think well can i just use my transaction emails for promotion emails absolutely you can it's not recommended because um you know pinpoint has a lot more functionality around promotional emails they're built differently and so you know just understand that those two have overlapping responsibilities but generally you should use them for what they're for then you have amazon workmail and this is just an email web client so it's similar to gmail or outlook you can create company emails read write and send emails from a web client within the database management console so there you go let us compare amazon inspector versus adabus trusted advisor so both of these are security tools and they both perform audits but what they do is slightly different so amazon inspector audits a single ec2 instance that you've selected or i suppose you could select multiple ec2s it generates a report from a long list of security checks um and so trusted advisor has checks but uh the the key difference here is that it doesn't generate out a pdf report though i'm sure you could export csv data if you wanted to and then turn that into a report it gives you a holistic view of recommendations across multiple services and best practices so for example if you have an open port on the security groups that can tell you about about that you should enable mfa on your root account when using trusted advisor things like that one thing though is that trust advisor isn't just for security does checks across um five different things but they both use security and they both technically do checks okay so there are a few services that have connected the name you'd think they'd be related in some way but they absolutely are not and they don't even have similar functionality but let's take a look here so we know the difference the first is direct connect it is a dedicated fiber optics connection from your data center to aws it's intended for large enterprises with their own data center and they need an insanely fast and private connection directly to aws and you'll notice they give private and emphasis because if you need a secure connection you need to apply a database virtual private network connection on top of direct connect then you have amazon connect this is a call center as a service get a tollfree number accept inbound and outbound calls set up automated phone systems uh so if you ever heard of an interactive voice system at ibs this is basically what amazon connect is you have media connect this is the new version of elastic transcoder it converts videos to different video types so if you have let's say a thousand videos you need to transcode them into different video formats maybe you need to apply watermarks insert introduction videos in front of each one this is what you use media connect for okay just in case you see elastic transcoder as an option i just want you to know what it is compared to media connect so both these services are used for transcoding and technically elastic transcoder is the old way and it was elemental media convert or just media convert is the new way so elastic transcoder was the original transcoding service it may still have chromatic apis or workflows not available in media convert so this could be reasons why we see legacy customers still using it or you know it's just too much effort for them to upgrade to the new one it transcodes videos to streaming formats media convert is more robust transcoding service that can perform various operations during transcoding so it also translates videos to streaming different streaming formats but it overlays images it inserts video clips extracts captions data it has a robust ui so generally it's recommended to use the uh media convert in terms of costs they're basically the same so there's no reason not to use media convert okay so it was artifact versus amazon inspector get commonly mixed up all the time but both artifact inspector compiler pdf reports so that's where the confusion comes from but let's talk about what is different about the reports so abus artifact enables inspector so for artifact you're answering why should an enterprise trust aws it generates a security report that's based on global compliance frameworks such as sock or pci or a variety of others where amazon inspector is all about how do we know the cc2 instance is secure can you prove it so it runs a script that analyzes your ec2 instance then generates a pdf report telling you which security checks had passed so the idea here is it's an auto tool for security of ec2 instances so there you go so let's compare elb versus alb versus nlb versus jwlb versus clb uh because you know when i was first learning aws i was getting confused because there was elastic load balancer but there was these other ones so what gives right so what's happening here is that there is a main service called elastic load balancer elb and it has four different types of possible load balancers so we'll go through all the types so the first is application load bouncer commonly uh initializes alb and so this operates on layer seven for https so this makes sense because that is the application layer and it has some special powers in terms of routing rules so the idea here is you can create rules to change routing based on information found within the https request so let's say you wanted some routes to go that have a particular subdomain to this server and a different subdomain to another one you could do that and because it is an application load bouncer you can attach a web application firewall for protection you can't attach this on the nlb or other ones because they're not application based so that is just a little caveat there then you have network load bouncer uh commonly abbreviated to nlb this operates on layer three and four so we're talking tcp udp this is great for when you have extreme performance that requires tcp and tls traffic it's capable of handling millions of requests per seconds while maintaining ultra low latency it's optimized for sudden and volatile traffic patterns while using a single static ip address per availability zone if you're making video games this is what they like to use is the network load balancer but it has other utilities outside of that then you have gateway load bouncer gwlb this is where you need to deploy a fleet of thirdparty virtual appliances that support uh i don't know how to say that in abbreviation but i'll just say it's geneve um and there's not much we need to know outside of that okay then there is the classic load bouncer uh commonly initializes clb this operates on layer three four and seven it's intended for applications that were built within the ec2 classic network it doesn't support target groups so albs at nlbs use target groups which is just an easier way of grouping together a bunch of targeted resources like compute that we're going to load balance to and with classic load balancer you just directly assign ec2 instances and it's going to be retired on august 15th of 2022 so yeah it looks like it can do a lot of stuff but it also doesn't have any of the superpowers of these specialized ones and so there's no reason to keep it around and generally you should not be using it and so yeah that's about it
