With timestamps:

00:03 - hey welcome to neural nets in javascript
00:06 - with brain.js i am super excited to
00:08 - teach you this course the goal is to
00:10 - give you a practical introduction to
00:12 - problem solving with neural networks
00:15 - what you're going to be learning in this
00:16 - course propagation both forward and
00:18 - backward layers neurons training error
00:20 - what feed forward neural networks are
00:22 - what recurrent neural networks are and a
00:23 - whole lot more we're going to build azor
00:25 - gate a counter a basic math network an
00:28 - image recognizer a sentiment analyzer
00:30 - and a children's book creator
00:33 - and how we're going to do it is with 17
00:35 - lectures where we're going to focus on
00:37 - practice over theory what that means is
00:38 - you are going to get your hands dirty
00:40 - but more than that you're going to walk
00:41 - away knowing the ideas behind neural
00:44 - networks there's as well a bunch of
00:45 - interactive challenges along the way
00:47 - and that brings me to
00:49 - our use of scrimba scrumba is a
00:51 - fantastic platform for learning and at
00:54 - any point during the entire lecture you
00:56 - can stop me it won't hurt my feelings
00:58 - you can just
00:59 - introduce brand new script and you can
01:01 - press
01:03 - command plus s if you're on a mac or
01:06 - control plus s if you're on linux or
01:09 - windows and it will execute exactly your
01:12 - code that is super important throughout
01:14 - this course as well i'm going to make
01:15 - regular reference to the console which
01:17 - is directly below here
01:22 - so if you see some numbers go down there
01:24 - like i'll go ahead and test that right
01:25 - now
01:28 - 0.05 just appeared that's super
01:31 - important anytime i start talking about
01:32 - if the neural net was good because it
01:34 - had a low error rate or if the neural
01:36 - net was bad because it had a higher rate
01:38 - just look down there and that will give
01:40 - you a little bit of reference as to what
01:42 - we're doing so let's get started this is
01:44 - going to be awesome
01:49 - this is our very first neural net this
01:51 - is going to be awesome
01:53 - so the first problem that we're going to
01:54 - tackle is called exclusive or and you
01:56 - can do some research on it if you like
01:58 - but more or less this is what happens
02:00 - you have inputs that are the same they
02:02 - result in a zero output when they differ
02:05 - it results in a one
02:07 - there's always two inputs there's always
02:09 - one output
02:10 - so let's take this very simple comment
02:13 - and let's translate it into something
02:15 - that the neural net or rather the
02:17 - javascript can understand let's have a
02:20 - variable we're going to call it
02:23 - training data
02:28 - and there's our very simple
02:30 - variable that represents all of our
02:31 - training data
02:33 - and let's go ahead and import
02:35 - brainjs
02:37 - brain.js is i'm just going to grab a
02:39 - link that
02:40 - imports it from a cdn
02:43 - content delivery network
02:49 - got that
02:50 - and
02:52 - next we want to instantiate a new
02:54 - instance of brain
02:57 - and we do that const
02:59 - i'm going to say net equals new
03:02 - brain dot neural network
03:10 - and down here
03:13 - we're going to say net
03:15 - dot
03:16 - train and we're going to give it our
03:19 - training data
03:20 - and now at line 16 by the time we get
03:23 - there
03:24 - the the net will have understood
03:26 - what our inputs and outputs are
03:29 - and so we can hear console log out
03:33 - net dot run
03:36 - one of our inputs so let's choose the
03:38 - first one
03:45 - got to
03:46 - give it our layers hidden layers three
03:49 - we'll get more into hidden layers later
03:52 - and now we're going to go ahead and hit
03:53 - run
03:55 - and now we have an output
03:58 - that's so awesome now the reason that
04:00 - this number here is not zero
04:03 - is because we're using
04:05 - uh a neural net and it's very hard for
04:07 - them to speak specifically zero and one
04:10 - they can speak close to that so
04:13 - that's exactly what we want is is a
04:15 - number close to zero which is 0.05
04:19 - now here's a challenge for you
04:21 - go ahead and get the next
04:23 - outputs
04:24 - console logged out
04:26 - and just kind of play around with their
04:27 - values and and see how the net operates
04:34 - in our last tutorial we talked about how
04:36 - to build a neural net a very simple one
04:38 - to solve exclusive or and in this one
04:40 - we're going to discuss how it did it so
04:42 - the neural net has different stages
04:44 - you'll notice i use two different
04:45 - methods here the first one is train and
04:47 - the other one's run now in train we do
04:50 - something called forward propagation and
04:51 - back propagation those terms may seem
04:54 - scary at first
04:55 - but they're actually quite simple in
04:57 - fact we're going to reduce their
04:58 - complexity down to something that even a
05:00 - child can understand you'll take a look
05:02 - at my slides here
05:04 - forward propagation and back propagation
05:06 - we have a ball
05:08 - we're going to take a ball and we're
05:09 - going to throw it at a goal now when we
05:11 - do that we're going to make a prediction
05:13 - as to how far
05:15 - the ball needs to go
05:16 - uh how much energy to put behind it the
05:18 - pathway of the ball etc i want you to go
05:21 - ahead and pause the video here
05:23 - and to think about the different steps
05:25 - that happen when you throw a ball at a
05:27 - goal
05:29 - the first step is prediction
05:31 - now in prediction we're going to think
05:33 - about how we're going to throw the ball
05:35 - where it needs to land how much power we
05:36 - need to put behind it
05:38 - that first step
05:40 - and with my ascii art showing off that
05:41 - we did not go far enough with the ball
05:44 - this step
05:45 - is for propagation
05:47 - we ourselves are making a prediction
05:50 - from that prediction we can see how far
05:52 - we were off from the actual goal
05:56 - we can measure that
05:58 - and that that step of measuring
06:00 - that is back propagation
06:03 - now the next thing that we want to do
06:07 - is make a determination as to what we're
06:08 - going to do next
06:11 - that is our second step of back
06:13 - propagation that is our learning step
06:17 - and you know how the story goes we throw
06:19 - the ball again it goes too far we
06:21 - measure that we make another prediction
06:22 - we throw the ball again
06:24 - third time's a charm that illustrates
06:28 - this very first method and everything
06:30 - that goes on inside the net the next
06:32 - stage is running our net
06:34 - now in running our net we no longer have
06:36 - to
06:37 - measure how far we are from the goal we
06:40 - already know
06:42 - and because of that there is no need to
06:44 - back propagate so that step goes away
06:47 - now throughout this entire training of
06:49 - the neural net the net is measuring and
06:52 - that measurement is referred to as error
06:57 - check this out if we go to our net
07:00 - during its training we can actually
07:02 - enable
07:03 - something really interesting
07:05 - we're going to give it a log function
07:10 - we're going to console log
07:13 - error
07:16 - and we're going to have our log period
07:20 - set to 100 intervals
07:23 - check this out
07:26 - now we can actually see the error how
07:28 - far off the net was and you can see for
07:30 - a time some of these errors may go down
07:33 - or may go up but eventually the net
07:35 - catches on and it starts to accelerate
07:39 - its ability to learn
07:41 - until
07:42 - the error rate starts to drop
07:44 - to a ridiculously low number
07:46 - not zero though
07:48 - until training is completed
07:50 - and once training is completed there's
07:52 - no need to continue training as we
07:54 - discussed we can then just forward
07:56 - propagate
08:01 - in our last tutorial we talked about how
08:03 - neural networks learn using forward
08:05 - propagation and back propagation in this
08:07 - tutorial we're going to understand more
08:09 - the structure of the programmatic neural
08:11 - net neural nets are actually quite
08:13 - simple they are composed of a function
08:15 - that receive inputs as an argument and
08:18 - produce outputs if we think of our
08:20 - neural net in this very simplistic way
08:23 - then really we can reduce the complexity
08:25 - of it down to one of the simplest
08:27 - functions that you can write pause the
08:30 - video here and just look at the
08:32 - structure
08:35 - now we're going to talk about
08:36 - how the network
08:38 - initiates if you think about when you
08:40 - were first born
08:42 - likely you didn't know very much over
08:45 - time though you begin to know more and
08:47 - more the neural net begins with a bunch
08:49 - of random values so everything that
08:51 - affects the outputs is just random at
08:53 - first you may ask yourself why the
08:55 - reason is because mathematically we've
08:58 - proven that is an effective way to start
09:01 - off with knowledge the knowledge is very
09:03 - random at first we don't know the idea
09:06 - it's not zero and it's not one it's
09:08 - somewhere in between over time we can
09:10 - shape that random data so that it
09:12 - finally becomes where we store what's
09:15 - going on inside of the neural net
09:17 - each neuron is quite literally
09:20 - math.random
09:21 - go ahead and pause the video here and
09:23 - get comfortable with the idea that the
09:25 - net starts out with random data
09:28 - next i want to talk about activation a
09:30 - really popular and effective activation
09:32 - function that's used nowadays is called
09:34 - relu
09:35 - relu looks kind of like this
09:37 - if we were to put it inside of a
09:39 - function
09:40 - the function would quite literally look
09:42 - like this
09:51 - that is our activation function called
09:53 - relu now activation functions are
09:56 - measured in back propagation
09:58 - using what is called their derivative
10:00 - i'll go ahead and put a link here in our
10:02 - bonus material two i'll go ahead and
10:04 - post some links that take you to where
10:06 - relu and its derivative are used in
10:08 - brain
10:13 - our last tutorial we talked about the
10:15 - structure of a neural net and in this
10:16 - one we're going to be talking about
10:18 - layers take a look at my illustration
10:20 - this is a neural net each circle
10:22 - represents a neuron the arrows represent
10:25 - a bit of math
10:28 - stacked circles
10:30 - are layers
10:32 - and so here we have what are called
10:34 - input layers
10:35 - that's this first one
10:37 - this next layer would be a hidden layer
10:41 - it's composed of two neurons
10:43 - the next is another hidden layer
10:45 - composed of two neurons
10:47 - and the last is called an output layer
10:49 - now in brain
10:51 - the input layers and output layers are
10:54 - configured for you kind of automatically
10:57 - however our hidden layers
10:59 - can be configured by us our first neural
11:02 - net was composed of two input neurons
11:06 - one hidden layer
11:07 - that had three neurons and an output
11:10 - layer that had one neuron just as our
11:13 - illustration has
11:14 - two
11:16 - neurons for the input layer
11:18 - two hidden layers the first having two
11:21 - neurons the second having two neurons
11:24 - and the last one having two neurons
11:26 - what's interesting about hidden layers
11:28 - is that's really where the majority of
11:31 - their storage is if you likened it to a
11:34 - human the hidden layers are where the
11:35 - ideas are you may run into a scenario
11:38 - where your neural net isn't learning
11:39 - i'll go ahead and recreate that here i'm
11:41 - going to change the hidden layers from a
11:43 - single hidden layer with three neurons
11:46 - to that of one and i'm going to log out
11:49 - training data
12:02 - watch what happens
12:05 - we hit 20 000 iterations without fully
12:08 - understanding the problem and how to
12:10 - solve it
12:11 - now we can easily fix that by changing
12:14 - our hidden layers to a single hidden
12:15 - layer
12:16 - of three neurons
12:19 - you can see we're able to train in a
12:20 - short amount of time 4 800 iterations we
12:23 - can as well have more than one hidden
12:25 - layer our illustration
12:27 - has two hidden layers we could mimic
12:29 - this exact configuration two hidden
12:31 - layers followed by two hidden layers and
12:34 - this way
12:37 - note of caution though the more hidden
12:39 - layers that you add the longer it takes
12:41 - for the neural net to train
12:43 - let's try it
12:44 - see we hit 20 000 iterations without
12:46 - actually fully training there is no hard
12:49 - and fast rules when it comes to hidden
12:51 - layers this would be an invitation to
12:53 - experiment something i have seen though
12:55 - is to treat the hidden layers sort of
12:57 - like a funnel so if you had for example
12:59 - 20 inputs you could have 15 hidden
13:02 - layers followed by 10 hidden layers
13:04 - followed by two output layers that's
13:05 - just an example though and many problems
13:08 - take on different configurations
13:10 - switching gears for a moment take a look
13:12 - back at our illustration you remember
13:13 - that we have these arrows and i said
13:15 - these arrows represent a bit of math and
13:17 - a feed forward neural net that math can
13:20 - be described as this
13:23 - we have our input weights times our
13:26 - inputs
13:27 - plus biases activated now this is simple
13:30 - math but the implications of it are huge
13:32 - pause here and just think about how
13:34 - simple that is and let it sink into your
13:36 - brain
13:39 - this tutorial series is really about the
13:40 - practical application of neural nets but
13:43 - if you're curious like me
13:46 - you can take a look here
13:48 - to see
13:50 - how brain uses this math as another
13:52 - bonus take a look at the additional
13:54 - options that are available for brain
13:56 - neural nets can be widely configured to
13:58 - solve many different problems all it
14:00 - takes is experimentation
14:02 - time and enthusiasm
14:08 - up till now we've concentrated on the
14:10 - basics of how a neural net works and
14:12 - we've used arrays for our training data
14:15 - but in this tutorial we're going to be
14:17 - talking about sending different shaped
14:19 - data into a neural net
14:21 - now to illustrate what i mean by that
14:23 - let's take a look at my slides
14:27 - here we have
14:28 - an array now it's a very simple array
14:31 - but it's an array and what makes arrays
14:34 - incredibly useful is we can reference
14:36 - the values
14:38 - by index
14:39 - for example the arrays index of 0 gives
14:42 - us 0.3 the array index of 1 gives us the
14:45 - value 0.1 arrays are useful in neural
14:48 - nets because they represent a collection
14:51 - generally of the same type of values and
14:53 - we can use the index to look up those
14:56 - values
14:57 - if we look at an array beside a neural
15:00 - net
15:03 - we can see each neuron
15:06 - associates meaning with each of the
15:09 - arrays
15:10 - indexes now when dealing with a
15:11 - collection of the same type of value
15:13 - this is perfect
15:15 - but what about when our data isn't in
15:18 - the form of an array
15:20 - what about for example
15:23 - objects it just so happens that brain.js
15:25 - was built for this type of practical use
15:28 - and we are going to build a neural net
15:30 - that uses objects
15:32 - for training to get us started i've
15:34 - included the browser version of brain.js
15:36 - here in the index.html file next let's
15:40 - define what our training data will look
15:42 - like
15:44 - our input
15:46 - will be an object that is going to have
15:49 - the properties red
15:51 - green
15:52 - and blue
15:55 - and our
15:57 - output
16:00 - will have properties
16:03 - light
16:05 - neutral
16:06 - and dark it just so happens i have some
16:08 - training data
16:09 - i'll go ahead and paste it in
16:12 - and you can see our colors
16:15 - and brightnesses
16:16 - what's really useful with brain.js is
16:18 - you don't have to define all the
16:19 - properties you can but you don't have to
16:22 - when they're missing it simply uses a
16:24 - zero in their place so in this first
16:28 - object we see that red is missing well
16:31 - red here will just simply be zero
16:33 - and you can see a similar practice on
16:35 - the brightnesses let's go ahead and turn
16:38 - these two different arrays of objects
16:40 - into our training data
16:42 - so we're going to define const
16:45 - training data equals an array now we're
16:48 - going to iterate over the brightnesses
16:49 - and colors and build up our training
16:51 - data
16:52 - so four
16:53 - we'll say let i equal zero i is less
16:57 - than
16:58 - colors.length
17:00 - i plus plus
17:02 - or you could use a for each or even a
17:04 - map
17:05 - training data
17:06 - dot push
17:08 - we're going to give it an object each
17:10 - one of our training sets is an object
17:12 - and that training set will have an input
17:14 - and that input will be of colors
17:18 - and our output
17:20 - will be from the brightnesses
17:22 - and those indexes are equivalent
17:27 - next we'll define our neural net
17:30 - so const
17:31 - net equals
17:33 - new brain dot neural net work
17:37 - and we're going to give this
17:40 - hidden layers
17:44 - i'll have a single layer with three
17:46 - neurons
17:50 - and next we can basically just train our
17:53 - neural net so net.train
17:57 - and
17:58 - we're going to give it the training data
18:03 - rather than logging out what is
18:05 - happening inside the neural net let's
18:06 - just
18:07 - get our stats what happens at the very
18:10 - end
18:11 - and we'll go ahead and log those
18:15 - to the console now let's see what we get
18:19 - cool 1200 iterations it learned it
18:21 - fairly quickly and let's see actually
18:23 - what the neural net outputs
18:25 - so net dot run
18:28 - and a value of
18:31 - red
18:32 - 0.9 of red and we'll
18:35 - log those values out
18:38 - let's see what we get
18:41 - very cool
18:42 - so you can see in the training set
18:45 - we did not include dark neutral and
18:48 - light in every single one
18:50 - of the brightnesses however brain is
18:53 - smart enough to combine those together
18:55 - and it gives us with red being 0.9
18:59 - that red is dark now
19:02 - as a bonus
19:05 - i could spell bonus correctly
19:08 - there we go what if we had to invert the
19:11 - problem
19:13 - what i mean by that
19:14 - is what if we are for example asking our
19:17 - neural net for colors rather than
19:20 - classifying
19:21 - their brightness in this scenario pause
19:24 - it here and think about how you would
19:25 - accomplish this
19:30 - so by inverting the problem
19:32 - our inputs
19:35 - would then be
19:38 - light
19:40 - neutral and dark
19:43 - and our output would be a color red
19:46 - green
19:48 - or blue for us to flip the values
19:51 - let's define our training data again
19:53 - this will be
19:54 - const inverted
19:57 - training data
20:00 - and we are going to
20:03 - for
20:06 - i equal zero
20:08 - i equals colors
20:11 - dot length
20:15 - we're going to take
20:18 - the inverted training data and push
20:22 - objects to it that are just like prior
20:27 - our input and output
20:30 - but we are going to change
20:33 - the input to accept brightnesses
20:36 - and our output to accept colors
20:39 - so that's our training data
20:41 - next let's define a
20:43 - inverted
20:45 - net
20:46 - those new brain dot neural
20:50 - neural network
20:52 - the same hidden layers
20:59 - and let's train it
21:03 - inverted
21:07 - stats
21:08 - equal
21:10 - inverted net
21:11 - dot train
21:14 - inverted
21:16 - training data
21:19 - we'll go ahead and train on this
21:22 - cool let's log out the stats
21:33 - there's our stats
21:36 - and we can see it actually didn't do a
21:38 - great job at learning the problem but
21:40 - that isn't the point of this exercise
21:42 - it's really just to understand the
21:44 - neural net from a different vantage
21:46 - point when you flip a neural net like
21:48 - this you're kind of generating values
21:50 - that can be really useful in predictions
21:57 - in this tutorial we're going to get a
21:58 - bit more adventurous and push the
22:00 - boundaries of what you can do with a
22:02 - neural net but doing so in a safe and
22:04 - easy manner
22:06 - but to help us understand take a look
22:09 - at my slides in our previous tutorials
22:12 - we used numbers directly in the neural
22:15 - net and numbers are just one of the
22:18 - types that exist in javascript and in
22:20 - other languages now javascript is fairly
22:22 - simple in its types in fact we have only
22:25 - boolean numbers objects strings null and
22:29 - undefined now it would be nice if we
22:31 - could feed
22:32 - these other types of values into the
22:35 - neural net so that it can understand
22:37 - context better and solve different and
22:39 - seemingly more complex problems
22:42 - so
22:44 - are we
22:45 - doomed then
22:47 - do they just speak numbers previously we
22:51 - used an object with a neural net and we
22:53 - did so using its properties defined with
22:56 - numbers but the principle of assigning a
22:58 - value to a neuron will provide us the
23:02 - answer for our neural net to speak more
23:05 - than just numbers
23:06 - now the question in your mind is
23:08 - probably how let me illustrate in a way
23:11 - that a child would understand this is a
23:14 - light switch it is off
23:16 - now if you were to ask a child to turn
23:18 - the light switch on
23:20 - they of course
23:23 - would
23:24 - but it wouldn't just happen like that
23:26 - it would happen more like my son does
23:29 - he sees that it is off
23:32 - and he becomes excited and he's taking
23:35 - gymnastics and so he will use that to
23:37 - his advantage he'll perform some amazing
23:39 - gymnastic maneuver over to the light
23:41 - switch
23:42 - turning it on
23:43 - and seeing that it's on that he just did
23:45 - something very useful he'll let out a
23:47 - cheer and do some gymnastic move off and
23:50 - away
23:52 - celebrating that he was a useful kid and
23:55 - that he has the ability to flip switches
23:59 - this ability to understand
24:01 - both
24:02 - off
24:03 - and on has huge implications now our
24:06 - computers they speak binary
24:08 - that's just ones and zeros so it's very
24:10 - very similar language to the neural net
24:13 - we could use this same practice we could
24:16 - say that zero is off and one is on now
24:20 - we sort of did this previously with
24:23 - objects via their property name but we
24:25 - fed the inputs directly into the neural
24:28 - net in this case we assign a neuron to a
24:32 - specific value that we're training
24:34 - towards either the input or the output
24:37 - and when that input or output neuron
24:39 - fires
24:41 - we basically just assign that value
24:44 - as one otherwise it's zero
24:47 - so just
24:49 - like our
24:50 - on and off
24:52 - we're taking
24:53 - these values like a boolean or null
24:56 - value or a string
24:59 - etc
25:00 - and we're simply assigning it to the
25:03 - input
25:04 - so here my potentially null value
25:07 - is being fired upon so it's
25:10 - a value of 1. my string 1 is as well the
25:13 - other values are not and as for output
25:15 - the net is going to try to learn those
25:17 - values just as before and so really the
25:19 - implications here are that we can send
25:20 - just about any value into a neural net
25:23 - so long as that value is represented by
25:26 - a neuron
25:27 - okay let's go ahead and get coding and
25:28 - then this will all make sense
25:31 - first off in our index.html file i have
25:33 - included the browser version of brain.js
25:36 - next we are going to get some data that
25:39 - we're eventually going to use to train
25:40 - on
25:41 - this initial data is an object with
25:43 - property names of restaurants whose
25:46 - values are what day we can eat free with
25:48 - kids
25:50 - and our objective is to find a way to
25:51 - get these
25:52 - string values
25:54 - represented as ones and zeros into the
25:56 - neural net and what we're going to do is
25:57 - give the neural net a day of the week
26:00 - and it's going to tell us where to go on
26:02 - that day of the week so that we can eat
26:04 - free with our kids
26:06 - now pause it here for a moment and think
26:09 - how you would accomplish this using that
26:12 - light switch analogy
26:17 - next let's go ahead and plan how we're
26:19 - going to input our training data into
26:21 - the neural net so if we are going to use
26:25 - the day of the week
26:26 - as the question we're going to ask our
26:29 - neural net that will be our input so our
26:31 - input is going to be a day of the week
26:33 - so
26:34 - monday
26:35 - tuesday
26:37 - wednesday
26:40 - etc
26:41 - our output
26:43 - is going to be
26:46 - the restaurant name
26:48 - so restaurant
27:00 - so that is our input and output for our
27:03 - neural net now next we're going to go
27:05 - ahead and build up our training data
27:08 - so let's go ahead and build
27:10 - that using a const
27:12 - that's going to be training data that's
27:14 - going to be an array
27:16 - and to
27:18 - put data into the training data from our
27:21 - restaurant so we'll need to iterate over
27:22 - our restaurants so for
27:24 - let
27:25 - grant
27:27 - name
27:28 - in
27:29 - restaurants
27:32 - and the day of the week is the value
27:36 - therein so cost
27:39 - day of week equals
27:42 - restaurants
27:44 - restaurant name
27:46 - all right so we've got our day of the
27:47 - week and now our restaurant
27:50 - and our training data
27:53 - we're going to push a value to it that
27:56 - has our inputs and outputs now our input
27:59 - is going to be
28:01 - this is where the rubber meets the road
28:02 - so to speak
28:04 - it's going to be our day
28:06 - of the week
28:07 - and we're going to assign that a value
28:10 - of 1. now just think about how simple
28:12 - that is if you need to pause it please
28:14 - do so but think about how simple
28:16 - what we just did is
28:18 - we are giving
28:20 - an input
28:22 - to the neural net of a day of the week
28:24 - assigned by
28:25 - its value of one
28:27 - now all the other days of the week
28:30 - because of the way that brain.js is
28:32 - built are going to initially be zero
28:34 - so only this day of the week will be one
28:36 - next let's go ahead and assign our
28:38 - output
28:41 - that's going to be it's restaurant name
28:46 - and we have just built
28:48 - our training data if you need a moment
28:50 - to pause and think about what we've just
28:52 - done
28:52 - please do so but it's a very simple
28:55 - principle
28:56 - the principle of represented values
28:59 - next let's go ahead and define our
29:01 - neural net
29:02 - that'll be const net new
29:05 - brain dot neural network
29:09 - and we're going to give it the same
29:10 - hidden layers as before
29:13 - single hidden layer with three neurons
29:17 - and all that is left to do is train on
29:20 - our training data so we'll do const
29:22 - stats
29:23 - equals net dot net.train
29:27 - training data
29:30 - and then we'll console log our stats out
29:34 - all right are you ready
29:36 - here we go
29:41 - look at that
29:42 - under 2000 iterations the neural net has
29:45 - deciphered
29:46 - what day of the week to eat for free
29:48 - with our kids that's nice but let's look
29:50 - at what comes out of the neural net
29:53 - console.log
29:54 - net.run
29:59 - we're going to say
30:01 - monday
30:06 - see what it says
30:08 - now this is where it gets kind of
30:09 - interesting
30:11 - all the restaurants are included with
30:14 - our result we just have a likelihood
30:16 - associated to each one of those
30:18 - restaurants but really what we want is
30:21 - to put a string in and to get a string
30:24 - from our neural net
30:26 - we're going to do that next
30:28 - if you can pause it just think about how
30:30 - you might be able to do that
30:36 - okay so we're going to create a function
30:39 - and its name is restaurant
30:47 - for day
30:49 - it's gonna
30:50 - get a day of
30:53 - week
30:57 - we're going to use that value with our
31:00 - neural net so net dot run
31:03 - day
31:04 - of the week
31:12 - this is going to be our result
31:20 - and from that result we'll have this
31:22 - collection
31:23 - of restaurants with the likelihood that
31:26 - we should eat there and the highest
31:28 - likelihood will be the correct
31:30 - prediction for the given day
31:32 - so what we're going to do is start out
31:34 - with
31:35 - a
31:37 - highest value
31:41 - and as well the highest
31:51 - and we're going to iterate over our
31:54 - results so for let
31:58 - rest
32:04 - name
32:05 - in
32:06 - result
32:08 - and then we're going to say
32:11 - if
32:12 - the result
32:14 - restaurant
32:16 - name
32:19 - is higher
32:21 - than our highest value
32:25 - we're going to set our highest value
32:27 - to that value
32:31 - name
32:32 - or is we're going to save the highest
32:35 - restaurant name
32:40 - close restaurant
32:44 - and from here we'll just return
32:47 - the highest restaurant name
32:49 - and so from this we're going to accept a
32:51 - day of the week a string we're going to
32:53 - put that into the neural net the neural
32:55 - net's going to give us its predictions
32:57 - those predictions are a list of
32:59 - restaurants
33:00 - we're going to iterate over those
33:02 - restaurants
33:03 - and then we're going to
33:05 - save the highest one
33:07 - and we're going to return the highest
33:09 - one
33:11 - we'll go ahead and log the results out
33:13 - so this is restaurant for day
33:16 - we're going to say monday
33:18 - and we'll test it all out
33:22 - there it is brilliant yellow coral
33:24 - perfect
33:25 - now let's
33:26 - let's add the rest of the days of the
33:28 - week so monday tuesday wednesday
33:30 - thursday friday saturday sunday
33:35 - tuesday
33:37 - [Music]
33:40 - thursday
33:44 - friday
33:52 - there we go
33:56 - we've got
33:58 - all the restaurants for the given days
34:00 - of the week so now we've got string in
34:03 - and string out for our neural net
34:06 - next as a bonus
34:09 - try and flip this logic the other way
34:12 - so that you are inputting a restaurant
34:14 - name and you are getting out a day of
34:17 - the week
34:19 - i'll leave you to it
34:26 - in this tutorial we're going to learn
34:27 - how to count
34:28 - and although that sounds kind of like an
34:30 - easy task at first it actually is a
34:32 - little bit difficult but when we use the
34:35 - right tools it becomes easy
34:38 - take a look at my slides and this will
34:40 - give us a background on where to get
34:42 - started this is exclusive or our first
34:45 - problem that we solved
34:47 - each of the empty squares is a zero and
34:50 - each of the black squares is a one
34:54 - now let's take a look at a different
34:55 - input one that may be a little bit more
34:57 - tricky
34:59 - it's hard to kind of see what this
35:01 - particular input means so let's
35:03 - rearrange it so that it's easier for us
35:06 - humans we can see this is a 4. now both
35:09 - of these inputs illustrate something
35:11 - that may have occurred to you already
35:14 - and that is
35:15 - that they have
35:17 - width and height
35:19 - or
35:20 - length now width and height is just sort
35:22 - of another way of looking at length
35:25 - width and height don't really change
35:27 - neural nets
35:28 - they are
35:29 - constant
35:30 - but in computers there are some rules
35:33 - that can be bent and others that can be
35:36 - broken
35:37 - and we'll start illustrating that now by
35:39 - going to the movies on your trip to the
35:42 - movies you're going to bring your best
35:43 - friend along and they're of course
35:45 - thrilled at going to the movies with you
35:48 - because it's the latest and greatest
35:49 - movie that you've been looking forward
35:51 - to
35:52 - and everything is going fantastic and in
35:54 - fact it's the cliffhanger scene right
35:57 - there at the middle but all of a sudden
35:59 - the screen goes black
36:05 - why why did this happen this the screen
36:08 - went black it was right as i was
36:10 - expecting something to happen next
36:13 - why were they expecting something to
36:15 - happen next
36:17 - well it's because they they built up a
36:19 - memory of what was happening up to that
36:22 - point that's important think about that
36:24 - just for a moment we'll come back to it
36:27 - back to our non-playing movie your best
36:29 - friend looks at you and they are of
36:31 - course thrilled that they're at the
36:32 - movies but are saddened that the movie
36:33 - will not continue to play
36:35 - and then all of a sudden it starts to
36:36 - play again and they're of course as
36:38 - thrilled as can be and the movie ends
36:40 - just as you would hoped
36:42 - so that is our illustration of the
36:44 - movies
36:45 - now if you think back about what we
36:47 - paused in the movie
36:49 - every frame of the movie
36:52 - was the same
36:53 - height and width
36:55 - every frame
36:56 - no frame was different in size if you
36:58 - think about that over time though a
37:01 - frame being one part of a movie one
37:05 - frozen image of the movie
37:08 - each frame
37:10 - has that constant size but there are
37:13 - hardly any movies that are the same
37:14 - duration they all have
37:16 - different times that they play out
37:19 - that
37:21 - duration
37:22 - that's depth that's our frames plural
37:26 - that's mini frames that is really
37:28 - important with neural nets
37:31 - the depth
37:33 - the frames
37:35 - how long the movie is and what happens
37:37 - on each frame and what leads to the next
37:39 - one that gives us a context as to what
37:42 - is happening in the movie
37:45 - it's the same with neural nets
37:48 - this context and a neural net recurs
37:53 - it's something that happens over and
37:54 - over again something that has to in a
37:56 - sense repeat
37:58 - this terminology in neural nets is
38:00 - called recurrent now that sounds like a
38:03 - very complex word recurrent oh no what
38:05 - are we going to do next
38:07 - it's actually quite simple and to
38:08 - illustrate that
38:10 - let's
38:11 - go simple let's let's go to something
38:13 - that even a child can understand
38:16 - one
38:18 - now at its very simplest if i go to a
38:20 - child and i say one likely the child
38:23 - will not understand what i'm talking
38:25 - about unless we've trained on that in
38:27 - previous sessions
38:29 - so one to a
38:31 - to a child is is foreign and it's the
38:33 - same with a neural net i don't know what
38:35 - you mean by one that's essentially what
38:37 - the neural net will say however as soon
38:39 - as we start giving it context it being a
38:42 - neural net or it being a child
38:44 - they can start to decipher what we are
38:46 - trying to ask from them
38:49 - i'll continue
38:50 - we talk to our child we say one
38:54 - two
38:54 - [Music]
38:55 - three
38:57 - four and we
38:58 - pause what do you think the child will
39:01 - reply with
39:04 - likely a response would be
39:06 - five
39:09 - now this is
39:10 - where recurrent happens with neural nets
39:13 - recurrence is like taking each of these
39:15 - states one two three and four and sort
39:18 - of putting them together in a sense
39:21 - adding them together into a kind of a
39:23 - pool
39:24 - and that poll says you know the most
39:26 - likely thing they're looking for is
39:28 - probably and then out comes a five that
39:31 - is depth it's the same as our movie it's
39:34 - the same as being able to see each frame
39:37 - and that depth happens over time
39:40 - if the movie played out of sequence if
39:42 - the frames were shuffled in a sense
39:45 - likely would get very little out of the
39:46 - movie now this context is sort of a
39:49 - observer
39:50 - that that looks at each of these steps
39:53 - and can guess what comes next
39:57 - that context that depth that time that
40:01 - recurrence
40:02 - all refer to a very similar concept
40:07 - that depth
40:09 - that time that context it's all dynamic
40:12 - they're all the same width and height or
40:14 - even the same length
40:16 - but they're not of the same depth
40:18 - because we can feed in more than one
40:21 - and the context is dynamic two in the
40:23 - sense that we can say one two and ask
40:26 - for what's next and it'll say three or
40:28 - we can say three four what's next and it
40:31 - will give us a five or we can even
40:33 - reverse it and say five four three two
40:36 - what's next and it'll give us a one
40:39 - that's how dynamic that recurrent
40:42 - concept is in our neural net the ability
40:45 - to sort of take in those multiple frames
40:48 - that's called a recurrent neural net and
40:52 - in this simplest form the feeding of for
40:55 - example numbers is like stepping through
40:58 - time or a time
41:00 - step and as i said in this tutorial
41:02 - we're going to learn how to
41:04 - count okay now to get started let's go
41:07 - ahead and include the browser version
41:11 - of
41:12 - brain.js
41:14 - so we'll add that here
41:19 - our training data it's going to have two
41:20 - different outcomes one is going to count
41:22 - from one to five and the next one is
41:24 - going to count from five
41:26 - to one
41:28 - we'll define our training data manually
41:31 - that'll be a const
41:33 - called training
41:34 - data
41:36 - and it is going to be an array
41:38 - and in that array we'll have two arrays
41:43 - the first one will be
41:45 - one two three four and five and the next
41:48 - one will be five four three two
41:50 - and one
41:53 - that's it that's our entire training
41:55 - data
41:57 - next we'll define our neural net
41:59 - const net
42:01 - equals
42:02 - now this is a new namespace in brain new
42:06 - brain dot recurrent
42:09 - dot long short term memory or lstm
42:14 - time
42:15 - step
42:18 - now to train the neural net we'll give
42:20 - it our training data
42:22 - using the train method
42:24 - net dot train
42:27 - training data
42:30 - and let's see what actually comes out of
42:33 - the neural net while we're training it
42:36 - by logging
42:38 - we're going to give it a log function
42:48 - let's go ahead and see what happens
42:52 - all right it trained really fast
42:54 - very cool
42:55 - but let's
42:56 - remove the logging now that we know that
42:58 - it can train and let's see actually what
43:00 - comes out of the neural net
43:03 - so
43:03 - console.log
43:05 - net dot run
43:07 - and we're going to give it part of one
43:09 - of the arrays that we defined in the
43:11 - beginning so one two three four
43:13 - and that's it for the first one
43:16 - and we'll do the same for the second one
43:18 - right because we wanna as well count
43:20 - down from five net dot run
43:23 - and we'll do five four three and two
43:26 - all right let's see what comes out
43:30 - there we go awesome we got exactly what
43:32 - we wanted and that is how you count
43:35 - using an element in our first run we
43:37 - gave it an array of one two three and
43:41 - four expecting a five and that's what we
43:44 - got four point nine eight
43:46 - and in the second one we sent in a five
43:49 - four three two
43:51 - and we're expecting a one just like we
43:54 - have up here in our training data and we
43:56 - got a 1.005
43:59 - so that's really exactly what we wanted
44:01 - now as a bonus
44:02 - try adding another training array
44:05 - that counts from 10 to 5.
44:08 - or even from 5 to 10.
44:16 - in our last tutorial we used a long
44:19 - short term memory time step neural
44:22 - network to count that's a mouthful and
44:25 - in this tutorial we're going to use that
44:27 - same sort of net and we're going to work
44:30 - up to predicting stock market data so
44:32 - let's take a look at how our data is
44:34 - shaped first
44:35 - our raw data is going to be where all of
44:38 - our values live now this isn't yet
44:40 - training data we're going to turn it
44:42 - into that our values are going to be
44:44 - from an object and that object is going
44:46 - to have properties open high
44:49 - low and close each one of those are
44:52 - numbers now let's take a look at our raw
44:54 - data for a moment now if you look for
44:56 - example at the open property you'll see
44:58 - it's quite a bit larger than numbers
44:59 - that we've used previously which were
45:01 - from 0 to 1 or
45:03 - 0 to 10. the values repeat and you'll
45:06 - see that open high low and close follow
45:09 - a similar pattern what we want to do
45:11 - though because the neural net was
45:13 - instantiated with values between 0 and 1
45:16 - and that is sort of the language that
45:17 - the neural net speaks if we just send
45:19 - these values into the neural net it's
45:21 - going to take it quite a bit of time to
45:23 - sort of ramp up its understanding of
45:26 - these larger numbers if you can imagine
45:28 - it's like walking up to somebody who has
45:30 - only ever heard whispers and then just
45:33 - yelling at them it would be first off
45:35 - rude
45:36 - and secondly it would just be loud it
45:38 - wouldn't be
45:40 - what they were used to what we want to
45:41 - do is make it easy for the neural net to
45:43 - interpret this data and this is a very
45:45 - common practice so let's start out by
45:47 - writing a function and this function is
45:49 - going to normalize our data but since
45:51 - normalize is the most friendly term
45:53 - let's just call it scale
45:55 - down
45:58 - that's our function name and scale down
46:01 - is going to accept a single object and
46:04 - we're going to call that a step
46:07 - so that's a step in time now the same
46:09 - object that we have coming in we just
46:11 - want to turn down those values
46:13 - and so we're going to return an object
46:16 - and that object will have
46:19 - open and for the time being we'll go
46:21 - ahead and return step.open or define it
46:23 - rather with step.open
46:25 - and the same for
46:27 - et cetera
46:38 - and that is our scale down function
46:40 - however we're not normalizing anything
46:41 - yet
46:46 - and since if we go over to our training
46:49 - data once more and we look at these
46:51 - values one of the lowest ones that we
46:52 - can come to is this value 138
46:55 - or so so what we're going to do to get
46:57 - these values easily into the neural net
47:00 - is simply divide by that value so 138
47:09 - there and that's our normalize function
47:13 - quite simple and to prove that it works
47:15 - let's go ahead and console log it out
47:18 - console.log
47:20 - scale down
47:21 - we'll just
47:23 - grab the first item
47:25 - in our raw data
47:29 - there and we can see the values are
47:31 - fairly close to between one and zero now
47:33 - if you'll pause here just for a moment
47:35 - think about how we go the other way
47:37 - scaling up or as they say denormalizing
47:45 - okay so now what we're going to do
47:47 - before we do any machine learning we're
47:49 - actually going to write the inverse of
47:50 - this function which would be the
47:52 - denormalize or in practical terms the
47:55 - scale up function
47:57 - so
47:58 - function scale up and it's going to take
48:01 - a
48:02 - step and we're going to return that same
48:04 - sort of signature that same sort of data
48:09 - but rather than
48:10 - dividing
48:12 - by 138 the exact inverse of dividing is
48:18 - multiply
48:19 - so we will multiply by 138
48:23 - and now we have our scale up function
48:26 - referred to normally as d normalize
48:30 - so normalize brings it down denormalize
48:33 - brings it up
48:35 - and to test them both
48:37 - side by side
48:38 - we'll go ahead and console.log
48:41 - scale
48:42 - up
48:43 - it'll look kind of funny scale down
48:47 - in our we'll have our raw data and we'll
48:50 - send that in
48:51 - and
48:52 - what do we get
48:54 - there we go
48:55 - our scaled up and scale down
48:58 - functions
49:00 - we're going to use these in our next
49:02 - neural net but this is a very common
49:04 - practice across neural nets in general
49:06 - not just for current neural nets
49:08 - it is normalizing our values a more
49:11 - common approach to normalizing your data
49:14 - would be to subtract the lowest value
49:17 - from all the other values that you're
49:19 - sending into the neural net and then to
49:21 - divide
49:22 - by the highest value minus the lowest
49:25 - value that sounds kind of confusing at
49:27 - first if we were to take one of these
49:28 - lines for example
49:30 - uh this first one had scaled down and we
49:33 - used it right here and the net result of
49:36 - this
49:37 - is let's say the step dot open
49:40 - was 140.
49:42 - we're going to subtract uh if 138 was
49:46 - the lowest
49:49 - and then we'll divide let's say the the
49:50 - highest value is 147
49:53 - minus 138
49:56 - we would get
50:04 - and we'll subtract
50:06 - 138
50:08 - from 147 and that equals nine
50:14 - and so the net result is two divided by
50:16 - nine
50:17 - and that equals zero point two two two
50:19 - two two to two and what's important
50:21 - about this is it's the end value is
50:23 - between zero and one try and rewrite the
50:26 - scale down and scale up functions to
50:28 - incorporate
50:29 - the more generalized approach which
50:31 - would be to subtract the lowest value
50:34 - divided by the highest minus the lowest
50:36 - value
50:41 - in our last tutorial we talked about
50:43 - normalizing data and in this tutorial
50:46 - we're going to write the neural net and
50:47 - we're going to put that normalized data
50:48 - into it
50:49 - so first let's scale all of our raw data
50:52 - we're going to call this scaled
50:55 - data it's not yet training data but it's
50:57 - close
50:58 - we're going to say raw data
51:01 - dot map and we're going to map over all
51:04 - those values using our scale down
51:06 - function
51:08 - and so now our scaled data will have
51:11 - all of those new values that are
51:13 - normalized next what we want to do is
51:16 - rather than feed in one long array of
51:19 - all these objects these properties so
51:21 - that the neural net memorizes this one
51:24 - long pattern we want the neural net to
51:26 - understand smaller patterns and to
51:29 - predict off of those and so how we're
51:32 - going to do that is we're going to
51:34 - create finally our training data
51:38 - and our training data is going to be an
51:40 - array of arrays
51:42 - we're going to take our scaled data
51:45 - and we're going to slice it into chunks
51:48 - of five
51:50 - starting at the first
51:52 - index
51:54 - and we're going to
51:57 - progress
51:59 - by
52:00 - five indexes each time
52:06 - and so that is our training data and we
52:09 - can console log it out
52:13 - make sure it looks right
52:16 - very nice
52:17 - okay so it's an array of arrays
52:21 - that's an important concept
52:23 - okay so now that we have our data
52:26 - chunked and normalized and ready to go
52:29 - into the neural net we're going to write
52:30 - our neural net so const net equals new
52:34 - brain dot recurrent that long short term
52:38 - memory time step
52:41 - that's our neural net and we're going to
52:43 - define it with a few options now this is
52:46 - important our scaled down
52:49 - data
52:50 - has four different properties here open
52:52 - high low and close
52:54 - that represents one point in time or one
52:57 - step through time so our neural net is
53:00 - going to have an input size of those
53:03 - properties
53:04 - being them four properties our input
53:07 - size will be
53:09 - four
53:10 - and it's very rare to deviate from that
53:12 - with output size
53:14 - so we'll go ahead and put an output size
53:15 - of 4 as well
53:18 - and so now we want to define our hidden
53:20 - layers
53:23 - and our hidden layers are simply going
53:25 - to be
53:26 - eight neurons and eight neurons so input
53:29 - size of four hidden layers of eight and
53:32 - eight and an output size of four
53:35 - and now we can learn our stock market
53:38 - data net dot
53:40 - train training data
53:44 - out
53:47 - now we're going to tweak our training
53:49 - options just a little bit here
53:52 - we're going to put our learning rate
53:56 - at
53:57 - 0.005 and the reason we're going to do
53:59 - that is so it doesn't sort of shoot past
54:02 - the values that we're looking for we
54:04 - want very small increments uh toward our
54:07 - goal
54:08 - and the next is our error threshold
54:11 - now the longer and the more data that
54:13 - you end up using with your neural net
54:16 - potentially longer it's going to take to
54:18 - train
54:18 - and so for this being just in the web
54:21 - browser and we want to train to a
54:23 - sufficiently good error i'm going to
54:24 - turn it down to 0.02
54:28 - and as well let's go ahead and log out
54:30 - our values
54:39 - let's clean this up
54:41 - all right and now let's learn let's see
54:43 - what comes out very cool
54:46 - all right so it trained let's log out
54:48 - what really matters so that's
54:50 - console.log and that'll be net.run
54:54 - and let's just give it uh the first
54:56 - item in our training data
54:58 - so our training data
55:01 - and let's see what comes out
55:03 - very cool so our net actually learns
55:06 - something and returns something now
55:07 - there's a problem here if we look at our
55:10 - original values
55:12 - they're all
55:14 - in the mid
55:15 - 140s or so
55:18 - but our values here are not
55:20 - they're close to zero and one so what do
55:23 - we want to do there well i'll give you a
55:24 - moment to think about it
55:29 - okay this is where we finally use
55:32 - scale up this is where we denormalize
55:35 - our values here we go i went ahead and
55:38 - added that as a wrapper around
55:41 - net run and we'll run it again and there
55:44 - we go the values that look very familiar
55:46 - our 140s or so one thing that would be
55:49 - really useful is to not just look one
55:51 - step into the future which is what we're
55:53 - doing right here that's what net.run
55:55 - would be it's going to take all of our
55:57 - existing steps and say hey this is the
55:59 - potential next step what we want to do
56:01 - is
56:02 - actually look and see
56:04 - what the next two or three steps may be
56:12 - in our last tutorial we predicted the
56:14 - next step for stock market data and in
56:18 - this tutorial we're going to predict the
56:20 - next three steps that's really
56:22 - interesting and cool and it's actually
56:23 - quite simple
56:24 - so we're going to
56:26 - actually just change one little method
56:29 - so rather than use net.run
56:32 - i'm going to comment this out
56:33 - and i'm going to say console.log
56:36 - we're going to do net dot forecast and
56:40 - let's say we just have
56:43 - a couple numbers to go on we're going to
56:46 - send in our training data
56:49 - but we're going to only send in
56:52 - a couple of steps of that data
56:57 - and then
56:59 - we're going to say with forecast that
57:01 - we'd like the next three steps
57:05 - and it doesn't stop there it's actually
57:06 - going to return an array and before we
57:09 - had wrapped
57:11 - run in scale up whereas here in
57:14 - net.forecast because we're getting an
57:16 - array rather than an object which would
57:18 - be the next step the array would be the
57:20 - next steps
57:21 - and so we can take this and map to scale
57:24 - up
57:25 - all right here we go
57:27 - and there's our data
57:29 - that's what the net has learned and it's
57:31 - its next prediction so we've got
57:34 - the beginning of one
57:36 - there's our our second
57:38 - and three i think the console doesn't
57:40 - log up very well with the cursor but the
57:43 - idea is there and so is the data
57:50 - in this tutorial we are going to take a
57:53 - recurrent neural network and we're going
57:55 - to learn math and we're going to do it
57:57 - character by character
57:59 - and at first that sounds kind of
58:02 - weird
58:03 - but you'll notice something interesting
58:05 - at first with our training data here
58:07 - there are no numbers not directly these
58:10 - are strings even though there are
58:11 - numbers inside of them and we are going
58:13 - to feed these directly into the neural
58:15 - net
58:16 - this is where it gets interesting
58:17 - especially with recurrent neural nets so
58:19 - the way that a recurrent neural net
58:21 - works
58:22 - is it has an input map
58:25 - and that map is kind of like an array
58:28 - and that array
58:31 - maps a
58:32 - value that is coming into the neural net
58:34 - to a neuron
58:36 - and it does so by index
58:38 - and this is how it works
58:40 - by first sitting in our training data
58:42 - the training data is analyzed and we
58:46 - pull out unique values like zero
58:50 - the next value being plus
58:52 - and the next value being equals
58:55 - because 0 is repeating up here and we've
58:58 - already established it
58:59 - equals would be the next one 0
59:02 - is repeating again and zero and plus
59:05 - repeats again and the next unique value
59:07 - we have is one in the next
59:10 - item in the array
59:12 - and that happens uh throughout our
59:13 - entire training set so one two three
59:17 - four
59:19 - five
59:20 - six
59:21 - seven
59:23 - eight
59:24 - and nine
59:25 - and so we we build sort of an input map
59:28 - and the reason why that's important is
59:31 - the input map
59:33 - lines up with neurons so our input map
59:35 - is more or less the same size
59:42 - as our input size
59:45 - and that's calculated internally inside
59:47 - of the neural net so that's uh it's
59:50 - important but we don't have to think
59:51 - about it so much
59:53 - so input map
59:55 - is the same size as input size and what
59:58 - that means is for each one of these
60:00 - characters they get their own neuron
60:04 - and so if we step through a problem for
60:06 - example like zero plus zero equals zero
60:10 - to the net we feed in a brand new array
60:14 - each time with that one value activated
60:18 - and so our input
60:21 - internally
60:26 - may look like this so we've got a
60:30 - a bunch of zeros
60:34 - and so
60:35 - the the inputs uh literally look like
60:41 - okay so input length equals input size
60:44 - so each one of these values
60:47 - is sort of attached to an input neuron
60:51 - and this is why that matters
60:54 - each time we feed in one of these
60:56 - characters it goes all the way through
60:58 - the neural net and then we can do that
61:00 - over and over and over again
61:02 - so
61:02 - the math problem zero plus zero equals
61:06 - one literally looks like this to the
61:08 - neural net it looks like
61:11 - zero
61:15 - plus
61:17 - zero
61:20 - and that's the third one
61:22 - equals
61:24 - zero
61:26 - so that is our first math problem sort
61:28 - of internally to the neural net but it
61:30 - it does everything
61:32 - sort of automatically for us
61:34 - and this is a good kind of magic it's
61:36 - not the bad kind it's the kind that is
61:38 - predictable so we'll go ahead and
61:39 - comment that out
61:40 - and this out too all right so we're
61:42 - ready to go ahead and predict our
61:44 - numbers so this is what happens next
61:45 - we're going to define our net const net
61:47 - equals new brain dot recurrent dot along
61:52 - short term memory
61:56 - and we're going to have hidden layers
61:59 - of
62:01 - 20
62:04 - and we're going to net net.train on our
62:06 - training data
62:10 - and we are going to set our
62:13 - error threshold
62:15 - to 0.025
62:20 - five
62:22 - and we're gonna go ahead and log out our
62:24 - value
62:26 - all right
62:27 - so we'll go ahead and learn it and we'll
62:28 - see what happens very cool now a word of
62:31 - caution here it did learn it but if you
62:34 - crank down your error threshold
62:37 - that may be able to learn it but in the
62:39 - browser it can take a long time and a
62:41 - long short term memory neural net is
62:43 - quite complex in contrast with our
62:45 - previous neural nets there's a lot more
62:47 - going on in the long short term memory
62:49 - than the others
62:51 - so our net was able to learn it but of
62:53 - course that's not the fun part the fun
62:55 - part is actually getting to see what
62:56 - comes out of the net so let's do that
62:59 - net dot run and we're just going to give
63:01 - it some straight up old strings
63:04 - 0 plus 1
63:06 - equals and we're going to see what comes
63:08 - out there uh but let's not stop there
63:10 - let's just think of a few
63:12 - let's see uh
63:14 - 4.1
63:16 - uh and depending on the error rate too
63:18 - some of these values may be a little bit
63:20 - incorrect
63:21 - uh but they should be pretty close
63:23 - it's uh quite entertaining what a neural
63:25 - net can
63:28 - can actually give us okay so we're going
63:29 - to run these three values and see what
63:32 - happens
63:33 - all right zero plus one
63:35 - is one four plus one is five and two
63:39 - plus one is three
63:42 - we nailed it awesome so that is
63:45 - recurrent neural networks learning math
63:47 - now if you want to do some
63:50 - bonus material
63:52 - light
63:53 - it up with some math problems
64:03 - in this tutorial we're going to read
64:04 - numbers like you would from an image and
64:07 - for visual simplicity our raw data is
64:09 - going to be of strings like you see here
64:12 - spaces are what we don't want the neural
64:14 - net to see and this asterisk is what we
64:17 - do want the neural net to see now we're
64:19 - going to use this simple string and a
64:21 - simple normalization function to convert
64:23 - these inputs right here two zeros and
64:25 - ones so let's go ahead and write that
64:27 - first
64:28 - so we're gonna create a function
64:31 - and we're gonna call it two number and
64:34 - it's gonna take a character as an input
64:36 - what we're gonna return from this
64:38 - function is if the character
64:40 - equals
64:42 - an asterisk
64:43 - we're going to return a 1.
64:45 - otherwise we're going to return a 0.
64:47 - that's our character by character
64:49 - normalization function now we also need
64:51 - to create a function that normalizes for
64:54 - the entire input like the entire array
64:57 - that function's going to be called to
64:58 - array and it's going to bring in a
65:00 - string now the string needs to be a
65:03 - specific size
65:04 - and if that string dot length isn't
65:09 - seven by seven
65:10 - and we're going to throw
65:12 - a new error string in wrong
65:15 - size
65:16 - otherwise we're going to take the string
65:19 - and we're going to split it
65:21 - and then we're going to map it
65:23 - to number
65:25 - and that'll be our return value that is
65:27 - everything that we need to normalize our
65:29 - input value cool so next what we want to
65:32 - do is i'm going to paste in some number
65:35 - data but you'll notice the number of
65:37 - data isn't just zero to nine it's
65:38 - actually a string and you can see that
65:40 - these functions start to make a little
65:42 - bit more sense now two array is going to
65:44 - take this string in
65:46 - and it's gonna split up this long set of
65:50 - characters and it's going to convert
65:52 - them to ones and zeros so that the net
65:55 - can sort of understand what is is being
65:58 - said here now we have zero through nine
66:01 - and each of these you can see
66:03 - looks exactly like the character it's
66:04 - representing let's test at least one of
66:06 - these values so console.log
66:08 - we're going to put nine
66:11 - very good and we can see that represents
66:14 - what the neural net is going to speak so
66:16 - let's build our neural net so const net
66:19 - equals
66:20 - new brain dot
66:22 - neural network
66:25 - we don't have to mess with size in this
66:26 - because it's a feed forward neural net
66:28 - so let's build our training data const
66:30 - training data equals that's going to be
66:32 - an array of objects that have an input
66:36 - and each of those inputs are going to be
66:37 - one of our values above so the first one
66:39 - let's just say will be zero and the
66:41 - output is going to be an object
66:45 - and that object is going to be of the
66:48 - property that we're classifying towards
66:50 - with a value of one very good so let's
66:52 - go ahead and add the additional
66:54 - numbers here you see we've got one two
66:57 - three all the way up to nine
66:59 - and so now all we need to do is train
67:01 - net.train
67:03 - training data
67:04 - and we're going to log our results out
67:07 - all right let's see what happens very
67:08 - cool 190 iterations
67:11 - fairly low error rate very cool but we
67:14 - still haven't seen anything interesting
67:16 - yet
67:17 - let's go ahead and actually see what the
67:19 - net is outputting so net dot run
67:22 - to array
67:23 - and we're going to give this
67:25 - the number eight and now let's see what
67:28 - the neural net outputs we'll say const
67:30 - result equals
67:32 - and we'll turn off our logging up here
67:34 - because we know the neuron that's
67:35 - training just fine all right here it
67:36 - goes
67:37 - very cool now what you'll notice here is
67:39 - that the output is all of the properties
67:42 - that we used in the training data
67:44 - they're compiled together and the one
67:46 - that is most likely has the highest
67:48 - value so here 8 is a likelihood of 0.7
67:53 - and that's really nice we don't have it
67:54 - though in a simplified format there is
67:56 - however a utility built into brain
67:59 - that is called
68:01 - likely that we can use here const or
68:03 - result equals brain dot
68:07 - likely and we're going to use our two
68:09 - array and we're going to send our net
68:12 - to it
68:13 - and here we're going to take
68:17 - this two array we're going to give it a
68:19 - string
68:20 - very good so that's our 8 and let's
68:23 - console log out the results all right
68:25 - here it goes
68:26 - very cool we got an 8 out of our neural
68:28 - net all right so that was a really
68:30 - simple example and it took a lot to work
68:32 - up to it because we've got a little bit
68:34 - of training data there some conversion
68:35 - etc but play around with this and see if
68:37 - you can get some different characters to
68:39 - be recognized by the neural net now a
68:40 - word of caution here this method will
68:43 - likely be a bit fragile in the real
68:44 - world without something like
68:46 - convolutions and those are coming to
68:47 - brain.js in version two but the thing to
68:50 - note here is that the principles still
68:51 - applies now there is a question to be
68:54 - begged here how resilient is a neural
68:56 - net why wouldn't we just use a bunch of
68:59 - if statements and a bunch of loops to
69:01 - build this example of an eight if i were
69:03 - to remove for example something that the
69:06 - neural net sees like one of these
69:07 - asterisks
69:09 - would the neural net still recognize
69:11 - this as an 8
69:12 - or if i did the same with a 2 or a 6 etc
69:16 - well let's find out
69:18 - see it still found the eight this
69:20 - demonstrates one of the coolest parts
69:21 - about neural nets and that they are
69:22 - dynamic and they're resilient and that's
69:24 - why they're so popular and so amazing
69:26 - and for your bonus
69:28 - write
69:29 - three
69:30 - examples
69:33 - that still match like this eight
69:36 - but are not like your training data
69:39 - and that'll demonstrate the resilience
69:41 - of the neural net
69:43 - have fun
69:48 - in this tutorial we're going to write a
69:50 - children's book using our current neural
69:52 - network
69:53 - and we'll start with our training data
69:55 - here and if you'll go ahead and think
69:57 - about pausing it here what type of
70:00 - neural net that would use based off our
70:02 - previous tutorials
70:06 - okay
70:06 - likely you arrived at
70:09 - a long short term memory recurrent
70:11 - neural network we're going to go ahead
70:13 - and train on our data
70:15 - and we're going to
70:16 - give it a few options our iterations
70:19 - we're going to stop at 1500 our error
70:22 - threshold
70:24 - we're going to stop at 0.011
70:26 - and we're going to log our details
70:28 - all right let's see if it rains
70:30 - very cool and now let's get it to output
70:33 - something interesting our first line
70:35 - from the training data
70:38 - net dot run
70:40 - is
70:41 - jane we're going to stop there with it
70:44 - and jumping to the more complex line
70:46 - i'm going to say net dot run
70:49 - it
70:50 - was just based off of these two lines
70:52 - let's see what the network outputs
70:54 - get rid of the log
70:56 - very cool so it starts out jane
70:59 - saw doug
71:01 - and jane looking at each other
71:02 - oh wow
71:04 - and then
71:05 - net.run it was love at first sight and
71:08 - spot had a front row seat it was a very
71:10 - special moment for all you see the net
71:13 - already altered a little bit of what was
71:14 - going on
71:16 - what you'll find is that depending on
71:18 - how you set up your neural net it can
71:19 - give you some really interesting outputs
71:21 - and the more data that you give it the
71:23 - more interesting it's going to get so
71:24 - your bonus here is to experiment
71:27 - with book writing
71:35 - in this tutorial we are going to learn
71:36 - how to detect sentiment from text
71:39 - directly and you'll notice our training
71:41 - data is a bit different than prior here
71:43 - we have an array
71:44 - of objects
71:46 - whose properties are
71:48 - strings
71:50 - rather than
71:51 - arrays of numbers and brain js is set up
71:54 - to go ahead and accept this type of
71:56 - input it's not a problem so let's go
71:57 - ahead and get started we'll build our
71:59 - neural net const net equals
72:02 - new brain dot recurrent dot long short
72:05 - term memory neural net and we'll go
72:08 - ahead and train net.train
72:11 - on training data
72:12 - now we're going to alter our training
72:15 - options
72:16 - we're going to stop at 100 iterations
72:18 - our error threshold
72:21 - is going to be 0.011
72:23 - and we're going to log
72:25 - our stats
72:27 - we're going to cut the chase and go
72:28 - ahead and output something interesting
72:30 - we are going to use an input that's not
72:33 - from our list above we're just going to
72:35 - use something that's similar to it so
72:37 - net dot run i am
72:39 - unhappy
72:41 - go ahead and log that out let's see what
72:43 - we get
72:44 - very cool
72:45 - we got sad
72:47 - let's remove our
72:48 - log all right now let's add an
72:50 - additional item
72:52 - net dot run
72:54 - i am
72:55 - happy
72:56 - very cool now i want to point out that
72:58 - we're not training this for very long
73:00 - and that the error threshold if you were
73:02 - to bring that a bit down and as well
73:04 - iterations it's going to be quite a bit
73:05 - more stable but what you may have not
73:07 - noticed is that we did not send into the
73:09 - neural net
73:11 - of values that we trained it with and
73:13 - yet it was still able to classify them
73:15 - as happy and sad so again we're kind of
73:17 - highlighting the dynamic ability of the
73:20 - neural net
73:21 - and for your bonus
73:25 - tweak the iterations and threshold be
73:28 - careful because in the browser you can
73:30 - cause it to train indefinitely or for a
73:32 - very long period of time
73:34 - but add five new examples in the
73:38 - training
73:39 - data
73:40 - that would be up here
73:43 - and then
73:45 - log out of five examples that aren't in
73:49 - the training data all right have fun
73:51 - yelling at the computer
73:57 - sentiment detection recurrent neural
73:59 - network
74:00 - and in this tutorial we're going to
74:01 - explain kind of how we did it
74:04 - now if you'll recall
74:05 - we use both inputs and outputs for the
74:08 - sentiment detection recurrent neural
74:10 - network
74:11 - that is kind of a new concept when
74:13 - talking about recurrent neural networks
74:15 - because we explain them kind of like
74:17 - going to the movies now there are no
74:19 - inputs and outputs with movies we're
74:21 - just an observer sort of watching what
74:23 - happens next however we can bend those
74:26 - rules in order to fit the paradigm of an
74:29 - input and an output and i'll explain how
74:32 - now previously we mentioned that a
74:33 - recurrent neural network builds an input
74:35 - map
74:36 - and that is kind of like an array and
74:38 - each value in that input map corresponds
74:41 - to characters
74:43 - that are used in the training data but
74:45 - those rules apply here as well nothing
74:47 - has changed the input character one
74:51 - gets its own special neuron
74:54 - and its own special map its own special
74:57 - lookup and as well the same is with the
75:00 - output of character two however our
75:03 - input map is not complete what is
75:05 - different here with the training data is
75:07 - that there is an unseen character a
75:09 - character that tells the neural net to
75:11 - transition between an input and an
75:14 - output
75:16 - we'll call that character the new
75:19 - idea character
75:21 - all right
75:22 - and so if we look at our training data
75:26 - we have an input one
75:28 - and then we have our new idea
75:32 - and then we have our output of two
75:36 - and so if we were to look at a set of
75:38 - arrays that consist of what is being fed
75:42 - into the neural net they look much like
75:43 - this so our input map we have three
75:46 - indexes three neurons okay
75:49 - and the first one is going to have a
75:51 - value of one
75:53 - that first value is activated that input
75:56 - character one
75:58 - that is activated here and our value
75:59 - that's being sent through the neural net
76:01 - next we transition to the new idea
76:03 - character and so there new idea gets a
76:07 - one the next character
76:10 - is followed by two
76:12 - so there we have zero
76:15 - zero
76:16 - one
76:17 - so each one of these items in the array
76:20 - correspond to a neuron that correspond
76:23 - to our input map that correspond to our
76:25 - training data
76:26 - it sounds kind of confusing at first
76:28 - but what you'll get to notice with
76:30 - recurrent neural networks and neural
76:32 - networks in general is that they're
76:34 - actually quite simple and even the math
76:36 - that represents them is quite simple
76:38 - there's just a lot of it by taking
76:40 - advantage of those rules and that math
76:42 - we can bend those rules to our will
76:45 - now for our bonus here
76:47 - what would the training data
76:50 - look like if we start with 2
76:54 - and end with
76:56 - 1. so 2 would be our input
76:59 - and our output
77:00 - would be
77:01 - one
77:02 - and then as well
77:04 - what would this set of arrays look like
77:07 - i'll go ahead and pause it here go ahead
77:09 - and write your answer and we'll compare
77:11 - notes
77:17 - all right and so now if we're to compare
77:20 - our answers
77:22 - they actually should look flipped and so
77:25 - our
77:26 - training data that has an input of
77:30 - 2 and an output
77:33 - of 1
77:36 - will internally to brain.js look like
77:38 - this it'll start with a 0
77:41 - 0
77:42 - 1
77:43 - zero one
77:44 - zero
77:46 - and then and finally with a one zero
77:48 - zero now this is all internal to brain
77:51 - js this isn't stuff that you have to
77:53 - learn it's just understanding the
77:54 - principles behind the neural net because
77:57 - again a neural net is simple there's
77:59 - just a lot of it
78:04 - in this tutorial we are going to use
78:06 - reinforcement learning this is a really
78:08 - exciting frontier of machine learning
78:10 - and we're going to use it right here
78:12 - you'll notice our training data is
78:13 - exclusive or right where we started
78:15 - originally and half of our training data
78:18 - has been commented out this is important
78:20 - what we're going to do is learn these
78:22 - first two items in our training data
78:24 - we're going to test against these first
78:26 - two items we're going to look at this
78:28 - next item and see that the net really
78:30 - doesn't understand it and then we're
78:31 - going to learn it so let's start by
78:33 - creating our neural net constnet equals
78:36 - new brain dot
78:39 - neural
78:40 - network
78:42 - and we had hidden layers
78:45 - of three
78:47 - and we'll do net.train
78:50 - that's training data we're gonna log out
78:52 - our results and let's see what we get
78:54 - very cool so we've learned these very
78:56 - first two items let's go ahead and
78:58 - remove our logging
78:59 - and then let's take a look at what the
79:01 - net actually outputs so net
79:03 - dot run and we're going to give it an
79:05 - input 0.0
79:08 - we're going to console log that and i'm
79:10 - going to do array dot from
79:17 - all right let's see what we get
79:21 - very cool and that's exactly what we
79:23 - were expecting however if we add one of
79:25 - the examples
79:28 - of like
79:29 - one zero
79:32 - let's see what we get
79:34 - all right very cool and that's what we
79:35 - were expecting the net has not been yet
79:37 - trained on this data and so this
79:38 - illustrates why reinforcement learning
79:40 - is so important and necessary our
79:42 - training data may not be available all
79:44 - at once it may come over time and so we
79:47 - want to make sure that our neural net
79:48 - can adjust for that so let's go ahead
79:50 - and adjust our training data adding one
79:52 - brand new example the one zero
79:55 - outputting of one and you'll get to see
79:57 - how we use reinforcement learning in
79:58 - this feed forward neural net alright so
80:01 - training data dot
80:04 - push
80:05 - and we're going to give it an object
80:07 - that's an input
80:10 - array
80:11 - and 1 0 is our input and then output is
80:14 - going to be of one array
80:17 - all right and then really all we have to
80:19 - do is take net.train
80:21 - put it down here and we can give it some
80:23 - titles to be fancy console.log
80:25 - before reinforcement
80:30 - well we'll just copy all of this
80:32 - and then here we'll do
80:35 - we'll say after
80:37 - reinforcement all right let's see what
80:39 - happens
80:42 - all right before
80:43 - we have two very similar values
80:46 - and after illustrates that our net was
80:49 - able to reinforce
80:51 - now as a bonus
80:56 - we go ahead and add the missing item
80:58 - from our training data
81:06 - all right have fun
81:11 - in this tutorial we're going to build a
81:12 - recommendation engine we're going to do
81:13 - so using the simplest sort of
81:15 - methodologies this is one of my favorite
81:17 - examples because it seems so hard when i
81:20 - first started in neural nets but in fact
81:21 - it's actually quite simple we're going
81:23 - to start with our training data which is
81:25 - color preference
81:27 - and imagine that this list was built you
81:29 - just gave somebody
81:31 - these list of colors blue red black
81:34 - green brown
81:35 - and they basically wrote a number by
81:38 - them how much they liked that color so
81:41 - they like blue a lot they like red a lot
81:43 - however they don't really like black
81:46 - green or brown and that's how we end up
81:47 - with these numbers on both the inputs
81:49 - and outputs so that's our training data
81:51 - and we can learn that really quick using
81:53 - a feed forward neural net so const net
81:56 - equals
81:57 - new brain dot
82:00 - neural network
82:04 - net dot train
82:06 - and we're going to have our training
82:07 - data
82:08 - we'll go ahead and log out
82:10 - our stats
82:11 - see what happens
82:13 - cool our net was able to train really
82:15 - quick on that data let's turn off
82:16 - logging and see what we get
82:19 - console.log
82:20 - array.from
82:26 - net dot run
82:30 - and we'll say blue
82:33 - one
82:34 - now we're going to duplicate this line
82:37 - we're going to put brown
82:40 - all right here we go
82:43 - all right so the user has a very strong
82:45 - preference towards blue and not so much
82:48 - towards brown we can see that here from
82:50 - our
82:51 - output so here's the scenario over time
82:53 - you go back to that original user you
82:55 - show them this list and based off of
82:57 - their experience things they like things
83:00 - they dislike they now have changed what
83:02 - they prefer and now brown is one of
83:05 - their favorite colors so how do we get
83:07 - that into a neural net why don't you go
83:08 - ahead and think about that just for a
83:10 - moment and pause it here
83:16 - okay for us to get brown into that
83:18 - training data we have to put it there so
83:20 - training data dot and we're going to say
83:22 - push
83:24 - let's say brown
83:26 - one
83:27 - and output
83:29 - we're gonna put as a one
83:33 - all right and so now we have our
83:34 - training data in there so now what do we
83:36 - need to do to get the net to understand
83:39 - what this new training data means i'll
83:41 - pause it here and let you figure it
83:46 - out okay
83:48 - next what you want to add is net.train
83:52 - and we're going to give it training data
83:55 - and we're going to log out the results
83:56 - again
83:58 - stats
84:01 - and console.log
84:03 - stats
84:04 - all right
84:05 - and now let's see what happens and
84:07 - actually let's go ahead and console log
84:10 - out our values but after we train we'll
84:13 - give us a fancy title as well so before
84:17 - and you'll notice that we are actually
84:20 - using reinforcement learning a bit here
84:24 - just very simple it's not very
84:25 - complicated at all so before preference
84:27 - change after preference change
84:30 - let's see what happens
84:32 - very cool
84:33 - so we scroll through our training
84:35 - actually let's go ahead and remove the
84:37 - logging there we know that it logged
84:40 - now let's see what happens
84:42 - very cool so now before our preference
84:43 - changed you can see that brown was rated
84:46 - fairly low
84:47 - but after our preference change it was
84:49 - rated at a mediocre point five now why
84:52 - could that be we'll pause it here and
84:53 - let you think about it
84:59 - okay the reason is this
85:01 - our training data
85:04 - still has reference to when their
85:08 - preference
85:09 - was
85:10 - towards zero rather than towards one we
85:12 - want to ensure that we only have their
85:14 - current preference so
85:16 - training data dot pop
85:19 - that's going to remove the last item
85:20 - from our array
85:22 - so we'll pretend this is some sort of
85:23 - user
85:24 - interaction or server interaction
85:26 - something that happened to adjust their
85:29 - preferences that gave us brand new
85:31 - training data and now we're going to
85:33 - train on it again and then after that
85:35 - preference change we're going to have
85:36 - our brand new updated values here we go
85:39 - all right
85:40 - before our preference change brown was
85:42 - rated fairly low at 0.05 and after it
85:45 - came out of a 0.89 so this illustrates
85:47 - both reinforcement learning and also how
85:49 - to build a very simple recommendation
85:51 - engine now your bonus here
85:56 - build your own recommendation engine
85:59 - what's your favorite food where do you
86:00 - like to go does that change over time
86:02 - has it changed since you were a little
86:04 - let the net know
86:05 - have fun
86:10 - awesome job you got all the way through
86:12 - the course i am super proud and i'm very
86:14 - excited that you were able to get
86:15 - through listening to me but also
86:17 - building and applying and seeing these
86:20 - new concepts and how they can actually
86:21 - be quite simple but as we're progressing
86:23 - i'd like to give you some takeaways for
86:25 - this course to be able to remember and
86:28 - to share with others and that is this
86:30 - machine learning is moving really fast
86:32 - and that is super exciting that means
86:34 - it's changing and it's being adopted
86:36 - we're solving problems that we've never
86:38 - been able to solve before and you have a
86:40 - voice in that machine learning can
86:41 - change directions and a lot of that has
86:43 - to do with our understanding of how to
86:45 - use it we're also arriving at simple
86:47 - solutions are are so much more dynamic
86:50 - and powerful than complex ones when
86:52 - applying it so if there is a practice
86:54 - that you want to see more of or if there
86:56 - is something you like to change about
86:58 - machine learning try and introduce it
87:00 - study it prove it test it to see if it
87:02 - is worthwhile and not only will you
87:04 - benefit machine learning for you you're
87:06 - going to help all of us to have those
87:08 - new tools and capabilities so machine
87:10 - learning is moving fast and you have a
87:13 - voice and where it goes from this course
87:15 - to i wanted really to highlight that
87:18 - machine learning is actually quite
87:19 - simple the terminologies behind it we
87:21 - can get very lost in them but when we
87:23 - arrive at the logical foundation of
87:26 - machine learning it is simple there's
87:27 - just a lot of it a lot of numbers a lot
87:29 - of logic and i want to highlight where
87:30 - you go next with this simplicity with
87:33 - three illustrations
87:36 - and they are equals mc squared and ac
87:39 - motor and washing your hands these may
87:41 - seem comical at first but follow me the
87:44 - first is equals mc squared albert
87:46 - einstein discovered equals mc squared
87:47 - because he reasoned on that electrons
87:49 - moving around an atom would be affected
87:51 - by gravity kind of like when you throw a
87:53 - ball into the wind like a beach ball or
87:55 - something like that a child can tell you
87:57 - if you throw a beach ball into the wind
87:58 - that is going to be affected by it and
88:00 - that's how he reasoned on the matter so
88:02 - the point is that albert einstein used
88:05 - intuition
88:06 - use your intuition when you're solving
88:09 - problems with machine learning and solve
88:11 - for simplicity like e equals m c squared
88:14 - the next illustration was that of an ac
88:16 - motor now a lot of people know that
88:18 - nikola tesla helped to build and refine
88:21 - the ac motor but if you go back further
88:23 - in his career you'll arrive at that when
88:26 - he was in school he was shown this
88:28 - dynamo type dc motor that had these
88:31 - brushes and that arced it sparked
88:34 - everywhere and was really crude and
88:35 - noisy and he asked his teacher why it
88:38 - needed to have
88:39 - the brushes why it needed to spark
88:41 - everywhere and his teacher was insulted
88:43 - by him asking the question and made him
88:45 - write an essay and it's a really
88:47 - interesting story but the point is this
88:48 - he asked why it was even there and from
88:51 - then on he started to model his career
88:53 - to build a motor that didn't have those
88:55 - brushes and he arrived at the ac
88:57 - induction motor and refined it and he
88:59 - eventually used that same idea to help
89:02 - power both chicago and new york
89:05 - simultaneously from niagara falls it was
89:07 - a simple question a simple idea that got
89:10 - him started nikola tesla used intuition
89:13 - something wasn't right about using all
89:15 - that complexity solve for simplicity use
89:18 - your intuition this last illustration is
89:20 - one of my favorites and it's a washing
89:22 - your hands and the reason is because
89:23 - it's such a simple practice most people
89:25 - know washing your hands it's common
89:27 - knowledge it helps you to be clean but
89:29 - prior to the mid-1800s it was not in
89:31 - fact in hospitals doctors would do all
89:33 - kinds of weird things like perform
89:35 - surgeries and then go help
89:37 - with births and they were wondering why
89:39 - their patients are dying left and right
89:41 - but it turns out that egonos symbolwise
89:43 - theorized not only that we were being
89:45 - infected by something but that we should
89:47 - wash our hands to prevent it and at
89:48 - first it was rejected the idea was
89:51 - absurd why why would you ever want to
89:52 - wash your hands which to us sounds
89:54 - absurd but eventually it was accepted
89:56 - the idea though is that ignosingwise did
89:58 - not have access to microscopes and it
90:00 - was not yet discovered what a germ was
90:02 - ignos symbolwise used intuition and it
90:05 - was a very simple problem and he used a
90:08 - very simple solution
90:10 - use your intuition find those simple
90:14 - solutions machine learning is being
90:16 - applied everywhere and that is super
90:18 - exciting because there is not an
90:20 - industry that is not directly or
90:22 - indirectly affected by it we are solving
90:24 - problems that we have never been able to
90:26 - as a race that is awesome but when
90:29 - you're applying machine learning use
90:32 - your intuition think problems through to
90:34 - the end and think about the simplicity
90:36 - and arrive at using that over complexity
90:39 - now this entire tutorial has been
90:42 - with brain.js and javascript and for
90:45 - that reason javascript thanks you and
90:47 - what i mean by that is until recently
90:49 - javascript really wasn't a very serious
90:50 - language with machine learning but that
90:52 - idea is changing and you are in part to
90:55 - be thanked for that in fact anybody who
90:58 - is looking at javascript as a potential
91:00 - solution for machine learning that is
91:02 - awesome and we should spread it like
91:04 - wildfire not just in javascript but in
91:06 - every language every computer language
91:08 - that is out there should have the
91:09 - capability of using machine learning
91:12 - because it is so amazing now i want to
91:14 - take this moment to invite you over to
91:17 - the machine learning movement in
91:18 - javascript which is the bri.im
91:20 - website and as well their slack channel
91:22 - there you're going to find like-minded
91:24 - individuals who are not only interested
91:26 - in javascript as a means of using
91:29 - machine learning but also challenging
91:31 - the ideas that are out there now and
91:33 - introducing new ones i look forward to
91:34 - talking with you there and there are
91:36 - many others that do as well thank you as
91:38 - well for letting me teach this course
91:40 - teaching using the scrimma platform has
91:41 - been a real honor and i really too thank
91:43 - the encouragement of the scrimba crew
91:46 - you know who you are for prodding me
91:47 - along to make this course and i want to
91:49 - thank you for your time and letting me
91:51 - talk about these ideas and try to arrive
91:53 - at some sort of simplicity with them if
91:55 - you're following the entire course all
91:57 - the way through to the end i look
91:59 - forward to seeing what you can build and
92:00 - now comes the fun part we get to use
92:02 - machine learning to solve real world
92:04 - problems
92:06 - i'll leave you to it

Cleaned transcript:

hey welcome to neural nets in javascript with brain.js i am super excited to teach you this course the goal is to give you a practical introduction to problem solving with neural networks what you're going to be learning in this course propagation both forward and backward layers neurons training error what feed forward neural networks are what recurrent neural networks are and a whole lot more we're going to build azor gate a counter a basic math network an image recognizer a sentiment analyzer and a children's book creator and how we're going to do it is with 17 lectures where we're going to focus on practice over theory what that means is you are going to get your hands dirty but more than that you're going to walk away knowing the ideas behind neural networks there's as well a bunch of interactive challenges along the way and that brings me to our use of scrimba scrumba is a fantastic platform for learning and at any point during the entire lecture you can stop me it won't hurt my feelings you can just introduce brand new script and you can press command plus s if you're on a mac or control plus s if you're on linux or windows and it will execute exactly your code that is super important throughout this course as well i'm going to make regular reference to the console which is directly below here so if you see some numbers go down there like i'll go ahead and test that right now 0.05 just appeared that's super important anytime i start talking about if the neural net was good because it had a low error rate or if the neural net was bad because it had a higher rate just look down there and that will give you a little bit of reference as to what we're doing so let's get started this is going to be awesome this is our very first neural net this is going to be awesome so the first problem that we're going to tackle is called exclusive or and you can do some research on it if you like but more or less this is what happens you have inputs that are the same they result in a zero output when they differ it results in a one there's always two inputs there's always one output so let's take this very simple comment and let's translate it into something that the neural net or rather the javascript can understand let's have a variable we're going to call it training data and there's our very simple variable that represents all of our training data and let's go ahead and import brainjs brain.js is i'm just going to grab a link that imports it from a cdn content delivery network got that and next we want to instantiate a new instance of brain and we do that const i'm going to say net equals new brain dot neural network and down here we're going to say net dot train and we're going to give it our training data and now at line 16 by the time we get there the the net will have understood what our inputs and outputs are and so we can hear console log out net dot run one of our inputs so let's choose the first one got to give it our layers hidden layers three we'll get more into hidden layers later and now we're going to go ahead and hit run and now we have an output that's so awesome now the reason that this number here is not zero is because we're using uh a neural net and it's very hard for them to speak specifically zero and one they can speak close to that so that's exactly what we want is is a number close to zero which is 0.05 now here's a challenge for you go ahead and get the next outputs console logged out and just kind of play around with their values and and see how the net operates in our last tutorial we talked about how to build a neural net a very simple one to solve exclusive or and in this one we're going to discuss how it did it so the neural net has different stages you'll notice i use two different methods here the first one is train and the other one's run now in train we do something called forward propagation and back propagation those terms may seem scary at first but they're actually quite simple in fact we're going to reduce their complexity down to something that even a child can understand you'll take a look at my slides here forward propagation and back propagation we have a ball we're going to take a ball and we're going to throw it at a goal now when we do that we're going to make a prediction as to how far the ball needs to go uh how much energy to put behind it the pathway of the ball etc i want you to go ahead and pause the video here and to think about the different steps that happen when you throw a ball at a goal the first step is prediction now in prediction we're going to think about how we're going to throw the ball where it needs to land how much power we need to put behind it that first step and with my ascii art showing off that we did not go far enough with the ball this step is for propagation we ourselves are making a prediction from that prediction we can see how far we were off from the actual goal we can measure that and that that step of measuring that is back propagation now the next thing that we want to do is make a determination as to what we're going to do next that is our second step of back propagation that is our learning step and you know how the story goes we throw the ball again it goes too far we measure that we make another prediction we throw the ball again third time's a charm that illustrates this very first method and everything that goes on inside the net the next stage is running our net now in running our net we no longer have to measure how far we are from the goal we already know and because of that there is no need to back propagate so that step goes away now throughout this entire training of the neural net the net is measuring and that measurement is referred to as error check this out if we go to our net during its training we can actually enable something really interesting we're going to give it a log function we're going to console log error and we're going to have our log period set to 100 intervals check this out now we can actually see the error how far off the net was and you can see for a time some of these errors may go down or may go up but eventually the net catches on and it starts to accelerate its ability to learn until the error rate starts to drop to a ridiculously low number not zero though until training is completed and once training is completed there's no need to continue training as we discussed we can then just forward propagate in our last tutorial we talked about how neural networks learn using forward propagation and back propagation in this tutorial we're going to understand more the structure of the programmatic neural net neural nets are actually quite simple they are composed of a function that receive inputs as an argument and produce outputs if we think of our neural net in this very simplistic way then really we can reduce the complexity of it down to one of the simplest functions that you can write pause the video here and just look at the structure now we're going to talk about how the network initiates if you think about when you were first born likely you didn't know very much over time though you begin to know more and more the neural net begins with a bunch of random values so everything that affects the outputs is just random at first you may ask yourself why the reason is because mathematically we've proven that is an effective way to start off with knowledge the knowledge is very random at first we don't know the idea it's not zero and it's not one it's somewhere in between over time we can shape that random data so that it finally becomes where we store what's going on inside of the neural net each neuron is quite literally math.random go ahead and pause the video here and get comfortable with the idea that the net starts out with random data next i want to talk about activation a really popular and effective activation function that's used nowadays is called relu relu looks kind of like this if we were to put it inside of a function the function would quite literally look like this that is our activation function called relu now activation functions are measured in back propagation using what is called their derivative i'll go ahead and put a link here in our bonus material two i'll go ahead and post some links that take you to where relu and its derivative are used in brain our last tutorial we talked about the structure of a neural net and in this one we're going to be talking about layers take a look at my illustration this is a neural net each circle represents a neuron the arrows represent a bit of math stacked circles are layers and so here we have what are called input layers that's this first one this next layer would be a hidden layer it's composed of two neurons the next is another hidden layer composed of two neurons and the last is called an output layer now in brain the input layers and output layers are configured for you kind of automatically however our hidden layers can be configured by us our first neural net was composed of two input neurons one hidden layer that had three neurons and an output layer that had one neuron just as our illustration has two neurons for the input layer two hidden layers the first having two neurons the second having two neurons and the last one having two neurons what's interesting about hidden layers is that's really where the majority of their storage is if you likened it to a human the hidden layers are where the ideas are you may run into a scenario where your neural net isn't learning i'll go ahead and recreate that here i'm going to change the hidden layers from a single hidden layer with three neurons to that of one and i'm going to log out training data watch what happens we hit 20 000 iterations without fully understanding the problem and how to solve it now we can easily fix that by changing our hidden layers to a single hidden layer of three neurons you can see we're able to train in a short amount of time 4 800 iterations we can as well have more than one hidden layer our illustration has two hidden layers we could mimic this exact configuration two hidden layers followed by two hidden layers and this way note of caution though the more hidden layers that you add the longer it takes for the neural net to train let's try it see we hit 20 000 iterations without actually fully training there is no hard and fast rules when it comes to hidden layers this would be an invitation to experiment something i have seen though is to treat the hidden layers sort of like a funnel so if you had for example 20 inputs you could have 15 hidden layers followed by 10 hidden layers followed by two output layers that's just an example though and many problems take on different configurations switching gears for a moment take a look back at our illustration you remember that we have these arrows and i said these arrows represent a bit of math and a feed forward neural net that math can be described as this we have our input weights times our inputs plus biases activated now this is simple math but the implications of it are huge pause here and just think about how simple that is and let it sink into your brain this tutorial series is really about the practical application of neural nets but if you're curious like me you can take a look here to see how brain uses this math as another bonus take a look at the additional options that are available for brain neural nets can be widely configured to solve many different problems all it takes is experimentation time and enthusiasm up till now we've concentrated on the basics of how a neural net works and we've used arrays for our training data but in this tutorial we're going to be talking about sending different shaped data into a neural net now to illustrate what i mean by that let's take a look at my slides here we have an array now it's a very simple array but it's an array and what makes arrays incredibly useful is we can reference the values by index for example the arrays index of 0 gives us 0.3 the array index of 1 gives us the value 0.1 arrays are useful in neural nets because they represent a collection generally of the same type of values and we can use the index to look up those values if we look at an array beside a neural net we can see each neuron associates meaning with each of the arrays indexes now when dealing with a collection of the same type of value this is perfect but what about when our data isn't in the form of an array what about for example objects it just so happens that brain.js was built for this type of practical use and we are going to build a neural net that uses objects for training to get us started i've included the browser version of brain.js here in the index.html file next let's define what our training data will look like our input will be an object that is going to have the properties red green and blue and our output will have properties light neutral and dark it just so happens i have some training data i'll go ahead and paste it in and you can see our colors and brightnesses what's really useful with brain.js is you don't have to define all the properties you can but you don't have to when they're missing it simply uses a zero in their place so in this first object we see that red is missing well red here will just simply be zero and you can see a similar practice on the brightnesses let's go ahead and turn these two different arrays of objects into our training data so we're going to define const training data equals an array now we're going to iterate over the brightnesses and colors and build up our training data so four we'll say let i equal zero i is less than colors.length i plus plus or you could use a for each or even a map training data dot push we're going to give it an object each one of our training sets is an object and that training set will have an input and that input will be of colors and our output will be from the brightnesses and those indexes are equivalent next we'll define our neural net so const net equals new brain dot neural net work and we're going to give this hidden layers i'll have a single layer with three neurons and next we can basically just train our neural net so net.train and we're going to give it the training data rather than logging out what is happening inside the neural net let's just get our stats what happens at the very end and we'll go ahead and log those to the console now let's see what we get cool 1200 iterations it learned it fairly quickly and let's see actually what the neural net outputs so net dot run and a value of red 0.9 of red and we'll log those values out let's see what we get very cool so you can see in the training set we did not include dark neutral and light in every single one of the brightnesses however brain is smart enough to combine those together and it gives us with red being 0.9 that red is dark now as a bonus i could spell bonus correctly there we go what if we had to invert the problem what i mean by that is what if we are for example asking our neural net for colors rather than classifying their brightness in this scenario pause it here and think about how you would accomplish this so by inverting the problem our inputs would then be light neutral and dark and our output would be a color red green or blue for us to flip the values let's define our training data again this will be const inverted training data and we are going to for i equal zero i equals colors dot length we're going to take the inverted training data and push objects to it that are just like prior our input and output but we are going to change the input to accept brightnesses and our output to accept colors so that's our training data next let's define a inverted net those new brain dot neural neural network the same hidden layers and let's train it inverted stats equal inverted net dot train inverted training data we'll go ahead and train on this cool let's log out the stats there's our stats and we can see it actually didn't do a great job at learning the problem but that isn't the point of this exercise it's really just to understand the neural net from a different vantage point when you flip a neural net like this you're kind of generating values that can be really useful in predictions in this tutorial we're going to get a bit more adventurous and push the boundaries of what you can do with a neural net but doing so in a safe and easy manner but to help us understand take a look at my slides in our previous tutorials we used numbers directly in the neural net and numbers are just one of the types that exist in javascript and in other languages now javascript is fairly simple in its types in fact we have only boolean numbers objects strings null and undefined now it would be nice if we could feed these other types of values into the neural net so that it can understand context better and solve different and seemingly more complex problems so are we doomed then do they just speak numbers previously we used an object with a neural net and we did so using its properties defined with numbers but the principle of assigning a value to a neuron will provide us the answer for our neural net to speak more than just numbers now the question in your mind is probably how let me illustrate in a way that a child would understand this is a light switch it is off now if you were to ask a child to turn the light switch on they of course would but it wouldn't just happen like that it would happen more like my son does he sees that it is off and he becomes excited and he's taking gymnastics and so he will use that to his advantage he'll perform some amazing gymnastic maneuver over to the light switch turning it on and seeing that it's on that he just did something very useful he'll let out a cheer and do some gymnastic move off and away celebrating that he was a useful kid and that he has the ability to flip switches this ability to understand both off and on has huge implications now our computers they speak binary that's just ones and zeros so it's very very similar language to the neural net we could use this same practice we could say that zero is off and one is on now we sort of did this previously with objects via their property name but we fed the inputs directly into the neural net in this case we assign a neuron to a specific value that we're training towards either the input or the output and when that input or output neuron fires we basically just assign that value as one otherwise it's zero so just like our on and off we're taking these values like a boolean or null value or a string etc and we're simply assigning it to the input so here my potentially null value is being fired upon so it's a value of 1. my string 1 is as well the other values are not and as for output the net is going to try to learn those values just as before and so really the implications here are that we can send just about any value into a neural net so long as that value is represented by a neuron okay let's go ahead and get coding and then this will all make sense first off in our index.html file i have included the browser version of brain.js next we are going to get some data that we're eventually going to use to train on this initial data is an object with property names of restaurants whose values are what day we can eat free with kids and our objective is to find a way to get these string values represented as ones and zeros into the neural net and what we're going to do is give the neural net a day of the week and it's going to tell us where to go on that day of the week so that we can eat free with our kids now pause it here for a moment and think how you would accomplish this using that light switch analogy next let's go ahead and plan how we're going to input our training data into the neural net so if we are going to use the day of the week as the question we're going to ask our neural net that will be our input so our input is going to be a day of the week so monday tuesday wednesday etc our output is going to be the restaurant name so restaurant so that is our input and output for our neural net now next we're going to go ahead and build up our training data so let's go ahead and build that using a const that's going to be training data that's going to be an array and to put data into the training data from our restaurant so we'll need to iterate over our restaurants so for let grant name in restaurants and the day of the week is the value therein so cost day of week equals restaurants restaurant name all right so we've got our day of the week and now our restaurant and our training data we're going to push a value to it that has our inputs and outputs now our input is going to be this is where the rubber meets the road so to speak it's going to be our day of the week and we're going to assign that a value of 1. now just think about how simple that is if you need to pause it please do so but think about how simple what we just did is we are giving an input to the neural net of a day of the week assigned by its value of one now all the other days of the week because of the way that brain.js is built are going to initially be zero so only this day of the week will be one next let's go ahead and assign our output that's going to be it's restaurant name and we have just built our training data if you need a moment to pause and think about what we've just done please do so but it's a very simple principle the principle of represented values next let's go ahead and define our neural net that'll be const net new brain dot neural network and we're going to give it the same hidden layers as before single hidden layer with three neurons and all that is left to do is train on our training data so we'll do const stats equals net dot net.train training data and then we'll console log our stats out all right are you ready here we go look at that under 2000 iterations the neural net has deciphered what day of the week to eat for free with our kids that's nice but let's look at what comes out of the neural net console.log net.run we're going to say monday see what it says now this is where it gets kind of interesting all the restaurants are included with our result we just have a likelihood associated to each one of those restaurants but really what we want is to put a string in and to get a string from our neural net we're going to do that next if you can pause it just think about how you might be able to do that okay so we're going to create a function and its name is restaurant for day it's gonna get a day of week we're going to use that value with our neural net so net dot run day of the week this is going to be our result and from that result we'll have this collection of restaurants with the likelihood that we should eat there and the highest likelihood will be the correct prediction for the given day so what we're going to do is start out with a highest value and as well the highest and we're going to iterate over our results so for let rest name in result and then we're going to say if the result restaurant name is higher than our highest value we're going to set our highest value to that value name or is we're going to save the highest restaurant name close restaurant and from here we'll just return the highest restaurant name and so from this we're going to accept a day of the week a string we're going to put that into the neural net the neural net's going to give us its predictions those predictions are a list of restaurants we're going to iterate over those restaurants and then we're going to save the highest one and we're going to return the highest one we'll go ahead and log the results out so this is restaurant for day we're going to say monday and we'll test it all out there it is brilliant yellow coral perfect now let's let's add the rest of the days of the week so monday tuesday wednesday thursday friday saturday sunday tuesday thursday friday there we go we've got all the restaurants for the given days of the week so now we've got string in and string out for our neural net next as a bonus try and flip this logic the other way so that you are inputting a restaurant name and you are getting out a day of the week i'll leave you to it in this tutorial we're going to learn how to count and although that sounds kind of like an easy task at first it actually is a little bit difficult but when we use the right tools it becomes easy take a look at my slides and this will give us a background on where to get started this is exclusive or our first problem that we solved each of the empty squares is a zero and each of the black squares is a one now let's take a look at a different input one that may be a little bit more tricky it's hard to kind of see what this particular input means so let's rearrange it so that it's easier for us humans we can see this is a 4. now both of these inputs illustrate something that may have occurred to you already and that is that they have width and height or length now width and height is just sort of another way of looking at length width and height don't really change neural nets they are constant but in computers there are some rules that can be bent and others that can be broken and we'll start illustrating that now by going to the movies on your trip to the movies you're going to bring your best friend along and they're of course thrilled at going to the movies with you because it's the latest and greatest movie that you've been looking forward to and everything is going fantastic and in fact it's the cliffhanger scene right there at the middle but all of a sudden the screen goes black why why did this happen this the screen went black it was right as i was expecting something to happen next why were they expecting something to happen next well it's because they they built up a memory of what was happening up to that point that's important think about that just for a moment we'll come back to it back to our nonplaying movie your best friend looks at you and they are of course thrilled that they're at the movies but are saddened that the movie will not continue to play and then all of a sudden it starts to play again and they're of course as thrilled as can be and the movie ends just as you would hoped so that is our illustration of the movies now if you think back about what we paused in the movie every frame of the movie was the same height and width every frame no frame was different in size if you think about that over time though a frame being one part of a movie one frozen image of the movie each frame has that constant size but there are hardly any movies that are the same duration they all have different times that they play out that duration that's depth that's our frames plural that's mini frames that is really important with neural nets the depth the frames how long the movie is and what happens on each frame and what leads to the next one that gives us a context as to what is happening in the movie it's the same with neural nets this context and a neural net recurs it's something that happens over and over again something that has to in a sense repeat this terminology in neural nets is called recurrent now that sounds like a very complex word recurrent oh no what are we going to do next it's actually quite simple and to illustrate that let's go simple let's let's go to something that even a child can understand one now at its very simplest if i go to a child and i say one likely the child will not understand what i'm talking about unless we've trained on that in previous sessions so one to a to a child is is foreign and it's the same with a neural net i don't know what you mean by one that's essentially what the neural net will say however as soon as we start giving it context it being a neural net or it being a child they can start to decipher what we are trying to ask from them i'll continue we talk to our child we say one two three four and we pause what do you think the child will reply with likely a response would be five now this is where recurrent happens with neural nets recurrence is like taking each of these states one two three and four and sort of putting them together in a sense adding them together into a kind of a pool and that poll says you know the most likely thing they're looking for is probably and then out comes a five that is depth it's the same as our movie it's the same as being able to see each frame and that depth happens over time if the movie played out of sequence if the frames were shuffled in a sense likely would get very little out of the movie now this context is sort of a observer that that looks at each of these steps and can guess what comes next that context that depth that time that recurrence all refer to a very similar concept that depth that time that context it's all dynamic they're all the same width and height or even the same length but they're not of the same depth because we can feed in more than one and the context is dynamic two in the sense that we can say one two and ask for what's next and it'll say three or we can say three four what's next and it will give us a five or we can even reverse it and say five four three two what's next and it'll give us a one that's how dynamic that recurrent concept is in our neural net the ability to sort of take in those multiple frames that's called a recurrent neural net and in this simplest form the feeding of for example numbers is like stepping through time or a time step and as i said in this tutorial we're going to learn how to count okay now to get started let's go ahead and include the browser version of brain.js so we'll add that here our training data it's going to have two different outcomes one is going to count from one to five and the next one is going to count from five to one we'll define our training data manually that'll be a const called training data and it is going to be an array and in that array we'll have two arrays the first one will be one two three four and five and the next one will be five four three two and one that's it that's our entire training data next we'll define our neural net const net equals now this is a new namespace in brain new brain dot recurrent dot long short term memory or lstm time step now to train the neural net we'll give it our training data using the train method net dot train training data and let's see what actually comes out of the neural net while we're training it by logging we're going to give it a log function let's go ahead and see what happens all right it trained really fast very cool but let's remove the logging now that we know that it can train and let's see actually what comes out of the neural net so console.log net dot run and we're going to give it part of one of the arrays that we defined in the beginning so one two three four and that's it for the first one and we'll do the same for the second one right because we wanna as well count down from five net dot run and we'll do five four three and two all right let's see what comes out there we go awesome we got exactly what we wanted and that is how you count using an element in our first run we gave it an array of one two three and four expecting a five and that's what we got four point nine eight and in the second one we sent in a five four three two and we're expecting a one just like we have up here in our training data and we got a 1.005 so that's really exactly what we wanted now as a bonus try adding another training array that counts from 10 to 5. or even from 5 to 10. in our last tutorial we used a long short term memory time step neural network to count that's a mouthful and in this tutorial we're going to use that same sort of net and we're going to work up to predicting stock market data so let's take a look at how our data is shaped first our raw data is going to be where all of our values live now this isn't yet training data we're going to turn it into that our values are going to be from an object and that object is going to have properties open high low and close each one of those are numbers now let's take a look at our raw data for a moment now if you look for example at the open property you'll see it's quite a bit larger than numbers that we've used previously which were from 0 to 1 or 0 to 10. the values repeat and you'll see that open high low and close follow a similar pattern what we want to do though because the neural net was instantiated with values between 0 and 1 and that is sort of the language that the neural net speaks if we just send these values into the neural net it's going to take it quite a bit of time to sort of ramp up its understanding of these larger numbers if you can imagine it's like walking up to somebody who has only ever heard whispers and then just yelling at them it would be first off rude and secondly it would just be loud it wouldn't be what they were used to what we want to do is make it easy for the neural net to interpret this data and this is a very common practice so let's start out by writing a function and this function is going to normalize our data but since normalize is the most friendly term let's just call it scale down that's our function name and scale down is going to accept a single object and we're going to call that a step so that's a step in time now the same object that we have coming in we just want to turn down those values and so we're going to return an object and that object will have open and for the time being we'll go ahead and return step.open or define it rather with step.open and the same for et cetera and that is our scale down function however we're not normalizing anything yet and since if we go over to our training data once more and we look at these values one of the lowest ones that we can come to is this value 138 or so so what we're going to do to get these values easily into the neural net is simply divide by that value so 138 there and that's our normalize function quite simple and to prove that it works let's go ahead and console log it out console.log scale down we'll just grab the first item in our raw data there and we can see the values are fairly close to between one and zero now if you'll pause here just for a moment think about how we go the other way scaling up or as they say denormalizing okay so now what we're going to do before we do any machine learning we're actually going to write the inverse of this function which would be the denormalize or in practical terms the scale up function so function scale up and it's going to take a step and we're going to return that same sort of signature that same sort of data but rather than dividing by 138 the exact inverse of dividing is multiply so we will multiply by 138 and now we have our scale up function referred to normally as d normalize so normalize brings it down denormalize brings it up and to test them both side by side we'll go ahead and console.log scale up it'll look kind of funny scale down in our we'll have our raw data and we'll send that in and what do we get there we go our scaled up and scale down functions we're going to use these in our next neural net but this is a very common practice across neural nets in general not just for current neural nets it is normalizing our values a more common approach to normalizing your data would be to subtract the lowest value from all the other values that you're sending into the neural net and then to divide by the highest value minus the lowest value that sounds kind of confusing at first if we were to take one of these lines for example uh this first one had scaled down and we used it right here and the net result of this is let's say the step dot open was 140. we're going to subtract uh if 138 was the lowest and then we'll divide let's say the the highest value is 147 minus 138 we would get and we'll subtract 138 from 147 and that equals nine and so the net result is two divided by nine and that equals zero point two two two two two to two and what's important about this is it's the end value is between zero and one try and rewrite the scale down and scale up functions to incorporate the more generalized approach which would be to subtract the lowest value divided by the highest minus the lowest value in our last tutorial we talked about normalizing data and in this tutorial we're going to write the neural net and we're going to put that normalized data into it so first let's scale all of our raw data we're going to call this scaled data it's not yet training data but it's close we're going to say raw data dot map and we're going to map over all those values using our scale down function and so now our scaled data will have all of those new values that are normalized next what we want to do is rather than feed in one long array of all these objects these properties so that the neural net memorizes this one long pattern we want the neural net to understand smaller patterns and to predict off of those and so how we're going to do that is we're going to create finally our training data and our training data is going to be an array of arrays we're going to take our scaled data and we're going to slice it into chunks of five starting at the first index and we're going to progress by five indexes each time and so that is our training data and we can console log it out make sure it looks right very nice okay so it's an array of arrays that's an important concept okay so now that we have our data chunked and normalized and ready to go into the neural net we're going to write our neural net so const net equals new brain dot recurrent that long short term memory time step that's our neural net and we're going to define it with a few options now this is important our scaled down data has four different properties here open high low and close that represents one point in time or one step through time so our neural net is going to have an input size of those properties being them four properties our input size will be four and it's very rare to deviate from that with output size so we'll go ahead and put an output size of 4 as well and so now we want to define our hidden layers and our hidden layers are simply going to be eight neurons and eight neurons so input size of four hidden layers of eight and eight and an output size of four and now we can learn our stock market data net dot train training data out now we're going to tweak our training options just a little bit here we're going to put our learning rate at 0.005 and the reason we're going to do that is so it doesn't sort of shoot past the values that we're looking for we want very small increments uh toward our goal and the next is our error threshold now the longer and the more data that you end up using with your neural net potentially longer it's going to take to train and so for this being just in the web browser and we want to train to a sufficiently good error i'm going to turn it down to 0.02 and as well let's go ahead and log out our values let's clean this up all right and now let's learn let's see what comes out very cool all right so it trained let's log out what really matters so that's console.log and that'll be net.run and let's just give it uh the first item in our training data so our training data and let's see what comes out very cool so our net actually learns something and returns something now there's a problem here if we look at our original values they're all in the mid 140s or so but our values here are not they're close to zero and one so what do we want to do there well i'll give you a moment to think about it okay this is where we finally use scale up this is where we denormalize our values here we go i went ahead and added that as a wrapper around net run and we'll run it again and there we go the values that look very familiar our 140s or so one thing that would be really useful is to not just look one step into the future which is what we're doing right here that's what net.run would be it's going to take all of our existing steps and say hey this is the potential next step what we want to do is actually look and see what the next two or three steps may be in our last tutorial we predicted the next step for stock market data and in this tutorial we're going to predict the next three steps that's really interesting and cool and it's actually quite simple so we're going to actually just change one little method so rather than use net.run i'm going to comment this out and i'm going to say console.log we're going to do net dot forecast and let's say we just have a couple numbers to go on we're going to send in our training data but we're going to only send in a couple of steps of that data and then we're going to say with forecast that we'd like the next three steps and it doesn't stop there it's actually going to return an array and before we had wrapped run in scale up whereas here in net.forecast because we're getting an array rather than an object which would be the next step the array would be the next steps and so we can take this and map to scale up all right here we go and there's our data that's what the net has learned and it's its next prediction so we've got the beginning of one there's our our second and three i think the console doesn't log up very well with the cursor but the idea is there and so is the data in this tutorial we are going to take a recurrent neural network and we're going to learn math and we're going to do it character by character and at first that sounds kind of weird but you'll notice something interesting at first with our training data here there are no numbers not directly these are strings even though there are numbers inside of them and we are going to feed these directly into the neural net this is where it gets interesting especially with recurrent neural nets so the way that a recurrent neural net works is it has an input map and that map is kind of like an array and that array maps a value that is coming into the neural net to a neuron and it does so by index and this is how it works by first sitting in our training data the training data is analyzed and we pull out unique values like zero the next value being plus and the next value being equals because 0 is repeating up here and we've already established it equals would be the next one 0 is repeating again and zero and plus repeats again and the next unique value we have is one in the next item in the array and that happens uh throughout our entire training set so one two three four five six seven eight and nine and so we we build sort of an input map and the reason why that's important is the input map lines up with neurons so our input map is more or less the same size as our input size and that's calculated internally inside of the neural net so that's uh it's important but we don't have to think about it so much so input map is the same size as input size and what that means is for each one of these characters they get their own neuron and so if we step through a problem for example like zero plus zero equals zero to the net we feed in a brand new array each time with that one value activated and so our input internally may look like this so we've got a a bunch of zeros and so the the inputs uh literally look like okay so input length equals input size so each one of these values is sort of attached to an input neuron and this is why that matters each time we feed in one of these characters it goes all the way through the neural net and then we can do that over and over and over again so the math problem zero plus zero equals one literally looks like this to the neural net it looks like zero plus zero and that's the third one equals zero so that is our first math problem sort of internally to the neural net but it it does everything sort of automatically for us and this is a good kind of magic it's not the bad kind it's the kind that is predictable so we'll go ahead and comment that out and this out too all right so we're ready to go ahead and predict our numbers so this is what happens next we're going to define our net const net equals new brain dot recurrent dot along short term memory and we're going to have hidden layers of 20 and we're going to net net.train on our training data and we are going to set our error threshold to 0.025 five and we're gonna go ahead and log out our value all right so we'll go ahead and learn it and we'll see what happens very cool now a word of caution here it did learn it but if you crank down your error threshold that may be able to learn it but in the browser it can take a long time and a long short term memory neural net is quite complex in contrast with our previous neural nets there's a lot more going on in the long short term memory than the others so our net was able to learn it but of course that's not the fun part the fun part is actually getting to see what comes out of the net so let's do that net dot run and we're just going to give it some straight up old strings 0 plus 1 equals and we're going to see what comes out there uh but let's not stop there let's just think of a few let's see uh 4.1 uh and depending on the error rate too some of these values may be a little bit incorrect uh but they should be pretty close it's uh quite entertaining what a neural net can can actually give us okay so we're going to run these three values and see what happens all right zero plus one is one four plus one is five and two plus one is three we nailed it awesome so that is recurrent neural networks learning math now if you want to do some bonus material light it up with some math problems in this tutorial we're going to read numbers like you would from an image and for visual simplicity our raw data is going to be of strings like you see here spaces are what we don't want the neural net to see and this asterisk is what we do want the neural net to see now we're going to use this simple string and a simple normalization function to convert these inputs right here two zeros and ones so let's go ahead and write that first so we're gonna create a function and we're gonna call it two number and it's gonna take a character as an input what we're gonna return from this function is if the character equals an asterisk we're going to return a 1. otherwise we're going to return a 0. that's our character by character normalization function now we also need to create a function that normalizes for the entire input like the entire array that function's going to be called to array and it's going to bring in a string now the string needs to be a specific size and if that string dot length isn't seven by seven and we're going to throw a new error string in wrong size otherwise we're going to take the string and we're going to split it and then we're going to map it to number and that'll be our return value that is everything that we need to normalize our input value cool so next what we want to do is i'm going to paste in some number data but you'll notice the number of data isn't just zero to nine it's actually a string and you can see that these functions start to make a little bit more sense now two array is going to take this string in and it's gonna split up this long set of characters and it's going to convert them to ones and zeros so that the net can sort of understand what is is being said here now we have zero through nine and each of these you can see looks exactly like the character it's representing let's test at least one of these values so console.log we're going to put nine very good and we can see that represents what the neural net is going to speak so let's build our neural net so const net equals new brain dot neural network we don't have to mess with size in this because it's a feed forward neural net so let's build our training data const training data equals that's going to be an array of objects that have an input and each of those inputs are going to be one of our values above so the first one let's just say will be zero and the output is going to be an object and that object is going to be of the property that we're classifying towards with a value of one very good so let's go ahead and add the additional numbers here you see we've got one two three all the way up to nine and so now all we need to do is train net.train training data and we're going to log our results out all right let's see what happens very cool 190 iterations fairly low error rate very cool but we still haven't seen anything interesting yet let's go ahead and actually see what the net is outputting so net dot run to array and we're going to give this the number eight and now let's see what the neural net outputs we'll say const result equals and we'll turn off our logging up here because we know the neuron that's training just fine all right here it goes very cool now what you'll notice here is that the output is all of the properties that we used in the training data they're compiled together and the one that is most likely has the highest value so here 8 is a likelihood of 0.7 and that's really nice we don't have it though in a simplified format there is however a utility built into brain that is called likely that we can use here const or result equals brain dot likely and we're going to use our two array and we're going to send our net to it and here we're going to take this two array we're going to give it a string very good so that's our 8 and let's console log out the results all right here it goes very cool we got an 8 out of our neural net all right so that was a really simple example and it took a lot to work up to it because we've got a little bit of training data there some conversion etc but play around with this and see if you can get some different characters to be recognized by the neural net now a word of caution here this method will likely be a bit fragile in the real world without something like convolutions and those are coming to brain.js in version two but the thing to note here is that the principles still applies now there is a question to be begged here how resilient is a neural net why wouldn't we just use a bunch of if statements and a bunch of loops to build this example of an eight if i were to remove for example something that the neural net sees like one of these asterisks would the neural net still recognize this as an 8 or if i did the same with a 2 or a 6 etc well let's find out see it still found the eight this demonstrates one of the coolest parts about neural nets and that they are dynamic and they're resilient and that's why they're so popular and so amazing and for your bonus write three examples that still match like this eight but are not like your training data and that'll demonstrate the resilience of the neural net have fun in this tutorial we're going to write a children's book using our current neural network and we'll start with our training data here and if you'll go ahead and think about pausing it here what type of neural net that would use based off our previous tutorials okay likely you arrived at a long short term memory recurrent neural network we're going to go ahead and train on our data and we're going to give it a few options our iterations we're going to stop at 1500 our error threshold we're going to stop at 0.011 and we're going to log our details all right let's see if it rains very cool and now let's get it to output something interesting our first line from the training data net dot run is jane we're going to stop there with it and jumping to the more complex line i'm going to say net dot run it was just based off of these two lines let's see what the network outputs get rid of the log very cool so it starts out jane saw doug and jane looking at each other oh wow and then net.run it was love at first sight and spot had a front row seat it was a very special moment for all you see the net already altered a little bit of what was going on what you'll find is that depending on how you set up your neural net it can give you some really interesting outputs and the more data that you give it the more interesting it's going to get so your bonus here is to experiment with book writing in this tutorial we are going to learn how to detect sentiment from text directly and you'll notice our training data is a bit different than prior here we have an array of objects whose properties are strings rather than arrays of numbers and brain js is set up to go ahead and accept this type of input it's not a problem so let's go ahead and get started we'll build our neural net const net equals new brain dot recurrent dot long short term memory neural net and we'll go ahead and train net.train on training data now we're going to alter our training options we're going to stop at 100 iterations our error threshold is going to be 0.011 and we're going to log our stats we're going to cut the chase and go ahead and output something interesting we are going to use an input that's not from our list above we're just going to use something that's similar to it so net dot run i am unhappy go ahead and log that out let's see what we get very cool we got sad let's remove our log all right now let's add an additional item net dot run i am happy very cool now i want to point out that we're not training this for very long and that the error threshold if you were to bring that a bit down and as well iterations it's going to be quite a bit more stable but what you may have not noticed is that we did not send into the neural net of values that we trained it with and yet it was still able to classify them as happy and sad so again we're kind of highlighting the dynamic ability of the neural net and for your bonus tweak the iterations and threshold be careful because in the browser you can cause it to train indefinitely or for a very long period of time but add five new examples in the training data that would be up here and then log out of five examples that aren't in the training data all right have fun yelling at the computer sentiment detection recurrent neural network and in this tutorial we're going to explain kind of how we did it now if you'll recall we use both inputs and outputs for the sentiment detection recurrent neural network that is kind of a new concept when talking about recurrent neural networks because we explain them kind of like going to the movies now there are no inputs and outputs with movies we're just an observer sort of watching what happens next however we can bend those rules in order to fit the paradigm of an input and an output and i'll explain how now previously we mentioned that a recurrent neural network builds an input map and that is kind of like an array and each value in that input map corresponds to characters that are used in the training data but those rules apply here as well nothing has changed the input character one gets its own special neuron and its own special map its own special lookup and as well the same is with the output of character two however our input map is not complete what is different here with the training data is that there is an unseen character a character that tells the neural net to transition between an input and an output we'll call that character the new idea character all right and so if we look at our training data we have an input one and then we have our new idea and then we have our output of two and so if we were to look at a set of arrays that consist of what is being fed into the neural net they look much like this so our input map we have three indexes three neurons okay and the first one is going to have a value of one that first value is activated that input character one that is activated here and our value that's being sent through the neural net next we transition to the new idea character and so there new idea gets a one the next character is followed by two so there we have zero zero one so each one of these items in the array correspond to a neuron that correspond to our input map that correspond to our training data it sounds kind of confusing at first but what you'll get to notice with recurrent neural networks and neural networks in general is that they're actually quite simple and even the math that represents them is quite simple there's just a lot of it by taking advantage of those rules and that math we can bend those rules to our will now for our bonus here what would the training data look like if we start with 2 and end with 1. so 2 would be our input and our output would be one and then as well what would this set of arrays look like i'll go ahead and pause it here go ahead and write your answer and we'll compare notes all right and so now if we're to compare our answers they actually should look flipped and so our training data that has an input of 2 and an output of 1 will internally to brain.js look like this it'll start with a 0 0 1 zero one zero and then and finally with a one zero zero now this is all internal to brain js this isn't stuff that you have to learn it's just understanding the principles behind the neural net because again a neural net is simple there's just a lot of it in this tutorial we are going to use reinforcement learning this is a really exciting frontier of machine learning and we're going to use it right here you'll notice our training data is exclusive or right where we started originally and half of our training data has been commented out this is important what we're going to do is learn these first two items in our training data we're going to test against these first two items we're going to look at this next item and see that the net really doesn't understand it and then we're going to learn it so let's start by creating our neural net constnet equals new brain dot neural network and we had hidden layers of three and we'll do net.train that's training data we're gonna log out our results and let's see what we get very cool so we've learned these very first two items let's go ahead and remove our logging and then let's take a look at what the net actually outputs so net dot run and we're going to give it an input 0.0 we're going to console log that and i'm going to do array dot from all right let's see what we get very cool and that's exactly what we were expecting however if we add one of the examples of like one zero let's see what we get all right very cool and that's what we were expecting the net has not been yet trained on this data and so this illustrates why reinforcement learning is so important and necessary our training data may not be available all at once it may come over time and so we want to make sure that our neural net can adjust for that so let's go ahead and adjust our training data adding one brand new example the one zero outputting of one and you'll get to see how we use reinforcement learning in this feed forward neural net alright so training data dot push and we're going to give it an object that's an input array and 1 0 is our input and then output is going to be of one array all right and then really all we have to do is take net.train put it down here and we can give it some titles to be fancy console.log before reinforcement well we'll just copy all of this and then here we'll do we'll say after reinforcement all right let's see what happens all right before we have two very similar values and after illustrates that our net was able to reinforce now as a bonus we go ahead and add the missing item from our training data all right have fun in this tutorial we're going to build a recommendation engine we're going to do so using the simplest sort of methodologies this is one of my favorite examples because it seems so hard when i first started in neural nets but in fact it's actually quite simple we're going to start with our training data which is color preference and imagine that this list was built you just gave somebody these list of colors blue red black green brown and they basically wrote a number by them how much they liked that color so they like blue a lot they like red a lot however they don't really like black green or brown and that's how we end up with these numbers on both the inputs and outputs so that's our training data and we can learn that really quick using a feed forward neural net so const net equals new brain dot neural network net dot train and we're going to have our training data we'll go ahead and log out our stats see what happens cool our net was able to train really quick on that data let's turn off logging and see what we get console.log array.from net dot run and we'll say blue one now we're going to duplicate this line we're going to put brown all right here we go all right so the user has a very strong preference towards blue and not so much towards brown we can see that here from our output so here's the scenario over time you go back to that original user you show them this list and based off of their experience things they like things they dislike they now have changed what they prefer and now brown is one of their favorite colors so how do we get that into a neural net why don't you go ahead and think about that just for a moment and pause it here okay for us to get brown into that training data we have to put it there so training data dot and we're going to say push let's say brown one and output we're gonna put as a one all right and so now we have our training data in there so now what do we need to do to get the net to understand what this new training data means i'll pause it here and let you figure it out okay next what you want to add is net.train and we're going to give it training data and we're going to log out the results again stats and console.log stats all right and now let's see what happens and actually let's go ahead and console log out our values but after we train we'll give us a fancy title as well so before and you'll notice that we are actually using reinforcement learning a bit here just very simple it's not very complicated at all so before preference change after preference change let's see what happens very cool so we scroll through our training actually let's go ahead and remove the logging there we know that it logged now let's see what happens very cool so now before our preference changed you can see that brown was rated fairly low but after our preference change it was rated at a mediocre point five now why could that be we'll pause it here and let you think about it okay the reason is this our training data still has reference to when their preference was towards zero rather than towards one we want to ensure that we only have their current preference so training data dot pop that's going to remove the last item from our array so we'll pretend this is some sort of user interaction or server interaction something that happened to adjust their preferences that gave us brand new training data and now we're going to train on it again and then after that preference change we're going to have our brand new updated values here we go all right before our preference change brown was rated fairly low at 0.05 and after it came out of a 0.89 so this illustrates both reinforcement learning and also how to build a very simple recommendation engine now your bonus here build your own recommendation engine what's your favorite food where do you like to go does that change over time has it changed since you were a little let the net know have fun awesome job you got all the way through the course i am super proud and i'm very excited that you were able to get through listening to me but also building and applying and seeing these new concepts and how they can actually be quite simple but as we're progressing i'd like to give you some takeaways for this course to be able to remember and to share with others and that is this machine learning is moving really fast and that is super exciting that means it's changing and it's being adopted we're solving problems that we've never been able to solve before and you have a voice in that machine learning can change directions and a lot of that has to do with our understanding of how to use it we're also arriving at simple solutions are are so much more dynamic and powerful than complex ones when applying it so if there is a practice that you want to see more of or if there is something you like to change about machine learning try and introduce it study it prove it test it to see if it is worthwhile and not only will you benefit machine learning for you you're going to help all of us to have those new tools and capabilities so machine learning is moving fast and you have a voice and where it goes from this course to i wanted really to highlight that machine learning is actually quite simple the terminologies behind it we can get very lost in them but when we arrive at the logical foundation of machine learning it is simple there's just a lot of it a lot of numbers a lot of logic and i want to highlight where you go next with this simplicity with three illustrations and they are equals mc squared and ac motor and washing your hands these may seem comical at first but follow me the first is equals mc squared albert einstein discovered equals mc squared because he reasoned on that electrons moving around an atom would be affected by gravity kind of like when you throw a ball into the wind like a beach ball or something like that a child can tell you if you throw a beach ball into the wind that is going to be affected by it and that's how he reasoned on the matter so the point is that albert einstein used intuition use your intuition when you're solving problems with machine learning and solve for simplicity like e equals m c squared the next illustration was that of an ac motor now a lot of people know that nikola tesla helped to build and refine the ac motor but if you go back further in his career you'll arrive at that when he was in school he was shown this dynamo type dc motor that had these brushes and that arced it sparked everywhere and was really crude and noisy and he asked his teacher why it needed to have the brushes why it needed to spark everywhere and his teacher was insulted by him asking the question and made him write an essay and it's a really interesting story but the point is this he asked why it was even there and from then on he started to model his career to build a motor that didn't have those brushes and he arrived at the ac induction motor and refined it and he eventually used that same idea to help power both chicago and new york simultaneously from niagara falls it was a simple question a simple idea that got him started nikola tesla used intuition something wasn't right about using all that complexity solve for simplicity use your intuition this last illustration is one of my favorites and it's a washing your hands and the reason is because it's such a simple practice most people know washing your hands it's common knowledge it helps you to be clean but prior to the mid1800s it was not in fact in hospitals doctors would do all kinds of weird things like perform surgeries and then go help with births and they were wondering why their patients are dying left and right but it turns out that egonos symbolwise theorized not only that we were being infected by something but that we should wash our hands to prevent it and at first it was rejected the idea was absurd why why would you ever want to wash your hands which to us sounds absurd but eventually it was accepted the idea though is that ignosingwise did not have access to microscopes and it was not yet discovered what a germ was ignos symbolwise used intuition and it was a very simple problem and he used a very simple solution use your intuition find those simple solutions machine learning is being applied everywhere and that is super exciting because there is not an industry that is not directly or indirectly affected by it we are solving problems that we have never been able to as a race that is awesome but when you're applying machine learning use your intuition think problems through to the end and think about the simplicity and arrive at using that over complexity now this entire tutorial has been with brain.js and javascript and for that reason javascript thanks you and what i mean by that is until recently javascript really wasn't a very serious language with machine learning but that idea is changing and you are in part to be thanked for that in fact anybody who is looking at javascript as a potential solution for machine learning that is awesome and we should spread it like wildfire not just in javascript but in every language every computer language that is out there should have the capability of using machine learning because it is so amazing now i want to take this moment to invite you over to the machine learning movement in javascript which is the bri.im website and as well their slack channel there you're going to find likeminded individuals who are not only interested in javascript as a means of using machine learning but also challenging the ideas that are out there now and introducing new ones i look forward to talking with you there and there are many others that do as well thank you as well for letting me teach this course teaching using the scrimma platform has been a real honor and i really too thank the encouragement of the scrimba crew you know who you are for prodding me along to make this course and i want to thank you for your time and letting me talk about these ideas and try to arrive at some sort of simplicity with them if you're following the entire course all the way through to the end i look forward to seeing what you can build and now comes the fun part we get to use machine learning to solve real world problems i'll leave you to it
