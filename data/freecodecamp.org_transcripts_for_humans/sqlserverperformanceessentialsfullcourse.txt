With timestamps:

00:00 - if you are going to use sql server at
00:02 - any sort of scale you're going to have
00:04 - to think about performance
00:06 - in this course you will learn the
00:08 - essentials of sql server performance
00:11 - you will see how to diagnose what is
00:13 - happening with a slow running sql
00:15 - statement and what strategies are
00:18 - available to make these statements run
00:20 - faster or john aurora teaches this
00:22 - course
00:23 - to follow along with the course you
00:25 - should have some development experience
00:27 - and some experience with sql server this
00:30 - course is divided into the six modules
00:32 - you see on the screen now
00:34 - you're going to learn everything from
00:36 - analyzing sql statements for performance
00:38 - to applying common best practices to
00:41 - ensure the best possible performance i
00:44 - encourage you to leave a comment with
00:46 - the most interesting thing you learn for
00:48 - the benefit of other campers who are
00:50 - watching this course
00:52 - you are watching the video of module 1
00:55 - getting started part 1 course
00:57 - introduction
00:58 - it is very annoying for every developer
01:01 - to get the result very slow from the
01:03 - queries that we are executed on the
01:05 - database
01:07 - and the slow result leads to the slow
01:09 - application performance which leads to
01:11 - unsatisfied customers
01:14 - in this course i am going to show you
01:16 - what you need to know as a developer
01:19 - about sql server to make sure that your
01:21 - data access code is performing well
01:25 - as a developer i am assuming that you
01:27 - are already comfortable in analyzing and
01:30 - debugging code in languages like c sharp
01:33 - and java
01:35 - but you may be uncomfortable with your
01:38 - level of knowledge of sql server
01:40 - and how things works in the database
01:43 - in this course i will help you feel more
01:45 - comfortable when you are working with
01:47 - sql server
01:49 - especially for the times that you are
01:51 - diagnosting and solving the performance
01:53 - issue
01:55 - and this is the most important part
01:57 - because in almost all system that we
01:59 - build our database is a critical
02:01 - component of the whole system
02:03 - so it is very important to understand
02:06 - that a database like sql server works so
02:08 - that we can make sure that we are
02:11 - getting the most out of it
02:13 - in this course we are going to focus on
02:15 - the performance aspect that we as a
02:18 - developer have control over
02:20 - that is making sql statement run as
02:23 - efficiently as possible
02:25 - and making sure that our application
02:27 - avoids the use of some well-known data
02:30 - access anti-patterns that are common
02:32 - cause for the performance problem
02:34 - this will give you the number of
02:36 - benefits for the application that we
02:37 - build
02:39 - the first benefit is efficient sql
02:42 - statements means that our statements
02:44 - will run faster
02:46 - making our application more responsive
02:48 - to our valuable users
02:50 - second benefit is efficient data access
02:53 - means that our application will be more
02:55 - scalable
02:56 - so we will be able to serve more number
02:59 - of users and transactions on the same
03:01 - hardware
03:02 - and finally we know that many
03:04 - applications are deployed on the cloud
03:07 - either as sql server deployed on a
03:09 - virtual machine hosted in the cloud
03:11 - or by using a platform as a service
03:14 - solutions like sql azure
03:16 - when you are hosting in the cloud you
03:18 - have to pay for the resources that you
03:20 - use
03:21 - so in that case if you have an
03:23 - inefficient sql
03:26 - it doesn't just make your application
03:28 - slow
03:29 - but it actually cost more to run since
03:32 - you are using more resources
03:34 - so you can see there are pretty much
03:37 - reasons why we want to understand how we
03:39 - can access our data in sql server as
03:42 - efficient as possible
03:44 - now there are some arguments that sql
03:46 - server performance is solely the domain
03:49 - of the database administrators
03:51 - but i believe it is important equally
03:54 - for application developers to have a
03:55 - strong grip of sql server performance
03:58 - concepts
03:59 - so let's discuss it in the next video
04:02 - why you as a application developer
04:04 - should learn about sql server
04:05 - performance
04:07 - you are watching the video of module 1
04:10 - getting started part 2 why developers
04:12 - should understand sql server performance
04:16 - there are many reasons why you as a
04:18 - developer should understand sql server
04:20 - performance
04:21 - one of the biggest reason is that
04:23 - generally the responsibility of the
04:25 - overall performance of the app is on the
04:27 - shoulders of the application development
04:29 - team
04:30 - and we all know that how much
04:33 - application performance is important for
04:35 - the user experience in today's time
04:38 - if our application is slow and
04:40 - unresponsive or can't scale to meet the
04:43 - number of users it need to be then the
04:46 - user is not satisfied with the app and
04:49 - looks for the alternative
04:51 - the second reason is as an application
04:53 - developer we have the view or scope for
04:56 - the whole system along with the
04:58 - knowledge of the application code we
05:00 - also have an idea of the use cases for
05:02 - the application
05:03 - and the data being used stored accessed
05:06 - by the application
05:08 - most important we have a view of how all
05:11 - the pieces of the applications fit
05:13 - together into different feature of our
05:15 - application
05:16 - and how these features are used by our
05:19 - users
05:20 - at the time of tuning sql statements or
05:23 - creating indexes in our database it is
05:26 - always good if we understand the larger
05:28 - context of our application such as what
05:31 - the application is trying to do
05:33 - and how it is trying to do
05:35 - so our knowledge of how the application
05:37 - works is greatly benefited in this case
05:41 - finally there will be no doubt at the
05:43 - times when you need to work closely with
05:46 - the dba team at your company
05:48 - this might be to search an issue from an
05:51 - application or do some database design
05:53 - work
05:54 - or any other reason
05:56 - by better knowing how sql server works
05:59 - you will be able to engage or help your
06:01 - dbas more efficiently
06:04 - you will be able to work more
06:06 - effectively analyzing problems and you
06:08 - will better understand what tools are
06:10 - available and how they can help solve
06:12 - these problems
06:14 - in the next video we are going to see
06:16 - what tools you required to follow along
06:18 - with me
06:20 - you are watching the video of module 1
06:22 - getting started part 3 tools you need
06:26 - in this course you will see me working
06:28 - in the local copy of sql server 2019
06:31 - developer edition
06:33 - however the lessons and techniques in
06:35 - this course will also be applicable in
06:38 - sql server 2008 2012
06:41 - 2014 2016 and also in 2017.
06:46 - if there is difference in how you
06:48 - perform something i will point that also
06:51 - if you are using azure sql as your
06:54 - database then there are no worries
06:57 - the tools and techniques you are going
06:58 - to see will also be applicable in that
07:00 - case also
07:02 - if there is difference in how you
07:04 - perform something i will point out that
07:06 - also
07:08 - there is a misconception among many
07:10 - peoples that to really understand sql
07:12 - server performance you need fancy
07:15 - expensive tools
07:16 - but it is not always true
07:19 - in fact all the tools that we are going
07:22 - to use in this course have already been
07:24 - installed on your workstation
07:27 - as they are a part of the sql server
07:29 - client installation
07:31 - if you have already installed sql server
07:33 - management studio
07:35 - you are ready for this course
07:37 - the features that i am going to show you
07:40 - in this course are already present in
07:42 - management studio and you might be
07:44 - unaware of those features
07:46 - i will also show you how we can query
07:49 - performance data out of sql server
07:51 - dynamic management views from management
07:53 - studio
07:54 - and how we can use management studio to
07:57 - set up event traces
08:00 - so by having basic sql server client
08:02 - tool installed and knowing how to use
08:05 - them you are done to accomplish almost
08:07 - all the performance tuning that you need
08:09 - to do
08:11 - you have to download sql server
08:12 - developer edition so that you can
08:14 - install the sample database and follow
08:16 - along
08:17 - i suggest to download from the link
08:19 - shown on screen
08:21 - and i will provide this link in the
08:23 - description box for your convenience
08:26 - press download
08:33 - and run the setup file
08:40 - and follow the instructions
08:43 - after installing sql server
08:45 - you have to download and install sql
08:47 - server management studio which is also
08:50 - called ssms
08:53 - you can download it from the link shown
08:54 - on the screen
08:56 - or i will provide the link in the
08:58 - description box for your convenience
09:01 - on this link click download sql server
09:03 - management studio
09:06 - and run the setup file and follow the
09:08 - instruction
09:11 - now speaking of demos
09:13 - i have a sample database for this course
09:16 - that based on a fictional university
09:18 - that all of the demos are based
09:21 - so in the next video i will show you how
09:24 - you can download a copy of the database
09:27 - and get that imported in your instance
09:29 - of sql server
09:31 - you are watching the video of module 1
09:33 - getting started part 4
09:35 - restore the sample database
09:38 - we are going to use the sample database
09:40 - throughout this course and this database
09:43 - contains enough data so that we can do
09:45 - some performance tuning on sql
09:48 - statements and get a feel of how things
09:51 - are going on in the database
09:55 - so first you have to download the simple
09:57 - data
09:58 - and you can download it from the link
10:00 - shown on the screen
10:02 - and i will provide this link in the
10:04 - description box also for your
10:05 - convenience
10:07 - download the appropriate file for the
10:08 - version of sql server that you have
10:11 - installed
10:12 - this is going to download a sql server
10:14 - backup file
10:16 - called students.b
10:19 - we will take this file and restore this
10:22 - in sql server to create a sample
10:24 - database
10:26 - to do this you have to go into sql
10:28 - server management studio
10:30 - and then in the database folder we just
10:33 - going to rightly
10:36 - and we select restore database
10:40 - that's going to bring up the dialog box
10:42 - here
10:43 - and we want to choose the second radio
10:45 - button which is device
10:48 - and then we will click on these three
10:50 - dots here to find our file
10:53 - then we will click add and this will
10:56 - bring up the file explorer here
10:59 - and i have my file out here in the d
11:02 - drive under the folder sql backups
11:05 - select the dot bak file
11:08 - i will say ok
11:11 - everything looks good here so i will say
11:14 - ok again
11:16 - and then this is going to go ahead and
11:18 - start the restore process
11:27 - so now you can see that i have this
11:30 - database
11:32 - and i can access this
11:35 - i have my tables in here
11:38 - i can use this just like any other
11:40 - database on my system
11:42 - so now that we have the sample database
11:46 - installed
11:48 - in the next video
11:49 - i will cover some basic sql server
11:51 - concepts you need to understand
11:54 - around tables and indexes before we dive
11:57 - into analyzing sql statements in the
12:00 - next module
12:01 - so we will go ahead and do that next
12:05 - you are watching the video of module 1
12:08 - getting started part 5 table concepts
12:12 - i would like to go through a couple of
12:14 - sql server concepts that will come up
12:17 - over and over again throughout the
12:18 - course to make sure that you are
12:20 - familiar with them
12:22 - we have all worked with tables of sql
12:25 - server
12:26 - but many of us may not have given much
12:30 - thought to how sql server actually
12:32 - stores the data inside the table
12:35 - typically
12:36 - table data in sql server is stored in
12:39 - the form of cluster index structure
12:42 - this means the data in the table will be
12:45 - stored in a tree like structure
12:47 - like you can see on the screen
12:50 - at the bottom of this tree
12:52 - we have the leaf nodes of the tree
12:55 - and this is where the actual data rows
12:57 - for the table are stored
13:00 - each of these leaf nodes is what is
13:02 - known as a page in sql server
13:05 - and the page is 8 kb in size
13:08 - depending on the size of the rows a
13:10 - table contains
13:12 - each page will generally store somewhere
13:14 - between 50 to a few hundred rows of data
13:17 - inside of it
13:19 - what is important to know is that the
13:21 - data in these bottom nodes of the tree
13:23 - is actually stored in sorted order
13:27 - and that sorted order is determined what
13:29 - is called the cluster key of the table
13:32 - by default the primary key of the table
13:35 - will be cluster key
13:38 - by having the data in sorted order in
13:41 - this bottom nodes sql server can
13:43 - maintain the tree structure like you see
13:45 - over the top of the data the nodes at
13:49 - the top of the page allow sql server to
13:51 - quickly traverse the tree and find the
13:54 - data it's looking for
13:56 - as we will see in example on the slide
13:59 - in the sample database i have a table
14:02 - named students
14:03 - and the primary key of the student table
14:06 - is the student id column
14:08 - so student id would be our cluster key
14:11 - in this case what that means is that
14:14 - down here in my leaf nodes where the
14:16 - data is stored my student data would be
14:20 - in sorted order by the student id
14:23 - so in this example the first leave node
14:26 - is going to be storing the data rows for
14:29 - students 1 to 100 the second node will
14:33 - be storing the rows for students 101 to
14:36 - 200 and so on
14:38 - in this top two levels of the tree we
14:41 - have a series of pointers that will help
14:44 - direct us to the data
14:46 - so let's say now that we want to find
14:49 - the row for student number 327
14:53 - we start up here at the root node and it
14:55 - will tell us that we need to go to the
14:58 - intermediate node that is leaf node that
15:01 - is in the second from the left
15:03 - and it is in the leaf node where we will
15:06 - find the data that we are looking for
15:09 - so you see you are able to traverse this
15:12 - tree in just three operations
15:14 - even for tables that have hundreds of
15:17 - thousands or even millions of rows in
15:19 - them
15:20 - by using a tree structure like this sql
15:23 - server is able to find the data it is
15:25 - looking for in a just few operations
15:27 - this tree structure that sql server uses
15:30 - for storing data is very efficient as
15:33 - long as we are looking up data by how
15:35 - the data is stored in the table
15:38 - in this case by the primary key which is
15:40 - a student id column
15:43 - however we often need to be able to look
15:46 - up the data by some other criteria say
15:49 - in this example we want to search for a
15:51 - student by their first name or last name
15:55 - and this creates a problem our data down
15:58 - here in the leaf nodes is sorted by
16:00 - student id
16:02 - but there is no correlation between a
16:04 - student's id number and their name
16:07 - students names will effectively have a
16:09 - random distribution throughout all the
16:11 - data pages
16:13 - at the bottom of the table
16:15 - so we have no idea where a particular
16:18 - name may be
16:20 - so what sql server will do in this case
16:23 - is perform a scan of the structure
16:26 - meaning it will read each and every one
16:29 - of these data pages here at the bottom
16:31 - of the cluster index structure looking
16:33 - for the data that matches your criteria
16:37 - for a table of any significant size
16:39 - though you will have hundreds or even
16:42 - thousands of these data pages
16:44 - in the bottom of the tree
16:45 - most likely many of this would need to
16:48 - be read of the disk so there is a
16:50 - significant i o cost and then sql server
16:54 - will have to use lot of cpu in order to
16:56 - read through each and every row
17:00 - what we can do though is we can create
17:02 - an index on our table
17:05 - though to help sql server finding that
17:07 - data more efficiently
17:09 - so let's discuss the index in the next
17:11 - video
17:12 - you are watching the video of module 1
17:14 - getting started part 6 index concepts
17:18 - a database index help us quickly find
17:21 - data in a table when we are searching
17:23 - for data by some other criteria other
17:26 - than the cluster key that our table is
17:28 - organized by
17:30 - for the example that we have just talked
17:33 - about searching for students by name
17:36 - we would want to create an index on the
17:38 - students table over the last name and
17:40 - the first name column
17:42 - as you see an index uses a tree
17:44 - structure just like our table
17:47 - but
17:48 - in this case the entries in the index
17:51 - are sorted by the index key
17:53 - which is in this case is the combination
17:55 - of the last name and the first name
17:57 - column
17:59 - and the index does not contain the
18:00 - actual data rows
18:02 - instead in the bottom nodes of the index
18:05 - that are represented
18:07 - and marked with the rp
18:10 - the index will contain row pointers back
18:12 - to the table which indicates where the
18:15 - corresponding data in the table is
18:17 - located
18:18 - if we look in the detail onto one of the
18:21 - bottom nodes in the index it would look
18:23 - something like this
18:25 - we see here that we have the key of the
18:27 - index the last name and the first name
18:29 - of the students
18:31 - and those are in sorted order
18:33 - and then we have the student id value
18:35 - for that student
18:37 - so for example
18:39 - if we have a sql statement with the
18:41 - where clause that you see on your screen
18:44 - where we are searching for this student
18:47 - by name sql server will first traverse
18:50 - the tree structure of the index to find
18:52 - the student id number
18:53 - and then it will traverse the tree
18:55 - structure of the table to locate the
18:57 - data for the student
19:00 - so there are actually two operations
19:02 - that must occur
19:04 - but each of this operation is very
19:06 - efficient
19:07 - and is usually turn out to be much
19:08 - faster to do these two operations
19:11 - traversing the tree structure
19:13 - then to scan through all the rows of a
19:16 - table
19:17 - we also can and often do have multiple
19:21 - indexes on a table in order to cover the
19:23 - different ways our application
19:26 - might search for data in that table
19:29 - so if we think about the students table
19:31 - we might want to find a student by the
19:33 - name as we just talk about or perhaps by
19:36 - their email addresses
19:38 - so in this case we would create two
19:41 - separate indexes on the students table
19:44 - each index being targeted in a way that
19:47 - our application searches for the data in
19:49 - the table
19:51 - what all of this means is that if your
19:54 - sql statement is using some criteria to
19:57 - locate data other than the primary key
20:00 - we are going to want that statement to
20:03 - use an index
20:05 - this way we can take the advantage of
20:07 - the tree structure of the index to
20:09 - quickly find the pointer to the data
20:11 - that we need and this is much more
20:13 - efficient and therefore much faster than
20:16 - having to scan through all the rows in
20:18 - the table from the next video we will
20:21 - start our next module which is module 2
20:24 - analyzing sql statements for performance
20:27 - in that we will see what our sql
20:29 - statements are doing in the sql server
20:33 - you are watching the video of module 1
20:35 - getting started part 7 summary of this
20:38 - module
20:39 - in this introductory module we talked
20:42 - about why it is important for you as a
20:45 - application developer to understand sql
20:47 - server performance
20:50 - almost every application written is
20:52 - going to use a database backend
20:55 - in some way
20:57 - and the database is almost always an
21:00 - important component of overall
21:02 - application performance
21:04 - since we as application developers
21:07 - understand what our application is
21:09 - trying to do
21:10 - when it runs a statement against sql
21:13 - server
21:14 - we can be very effective at database
21:16 - performance tuning once we know what to
21:19 - look for
21:20 - because we understand the larger context
21:23 - that all of our sql is running on
21:27 - we also covered the tools that you need
21:29 - to analyze sql server performance
21:33 - then we discuss how to get the sample
21:36 - database downloaded and installed so you
21:39 - can follow along throughout the course
21:43 - and finally
21:44 - we covered some basic concepts around
21:47 - how sql server stores data in tables and
21:51 - how indexes works
21:53 - so that we have a solid understanding of
21:55 - these fundamentals for the future
21:57 - modules in this course
21:59 - here
22:00 - we have successfully finished module 1
22:03 - getting started
22:05 - in the next module we are going to start
22:07 - looking at how to analyze the
22:09 - performance of individual sql statements
22:13 - you are watching the video of module 2
22:15 - analyzing sql statements for performance
22:18 - part 1 module introduction
22:21 - so you have a sql statement that is
22:24 - running slow
22:25 - or
22:26 - maybe you just have a statement that you
22:29 - aren't sure is optimized correctly
22:32 - and you need to know what to do next
22:36 - that is what this module is all about
22:38 - understanding how to analyze any sql
22:41 - statement to tell what they are doing
22:43 - and how they are doing to perform in a
22:45 - very quantitative way
22:47 - the first step in this process is to
22:50 - have sql statement give us an execution
22:53 - plan
22:54 - which is the detail steps of how sql
22:56 - server will run our statement when we
22:59 - submitted it to the database
23:02 - after that we will understand how to
23:04 - read the execution plan that sql server
23:06 - gives back to us
23:09 - and how we make sense of what sql server
23:11 - is telling us from a performance point
23:13 - of view
23:15 - we will also see how sql server
23:17 - calculates a cost value for each
23:19 - statement
23:20 - and in fact for each operation within a
23:23 - statement
23:25 - and by looking at these cost values we
23:27 - will tell what statements and operations
23:30 - are relatively more expensive than
23:32 - others
23:33 - we also see how sql server can give us
23:36 - execution statistics on our statements
23:39 - and this means the amount of cpu and io
23:43 - that the statement is consuming
23:45 - what all of this allows us to do is to
23:48 - understand in a very quantitative way
23:50 - how expensive
23:52 - a sql statement is
23:55 - and then when we started tuning that
23:57 - statement we have some very good metrics
24:00 - in place to judge just how much we have
24:03 - improved
24:04 - the performance of this statement
24:06 - we are also going to see little bit
24:08 - about how it is sometimes possible to
24:11 - improve the performance of a sql query
24:13 - just by changing the way that you write
24:16 - your sql
24:18 - and i will show an example of this
24:21 - and finally one of the keys to nla sql
24:24 - statements for performance is to
24:26 - understand how sql server is processing
24:28 - your query
24:30 - so i will briefly talk about the most
24:32 - common operations that you will see in
24:35 - the execution plan and highlight the
24:37 - ones that you especially want to watch
24:39 - out for
24:41 - so let's start right into the example of
24:44 - a sql query
24:45 - that isn't performing quite the way that
24:48 - it should be
24:49 - and see how we work through analysing
24:52 - and fixing this statement in the next
24:54 - video
24:55 - you are watching the video of module 2
24:57 - analysing sql statements for performance
25:00 - part 2 understanding how sql server will
25:03 - execute a sql statement
25:06 - i have a sql select statement here in my
25:08 - editor window that we are going to use
25:11 - and see the concepts around execution
25:14 - plan and how to analyze a sql statement
25:18 - you can see the output of this statement
25:21 - in the result pane
25:23 - and
25:24 - what this statement is doing is
25:26 - selecting all of the courses that a
25:28 - particular student has taken with a
25:30 - title of the course
25:32 - the number of the credits
25:34 - and the grade that the student received
25:37 - to get this information we need to join
25:39 - three tables
25:41 - we need to query the course enrollments
25:43 - table to get what courses the student
25:47 - has taken and what grade they received
25:50 - and then we will need to join through
25:52 - the course offerings table to get to the
25:54 - courses table so we can get the course
25:57 - number and course title and credits
26:01 - so you might imagine a query like this
26:04 - being run on a web page that produce a
26:06 - grade report for an individual student
26:09 - so i am going to go ahead and run this
26:12 - query
26:13 - and we do see that it takes second to
26:16 - run
26:18 - so maybe you have identified this query
26:21 - as something you need to take a look at
26:24 - and we want to figure out how we can
26:26 - make this query run faster
26:30 - so the first step in that process is to
26:32 - have sql server give us the execution
26:34 - plan
26:35 - that it's using to process the query
26:39 - and to do that we are going to go up to
26:42 - this button on our toolbar with the
26:45 - three boxes and a little icon in it
26:48 - and this is labeled display estimated
26:50 - execution plan
26:52 - and so as you can see if i mouse over
26:55 - over that and the tool tips comes up
26:58 - you can also use a keyboard shortcut of
27:01 - control plus l
27:03 - now clicking this button is not going to
27:06 - actually execute the statement that is
27:08 - in the window
27:09 - it just going to send the statement to
27:12 - sql server to our sql server to return
27:14 - back to us the plan that it would use to
27:17 - run the statement
27:18 - so let's go ahead and do that
27:22 - and so now down here
27:24 - we see in our result pane instead of the
27:26 - data coming back for this query
27:28 - we see a graphical representation of the
27:30 - execution plan
27:33 - what sql server has done is it taken our
27:36 - statement and it broken it down into the
27:38 - individual steps that sql server is
27:41 - going to have to perform when it
27:43 - executes this statement
27:45 - and each of this step is represented by
27:48 - one of this icon that you can see down
27:51 - here in the bottom
27:53 - now you might have a question that how
27:56 - did sql server arrive at this plan
27:58 - well
27:59 - it looked at the tables our statement is
28:02 - accessing and any index that are on
28:04 - those tables and what we are trying to
28:07 - do in our statement
28:09 - and
28:10 - it very quickly compared all of the
28:12 - different options for how our statement
28:14 - could be processed
28:16 - and it determined that this set of
28:18 - operation in this order was the most
28:21 - efficient one
28:22 - when we have a statement that we don't
28:25 - think is performing the way that it
28:27 - should
28:28 - the first thing that we want to do is we
28:30 - want to pull up the execution plan like
28:33 - this
28:34 - and understand what step sql server is
28:36 - performing
28:38 - and if
28:39 - any of these steps are inefficient and
28:41 - time consuming
28:44 - if there is anything we can do about
28:46 - those steps to affect them or affect the
28:49 - overall plan and get them to be more
28:51 - efficient
28:53 - this is just like if you are
28:54 - troubleshooting a slow code in a
28:56 - language like c sharp or java
28:59 - you would go and find out what steps are
29:01 - executed in that program
29:04 - and figure out if any of those steps was
29:07 - bottlenecked or could be executed faster
29:11 - and what you could do speed things up
29:15 - and really the same thing that we are
29:17 - going to do here
29:19 - so in the next video we are going to
29:21 - understand how to read and understand
29:23 - the execution plan that sql server has
29:25 - given us
29:27 - and that's given us some clue about how
29:30 - we can make this query run faster
29:33 - you are watching the video of module 2
29:35 - analyzing sql statements for performance
29:39 - part 3 reading and interpreting an
29:41 - execution plan for an sql statement
29:45 - let me zoom in on the execution plan to
29:48 - make this plan easier for us to read
29:51 - to read an execution plan we want to
29:53 - work from right to left and from top to
29:56 - bottom
29:58 - you can think of this as though the
30:00 - operations in the right are getting
30:02 - executed first
30:04 - and then as we move to the left this
30:06 - intermediate operations will build upon
30:09 - the already completed operations
30:11 - and eventually we will get to this upper
30:14 - hand corner
30:15 - which represents our complete statement
30:19 - actually this isn't quite hundred
30:21 - percent accurate
30:23 - since sequel server will parallelize
30:26 - parts of our statement for us
30:28 - such that
30:30 - things aren't strictly sequential
30:33 - but for analysis purpose it's easiest if
30:36 - you think of things happening serially
30:38 - starting from the right and moving to
30:40 - the left
30:41 - so we will start out on the right
30:44 - with this cluster index scan operation
30:47 - and we can see that on the second line
30:50 - this operation is occurring against the
30:52 - course enrollment table
30:54 - we can also see on the third line
30:57 - what percentage of the total cost of the
30:59 - query this particular operation accounts
31:01 - for
31:02 - and we see that this is 91 percent in
31:05 - this case
31:07 - so that immediately signals to us that
31:09 - when we get to the point of tuning the
31:11 - statement
31:13 - this is an operation that we really
31:14 - going to want to take a look at
31:18 - in this case i will go through each
31:20 - operations in the execution plan
31:23 - so you can get a feel for how to read
31:25 - these plans that sql server gives you
31:29 - but a lot of times what you will do is
31:31 - you will bring up execution plan and
31:34 - look for the operations that are
31:35 - accounting for the highest percentage of
31:37 - the overall statement cost
31:39 - and immediately focus it on those so
31:42 - this cost percentage can really serve as
31:44 - a quick filter of what you are looking
31:46 - for if i mouse over any operation i will
31:49 - get a pop-up tool tip that going to give
31:52 - me some additional details information
31:54 - on that particular operation
31:56 - so we see at the top of the tooltip we
31:59 - have the name of the operation again
32:01 - below that we have a little bit of
32:03 - description of the operation that sql
32:05 - server give us in this case we want to
32:08 - remember that a cluster index structure
32:10 - is a typical way that data in a table is
32:13 - stored in sql server so the word scan
32:16 - indicates that sql server is reading the
32:19 - entire cluster index
32:21 - so what that means is that in this
32:23 - operation sql server is reading each and
32:26 - every row in the course enrollments
32:28 - table
32:29 - and that's a table that has 37 lakh rows
32:32 - in it when you are reading execution
32:34 - plans
32:35 - this is actually something that you want
32:38 - to look for is if you have any cluster
32:41 - index scan operations on tables that you
32:43 - know contains a lot of data because what
32:47 - a cluster index scan really means is
32:49 - that you are reading all of the rows in
32:51 - that table and for large tables that of
32:54 - course is going to be very expensive and
32:56 - takes a lot of time
32:58 - so knowing this that we are processing
33:00 - 37 lakh rows in this operation we have a
33:03 - good idea already of why this particular
33:06 - operation is accounting for 91 percent
33:09 - of the cost of this statement
33:11 - now you might ask what rows are we
33:13 - trying to find in this table
33:16 - and that can be answered by looking at
33:18 - the predicate data on this pop-up
33:21 - which is down here at the bottom
33:23 - so you see our where clause for the
33:26 - statement and what is happening is that
33:28 - we are trying to find out the rows for
33:30 - this particular student
33:32 - but as we just talked about
33:34 - we are having to look through all of the
33:36 - rows in the table to do that
33:39 - we can also get an estimate of how many
33:41 - rows sql server expect to find in this
33:43 - operation and that's given by estimate
33:46 - number of rows value which is shown up
33:48 - here the way that sql server calculate
33:51 - this is because sql server keeps some
33:53 - statistics on each table
33:56 - in the database and these are statistics
33:58 - like how many rows are there in the
34:00 - table and what the rough distribution of
34:03 - data in that table is
34:05 - sql server uses that information in
34:07 - order to form its execution plan and
34:10 - that's also helpful to us here
34:12 - because it tells us that sql server is
34:15 - only expecting to find 45 rows even
34:18 - though we know that it reading a very
34:20 - large number of rows from this table the
34:23 - final item i want to talk about in this
34:25 - pop-up are these items that are labeled
34:27 - with the word cost and you can see that
34:30 - there are four of them here cost is just
34:33 - a value that sql server used to express
34:35 - how expensive a particular operation or
34:37 - entire statements is compared to another
34:40 - statements or operation
34:42 - as you see here there is a cpu cost
34:45 - which reflects how much cpu this
34:47 - operation is going to use and an i o
34:49 - cost reflecting how expensive the io is
34:52 - for this operation
34:54 - so basically what sql server has done is
34:57 - it's taken the amount of cpu it thinks
35:00 - this operation will consume and the
35:02 - amount of io it's taken those value and
35:05 - its normalized those amount against a
35:07 - scale
35:08 - such that
35:10 - it can add each of those values together
35:13 - and get a single value which represents
35:15 - how expensive this operation is
35:18 - and that's what we see on this estimated
35:21 - operator cost line
35:23 - now we also get a cost for an entire
35:26 - statement
35:27 - which if we go over here and we look at
35:30 - the select in the upper hand left corner
35:32 - we get we see that the estimate subtree
35:35 - cost is 12 here
35:38 - and we can use this value so that we can
35:40 - compare the relative expense of running
35:42 - this statement versus other statement
35:44 - that we have
35:46 - and that helps guide us about what are
35:48 - our most expensive sql statement and
35:52 - which statements that we should tune
35:54 - first
35:55 - so let's go ahead and continue to read
35:58 - this execution plan and understand what
36:00 - is going on in it
36:02 - so we just read the course enrollments
36:04 - table and we got out the records for a
36:07 - particular students in there
36:09 - and then
36:10 - if we come over here to the left we see
36:13 - this step of parallelism if you are
36:16 - unaware of the concept parallelism
36:18 - then i would like to give you a brief
36:20 - explanation about it
36:22 - parallelism is a concept in which the
36:25 - big task is divided into the small task
36:27 - that runs simultaneously and create
36:30 - desired result and finally that partial
36:32 - result of each small task combined into
36:35 - the final big task
36:37 - this concept of
36:38 - parallelism helped to run queries much
36:41 - faster
36:43 - ok
36:44 - now we proceed further in the left
36:47 - and now we can see the nested loop joint
36:51 - and we can mouse over this
36:53 - and from the description it says that we
36:56 - are going to take each row in the upper
36:58 - data set and we are going to prop or
37:01 - investigate the lower data set
37:03 - in order to get that information
37:06 - so
37:06 - this is what is doing our join between
37:09 - these two tables
37:11 - if we go back over here
37:13 - we see that this is the course offering
37:15 - table
37:16 - which is down here on the bottom
37:19 - now this is a cluster index seek
37:21 - operation
37:22 - so it means what sql server is doing is
37:26 - it's actually using the tree structure
37:28 - of the cluster index in order to find
37:30 - the data
37:31 - and as you see here the cost is just one
37:34 - percent
37:35 - and if we mouse over this
37:38 - both of our cost values are very very
37:40 - low for this operation
37:42 - but basically what these three
37:44 - operations here are doing together is
37:46 - finding the data it needs in course
37:48 - enrollments and then in joining that
37:51 - over to course offerings so now we have
37:54 - an intermediate data set of those two
37:56 - table combined in the next step here in
37:59 - nested loops sql server will join the
38:02 - previous data set of course enrollment
38:04 - and course offerings and the result data
38:06 - set after fetching data from the course
38:08 - table
38:09 - and in that it again perform parallelism
38:12 - to generate result
38:14 - so finally we get up here to the select
38:17 - box
38:18 - and this represents our entire statement
38:21 - we already looked at this
38:23 - and
38:24 - this has our entire cost inside of it
38:27 - and also gives us an estimate number of
38:29 - rows
38:30 - now we know that we actually are getting
38:32 - more rows back
38:34 - again
38:35 - the statistic data is just an estimate
38:38 - the sql server has
38:40 - but this is basically the procedure that
38:43 - you use to read an execution plan what
38:46 - you are going to want to do is move from
38:49 - right to left and then when you
38:51 - encounter a join operation
38:53 - you want to read those
38:55 - from top to bottom
38:58 - you can see what each operation is doing
39:01 - and how it fits it with the operations
39:04 - around it
39:05 - and if you want to see more details
39:08 - then you have to mouse over one of the
39:09 - operations so you can get that pop-up
39:12 - box
39:13 - this is my suggestion to you that you
39:15 - don't have to analyze each and every
39:17 - operation but instead you can just focus
39:20 - on the high cost operations
39:22 - and that helps you understand where you
39:24 - want to focus your tuning efforts
39:27 - in the next video i will demonstrate one
39:29 - additional technique for analyzing a
39:31 - statement and that is how we get the
39:34 - amount of cpu and io that a statement
39:38 - uses when you actually run the statement
39:41 - so we will do that next
39:44 - you are watching the video of module 2
39:46 - analyzing sql statements for performance
39:50 - part 4 getting execution statistics for
39:53 - a sql statement
39:55 - technique that you will find useful is
39:58 - to use the set statistics commands in
40:00 - management studio
40:02 - so that you can get detailed i o and cpu
40:06 - statistics about the statements being
40:08 - run to do this we are just going to run
40:11 - two commands in our sql editor window
40:14 - set statistics io on
40:17 - and set statistics time on and then i am
40:21 - going to go ahead and execute both of
40:24 - these commands
40:26 - so now when i run a statement i am going
40:29 - to get some detailed information back in
40:31 - the messages tab
40:33 - down here in the bottom in the result
40:34 - pane
40:36 - so just to be clear to get this detailed
40:39 - cpu and io data back
40:41 - you do need to actually execute your
40:43 - statement
40:45 - so unlike in the last segment where we
40:47 - looked at where getting an estimate
40:49 - execution plan
40:51 - didn't actually run the statement
40:53 - this is actually going to run the
40:55 - statement again sql server
40:58 - so
40:59 - you want to be careful with any dml
41:01 - statements like update or deletes
41:04 - because those of course would actually
41:07 - be modified the data in your database
41:10 - in this case i am just running a select
41:12 - statement
41:13 - so i can run that like a normal one
41:18 - so i will go ahead and do that now
41:21 - i will make sure that we get the entire
41:23 - statement here
41:25 - and i will execute this statement
41:27 - so now you see that here is our results
41:31 - and that actually did run a little bit
41:33 - quicker
41:34 - and so the reason that ran quicker is
41:36 - because sql server actually has cached
41:39 - the data that this query needs in memory
41:43 - but as we are going to see
41:45 - if we click over here on the message tab
41:48 - this statement is still pretty
41:50 - inefficient
41:51 - so there are some tuning opportunities
41:54 - present there
41:55 - so let's take a look at that
41:58 - so the first set of items which is right
42:00 - here tells us how long sql server took
42:04 - to create an execution plan for our
42:06 - statement
42:08 - remember the last part where we talk
42:10 - about that when sql server gets a sql
42:13 - statement submitted
42:15 - it has to break the statements up into
42:17 - the individual operations that is going
42:19 - to run
42:20 - and this is the amount of time that it
42:22 - took
42:23 - in this case sql server already has an
42:25 - execution plan cached in memory that it
42:28 - would use so this actually didn't take
42:31 - any time at all in this case
42:34 - next we have some io information for the
42:36 - database object sql server has to read
42:39 - in order to process our statement
42:42 - as we see there were three tables in
42:45 - this case the data needed to be read
42:47 - from
42:48 - and the important number that we want to
42:50 - pay attention to here are these logical
42:54 - reads
42:55 - a logical read is when sql server has to
42:58 - read a page either from memory or disk
43:01 - in order to process the statement
43:03 - so it really helps to indicate how much
43:06 - data is having to be processed in order
43:08 - to run the statement
43:10 - we see here that the course enrollment
43:13 - stable that was over 12 000 logical
43:16 - reads
43:17 - that had to occur
43:18 - and this is because as we saw in the
43:21 - last segment sql server is reading the
43:23 - entire table to find the data that we
43:26 - need
43:27 - each logical read is a page of sql
43:30 - server and remember a page is 8 kb in
43:33 - size
43:34 - so 12 000 times 8 kb we can see that's
43:38 - quite a bit of data that sql server is
43:41 - having to process through here
43:43 - if we move down to the last section it
43:46 - gives us the amount of cpu it took to
43:49 - actually execute the statement
43:51 - and how long that process took now
43:54 - sometimes you will see in this section
43:57 - an amount of cpu that is larger than the
44:00 - elapsed time
44:01 - and what's happening in those cases is
44:03 - that sql server has used some parallel
44:06 - execution for at least part of your
44:08 - statement
44:09 - as such
44:10 - since your cpu on multiple cores
44:14 - we can use more cpu time than what it
44:16 - actually took for the statement to run
44:20 - overall
44:21 - these commands do gives us a good
44:22 - picture of the amount of resources up on
44:25 - the database server that are being used
44:27 - by this statement
44:29 - of course what we are aiming for is that
44:32 - for each one of our statement we want to
44:34 - minimize this number and so that i
44:37 - myself do
44:38 - is i really focus on this number of
44:40 - logical reads
44:42 - a high number of logical reads often
44:44 - indicates a statement is inefficient
44:48 - because it's reading a bunch of data
44:50 - that ultimately it's going to filter out
44:52 - and throw away in some other operation
44:55 - in the statement
44:56 - i also find that when you are reading a
44:59 - lot of data in terms of a lot of logical
45:01 - reads
45:02 - that tends to consume a lot of cpu and
45:05 - ultimately those statements take longer
45:07 - to process
45:09 - so between the execution plan we saw in
45:12 - the last part and this execution
45:14 - statistic we have a pretty good idea of
45:16 - what's going on in this statement
45:19 - so
45:20 - in the next video we are going to
45:22 - actually start working on this and
45:24 - tuning this statement to make it run
45:26 - more efficiently
45:28 - you are watching the video of module 2
45:30 - analyzing sql statements for performance
45:32 - part 5 improving statement performance
45:35 - by adding an index
45:38 - i bought the execution plan for our sql
45:40 - statement back
45:41 - and by now we realized it this operation
45:45 - here the cluster index scan that is
45:47 - responsible for most of the cost of the
45:49 - sql statement
45:51 - as we discussed this operation is
45:53 - reading all 37 lakh rows which is 12 000
45:57 - pages from the table
45:59 - just to find out the handful of rows
46:01 - that we need for one student
46:04 - so there is a lot of waste here
46:06 - so
46:07 - what we can do now
46:09 - well actually you have probably noticed
46:11 - that sql server has given us a hint and
46:14 - it's telling us that we really need to
46:17 - create an index on this table
46:20 - so i am going to right click on this
46:22 - recommendation and go to the missing
46:25 - index details so that we can see exactly
46:28 - what is telling us
46:29 - in this case the suggestion is to create
46:32 - an index on the student id column in the
46:34 - course enrollments table
46:36 - and that exactly that we want to do if
46:39 - we look back at our statement we are
46:41 - using the student id column as a filter
46:44 - so we can find just one particular
46:46 - student and if we think about this table
46:49 - in the larger context of our application
46:51 - and how it might be used it is going to
46:54 - be pretty typical use case that we have
46:57 - a student id and we want to find out of
47:00 - all the courses that a student is
47:02 - enrolled in so creating an index on the
47:04 - student id column of the course
47:06 - enrollments table will allow us to
47:09 - quickly search for course enrollments by
47:11 - student id
47:13 - and that's make a lot of logical sense
47:15 - so let's go ahead and get this index
47:18 - created
47:19 - so i just need to uncomment this
47:22 - and then go up here and i will have to
47:25 - give this index a name
47:30 - and this is just how i like to name
47:33 - indexes
47:35 - you actually can give it valid name that
47:38 - you want to
47:40 - and then i am going to go ahead and
47:42 - create this index
47:45 - and so that will take a few seconds here
47:48 - and there we go
47:50 - and so
47:51 - now let's head back to our sql statement
47:54 - and we will get a new execution plan and
47:56 - see how things have changed
47:58 - okay
47:59 - we can see out here that the cluster
48:02 - index scan operation has been replaced
48:04 - with an index seek
48:06 - and then a key lookup
48:08 - so
48:09 - if we go and we look at this here
48:12 - we see the seek predicate now is on the
48:14 - student id
48:16 - so
48:16 - we are using the index to look up the
48:18 - students and notice the cost of this
48:21 - operation
48:22 - it's down there around 0.003
48:26 - it looks like
48:27 - so this is much much cheaper than the
48:30 - 10.92 value that we saw before now as we
48:35 - talk about in the module 1 part 6 at
48:37 - this course
48:38 - all this does here it lookups just the
48:41 - information in the index
48:43 - and then we have to go to the table to
48:46 - get the actual data for those rows
48:49 - so
48:50 - that's what this key lookup is here and
48:53 - it's doing that with a nested loop join
48:56 - and so this is what actually going up
48:58 - and looking up the data in the table
49:00 - so that we can get the data to come out
49:03 - here
49:04 - so we can see that the rest of the plan
49:07 - looks basically the same if we do look
49:10 - at the overall query though we see that
49:13 - our estimate cost now is around 0.207
49:17 - right in here
49:18 - remember before it was way up over 12.01
49:22 - so we have seen a significant saving
49:25 - here
49:26 - so
49:27 - i am going to go ahead and i am going to
49:29 - run this query again
49:31 - and i still have the statistics turned
49:34 - on
49:34 - so we can see the number of i o and the
49:37 - amount of cpu that it's using
49:40 - so i will run this and look at the
49:42 - message tab and you can see here first
49:45 - of all on logical rates before from the
49:48 - course enrollments table we had over 12
49:50 - 000 logical reads
49:52 - and now we are down to 134 only
49:55 - so by using that index we are traversing
49:58 - the tree structure of the index
50:00 - we are doing much much less work before
50:04 - and that's accounting for our statement
50:06 - being more efficient
50:07 - you see down here at the bottom of the
50:09 - sql server execution times we are
50:12 - actually showing zero both for cpu
50:15 - and ellipse time
50:17 - in reality it's probably isn't zero cpu
50:20 - time or zero ellipse time
50:22 - it just below the measurement threshold
50:25 - so again now we have a very efficient
50:27 - statement
50:28 - so i am going to take this result and
50:31 - put them on a slide so we can discuss
50:33 - them
50:35 - i have summarized the result on this
50:37 - slide
50:38 - and as you can see the improvement is
50:40 - dramatic
50:41 - now you might be thinking that the
50:43 - elapsed time only improved by
50:45 - approximately 100 milliseconds
50:48 - but remember i am the only user on my pc
50:52 - running sql server on an actual
50:54 - production system you may have hundreds
50:56 - of user running statement at a time
50:59 - and you may also be dealing with the
51:01 - data sets that are much much larger than
51:03 - what i have in my sample database
51:07 - so on an actual production system the
51:10 - efficiency of the first statement would
51:12 - certainly result in a less scalable
51:14 - system and would probably also result in
51:16 - the longer wait times for users
51:19 - as this inefficient query used up all
51:22 - the resources causing contention on the
51:24 - server
51:25 - i have actually seen this before where a
51:28 - single query that was doing a full table
51:30 - scan brought an entire system down
51:33 - because the query was run often enough
51:36 - to consume all the cpu on the database
51:38 - server which then slow down every other
51:41 - sql statement in the application
51:44 - because those statements were having to
51:46 - wait to get on the cpu
51:49 - so it is really a key to keep all the
51:51 - sql statements in our application
51:54 - running as efficiently as possible
51:56 - especially for statement that are
51:58 - frequently run in our application
52:01 - with that in mind let's look at another
52:03 - sql statement that we need to tune in
52:06 - the next video
52:07 - but in this case the answer is not to
52:10 - add an index
52:11 - you are watching the video of module 2
52:14 - analyzing sql statement for performance
52:17 - part 6 rewriting sql statements for
52:20 - improved performance
52:22 - i have another statement in my editor
52:24 - window and this statement is designed to
52:26 - get all of the courses section that no
52:29 - students are sign up for so
52:32 - you might imagine a university running a
52:35 - query like this at the start of a
52:37 - semester to find out any courses section
52:40 - that need to be cancelled
52:42 - to accomplish this this query is doing a
52:45 - left join
52:47 - and then looking for any course
52:48 - enrollment records that are null
52:51 - because that indicates that no one
52:53 - signed up for the class
52:56 - we can see the result in the lower pane
52:59 - and we see that we do have a couple of
53:01 - sections then no one has signed up for
53:05 - what we are really interested is the
53:07 - execution plan for this query so let's
53:10 - take a look at that
53:12 - if i mouse over the overall query here
53:14 - we will see that there is an
53:17 - overall cost of 8 and if we look at the
53:20 - execution plan
53:21 - it looks like that cost is being driven
53:24 - by the index seek lookups occurring
53:26 - against the index in the course
53:28 - environments table down here at the
53:30 - bottom
53:31 - so what is happening in these three
53:33 - operations here is that sql server is
53:36 - reading all of the offerings for the
53:38 - courses in a current semester
53:40 - and then through this nested loop join
53:43 - it is probing an index on the course
53:45 - enrollments table
53:48 - ix course enrollments course offering id
53:52 - and looking up how many enrollments
53:54 - there are for that course
53:57 - the issue here is that sql server is
53:59 - looking for all the enrollments for each
54:01 - course
54:03 - when we really only need to know if
54:05 - there is any student enroll in the
54:07 - course
54:08 - since we are looking up all the
54:10 - environments these operations turns out
54:12 - to be pretty expensive
54:14 - making our whole query pretty expensive
54:17 - now in this case the answer is not an
54:20 - index
54:20 - but instead for us to change how we
54:23 - write this query
54:25 - so let's take an alternate approach
54:29 - this version of the query uses the not
54:31 - exist clause to look for any code
54:33 - offerings that don't have any records in
54:36 - the course environments table so
54:38 - let's go ahead and run this version of
54:40 - query
54:42 - and we see we get the same answer as
54:45 - before
54:46 - so now let's take a look at the
54:47 - execution plan
54:49 - and we see
54:51 - if we mouse over this query
54:54 - that our overall cost for this version
54:56 - of this statement is quite a bit lower
54:59 - down around 0.85
55:02 - and if we look at the entire plan
55:04 - we notice this top operation here
55:07 - and if we mouse over this
55:10 - we see the description tell us that we
55:12 - are just going to return the first few
55:14 - rows from the previous operation
55:16 - and looking down at the bottom we see
55:19 - that the expression is top one
55:21 - so it is actually just getting the first
55:24 - row from the index seek operation which
55:26 - is over here to the right
55:28 - so what this means is that sql server is
55:31 - reading all of the course offerings just
55:33 - like it was doing before
55:36 - but
55:36 - then in the nested loop join what it is
55:39 - able to do is able to prop the index
55:42 - looking for any single row that matches
55:44 - the course offering id number
55:47 - so sql server doesn't have to read the
55:49 - data for every student enrolled in the
55:51 - course
55:52 - it is just looking to see if at least
55:55 - one student is in the course and that's
55:58 - more efficient than getting the data for
56:00 - each and every student android
56:03 - so think of this like you are searching
56:05 - an array of values
56:07 - if you can jump out of this search when
56:09 - you find any value that matches
56:11 - what you are searching for
56:13 - that's more efficient than having to
56:15 - find every value that matches
56:18 - because once you find the first matching
56:20 - value of course you can jump out of the
56:22 - search functions and that's essential
56:25 - what's happening here
56:28 - now i also want to point out that sql
56:31 - server is giving me a create index
56:33 - recommendation here
56:34 - but in this case i am actually choosing
56:37 - to ignore this suggestion
56:38 - the recommendation here is basically sql
56:41 - server wants me to create an index on
56:43 - the course offerings table that contains
56:46 - every column of the
56:47 - table
56:49 - so
56:50 - effectively i will be duplicating all of
56:52 - the data in the table
56:54 - we will talk in the upcoming module
56:57 - about why this is not a good idea but i
57:00 - want to point out this out so that you
57:02 - understand when sql server gives you
57:04 - this recommendation
57:06 - you want to be sure and analyze
57:09 - what sql server is telling you
57:11 - and that it makes sense and don't simply
57:14 - create an index because sequels are made
57:16 - in a recommendation
57:18 - the main point of the two queries we
57:20 - just looked at
57:22 - though
57:23 - is that something the way to improve the
57:25 - performance of sql statement is to
57:27 - rewrite the statement in a different way
57:30 - a sub query can be written as a join or
57:33 - you might change a join to a sub query
57:36 - and as we just showed sometimes using
57:39 - the where exist or where not exist
57:42 - clause can improve a plan because sql
57:45 - server can satisfy the condition with
57:47 - any record that matches the condition
57:50 - instead of having to read all matching
57:53 - records from a table
57:56 - so keep these options in mind when you
57:58 - tune your sql statements
58:00 - in many cases in the modern version of
58:02 - sql server the query optimizer is smart
58:05 - enough to find the best execution plan
58:08 - regardless of how you write your
58:10 - statement
58:12 - but there are few cases where writing
58:14 - your statement a different way will help
58:17 - the query optimizer find a more
58:18 - efficient plan
58:20 - so keep this option in mind
58:22 - so in the next video i am going to
58:24 - summarize the common execution plan
58:27 - operations for better understanding
58:30 - so let's jump ahead
58:32 - you are watching the video of module 2
58:34 - analyzing sql statements for performance
58:37 - part 7 common execution plan operations
58:41 - we have looked through a couple of
58:43 - execution plan so i want to summarize
58:46 - some of the common operations that you
58:47 - are going to see
58:49 - when you are accessing the data either
58:51 - in a table or in an index these are the
58:54 - typical data access operations you are
58:56 - going to see
58:57 - i am not going to read through each one
59:00 - of these
59:01 - but i do want to point out there are
59:03 - really two different types of operations
59:05 - that are listed here
59:07 - and those are scan operations and seek
59:09 - operations
59:11 - what you need to remember is that a scan
59:13 - operation means that sql server is
59:15 - reading the entire data structure that
59:18 - is acting on
59:19 - so either an entire table or an index
59:22 - and that is of course going to be
59:24 - expensive for any table or index that
59:26 - has a large amount of data in it
59:29 - seek operations on the other hand use
59:31 - the tree structure that the data is laid
59:34 - out in
59:35 - so they can be very efficient to find
59:37 - the data that they are looking for
59:40 - the takeaway is when we are looking at
59:42 - an execution plan
59:44 - and we see scan operations on large
59:47 - table we want to ask why a scan
59:49 - operation is being used and how we can
59:52 - get sql server to change over to using a
59:54 - seek operation
59:56 - this slide summarize the join operations
59:58 - that you will see sql server perform
60:01 - we have seen the nested loop joined
60:03 - quite a bit and the implementation is
60:05 - exactly like the name implies
60:07 - mud join tends to be used more
60:09 - infrequently because the data for both
60:12 - data sets need to be in a sorted order
60:14 - for sql server to use the join
60:16 - if the data for one of the data sets is
60:19 - not already sorted correctly
60:21 - then sql server will have to perform a
60:23 - sort operation first
60:25 - and then perform the join
60:27 - if the data is already sorted mud join
60:30 - tends to be very efficient but if sql
60:33 - server has to first sort the data to use
60:35 - this operation then one of the outer
60:38 - join operations tends to be more
60:40 - efficient
60:41 - the last type of join is the hash match
60:44 - which is sometimes referred to a hash
60:46 - join
60:47 - this is used when sql server needs to
60:49 - join two large data sets and there isn't
60:52 - already some sort of key
60:54 - like an index key that can be used to
60:57 - join the data sets together
61:00 - so what sql server does is it builds a
61:02 - hash table on a smaller data sets on a
61:05 - join key and then iterates through the
61:07 - larger data sets probing the hash table
61:10 - for matching values
61:11 - typically when you see a hash match join
61:14 - it is relatively expensive operation
61:17 - build a hash table in memory takes time
61:20 - and it can take a lot of
61:22 - memory plus as we said sql server will
61:26 - only use a hash match when large data
61:28 - sets are involved
61:30 - so when you see a hash match join you
61:33 - want to ask yourself why a hash match
61:35 - join is required
61:37 - first is that any way to narrow down the
61:39 - data set that are being joined together
61:42 - so that not as much as data need to be
61:44 - joined
61:45 - for example you might look to see if you
61:48 - can include a more restrictive where
61:50 - clause in your statement to cut down on
61:52 - the size of the data that need to be
61:54 - joined
61:55 - second a hash match join indicates there
61:57 - is no existing key like an index key
62:01 - that can be used for join
62:03 - many times this indicates that you are
62:05 - missing an index on one of the foreign
62:07 - relationship between your tables
62:10 - so adding an index may be appropriate
62:13 - again in the next module we will be
62:16 - talking quite a bit about creating good
62:18 - indexes including what columns in a
62:20 - table you want to ensure to index and
62:23 - this usually includes any foreign key
62:25 - columns that your table has
62:27 - there are many operations an execution
62:30 - plan can contain but covering each and
62:32 - every one would take a very long time
62:34 - and lead to some very boring video for
62:36 - you to watch
62:38 - if you do encounter an operation that
62:40 - you need to understand more about that
62:43 - what is provided in the description that
62:45 - sql server management studio gives to
62:47 - you
62:48 - then i would suggest you to take a look
62:51 - at this book on sql server execution
62:53 - plans you can use the url shown on the
62:56 - screen
62:57 - and i will include this url in the
62:59 - description box for your convenience
63:02 - and as you see the ebook can be
63:04 - downloaded for free
63:05 - and you can actually don't ever have to
63:07 - register it
63:09 - this is a direct download link here
63:11 - the book contains a lot of material on
63:14 - how to read execution plan as well as
63:16 - detail information on most of the
63:18 - operation that can be contained within
63:20 - an execution plan
63:22 - as a developer you are probably not
63:24 - going to sit down and read this book
63:26 - from cover to cover but it is a useful
63:28 - reference to have around for the times
63:31 - that you don't quite understand why
63:33 - something is working the way you think
63:37 - that it should and need a little bit
63:39 - more in-depth explanation of how an
63:42 - operation work so with that let's go
63:44 - ahead and summarize everything we have
63:46 - learned in this module and wrap things
63:48 - up you are watching the video of module
63:51 - 2 analyzing sql statements for
63:53 - performance part 8 module summary
63:57 - in this video let's summarize what we
63:59 - have discussed in this module about
64:01 - analyzing sql statements for performance
64:05 - when you have a statement you need to
64:07 - analyze the first step you are going to
64:09 - take is to get execution plan of that
64:12 - statement
64:14 - getting the execution plan doesn't
64:16 - actually run the statement
64:17 - but just tell us how sql server will
64:20 - process the statement so we can analyze
64:22 - the plan for any performance bottlenecks
64:26 - once you get the plan you want to read
64:28 - the plan from right to left and when you
64:30 - encounter a join operation you have to
64:33 - read from top to bottom
64:35 - so you analyze the operation at the top
64:37 - of the join first and then the operation
64:40 - on the bottom
64:41 - the statement in each individual
64:44 - operation is going to have a cost value
64:46 - assigned to it
64:47 - and cost is simply a single value that
64:50 - combines together both the amount of cpu
64:53 - and are you required by the statement or
64:55 - the operation
64:57 - with any statement we want to focus our
64:59 - attention on the operation that have
65:01 - relatively high cost
65:03 - because a high cost means that those
65:06 - operations are consuming lot of
65:07 - resources and consequently that will
65:10 - take longer time to complete
65:12 - at a statement level we can use the cost
65:15 - value to compare the relative expense of
65:17 - executing different statements within
65:19 - our application
65:20 - and again we want to focus our attention
65:24 - on the high cost statement our
65:25 - application runs
65:27 - sql server can also give us a detailed
65:30 - execution statistics about each
65:32 - statement we run if we use the set
65:34 - statistics command in management studio
65:42 - what you want to pay special attention
65:44 - is the number of logical reads
65:46 - as each logical read represent an 8 kb
65:50 - page of data
65:51 - a high number of logical reads often
65:53 - represents an inefficient operation that
65:56 - is having to read a lot of data to find
65:58 - the piece of data that is really needed
66:01 - once we understand what operations are
66:03 - causing the statement to be slow we can
66:06 - start to tune the statement to improve
66:08 - its performance
66:10 - we looked at one case where the solution
66:12 - was to add an index
66:14 - as this allowed sql server to use the
66:16 - tree structure of the index to find the
66:18 - data it needed
66:20 - rather than scanning the entire table
66:23 - we also look at the example where we
66:26 - change our sql statement and this is
66:28 - another solution you may want to
66:30 - consider sometimes changing a sub query
66:33 - to a join or may be using the exist
66:36 - keyword helps sql server find a more
66:39 - efficient execution plan
66:41 - you can also do a general evaluation of
66:43 - your statement and see if there is an
66:45 - opportunity to include more selective
66:48 - where clause
66:49 - or possibly change your statement to
66:52 - where it's asking the question you want
66:54 - answer in a slightly different way
66:57 - what getting an execution plan allows
66:59 - you to do to see the individual steps
67:02 - that have to be executed to run your
67:04 - statement and to find which statements
67:07 - are most costly
67:09 - then you can focus on this operation to
67:11 - see what you can do
67:13 - to make these operations more efficient
67:17 - one of the most important tool we have
67:19 - is to create a good index on our table
67:21 - that the statement in our application
67:23 - will use
67:25 - so we will dive into the topic of how to
67:27 - create effective indexes in the next
67:29 - module
67:30 - you are watching the video of module 3
67:33 - building effective index part 1 module
67:36 - introduction
67:37 - probably the most important step you can
67:40 - take to make sure the sql in your
67:42 - application performs well
67:44 - for which we have to create effective
67:46 - indexes on the table in your database
67:49 - and that is exactly what we are going to
67:51 - cover in this module
67:53 - we are going to start off with a
67:54 - revision on index terminology just to
67:57 - make sure that some definitions are
67:59 - fresh in your mind as we go through this
68:01 - module
68:03 - second we will discuss what column in
68:05 - your database you want to create index
68:07 - and why
68:09 - then we are going to turn our attention
68:11 - to two characteristics you need to pay
68:13 - attention in order to make sure
68:16 - that your indexes are effective
68:18 - and those are the order of the columns
68:21 - in the index and the selectivity of your
68:23 - indexes
68:25 - we will move on and talk about covering
68:27 - indexes and include columns and when
68:29 - those are useful
68:31 - and then we will discuss the effect of
68:33 - using a function in your where clause
68:36 - and how these affect if sql server can
68:38 - use an index for your statement
68:41 - after that we will see the concept of
68:43 - over indexing which is when too many
68:46 - indexes are created on a table which
68:48 - slows down dml statements like insert
68:51 - and updates against the table
68:53 - and then we will wrap up with the
68:55 - discussion about index recommendation
68:57 - provided to you by sql server and some
69:00 - advice on how to interpret those we have
69:03 - a lot of material to cover so let's go
69:05 - ahead and review some index terminology
69:08 - in the next video
69:10 - you are watching the video of module 3
69:12 - building effective indexes part 2 index
69:15 - terminology refresher
69:18 - before we go ahead i want to take your
69:21 - just few minutes and refresh on some
69:23 - index terminology so these definitions
69:26 - are refresh in your mind as we go
69:28 - through this material
69:31 - when you look at the sql server
69:32 - documentation or any writings on sql
69:35 - server you are going to see the terms
69:38 - cluster index and non-cluster index
69:41 - a cluster index is the structure that
69:43 - sql server stores the data in the table
69:47 - typically a cluster index is built on
69:49 - the primary key of the table so the data
69:52 - in the table is arranged in the tree
69:54 - structure organized by the primary key
69:58 - so
69:58 - when we are talking about cluster index
70:01 - we are really talking about the
70:02 - structure that contains the data for the
70:04 - table
70:05 - and how the data is physically stored in
70:08 - a table on disk
70:11 - any other index you create on a table is
70:13 - going to be non cluster index a
70:16 - non-cluster index is built over one or
70:19 - more columns of the table
70:20 - which define the index key
70:23 - and then the index also stored a row
70:26 - pointer to where in the table the
70:27 - matching rows for that index key are
70:30 - located
70:32 - in this module when i talk about indexes
70:35 - i am really talking about these non
70:36 - cluster indexes
70:38 - because you are going to need to query
70:40 - the data in your tables by columns other
70:43 - than the primary key and creative
70:45 - effective non-cluster index is really
70:47 - the key to achieving good performance
70:49 - when you are doing so
70:51 - there are two types of operations that
70:53 - can be performed against the key
70:56 - a scan operation or a c corporation a
70:59 - scan operation is going to read the
71:01 - entire index to find the matching values
71:03 - it is looking for
71:05 - whereas a seek operation is going to
71:08 - traverse the tree structure of the index
71:10 - in order to find it matches
71:13 - when we say a sql statement is using an
71:16 - index
71:17 - what we really mean is that the
71:18 - statement is conducting a seek operation
71:21 - against the index
71:23 - because
71:23 - this is much more efficient than a scan
71:26 - operation
71:27 - and is really using the index and as it
71:30 - is designed to be used
71:32 - so when you examine operations against
71:34 - indexes in your execution plans
71:37 - you really want to be looking for the
71:39 - seek operations
71:41 - if instead you are seeing scan
71:43 - operations while sql server is accessing
71:45 - the index
71:47 - it isn't really what you want from a
71:49 - performance point of view
71:51 - so
71:52 - you want to examine why you are getting
71:54 - a scan operation
71:56 - and not a seek operation
71:58 - now that we have refreshed on some
72:00 - fundamentals
72:02 - so let's talk about what columns we
72:04 - should be thinking about indexing in our
72:07 - database to deliver good application
72:08 - performance
72:10 - you are watching the video of module 3
72:12 - building effective indexes part 3 what
72:16 - should i index in my database
72:18 - we saw an example in the last module
72:20 - where we dramatically improved the
72:22 - performance of a sql statement by adding
72:25 - an index to a table
72:27 - by adding the correct index to our
72:29 - database we are going a long way to
72:31 - assure that our application's data
72:33 - access layer will perform well so
72:36 - what we want to do is to understand what
72:39 - tables and columns we want to be
72:41 - indexing in our database from the start
72:44 - so we can avoid any performance issues
72:46 - in our applications
72:48 - there are really two categories of
72:51 - columns we want to make sure to index
72:54 - the first set of columns we want to make
72:56 - sure and index are those that are being
72:59 - used in where clauses of our sql
73:01 - statements
73:02 - whether we are running a select update
73:05 - or delete statement the where clause is
73:07 - how sql server locates the rows we are
73:10 - interested in for our statement
73:14 - having an index on the columns used in
73:16 - the where clause means that sql server
73:18 - will be able to find the rows needed by
73:20 - the statement
73:22 - by using an index seek operation which
73:24 - is very fast and efficient
73:26 - now it is very possible that different
73:28 - statements in our application will
73:30 - contain different where clauses
73:32 - when accessing the same table like we
73:35 - see here
73:36 - in this case we would want to create an
73:38 - additional index to support each
73:40 - statement
73:42 - so
73:43 - in this case we would create a second
73:45 - index over email address
73:48 - you can imagine a search form in your
73:50 - application that allows the user to
73:52 - search for students by their name or by
73:55 - their email address
73:57 - and these two indexes would correlate to
73:59 - those two different use cases
74:02 - now you might also want to search for a
74:04 - student by the student id
74:06 - but in this case student id is the
74:09 - primary key of the students table
74:11 - and sql server automatically creates an
74:13 - index on the primary key columns of the
74:15 - data
74:16 - so we don't need to do that by ourself
74:21 - but for any other way that we are going
74:23 - to be looking up the data on a table
74:26 - we are going to have an index on those
74:28 - column in the where clause that we are
74:30 - using to look the data up
74:33 - the second category of columns that we
74:35 - want to make sure and index are any
74:37 - foreign key columns in our table
74:40 - there are two reasons for this
74:42 - first
74:44 - when we join two tables together sql
74:46 - server will look up the rows in one
74:48 - table and then find the corresponding
74:50 - rows in the second table
74:53 - and to do this sql server is going to be
74:55 - searching for those matching rows in the
74:57 - second table by their foreign key values
75:01 - second when we query data in our
75:03 - application we are often traversing this
75:06 - foreign key relationship to load the
75:08 - data that our application needs
75:11 - we saw an example of this in the last
75:13 - module where we had a query that load
75:16 - the grades for a particular student by
75:18 - their student id
75:19 - if we think about this query in the
75:21 - larger context of the application we
75:23 - probably had some page in our
75:25 - application that the student used a
75:27 - login and once logged in our application
75:30 - kept track of the student's id number
75:33 - during the login session
75:35 - then as we navigate the various pages in
75:37 - our application the student id number
75:40 - will be feed into various queries like
75:42 - the one that we see on our screen
75:44 - to get the data related to the student
75:47 - if you have written any data access code
75:50 - you realize it it is quite common to
75:52 - query across the foreign keys in the
75:54 - table like this
75:56 - is what we are really doing is squaring
75:59 - across parent child relationships
76:01 - so now
76:02 - that we know that what columns we should
76:04 - be looking at indexing
76:06 - let's understand in the next video what
76:09 - makes an index effective
76:11 - so that sql server can actually use it
76:14 - in our sql statements
76:16 - you are watching the video of module 3
76:18 - building effective indexes part 4 by
76:21 - index column order matters
76:24 - in any database some of our indexes will
76:27 - have multiple columns
76:29 - and the order of the columns in the
76:30 - index determine if sql server will be
76:33 - able to use the index and how sql server
76:36 - can use the index
76:38 - so
76:39 - let's look at an example to understand
76:42 - this
76:43 - i have this index that you see on your
76:45 - screen already created on my applicants
76:48 - table
76:49 - and you see the query i want to run in
76:51 - the editor window as well just below
76:54 - the query is trying to find an applicant
76:56 - by their last name with their state
76:59 - so you might imagine a query like this
77:01 - being run from a sort of search form in
77:04 - one of your application to let an
77:06 - admission counselor finds an applicant
77:09 - what i want to do is to point out that
77:11 - in the index the column order is first
77:14 - name last name and then state and in the
77:17 - where clause of the query i have just
77:19 - the last name and the state specified
77:21 - which is the second and third column of
77:23 - the index
77:25 - so
77:25 - let's get an execution plan and see how
77:28 - sql server will process this statement
77:31 - what i want to focus your attention on
77:34 - is this operation here
77:36 - this is the scan operation against our
77:38 - index and notice that i said scan
77:41 - operation
77:42 - a scan operation like this is actually
77:45 - going to read the entire index
77:47 - so
77:48 - this is not going to use the tree
77:50 - structure of the index to find the
77:51 - matching keys
77:53 - but instead reach each and every key of
77:55 - the index in order to find the matching
77:57 - values
77:59 - so
78:00 - this is like a big linear search through
78:02 - all of the keys of the index
78:05 - what we want to see here is a seek
78:07 - operation
78:08 - because that would mean that sql server
78:10 - is using the tree structure that the
78:12 - index is organized by to find the
78:14 - matching keys
78:16 - and this would be much more efficient
78:19 - the reason why sequel server is using a
78:22 - scan operation rather than a c
78:24 - corporation in this case is because our
78:26 - where clause does not include the first
78:28 - column
78:29 - of the index
78:31 - which in this case is the first name
78:33 - column
78:34 - if you don't include this leading column
78:36 - of the index in your where clause sql
78:39 - server will either not be able to use
78:41 - the index at all or it may have to scan
78:44 - the entire index like we see here
78:47 - which is not very efficient and i will
78:49 - show you some stats in this case to
78:51 - prove it
78:53 - so if we mouse over the query
78:56 - we will be able to see the cost for the
78:58 - entire query
78:59 - and that's about 4.3
79:02 - and now i have already actually set the
79:04 - set statistics io and time on so if run
79:09 - this query we will get additional
79:10 - execution stats and we can see that this
79:13 - query is taking over 2600 logical reads
79:17 - to process
79:19 - and that includes read both against the
79:21 - table and the indexes on the table
79:24 - so this really isn't very efficient
79:27 - so let's think about what we can do to
79:30 - fix this
79:31 - we could modify our query and make the
79:34 - user include a first name value
79:36 - but this is not really very realistic
79:39 - the reason our user didn't include a
79:42 - first name value is probably because
79:44 - they don't know it
79:45 - so we can't really require them to
79:47 - provide a value that they don't know
79:50 - what we can do though is we can change
79:53 - the order of the columns in our index
79:55 - so let's do that
79:58 - so what i am going to do is i am going
80:00 - to drop the current index
80:03 - and there we go
80:04 - and now i am going to recreate an index
80:08 - and in this case i am going to have the
80:10 - last name column in the first position
80:12 - of this index and the first name column
80:14 - in the second position
80:16 - so i am going to go ahead and create
80:18 - this
80:21 - and now i will take this query and i am
80:23 - going to get the execution plan again
80:27 - we see now that the operation here has
80:29 - changed to a index seek operation and
80:32 - that is what we want because it means
80:34 - that sql server is traversing the tree
80:36 - structure of the index to find the
80:38 - matching index keys
80:40 - so this is going to be a lot more
80:42 - efficient and we can prove that by
80:44 - looking at some of the stats
80:46 - so first we are going to look at the
80:49 - cost here of the overall query
80:51 - and we see now that's down at 1.4
80:56 - much more efficient than before
80:58 - and if we run this query again we see
81:01 - that now we are only doing 188 logical
81:04 - grids
81:05 - whereas before
81:07 - we are doing over 2600
81:10 - so again
81:12 - this is much more efficient
81:14 - and the reason why is because when we
81:17 - have those columns in the index in the
81:19 - right order
81:20 - such that sql server can use the index
81:23 - then sql server is having to process a
81:26 - lot less data because it can directly
81:29 - find the data that it needs
81:31 - this has important implications for when
81:34 - we create multi-column indexes in terms
81:36 - of what order we should put the columns
81:38 - in
81:40 - what we want to do is to think about how
81:42 - our user query the data in the table
81:45 - we are creating the index on and what
81:48 - different combinations of columns are
81:50 - used in the where clauses of these
81:52 - queries
81:53 - then
81:54 - with each combinations of column we want
81:57 - to put the most frequently included
81:59 - columns as the first column of the index
82:01 - the second most frequently used column
82:03 - in the second column of the index and so
82:05 - on
82:06 - so in the first example of this slide
82:09 - since the last name column is always
82:11 - included when doing and search by name
82:14 - it is in the first position in the index
82:16 - first name is frequently included so it
82:19 - is in the second position and only
82:21 - sometimes the state is included so that
82:24 - is in the last position
82:26 - if the user only includes the last name
82:28 - in the query or the last name and the
82:30 - first name that's okay sql server can
82:33 - still use this index because we have the
82:35 - leading column of the index and these
82:38 - columns are selective enough which we
82:40 - will talk about in a moment
82:42 - now our user might also need to search
82:45 - this table for data in a complete
82:47 - different fashion as well
82:49 - in this case by the state and the city
82:52 - that an applicant is from
82:55 - so in this case we need to create a
82:57 - second index on the table because we
83:00 - need the state column to be in the lead
83:02 - position so sql server can use this
83:04 - second index so what you will end up
83:07 - with on your table is an index that is
83:10 - target at supporting each of the
83:12 - different ways that your user of your
83:15 - application access data on this table
83:18 - so it's pretty typical that on the key
83:20 - tables of your application you will
83:22 - probably have from two to four different
83:24 - indexes to support these different query
83:27 - combinations
83:28 - the next step in the process though is
83:30 - to make sure that the index are
83:32 - selective enough so that sql server can
83:35 - use them
83:36 - and we will look at this in the next
83:38 - video
83:39 - you are watching this video of module 3
83:42 - building effective indexes part 5 index
83:45 - selectivity
83:47 - the second factor that governs how
83:49 - effective an index will be is the
83:51 - selectivity of the index
83:54 - and selectivity is simply a way of
83:56 - saying how many or how few rows there
83:59 - are in the table for each key value in
84:01 - the index
84:03 - for our indexes to be used by sql server
84:06 - and to be effective at speeding up the
84:08 - performance we want our indexes to be as
84:11 - selective as possible
84:13 - that is each value of the index key
84:15 - should only correspond to a few rows in
84:18 - a table
84:19 - or perhaps even only one row let's see
84:22 - why this matters we know that when sql
84:25 - server uses an index it traverses the
84:28 - tree structure of the index to finding
84:30 - the matching keys in the index when it
84:32 - finds the matching key or keys it will
84:35 - read from the index the value of the row
84:37 - identifiers for the rows that matches
84:39 - those index keys
84:41 - in a typical table this row identifiers
84:44 - are just the primary key values of the
84:46 - matching rows
84:48 - then sql server takes this row
84:50 - identifier and look up the actual rows
84:52 - in the table
84:53 - which as we have talked about is usually
84:56 - another tree structure called a cluster
84:58 - index
84:59 - so
85:00 - if the index is selective we will only
85:02 - find a few matching values in the index
85:05 - that we have to look up in the table
85:07 - so we are doing a small number of i o
85:10 - operations overall but what if our index
85:13 - isn't very selective what if for the
85:16 - index key we look up we get back several
85:19 - thousand matches
85:20 - well then we are going to have come over
85:23 - to the table and look up each and every
85:26 - one of those rows
85:27 - so that is several thousand times we are
85:30 - going to have to look up data in this
85:32 - table
85:33 - and since our data is probably randomly
85:36 - distributed throughout the table
85:38 - we are going to end up reading most if
85:40 - not all of the pages in the table anyway
85:43 - so in this case it is actually more
85:46 - efficient for sql server not to use the
85:48 - index and just to read the entire table
85:51 - anyway
85:52 - this way sql server doesn't have to
85:55 - incur the i o of reading the index
85:57 - because the index isn't really helpful
86:00 - in terms of narrowing down what data sql
86:02 - server needs to find
86:04 - when we say that we want our indexes to
86:07 - be selective
86:08 - what we are really saying is that we
86:10 - want them to really help sql server
86:13 - target exactly where the data is that we
86:16 - need to find
86:17 - and to do that we want our indexes to
86:20 - have a high number of unique key values
86:22 - compared to the total number of rows in
86:24 - the table let's jump into management
86:27 - studio and look at some example of this
86:29 - i have created the index at the top of
86:31 - the screen on the students table and i
86:34 - am going to run the query that's at the
86:36 - bottom of the text editor
86:38 - but before i do i am going to give you
86:41 - some statistics about the students table
86:44 - there are about 1 lakh 20 000 rows in
86:46 - the students table
86:48 - and in terms of unique first and last
86:50 - name combinations there are about 1 lakh
86:52 - 7 000 of those
86:54 - so we can tell from those numbers the
86:56 - combination of last and first name that
86:59 - is index is built on is a pretty
87:01 - selective
87:02 - criteria while we do have multiple
87:05 - students that have the same first and
87:07 - last name overall this criteria is
87:09 - pretty unique the calculation that sql
87:12 - server does internally in its statistics
87:15 - to determine selectivity is more complex
87:17 - than a simple ratio
87:19 - but
87:20 - this ratio gives us a good idea of how
87:22 - unique the values in this index are
87:25 - and i find that for everyday use this
87:28 - simple ratio works pretty well
87:30 - so we would expect that sql server would
87:33 - use this index when it executes the
87:35 - query that we see we will get the
87:37 - execution plan to see that indeed that
87:40 - is the case
87:41 - and so we see here that sql server is
87:44 - doing an index seek operation against
87:46 - that index
87:47 - so indeed our index is being used
87:50 - if i mouse over the index seek operation
87:53 - you see that sql server is expecting
87:55 - about 25 values in the index would match
87:58 - this criteria
87:59 - and again sql server is calculating this
88:02 - value based on the statistics it has on
88:04 - the table and on the index
88:06 - so its calculation is more sophisticated
88:09 - than our simple ratio
88:12 - but this is what sql server is expecting
88:14 - to happen then it's going to come down
88:17 - to this key lookup operation and look up
88:19 - each one of those rows and if we mouse
88:22 - over the key lookup operation we again
88:25 - see that value that's about 25 in terms
88:28 - on the number of estimated executions
88:31 - so at this threshold sql survey has
88:34 - calculated that using the index is the
88:36 - right decision because performing the
88:38 - estimated 25 lookups on the table is
88:41 - going to be much faster than reading the
88:44 - entire table
88:45 - now if we run this query we did actually
88:48 - get back 4 rows of the data
88:51 - so this lookup operation actually would
88:53 - be executed 4 times not 25 times
88:56 - the point is though that the lookup
88:59 - operation is only executed a handful of
89:01 - times and we only need to read a few
89:04 - data pages from the table
89:06 - so we are being very targeted in the
89:09 - data that we read which is what an index
89:11 - is supposed to give us
89:13 - let's look at the different situation
89:15 - though when we don't have a selective
89:16 - index and see what sql server does in
89:19 - that situation
89:21 - if i go over to this other tab that i
89:24 - have here
89:25 - we see again that we have another index
89:28 - on the students table that i have
89:30 - created
89:31 - and this index is just over us states
89:34 - run the query that you see here in the
89:36 - bottom of the text editor
89:38 - and what that's going to do is it's
89:41 - going to look up all the students that
89:43 - live in appleton wisconsin
89:45 - remember we have about 1 lakh 20 000
89:48 - rows of students in the table
89:50 - and in terms of states abbreviation
89:52 - there are actually 52 distinct value in
89:54 - our students table
89:57 - so we have the 50 states the district of
89:59 - columbia
90:00 - and maybe we have some students from
90:03 - puerto rico and another u.s territories
90:06 - if we divide 1 lakh 20 000 rows by 50 we
90:09 - get about 2400 matches per index in this
90:12 - case
90:14 - now this max isn't perfect because of
90:17 - course there are some states like
90:18 - california where proportionately we are
90:21 - going to have many many more students
90:23 - than in the small states like wyoming
90:26 - but for a rough estimated to start with
90:28 - what this is telling us is that for
90:30 - having an index just on the state column
90:33 - it is not going to be very selective
90:35 - because we are actually going to get
90:37 - about 2400 students back
90:40 - so let's go ahead and get the execution
90:42 - plan of this query
90:44 - and see what sql server does with this
90:47 - statement what we see here is that sql
90:50 - server actually isn't using this index
90:52 - on the state column in the query
90:55 - and that is because when sql server look
90:57 - at its statistics
90:59 - we would be getting a lot of matches
91:01 - back from that index
91:03 - and when you read all of those matches
91:06 - out of the index and then go and look up
91:09 - all of those values in the table that
91:12 - actually was going to be more expensive
91:14 - than just reading the entire table
91:15 - direct so sql server not using our index
91:19 - is really telling us that in sql
91:21 - server's eyes the index isn't that
91:23 - helpful because it's not selective
91:26 - enough
91:27 - now you might say that i am not sure i
91:30 - believe that
91:31 - i think that the index would still be
91:33 - more efficient
91:34 - so what i am going to do is i am going
91:37 - to force sql server to use the index on
91:40 - this table
91:41 - and i am going to demonstrate to you
91:43 - that actually the sql server optimizer
91:45 - has made the right choice here
91:47 - so to do this i am going to modify our
91:50 - query to use a database hint and what
91:53 - this hint is going to do is going to
91:55 - tell sql server that i want to use that
91:58 - index
91:59 - and normally i strongly recommend that
92:01 - you avoid using hints in your
92:03 - application
92:04 - as today the sql optimizer included with
92:08 - sql server is very very good
92:10 - and it's almost always come with the
92:12 - correct answer in the most efficient way
92:15 - to execute any statement
92:17 - in 99.9 percent of the cases today the
92:20 - sql optimizer comes with the right
92:22 - answer so the only way i could use a
92:25 - hint in a production application
92:27 - is
92:28 - if it was instructed to do so by
92:30 - microsoft support or i want to
92:32 - demonstrate something like i am going to
92:34 - do now
92:35 - so we have our hint in there and now
92:39 - once again i am going to get our
92:41 - estimated execution plan
92:43 - and we see now a sql server using the
92:46 - index
92:47 - but if we mouse over this index we see
92:50 - that sql server thinks that it is going
92:52 - to get back from the index over 2300
92:55 - matching rows
92:56 - and if we go down to the key lookup
92:59 - operation once again we are going to
93:01 - have to look up 2300 different values in
93:03 - the table
93:05 - and this is going to turn out to be very
93:07 - expensive
93:08 - in fact
93:09 - then just scanning the entire student
93:11 - table i did run both version of this
93:14 - query prior to recording
93:16 - one without the hint and one with the
93:18 - hint
93:19 - so i could summarize the stats and show
93:22 - them to you
93:23 - which i am doing here
93:26 - what you can see here is that by forcing
93:28 - sql server to use the index
93:31 - that indeed is more expensive
93:33 - and what this is driven by is the fact
93:36 - that by just having the index on the
93:38 - state column that is not selective
93:40 - enough
93:41 - so if we force sql server to use the
93:44 - index it's actually having to read and
93:46 - process more data then if it just read
93:49 - the entire table directly
93:51 - this is an important point many times
93:53 - someone will create the index and they
93:55 - think that the index they created is
93:57 - going to speed up the things
94:00 - but when they look sql server isn't even
94:03 - using the index
94:04 - as
94:05 - we are saying in this case
94:08 - sql server is making the right decision
94:10 - not to use the index because using the
94:12 - index would be more expensive
94:14 - and the reason why is because of
94:16 - selectivity
94:18 - because sql server has to read a large
94:21 - part of the index and then conduct a
94:23 - large number of lookup operations that's
94:25 - accounting for roughly 3500 extra
94:27 - logical ios that have to be read and
94:30 - processed by sql server so the answer
94:32 - here is we need to make our index more
94:34 - selective so let's do that
94:38 - we are going to go ahead and drop this
94:40 - index and then we are going to create a
94:42 - more selective index
94:44 - which is both over state and city
94:48 - in terms of unique combinations for city
94:50 - and state together there are about 13
94:52 - thousands of rows
94:55 - so we are in a much better place in
94:57 - terms of selectivity for this index so
95:00 - let's get this created
95:02 - now we will go ahead and get the
95:04 - execution plan again for our query
95:06 - and actually we have to get rid of our
95:09 - hint in order to make this work because
95:12 - that index does not exist anymore
95:15 - and we see now with this more selective
95:18 - index sql server is able to use the
95:21 - index
95:22 - because we see this index seek operation
95:24 - here
95:26 - if we mouse over this we see now sql
95:28 - server is expecting to get back about 20
95:31 - rows in reality it's actually going to
95:33 - get a few more than that
95:36 - but because this index is more selective
95:39 - it's something that can be used by sql
95:41 - server because we are doing a better job
95:43 - of targeting the data that we want
95:46 - one of the takeaways from this is that
95:49 - if we have a column that's not very
95:51 - selective by itself that doesn't mean
95:54 - that we can't use the column in the
95:56 - index
95:57 - it means that we probably need to be
95:59 - using that column in conjunction with
96:01 - other columns in our index
96:04 - and
96:05 - if we need to make sure that the
96:07 - combinations of column together is
96:08 - selected selectivity is really important
96:11 - concept when we start looking at our
96:14 - indexes and the where clauses of our
96:16 - statements
96:18 - so let's look in the next video at some
96:21 - other ways in which selectivity matters
96:24 - you are watching the video of module 3
96:26 - building effective indexes part 6
96:29 - like clauses and index selectivity
96:33 - one special case i want to talk about
96:35 - with regards to indexes and selectivity
96:38 - is when you use a like clause in your
96:41 - sql statement
96:42 - first you need to know that if you use
96:45 - this percentage sign which is the
96:47 - wildcard character at the front of the
96:49 - value in the like clause
96:51 - then sql server will not be able to use
96:54 - an index for that column
96:56 - remember the key in index are in sorted
96:59 - order
97:00 - and by having a leading wildcard
97:01 - character in your search value sql
97:04 - server isn't able to use the sorted
97:06 - order of the data to take its advantage
97:09 - so in this case sql server will have to
97:12 - resort to some sort of scan operation
97:15 - either of the entire index or of the
97:17 - entire table
97:20 - when the wildcard character is used
97:22 - somewhere else in the like value of your
97:25 - statement
97:26 - as we see here
97:28 - this has an impact on the selectivity of
97:30 - your index
97:32 - basically only the characters to the
97:34 - left of the percentage sign counts in
97:36 - terms of selectivity for the statement
97:38 - and the index
97:40 - so in this case maybe i am not sure if
97:43 - the last name of the applicant i am
97:45 - looking for is harris or harrison so i
97:49 - am using the wildcard character to
97:50 - search and try to find out the right
97:52 - applicant
97:54 - and i am doing the same thing for the
97:55 - first name
97:57 - but in this case i am only know that the
97:59 - first name starts with the letter t
98:02 - if we get the execution plan for this
98:04 - statement we see that we will still able
98:07 - to use the index
98:09 - because
98:10 - we have specified enough characters in
98:12 - the value that we are searching for
98:15 - but this is still selective enough
98:18 - but let's look at what happened if i
98:20 - trim down the value that we are
98:22 - searching for in the last name to just a
98:24 - couple of characters
98:26 - and i will run the execution plan again
98:30 - and now we see that sql server is doing
98:33 - a scan operation of the table
98:35 - it is not able to use the index and the
98:38 - reason why is because we haven't
98:40 - provided enough information here
98:42 - that we can use the index
98:44 - so really our query just isn't selective
98:47 - enough here
98:49 - the reason i bring this up is because
98:51 - many times i see search pages in
98:53 - applications that don't enforce any sort
98:56 - of minimum requirements on the user in
98:59 - terms of the data that they must supply
99:02 - now sql server will happily run this
99:04 - statement for you
99:06 - they are just going to take a lot of
99:08 - resources and they are going to take a
99:10 - lot of time to finish so selectivity is
99:14 - really a two-way street
99:15 - first we want to make sure that our
99:18 - indexes are selective
99:20 - second we want to make sure that we are
99:22 - including specific enough criteria for
99:24 - our where clause of our sql statements
99:28 - such that our statements are selective
99:31 - and those statement will be able to use
99:32 - the indexes that we have created in our
99:35 - databases
99:37 - this is where we have to use our
99:38 - knowledge of how the user is searching
99:40 - for and accessing data to design good
99:43 - indexes
99:45 - and enforces reasonable constraint on
99:47 - how the user is going to search for data
99:49 - in our application
99:52 - as developers we have a good
99:53 - understanding of how our applications
99:55 - are trying to access our data
99:58 - and we have a good understanding of what
99:59 - the data in our database is
100:02 - so it is really just a matter of
100:04 - combining this knowledge together so we
100:07 - have good indexes in our databases
100:10 - and we are writing sql statements in our
100:12 - application that can take advantage of
100:14 - those indexes
100:16 - in the next video we are going to talk
100:18 - about how using functions in the where
100:20 - clauses of your statement affect sql
100:23 - servers ability to use an index
100:26 - you are watching the video of module 3
100:28 - building effective indexes part 7
100:31 - how functions in the where clause affect
100:33 - indexes
100:35 - something you want to be aware of is
100:37 - that if you have a function on the left
100:40 - side or column side in your where clause
100:43 - sql server will not be able to use an
100:45 - index on the table
100:47 - so you see here that i have an index on
100:50 - a table over
100:52 - email address
100:53 - however looking at my query on the left
100:55 - hand side of the where clause where we
100:57 - have the column definition
100:59 - you can see that in this case i am using
101:02 - a couple of different functions to
101:04 - compute the value of just the local part
101:07 - of the email address
101:08 - meaning everything left of the address
101:11 - sign
101:12 - in the email address
101:13 - if you have any sort of function over
101:16 - here on the left hand side of the where
101:18 - clause processing the value in the
101:20 - column
101:21 - sql server is not going to be able to
101:24 - use a regular index created over the
101:26 - column
101:27 - like we see
101:28 - has been created up here
101:31 - if we look down at the execution plan we
101:33 - can see that is the case
101:36 - because we see that sql server is doing
101:38 - a scan of the index not a seek operation
101:42 - so this query isn't going to perform
101:44 - nearly as well
101:46 - the reason why this is happening is
101:48 - because what is sorted in the index are
101:51 - the actual email address values
101:53 - not the computed values
101:56 - so what sql server has to do is to run
101:59 - this function in real time while the
102:02 - query is executing to create those
102:04 - computed values
102:05 - and then
102:06 - it can do the comparison
102:08 - and so to do that you are going to incur
102:11 - some processing cost and not have a very
102:14 - fast query
102:15 - so
102:16 - you want to keep this in mind whenever
102:18 - you see a function like this operating
102:20 - on a column value in the where clause of
102:23 - one of your queries
102:24 - if this is something that you really
102:26 - need to do in your application
102:29 - though there is a technique
102:31 - we can use to address this situation
102:34 - what we are going to do is we are going
102:36 - to add a computed column to the table
102:39 - and then we are going to create an index
102:41 - over that computed column
102:43 - and in doing so sql server will be able
102:46 - to use that index over the computed
102:49 - column
102:50 - so let's do that
102:53 - so first i am going to use alter table
102:55 - statement in order to create the
102:57 - computed column
102:59 - so you can see in this statement i have
103:02 - that same formula with those two
103:04 - functions
103:06 - i could use any functions i want to here
103:09 - even as user defined function
103:12 - the only thing that i need to remember
103:14 - is in order to create an index over this
103:16 - computed column
103:18 - the expression that i put in here has to
103:21 - be deterministic
103:23 - that is it always has to return the same
103:25 - value for the same inputs
103:27 - so let's go ahead and get this computed
103:30 - column created
103:32 - and now that the column is created
103:35 - i actually can query the value back out
103:38 - and i will show those to you
103:41 - if we scroll over here to the right
103:44 - we can see here in our computed column
103:47 - over here on the right
103:49 - and at this point what sql server is
103:51 - doing is it's just calculating the value
103:53 - on the fly
103:55 - when we request this rows out of the
103:57 - table in our query
103:59 - hence the name computed column
104:02 - what we can do now though is we can
104:04 - create an index over this column
104:07 - so let's do that
104:09 - so you see the create index syntax looks
104:12 - just like any other index
104:14 - that we might create
104:16 - what will happen though is when we run
104:18 - this create index statement is that sql
104:22 - server is going to compute all of the
104:23 - values of this computed column for each
104:26 - row of the table
104:28 - and it's going to store those computed
104:30 - values in the index
104:32 - so let's go ahead and get this index
104:35 - created
104:36 - and there we go
104:39 - so now we are going to go back up to our
104:41 - original query and we are going to get
104:43 - the execution plan for that again
104:46 - if i can select the right thing and we
104:49 - see now sql server is able to use this
104:52 - new index that we have created over the
104:54 - computed column
104:56 - and i will mouse over this
104:59 - so you can see that it is using indeed
105:02 - that index
105:04 - and the reason why is because the
105:06 - formula in the left hand side of our
105:09 - where clause matches the formula of the
105:11 - computed column value that the index was
105:13 - created over
105:15 - so
105:16 - what do you want to remember is to be on
105:18 - the lookout for any type of function or
105:21 - expression in your where clauses of your
105:24 - sql statements on that left hand side of
105:26 - the where clause expression
105:28 - because those won't be able to use
105:31 - regular indexes on your table
105:34 - however if you do need to be able to
105:36 - search for data using a computed value
105:39 - then you can use the technique that i
105:41 - have just shown you here
105:44 - where
105:45 - you first create a computed column on
105:47 - the table
105:48 - and then you create an index over that
105:50 - computed column
105:52 - and that will allow your statement to
105:54 - use an index
105:56 - and consequently perform much faster
106:00 - we have covered the fundamental aspects
106:02 - of indexes but there is more to learn
106:04 - about
106:05 - so we will continue our discussion by
106:08 - talking about including columns and
106:10 - covering index next you are watching the
106:12 - video of module 3 building effective
106:14 - indexes part 8
106:17 - include columns and covering indexes
106:20 - if you take a look at the sql server
106:22 - documentation or some of the index
106:24 - recommendation you get back from
106:26 - management studio you are going to see
106:28 - this index with the keyword include
106:31 - so let's understand what is going on
106:33 - here
106:34 - the include keyword lets you specify
106:36 - that one or more columns values should
106:39 - be stored in the index with the index
106:41 - key
106:42 - but they are not part of the index key
106:44 - so this include columns can't be used by
106:47 - sql server when it searches the index
106:50 - and they don't affect how sql server
106:53 - will lay the index data out in the tree
106:56 - structure
106:57 - they will just have their values stored
106:59 - with the index
107:01 - the reason why this can be useful is
107:03 - because we can create what is called a
107:06 - covering index by using these values
107:09 - normally when sql server uses an index
107:12 - it looks up the index keys in the index
107:15 - to get an row pointer to where the data
107:17 - is in the table
107:19 - and then it has to perform a key lookup
107:22 - operation to get the actual raw data out
107:24 - of the table
107:26 - and we have seen several example of this
107:29 - a covering index is a term used when sql
107:32 - server can get all of the data it needs
107:34 - for a query from an index itself and it
107:37 - doesn't need to perform the key lookup
107:39 - operation there are two factors at play
107:42 - here
107:42 - one notice that we are not using a
107:45 - select star in our query
107:48 - but i am spelling out exactly what
107:50 - columns i want and two notice that all
107:53 - of these columns are in the index
107:55 - either as an index key or as an include
107:59 - column
108:00 - so sql server does not need to go find
108:03 - the actual row in the table to read the
108:06 - data
108:07 - because it has everything that it needs
108:10 - right here in the index
108:12 - and we can see that down here in our
108:15 - execution plan
108:16 - because we noticed that we have the
108:18 - index seek operation
108:20 - but no corresponding key look of
108:22 - operation like we have seen before
108:24 - because in this case we have already all
108:27 - the data that we need
108:29 - if there is just one column that we
108:31 - don't have the data for in the index
108:34 - sql server would have to perform the
108:36 - lookup operation on the table
108:38 - and i can demonstrate that for you if i
108:41 - add the telephone column to this query
108:44 - and now i will get the execution plan
108:47 - for the query again
108:49 - now since telephone is needed by the
108:51 - query and it's not part of the index the
108:54 - index is not a covering index for this
108:56 - query
108:57 - and sql server has to go to the table to
109:00 - get that value
109:02 - if we take out telephone and run our
109:04 - execution plan again we see again we
109:06 - just need the index to fulfill the needs
109:09 - of this query
109:10 - sometimes you will have a query like
109:13 - this where you only need one or maybe
109:15 - two columns that are not in the index
109:18 - and that you are having to go to the
109:20 - table to look up the values for
109:23 - in this case it can make sense to use an
109:26 - include column or 2 so that your index
109:29 - can cover the query and avoid the key
109:31 - lookup operation
109:33 - if you start adding 3 or 4 or more
109:36 - columns as include columns in your
109:38 - indexes though then that is probably a
109:41 - warning that you are going a little too
109:44 - far with include columns in creating
109:46 - covering indexes
109:47 - what you are essentially doing is making
109:49 - another skinned down version of your
109:51 - table
109:52 - but what that mean is that when any of
109:55 - these columns get updated sql server
109:58 - will also have to update the entries in
110:00 - the index to keep it up to date
110:03 - so you are going to incur higher
110:05 - maintenance cost for your index by doing
110:07 - this
110:08 - if you have a query that only needs one
110:10 - or two columns including columns and a
110:13 - covering index can provide a nice
110:15 - additional performance boost just be
110:17 - careful not to go overboard and include
110:20 - too many columns
110:22 - such that you basically now have another
110:25 - copy of your table
110:27 - sometimes to improve performance we need
110:29 - to create numerous indexes on the table
110:32 - which unknowingly leads to over indexing
110:36 - in the next video we are going to
110:38 - discuss about over indexing
110:40 - you are watching the video module 3
110:42 - building effective indexes part 9 over
110:45 - indexing
110:47 - we have seen how indexes can speed up
110:50 - our sql statements and reduce the number
110:52 - of resources it takes for sql server to
110:55 - run our statements
110:57 - so you might be tempted to think that we
110:59 - should create indexes for every column
111:01 - on every index in our database
111:04 - even if those columns are rarely used in
111:07 - any where clauses or join clauses for
111:09 - our statements
111:11 - unfortunately this isn't quite the case
111:14 - while you want to create indexes that
111:16 - will be used by the sql statements in
111:19 - your application
111:20 - you also want to be careful not to over
111:22 - index your database
111:24 - and by over index i mean create indexes
111:27 - that aren't going to be used by any
111:30 - statements in your or any other
111:32 - application that are running against
111:34 - your database
111:36 - the reason why is that index has a
111:38 - maintenance cost of them
111:40 - and if these indexes aren't helping
111:43 - speed things up in your queries then you
111:46 - are paying this maintenance cost without
111:48 - getting any value from this indexes
111:51 - so let's understand this more
111:54 - when we create an index sql server
111:56 - creates a separate physical structure
111:58 - that contains the data for the index
112:01 - namely the values of the index keys and
112:04 - any include columns that are stored in
112:06 - the tree structure of the index
112:09 - when any sort of dml operation is
112:11 - performed on the table sequel server has
112:14 - to keep all of the indexes on the table
112:17 - in sync with the data in the table
112:20 - so this means that when an insert or
112:23 - delete statement is performed against
112:25 - the table sql server will also have to
112:28 - add or remove an entry from all the
112:31 - indexes on the table
112:33 - when an update statement is running if
112:35 - the values of any of the column in the
112:37 - index are modified then the index has to
112:40 - be updated as well
112:42 - all of this has to be happened when the
112:44 - dms statement executes against the table
112:46 - so sql server can keep all of the data
112:49 - in a consistent state
112:51 - so you want to regard your indexes as
112:54 - investments
112:56 - if an index is being used by statement
112:58 - in your application and is helping
113:00 - speeding up those statements then the
113:03 - cost of the index is well worth it
113:05 - because for most applications the
113:07 - queries that the index helps speed up
113:09 - are run far more frequently than any dms
113:12 - statements against the table
113:15 - so the index is good investment if
113:18 - however you have an index that is never
113:20 - used or is used only rarely
113:23 - you want to investigate why the index is
113:25 - not being used and possibly consider
113:28 - dropping it
113:30 - so how can you tell this
113:33 - well sql server contains a number of
113:35 - views called dynamic management views
113:37 - that gives you
113:38 - access to all types of performance
113:40 - statistics and diagnostics information
113:43 - within the database engine itself
113:47 - we will talk much more about these views
113:49 - in the next module
113:50 - but among these views is one that gives
113:53 - us index usage statistics
113:56 - so we can use a query like the one you
113:58 - see on the screen to look at the values
114:01 - in the user c column
114:03 - and compare it to the user updates
114:05 - column and as long as we have more seeks
114:09 - than updates we are in a good shape
114:12 - otherwise if all we are seeing are
114:14 - values in the user update column
114:17 - this indicates we are paying the cost of
114:19 - the index
114:21 - but not getting any value out of it
114:23 - so it may be appropriate to drop the
114:26 - index
114:27 - there is one more topic i want to
114:29 - discuss and that is the index
114:31 - recommendation that are made by sql
114:33 - server that we have seen pop-up
114:35 - throughout the module in management
114:37 - studio
114:38 - so we will discuss it in the next video
114:41 - you are watching the video of module 3
114:43 - building effective indexes part 10
114:46 - interpreting sql server index
114:48 - recommendations
114:50 - no doubt as you have been watching this
114:52 - module you have been seen a number of
114:55 - index recommendations pop-up in
114:57 - management studio while i have been
115:00 - demonstrating different concepts
115:03 - and as you experiment with different
115:05 - statements that your application runs no
115:08 - doubt you will see this recommendation
115:10 - pop up then as well
115:12 - it is very useful to have these
115:14 - recommendations from sql server
115:17 - but i would remind you these are just
115:20 - suggestions and you should not
115:22 - automatically create an index because
115:25 - sql server makes a suggestion to do so
115:28 - in my opinion sql server can be overly
115:31 - aggressive with some of its
115:33 - recommendations
115:34 - and if you follow each and every
115:37 - recommendation
115:38 - you would probably end up with the
115:40 - database that was over indexed
115:43 - and therefore had sub optimal
115:44 - performance when it comes to dml
115:47 - statements
115:48 - let's look at an example of this that we
115:51 - have saw in this module
115:53 - earlier i executed this query with
115:56 - selected applicants based on their last
115:58 - name and state
116:00 - i already had an index over the last
116:03 - name first name and state columns
116:06 - which sql server was able to use for
116:08 - this query
116:09 - and you can see
116:11 - though sql server is giving me a
116:12 - recommendation to create a second index
116:15 - over just the last name and state column
116:18 - it is true that this second index would
116:21 - result in somewhat better performance
116:22 - for this particular variation of the
116:24 - query
116:25 - however since there is already an index
116:28 - that sql server is able to use
116:30 - and use pretty effectively
116:33 - i would personally pass on creating this
116:35 - second index
116:36 - the recommendations that sql servers
116:39 - give you are based on the individual
116:41 - query that you are running at the moment
116:44 - so
116:45 - sql server will try to optimize an index
116:47 - for that exact query
116:49 - but what you really want to do is to
116:51 - look at the different variations of your
116:53 - statement and their where clauses and
116:56 - come up with the set of indexes that
116:58 - will work across all the different
117:00 - combinations that you have
117:02 - this means a couple of variations of the
117:04 - statement may not be 100 optimized but
117:07 - again we have to consider the trade-off
117:10 - around creating multiple extra indexes
117:13 - versus the cost that we will have to pay
117:15 - to maintain each of those indexes
117:18 - but it is to say that you should take
117:21 - this recommendation just as suggestion
117:24 - and then it is up to you to combine your
117:26 - knowledge of your application the data
117:29 - in the database and the suggestion to
117:30 - come up with the right set of indexes
117:32 - for your tables
117:34 - let's summarize what we have learned and
117:36 - wrap up this module in the next video
117:39 - you are watching the video module 3
117:41 - building effective indexes part 11
117:44 - module summary
117:46 - having effective database indexes is
117:48 - probably the most important factor in
117:50 - determining the performance of your
117:52 - application data access layer
117:55 - so in this module we talked about how
117:58 - you build effective indexes in your
117:59 - database we lead off by talking about
118:03 - what columns you want to be creating
118:05 - indexes in your database
118:07 - first among this are any columns that
118:10 - you are using in the where clauses of
118:12 - your sql statements
118:13 - as this is the way that your application
118:16 - locates the data that it needs
118:18 - additionally we talk about the need to
118:20 - index the foreign key columns of your
118:22 - table to support any join operation we
118:25 - do
118:26 - as well as the fact that often our
118:28 - application will query data across these
118:30 - foreign key relationships
118:32 - once we know the columns we need to
118:34 - create index on we need to make sure
118:37 - that sql server will actually be able to
118:39 - use the index we create
118:41 - the first thing we need to make sure of
118:44 - that the column in our index are in the
118:46 - right order as sql server will not be
118:50 - able to use the index if the first
118:51 - column in the index is not in the where
118:53 - clause or join condition of the sql
118:56 - statement
118:57 - so that means when we create our indexes
119:00 - we want to put the column that is most
119:03 - frequently used as part of our where
119:05 - criteria at the front of the index
119:08 - followed by the next most frequently
119:10 - used column second and so on
119:13 - we may need to create multiple indexes
119:15 - to support multiple use cases
119:17 - but this is where we can use our
119:20 - knowledge of the application and how our
119:22 - user query the data to determine the
119:24 - best column order for our indexes
119:27 - secondly we want to make sure that our
119:30 - indexes are selective enough
119:32 - the idea of an index is that it helps
119:34 - sql server quickly target a particular
119:36 - piece of data
119:38 - so we want indexes that have relatively
119:41 - few rows per index key
119:43 - because this really helps sql server to
119:46 - target on the right data
119:48 - if we have a column that by itself is
119:50 - not very selective we can still use it
119:53 - so long as we have other column in the
119:55 - index that makes the entire index
119:57 - selective
119:59 - and we make sure that the where clause
120:01 - of our sql statement is also selected
120:04 - oftentimes we need to use the like
120:06 - clause in our statements and in doing so
120:09 - we need to remember that if we use the
120:12 - wildcard character at the front of the
120:14 - like value
120:15 - sql server will not be able to use an
120:18 - index on the column
120:19 - if we have the percentage sign somewhere
120:21 - else in our value sql server will be
120:24 - able to use the index so long as we make
120:27 - sure that we have enough information in
120:30 - the supplied value that it is still
120:33 - selective
120:34 - we also saw that if we have a function
120:37 - in the where clause of a sql statement
120:40 - this will also cause sql server not to
120:42 - use an index
120:43 - and it is this function here on the left
120:46 - hand side or column side of the where
120:48 - clause that is the problem
120:50 - because those computed values are not
120:52 - stored anywhere so
120:54 - sql server has to read all the data for
120:57 - the column and compute these values on
120:59 - the fly
121:01 - if we do have a use case in our
121:03 - application where we need to do
121:05 - something like this and this example of
121:08 - using the sound x function to do a
121:10 - phonetic search might be a good one
121:13 - then what we can do is to create a
121:15 - computed column or column on the table
121:18 - and then create an index over those
121:21 - computed columns and then the sql
121:23 - statement we see here would be able to
121:26 - use the index over those computed
121:28 - columns values
121:30 - we then talk about include columns and
121:32 - covering indexes
121:33 - a covering index is when sql server
121:36 - doesn't have to perform a table lookup
121:38 - to fulfill a query
121:40 - but instead can get all the data it
121:43 - needs directly from the index
121:45 - so
121:46 - if we already have all but one or two of
121:49 - the columns we need for a query in an
121:51 - index we could use an include column to
121:53 - make that index and covering index
121:56 - which provides a little bit of an
121:58 - additional performance boost because sql
122:00 - server can avoid the additional lookup
122:02 - operation
122:04 - the last major topic we discussed was
122:07 - over indexing
122:09 - indexes do a great deal to speed up our
122:12 - statements
122:13 - especially our queries
122:15 - but they do require more maintenance
122:19 - as
122:19 - sql server has to keep all of the index
122:22 - on a table up to date whenever a dms
122:25 - statement is executed against that table
122:28 - so we want to make sure that we only
122:30 - create indexes that are really being
122:32 - used by the statement in our application
122:35 - so we don't have an adverse impact on
122:37 - any dml operation that we may have
122:41 - that's wrap up our discussion of indexes
122:44 - in the next module i am going to show
122:46 - you how to find performance bottleneck
122:49 - in sql server using sql survey built in
122:51 - dynamic management views
122:53 - these views contain all sorts of useful
122:56 - performance information like what
122:58 - statement is taking the longest to
123:00 - execute
123:01 - and if sql server thinks that we are
123:03 - missing an index that would improve
123:05 - performance
123:06 - knowing what information is available in
123:09 - this views and how to access them can
123:12 - really speed up the performance
123:13 - troubleshooting process
123:16 - so i hope you will join me in the next
123:18 - module
123:20 - you are watching the video of module 4
123:22 - finding performance bottlenecks in sql
123:24 - server
123:26 - part 1 module introduction
123:29 - one of the best feature about sql server
123:31 - is that internally it is always taking
123:34 - data about sql statements that are being
123:36 - run in the database and other events
123:39 - that are happening
123:41 - and we can access this data to give us
123:43 - an overall picture of what is happening
123:45 - inside of sql server with regards to our
123:48 - application
123:49 - for example sql server will give us
123:52 - statistics on all of the sql statements
123:54 - that have been executed in our
123:56 - application database over the last
123:58 - several hours
124:00 - including how many times a statement has
124:02 - been executed
124:04 - how long the statement took on average
124:06 - to run
124:07 - how much cpu would the statements
124:09 - consumed and how much average io needed
124:11 - to be performed
124:13 - with data like this we can quickly
124:15 - identify what statements in our
124:18 - applications are running well and which
124:20 - one we might need to do some performance
124:23 - tuning on
124:24 - now you might think that you need some
124:26 - fancy expensive tools to do all of this
124:30 - and indeed there are lot of tools out
124:33 - there on the market that exposes this
124:35 - information to you in a nice user
124:38 - interface
124:39 - however these tools are just looking at
124:41 - data that is already inside of sql
124:43 - server and all you have to do is to know
124:46 - where to look to find this data
124:49 - and then you can just write queries to
124:51 - get all of the information that you need
124:54 - the source of this data is a series of
124:57 - views in sql server known as a dynamic
125:00 - management views often referred to as
125:02 - the dmvs for short if you go to this
125:05 - page on the microsoft website you can
125:08 - navigate through all of the different
125:09 - dmvs that are available
125:12 - and the data they contain
125:14 - what we are going to do in this module
125:16 - is just concentrate on a few use cases
125:19 - that are the most important to us as
125:21 - developer and the dmvs that we need to
125:23 - solve this use cases we are going to
125:26 - start off in this module by talking
125:28 - about the permission that you need to
125:30 - query the dmvs then we are going to talk
125:33 - about how to use the dmvs to see
125:35 - information like who is connected to sql
125:38 - server and how many resources each
125:40 - session is using
125:42 - then we are going to talk about how we
125:44 - can find what sql statements are
125:46 - currently executing against our server
125:49 - then we will talk about how to find the
125:52 - most expensive statements that are
125:54 - running against our database
125:56 - next we will talk about how we can query
125:59 - sql server to see if it has any
126:01 - recommendation about indexes that might
126:04 - be missing in our database that we need
126:06 - to create
126:07 - and finally we will discuss how we can
126:09 - find if our current index are being used
126:12 - and how often
126:14 - although this gives us a good database
126:16 - level summary of how our application is
126:19 - interacting with sql server
126:21 - and it makes it easy for us to spot any
126:24 - performance bottlenecks that may exist
126:26 - one last item i do want to mention is
126:29 - that all of the queries you are going to
126:31 - see in this module are located on my
126:32 - blog
126:33 - so if you go to the link that you see
126:35 - here on the screen i will also mention
126:38 - this link in the description for your
126:40 - convenience all this query will be there
126:43 - so you don't need to worry about pausing
126:45 - the video and trying to type this query
126:48 - in on your own
126:49 - so with that let's start by talking
126:51 - about the permission we need in sql
126:54 - server in order to access the dmvs and
126:56 - run the queries that we are going to see
126:58 - in this module in order to query the
127:00 - dynamic management views your sql server
127:03 - user needs to have the view server state
127:05 - permission granted to it and unless you
127:08 - are a dba you are unlikely to have this
127:10 - permission by default
127:12 - traditionally sql server's dynamic
127:14 - management views has been thought of as
127:16 - a tool for the dba because they contain
127:19 - information not just related to
127:21 - performance but also about system health
127:24 - and configuration the view server state
127:26 - permission gives you access to all of
127:28 - this information and for some dbs this
127:31 - creates some security concern
127:33 - to give away that level of access
127:35 - it may be possible to get this level of
127:38 - access in your development and test
127:40 - environments but it completely depends
127:42 - on the rules of the company whether this
127:44 - level of access will be available to you
127:47 - as a non-dba user in a production
127:49 - database system
127:50 - if you are at a company that is not
127:52 - comfortable granting this access in
127:54 - production to developers you still have
127:56 - a couple of options though
127:58 - first you could have your dba execute
128:01 - the query
128:02 - shown in this module and forward those
128:04 - result to you
128:05 - the second option is that you could work
128:08 - with your dba team to create some views
128:10 - over the top of the sql server dmvs that
128:13 - only exposes a specific subset of the
128:16 - dmv data to you which would relax any
128:18 - security concerns
128:20 - for this module i am going to assume
128:22 - that you do have the view server state
128:24 - permission available to you as i show
128:26 - you these queries so with that being
128:28 - said let's start taking a look at some
128:31 - of the information sql server can
128:33 - provide to us around connections and
128:35 - sessions in the database
128:37 - you are watching the video of module 4
128:39 - finding performance bottlenecks in sql
128:42 - server part 2 getting information about
128:45 - sql server sessions and resource usage
128:48 - the first query we are going to look at
128:51 - is a query that uses the dm
128:53 - exec sessions dmv in sql server to get a
128:58 - list of all the clients that are
128:59 - connected to our sql server database
129:03 - on my laptop i will get some results
129:05 - from another process that i have running
129:08 - so let's go ahead and run this query
129:10 - right now
129:12 - so we see we get back some information
129:14 - like the session id and the database
129:17 - that the session is currently connected
129:18 - to
129:20 - we also get a status if this session is
129:23 - doing anything at the moment either
129:25 - running or sleeping
129:27 - and we also get when the session was
129:29 - initiated
129:32 - then we get some performance stats about
129:34 - each session
129:36 - the cpu time here is in millisecond
129:39 - memory usage is in 8 kb block and we
129:42 - also have read write and logical leads
129:46 - so if we had a session that are
129:48 - consuming a lot of resources and maybe
129:51 - slowing everyone else down that are
129:53 - connected to sql server we did be able
129:56 - to tell that from this data
129:59 - next we see that we have some
130:01 - information about the client process
130:03 - that owns this session
130:06 - including the host name the program name
130:08 - and the host process id
130:11 - so if we need to know where a session
130:13 - was coming from this would be enough
130:15 - information to help us figure that out
130:18 - now a lot of times what you are going to
130:20 - see in this program name column is
130:22 - something like we see here
130:25 - net sql client data provider
130:27 - and not the actual name of the program
130:29 - that's making the connection
130:31 - for any dotnet program it is this string
130:34 - that is put in by default
130:36 - what you can do though is in the
130:38 - connection string of your program you
130:40 - can include this property application
130:42 - name and then the value that you specify
130:45 - here in the application name property
130:47 - will be used to populate the program
130:49 - name in the column in sql server when
130:52 - you have multiple applications that are
130:54 - using the same database this can be
130:56 - really useful because then you can very
130:59 - easily tell which session is coming from
131:01 - which application
131:02 - the result of this query are useful in
131:04 - two situations one like we said when
131:08 - there is a session that is consuming a
131:09 - lot of resources on sql server
131:12 - second we are also getting a lot of
131:14 - connectivity information here in terms
131:16 - of what clients are connected and how
131:19 - many connection each client is using
131:21 - so for example if i have 4 servers in
131:24 - web cluster i would expect to see about
131:27 - the same number of connections from each
131:29 - one of those machines if i saw that one
131:31 - of those web server didn't have any
131:33 - connection to sql server i would want to
131:36 - go and check the connection string on
131:38 - that machine
131:39 - and also make sure that server was
131:41 - actually receiving in processing web
131:43 - request so there is actually quite a bit
131:46 - that you can tell just by looking at the
131:48 - session information
131:50 - now another question you might want to
131:52 - answer is what sql statement are
131:54 - currently executing in sql server right
131:56 - now
131:57 - so we will look at how to answer that
132:00 - question in the next video
132:02 - you are watching the video of module 4
132:04 - finding performance bottlenecks in sql
132:07 - server
132:08 - part 3 finding what sql statements are
132:10 - currently executing
132:13 - sometimes we have the need to see what
132:16 - statements are running right now in sql
132:18 - server
132:19 - imagine you have a situation where your
132:21 - application is running very slowly or
132:23 - maybe even appears to be unresponsive
132:26 - and you need to quickly be able to treat
132:29 - that situation
132:30 - so you may be trying to figure out if
132:33 - this is a problem in the application
132:34 - code
132:35 - maybe a web service you are calling is
132:38 - down or maybe you have a query in sql
132:40 - server that takes very long time
132:44 - so what you want to be able to do is
132:46 - look at sql server and quickly figure
132:49 - out what your app is doing in the
132:50 - database
132:52 - and if the problem you are seeing is in
132:55 - the database tire and so we can answer
132:57 - this question with a query like we see
132:59 - on our screen right now
133:02 - this query seems quite long but we will
133:05 - walk through it and we will see that
133:07 - actually it's pretty straightforward
133:09 - so we will scroll down to where we can
133:12 - look and see what dmvs this query is
133:15 - using first
133:17 - we see that what this query does is it
133:19 - joined together the dm
133:21 - exec
133:23 - sessions and dm ex ec request view so
133:28 - that we can get what statements are
133:30 - currently running for each session
133:32 - then we need to bring in the text of
133:35 - this statement and we do that with the
133:38 - dm
133:38 - exec sql text view and the execution
133:42 - plan of the statement is brought in with
133:45 - the dm exec query plan view
133:49 - what this will allow us to do is that if
133:52 - we see a statement that we want to know
133:54 - more about we will be able to just click
133:56 - on that execution plan in the result set
133:59 - and bring that execution plan up
134:02 - so that is pretty useful
134:04 - also in the dm exec request view there
134:07 - is a column called blocking session id
134:10 - and if there is another session that has
134:13 - a sql statement that is blocking this
134:15 - sql statement
134:16 - that column will have a non-zero value
134:19 - so that is what these three views here
134:22 - are about
134:24 - if a sql statement is being blocked by
134:26 - another session we will get all of the
134:29 - information on the blocking statement
134:30 - and blocking session right here in the
134:32 - same query
134:34 - looking at the where clause these lines
134:36 - tells sql server not to return any
134:39 - statement from our current session
134:41 - so basically we are not going to get our
134:43 - dmv query that we are looking back here
134:46 - in our result set
134:48 - also we have this line here which is
134:50 - going to limit the result to the current
134:52 - database if you want to see all of the
134:55 - activities on the server you would just
134:58 - remove this line or command it out
135:01 - let's jump back up to the top to where
135:03 - we can see what data we are getting back
135:07 - we have the session id of the sql server
135:09 - session
135:10 - and we also have the sql statement here
135:13 - these three line here that are labeled
135:15 - sql statement what these are doing is if
135:18 - the statement that running is part of a
135:20 - stored procedure these are getting the
135:23 - individual sql statement so we can look
135:25 - at
135:27 - if we have just a plain old sql
135:29 - statement then the sql statement field
135:32 - and the parent statement field which is
135:34 - down here those are going to be the same
135:38 - but if we are inside of a stored
135:39 - procedure we will get the individual
135:42 - statement here and then parent statement
135:45 - will be the name of the stored procedure
135:48 - you can see we also have some
135:50 - information about the client that is
135:52 - running the statement and this includes
135:54 - the host name the program
135:57 - and the process id of that client
135:59 - so for example
136:02 - if an asp.net web app was running a
136:04 - statement
136:05 - the process id field here would contain
136:08 - the process id of the worker process
136:10 - over on the iis web server
136:14 - then we have some statistics about the
136:16 - statement itself
136:17 - including what time the statement
136:19 - started
136:20 - how long it has been running
136:23 - how much cpu it has consumed and how
136:25 - much io it has performed
136:28 - in most situation these numbers are
136:29 - going to be quite low
136:31 - because most statements run for a couple
136:34 - of hundred of milliseconds
136:36 - if you have a statement though that's
136:38 - taking a long time you would see that
136:40 - reflected in these values
136:43 - and if you can run this query over and
136:45 - over again you would see these numbers
136:47 - continue to increase with each
136:49 - subsequent run
136:52 - so that it could indicate that the
136:54 - statement is not just running a long
136:56 - time but also consuming a lot of
136:58 - resources
136:59 - as it's running on the database server
137:02 - we can see that we are also getting the
137:04 - execution plan back in the query plan
137:06 - field
137:07 - so if we do have a statement running we
137:10 - can look and see what is doing inside of
137:13 - the sql server
137:14 - and then we also have the blocking
137:16 - session information
137:18 - but this will only be populated if
137:21 - indeed our statement is being blocked
137:24 - so let's go ahead and run this query and
137:27 - see what we get
137:29 - so here we see that on my system at the
137:32 - moment that i run this query we have few
137:34 - statements coming back
137:36 - on a real production system you would
137:38 - probably see more statements in more
137:40 - variety than what we are seeing here by
137:42 - just running locally on my laptop
137:46 - we see all of the fields that we talk
137:48 - about so if we want the sql it's right
137:51 - there and then we also see some
137:53 - statistics
137:55 - what i want to show you though is if i
137:57 - move over here to the right we have the
138:00 - execution plan labeled in this query
138:02 - plan field
138:04 - and i can just click on this
138:06 - so management studio popups the
138:08 - execution plan that's being used by the
138:10 - statement and that's pretty useful
138:13 - because we can see the actual execution
138:16 - plan that the statement is running
138:18 - especially we have a performance problem
138:21 - in a production environment
138:23 - because this can clue us in about what's
138:27 - actually happening up on our database
138:29 - server
138:30 - it's useful to know what's running right
138:32 - now
138:33 - but other times we want to be able to
138:35 - get a bigger picture view of performance
138:38 - and what statements are performing well
138:40 - and which ones aren't
138:42 - so we will talk about the query that
138:44 - will do that for us in the next video
138:47 - you are watching the video of module 4
138:50 - finding performance bottlenecks in sql
138:52 - server
138:53 - part 4 finding the slowest and most
138:56 - expensive sql statement
138:58 - one of the most useful thing you can do
139:00 - with the sql server dms is to have sql
139:03 - server gives you a list of all the
139:06 - statements that your application has run
139:09 - along with the execution statistics for
139:11 - those statement
139:13 - if you are looking for some quick bins
139:15 - for performance in your application this
139:18 - query is a great place to start
139:20 - because this immediately tells you what
139:23 - statements are performing well and what
139:25 - statements you need to target for
139:27 - performance tuning
139:29 - so how does sql server know this
139:32 - every time sql server processes a
139:34 - statement it keeps tracks of the
139:37 - execution plan that it uses
139:39 - and some execution statistics about the
139:42 - statement and store this data in an
139:45 - in-memory structure
139:47 - inside of sql server
139:49 - since this data is in memory what data
139:52 - inside the structure can and does change
139:55 - as new statements are run and old
139:57 - statements are cached out
139:59 - but generally you will have the last
140:01 - several hours worth of data available in
140:04 - the dmv that shows you this view
140:07 - and this is plenty of data to get a
140:09 - strong pulse about what is happening
140:11 - with your application
140:12 - so let's look at this query
140:15 - this query is based around the dm exec
140:19 - query stats view and you can see that we
140:22 - are also pulling in the
140:23 - text of the sql statement and the
140:25 - execution plan for the statement
140:28 - this is handy to pull in the execution
140:30 - plan because if you do find a poorly
140:33 - performing statement we can look at the
140:35 - plan being used and use the skills we
140:38 - have learned in the last couple of
140:39 - modules to analyze the plan for the
140:41 - statement
140:43 - if we look back up towards the top of
140:45 - the editor window
140:47 - we see the data that we are returning
140:50 - we see that we have the sql text like
140:53 - before so we know what statements we are
140:55 - talking about
140:57 - and then we have a number of stats about
140:59 - this statement
141:00 - like the number of times that it's been
141:03 - executed the average amount of cpu per
141:05 - execution and the total amount of cpu
141:08 - across all executions
141:10 - and we have similar measures for both
141:12 - logical i o and alph stamp
141:15 - one thing i do want to point out is that
141:18 - the total rows column here
141:21 - this was added in sql server 2008 r2
141:24 - so if you are still on sql server 2008
141:28 - you will want to comment this line out
141:31 - what is useful to do though with this
141:33 - query is that we can take the result of
141:35 - the query and we can sort those result
141:38 - by various parameters
141:40 - with this query right now at the moment
141:42 - i am actually sorting by the average
141:44 - alerts time so that's going to be
141:47 - statements that taking the longest
141:49 - however i could also sort this by the
141:51 - average amount of cpu time per statement
141:54 - execution or the average amount of io to
141:57 - find the most resource hungry statements
142:01 - if you can also be really interested to
142:03 - sort the results by the number of
142:04 - execution so you can see what statements
142:07 - from your applications are getting run
142:10 - most frequently
142:12 - if you see a statement that has a very
142:14 - high number of executions maybe that
142:17 - statement is being executed inside of a
142:19 - loop
142:20 - and you want to rethink how you are
142:22 - doing that data access or maybe the data
142:24 - returned by that statement is
142:26 - essentially static
142:28 - so you might want to consider if you
142:30 - could possibly catch that data
142:33 - so like all the queries that i am
142:34 - showing you in this module take a moment
142:37 - and experiment with these queries in
142:38 - your database
142:40 - because that's really the best way to
142:42 - learn how to use this query and what
142:44 - they can tell you
142:46 - so let's go ahead and run this query
142:50 - so this is what your data is going to
142:52 - look like when it come back
142:54 - and this is pretty straightforward to
142:56 - interpret this data
142:59 - one thing that i want to point out is
143:01 - our dime based measurements
143:03 - which are the cpu time and the ellipse
143:05 - time column this units are in
143:07 - microseconds
143:08 - so we move the decimal places over 6
143:11 - places to the left
143:13 - we can see that i have got a number of
143:15 - statements here that are taking over 5
143:17 - seconds
143:18 - and it look like those statements are
143:21 - also are using a lot of cpu
143:24 - so if this was a real database i did be
143:27 - pulling the sql out of here
143:29 - and then i will be scrolling over here
143:32 - to the right where i can see the
143:34 - execution plan for the statements to
143:36 - analyze what is going on
143:39 - so you can see this query is very useful
143:41 - because it can help you immediately
143:43 - pinpoint any statement that are really
143:45 - slowing down your application so you can
143:48 - get those fixed
143:50 - even after i have tuned all of these
143:52 - statements in my application i still
143:54 - like to run this query every week or so
143:58 - just to make sure that nothing has
144:00 - changed or snug up on me in terms of a
144:03 - performance issue
144:05 - next we are going to take a look at how
144:08 - sql server can give us some
144:09 - recommendation on if any indexes are
144:12 - missing in our database
144:14 - you are watching the video of module 4
144:17 - finding performance bottlenecks in sql
144:19 - server
144:20 - part 5 getting sql servers
144:22 - recommendation on missing indexes
144:26 - we have seen in this course that when we
144:29 - run a sql statement in management studio
144:32 - if sql server thinks the statement would
144:35 - benefit from an index it will give us
144:37 - that recommendation right in management
144:39 - studio
144:40 - sql server doesn't just make this
144:42 - recommendation in management studio
144:45 - but
144:46 - anything a sql statement is run these
144:48 - recommendations are logged to a series
144:51 - of dmvs known as the missing index dmes
144:54 - we don't want to automatically create
144:56 - every index that is recommended
144:59 - in this series of views
145:01 - but looking at this result can help us
145:03 - finding an index that we may have
145:05 - otherwise overlooked
145:08 - the query you see on your screen will
145:10 - join these views together and give us
145:13 - some statistics about the
145:14 - recommendations
145:16 - so let's go ahead and run this query to
145:19 - see what we get
145:21 - the first column in the result set is
145:23 - the table name
145:24 - and this is the table that the index
145:26 - recommendation is for
145:29 - then we have three columns contain
145:31 - information about what columns a
145:33 - potential index would be created across
145:35 - and we see the column names are given to
145:38 - us in a comma separated list within each
145:41 - of these columns
145:42 - mainly you are going to be looking at
145:44 - the column called equality columns
145:48 - because most where clauses and join
145:50 - clauses are based on the equality
145:52 - relationship
145:55 - we also see that we have some stats
145:57 - about why sql server thinks this index
146:00 - would be a good idea
146:02 - this two column here user scans and user
146:05 - seeks
146:06 - represent the number of times that sql
146:08 - server could have used this index had
146:11 - this index exist
146:12 - so for this top row this means that sql
146:16 - server could have performed an index
146:18 - seek operation against this index over
146:21 - 12 000 times had the index existed
146:25 - we also see the average cost of the
146:27 - statements that could have used the
146:28 - index was and this is the cost of the
146:31 - statements without the index existing
146:34 - the next column
146:35 - avg user imp
146:38 - tells us the percentage that sql server
146:40 - thinks that this cost would have reduced
146:43 - if we have had this index
146:47 - finally the last two columns are some
146:49 - simple calculations
146:51 - the first column give us the average
146:53 - cost saving per statement we would have
146:55 - by having this index
146:57 - and the last column give us the total
146:59 - saving we would have across all
147:01 - statements that could benefit having
147:03 - this index
147:06 - so this last value is useful to sort
147:08 - these results to get an idea
147:11 - what the most impactful index to create
147:13 - might be
147:15 - now when you run this query and pull
147:17 - this list up
147:18 - what you don't want to do is just start
147:21 - working your way down the list and
147:23 - creating an index for each row in this
147:26 - result set
147:27 - this is because sql server has a
147:29 - tendency to recommend an index to
147:31 - optimize every individual statement and
147:34 - if you did that you would be in a over
147:37 - index situation
147:39 - which would slow down all your dml
147:41 - statements against your table
147:44 - instead what you want to do is to scan
147:47 - through the result for each table you
147:49 - will able to pick out some pattern of
147:51 - index recommendation across similar
147:53 - columns
147:54 - and then you can combine the information
147:57 - with your knowledge of how your
147:59 - application works to come up with the
148:00 - right sets of index for each table
148:04 - so for example this last 5
148:06 - recommendations that are in the list are
148:08 - all about the student table
148:10 - and by scanning through the columns that
148:12 - are recommended we see that all of the
148:15 - recommendation includes the last name
148:17 - column
148:18 - so we know that we need to have an index
148:20 - that start with last name
148:22 - we also see that first name is used in
148:25 - three of the recommendation so that is
148:27 - good choice for the second column in our
148:29 - index and then maybe we will choose
148:31 - state as a third column because that's
148:34 - used two of the recommendation as well
148:37 - the point is that you want to look at
148:39 - the different recommendations for a
148:41 - table and extract out the common columns
148:44 - such that you are creating the fewest
148:46 - number of possible indexes on the table
148:49 - in a lot of cases you will have a number
148:51 - of similar recommendations that all have
148:54 - a common subset of columns
148:56 - so you want to create just one index
148:58 - that contains those common columns
149:01 - this way you are creating an index that
149:04 - can benefit multiple statements rather
149:06 - than trying to create individual index
149:08 - for each statement which would be
149:10 - inefficient
149:11 - so when you look at the results of this
149:14 - query keep in mind that these are just
149:16 - recommendations
149:17 - and it's up to you to analyze and
149:19 - combine this recommendation together
149:22 - into the actual indexes that you are
149:24 - going to want to create on your tables
149:27 - now the flip side of missing an index is
149:29 - having index on table that aren't being
149:32 - used
149:33 - so we will look at a dmv query that can
149:35 - help us finding those situation in the
149:38 - next video
149:39 - you are watching the video of module 4
149:41 - finding performance button lag in sql
149:44 - server part 6 finding indexes that are
149:47 - not being used
149:49 - in the last module
149:50 - we talked about how sql server has to
149:52 - maintain any index you create on your
149:55 - tables
149:56 - by updating an index every time a dml
149:59 - statement is executed
150:01 - against the table
150:03 - so while an index will greatly speed up
150:06 - a query there is a slight performance
150:08 - penalty in terms of executing a dms
150:11 - statements against the table
150:14 - therefore we want to make sure our
150:16 - application is using all of the indexes
150:18 - we have created in our database
150:21 - because otherwise we are paying the cost
150:23 - of maintaining an index which is not
150:25 - being used or
150:27 - not giving any benefit
150:29 - and we can do this with a query that
150:32 - looks like what you are seeing on your
150:34 - screen
150:35 - which uses the dm db index usage stats
150:39 - dmv in conjunction with the sys indexes
150:43 - system
150:44 - views i will run this query and then we
150:48 - can discuss how it works
150:50 - so we see in the first few columns we
150:53 - have information like the table name
150:56 - the index name and the index type
150:59 - remember that an index type of cluster
151:01 - really means a table data
151:04 - as this is how sql server normally
151:06 - stores the data for a table in a cluster
151:09 - index structure
151:11 - what is really of interest to us though
151:14 - are these statistics that are located
151:16 - out at the right of the result set
151:18 - and i am going to expand this column
151:21 - just a little bit so we can read this a
151:23 - little bit better
151:25 - and there we go
151:27 - so user seeks scan and lookups each
151:31 - represent the number of times
151:33 - respectively that the index has been
151:35 - used of those types of operations since
151:38 - sql server was last restarted
151:41 - so for a standard non-cluster index that
151:44 - we would evaluate
151:46 - we would be most interested in the value
151:48 - of user 6
151:49 - as
151:50 - this is the number of times that sql
151:52 - server is using the index as intended by
151:55 - searching the tree structure of the
151:57 - index for matching keys
151:59 - we also see that we have a column named
152:01 - user updates
152:03 - and this value reflects the number of
152:05 - times that sql server has had to update
152:07 - the index
152:08 - because of a dml statement being run
152:11 - against the table
152:12 - now when you run these statements on
152:14 - your database you aren't going to see
152:17 - all zeroes in this columns like i am
152:20 - but you will see actual values
152:24 - what you are looking for here is indexes
152:27 - that have high number of updates and
152:29 - very low usages
152:31 - generally meaning a low number of user
152:33 - seeks operation
152:35 - this means that we are having to update
152:37 - the index more often than its being used
152:40 - so if we find one of these situations we
152:42 - want to investigate why this is
152:44 - happening
152:45 - maybe the index has a wrong column order
152:48 - or it isn't selective enough
152:51 - or maybe something has changed in our
152:53 - application such that we are not
152:55 - querying a table in the same way that we
152:57 - are used to
152:59 - whatever the case you want to understand
153:01 - the situation and see if you can
153:03 - possibly modify that index so it can be
153:06 - used
153:07 - or if you determine that the index
153:09 - really doesn't have a purpose then you
153:11 - want to drop the index so it isn't
153:14 - slowing down your dms statements
153:17 - i do want to give you one word of
153:19 - caution
153:20 - though sometimes you have an index that
153:23 - isn't used by your website or other
153:25 - interactive applications
153:27 - but it is critical to a night batch
153:30 - process
153:31 - in this case you will have only a
153:33 - handful of usages of the index
153:36 - because of course a nightly batch
153:38 - process usually runs just once per night
153:42 - but you don't want to drop these indexes
153:45 - because otherwise there will be serious
153:47 - performance consequences for those batch
153:50 - processes
153:51 - so just make sure that when you are
153:53 - getting ready to drop an index you have
153:56 - really thought through all of the
153:58 - processes that may be using the
154:00 - underlying table
154:01 - and make sure you have not missed an
154:03 - important use case like a nightly or
154:05 - weekly batch process
154:07 - let's go ahead and wrap up our
154:09 - discussion of dmvs and this module in
154:12 - the next video
154:13 - you are watching the video of module 4
154:16 - finding performance bottlenecks in sql
154:18 - server part 7 module summary
154:22 - in this module we have seen that sql
154:24 - server collects all types of diagnosis
154:26 - and performance information and makes it
154:29 - available to us via its dynamic
154:32 - management views
154:34 - by queering these views we can get an
154:36 - overall picture of what is happening
154:38 - with our application inside of sql
154:40 - server
154:41 - so we can quickly identify problem areas
154:44 - and address them
154:46 - we looked at five different queries that
154:48 - we could use and these queries gave us
154:50 - information on what client had sessions
154:53 - connected to sql server
154:55 - what sql statements are currently
154:57 - executing
154:59 - what statements are taking the longest
155:01 - to run and using the most resources
155:03 - inside the
155:05 - sql server
155:06 - what indexes might be missing in our
155:08 - database and what the current usage of
155:11 - our existing indexes is
155:14 - there are many more queries that could
155:16 - be written
155:17 - and of course the queries we have shown
155:20 - here
155:21 - would all be modified
155:23 - really the best way to learn about dmvs
155:25 - is to try them out on your system
155:28 - and you will be sure to learn something
155:30 - about your application data access layer
155:33 - in the next module we are going to
155:35 - continue talking a more global view of
155:37 - how our application interacts with sql
155:40 - server and demonstrate how you can trace
155:43 - all the sql statements that your
155:45 - application sends to sql server
155:48 - and at the same time collect detailed
155:50 - performance data about them
155:53 - so let's start it from the next video
155:56 - you are watching the video of module 5
155:58 - capturing what your application is doing
156:01 - inside sql server part 1 module
156:04 - introduction
156:06 - in this module we are going to talk
156:08 - about the ways you can trace all your
156:10 - data access statements inside of sql
156:13 - server
156:14 - and by trace i mean capture each
156:17 - statement your application run inside of
156:20 - sql server along with some performance
156:22 - statistics about that statement
156:25 - why would we want to do this
156:28 - well
156:28 - there are couple of common reasons
156:31 - one as a developer we might want to know
156:34 - exactly what a process inside one of our
156:37 - applications is doing inside of sql
156:40 - server
156:41 - maybe we are trying to debug a process
156:44 - that doesn't seem to be working
156:46 - correctly or maybe we have taken over
156:49 - support of an existing application
156:53 - what tracing allows us to do is to see
156:55 - the exact sql statements the application
156:58 - is running with their values and the
157:01 - order in which they are being run
157:03 - in so
157:04 - we can tell exactly what table are being
157:06 - hit and how they are being used
157:10 - which is really useful for debugging
157:13 - second when we use the tracing
157:15 - capabilities that are built into sql
157:18 - server we can get detailed performance
157:20 - information for each statement that is
157:23 - being executed from our application
157:26 - in the last module we saw how we can get
157:28 - aggregate statistics like average cpu
157:31 - time or average logical reads by
157:34 - querying sql servers dynamic management
157:36 - views
157:38 - as we will see when we trace our data
157:41 - access in sql server
157:43 - we will be able to get statement level
157:45 - statistics for each statement that is
157:48 - run
157:49 - so we can really understand in detail
157:51 - how each statement is performing
157:54 - so we might use tracing in one of our
157:57 - development environments to understand
157:59 - how one of our applications is working
158:02 - or we might use it in a test environment
158:05 - during a load test of our application
158:07 - together
158:09 - detailed information about what parts of
158:11 - our data access are performing and which
158:14 - parts needs to be tuned
158:16 - and finally we could even use a trace in
158:19 - our production environment if we wanted
158:21 - to capture detailed information about
158:23 - every statement that took over a certain
158:26 - amount of time let's say 5 second or
158:30 - accelerate some other performance
158:31 - threshold
158:33 - in sql server there are two different
158:35 - ways which you can trace what is
158:37 - happening inside of the database
158:39 - and which one you choose depends on the
158:42 - version of sql server that you are
158:44 - running
158:45 - if you are running sql server 2008 r2 or
158:48 - earlier
158:50 - you should use a tool called sql
158:52 - profiler and this is provided as a
158:55 - separate gui in the sql server client
158:57 - tools
158:58 - if you are running sql server 2012 or
159:02 - later
159:03 - or using sql azure then you are going to
159:06 - use what are known as sql server
159:09 - extended events to perform your trace
159:12 - going forward sql server extended events
159:14 - are going to be standard and in fact the
159:17 - only way to perform a trace in sql
159:19 - server
159:21 - as the sql profiler tool was depreciated
159:24 - in sql server 2012
159:26 - you can check out this on microsoft site
159:29 - with the link shown on screen
159:31 - but you will still find sql profiler
159:34 - widely used
159:35 - so we will cover it
159:37 - but known that the trend is moving
159:39 - towards extended events
159:42 - so if you are on sql server 2012 or
159:45 - later you want to be investing your time
159:48 - in learning how to use extended events
159:50 - and probably skip learning profiler
159:53 - regardless of which approach you end up
159:55 - taking
159:56 - running a trace on sql server requires
159:58 - some pretty high level permission in the
160:00 - database
160:01 - which we will discuss when we
160:03 - demonstrate each approach
160:06 - this is because a trace can capture
160:08 - every sql statements and it value that
160:11 - are being executed against the database
160:14 - and clearly there are some security
160:16 - concern in letting someone do this
160:18 - if you have sql server installed on your
160:20 - workstation
160:22 - you will be able to run a trace for any
160:24 - local testing that you do against that
160:27 - local instance
160:28 - for your development and test
160:30 - environments if you will be able to run
160:32 - a trace is going to be vary by company
160:36 - in production you will almost certainly
160:38 - not have permission to run a trace by
160:40 - yourself
160:41 - and will need to engage someone from
160:43 - your dba team to help you run a trace
160:47 - however don't be discouraged by this
160:50 - quite the opposite
160:52 - being able to trace all of your
160:53 - applications sql is a powerful
160:55 - capability and it is important that you
160:58 - have a developer be aware of its
161:00 - capability and what it can do so that
161:04 - when you need to do it you know that the
161:06 - capability exists and you can ask for it
161:09 - so what we are going to do in this
161:11 - module is guide you through using both
161:14 - sql profiler and sql server extended
161:17 - events to take a trace for each method i
161:20 - will show how to set up a trace
161:22 - including what events you can log how
161:25 - you can filter the data you capture and
161:27 - how you log this data to a file for
161:30 - later review
161:31 - i will also discuss what you need to do
161:34 - in order to analyze this data
161:36 - when we are finished you will have a
161:38 - good overview of how to set up a trace
161:41 - in both tools
161:43 - and a good idea of what data you can
161:45 - capture within sql server
161:48 - this will help you understand what
161:49 - tracing capabilities exist in sql server
161:53 - so you know when a trace might be useful
161:56 - in your application development process
161:59 - so let's get started by looking at the
162:01 - sql profiler tool in the next video you
162:04 - are watching the video module 5
162:06 - capturing what your application is doing
162:09 - inside sql server
162:10 - part 2 setting up of sql profiler trace
162:14 - to run sql profiler you have to go to
162:17 - your microsoft sql server tools in your
162:20 - start menu
162:21 - here you can see on must install
162:24 - it is labeled sql server profiler 18
162:28 - and you just click on this icon
162:31 - you can also start sql server profiler
162:34 - from sql server management studio
162:37 - for that
162:38 - click on tools menu
162:40 - then the first menu is sql server
162:43 - profiler
162:45 - once you click on the sql server
162:46 - profiler menu
162:48 - the app will open
162:51 - and this is what the screen will look
162:53 - like
162:54 - you want to go up here to the left upper
162:57 - corner
162:58 - and click on the icon that is all the
163:00 - way on the left to start a new dress
163:04 - then as we see here we have a login
163:07 - dialog box pop-up and what we need to do
163:09 - here is to login to the sql server
163:12 - instance
163:13 - where we want to run the sql trace on
163:16 - also we need to log into sql server as a
163:19 - user that has the altered trace
163:22 - permission
163:23 - otherwise when we click connect here we
163:25 - are going to get an error
163:28 - in this case i am just running a trace
163:30 - on my local sql server instance so i
163:33 - will have a permission to do this
163:36 - but as we talked about in the intro of
163:39 - the module you may or may not have
163:41 - rights to run a trace against different
163:44 - sql server
163:45 - at your company
163:47 - so i will click connect here and then we
163:51 - will presented with this dialog box
163:53 - where we can specify the various options
163:56 - for the trace that we are going to
163:58 - perform
163:59 - so first we will specify a name and then
164:02 - we don't have to specify an output file
164:05 - where we want this data to go
164:08 - but it's a generally a good idea so you
164:11 - can review the data later
164:13 - so we are going to check this box to
164:15 - save to a file
164:17 - and we will specify a file name here
164:20 - and then we get this option to specify a
164:23 - maximum file size
164:25 - which i am going to set 25 mb
164:28 - then i also want to make sure that file
164:31 - rollover is set
164:33 - so as this files fill up profiler with
164:36 - roll them over to a new file
164:39 - now what i want to do is to set the
164:41 - events i want to capture
164:44 - and some filter criteria about what to
164:46 - capture
164:47 - and i do that on this second tab called
164:50 - event selection
164:52 - you can see we already have some default
164:54 - selected
164:55 - and from a developer perspective
164:58 - wanting to trace some sql statements and
165:01 - get their performance information
165:03 - these are actually some pretty good
165:05 - defaults
165:06 - these names are pretty self-explanatory
165:09 - but if you mouse over one of them you do
165:12 - get some additional description down
165:14 - here in the lower part of the dialog
165:17 - the rpc completed event will fire when a
165:19 - stored procedure completes
165:21 - and the sql batch complete event fires
165:24 - when individual statements completes
165:26 - so you want to make sure to have both of
165:29 - this event checked
165:30 - so you can capture all the sql from your
165:33 - application
165:35 - this is important even if you are not
165:37 - using stored procedure explicitly
165:39 - because the way that some of the
165:41 - database drivers work
165:43 - they actually in some cases will wrap
165:46 - your sql inside of the sp execute sql
165:49 - build in procedure
165:51 - so you just want to make sure that both
165:53 - of these boxes are checked also you want
165:56 - to make sure to check the box labeled
165:59 - text data here on rpc completed
166:02 - because this is what is going to let us
166:04 - see the sql that's been executed as part
166:07 - of that event
166:09 - if you can see that we have a number of
166:11 - other parameters here like cpu and
166:14 - logical reads and i suggest you to leave
166:17 - all of this checked so you get this
166:19 - performance information
166:22 - now there are many other events that you
166:24 - can capture and to see all of these
166:26 - events you check this box here named
166:28 - show all events
166:30 - as you can see this list is quite
166:32 - extensive
166:34 - though most of the items in this list
166:36 - are of more interest to dbs than to us
166:39 - as developers
166:40 - i am not going to go through each and
166:42 - every one of these obviously but there
166:46 - is one that i want to point out and that
166:49 - is in the error and warnings section and
166:52 - the event name is user error messages if
166:55 - you have a sequel statement in your
166:57 - application that throws an error when it
166:59 - runs
167:00 - that error will be captured by this
167:02 - event
167:03 - so this can be useful debugging tool if
167:05 - that error isn't otherwise getting
167:07 - bubbled up to your application logs
167:10 - this event will also log some
167:12 - informational messages that sql server
167:15 - generates but for our purposes we can
167:18 - disregard those informational messages
167:21 - because what we are really interested in
167:23 - for this event is the ability to capture
167:25 - errors from our sql statements
167:28 - i am going to uncheck the show all
167:31 - events check box and you see this gets
167:34 - up back to just the event that we are
167:36 - going to capture
167:37 - and now we want to talk about this other
167:40 - button here column filters
167:42 - the way we have things right now we are
167:44 - going to capture every login logout and
167:47 - sql statements that executed across the
167:50 - entire sql server instance
167:52 - if you are running a trace against your
167:54 - local install of sql server
167:57 - that will probably work fine because you
167:59 - are probably the only user but on a busy
168:02 - server you are going to capture a lot of
168:04 - events that you don't care about
168:07 - this not only makes it harder to sort
168:10 - through all of the data and find what is
168:12 - important to you
168:14 - but it can also have a negative impact
168:16 - on the performance of sql server because
168:19 - you are capturing so many events
168:21 - so we want to filter the data so that we
168:23 - just capturing the data that's of
168:25 - interest to us
168:27 - one of the things you probably want to
168:29 - do is to limit the data you collect to
168:31 - just your application
168:33 - you can do this using the application
168:35 - name filter or by the login name filter
168:38 - assuming that your application uses a
168:40 - unique login for sql server
168:43 - if you go to the application name root
168:45 - remember you need to set this
168:47 - application name in the connection
168:49 - string your application uses
168:52 - in dotnet this is done by including the
168:54 - application name parameter in the
168:56 - connection string and something similar
168:58 - can be done in other languages as well
169:01 - you can also limit your trace to
169:02 - statements that takes a longer time or
169:05 - use a lot of resources with columns like
169:07 - duration
169:08 - cpu or writes
169:11 - to set one of these just click on it and
169:14 - then
169:14 - go over here to the criteria you want to
169:17 - set
169:19 - and in this case i want queries that
169:21 - takes longer than 5 seconds
169:23 - so that's 5000 milliseconds
169:26 - and set the value
169:28 - and now this trace would capture just
169:31 - this long running queries
169:33 - if you want to get rid of this criteria
169:36 - which i do for the demo
169:38 - that i am going to run in just a second
169:40 - here then you just double click on the
169:42 - values so that you can edit the values
169:45 - and delete it
169:46 - and now that criteria will not be used
169:49 - anymore
169:50 - so now we have our trace setup and in
169:52 - the next video we will continue with
169:55 - this and go ahead and run this trace and
169:57 - collect some data you are watching the
170:00 - video module 5 capturing what your
170:02 - application is doing inside sql server
170:05 - part 3 running a sequel profile address
170:10 - so we got our sql profiler trace setup
170:13 - in the last segment
170:14 - and now all we have to do to run the
170:17 - trace is to click this button here
170:20 - that says run
170:23 - now in the background you should know i
170:25 - do have a program running
170:27 - that's generating some synthetic load
170:30 - against my sql server instance
170:33 - so we will be able to see something when
170:35 - we actually run this trace
170:38 - so i will go ahead and click this button
170:42 - and there we go
170:44 - we see this window pop-up and we can see
170:46 - the events that are being captured by
170:48 - the trace in this window
170:51 - now running the trace interactively like
170:54 - this is fine if you are on a local
170:56 - machine like i am
170:58 - or otherwise maybe a dev server that has
171:01 - a very very light load on it
171:04 - but you don't want to run a trace
171:06 - interactively like i am doing against a
171:09 - busy server because
171:11 - otherwise you could really cause some
171:13 - performance degradation on that server
171:17 - there is a way to convert our setup here
171:20 - to run what is called a server side
171:22 - trace
171:23 - that runs just up on the server itself
171:26 - and captures its data to a file
171:29 - and that's more efficient and i will
171:32 - show you how to do that in just a moment
171:35 - but for now
171:37 - know that this interactive mode is
171:39 - something that you only want to be doing
171:42 - locally
171:43 - or maybe you do it for just a very brief
171:45 - instance of time on a server to make
171:48 - sure you are capturing the right data
171:50 - and then you turn it off
171:53 - that way you can avoid any performance
171:56 - impact
171:57 - i am going to actually go ahead and stop
171:59 - this trace
172:00 - because i think we have captured enough
172:03 - data to review in the time that i have
172:06 - been talking
172:08 - and so to stop the trace we just hit
172:10 - this stop button here
172:13 - in the main pane
172:14 - we see the event that are captured by
172:16 - the stress in the order
172:18 - they occurred
172:20 - if i click one of these i will see the
172:23 - text of the command down here in the
172:25 - lower pane with the values that were
172:27 - submitting for this query
172:29 - i can also see up here in the main grid
172:32 - some of the performance stats about this
172:35 - statement
172:36 - like cpu reads rights and duration
172:40 - so what i can do is i can scroll through
172:43 - here and i can look for statements that
172:45 - were the most expensive ones
172:48 - and then inspect the sql for that
172:50 - statement
172:51 - and any parameters that were used in
172:54 - that statement
172:56 - what is lacking about this user
172:57 - interface is that there is no way to
173:00 - sort or filter the data once it's been
173:02 - taken
173:04 - so you can see i can go up here and
173:06 - click on the column header and nothing
173:09 - happens
173:11 - i am not getting the data to sort here
173:14 - so if you want to be able to slice and
173:16 - dice this data
173:18 - what you are going to do is to save this
173:20 - data to a database table
173:22 - and then you can query it with just some
173:25 - normal sql like any other table
173:28 - so to do this you are going to go up
173:30 - here to the file menu and then down to
173:33 - save as
173:35 - and then select trace table
173:37 - this will prompt you for a database
173:39 - login and what you are going to want is
173:42 - a place and a login that profiler is
173:44 - going to be able to create a table to
173:47 - store this trace data in if that table
173:50 - doesn't already exist and so what i have
173:53 - done on my machine here is i have
173:55 - created a separate trace database called
173:58 - trace data on my local sql server
174:00 - instance
174:01 - and that's where i am going to put this
174:04 - data
174:05 - so i will go ahead and login
174:08 - and then i am going to find trace data
174:11 - in this list for my databases i will
174:13 - leave the schema as dbo and finally i am
174:16 - going to give this table a name
174:19 - and now i will click ok
174:21 - and so that's going to do is to take
174:24 - this data and create that table and
174:26 - insert all of the data into the table
174:29 - and so now if i pop up over to
174:32 - management studio i can go and i can
174:36 - find the database
174:38 - and i will open the tables and there is
174:41 - our table
174:42 - and i will just right click so that i
174:45 - can grab the first thousand rows
174:47 - and there those rows are
174:50 - so obviously i could write sql queries
174:53 - against this table like any other table
174:56 - and i could filter and sort this data
174:59 - however i wanted to
175:01 - now that i have it in a table
175:04 - we mentioned that you could also run a
175:06 - trace as a server-side trace which is
175:08 - more efficient and therefore more
175:10 - appropriate if we are trying to trace
175:13 - access in a server environment rather
175:15 - than on our local machine
175:17 - so let's see how to do that in the next
175:21 - video
175:22 - you are watching the video module 5
175:24 - capturing what your application is doing
175:26 - inside sql server part 4 running a trace
175:30 - as a server-side trace
175:32 - when you use sql profiler to trace your
175:35 - sql statements what you really want to
175:38 - do is to run your trace as a server-side
175:40 - trace rather than a interactive mode
175:44 - because a server-side trace will consume
175:47 - fewer resources in sql server
175:50 - so what we do is use the sql profiler
175:53 - tool to setup the trace
175:56 - meaning we select the events we want and
175:58 - add in any filters and then we can
176:01 - instruct profiler to give us a sql file
176:04 - of the commands
176:06 - that need to be run in order to run this
176:08 - trace on the server side
176:10 - so let's see how to do this
176:14 - i have the same trace here that we have
176:16 - been using throughout
176:18 - and we have this all set up
176:21 - and then all we need to do is go up here
176:25 - to file
176:26 - export and script trace definition
176:29 - then we will choose from sql server 2005
176:32 - to 2019
176:34 - and we are just going to save the sql
176:37 - file somewhere on our local machine
176:41 - so now what we need to do is we need to
176:44 - open that sql file in management studio
176:48 - and i will jump over to management
176:50 - studio and i actually have a version of
176:52 - this file that i saved earlier
176:56 - already opened up here in the window
176:58 - that we can see
177:00 - we see that all the sql file is a series
177:03 - of commands that we are going to run on
177:05 - sql server
177:07 - the only thing that we need to do is to
177:09 - put in a file name of where the data is
177:12 - going to be logged to
177:13 - and we see that we need to do that right
177:16 - here and there is nice long command
177:19 - telling us exactly what we need to do
177:22 - the file path is going to be the path
177:24 - that is going to be up on a sql server
177:28 - so that need to be a directory that
177:30 - exists on a machine that hosting sql
177:33 - server
177:34 - in my case i am running sql server
177:36 - locally so the client and server are one
177:39 - of the same
177:41 - but just remember that part is up on the
177:44 - server
177:45 - so i will go ahead and set this
177:48 - and then we will look through the rest
177:50 - of the file here just to see what else
177:53 - is in here
177:54 - and what all these commands do is they
177:57 - set up the events that we want to
177:59 - capture
178:00 - and any filters
178:02 - so sql profiler has generated all of the
178:05 - correct
178:06 - sp trace
178:08 - set event commands for us so we don't
178:11 - have to look up all of these different
178:13 - codes and ids
178:15 - so all we have to do to start the trace
178:18 - now is to run this script
178:21 - again you will need to be logged in as a
178:23 - user with the altered trace permission
178:26 - in order to run this trace
178:28 - otherwise you are going to get an error
178:31 - the user i am logged in as does have
178:34 - this permission
178:35 - so i am going to go ahead and get this
178:38 - trace started
178:39 - and so now this trace will be running up
178:42 - on the server and collecting data
178:45 - and logging that data to the output file
178:48 - that we specified
178:50 - one important thing to note here is the
178:52 - id number for the stress
178:54 - that you get back down here in the
178:57 - result pane
178:58 - because this is what we are going to
179:00 - need to stop the trace once we have
179:03 - captured all of the data that we need
179:06 - so let's talk next about how we manage
179:09 - this trace so first of all you might
179:12 - want a status on what traces are running
179:14 - on sql server
179:16 - or maybe you forgot that number of the
179:18 - trace that we have just started
179:20 - so let's see how we can find that
179:22 - information out
179:24 - i will go over here to this other window
179:27 - where i have a couple of queries
179:30 - and what you want to do is you want to
179:32 - query the system function fn trace get
179:36 - info
179:37 - and by passing a 0 to this function
179:41 - you will get back information on all
179:43 - traces that currently exist in sql
179:45 - server
179:46 - so i am going to go ahead and run this
179:49 - statement
179:50 - and you see we get this information back
179:53 - but this isn't a very user friendly
179:56 - format
179:57 - trace number one is the system default
179:59 - trace that's always running
180:02 - and then we see the information we have
180:04 - on your trace
180:05 - and we can convert it out our trace id
180:09 - the file and status information from
180:11 - this
180:12 - but it's not very user friendly so if we
180:16 - run this second query here
180:18 - this will give us a little bit
180:20 - friendlier view of the data so let's go
180:22 - ahead and run that
180:24 - and you see
180:25 - that's the same information
180:27 - it is just in a little bit better format
180:29 - for us to consume
180:31 - so now we have our trace that's been
180:34 - running for a while
180:36 - we have collected the information that
180:37 - we need
180:39 - and we want to stop this trace
180:41 - and so how do we do that
180:44 - well
180:45 - what we do is we run a stored procedure
180:47 - name sp trace set status
180:50 - and i have the various use cases of that
180:53 - stored procedure over in this third
180:56 - window
180:57 - so as you can see here you pass in your
181:00 - trace id and a value of 0 if you want to
181:03 - stop the trace
181:05 - if after some time you want to restart
181:08 - the trace you would pass in the trace id
181:11 - and a value of 1 to restart that trace
181:15 - so then after you have stopped the trace
181:17 - if you want to remove the definition of
181:19 - the trace from sql server
181:21 - you would call this procedure again with
181:24 - the value of 2
181:25 - and what that does is it just remove the
181:28 - definition of the trace in sql server
181:30 - it doesn't go out and delete any trace
181:33 - file that was generated whenever sql
181:36 - server restarts all these
181:38 - definitions get removed
181:40 - this is just a way that we can clean up
181:42 - things
181:43 - when we are finished and keep our sql
181:46 - server tidy so i am going to go ahead
181:50 - and i am going to stop this trace
181:52 - and so at this point we have covered
181:55 - what you need to know about how to run a
181:58 - server side trace
182:00 - now once one of these traces has been
182:03 - taken either by you or maybe somebody on
182:06 - your dba team
182:07 - and you have this dot trc file
182:10 - of the trace data
182:12 - how do you view the data that's in this
182:14 - file that's pretty easy
182:17 - you just open them up in sql profiler so
182:21 - i am going to go back
182:23 - to sql profiler and i will select file
182:27 - open and then trace file
182:31 - and then i just find the trace file i
182:33 - want to open
182:35 - i select and say open in the dialog box
182:39 - and now that file will open up in
182:41 - profiler
182:42 - from here you can review the file in
182:45 - profiler or use profiler to load the
182:47 - data into a table in sql server whatever
182:51 - meets your needs
182:52 - this wraps up our discussion of sql
182:55 - profiler so now we are going to move on
182:58 - and talk about sql server extended
183:00 - events
183:01 - which for our purpose do much the same
183:04 - thing
183:05 - but is really the preferred tool on new
183:07 - versions of sql server for taking traces
183:09 - like this
183:11 - and also gives us the capability to
183:13 - trace sql statements on sql azure also
183:16 - so we will take a look at extended
183:18 - events in the next video
183:21 - you are watching the video of module 5
183:23 - capturing what your application is doing
183:25 - inside sql server
183:27 - part 5 introduction to using extended
183:30 - events for sql tracing
183:32 - if you are using sql server 2012 or
183:35 - later or using sql azure then extended
183:39 - events is the way to go in terms of
183:41 - tracing what sql is running inside of
183:44 - your database
183:46 - sql server extended events use the event
183:49 - tracing for windows or etw framework to
183:53 - trace data and this framework is newer
183:56 - and more efficient in terms of server
183:59 - resources than the older tracing
184:01 - framework
184:02 - using sql profiler
184:05 - extended events also gives you access to
184:07 - many more events within sql server that
184:10 - can be traced and logged and many more
184:13 - options in terms of filtering those
184:15 - events down to only the ones that you
184:17 - want
184:19 - so you want to make sure that you are
184:21 - familiar with using extended events for
184:23 - tracing
184:24 - because this is the direction that
184:26 - microsoft has chosen
184:28 - and will be the only option available on
184:30 - future version of sql server to run an
184:33 - extended event trace the user running
184:36 - the trace is going to need some
184:37 - permission in order to setup and start
184:40 - the trace
184:41 - extended events was actually first
184:44 - introduced in sql server 2008 r2
184:47 - and if you are on sql server 2008 r2
184:51 - then your user will need the control
184:53 - server permission granted to it
184:56 - on sql server 2012 or later you will
184:59 - need the alter any event session
185:01 - permission in order to define and run an
185:04 - extended event trace
185:06 - so this is a much more targeted
185:08 - permission
185:09 - however having only alter any event
185:12 - session will only allow the user to
185:14 - create an extended event trace through
185:16 - sql
185:18 - if you want to be able to use the gui
185:21 - within sql server management studio in
185:23 - order to define your extended events
185:25 - capture session
185:27 - as we are going to do here you also need
185:30 - the view server state permission for
185:32 - your user
185:34 - if you will probably have this
185:35 - permission on your local instance of sql
185:37 - server that's running on your
185:39 - workstation and it may be possible to
185:42 - get these access rights in your dev and
185:45 - potentially your test environments
185:48 - in production at a lot of companies you
185:50 - will probably be asking your dba group
185:52 - to define and run these stresses for you
185:55 - because of security concerns around
185:58 - production data in servers
186:01 - but again these tools can be very useful
186:04 - in your development and application
186:05 - support process
186:07 - so we will introduce these tools here so
186:10 - you are familiar with what capabilities
186:12 - exist and you are aware of all the tools
186:15 - that can help you in diagnosting and
186:17 - solving problems within sql server
186:20 - in terms of setting up extended event
186:22 - session to capture events for the
186:24 - on-premises version of sql server and
186:26 - for sql azure about 90 percent of the
186:29 - steps that you need to perform are
186:31 - common between the two platforms
186:34 - there are some small differences though
186:37 - so what i am going to do over the next
186:40 - few clips
186:41 - show you how you would set up extended
186:43 - events to trace your sql on an
186:45 - on-premises version of sql server
186:48 - so let's jump in and create new extended
186:51 - event session to capture some of the sql
186:54 - inside of sql server in the next video
186:57 - you are watching the video of module 5
187:00 - capturing what your application is doing
187:02 - inside sql server
187:04 - part 6 setting up an extended event
187:07 - trace
187:08 - session to set up an extended event
187:12 - session we use sql server management
187:14 - studio
187:15 - what you want to do is in your object
187:18 - explorer go down and open up the
187:20 - management folder
187:22 - then you will see extended events under
187:25 - it
187:27 - so expand that
187:29 - and finally expand sessions
187:32 - and you will see the existing extended
187:34 - event trace sessions defined on your sql
187:37 - server
187:38 - to define a new session right click on
187:40 - the session folder and you see you have
187:43 - two choices here
187:45 - new session wizard and new session
187:47 - and i suggest you select the second
187:49 - choice new session because this is
187:53 - pretty straight forward to do and there
187:55 - is not really a need for a wizard
187:58 - once we do that we will get this window
188:00 - to pop up and if we look on the left we
188:04 - have this four pages of configuration
188:06 - that we will go through to define this
188:08 - capture session
188:09 - so we will start out here on this
188:12 - general page
188:13 - first we want to give our session a name
188:17 - and it is good to make this name
188:19 - descriptive because this is what's going
188:21 - to show up in the list of management
188:23 - studio and it will help identify this
188:26 - configuration in the future
188:28 - so i will put in a name and then we have
188:31 - this drop down of template we can choose
188:33 - from
188:34 - and we can see those template and in
188:37 - this case i am going to choose the one
188:39 - named query batch tracking
188:41 - once you select a template you will see
188:44 - that sql server does gives us a good
188:46 - description of each one of this template
188:49 - all these templates do is pre-select
188:52 - some of the events that we can choose
188:54 - from on the next page
188:56 - so they make setting up your trace a
188:58 - little bit faster for us as developers
189:01 - this query batch tracking is a really
189:03 - good starting point and then if we wish
189:06 - we can add more events on the next page
189:09 - finally on this screen we have some
189:11 - options that we can start the trace at
189:13 - server startup or as soon as we create
189:16 - this trace
189:17 - and i am going to leave each one of
189:19 - these boxes unchecked as we will just
189:22 - start our session manually when we are
189:24 - ready to
189:25 - the next phase that we want to look at
189:27 - is the event page so i will click on
189:31 - that and on this page the events that
189:34 - are currently selected to be captured
189:36 - are over here on the right in this list
189:40 - and then
189:41 - this main section in the middle of the
189:43 - page is where we can search for and
189:45 - select any other events of interest
189:48 - you can search for event with the event
189:51 - search box so i am actually going to do
189:53 - that for an event
189:55 - and we see the list is now filtered by
189:57 - my search criteria i am going to click
190:00 - on this event query post compilation
190:02 - show plan
190:03 - and when i do notice that these two
190:06 - controls on the bottom populated given
190:08 - me a description of what this event is
190:11 - and the data fields that this event will
190:13 - capture
190:15 - for this particular event if i scroll
190:17 - through the description i see that sql
190:20 - server is warning me that collecting
190:22 - this event can be very expensive
190:24 - so if i do choose to turn this event on
190:28 - i want to be mindful of that and only
190:30 - collect data for a brief period of time
190:34 - as not to impact the rest of my sql
190:36 - server
190:37 - so that's where the description field
190:39 - can be really useful because sql server
190:42 - lets us know if we are collecting a high
190:44 - resource usage event
190:47 - if we did want to add this or any other
190:49 - event to be collected as part of this
190:51 - trace session we just use these two
190:53 - buttons here to manage what events were
190:56 - selected
190:57 - once we have the list of events then we
190:59 - have to capture
191:00 - we can configure what data we want
191:02 - collected for each of these events
191:05 - and to do that
191:06 - we click on this configuration button
191:09 - that is right here
191:10 - that button take us to this screen where
191:13 - we have our selected events on the left
191:15 - side now and the configuration for each
191:18 - event is over here on the right
191:20 - in terms of the data that you collect
191:23 - for each event that is divided into two
191:25 - sections
191:26 - fields that are globally available to
191:28 - all events which is on this first tab
191:31 - here
191:32 - and event specific fields which are out
191:34 - here on the third tab
191:36 - so for the rpc complete events we see we
191:40 - are always going to collect performance
191:42 - data like the amount of cpu time
191:44 - duration and logical reads
191:48 - and that is good because those are
191:50 - fields of interest to us
191:52 - and also on the rpc completed event you
191:54 - just wanted to make sure that the
191:56 - statement field is checked
191:58 - because this will allow you to see first
192:00 - of all the statement and also the
192:02 - parameter that were passed either to the
192:04 - stored procedure or the parameterized
192:06 - query
192:07 - over on the global field tab there is
192:10 - one field you want to make sure and
192:11 - select and that is the sql text field
192:15 - the reason why you want to make sure
192:17 - this field is selected is because this
192:19 - field gives you just the sql text of the
192:21 - sql statement without the parameter
192:24 - values
192:25 - and we will see in a moment that allow
192:27 - us to group all of the various
192:29 - executions of a particular statement
192:31 - together when we start analyzing our
192:33 - data
192:35 - so you want to make sure that the sql
192:37 - text field is selected
192:39 - and we want to do this both for the rpc
192:41 - completed event and then also for the
192:44 - sql batch completed event
192:46 - and then you can just look through the
192:49 - list of the rest of the global fields
192:51 - and see if there are any that make sense
192:54 - for the trace that you are putting
192:55 - together
192:56 - for example you might want to capture
192:58 - fields like client host name or username
193:01 - to know what application ran a
193:02 - particular statement
193:04 - the last bit of configuration to do on
193:06 - this screen is to configure the filters
193:08 - for the event and we want to do this so
193:11 - we can focus on the data that we are
193:13 - really interested in
193:15 - and just collected the data
193:17 - otherwise we are going to have a lot of
193:19 - extra data that we are going to have to
193:21 - sort through and by focusing in on just
193:24 - the data we are interested in
193:26 - we can lighten the footprint of any
193:28 - additional load that we are putting on
193:30 - sql server
193:32 - to do this we use the filter tab and
193:34 - again we are going to need to do this
193:36 - for each event type that we are
193:38 - capturing we see that we already have a
193:41 - couple of default filters in here
193:43 - and what this filter will do is keep our
193:46 - session from capturing any internal
193:48 - system activity run internally by sql
193:50 - server
193:52 - since that activity is probably not of
193:54 - interest to us what we do want to do
193:57 - though is to limit our session to only
194:00 - capture data from our application
194:02 - so there are a couple of ways that we
194:04 - can do that
194:05 - you just click here to add a clause and
194:08 - now i will click on this middle box and
194:10 - you see all of the criteria that i can
194:13 - filter by
194:14 - so if i am on a shared sql server box
194:17 - that has many many databases i could
194:20 - come down here to sql server
194:23 - database name and now i will enter the
194:25 - value of students in the value field and
194:28 - i will only capture this event sql batch
194:30 - completed in the students database
194:33 - so that will effectively limit me to
194:36 - only sql statements that are run in the
194:38 - database
194:39 - there are many criterias in this list as
194:41 - you see
194:42 - so you could only capture statements
194:45 - that took a certain amount of time or
194:47 - longer or statements that took a certain
194:49 - amount of cpu and this could be useful
194:52 - if you are trying to identify what the
194:54 - real long running statement in your
194:56 - application were
194:58 - you could also put a filter on the sql
195:00 - text field and then there is like
195:02 - operator available
195:04 - so
195:05 - you could use this to capture all of the
195:07 - statements then run against a certain
195:10 - table in your database
195:12 - i will set up the filter for the other
195:14 - two event types offline so we can move
195:16 - on to the last item of configuration we
195:19 - are going to do
195:21 - and that is where we are going to store
195:23 - the data that we capture as part of this
195:25 - extended event session trace
195:28 - we configure that on the data storage
195:30 - page so i will click over there and you
195:33 - can see by default the storage is of
195:35 - type ring buffer and what this is as in
195:39 - memory data structure that acts like a
195:41 - queue
195:42 - so in this case you see we will keep
195:44 - just the last 1000 items
195:47 - we could use this but what you probably
195:49 - really want to do is to process this
195:52 - data you capture to a file
195:54 - so let's do that
195:56 - so i will remove this ring buffer by
195:58 - clicking on the remove button
196:00 - now i will click add
196:02 - and i will choose a type and what i want
196:05 - is an event file
196:07 - and now i will go down here and i will
196:09 - select the name and location for a file
196:12 - and now i am going to say i want a max
196:14 - file of 250 mb for my file
196:17 - and now this data will be captured in
196:19 - the file which is going to be more
196:22 - useful for us
196:23 - the last page of configuration is the
196:25 - advanced page
196:26 - but we will be in a good shape just to
196:29 - accept the defaults there
196:31 - now to create this extended event
196:33 - session that i have it all configured
196:35 - all i need to do is to hit ok button
196:39 - and this session will be created
196:41 - so in the next video what we are going
196:43 - to do is we are going to run this
196:45 - session and we are going to capture some
196:47 - data on our sql server
196:50 - you are watching the video of module 5
196:53 - capturing what your application is doing
196:55 - inside sql server
196:57 - part 7
196:59 - running and configuring the display
197:00 - settings for an extended event trace
197:04 - now that we have our trace defined
197:06 - we just right click on the extended
197:09 - event session and say start session
197:12 - and so now this trace session is running
197:15 - and collecting data to the file that we
197:17 - set up for the session to log data to
197:21 - if we want to stop taking data i just
197:23 - right click again and i would say stop
197:26 - session
197:28 - but i am not going to do that just yet
197:31 - we can see here on our menu that we can
197:34 - also have another option watch live data
197:37 - that we can use while the session is
197:39 - running and we can use this to peek at
197:42 - any data that the session is capturing
197:45 - so if we click on this this is going to
197:47 - bring up this window and now as the
197:50 - session captures data in real time those
197:53 - events are going to get displayed in
197:55 - this window
197:56 - we are still capturing and logging all
197:58 - of these events to a file we set up
198:01 - this just gives us a live view of things
198:04 - up here in the top part of the window we
198:07 - see a list of the captured events and if
198:09 - we click on one of these then we get the
198:12 - details of that event down here in the
198:14 - bottom part of the window
198:16 - now having to click on each and every
198:18 - event to inspect its results and to find
198:21 - the data you are looking for in not real
198:24 - user friendly
198:26 - and you are going to quickly tire off
198:28 - clicking through each one of these
198:30 - events trying to find what you are
198:31 - looking for
198:33 - so lets look next at how we can adjust
198:35 - this view of our data into something
198:38 - that is more useful
198:40 - so let's see how we can make our view of
198:42 - the data a little bit more useful here
198:45 - we can add columns to the upper part of
198:48 - this view and turn it into more of a
198:50 - data grid view in a couple of ways
198:53 - first if we have an event selected we
198:56 - can come down here to the data of the
198:58 - event and just right click on an item
199:01 - and select show column in table
199:03 - and then as we see that column now shows
199:06 - up in our table above
199:08 - the other thing we can do is right click
199:11 - on the table header and say choose
199:13 - columns
199:14 - and now we get this dialog where we can
199:17 - choose the columns that we want to see
199:19 - and the order that we want to see them
199:21 - in
199:22 - so i am going to choose a few columns
199:25 - here
199:26 - and i will change this order up a little
199:28 - bit
199:29 - and now when i click ok we can see that
199:32 - i have more of a grid view here in the
199:34 - top part of the pin
199:36 - and that makes things a lot more useful
199:39 - if you want to turn off this bottom part
199:41 - you just go up here and you click on the
199:43 - button here
199:45 - and that will toggle the bottom part of
199:47 - the display on and off
199:50 - and then once you have a column setup
199:52 - that you like
199:54 - what you want to do is you want to save
199:56 - that setup so you can pull it up again
199:59 - anytime you are viewing extended events
200:02 - either a file of extended events or when
200:05 - you are capturing live events
200:07 - so to do that you come up here to
200:10 - display settings and click save as
200:12 - and we will choose a file name
200:14 - and now i will click save
200:17 - and so now when i go back into
200:19 - management studio and i want to bring up
200:21 - this view up all i have to do is go up
200:24 - here to the display settings
200:26 - say open recent and there is our
200:29 - configuration file
200:30 - and we would just click on this
200:33 - so let's move into analyzing some of the
200:36 - data that we have captured in the next
200:38 - video
200:39 - you are watching the video of module 5
200:41 - capturing what your application is doing
200:43 - inside sql server part 8 analyzing
200:46 - extended event trace data
200:49 - when an extended event session produces
200:51 - an output file the file produced is an
200:54 - xel file
200:56 - and you can open this file right in the
200:58 - management studio so you just go to the
201:01 - file
201:02 - open
201:03 - file again and then just find the xcl
201:05 - file on your hard drive that you want to
201:08 - open and click open and so there we go
201:12 - we can view our data now
201:14 - i am also going to get my column
201:16 - definitions
201:17 - so i am going to go up and i am going to
201:20 - do that real quick and now
201:23 - we have got a better view of our data if
201:26 - you watch the clips on sql profiler or
201:29 - you view sql profiler you know that
201:32 - profiler was pretty limited in terms of
201:35 - what we could do with the data
201:37 - but for this extended event files we
201:40 - actually can do quite a bit in
201:42 - management studio with this data
201:45 - first of all i can sort the data just by
201:47 - clicking on the column header
201:50 - i can also filter the data right here in
201:52 - the ui by clicking on the filter button
201:55 - that's going to bring up this dialog
201:57 - where we can add filter clauses
202:00 - so you just click to add a clause and
202:03 - then you can select the criteria the
202:05 - operator and the values
202:08 - if i use the arrow key here to cycle
202:10 - through some of these values you can see
202:12 - all the fields that we are collecting
202:14 - for this event we can now filter on
202:17 - so i am going to select cpu time as a
202:20 - filter then i can select greater than
202:22 - equal to and finally i give this filter
202:25 - a value of 2 lakhs
202:27 - because this value is in microseconds so
202:30 - that will be 2 seconds so this will now
202:33 - show us in the ui all the statements
202:35 - that took two seconds or more of cpu
202:38 - time
202:39 - so filter like this can be very useful
202:41 - in order to narrow down the data set and
202:44 - look at just what you need
202:46 - in addition to a filter on something
202:48 - like duration or cpu time we could also
202:51 - create a filter that was only for a
202:53 - certain time frame statements from a
202:55 - certain host or a filter that would look
202:58 - for a certain table name in the
202:59 - statement text for our event
203:02 - so we could see just the statement that
203:04 - run against a single table
203:07 - so this is pretty useful what we can do
203:10 - with filtering right here in management
203:11 - studio now
203:13 - and we don't have to go through the
203:14 - extra step of exporting our data and
203:17 - getting it into excel or something like
203:19 - that we can do all of that right here
203:22 - in addition if we do filter the data
203:25 - down and we want to share just that
203:27 - filter subset of data with some of our
203:29 - colleagues
203:30 - we can go up here
203:32 - to the extended event menu item then
203:35 - down to export to
203:36 - and you can see that we can export this
203:39 - filter view of the data to a separate
203:41 - xcl file
203:43 - a table in sql server or a csv file
203:46 - so that's really useful when you just
203:48 - want to share a subset of data with
203:50 - someone to analyze
203:52 - one of the other features that
203:54 - management studio gives us is that
203:56 - rather than viewing just a list of raw
203:58 - events like we are now
204:00 - we actually can group our events
204:02 - together and generate aggregate
204:04 - statistics on them
204:06 - so let's do that first i am going to
204:09 - aggregate my data together
204:11 - and i am going to do that by the sql
204:13 - text of this statement
204:15 - which is this column sql text
204:18 - remember this was the global field for
204:21 - our events that we made sure to capture
204:24 - when we set up our trace and we did that
204:27 - because this field contains just the sql
204:29 - of the statement or the stored procedure
204:31 - name and not the parameters
204:33 - so now that we can group our statements
204:36 - by that sql
204:38 - so we see that view of the data here and
204:41 - this basically tells us how many times
204:43 - of sql statement or a stored procedure
204:45 - executed while we are running our trace
204:49 - if we want to see the individual
204:50 - statement we just click on the little
204:52 - plus sign
204:53 - and then we see the individual
204:55 - statements under that grouping
204:57 - we can even sort these statements within
204:59 - the group by double clicking the column
205:02 - headers
205:03 - so of course one of the things you want
205:05 - to look here is if you have a statement
205:08 - that's being executed thousands of times
205:10 - more than any other statements and then
205:13 - ask yourself what that is
205:15 - for example maybe a statement is being
205:17 - executed inside of a loop
205:19 - so your application is going to the
205:22 - database over and over and again
205:24 - and that could be an indication that you
205:26 - want to take a look at that piece of
205:28 - code and refactor it
205:30 - the next thing that we can do is we can
205:33 - get some aggregate statistics for each
205:35 - of this statement
205:36 - and we do that by clicking on the
205:38 - aggregation button
205:40 - and now for all of these other fields
205:42 - that we are displaying we see we can get
205:45 - some aggregate statistics like average
205:47 - or sum so for this example i am going to
205:50 - sum up the cpu and the duration
205:53 - and i am going to click ok
205:56 - and now i see i have the summary
205:58 - statistics displayed along with the
206:00 - statements so what this allows you to do
206:03 - is to run a trace which is going to
206:05 - capture data about a specific process
206:08 - and then you could filter down just to
206:10 - that data either at capture time or
206:13 - after the fact in this video
206:15 - using the filter button as we just saw
206:18 - and then for that process that you have
206:20 - the data for you could figure out not
206:23 - only which statements had the highest
206:24 - average cpu usage or the highest
206:27 - duration which would indicate the
206:29 - longest average weight but you can also
206:32 - see that for all the statements that
206:34 - were running by this process or in this
206:36 - window of time that you have captured
206:40 - which statement has the most total cpu
206:42 - or resulted in the most total duration
206:44 - that the application was waiting so you
206:47 - can really zero in on what a process is
206:49 - doing
206:50 - and where the performance bottlenecks
206:52 - are in that process in terms of the data
206:55 - access
206:56 - so we have seen how to use extended
206:58 - events to capture address with the
207:00 - on-premises version of sql server taking
207:03 - the trace in azure is very similar there
207:06 - are just a few differences so we will
207:08 - cover those in the next video
207:11 - you are watching the video of module 5
207:13 - capturing what your application is doing
207:15 - inside sql server
207:17 - part 9 using extended events in sql
207:21 - azure
207:22 - if you are using sql azure you can still
207:25 - capture what your application is doing
207:27 - inside of sql azure with extended events
207:31 - but there are couple of small
207:33 - differences that you need to account for
207:36 - first of all you are going to want a
207:38 - version of sql server management studio
207:41 - that fully supports sql azure
207:44 - what this means is that you want to get
207:46 - the latest version of management studio
207:48 - and use that to set up your extended
207:50 - event sessions
207:52 - earlier versions of management studio
207:54 - would allow you to connect to a sql
207:56 - azure database and run queries
207:59 - but if you try to create an extended
208:01 - event session you would get an error
208:05 - with the latest version of management
208:06 - studio though the ability is setup and
208:10 - extended event session has been extended
208:12 - to sql azure database as well
208:15 - so you will be able to use the same user
208:18 - interface that we have seen over the
208:20 - last few clips to setup your event
208:22 - capture session in sql azure
208:26 - the second thing you need to do is to
208:28 - set up a storage location of where you
208:31 - can write your extended event trace
208:33 - files to
208:34 - with sql azure being a platform as a
208:36 - service offering you don't have a
208:38 - traditional file system available where
208:41 - you can write files to
208:42 - but what you can do is to write your
208:45 - files to azure storage
208:47 - and this article on microsoft.com tells
208:51 - you exactly how to do that
208:53 - if you read through this article you
208:55 - will see that there are two steps
208:58 - step one is that you run a powershell
209:00 - script to create azure storage container
209:04 - and this part also create the storage
209:06 - access policy that will be used to allow
209:08 - your sql azure instance to be able to
209:11 - write to that container
209:13 - when you run this powershell script you
209:15 - will get an output like i am showing
209:17 - here
209:18 - and it is this information that you will
209:20 - need to feed into the second step of the
209:23 - process
209:24 - step 2
209:25 - evolves running some transact sql in
209:28 - your sql azure database
209:31 - if you look at the tsql on the
209:32 - microsoft.com page as i am here you see
209:36 - that the sql creates a sample data at
209:38 - the top of the script and then a sample
209:41 - trace at the bottom
209:43 - what is really important though is step
209:45 - number two because this is what gives
209:48 - sql azure the credential it needs to
209:50 - write to the storage location
209:53 - so what is happening here is that you
209:55 - are creating some azure storage
209:57 - putting a security policy on that
209:59 - storage and then giving the credential
210:02 - needed to write to that storage to your
210:04 - sql azure instance
210:06 - the script covered the details of the
210:08 - exact command to do this but
210:10 - big picture that's what's going on
210:14 - once you have the storage setup now we
210:16 - can go into management studio and define
210:19 - an extended event session
210:22 - when you are looking at an azure
210:23 - database you will see that the extended
210:26 - event is under the database itself not
210:29 - out under a management folder like in
210:32 - the on premises version
210:34 - but otherwise we just right click on the
210:36 - sessions folder and say new session just
210:39 - like we did before
210:41 - the selection of the events to capture
210:43 - what fields you want to capture in those
210:46 - events and the setup of any filter is
210:48 - going to be like before
210:50 - the only real difference is that when we
210:52 - get the data storage tab
210:54 - you are going to put the url of your
210:57 - azure storage container in the storage
210:59 - url field with the file name
211:01 - and now this is where your extended
211:04 - event file is going to be created
211:06 - so really the process of creating a
211:08 - trace using extended events on sql azure
211:11 - is very similar to the on-premises
211:13 - version of sql server
211:15 - just make sure that you have the latest
211:17 - version of management studio and get a
211:20 - storage location setup
211:22 - and then you will be able to capture
211:24 - what your application is doing inside of
211:26 - a azure database
211:28 - just like you would be able to do with
211:30 - the on-premises version of sql server
211:33 - let's go ahead and wrap up this module
211:35 - in the next video
211:37 - you are watching the video of module 5
211:39 - capturing what your application is doing
211:41 - inside sql server part 10 module summary
211:45 - there are many circumstances where we
211:47 - would like to be able to tell exactly
211:49 - what our application are doing inside of
211:52 - sql server and by making use of the
211:54 - tracing capabilities built into sql
211:57 - server we can do that
211:59 - this can really gives us a lot of
212:01 - insight into what our application is
212:03 - doing because we can capture the exact
212:06 - sequence that our sql is being executed
212:09 - in
212:10 - so this is really useful for debugging
212:13 - we can also get detailed statistics on
212:15 - the execution of each statement that is
212:18 - run so we can quickly identify which
212:20 - statements are using the most resources
212:23 - or taking the longest
212:25 - finally we get a good idea of how often
212:28 - our application is running different
212:30 - statements against sql server and
212:33 - sometimes simply knowing how often a
212:35 - statement is run tells you something
212:38 - important about how your application is
212:40 - executing
212:41 - or maybe that there's an opportunity to
212:44 - combine multiple sql statements into
212:47 - single statement
212:49 - if we are on sql server 2008 r2 or
212:52 - earlier we will use the sql profiler
212:55 - tool to set up this trace
212:58 - and then most likely we will export the
213:00 - definition to a trace definition in sql
213:03 - so we can run the trace up on the server
213:07 - since that consume fewer resources for
213:09 - sql server 2012 and beyond
213:12 - and for sql azure we will use sql server
213:16 - extended events
213:17 - and we can create an extended event
213:19 - session right in management studio in
213:21 - both cases we are going to select the
213:23 - types of events that we want to capture
213:26 - there are a wide variety of events that
213:28 - you can capture in the database
213:30 - but for us developers usually we are
213:33 - interested in tracing our sql so the sql
213:37 - batch complete and rpc complete events
213:40 - are usually the starting point of us
213:42 - this means that every time our
213:44 - application runs a sql statement or
213:47 - stored procedure we will be able to
213:49 - capture the text of what was run any
213:52 - parameter used for the execution and
213:54 - some detailed performance information
213:56 - like how long the statement took
213:59 - the number of rows written the amount of
214:01 - cpu and how many reads the statement
214:04 - performed we could also supplement these
214:06 - events with other events of interest to
214:08 - us for example we see here events around
214:12 - the start and completion of individual
214:14 - sql statements and around transactions
214:18 - finally we will set up some appropriate
214:20 - filters so we can capture only the data
214:23 - that we are interested in
214:25 - we might filter by the database name or
214:27 - sql server user to look at just the
214:30 - activity from our application
214:32 - but we might even filters by the text in
214:34 - the sql statement just to identify
214:36 - statements that are running against a
214:39 - certain table
214:40 - having good filters like this means
214:43 - there is less data to sort through
214:45 - and it also helps limit the load that we
214:48 - are putting on sql server
214:50 - once we run our trace we will usually
214:53 - end up with an output file of all the
214:55 - events in it that we captured and we can
214:58 - analyze
214:59 - if you are using sql profiler you can
215:02 - just scan through that file or you can
215:05 - export that data into a table in sql
215:08 - server
215:09 - to do detailed analysis
215:11 - if the output is from an extended event
215:13 - session we saw that you could do a lot
215:16 - of your analysis right in management
215:18 - studio itself
215:19 - what all of this gives us is the
215:21 - capability to see step by step what our
215:24 - application is doing inside of sql
215:26 - server
215:27 - so this really helps us understand what
215:30 - our data access layer is doing
215:32 - and where we can mix improvements in the
215:35 - next video we are going to see our last
215:37 - module of this course that is how we can
215:40 - apply common performance practices
215:43 - so let's get start next
215:48 - you are watching the video of module 6
215:50 - applying common performance practices
215:53 - part 1
215:54 - module introduction
215:56 - in the other modules of this course we
215:58 - have concentrated on what is happening
216:00 - inside of sql server
216:02 - in this module our focus is a little bit
216:05 - different
216:06 - in that we are going to go through some
216:08 - common performance practices that you
216:11 - want to apply in your application code
216:14 - first we will talk about why you want to
216:16 - make sure to use parameterize sql
216:19 - when you write your data access code
216:22 - then we will talk about if it really is
216:24 - faster to use stored procedures instead
216:27 - of including your sql directly inside of
216:30 - your application
216:32 - next we will talk about commit frequency
216:35 - and how that can affect the performance
216:37 - of your data access layer
216:39 - then we will talk a little bit about
216:42 - object relational mappers or orms
216:46 - first we will talk about how using an
216:48 - orm can make what is happening inside of
216:50 - sql server a little less visible
216:54 - and what you can do about it
216:56 - and finally we will wrap up by talking
216:58 - about the n plus 1 select problem which
217:01 - is commonly encountered performance
217:03 - problem when using orms but fortunately
217:07 - is easy to resolve
217:10 - so let's get started by talking about
217:12 - why it is important to parameterize the
217:14 - sql in your application
217:17 - you are watching the video of module 6
217:20 - applying common performance practices
217:22 - part 2
217:23 - use parameterize sql
217:26 - one practice you want to make sure to
217:28 - adopt to use parameterized sql
217:30 - statements in your data access layer
217:33 - what do you mean by this
217:35 - consider the sql statement you see on
217:37 - your screen
217:39 - the statement is using simple string
217:41 - concatenation to dynamically build the
217:43 - sql statement to be run against the
217:45 - database
217:46 - what this means is that every time this
217:49 - data access method is called with
217:51 - different values a different sql string
217:54 - is going to be generated and then sent
217:56 - to sql server to be executed contrast
217:58 - that without data access method that is
218:00 - using parameters like the one you see
218:03 - now on your screen
218:05 - these two values with the address sign
218:07 - in your statement signify sql parameters
218:10 - to sql server and then down here we
218:13 - attach the values that we want to use
218:15 - for those parameters when we submit the
218:17 - statement to the database
218:20 - so every time this method is run the
218:22 - same sql is sent to the sql server
218:25 - and it's just the parameter values that
218:28 - are attached outside of the sql tags
218:30 - that will vary
218:32 - in sql server we will then be able to
218:35 - substitute those parameters in up on the
218:38 - server itself there are big two
218:40 - advantages to this approach of using
218:42 - parameters first
218:44 - this helps prevent sql injection attacks
218:48 - because any value passed in this way are
218:50 - automatically escaped by the
218:52 - database so it doesn't matter if a
218:55 - malicious user has included
218:58 - some code characters or other special
219:00 - characters in here to try and do
219:02 - something inferior
219:05 - those characters are going to be
219:06 - automatically escaped in our statement
219:09 - and that goes a long way into feeding
219:12 - sql injection attacks like this
219:14 - the automatic escaping of these strings
219:17 - also means that you don't have to do
219:19 - anything special in your code
219:21 - when a user wants to legitimately
219:24 - include a single code character in the
219:26 - value they are entering
219:28 - like when they are typing the name o
219:30 - corner or
219:32 - orale
219:34 - secondly using a parameterized sql
219:36 - statement like this will also perform
219:38 - faster and use fewer resources on sql
219:41 - server
219:42 - why is this
219:43 - remember that every time a sql statement
219:45 - is sent to sql server sql server has to
219:48 - determine an execution plan of how to
219:50 - execute that statement
219:53 - what actually happens is that sql server
219:55 - looks in an area of memory known as the
219:57 - plan cache
219:59 - to see if it has already executed the
220:01 - same statement before
220:03 - and therefore already has an execution
220:05 - plan that can be used
220:07 - if yes then sql server can just use this
220:10 - cache execution plan
220:12 - if not sql server will have to do
220:15 - additional work to come up with an
220:16 - execution plan either through a process
220:19 - known as a simple parameterization or
220:21 - through generating a complete new
220:23 - execution plan
220:24 - if you are using parameterized sql
220:27 - statements then once that statement is
220:29 - run the first time every subsequent
220:32 - execution of that statement is going to
220:34 - be able to use the same cached execution
220:36 - plan
220:37 - even when different values are included
220:39 - with a new statement execution
220:42 - if you are dynamically generating your
220:44 - sql statements like we saw in the string
220:46 - concatenation example
220:48 - the actual sql is different each time so
220:51 - sql server has to perform this
220:53 - additional work to get an execution plan
220:56 - each time
220:57 - and that additional work will slow down
220:59 - your application
221:00 - but maybe more importantly make it less
221:03 - scalable
221:04 - the code that you saw earlier in this
221:06 - clip was part of the simple test
221:08 - application that i wrote that run the
221:11 - same sql statement several thousand
221:13 - times with different values
221:15 - and you see the result of the test on
221:17 - your screen
221:19 - the test took 8.5 seconds of elapsed
221:21 - time to run almost 7 seconds of cpu time
221:24 - and we use 79 mb
221:27 - up in sql to store all those different
221:29 - execution plans
221:31 - using parameterized sql statements all
221:34 - of those same sql statement executed in
221:36 - under 1 second
221:38 - and we used almost no cpu time on the
221:40 - database server since we are using the
221:43 - same plan over and over again
221:46 - and as you can see we just use 104 kb of
221:50 - memory
221:51 - in the database to cache our execution
221:53 - plan
221:54 - so we see that using parameterize sql
221:57 - not just makes our statement execute
221:59 - faster but also consumes significant
222:01 - fewer resources on our database server
222:04 - and this is a big deal because most
222:07 - database servers are pretty busy
222:09 - supporting many many concurrent users so
222:11 - being efficient about resource
222:13 - consumption is important
222:15 - if you are using an orm like entity
222:17 - framework entity framework core or
222:20 - hibernate these tools should already be
222:22 - parameterize your sql for you
222:25 - but if you are writing your data access
222:27 - layer using something like ado.net or
222:29 - jdbc or even using a micro orm
222:33 - where you are still writing the sql
222:34 - statements directly
222:36 - make sure that you are parameterizing
222:38 - your sql statements
222:40 - this will provide a performance boost
222:42 - for your application as well as help to
222:45 - keep you safe from sql injection attacks
222:48 - after watching this video you might ask
222:50 - that are store procedure faster than sql
222:53 - in application code
222:55 - so we are going to see that in the next
222:57 - video
222:59 - you are watching the video module 6
223:01 - applying common performance practices
223:03 - part 3
223:04 - r stored procedures faster than sql in
223:07 - application code
223:09 - if you are being doing database
223:10 - development for any period of time
223:13 - you have probably heard someone say that
223:15 - you should use stored procedure for your
223:18 - data access layer because stored
223:21 - procedures offers better performance
223:23 - than having the sql in your application
223:25 - code
223:26 - this is one of those statements that's
223:28 - been around a long time and it is
223:31 - partially correct however these
223:33 - statements alone does not tells the full
223:35 - story so let's explain what is going on
223:39 - here
223:40 - in the last section we compared the
223:42 - performance of dynamically generated sql
223:44 - statements to that of parameterize sql
223:48 - and we saw that parameterized sql was
223:50 - not just faster but also used less cpu
223:53 - and memory on the database server
223:56 - the main reason for this is that when
223:58 - using dynamic sql sql server has to do
224:01 - additional work to parse each unique sql
224:04 - statement and come up with an execution
224:06 - plan
224:07 - whereas in the parameterized sql case we
224:10 - don't need to parse the sql text each
224:12 - time
224:13 - and can use the same plan over and over
224:15 - again
224:16 - when people say using stored procedure
224:19 - offers a performance advantage what they
224:21 - are referring to is that using stored
224:24 - procedure will outperform dynamically
224:26 - generated sql
224:28 - from a performance standpoint stored
224:31 - procedure are very similar to
224:32 - parameterized sql statements
224:35 - you pass in some values to a stored
224:37 - procedure these are attached to a
224:39 - statement or statements within the
224:41 - stored procedure that needed them but
224:44 - effectively sql server has already
224:46 - passed the procedure and has an
224:48 - execution plan
224:49 - so you are getting the same benefit as
224:51 - in the parameter sql case
224:54 - when you compare the performance of
224:55 - using stored procedure to using
224:57 - parameterized sql statements the
225:00 - performance is essentially the same
225:02 - there is effectively no difference
225:04 - between the two
225:06 - now this doesn't mean that there aren't
225:09 - good reasons to use stored procedures
225:12 - there are
225:13 - especially when it comes to security and
225:15 - restricting excess of data
225:17 - but from a performance point of view
225:20 - parameterized sql statements and stored
225:22 - procedure perform about the same
225:24 - so the next time you hear the statement
225:26 - that stored procedures offer better
225:28 - performance you will understand this
225:30 - statement comes with a qualifier
225:33 - stored procedures do perform better than
225:35 - dynamic sql but as we see in the last
225:38 - clip we don't want to be writing dynamic
225:41 - sql anyway
225:42 - otherwise stored procession and
225:44 - parameter sequence statements offer the
225:46 - same benefits in terms of performance
225:49 - so as long as you choose one of these
225:51 - strategies you will be fine from a
225:53 - performance perspective
225:55 - in the next video we are going to take a
225:58 - look at commit behavior and how this can
226:00 - affect the performance of your data
226:02 - access code
226:03 - you are watching the video of module 6
226:06 - applying common performance practices
226:08 - part 4 commit behavior and performance
226:13 - when you run an insert update or delete
226:15 - statement from your application against
226:18 - sql server an implicit commit is issued
226:21 - for each statement
226:23 - because by default the sql server driver
226:26 - is set with it auto commit behavior
226:28 - turned on
226:30 - this is true not just for the dotnet
226:32 - data provider but also for jdbc oledb
226:36 - and odbc as well
226:38 - so consider the code sample that you see
226:41 - on your screen
226:42 - what i am doing is adding all of the
226:44 - courses a student wants to enroll in for
226:47 - a semester
226:50 - and these environments are in a list
226:53 - and the typical student will have five
226:55 - or six courses that they want to enroll
226:57 - in
226:58 - so some pretty simple data access code
227:01 - we just iterate over each time in the
227:03 - list and run our insert command
227:06 - however because of the default auto
227:08 - commit behavior of the sql server driver
227:12 - when this code is run there will be an
227:14 - implicit data comment done each time one
227:17 - of this statement is executed
227:19 - what this means is that up on the sql
227:22 - server sql server has to write an entry
227:25 - to the transaction log to make sure
227:27 - there is a record of the dml operation
227:29 - being performed
227:30 - so there is a write to the transaction
227:33 - log incurred each time one of this
227:35 - statement is executed
227:37 - what we really intended though with this
227:39 - code is probably to insert all of the
227:41 - records at once for the student or none
227:44 - of the records at all so this work
227:46 - should be performed inside of a
227:48 - transaction
227:49 - this is a pretty common pattern to use a
227:51 - transaction like this
227:53 - another example being an e-commerce site
227:56 - inserting items with an order and we
227:59 - want all of those inserted to succeed or
228:02 - none of them so our database is in
228:05 - consistent state
228:07 - so to accomplish this we are going to
228:09 - create a transaction here
228:11 - then we will pass the transaction to the
228:13 - constructor of the sql command to attach
228:16 - it to the command
228:18 - and finally when all the items are
228:20 - inserted we commit to the transaction so
228:23 - this makes much more business sense
228:26 - but as it turns out it also performs
228:28 - better because now we are committing
228:31 - once per student rather than once per
228:33 - enrollment record
228:35 - so sql server doesn't have to write to
228:37 - the transaction log as often
228:39 - i have some results of when i run this
228:42 - program before i started recording that
228:44 - i can show you
228:46 - and you can see here that using a first
228:48 - method when we had to the default
228:50 - autonomic behavior on it took a little
228:53 - bit more than 30 seconds to insert one
228:55 - leg 65 000 rows
228:58 - when i used a transaction
229:00 - which is also more correct from a
229:02 - business sense it took a little more
229:04 - than 23 seconds
229:06 - so about 7 seconds faster on those same
229:09 - 1 lakh 65 000 rows
229:12 - now these numbers of course aren't
229:14 - absolutes and you will likely get
229:16 - somewhat different results on your
229:18 - system
229:19 - but the point is that we want to pay
229:21 - attention to our commit behavior because
229:25 - if we are committing after every dml
229:27 - statement we execute and we really don't
229:30 - need to
229:31 - we are imposing more of a load on sql
229:33 - server and that's going to slow things
229:36 - down so how often should you commit as
229:39 - often as your business transaction
229:41 - dictates
229:42 - and in some cases that may very well be
229:46 - after every statement
229:48 - if not though
229:50 - just make sure that you are explicitly
229:52 - using a transaction so you are
229:55 - controlling the commit behavior and not
229:57 - using the default auto commit behavior
229:59 - of the database driver
230:01 - if you are using an orm most of the time
230:04 - your orm should be handling this
230:06 - property for you
230:07 - however there are some other issues you
230:10 - want to watch out when you're using an
230:12 - orm
230:13 - so we will talk about those in the next
230:15 - video
230:16 - you are watching the video module 6
230:18 - applying common performance practices
230:21 - part 5 object relational mapper just
230:23 - generates sql
230:25 - over the last few years object
230:27 - relational mappers have become very
230:29 - popular in the development community
230:32 - and it's easy to see why orms allow us
230:35 - to
230:36 - work at higher level of abstraction and
230:39 - they also eliminate the need to write a
230:41 - lot of repetitive data access code to
230:43 - get data into and out of our database
230:47 - thereby helping to increase our
230:49 - productivity as developers
230:52 - on the downside removing us from having
230:54 - to work directly with the database gives
230:57 - us less visibility and less control into
231:00 - how our data access code is executed
231:03 - and so one of the side effects of this
231:05 - is that if you just leave your data
231:07 - access up to the orm
231:09 - and don't think about how that data
231:11 - access is performed you can get very
231:14 - poor performance
231:15 - let's see what we mean by this
231:18 - i have a very simple application here
231:20 - that is using entity framework
231:23 - and the intention here is to search all
231:25 - of the current application who have a
231:28 - gpa over 3.5 and the math sat score of
231:32 - over 700 the code here is perfectly
231:35 - valid and will return the desired
231:37 - results
231:38 - the issue is that in this case there is
231:40 - not an index on any of these columns so
231:43 - this query will end up performing as
231:45 - full table scan
231:47 - which in this case will not result in
231:49 - very good performance
231:51 - having an iq variable
231:53 - we have a fluent interface that is very
231:55 - flexible and it becomes easy to just
231:58 - think about attaching the necessary
232:00 - expression here in code and not to think
232:03 - what needs to happen when the expression
232:06 - is converted to sql
232:07 - and sent to the database
232:10 - so we just need to keep in mind as we
232:12 - use our orm ultimately this expression
232:14 - will turn into sql
232:17 - and all of the rules we have talked
232:18 - about in this course
232:20 - like being properly indexed and having a
232:22 - query with a selective where clause
232:24 - still apply if you have any question
232:27 - about exactly what is happening you can
232:30 - get the actual sequel that is being
232:31 - generated by the orm by tracing your
232:34 - application as we showed in the last
232:36 - module
232:37 - or by using some of the client
232:38 - client-side tracing techniques that are
232:41 - present in most orms
232:43 - then once you have the sql you can
232:46 - generate an execution plan for the
232:47 - statement and apply all other techniques
232:50 - we have learned throughout the course to
232:52 - make sure the statement performs the way
232:54 - you need to it
232:56 - then once you have the sql you can
232:58 - generate an execution plan for the
232:59 - statement and apply all the other
233:02 - techniques we have learned throughout
233:03 - the course
233:05 - to make sure that statement performs the
233:07 - way you need it to in the next video we
233:10 - will look at another issue that
233:11 - frequently occurs with orm and also
233:14 - occurs in other scenarios and that is
233:16 - the n plus 1 query issue
233:19 - you are watching the video module 6
233:21 - applying common performance practices
233:24 - part 6
233:25 - solving the n plus 1 selects problem
233:29 - another performance pitfall you want to
233:31 - be on the lookout for especially when
233:33 - using an orm is the n plus 1 selects
233:36 - problem
233:38 - the problem occurs when you have a
233:39 - parent object loading its child data
233:42 - and in loading that child data the data
233:45 - access layer issues a separate sequence
233:47 - statement for each child object that
233:49 - needs to be loaded
233:51 - let's look at an example of this
233:54 - we have a piece of code here and what
233:57 - this code is trying to do is to get the
233:59 - cumulative gpa for a student
234:01 - so it needs to get all of the courses
234:04 - taken by the student
234:05 - their grades in each course and the
234:08 - number of credits each course is worth
234:11 - in order to perform that calculation
234:13 - so this seems pretty straightforward we
234:16 - load a student object and then through
234:19 - the navigation properties on the object
234:21 - we can get to all of the other data
234:24 - that we need to do this calculation
234:26 - what happens though is that initially
234:29 - our students object doesn't have all of
234:31 - the data that it needs
234:33 - so the data for this navigations
234:36 - property are lazy loaded
234:38 - i have an extended event session tracing
234:42 - all of my sql right now
234:44 - and i am going to actually run this
234:46 - program so we can see what's happening
234:49 - and so we see that the program just
234:52 - prints the name and gpa
234:54 - but now i am going to switch over to the
234:56 - trace results so we can see what sql was
235:00 - run
235:01 - i have my trace file open here and i
235:04 - have done a little bit of cleanup to
235:06 - filter out all of the sp reset
235:08 - connections messages
235:10 - that the sql server driver issues when
235:13 - it retrieves a connection from the
235:15 - connection pool
235:16 - so you see right away there are quite a
235:20 - few statements here that are executing
235:22 - and if we group these statements
235:24 - together we actually get a better view
235:27 - of what's going on
235:28 - there are some queries in here that are
235:31 - over headed for entity framework
235:33 - but we are not worried about those
235:36 - what we care about are these two
235:39 - statements that are executed 42 times
235:42 - each
235:43 - and if you expanded out the sequel that
235:46 - in each of those statements
235:48 - you would see that these two queries
235:50 - that are grabbing the course offerings
235:52 - and course records from the database
235:54 - what is happening is that entity
235:56 - framework is able to get all of the
235:58 - courses this student has taken with a
236:00 - single query from a course enrollment
236:02 - table
236:03 - but then in iterates through each of
236:05 - those course enrollment records and to
236:08 - get over to the course records where the
236:10 - number of credits for the class is
236:12 - stored it first has to get each course
236:15 - offering record and then each course
236:18 - record and so one by one it is iterating
236:21 - through this
236:22 - lazy loading each of those objects for
236:25 - however many courses that this student
236:27 - has taken
236:29 - and we can see here
236:30 - this student has taken 42 courses so
236:34 - that means we have 42 unique courses
236:36 - offering records we have issued queries
236:39 - for
236:40 - and then again 42 unique courses records
236:43 - that we also had to load
236:45 - we also had to load some grade records
236:48 - so we could see what each grade is worth
236:52 - but there is only two unique values for
236:54 - those
236:55 - so
236:56 - that's just two more statement in this
236:58 - case
236:59 - but you see the problem to calculate the
237:02 - student's gpa
237:04 - entity framework is having to run 87
237:06 - individual queries by my count
237:10 - so that's going to be very chatty and
237:12 - not very efficient
237:14 - the issue here is the lazy loading
237:17 - pattern
237:18 - which fetches data just as it needed
237:21 - while this is convenient for many
237:24 - scenarios
237:25 - it's not the right pattern for this
237:27 - scenario because we are having to go
237:29 - back and forth to the database to get
237:32 - all of this database piece by piece
237:35 - what we can do instead is use eager
237:37 - loading to instruct entity framework to
237:39 - load all of these data upfront and then
237:42 - entity framework will create a single
237:44 - query
237:45 - that gets all the data we need rather
237:47 - than issuing all of these little queries
237:51 - so to do that we just use the include
237:54 - method up here when we fetch our student
237:57 - object
237:58 - so i will put that in now
238:01 - and this first line is going to tell
238:03 - entity framework to get all of the
238:05 - courses offering as well as courses for
238:07 - the student and the second line will get
238:10 - all of the grades attached to any of the
238:12 - course enrollments data so when we get
238:14 - our student object we will have all the
238:17 - data we need to do this calculation
238:20 - so i am going to save this file and
238:22 - rerun this program so we can get a
238:25 - second trace of what's happening here
238:27 - and now we will look at that race we can
238:30 - see that this time we have only one
238:32 - statement and in fact if i take this
238:35 - statement here
238:36 - and i look at it and i have copied that
238:39 - in a notepad again so we can do just
238:42 - that
238:43 - we see that this one statement is
238:45 - getting all of the data that we need
238:48 - so in this case this is going to be much
238:51 - more efficient because we are not having
238:53 - to go back and forth to the database
238:55 - many many times this can happen when you
238:58 - are lazy loading objects in an orm as we
239:01 - have just seen
239:02 - but i have also seen this problem in
239:05 - traditional data access code
239:07 - where the code was written in such a way
239:10 - that it was retrieving all the child
239:12 - data for an object in an one by one
239:14 - fashion
239:15 - the point is when you are loading child
239:17 - data like this you want to think about
239:20 - the internals of how that is happening
239:23 - and make sure that you are not iterating
239:25 - over some sort of collection and issuing
239:28 - a sequel statement for each object in
239:30 - that collection
239:31 - because that's going to be very chatty
239:34 - and that's not going to perform very
239:36 - well
239:36 - so think about this whenever you are
239:38 - loading data that's child data of
239:40 - another object and if needed we don't
239:44 - hesitate to use one of the various
239:46 - tracing facilities to make sure that you
239:48 - are not issuing a whole bunch of
239:50 - individual sql statements in order to
239:52 - get the data in the next video let's
239:54 - wrap up this last module of this course
239:57 - you are watching the video module 6
239:59 - applying common performance practices
240:02 - part 7 module summary
240:05 - in this module we looked at some
240:07 - application practices that you want to
240:09 - make sure and implement for your
240:11 - application to have the best performance
240:13 - possible
240:15 - first we talk about making sure that all
240:18 - of the sql in your application is
240:20 - parameterized sql
240:22 - this not only protects your sql
240:24 - injection attacks but has a performance
240:26 - benefit as well
240:29 - as sql server can reuse the same
240:32 - execution plan over and over
240:34 - making your code run faster and saving
240:37 - resources within sql server
240:40 - we also talked about stored procedure
240:42 - and showed that from a performance
240:44 - standpoint parameters sql and stored
240:46 - procedures are equivalent
240:49 - so you can be comfortable choosing
240:52 - whichever approach best fits for your
240:55 - situation
240:56 - then we talk about commit frequency and
240:59 - how auto commit is turned on in the sql
241:01 - server driver by default in some cases
241:05 - it makes sense to commit after each dml
241:08 - statement
241:09 - but otherwise using transaction and
241:11 - committing however often makes logical
241:14 - business sense
241:16 - not only in more correct for your
241:18 - application
241:20 - it will also perform better as well
241:22 - finally we talked about orms
241:25 - orms are a great tool that enhance our
241:27 - productivity and save us from writing
241:30 - the same data access code over and over
241:32 - again
241:33 - but
241:34 - this additional level of abstraction
241:37 - sometimes makes it hard to understand
241:39 - what is happening inside of sql server
241:42 - so just remember that ultimately
241:45 - everything does have to get converted to
241:47 - a sql statement and then all of the
241:50 - normal rules about selectivity and
241:52 - indexing apply
241:55 - finally if you are loading any child
241:57 - object especially through lazy loading
242:00 - be on the lookout for the n plus 1
242:02 - select problem
242:04 - basically each child object will load
242:06 - one by one from the database
242:09 - and this chattiness has a tendency to
242:11 - slow down things
242:13 - so be on a lookout for this and use
242:16 - eager loading in those situations where
242:18 - it makes more sense
242:20 - this brings us to the end of the course
242:22 - and i hope you have found your time well
242:24 - spent in learning how sql server works
242:27 - from a performance standpoint
242:29 - and what you can do to make sure your
242:31 - application have optimal performance
242:35 - we have introduced a wide variety of
242:37 - concepts and tools that can help you
242:39 - understand
242:40 - what sql server is doing
242:42 - and what you need to do in order to
242:44 - performance tuning your application
242:46 - the best course of action now is to get
242:49 - some hands-on experience with these
242:51 - tools by looking at an application and
242:53 - database that you are familiar with
242:56 - because this is how you really learn
242:58 - what to look for
243:00 - and what the data is telling you
243:02 - thank you very much for your valuable
243:04 - time
243:05 - if you have any feedback please feel
243:08 - free to contact me on twitter at icode
243:11 - mechanic and you can also send me the
243:14 - mail on icodemechanic gmail.com
243:17 - you can also comment in the comment
243:19 - section down here at youtube
243:22 - thank you very much again and good luck
243:24 - for your future

Cleaned transcript:

if you are going to use sql server at any sort of scale you're going to have to think about performance in this course you will learn the essentials of sql server performance you will see how to diagnose what is happening with a slow running sql statement and what strategies are available to make these statements run faster or john aurora teaches this course to follow along with the course you should have some development experience and some experience with sql server this course is divided into the six modules you see on the screen now you're going to learn everything from analyzing sql statements for performance to applying common best practices to ensure the best possible performance i encourage you to leave a comment with the most interesting thing you learn for the benefit of other campers who are watching this course you are watching the video of module 1 getting started part 1 course introduction it is very annoying for every developer to get the result very slow from the queries that we are executed on the database and the slow result leads to the slow application performance which leads to unsatisfied customers in this course i am going to show you what you need to know as a developer about sql server to make sure that your data access code is performing well as a developer i am assuming that you are already comfortable in analyzing and debugging code in languages like c sharp and java but you may be uncomfortable with your level of knowledge of sql server and how things works in the database in this course i will help you feel more comfortable when you are working with sql server especially for the times that you are diagnosting and solving the performance issue and this is the most important part because in almost all system that we build our database is a critical component of the whole system so it is very important to understand that a database like sql server works so that we can make sure that we are getting the most out of it in this course we are going to focus on the performance aspect that we as a developer have control over that is making sql statement run as efficiently as possible and making sure that our application avoids the use of some wellknown data access antipatterns that are common cause for the performance problem this will give you the number of benefits for the application that we build the first benefit is efficient sql statements means that our statements will run faster making our application more responsive to our valuable users second benefit is efficient data access means that our application will be more scalable so we will be able to serve more number of users and transactions on the same hardware and finally we know that many applications are deployed on the cloud either as sql server deployed on a virtual machine hosted in the cloud or by using a platform as a service solutions like sql azure when you are hosting in the cloud you have to pay for the resources that you use so in that case if you have an inefficient sql it doesn't just make your application slow but it actually cost more to run since you are using more resources so you can see there are pretty much reasons why we want to understand how we can access our data in sql server as efficient as possible now there are some arguments that sql server performance is solely the domain of the database administrators but i believe it is important equally for application developers to have a strong grip of sql server performance concepts so let's discuss it in the next video why you as a application developer should learn about sql server performance you are watching the video of module 1 getting started part 2 why developers should understand sql server performance there are many reasons why you as a developer should understand sql server performance one of the biggest reason is that generally the responsibility of the overall performance of the app is on the shoulders of the application development team and we all know that how much application performance is important for the user experience in today's time if our application is slow and unresponsive or can't scale to meet the number of users it need to be then the user is not satisfied with the app and looks for the alternative the second reason is as an application developer we have the view or scope for the whole system along with the knowledge of the application code we also have an idea of the use cases for the application and the data being used stored accessed by the application most important we have a view of how all the pieces of the applications fit together into different feature of our application and how these features are used by our users at the time of tuning sql statements or creating indexes in our database it is always good if we understand the larger context of our application such as what the application is trying to do and how it is trying to do so our knowledge of how the application works is greatly benefited in this case finally there will be no doubt at the times when you need to work closely with the dba team at your company this might be to search an issue from an application or do some database design work or any other reason by better knowing how sql server works you will be able to engage or help your dbas more efficiently you will be able to work more effectively analyzing problems and you will better understand what tools are available and how they can help solve these problems in the next video we are going to see what tools you required to follow along with me you are watching the video of module 1 getting started part 3 tools you need in this course you will see me working in the local copy of sql server 2019 developer edition however the lessons and techniques in this course will also be applicable in sql server 2008 2012 2014 2016 and also in 2017. if there is difference in how you perform something i will point that also if you are using azure sql as your database then there are no worries the tools and techniques you are going to see will also be applicable in that case also if there is difference in how you perform something i will point out that also there is a misconception among many peoples that to really understand sql server performance you need fancy expensive tools but it is not always true in fact all the tools that we are going to use in this course have already been installed on your workstation as they are a part of the sql server client installation if you have already installed sql server management studio you are ready for this course the features that i am going to show you in this course are already present in management studio and you might be unaware of those features i will also show you how we can query performance data out of sql server dynamic management views from management studio and how we can use management studio to set up event traces so by having basic sql server client tool installed and knowing how to use them you are done to accomplish almost all the performance tuning that you need to do you have to download sql server developer edition so that you can install the sample database and follow along i suggest to download from the link shown on screen and i will provide this link in the description box for your convenience press download and run the setup file and follow the instructions after installing sql server you have to download and install sql server management studio which is also called ssms you can download it from the link shown on the screen or i will provide the link in the description box for your convenience on this link click download sql server management studio and run the setup file and follow the instruction now speaking of demos i have a sample database for this course that based on a fictional university that all of the demos are based so in the next video i will show you how you can download a copy of the database and get that imported in your instance of sql server you are watching the video of module 1 getting started part 4 restore the sample database we are going to use the sample database throughout this course and this database contains enough data so that we can do some performance tuning on sql statements and get a feel of how things are going on in the database so first you have to download the simple data and you can download it from the link shown on the screen and i will provide this link in the description box also for your convenience download the appropriate file for the version of sql server that you have installed this is going to download a sql server backup file called students.b we will take this file and restore this in sql server to create a sample database to do this you have to go into sql server management studio and then in the database folder we just going to rightly and we select restore database that's going to bring up the dialog box here and we want to choose the second radio button which is device and then we will click on these three dots here to find our file then we will click add and this will bring up the file explorer here and i have my file out here in the d drive under the folder sql backups select the dot bak file i will say ok everything looks good here so i will say ok again and then this is going to go ahead and start the restore process so now you can see that i have this database and i can access this i have my tables in here i can use this just like any other database on my system so now that we have the sample database installed in the next video i will cover some basic sql server concepts you need to understand around tables and indexes before we dive into analyzing sql statements in the next module so we will go ahead and do that next you are watching the video of module 1 getting started part 5 table concepts i would like to go through a couple of sql server concepts that will come up over and over again throughout the course to make sure that you are familiar with them we have all worked with tables of sql server but many of us may not have given much thought to how sql server actually stores the data inside the table typically table data in sql server is stored in the form of cluster index structure this means the data in the table will be stored in a tree like structure like you can see on the screen at the bottom of this tree we have the leaf nodes of the tree and this is where the actual data rows for the table are stored each of these leaf nodes is what is known as a page in sql server and the page is 8 kb in size depending on the size of the rows a table contains each page will generally store somewhere between 50 to a few hundred rows of data inside of it what is important to know is that the data in these bottom nodes of the tree is actually stored in sorted order and that sorted order is determined what is called the cluster key of the table by default the primary key of the table will be cluster key by having the data in sorted order in this bottom nodes sql server can maintain the tree structure like you see over the top of the data the nodes at the top of the page allow sql server to quickly traverse the tree and find the data it's looking for as we will see in example on the slide in the sample database i have a table named students and the primary key of the student table is the student id column so student id would be our cluster key in this case what that means is that down here in my leaf nodes where the data is stored my student data would be in sorted order by the student id so in this example the first leave node is going to be storing the data rows for students 1 to 100 the second node will be storing the rows for students 101 to 200 and so on in this top two levels of the tree we have a series of pointers that will help direct us to the data so let's say now that we want to find the row for student number 327 we start up here at the root node and it will tell us that we need to go to the intermediate node that is leaf node that is in the second from the left and it is in the leaf node where we will find the data that we are looking for so you see you are able to traverse this tree in just three operations even for tables that have hundreds of thousands or even millions of rows in them by using a tree structure like this sql server is able to find the data it is looking for in a just few operations this tree structure that sql server uses for storing data is very efficient as long as we are looking up data by how the data is stored in the table in this case by the primary key which is a student id column however we often need to be able to look up the data by some other criteria say in this example we want to search for a student by their first name or last name and this creates a problem our data down here in the leaf nodes is sorted by student id but there is no correlation between a student's id number and their name students names will effectively have a random distribution throughout all the data pages at the bottom of the table so we have no idea where a particular name may be so what sql server will do in this case is perform a scan of the structure meaning it will read each and every one of these data pages here at the bottom of the cluster index structure looking for the data that matches your criteria for a table of any significant size though you will have hundreds or even thousands of these data pages in the bottom of the tree most likely many of this would need to be read of the disk so there is a significant i o cost and then sql server will have to use lot of cpu in order to read through each and every row what we can do though is we can create an index on our table though to help sql server finding that data more efficiently so let's discuss the index in the next video you are watching the video of module 1 getting started part 6 index concepts a database index help us quickly find data in a table when we are searching for data by some other criteria other than the cluster key that our table is organized by for the example that we have just talked about searching for students by name we would want to create an index on the students table over the last name and the first name column as you see an index uses a tree structure just like our table but in this case the entries in the index are sorted by the index key which is in this case is the combination of the last name and the first name column and the index does not contain the actual data rows instead in the bottom nodes of the index that are represented and marked with the rp the index will contain row pointers back to the table which indicates where the corresponding data in the table is located if we look in the detail onto one of the bottom nodes in the index it would look something like this we see here that we have the key of the index the last name and the first name of the students and those are in sorted order and then we have the student id value for that student so for example if we have a sql statement with the where clause that you see on your screen where we are searching for this student by name sql server will first traverse the tree structure of the index to find the student id number and then it will traverse the tree structure of the table to locate the data for the student so there are actually two operations that must occur but each of this operation is very efficient and is usually turn out to be much faster to do these two operations traversing the tree structure then to scan through all the rows of a table we also can and often do have multiple indexes on a table in order to cover the different ways our application might search for data in that table so if we think about the students table we might want to find a student by the name as we just talk about or perhaps by their email addresses so in this case we would create two separate indexes on the students table each index being targeted in a way that our application searches for the data in the table what all of this means is that if your sql statement is using some criteria to locate data other than the primary key we are going to want that statement to use an index this way we can take the advantage of the tree structure of the index to quickly find the pointer to the data that we need and this is much more efficient and therefore much faster than having to scan through all the rows in the table from the next video we will start our next module which is module 2 analyzing sql statements for performance in that we will see what our sql statements are doing in the sql server you are watching the video of module 1 getting started part 7 summary of this module in this introductory module we talked about why it is important for you as a application developer to understand sql server performance almost every application written is going to use a database backend in some way and the database is almost always an important component of overall application performance since we as application developers understand what our application is trying to do when it runs a statement against sql server we can be very effective at database performance tuning once we know what to look for because we understand the larger context that all of our sql is running on we also covered the tools that you need to analyze sql server performance then we discuss how to get the sample database downloaded and installed so you can follow along throughout the course and finally we covered some basic concepts around how sql server stores data in tables and how indexes works so that we have a solid understanding of these fundamentals for the future modules in this course here we have successfully finished module 1 getting started in the next module we are going to start looking at how to analyze the performance of individual sql statements you are watching the video of module 2 analyzing sql statements for performance part 1 module introduction so you have a sql statement that is running slow or maybe you just have a statement that you aren't sure is optimized correctly and you need to know what to do next that is what this module is all about understanding how to analyze any sql statement to tell what they are doing and how they are doing to perform in a very quantitative way the first step in this process is to have sql statement give us an execution plan which is the detail steps of how sql server will run our statement when we submitted it to the database after that we will understand how to read the execution plan that sql server gives back to us and how we make sense of what sql server is telling us from a performance point of view we will also see how sql server calculates a cost value for each statement and in fact for each operation within a statement and by looking at these cost values we will tell what statements and operations are relatively more expensive than others we also see how sql server can give us execution statistics on our statements and this means the amount of cpu and io that the statement is consuming what all of this allows us to do is to understand in a very quantitative way how expensive a sql statement is and then when we started tuning that statement we have some very good metrics in place to judge just how much we have improved the performance of this statement we are also going to see little bit about how it is sometimes possible to improve the performance of a sql query just by changing the way that you write your sql and i will show an example of this and finally one of the keys to nla sql statements for performance is to understand how sql server is processing your query so i will briefly talk about the most common operations that you will see in the execution plan and highlight the ones that you especially want to watch out for so let's start right into the example of a sql query that isn't performing quite the way that it should be and see how we work through analysing and fixing this statement in the next video you are watching the video of module 2 analysing sql statements for performance part 2 understanding how sql server will execute a sql statement i have a sql select statement here in my editor window that we are going to use and see the concepts around execution plan and how to analyze a sql statement you can see the output of this statement in the result pane and what this statement is doing is selecting all of the courses that a particular student has taken with a title of the course the number of the credits and the grade that the student received to get this information we need to join three tables we need to query the course enrollments table to get what courses the student has taken and what grade they received and then we will need to join through the course offerings table to get to the courses table so we can get the course number and course title and credits so you might imagine a query like this being run on a web page that produce a grade report for an individual student so i am going to go ahead and run this query and we do see that it takes second to run so maybe you have identified this query as something you need to take a look at and we want to figure out how we can make this query run faster so the first step in that process is to have sql server give us the execution plan that it's using to process the query and to do that we are going to go up to this button on our toolbar with the three boxes and a little icon in it and this is labeled display estimated execution plan and so as you can see if i mouse over over that and the tool tips comes up you can also use a keyboard shortcut of control plus l now clicking this button is not going to actually execute the statement that is in the window it just going to send the statement to sql server to our sql server to return back to us the plan that it would use to run the statement so let's go ahead and do that and so now down here we see in our result pane instead of the data coming back for this query we see a graphical representation of the execution plan what sql server has done is it taken our statement and it broken it down into the individual steps that sql server is going to have to perform when it executes this statement and each of this step is represented by one of this icon that you can see down here in the bottom now you might have a question that how did sql server arrive at this plan well it looked at the tables our statement is accessing and any index that are on those tables and what we are trying to do in our statement and it very quickly compared all of the different options for how our statement could be processed and it determined that this set of operation in this order was the most efficient one when we have a statement that we don't think is performing the way that it should the first thing that we want to do is we want to pull up the execution plan like this and understand what step sql server is performing and if any of these steps are inefficient and time consuming if there is anything we can do about those steps to affect them or affect the overall plan and get them to be more efficient this is just like if you are troubleshooting a slow code in a language like c sharp or java you would go and find out what steps are executed in that program and figure out if any of those steps was bottlenecked or could be executed faster and what you could do speed things up and really the same thing that we are going to do here so in the next video we are going to understand how to read and understand the execution plan that sql server has given us and that's given us some clue about how we can make this query run faster you are watching the video of module 2 analyzing sql statements for performance part 3 reading and interpreting an execution plan for an sql statement let me zoom in on the execution plan to make this plan easier for us to read to read an execution plan we want to work from right to left and from top to bottom you can think of this as though the operations in the right are getting executed first and then as we move to the left this intermediate operations will build upon the already completed operations and eventually we will get to this upper hand corner which represents our complete statement actually this isn't quite hundred percent accurate since sequel server will parallelize parts of our statement for us such that things aren't strictly sequential but for analysis purpose it's easiest if you think of things happening serially starting from the right and moving to the left so we will start out on the right with this cluster index scan operation and we can see that on the second line this operation is occurring against the course enrollment table we can also see on the third line what percentage of the total cost of the query this particular operation accounts for and we see that this is 91 percent in this case so that immediately signals to us that when we get to the point of tuning the statement this is an operation that we really going to want to take a look at in this case i will go through each operations in the execution plan so you can get a feel for how to read these plans that sql server gives you but a lot of times what you will do is you will bring up execution plan and look for the operations that are accounting for the highest percentage of the overall statement cost and immediately focus it on those so this cost percentage can really serve as a quick filter of what you are looking for if i mouse over any operation i will get a popup tool tip that going to give me some additional details information on that particular operation so we see at the top of the tooltip we have the name of the operation again below that we have a little bit of description of the operation that sql server give us in this case we want to remember that a cluster index structure is a typical way that data in a table is stored in sql server so the word scan indicates that sql server is reading the entire cluster index so what that means is that in this operation sql server is reading each and every row in the course enrollments table and that's a table that has 37 lakh rows in it when you are reading execution plans this is actually something that you want to look for is if you have any cluster index scan operations on tables that you know contains a lot of data because what a cluster index scan really means is that you are reading all of the rows in that table and for large tables that of course is going to be very expensive and takes a lot of time so knowing this that we are processing 37 lakh rows in this operation we have a good idea already of why this particular operation is accounting for 91 percent of the cost of this statement now you might ask what rows are we trying to find in this table and that can be answered by looking at the predicate data on this popup which is down here at the bottom so you see our where clause for the statement and what is happening is that we are trying to find out the rows for this particular student but as we just talked about we are having to look through all of the rows in the table to do that we can also get an estimate of how many rows sql server expect to find in this operation and that's given by estimate number of rows value which is shown up here the way that sql server calculate this is because sql server keeps some statistics on each table in the database and these are statistics like how many rows are there in the table and what the rough distribution of data in that table is sql server uses that information in order to form its execution plan and that's also helpful to us here because it tells us that sql server is only expecting to find 45 rows even though we know that it reading a very large number of rows from this table the final item i want to talk about in this popup are these items that are labeled with the word cost and you can see that there are four of them here cost is just a value that sql server used to express how expensive a particular operation or entire statements is compared to another statements or operation as you see here there is a cpu cost which reflects how much cpu this operation is going to use and an i o cost reflecting how expensive the io is for this operation so basically what sql server has done is it's taken the amount of cpu it thinks this operation will consume and the amount of io it's taken those value and its normalized those amount against a scale such that it can add each of those values together and get a single value which represents how expensive this operation is and that's what we see on this estimated operator cost line now we also get a cost for an entire statement which if we go over here and we look at the select in the upper hand left corner we get we see that the estimate subtree cost is 12 here and we can use this value so that we can compare the relative expense of running this statement versus other statement that we have and that helps guide us about what are our most expensive sql statement and which statements that we should tune first so let's go ahead and continue to read this execution plan and understand what is going on in it so we just read the course enrollments table and we got out the records for a particular students in there and then if we come over here to the left we see this step of parallelism if you are unaware of the concept parallelism then i would like to give you a brief explanation about it parallelism is a concept in which the big task is divided into the small task that runs simultaneously and create desired result and finally that partial result of each small task combined into the final big task this concept of parallelism helped to run queries much faster ok now we proceed further in the left and now we can see the nested loop joint and we can mouse over this and from the description it says that we are going to take each row in the upper data set and we are going to prop or investigate the lower data set in order to get that information so this is what is doing our join between these two tables if we go back over here we see that this is the course offering table which is down here on the bottom now this is a cluster index seek operation so it means what sql server is doing is it's actually using the tree structure of the cluster index in order to find the data and as you see here the cost is just one percent and if we mouse over this both of our cost values are very very low for this operation but basically what these three operations here are doing together is finding the data it needs in course enrollments and then in joining that over to course offerings so now we have an intermediate data set of those two table combined in the next step here in nested loops sql server will join the previous data set of course enrollment and course offerings and the result data set after fetching data from the course table and in that it again perform parallelism to generate result so finally we get up here to the select box and this represents our entire statement we already looked at this and this has our entire cost inside of it and also gives us an estimate number of rows now we know that we actually are getting more rows back again the statistic data is just an estimate the sql server has but this is basically the procedure that you use to read an execution plan what you are going to want to do is move from right to left and then when you encounter a join operation you want to read those from top to bottom you can see what each operation is doing and how it fits it with the operations around it and if you want to see more details then you have to mouse over one of the operations so you can get that popup box this is my suggestion to you that you don't have to analyze each and every operation but instead you can just focus on the high cost operations and that helps you understand where you want to focus your tuning efforts in the next video i will demonstrate one additional technique for analyzing a statement and that is how we get the amount of cpu and io that a statement uses when you actually run the statement so we will do that next you are watching the video of module 2 analyzing sql statements for performance part 4 getting execution statistics for a sql statement technique that you will find useful is to use the set statistics commands in management studio so that you can get detailed i o and cpu statistics about the statements being run to do this we are just going to run two commands in our sql editor window set statistics io on and set statistics time on and then i am going to go ahead and execute both of these commands so now when i run a statement i am going to get some detailed information back in the messages tab down here in the bottom in the result pane so just to be clear to get this detailed cpu and io data back you do need to actually execute your statement so unlike in the last segment where we looked at where getting an estimate execution plan didn't actually run the statement this is actually going to run the statement again sql server so you want to be careful with any dml statements like update or deletes because those of course would actually be modified the data in your database in this case i am just running a select statement so i can run that like a normal one so i will go ahead and do that now i will make sure that we get the entire statement here and i will execute this statement so now you see that here is our results and that actually did run a little bit quicker and so the reason that ran quicker is because sql server actually has cached the data that this query needs in memory but as we are going to see if we click over here on the message tab this statement is still pretty inefficient so there are some tuning opportunities present there so let's take a look at that so the first set of items which is right here tells us how long sql server took to create an execution plan for our statement remember the last part where we talk about that when sql server gets a sql statement submitted it has to break the statements up into the individual operations that is going to run and this is the amount of time that it took in this case sql server already has an execution plan cached in memory that it would use so this actually didn't take any time at all in this case next we have some io information for the database object sql server has to read in order to process our statement as we see there were three tables in this case the data needed to be read from and the important number that we want to pay attention to here are these logical reads a logical read is when sql server has to read a page either from memory or disk in order to process the statement so it really helps to indicate how much data is having to be processed in order to run the statement we see here that the course enrollment stable that was over 12 000 logical reads that had to occur and this is because as we saw in the last segment sql server is reading the entire table to find the data that we need each logical read is a page of sql server and remember a page is 8 kb in size so 12 000 times 8 kb we can see that's quite a bit of data that sql server is having to process through here if we move down to the last section it gives us the amount of cpu it took to actually execute the statement and how long that process took now sometimes you will see in this section an amount of cpu that is larger than the elapsed time and what's happening in those cases is that sql server has used some parallel execution for at least part of your statement as such since your cpu on multiple cores we can use more cpu time than what it actually took for the statement to run overall these commands do gives us a good picture of the amount of resources up on the database server that are being used by this statement of course what we are aiming for is that for each one of our statement we want to minimize this number and so that i myself do is i really focus on this number of logical reads a high number of logical reads often indicates a statement is inefficient because it's reading a bunch of data that ultimately it's going to filter out and throw away in some other operation in the statement i also find that when you are reading a lot of data in terms of a lot of logical reads that tends to consume a lot of cpu and ultimately those statements take longer to process so between the execution plan we saw in the last part and this execution statistic we have a pretty good idea of what's going on in this statement so in the next video we are going to actually start working on this and tuning this statement to make it run more efficiently you are watching the video of module 2 analyzing sql statements for performance part 5 improving statement performance by adding an index i bought the execution plan for our sql statement back and by now we realized it this operation here the cluster index scan that is responsible for most of the cost of the sql statement as we discussed this operation is reading all 37 lakh rows which is 12 000 pages from the table just to find out the handful of rows that we need for one student so there is a lot of waste here so what we can do now well actually you have probably noticed that sql server has given us a hint and it's telling us that we really need to create an index on this table so i am going to right click on this recommendation and go to the missing index details so that we can see exactly what is telling us in this case the suggestion is to create an index on the student id column in the course enrollments table and that exactly that we want to do if we look back at our statement we are using the student id column as a filter so we can find just one particular student and if we think about this table in the larger context of our application and how it might be used it is going to be pretty typical use case that we have a student id and we want to find out of all the courses that a student is enrolled in so creating an index on the student id column of the course enrollments table will allow us to quickly search for course enrollments by student id and that's make a lot of logical sense so let's go ahead and get this index created so i just need to uncomment this and then go up here and i will have to give this index a name and this is just how i like to name indexes you actually can give it valid name that you want to and then i am going to go ahead and create this index and so that will take a few seconds here and there we go and so now let's head back to our sql statement and we will get a new execution plan and see how things have changed okay we can see out here that the cluster index scan operation has been replaced with an index seek and then a key lookup so if we go and we look at this here we see the seek predicate now is on the student id so we are using the index to look up the students and notice the cost of this operation it's down there around 0.003 it looks like so this is much much cheaper than the 10.92 value that we saw before now as we talk about in the module 1 part 6 at this course all this does here it lookups just the information in the index and then we have to go to the table to get the actual data for those rows so that's what this key lookup is here and it's doing that with a nested loop join and so this is what actually going up and looking up the data in the table so that we can get the data to come out here so we can see that the rest of the plan looks basically the same if we do look at the overall query though we see that our estimate cost now is around 0.207 right in here remember before it was way up over 12.01 so we have seen a significant saving here so i am going to go ahead and i am going to run this query again and i still have the statistics turned on so we can see the number of i o and the amount of cpu that it's using so i will run this and look at the message tab and you can see here first of all on logical rates before from the course enrollments table we had over 12 000 logical reads and now we are down to 134 only so by using that index we are traversing the tree structure of the index we are doing much much less work before and that's accounting for our statement being more efficient you see down here at the bottom of the sql server execution times we are actually showing zero both for cpu and ellipse time in reality it's probably isn't zero cpu time or zero ellipse time it just below the measurement threshold so again now we have a very efficient statement so i am going to take this result and put them on a slide so we can discuss them i have summarized the result on this slide and as you can see the improvement is dramatic now you might be thinking that the elapsed time only improved by approximately 100 milliseconds but remember i am the only user on my pc running sql server on an actual production system you may have hundreds of user running statement at a time and you may also be dealing with the data sets that are much much larger than what i have in my sample database so on an actual production system the efficiency of the first statement would certainly result in a less scalable system and would probably also result in the longer wait times for users as this inefficient query used up all the resources causing contention on the server i have actually seen this before where a single query that was doing a full table scan brought an entire system down because the query was run often enough to consume all the cpu on the database server which then slow down every other sql statement in the application because those statements were having to wait to get on the cpu so it is really a key to keep all the sql statements in our application running as efficiently as possible especially for statement that are frequently run in our application with that in mind let's look at another sql statement that we need to tune in the next video but in this case the answer is not to add an index you are watching the video of module 2 analyzing sql statement for performance part 6 rewriting sql statements for improved performance i have another statement in my editor window and this statement is designed to get all of the courses section that no students are sign up for so you might imagine a university running a query like this at the start of a semester to find out any courses section that need to be cancelled to accomplish this this query is doing a left join and then looking for any course enrollment records that are null because that indicates that no one signed up for the class we can see the result in the lower pane and we see that we do have a couple of sections then no one has signed up for what we are really interested is the execution plan for this query so let's take a look at that if i mouse over the overall query here we will see that there is an overall cost of 8 and if we look at the execution plan it looks like that cost is being driven by the index seek lookups occurring against the index in the course environments table down here at the bottom so what is happening in these three operations here is that sql server is reading all of the offerings for the courses in a current semester and then through this nested loop join it is probing an index on the course enrollments table ix course enrollments course offering id and looking up how many enrollments there are for that course the issue here is that sql server is looking for all the enrollments for each course when we really only need to know if there is any student enroll in the course since we are looking up all the environments these operations turns out to be pretty expensive making our whole query pretty expensive now in this case the answer is not an index but instead for us to change how we write this query so let's take an alternate approach this version of the query uses the not exist clause to look for any code offerings that don't have any records in the course environments table so let's go ahead and run this version of query and we see we get the same answer as before so now let's take a look at the execution plan and we see if we mouse over this query that our overall cost for this version of this statement is quite a bit lower down around 0.85 and if we look at the entire plan we notice this top operation here and if we mouse over this we see the description tell us that we are just going to return the first few rows from the previous operation and looking down at the bottom we see that the expression is top one so it is actually just getting the first row from the index seek operation which is over here to the right so what this means is that sql server is reading all of the course offerings just like it was doing before but then in the nested loop join what it is able to do is able to prop the index looking for any single row that matches the course offering id number so sql server doesn't have to read the data for every student enrolled in the course it is just looking to see if at least one student is in the course and that's more efficient than getting the data for each and every student android so think of this like you are searching an array of values if you can jump out of this search when you find any value that matches what you are searching for that's more efficient than having to find every value that matches because once you find the first matching value of course you can jump out of the search functions and that's essential what's happening here now i also want to point out that sql server is giving me a create index recommendation here but in this case i am actually choosing to ignore this suggestion the recommendation here is basically sql server wants me to create an index on the course offerings table that contains every column of the table so effectively i will be duplicating all of the data in the table we will talk in the upcoming module about why this is not a good idea but i want to point out this out so that you understand when sql server gives you this recommendation you want to be sure and analyze what sql server is telling you and that it makes sense and don't simply create an index because sequels are made in a recommendation the main point of the two queries we just looked at though is that something the way to improve the performance of sql statement is to rewrite the statement in a different way a sub query can be written as a join or you might change a join to a sub query and as we just showed sometimes using the where exist or where not exist clause can improve a plan because sql server can satisfy the condition with any record that matches the condition instead of having to read all matching records from a table so keep these options in mind when you tune your sql statements in many cases in the modern version of sql server the query optimizer is smart enough to find the best execution plan regardless of how you write your statement but there are few cases where writing your statement a different way will help the query optimizer find a more efficient plan so keep this option in mind so in the next video i am going to summarize the common execution plan operations for better understanding so let's jump ahead you are watching the video of module 2 analyzing sql statements for performance part 7 common execution plan operations we have looked through a couple of execution plan so i want to summarize some of the common operations that you are going to see when you are accessing the data either in a table or in an index these are the typical data access operations you are going to see i am not going to read through each one of these but i do want to point out there are really two different types of operations that are listed here and those are scan operations and seek operations what you need to remember is that a scan operation means that sql server is reading the entire data structure that is acting on so either an entire table or an index and that is of course going to be expensive for any table or index that has a large amount of data in it seek operations on the other hand use the tree structure that the data is laid out in so they can be very efficient to find the data that they are looking for the takeaway is when we are looking at an execution plan and we see scan operations on large table we want to ask why a scan operation is being used and how we can get sql server to change over to using a seek operation this slide summarize the join operations that you will see sql server perform we have seen the nested loop joined quite a bit and the implementation is exactly like the name implies mud join tends to be used more infrequently because the data for both data sets need to be in a sorted order for sql server to use the join if the data for one of the data sets is not already sorted correctly then sql server will have to perform a sort operation first and then perform the join if the data is already sorted mud join tends to be very efficient but if sql server has to first sort the data to use this operation then one of the outer join operations tends to be more efficient the last type of join is the hash match which is sometimes referred to a hash join this is used when sql server needs to join two large data sets and there isn't already some sort of key like an index key that can be used to join the data sets together so what sql server does is it builds a hash table on a smaller data sets on a join key and then iterates through the larger data sets probing the hash table for matching values typically when you see a hash match join it is relatively expensive operation build a hash table in memory takes time and it can take a lot of memory plus as we said sql server will only use a hash match when large data sets are involved so when you see a hash match join you want to ask yourself why a hash match join is required first is that any way to narrow down the data set that are being joined together so that not as much as data need to be joined for example you might look to see if you can include a more restrictive where clause in your statement to cut down on the size of the data that need to be joined second a hash match join indicates there is no existing key like an index key that can be used for join many times this indicates that you are missing an index on one of the foreign relationship between your tables so adding an index may be appropriate again in the next module we will be talking quite a bit about creating good indexes including what columns in a table you want to ensure to index and this usually includes any foreign key columns that your table has there are many operations an execution plan can contain but covering each and every one would take a very long time and lead to some very boring video for you to watch if you do encounter an operation that you need to understand more about that what is provided in the description that sql server management studio gives to you then i would suggest you to take a look at this book on sql server execution plans you can use the url shown on the screen and i will include this url in the description box for your convenience and as you see the ebook can be downloaded for free and you can actually don't ever have to register it this is a direct download link here the book contains a lot of material on how to read execution plan as well as detail information on most of the operation that can be contained within an execution plan as a developer you are probably not going to sit down and read this book from cover to cover but it is a useful reference to have around for the times that you don't quite understand why something is working the way you think that it should and need a little bit more indepth explanation of how an operation work so with that let's go ahead and summarize everything we have learned in this module and wrap things up you are watching the video of module 2 analyzing sql statements for performance part 8 module summary in this video let's summarize what we have discussed in this module about analyzing sql statements for performance when you have a statement you need to analyze the first step you are going to take is to get execution plan of that statement getting the execution plan doesn't actually run the statement but just tell us how sql server will process the statement so we can analyze the plan for any performance bottlenecks once you get the plan you want to read the plan from right to left and when you encounter a join operation you have to read from top to bottom so you analyze the operation at the top of the join first and then the operation on the bottom the statement in each individual operation is going to have a cost value assigned to it and cost is simply a single value that combines together both the amount of cpu and are you required by the statement or the operation with any statement we want to focus our attention on the operation that have relatively high cost because a high cost means that those operations are consuming lot of resources and consequently that will take longer time to complete at a statement level we can use the cost value to compare the relative expense of executing different statements within our application and again we want to focus our attention on the high cost statement our application runs sql server can also give us a detailed execution statistics about each statement we run if we use the set statistics command in management studio what you want to pay special attention is the number of logical reads as each logical read represent an 8 kb page of data a high number of logical reads often represents an inefficient operation that is having to read a lot of data to find the piece of data that is really needed once we understand what operations are causing the statement to be slow we can start to tune the statement to improve its performance we looked at one case where the solution was to add an index as this allowed sql server to use the tree structure of the index to find the data it needed rather than scanning the entire table we also look at the example where we change our sql statement and this is another solution you may want to consider sometimes changing a sub query to a join or may be using the exist keyword helps sql server find a more efficient execution plan you can also do a general evaluation of your statement and see if there is an opportunity to include more selective where clause or possibly change your statement to where it's asking the question you want answer in a slightly different way what getting an execution plan allows you to do to see the individual steps that have to be executed to run your statement and to find which statements are most costly then you can focus on this operation to see what you can do to make these operations more efficient one of the most important tool we have is to create a good index on our table that the statement in our application will use so we will dive into the topic of how to create effective indexes in the next module you are watching the video of module 3 building effective index part 1 module introduction probably the most important step you can take to make sure the sql in your application performs well for which we have to create effective indexes on the table in your database and that is exactly what we are going to cover in this module we are going to start off with a revision on index terminology just to make sure that some definitions are fresh in your mind as we go through this module second we will discuss what column in your database you want to create index and why then we are going to turn our attention to two characteristics you need to pay attention in order to make sure that your indexes are effective and those are the order of the columns in the index and the selectivity of your indexes we will move on and talk about covering indexes and include columns and when those are useful and then we will discuss the effect of using a function in your where clause and how these affect if sql server can use an index for your statement after that we will see the concept of over indexing which is when too many indexes are created on a table which slows down dml statements like insert and updates against the table and then we will wrap up with the discussion about index recommendation provided to you by sql server and some advice on how to interpret those we have a lot of material to cover so let's go ahead and review some index terminology in the next video you are watching the video of module 3 building effective indexes part 2 index terminology refresher before we go ahead i want to take your just few minutes and refresh on some index terminology so these definitions are refresh in your mind as we go through this material when you look at the sql server documentation or any writings on sql server you are going to see the terms cluster index and noncluster index a cluster index is the structure that sql server stores the data in the table typically a cluster index is built on the primary key of the table so the data in the table is arranged in the tree structure organized by the primary key so when we are talking about cluster index we are really talking about the structure that contains the data for the table and how the data is physically stored in a table on disk any other index you create on a table is going to be non cluster index a noncluster index is built over one or more columns of the table which define the index key and then the index also stored a row pointer to where in the table the matching rows for that index key are located in this module when i talk about indexes i am really talking about these non cluster indexes because you are going to need to query the data in your tables by columns other than the primary key and creative effective noncluster index is really the key to achieving good performance when you are doing so there are two types of operations that can be performed against the key a scan operation or a c corporation a scan operation is going to read the entire index to find the matching values it is looking for whereas a seek operation is going to traverse the tree structure of the index in order to find it matches when we say a sql statement is using an index what we really mean is that the statement is conducting a seek operation against the index because this is much more efficient than a scan operation and is really using the index and as it is designed to be used so when you examine operations against indexes in your execution plans you really want to be looking for the seek operations if instead you are seeing scan operations while sql server is accessing the index it isn't really what you want from a performance point of view so you want to examine why you are getting a scan operation and not a seek operation now that we have refreshed on some fundamentals so let's talk about what columns we should be thinking about indexing in our database to deliver good application performance you are watching the video of module 3 building effective indexes part 3 what should i index in my database we saw an example in the last module where we dramatically improved the performance of a sql statement by adding an index to a table by adding the correct index to our database we are going a long way to assure that our application's data access layer will perform well so what we want to do is to understand what tables and columns we want to be indexing in our database from the start so we can avoid any performance issues in our applications there are really two categories of columns we want to make sure to index the first set of columns we want to make sure and index are those that are being used in where clauses of our sql statements whether we are running a select update or delete statement the where clause is how sql server locates the rows we are interested in for our statement having an index on the columns used in the where clause means that sql server will be able to find the rows needed by the statement by using an index seek operation which is very fast and efficient now it is very possible that different statements in our application will contain different where clauses when accessing the same table like we see here in this case we would want to create an additional index to support each statement so in this case we would create a second index over email address you can imagine a search form in your application that allows the user to search for students by their name or by their email address and these two indexes would correlate to those two different use cases now you might also want to search for a student by the student id but in this case student id is the primary key of the students table and sql server automatically creates an index on the primary key columns of the data so we don't need to do that by ourself but for any other way that we are going to be looking up the data on a table we are going to have an index on those column in the where clause that we are using to look the data up the second category of columns that we want to make sure and index are any foreign key columns in our table there are two reasons for this first when we join two tables together sql server will look up the rows in one table and then find the corresponding rows in the second table and to do this sql server is going to be searching for those matching rows in the second table by their foreign key values second when we query data in our application we are often traversing this foreign key relationship to load the data that our application needs we saw an example of this in the last module where we had a query that load the grades for a particular student by their student id if we think about this query in the larger context of the application we probably had some page in our application that the student used a login and once logged in our application kept track of the student's id number during the login session then as we navigate the various pages in our application the student id number will be feed into various queries like the one that we see on our screen to get the data related to the student if you have written any data access code you realize it it is quite common to query across the foreign keys in the table like this is what we are really doing is squaring across parent child relationships so now that we know that what columns we should be looking at indexing let's understand in the next video what makes an index effective so that sql server can actually use it in our sql statements you are watching the video of module 3 building effective indexes part 4 by index column order matters in any database some of our indexes will have multiple columns and the order of the columns in the index determine if sql server will be able to use the index and how sql server can use the index so let's look at an example to understand this i have this index that you see on your screen already created on my applicants table and you see the query i want to run in the editor window as well just below the query is trying to find an applicant by their last name with their state so you might imagine a query like this being run from a sort of search form in one of your application to let an admission counselor finds an applicant what i want to do is to point out that in the index the column order is first name last name and then state and in the where clause of the query i have just the last name and the state specified which is the second and third column of the index so let's get an execution plan and see how sql server will process this statement what i want to focus your attention on is this operation here this is the scan operation against our index and notice that i said scan operation a scan operation like this is actually going to read the entire index so this is not going to use the tree structure of the index to find the matching keys but instead reach each and every key of the index in order to find the matching values so this is like a big linear search through all of the keys of the index what we want to see here is a seek operation because that would mean that sql server is using the tree structure that the index is organized by to find the matching keys and this would be much more efficient the reason why sequel server is using a scan operation rather than a c corporation in this case is because our where clause does not include the first column of the index which in this case is the first name column if you don't include this leading column of the index in your where clause sql server will either not be able to use the index at all or it may have to scan the entire index like we see here which is not very efficient and i will show you some stats in this case to prove it so if we mouse over the query we will be able to see the cost for the entire query and that's about 4.3 and now i have already actually set the set statistics io and time on so if run this query we will get additional execution stats and we can see that this query is taking over 2600 logical reads to process and that includes read both against the table and the indexes on the table so this really isn't very efficient so let's think about what we can do to fix this we could modify our query and make the user include a first name value but this is not really very realistic the reason our user didn't include a first name value is probably because they don't know it so we can't really require them to provide a value that they don't know what we can do though is we can change the order of the columns in our index so let's do that so what i am going to do is i am going to drop the current index and there we go and now i am going to recreate an index and in this case i am going to have the last name column in the first position of this index and the first name column in the second position so i am going to go ahead and create this and now i will take this query and i am going to get the execution plan again we see now that the operation here has changed to a index seek operation and that is what we want because it means that sql server is traversing the tree structure of the index to find the matching index keys so this is going to be a lot more efficient and we can prove that by looking at some of the stats so first we are going to look at the cost here of the overall query and we see now that's down at 1.4 much more efficient than before and if we run this query again we see that now we are only doing 188 logical grids whereas before we are doing over 2600 so again this is much more efficient and the reason why is because when we have those columns in the index in the right order such that sql server can use the index then sql server is having to process a lot less data because it can directly find the data that it needs this has important implications for when we create multicolumn indexes in terms of what order we should put the columns in what we want to do is to think about how our user query the data in the table we are creating the index on and what different combinations of columns are used in the where clauses of these queries then with each combinations of column we want to put the most frequently included columns as the first column of the index the second most frequently used column in the second column of the index and so on so in the first example of this slide since the last name column is always included when doing and search by name it is in the first position in the index first name is frequently included so it is in the second position and only sometimes the state is included so that is in the last position if the user only includes the last name in the query or the last name and the first name that's okay sql server can still use this index because we have the leading column of the index and these columns are selective enough which we will talk about in a moment now our user might also need to search this table for data in a complete different fashion as well in this case by the state and the city that an applicant is from so in this case we need to create a second index on the table because we need the state column to be in the lead position so sql server can use this second index so what you will end up with on your table is an index that is target at supporting each of the different ways that your user of your application access data on this table so it's pretty typical that on the key tables of your application you will probably have from two to four different indexes to support these different query combinations the next step in the process though is to make sure that the index are selective enough so that sql server can use them and we will look at this in the next video you are watching this video of module 3 building effective indexes part 5 index selectivity the second factor that governs how effective an index will be is the selectivity of the index and selectivity is simply a way of saying how many or how few rows there are in the table for each key value in the index for our indexes to be used by sql server and to be effective at speeding up the performance we want our indexes to be as selective as possible that is each value of the index key should only correspond to a few rows in a table or perhaps even only one row let's see why this matters we know that when sql server uses an index it traverses the tree structure of the index to finding the matching keys in the index when it finds the matching key or keys it will read from the index the value of the row identifiers for the rows that matches those index keys in a typical table this row identifiers are just the primary key values of the matching rows then sql server takes this row identifier and look up the actual rows in the table which as we have talked about is usually another tree structure called a cluster index so if the index is selective we will only find a few matching values in the index that we have to look up in the table so we are doing a small number of i o operations overall but what if our index isn't very selective what if for the index key we look up we get back several thousand matches well then we are going to have come over to the table and look up each and every one of those rows so that is several thousand times we are going to have to look up data in this table and since our data is probably randomly distributed throughout the table we are going to end up reading most if not all of the pages in the table anyway so in this case it is actually more efficient for sql server not to use the index and just to read the entire table anyway this way sql server doesn't have to incur the i o of reading the index because the index isn't really helpful in terms of narrowing down what data sql server needs to find when we say that we want our indexes to be selective what we are really saying is that we want them to really help sql server target exactly where the data is that we need to find and to do that we want our indexes to have a high number of unique key values compared to the total number of rows in the table let's jump into management studio and look at some example of this i have created the index at the top of the screen on the students table and i am going to run the query that's at the bottom of the text editor but before i do i am going to give you some statistics about the students table there are about 1 lakh 20 000 rows in the students table and in terms of unique first and last name combinations there are about 1 lakh 7 000 of those so we can tell from those numbers the combination of last and first name that is index is built on is a pretty selective criteria while we do have multiple students that have the same first and last name overall this criteria is pretty unique the calculation that sql server does internally in its statistics to determine selectivity is more complex than a simple ratio but this ratio gives us a good idea of how unique the values in this index are and i find that for everyday use this simple ratio works pretty well so we would expect that sql server would use this index when it executes the query that we see we will get the execution plan to see that indeed that is the case and so we see here that sql server is doing an index seek operation against that index so indeed our index is being used if i mouse over the index seek operation you see that sql server is expecting about 25 values in the index would match this criteria and again sql server is calculating this value based on the statistics it has on the table and on the index so its calculation is more sophisticated than our simple ratio but this is what sql server is expecting to happen then it's going to come down to this key lookup operation and look up each one of those rows and if we mouse over the key lookup operation we again see that value that's about 25 in terms on the number of estimated executions so at this threshold sql survey has calculated that using the index is the right decision because performing the estimated 25 lookups on the table is going to be much faster than reading the entire table now if we run this query we did actually get back 4 rows of the data so this lookup operation actually would be executed 4 times not 25 times the point is though that the lookup operation is only executed a handful of times and we only need to read a few data pages from the table so we are being very targeted in the data that we read which is what an index is supposed to give us let's look at the different situation though when we don't have a selective index and see what sql server does in that situation if i go over to this other tab that i have here we see again that we have another index on the students table that i have created and this index is just over us states run the query that you see here in the bottom of the text editor and what that's going to do is it's going to look up all the students that live in appleton wisconsin remember we have about 1 lakh 20 000 rows of students in the table and in terms of states abbreviation there are actually 52 distinct value in our students table so we have the 50 states the district of columbia and maybe we have some students from puerto rico and another u.s territories if we divide 1 lakh 20 000 rows by 50 we get about 2400 matches per index in this case now this max isn't perfect because of course there are some states like california where proportionately we are going to have many many more students than in the small states like wyoming but for a rough estimated to start with what this is telling us is that for having an index just on the state column it is not going to be very selective because we are actually going to get about 2400 students back so let's go ahead and get the execution plan of this query and see what sql server does with this statement what we see here is that sql server actually isn't using this index on the state column in the query and that is because when sql server look at its statistics we would be getting a lot of matches back from that index and when you read all of those matches out of the index and then go and look up all of those values in the table that actually was going to be more expensive than just reading the entire table direct so sql server not using our index is really telling us that in sql server's eyes the index isn't that helpful because it's not selective enough now you might say that i am not sure i believe that i think that the index would still be more efficient so what i am going to do is i am going to force sql server to use the index on this table and i am going to demonstrate to you that actually the sql server optimizer has made the right choice here so to do this i am going to modify our query to use a database hint and what this hint is going to do is going to tell sql server that i want to use that index and normally i strongly recommend that you avoid using hints in your application as today the sql optimizer included with sql server is very very good and it's almost always come with the correct answer in the most efficient way to execute any statement in 99.9 percent of the cases today the sql optimizer comes with the right answer so the only way i could use a hint in a production application is if it was instructed to do so by microsoft support or i want to demonstrate something like i am going to do now so we have our hint in there and now once again i am going to get our estimated execution plan and we see now a sql server using the index but if we mouse over this index we see that sql server thinks that it is going to get back from the index over 2300 matching rows and if we go down to the key lookup operation once again we are going to have to look up 2300 different values in the table and this is going to turn out to be very expensive in fact then just scanning the entire student table i did run both version of this query prior to recording one without the hint and one with the hint so i could summarize the stats and show them to you which i am doing here what you can see here is that by forcing sql server to use the index that indeed is more expensive and what this is driven by is the fact that by just having the index on the state column that is not selective enough so if we force sql server to use the index it's actually having to read and process more data then if it just read the entire table directly this is an important point many times someone will create the index and they think that the index they created is going to speed up the things but when they look sql server isn't even using the index as we are saying in this case sql server is making the right decision not to use the index because using the index would be more expensive and the reason why is because of selectivity because sql server has to read a large part of the index and then conduct a large number of lookup operations that's accounting for roughly 3500 extra logical ios that have to be read and processed by sql server so the answer here is we need to make our index more selective so let's do that we are going to go ahead and drop this index and then we are going to create a more selective index which is both over state and city in terms of unique combinations for city and state together there are about 13 thousands of rows so we are in a much better place in terms of selectivity for this index so let's get this created now we will go ahead and get the execution plan again for our query and actually we have to get rid of our hint in order to make this work because that index does not exist anymore and we see now with this more selective index sql server is able to use the index because we see this index seek operation here if we mouse over this we see now sql server is expecting to get back about 20 rows in reality it's actually going to get a few more than that but because this index is more selective it's something that can be used by sql server because we are doing a better job of targeting the data that we want one of the takeaways from this is that if we have a column that's not very selective by itself that doesn't mean that we can't use the column in the index it means that we probably need to be using that column in conjunction with other columns in our index and if we need to make sure that the combinations of column together is selected selectivity is really important concept when we start looking at our indexes and the where clauses of our statements so let's look in the next video at some other ways in which selectivity matters you are watching the video of module 3 building effective indexes part 6 like clauses and index selectivity one special case i want to talk about with regards to indexes and selectivity is when you use a like clause in your sql statement first you need to know that if you use this percentage sign which is the wildcard character at the front of the value in the like clause then sql server will not be able to use an index for that column remember the key in index are in sorted order and by having a leading wildcard character in your search value sql server isn't able to use the sorted order of the data to take its advantage so in this case sql server will have to resort to some sort of scan operation either of the entire index or of the entire table when the wildcard character is used somewhere else in the like value of your statement as we see here this has an impact on the selectivity of your index basically only the characters to the left of the percentage sign counts in terms of selectivity for the statement and the index so in this case maybe i am not sure if the last name of the applicant i am looking for is harris or harrison so i am using the wildcard character to search and try to find out the right applicant and i am doing the same thing for the first name but in this case i am only know that the first name starts with the letter t if we get the execution plan for this statement we see that we will still able to use the index because we have specified enough characters in the value that we are searching for but this is still selective enough but let's look at what happened if i trim down the value that we are searching for in the last name to just a couple of characters and i will run the execution plan again and now we see that sql server is doing a scan operation of the table it is not able to use the index and the reason why is because we haven't provided enough information here that we can use the index so really our query just isn't selective enough here the reason i bring this up is because many times i see search pages in applications that don't enforce any sort of minimum requirements on the user in terms of the data that they must supply now sql server will happily run this statement for you they are just going to take a lot of resources and they are going to take a lot of time to finish so selectivity is really a twoway street first we want to make sure that our indexes are selective second we want to make sure that we are including specific enough criteria for our where clause of our sql statements such that our statements are selective and those statement will be able to use the indexes that we have created in our databases this is where we have to use our knowledge of how the user is searching for and accessing data to design good indexes and enforces reasonable constraint on how the user is going to search for data in our application as developers we have a good understanding of how our applications are trying to access our data and we have a good understanding of what the data in our database is so it is really just a matter of combining this knowledge together so we have good indexes in our databases and we are writing sql statements in our application that can take advantage of those indexes in the next video we are going to talk about how using functions in the where clauses of your statement affect sql servers ability to use an index you are watching the video of module 3 building effective indexes part 7 how functions in the where clause affect indexes something you want to be aware of is that if you have a function on the left side or column side in your where clause sql server will not be able to use an index on the table so you see here that i have an index on a table over email address however looking at my query on the left hand side of the where clause where we have the column definition you can see that in this case i am using a couple of different functions to compute the value of just the local part of the email address meaning everything left of the address sign in the email address if you have any sort of function over here on the left hand side of the where clause processing the value in the column sql server is not going to be able to use a regular index created over the column like we see has been created up here if we look down at the execution plan we can see that is the case because we see that sql server is doing a scan of the index not a seek operation so this query isn't going to perform nearly as well the reason why this is happening is because what is sorted in the index are the actual email address values not the computed values so what sql server has to do is to run this function in real time while the query is executing to create those computed values and then it can do the comparison and so to do that you are going to incur some processing cost and not have a very fast query so you want to keep this in mind whenever you see a function like this operating on a column value in the where clause of one of your queries if this is something that you really need to do in your application though there is a technique we can use to address this situation what we are going to do is we are going to add a computed column to the table and then we are going to create an index over that computed column and in doing so sql server will be able to use that index over the computed column so let's do that so first i am going to use alter table statement in order to create the computed column so you can see in this statement i have that same formula with those two functions i could use any functions i want to here even as user defined function the only thing that i need to remember is in order to create an index over this computed column the expression that i put in here has to be deterministic that is it always has to return the same value for the same inputs so let's go ahead and get this computed column created and now that the column is created i actually can query the value back out and i will show those to you if we scroll over here to the right we can see here in our computed column over here on the right and at this point what sql server is doing is it's just calculating the value on the fly when we request this rows out of the table in our query hence the name computed column what we can do now though is we can create an index over this column so let's do that so you see the create index syntax looks just like any other index that we might create what will happen though is when we run this create index statement is that sql server is going to compute all of the values of this computed column for each row of the table and it's going to store those computed values in the index so let's go ahead and get this index created and there we go so now we are going to go back up to our original query and we are going to get the execution plan for that again if i can select the right thing and we see now sql server is able to use this new index that we have created over the computed column and i will mouse over this so you can see that it is using indeed that index and the reason why is because the formula in the left hand side of our where clause matches the formula of the computed column value that the index was created over so what do you want to remember is to be on the lookout for any type of function or expression in your where clauses of your sql statements on that left hand side of the where clause expression because those won't be able to use regular indexes on your table however if you do need to be able to search for data using a computed value then you can use the technique that i have just shown you here where you first create a computed column on the table and then you create an index over that computed column and that will allow your statement to use an index and consequently perform much faster we have covered the fundamental aspects of indexes but there is more to learn about so we will continue our discussion by talking about including columns and covering index next you are watching the video of module 3 building effective indexes part 8 include columns and covering indexes if you take a look at the sql server documentation or some of the index recommendation you get back from management studio you are going to see this index with the keyword include so let's understand what is going on here the include keyword lets you specify that one or more columns values should be stored in the index with the index key but they are not part of the index key so this include columns can't be used by sql server when it searches the index and they don't affect how sql server will lay the index data out in the tree structure they will just have their values stored with the index the reason why this can be useful is because we can create what is called a covering index by using these values normally when sql server uses an index it looks up the index keys in the index to get an row pointer to where the data is in the table and then it has to perform a key lookup operation to get the actual raw data out of the table and we have seen several example of this a covering index is a term used when sql server can get all of the data it needs for a query from an index itself and it doesn't need to perform the key lookup operation there are two factors at play here one notice that we are not using a select star in our query but i am spelling out exactly what columns i want and two notice that all of these columns are in the index either as an index key or as an include column so sql server does not need to go find the actual row in the table to read the data because it has everything that it needs right here in the index and we can see that down here in our execution plan because we noticed that we have the index seek operation but no corresponding key look of operation like we have seen before because in this case we have already all the data that we need if there is just one column that we don't have the data for in the index sql server would have to perform the lookup operation on the table and i can demonstrate that for you if i add the telephone column to this query and now i will get the execution plan for the query again now since telephone is needed by the query and it's not part of the index the index is not a covering index for this query and sql server has to go to the table to get that value if we take out telephone and run our execution plan again we see again we just need the index to fulfill the needs of this query sometimes you will have a query like this where you only need one or maybe two columns that are not in the index and that you are having to go to the table to look up the values for in this case it can make sense to use an include column or 2 so that your index can cover the query and avoid the key lookup operation if you start adding 3 or 4 or more columns as include columns in your indexes though then that is probably a warning that you are going a little too far with include columns in creating covering indexes what you are essentially doing is making another skinned down version of your table but what that mean is that when any of these columns get updated sql server will also have to update the entries in the index to keep it up to date so you are going to incur higher maintenance cost for your index by doing this if you have a query that only needs one or two columns including columns and a covering index can provide a nice additional performance boost just be careful not to go overboard and include too many columns such that you basically now have another copy of your table sometimes to improve performance we need to create numerous indexes on the table which unknowingly leads to over indexing in the next video we are going to discuss about over indexing you are watching the video module 3 building effective indexes part 9 over indexing we have seen how indexes can speed up our sql statements and reduce the number of resources it takes for sql server to run our statements so you might be tempted to think that we should create indexes for every column on every index in our database even if those columns are rarely used in any where clauses or join clauses for our statements unfortunately this isn't quite the case while you want to create indexes that will be used by the sql statements in your application you also want to be careful not to over index your database and by over index i mean create indexes that aren't going to be used by any statements in your or any other application that are running against your database the reason why is that index has a maintenance cost of them and if these indexes aren't helping speed things up in your queries then you are paying this maintenance cost without getting any value from this indexes so let's understand this more when we create an index sql server creates a separate physical structure that contains the data for the index namely the values of the index keys and any include columns that are stored in the tree structure of the index when any sort of dml operation is performed on the table sequel server has to keep all of the indexes on the table in sync with the data in the table so this means that when an insert or delete statement is performed against the table sql server will also have to add or remove an entry from all the indexes on the table when an update statement is running if the values of any of the column in the index are modified then the index has to be updated as well all of this has to be happened when the dms statement executes against the table so sql server can keep all of the data in a consistent state so you want to regard your indexes as investments if an index is being used by statement in your application and is helping speeding up those statements then the cost of the index is well worth it because for most applications the queries that the index helps speed up are run far more frequently than any dms statements against the table so the index is good investment if however you have an index that is never used or is used only rarely you want to investigate why the index is not being used and possibly consider dropping it so how can you tell this well sql server contains a number of views called dynamic management views that gives you access to all types of performance statistics and diagnostics information within the database engine itself we will talk much more about these views in the next module but among these views is one that gives us index usage statistics so we can use a query like the one you see on the screen to look at the values in the user c column and compare it to the user updates column and as long as we have more seeks than updates we are in a good shape otherwise if all we are seeing are values in the user update column this indicates we are paying the cost of the index but not getting any value out of it so it may be appropriate to drop the index there is one more topic i want to discuss and that is the index recommendation that are made by sql server that we have seen popup throughout the module in management studio so we will discuss it in the next video you are watching the video of module 3 building effective indexes part 10 interpreting sql server index recommendations no doubt as you have been watching this module you have been seen a number of index recommendations popup in management studio while i have been demonstrating different concepts and as you experiment with different statements that your application runs no doubt you will see this recommendation pop up then as well it is very useful to have these recommendations from sql server but i would remind you these are just suggestions and you should not automatically create an index because sql server makes a suggestion to do so in my opinion sql server can be overly aggressive with some of its recommendations and if you follow each and every recommendation you would probably end up with the database that was over indexed and therefore had sub optimal performance when it comes to dml statements let's look at an example of this that we have saw in this module earlier i executed this query with selected applicants based on their last name and state i already had an index over the last name first name and state columns which sql server was able to use for this query and you can see though sql server is giving me a recommendation to create a second index over just the last name and state column it is true that this second index would result in somewhat better performance for this particular variation of the query however since there is already an index that sql server is able to use and use pretty effectively i would personally pass on creating this second index the recommendations that sql servers give you are based on the individual query that you are running at the moment so sql server will try to optimize an index for that exact query but what you really want to do is to look at the different variations of your statement and their where clauses and come up with the set of indexes that will work across all the different combinations that you have this means a couple of variations of the statement may not be 100 optimized but again we have to consider the tradeoff around creating multiple extra indexes versus the cost that we will have to pay to maintain each of those indexes but it is to say that you should take this recommendation just as suggestion and then it is up to you to combine your knowledge of your application the data in the database and the suggestion to come up with the right set of indexes for your tables let's summarize what we have learned and wrap up this module in the next video you are watching the video module 3 building effective indexes part 11 module summary having effective database indexes is probably the most important factor in determining the performance of your application data access layer so in this module we talked about how you build effective indexes in your database we lead off by talking about what columns you want to be creating indexes in your database first among this are any columns that you are using in the where clauses of your sql statements as this is the way that your application locates the data that it needs additionally we talk about the need to index the foreign key columns of your table to support any join operation we do as well as the fact that often our application will query data across these foreign key relationships once we know the columns we need to create index on we need to make sure that sql server will actually be able to use the index we create the first thing we need to make sure of that the column in our index are in the right order as sql server will not be able to use the index if the first column in the index is not in the where clause or join condition of the sql statement so that means when we create our indexes we want to put the column that is most frequently used as part of our where criteria at the front of the index followed by the next most frequently used column second and so on we may need to create multiple indexes to support multiple use cases but this is where we can use our knowledge of the application and how our user query the data to determine the best column order for our indexes secondly we want to make sure that our indexes are selective enough the idea of an index is that it helps sql server quickly target a particular piece of data so we want indexes that have relatively few rows per index key because this really helps sql server to target on the right data if we have a column that by itself is not very selective we can still use it so long as we have other column in the index that makes the entire index selective and we make sure that the where clause of our sql statement is also selected oftentimes we need to use the like clause in our statements and in doing so we need to remember that if we use the wildcard character at the front of the like value sql server will not be able to use an index on the column if we have the percentage sign somewhere else in our value sql server will be able to use the index so long as we make sure that we have enough information in the supplied value that it is still selective we also saw that if we have a function in the where clause of a sql statement this will also cause sql server not to use an index and it is this function here on the left hand side or column side of the where clause that is the problem because those computed values are not stored anywhere so sql server has to read all the data for the column and compute these values on the fly if we do have a use case in our application where we need to do something like this and this example of using the sound x function to do a phonetic search might be a good one then what we can do is to create a computed column or column on the table and then create an index over those computed columns and then the sql statement we see here would be able to use the index over those computed columns values we then talk about include columns and covering indexes a covering index is when sql server doesn't have to perform a table lookup to fulfill a query but instead can get all the data it needs directly from the index so if we already have all but one or two of the columns we need for a query in an index we could use an include column to make that index and covering index which provides a little bit of an additional performance boost because sql server can avoid the additional lookup operation the last major topic we discussed was over indexing indexes do a great deal to speed up our statements especially our queries but they do require more maintenance as sql server has to keep all of the index on a table up to date whenever a dms statement is executed against that table so we want to make sure that we only create indexes that are really being used by the statement in our application so we don't have an adverse impact on any dml operation that we may have that's wrap up our discussion of indexes in the next module i am going to show you how to find performance bottleneck in sql server using sql survey built in dynamic management views these views contain all sorts of useful performance information like what statement is taking the longest to execute and if sql server thinks that we are missing an index that would improve performance knowing what information is available in this views and how to access them can really speed up the performance troubleshooting process so i hope you will join me in the next module you are watching the video of module 4 finding performance bottlenecks in sql server part 1 module introduction one of the best feature about sql server is that internally it is always taking data about sql statements that are being run in the database and other events that are happening and we can access this data to give us an overall picture of what is happening inside of sql server with regards to our application for example sql server will give us statistics on all of the sql statements that have been executed in our application database over the last several hours including how many times a statement has been executed how long the statement took on average to run how much cpu would the statements consumed and how much average io needed to be performed with data like this we can quickly identify what statements in our applications are running well and which one we might need to do some performance tuning on now you might think that you need some fancy expensive tools to do all of this and indeed there are lot of tools out there on the market that exposes this information to you in a nice user interface however these tools are just looking at data that is already inside of sql server and all you have to do is to know where to look to find this data and then you can just write queries to get all of the information that you need the source of this data is a series of views in sql server known as a dynamic management views often referred to as the dmvs for short if you go to this page on the microsoft website you can navigate through all of the different dmvs that are available and the data they contain what we are going to do in this module is just concentrate on a few use cases that are the most important to us as developer and the dmvs that we need to solve this use cases we are going to start off in this module by talking about the permission that you need to query the dmvs then we are going to talk about how to use the dmvs to see information like who is connected to sql server and how many resources each session is using then we are going to talk about how we can find what sql statements are currently executing against our server then we will talk about how to find the most expensive statements that are running against our database next we will talk about how we can query sql server to see if it has any recommendation about indexes that might be missing in our database that we need to create and finally we will discuss how we can find if our current index are being used and how often although this gives us a good database level summary of how our application is interacting with sql server and it makes it easy for us to spot any performance bottlenecks that may exist one last item i do want to mention is that all of the queries you are going to see in this module are located on my blog so if you go to the link that you see here on the screen i will also mention this link in the description for your convenience all this query will be there so you don't need to worry about pausing the video and trying to type this query in on your own so with that let's start by talking about the permission we need in sql server in order to access the dmvs and run the queries that we are going to see in this module in order to query the dynamic management views your sql server user needs to have the view server state permission granted to it and unless you are a dba you are unlikely to have this permission by default traditionally sql server's dynamic management views has been thought of as a tool for the dba because they contain information not just related to performance but also about system health and configuration the view server state permission gives you access to all of this information and for some dbs this creates some security concern to give away that level of access it may be possible to get this level of access in your development and test environments but it completely depends on the rules of the company whether this level of access will be available to you as a nondba user in a production database system if you are at a company that is not comfortable granting this access in production to developers you still have a couple of options though first you could have your dba execute the query shown in this module and forward those result to you the second option is that you could work with your dba team to create some views over the top of the sql server dmvs that only exposes a specific subset of the dmv data to you which would relax any security concerns for this module i am going to assume that you do have the view server state permission available to you as i show you these queries so with that being said let's start taking a look at some of the information sql server can provide to us around connections and sessions in the database you are watching the video of module 4 finding performance bottlenecks in sql server part 2 getting information about sql server sessions and resource usage the first query we are going to look at is a query that uses the dm exec sessions dmv in sql server to get a list of all the clients that are connected to our sql server database on my laptop i will get some results from another process that i have running so let's go ahead and run this query right now so we see we get back some information like the session id and the database that the session is currently connected to we also get a status if this session is doing anything at the moment either running or sleeping and we also get when the session was initiated then we get some performance stats about each session the cpu time here is in millisecond memory usage is in 8 kb block and we also have read write and logical leads so if we had a session that are consuming a lot of resources and maybe slowing everyone else down that are connected to sql server we did be able to tell that from this data next we see that we have some information about the client process that owns this session including the host name the program name and the host process id so if we need to know where a session was coming from this would be enough information to help us figure that out now a lot of times what you are going to see in this program name column is something like we see here net sql client data provider and not the actual name of the program that's making the connection for any dotnet program it is this string that is put in by default what you can do though is in the connection string of your program you can include this property application name and then the value that you specify here in the application name property will be used to populate the program name in the column in sql server when you have multiple applications that are using the same database this can be really useful because then you can very easily tell which session is coming from which application the result of this query are useful in two situations one like we said when there is a session that is consuming a lot of resources on sql server second we are also getting a lot of connectivity information here in terms of what clients are connected and how many connection each client is using so for example if i have 4 servers in web cluster i would expect to see about the same number of connections from each one of those machines if i saw that one of those web server didn't have any connection to sql server i would want to go and check the connection string on that machine and also make sure that server was actually receiving in processing web request so there is actually quite a bit that you can tell just by looking at the session information now another question you might want to answer is what sql statement are currently executing in sql server right now so we will look at how to answer that question in the next video you are watching the video of module 4 finding performance bottlenecks in sql server part 3 finding what sql statements are currently executing sometimes we have the need to see what statements are running right now in sql server imagine you have a situation where your application is running very slowly or maybe even appears to be unresponsive and you need to quickly be able to treat that situation so you may be trying to figure out if this is a problem in the application code maybe a web service you are calling is down or maybe you have a query in sql server that takes very long time so what you want to be able to do is look at sql server and quickly figure out what your app is doing in the database and if the problem you are seeing is in the database tire and so we can answer this question with a query like we see on our screen right now this query seems quite long but we will walk through it and we will see that actually it's pretty straightforward so we will scroll down to where we can look and see what dmvs this query is using first we see that what this query does is it joined together the dm exec sessions and dm ex ec request view so that we can get what statements are currently running for each session then we need to bring in the text of this statement and we do that with the dm exec sql text view and the execution plan of the statement is brought in with the dm exec query plan view what this will allow us to do is that if we see a statement that we want to know more about we will be able to just click on that execution plan in the result set and bring that execution plan up so that is pretty useful also in the dm exec request view there is a column called blocking session id and if there is another session that has a sql statement that is blocking this sql statement that column will have a nonzero value so that is what these three views here are about if a sql statement is being blocked by another session we will get all of the information on the blocking statement and blocking session right here in the same query looking at the where clause these lines tells sql server not to return any statement from our current session so basically we are not going to get our dmv query that we are looking back here in our result set also we have this line here which is going to limit the result to the current database if you want to see all of the activities on the server you would just remove this line or command it out let's jump back up to the top to where we can see what data we are getting back we have the session id of the sql server session and we also have the sql statement here these three line here that are labeled sql statement what these are doing is if the statement that running is part of a stored procedure these are getting the individual sql statement so we can look at if we have just a plain old sql statement then the sql statement field and the parent statement field which is down here those are going to be the same but if we are inside of a stored procedure we will get the individual statement here and then parent statement will be the name of the stored procedure you can see we also have some information about the client that is running the statement and this includes the host name the program and the process id of that client so for example if an asp.net web app was running a statement the process id field here would contain the process id of the worker process over on the iis web server then we have some statistics about the statement itself including what time the statement started how long it has been running how much cpu it has consumed and how much io it has performed in most situation these numbers are going to be quite low because most statements run for a couple of hundred of milliseconds if you have a statement though that's taking a long time you would see that reflected in these values and if you can run this query over and over again you would see these numbers continue to increase with each subsequent run so that it could indicate that the statement is not just running a long time but also consuming a lot of resources as it's running on the database server we can see that we are also getting the execution plan back in the query plan field so if we do have a statement running we can look and see what is doing inside of the sql server and then we also have the blocking session information but this will only be populated if indeed our statement is being blocked so let's go ahead and run this query and see what we get so here we see that on my system at the moment that i run this query we have few statements coming back on a real production system you would probably see more statements in more variety than what we are seeing here by just running locally on my laptop we see all of the fields that we talk about so if we want the sql it's right there and then we also see some statistics what i want to show you though is if i move over here to the right we have the execution plan labeled in this query plan field and i can just click on this so management studio popups the execution plan that's being used by the statement and that's pretty useful because we can see the actual execution plan that the statement is running especially we have a performance problem in a production environment because this can clue us in about what's actually happening up on our database server it's useful to know what's running right now but other times we want to be able to get a bigger picture view of performance and what statements are performing well and which ones aren't so we will talk about the query that will do that for us in the next video you are watching the video of module 4 finding performance bottlenecks in sql server part 4 finding the slowest and most expensive sql statement one of the most useful thing you can do with the sql server dms is to have sql server gives you a list of all the statements that your application has run along with the execution statistics for those statement if you are looking for some quick bins for performance in your application this query is a great place to start because this immediately tells you what statements are performing well and what statements you need to target for performance tuning so how does sql server know this every time sql server processes a statement it keeps tracks of the execution plan that it uses and some execution statistics about the statement and store this data in an inmemory structure inside of sql server since this data is in memory what data inside the structure can and does change as new statements are run and old statements are cached out but generally you will have the last several hours worth of data available in the dmv that shows you this view and this is plenty of data to get a strong pulse about what is happening with your application so let's look at this query this query is based around the dm exec query stats view and you can see that we are also pulling in the text of the sql statement and the execution plan for the statement this is handy to pull in the execution plan because if you do find a poorly performing statement we can look at the plan being used and use the skills we have learned in the last couple of modules to analyze the plan for the statement if we look back up towards the top of the editor window we see the data that we are returning we see that we have the sql text like before so we know what statements we are talking about and then we have a number of stats about this statement like the number of times that it's been executed the average amount of cpu per execution and the total amount of cpu across all executions and we have similar measures for both logical i o and alph stamp one thing i do want to point out is that the total rows column here this was added in sql server 2008 r2 so if you are still on sql server 2008 you will want to comment this line out what is useful to do though with this query is that we can take the result of the query and we can sort those result by various parameters with this query right now at the moment i am actually sorting by the average alerts time so that's going to be statements that taking the longest however i could also sort this by the average amount of cpu time per statement execution or the average amount of io to find the most resource hungry statements if you can also be really interested to sort the results by the number of execution so you can see what statements from your applications are getting run most frequently if you see a statement that has a very high number of executions maybe that statement is being executed inside of a loop and you want to rethink how you are doing that data access or maybe the data returned by that statement is essentially static so you might want to consider if you could possibly catch that data so like all the queries that i am showing you in this module take a moment and experiment with these queries in your database because that's really the best way to learn how to use this query and what they can tell you so let's go ahead and run this query so this is what your data is going to look like when it come back and this is pretty straightforward to interpret this data one thing that i want to point out is our dime based measurements which are the cpu time and the ellipse time column this units are in microseconds so we move the decimal places over 6 places to the left we can see that i have got a number of statements here that are taking over 5 seconds and it look like those statements are also are using a lot of cpu so if this was a real database i did be pulling the sql out of here and then i will be scrolling over here to the right where i can see the execution plan for the statements to analyze what is going on so you can see this query is very useful because it can help you immediately pinpoint any statement that are really slowing down your application so you can get those fixed even after i have tuned all of these statements in my application i still like to run this query every week or so just to make sure that nothing has changed or snug up on me in terms of a performance issue next we are going to take a look at how sql server can give us some recommendation on if any indexes are missing in our database you are watching the video of module 4 finding performance bottlenecks in sql server part 5 getting sql servers recommendation on missing indexes we have seen in this course that when we run a sql statement in management studio if sql server thinks the statement would benefit from an index it will give us that recommendation right in management studio sql server doesn't just make this recommendation in management studio but anything a sql statement is run these recommendations are logged to a series of dmvs known as the missing index dmes we don't want to automatically create every index that is recommended in this series of views but looking at this result can help us finding an index that we may have otherwise overlooked the query you see on your screen will join these views together and give us some statistics about the recommendations so let's go ahead and run this query to see what we get the first column in the result set is the table name and this is the table that the index recommendation is for then we have three columns contain information about what columns a potential index would be created across and we see the column names are given to us in a comma separated list within each of these columns mainly you are going to be looking at the column called equality columns because most where clauses and join clauses are based on the equality relationship we also see that we have some stats about why sql server thinks this index would be a good idea this two column here user scans and user seeks represent the number of times that sql server could have used this index had this index exist so for this top row this means that sql server could have performed an index seek operation against this index over 12 000 times had the index existed we also see the average cost of the statements that could have used the index was and this is the cost of the statements without the index existing the next column avg user imp tells us the percentage that sql server thinks that this cost would have reduced if we have had this index finally the last two columns are some simple calculations the first column give us the average cost saving per statement we would have by having this index and the last column give us the total saving we would have across all statements that could benefit having this index so this last value is useful to sort these results to get an idea what the most impactful index to create might be now when you run this query and pull this list up what you don't want to do is just start working your way down the list and creating an index for each row in this result set this is because sql server has a tendency to recommend an index to optimize every individual statement and if you did that you would be in a over index situation which would slow down all your dml statements against your table instead what you want to do is to scan through the result for each table you will able to pick out some pattern of index recommendation across similar columns and then you can combine the information with your knowledge of how your application works to come up with the right sets of index for each table so for example this last 5 recommendations that are in the list are all about the student table and by scanning through the columns that are recommended we see that all of the recommendation includes the last name column so we know that we need to have an index that start with last name we also see that first name is used in three of the recommendation so that is good choice for the second column in our index and then maybe we will choose state as a third column because that's used two of the recommendation as well the point is that you want to look at the different recommendations for a table and extract out the common columns such that you are creating the fewest number of possible indexes on the table in a lot of cases you will have a number of similar recommendations that all have a common subset of columns so you want to create just one index that contains those common columns this way you are creating an index that can benefit multiple statements rather than trying to create individual index for each statement which would be inefficient so when you look at the results of this query keep in mind that these are just recommendations and it's up to you to analyze and combine this recommendation together into the actual indexes that you are going to want to create on your tables now the flip side of missing an index is having index on table that aren't being used so we will look at a dmv query that can help us finding those situation in the next video you are watching the video of module 4 finding performance button lag in sql server part 6 finding indexes that are not being used in the last module we talked about how sql server has to maintain any index you create on your tables by updating an index every time a dml statement is executed against the table so while an index will greatly speed up a query there is a slight performance penalty in terms of executing a dms statements against the table therefore we want to make sure our application is using all of the indexes we have created in our database because otherwise we are paying the cost of maintaining an index which is not being used or not giving any benefit and we can do this with a query that looks like what you are seeing on your screen which uses the dm db index usage stats dmv in conjunction with the sys indexes system views i will run this query and then we can discuss how it works so we see in the first few columns we have information like the table name the index name and the index type remember that an index type of cluster really means a table data as this is how sql server normally stores the data for a table in a cluster index structure what is really of interest to us though are these statistics that are located out at the right of the result set and i am going to expand this column just a little bit so we can read this a little bit better and there we go so user seeks scan and lookups each represent the number of times respectively that the index has been used of those types of operations since sql server was last restarted so for a standard noncluster index that we would evaluate we would be most interested in the value of user 6 as this is the number of times that sql server is using the index as intended by searching the tree structure of the index for matching keys we also see that we have a column named user updates and this value reflects the number of times that sql server has had to update the index because of a dml statement being run against the table now when you run these statements on your database you aren't going to see all zeroes in this columns like i am but you will see actual values what you are looking for here is indexes that have high number of updates and very low usages generally meaning a low number of user seeks operation this means that we are having to update the index more often than its being used so if we find one of these situations we want to investigate why this is happening maybe the index has a wrong column order or it isn't selective enough or maybe something has changed in our application such that we are not querying a table in the same way that we are used to whatever the case you want to understand the situation and see if you can possibly modify that index so it can be used or if you determine that the index really doesn't have a purpose then you want to drop the index so it isn't slowing down your dms statements i do want to give you one word of caution though sometimes you have an index that isn't used by your website or other interactive applications but it is critical to a night batch process in this case you will have only a handful of usages of the index because of course a nightly batch process usually runs just once per night but you don't want to drop these indexes because otherwise there will be serious performance consequences for those batch processes so just make sure that when you are getting ready to drop an index you have really thought through all of the processes that may be using the underlying table and make sure you have not missed an important use case like a nightly or weekly batch process let's go ahead and wrap up our discussion of dmvs and this module in the next video you are watching the video of module 4 finding performance bottlenecks in sql server part 7 module summary in this module we have seen that sql server collects all types of diagnosis and performance information and makes it available to us via its dynamic management views by queering these views we can get an overall picture of what is happening with our application inside of sql server so we can quickly identify problem areas and address them we looked at five different queries that we could use and these queries gave us information on what client had sessions connected to sql server what sql statements are currently executing what statements are taking the longest to run and using the most resources inside the sql server what indexes might be missing in our database and what the current usage of our existing indexes is there are many more queries that could be written and of course the queries we have shown here would all be modified really the best way to learn about dmvs is to try them out on your system and you will be sure to learn something about your application data access layer in the next module we are going to continue talking a more global view of how our application interacts with sql server and demonstrate how you can trace all the sql statements that your application sends to sql server and at the same time collect detailed performance data about them so let's start it from the next video you are watching the video of module 5 capturing what your application is doing inside sql server part 1 module introduction in this module we are going to talk about the ways you can trace all your data access statements inside of sql server and by trace i mean capture each statement your application run inside of sql server along with some performance statistics about that statement why would we want to do this well there are couple of common reasons one as a developer we might want to know exactly what a process inside one of our applications is doing inside of sql server maybe we are trying to debug a process that doesn't seem to be working correctly or maybe we have taken over support of an existing application what tracing allows us to do is to see the exact sql statements the application is running with their values and the order in which they are being run in so we can tell exactly what table are being hit and how they are being used which is really useful for debugging second when we use the tracing capabilities that are built into sql server we can get detailed performance information for each statement that is being executed from our application in the last module we saw how we can get aggregate statistics like average cpu time or average logical reads by querying sql servers dynamic management views as we will see when we trace our data access in sql server we will be able to get statement level statistics for each statement that is run so we can really understand in detail how each statement is performing so we might use tracing in one of our development environments to understand how one of our applications is working or we might use it in a test environment during a load test of our application together detailed information about what parts of our data access are performing and which parts needs to be tuned and finally we could even use a trace in our production environment if we wanted to capture detailed information about every statement that took over a certain amount of time let's say 5 second or accelerate some other performance threshold in sql server there are two different ways which you can trace what is happening inside of the database and which one you choose depends on the version of sql server that you are running if you are running sql server 2008 r2 or earlier you should use a tool called sql profiler and this is provided as a separate gui in the sql server client tools if you are running sql server 2012 or later or using sql azure then you are going to use what are known as sql server extended events to perform your trace going forward sql server extended events are going to be standard and in fact the only way to perform a trace in sql server as the sql profiler tool was depreciated in sql server 2012 you can check out this on microsoft site with the link shown on screen but you will still find sql profiler widely used so we will cover it but known that the trend is moving towards extended events so if you are on sql server 2012 or later you want to be investing your time in learning how to use extended events and probably skip learning profiler regardless of which approach you end up taking running a trace on sql server requires some pretty high level permission in the database which we will discuss when we demonstrate each approach this is because a trace can capture every sql statements and it value that are being executed against the database and clearly there are some security concern in letting someone do this if you have sql server installed on your workstation you will be able to run a trace for any local testing that you do against that local instance for your development and test environments if you will be able to run a trace is going to be vary by company in production you will almost certainly not have permission to run a trace by yourself and will need to engage someone from your dba team to help you run a trace however don't be discouraged by this quite the opposite being able to trace all of your applications sql is a powerful capability and it is important that you have a developer be aware of its capability and what it can do so that when you need to do it you know that the capability exists and you can ask for it so what we are going to do in this module is guide you through using both sql profiler and sql server extended events to take a trace for each method i will show how to set up a trace including what events you can log how you can filter the data you capture and how you log this data to a file for later review i will also discuss what you need to do in order to analyze this data when we are finished you will have a good overview of how to set up a trace in both tools and a good idea of what data you can capture within sql server this will help you understand what tracing capabilities exist in sql server so you know when a trace might be useful in your application development process so let's get started by looking at the sql profiler tool in the next video you are watching the video module 5 capturing what your application is doing inside sql server part 2 setting up of sql profiler trace to run sql profiler you have to go to your microsoft sql server tools in your start menu here you can see on must install it is labeled sql server profiler 18 and you just click on this icon you can also start sql server profiler from sql server management studio for that click on tools menu then the first menu is sql server profiler once you click on the sql server profiler menu the app will open and this is what the screen will look like you want to go up here to the left upper corner and click on the icon that is all the way on the left to start a new dress then as we see here we have a login dialog box popup and what we need to do here is to login to the sql server instance where we want to run the sql trace on also we need to log into sql server as a user that has the altered trace permission otherwise when we click connect here we are going to get an error in this case i am just running a trace on my local sql server instance so i will have a permission to do this but as we talked about in the intro of the module you may or may not have rights to run a trace against different sql server at your company so i will click connect here and then we will presented with this dialog box where we can specify the various options for the trace that we are going to perform so first we will specify a name and then we don't have to specify an output file where we want this data to go but it's a generally a good idea so you can review the data later so we are going to check this box to save to a file and we will specify a file name here and then we get this option to specify a maximum file size which i am going to set 25 mb then i also want to make sure that file rollover is set so as this files fill up profiler with roll them over to a new file now what i want to do is to set the events i want to capture and some filter criteria about what to capture and i do that on this second tab called event selection you can see we already have some default selected and from a developer perspective wanting to trace some sql statements and get their performance information these are actually some pretty good defaults these names are pretty selfexplanatory but if you mouse over one of them you do get some additional description down here in the lower part of the dialog the rpc completed event will fire when a stored procedure completes and the sql batch complete event fires when individual statements completes so you want to make sure to have both of this event checked so you can capture all the sql from your application this is important even if you are not using stored procedure explicitly because the way that some of the database drivers work they actually in some cases will wrap your sql inside of the sp execute sql build in procedure so you just want to make sure that both of these boxes are checked also you want to make sure to check the box labeled text data here on rpc completed because this is what is going to let us see the sql that's been executed as part of that event if you can see that we have a number of other parameters here like cpu and logical reads and i suggest you to leave all of this checked so you get this performance information now there are many other events that you can capture and to see all of these events you check this box here named show all events as you can see this list is quite extensive though most of the items in this list are of more interest to dbs than to us as developers i am not going to go through each and every one of these obviously but there is one that i want to point out and that is in the error and warnings section and the event name is user error messages if you have a sequel statement in your application that throws an error when it runs that error will be captured by this event so this can be useful debugging tool if that error isn't otherwise getting bubbled up to your application logs this event will also log some informational messages that sql server generates but for our purposes we can disregard those informational messages because what we are really interested in for this event is the ability to capture errors from our sql statements i am going to uncheck the show all events check box and you see this gets up back to just the event that we are going to capture and now we want to talk about this other button here column filters the way we have things right now we are going to capture every login logout and sql statements that executed across the entire sql server instance if you are running a trace against your local install of sql server that will probably work fine because you are probably the only user but on a busy server you are going to capture a lot of events that you don't care about this not only makes it harder to sort through all of the data and find what is important to you but it can also have a negative impact on the performance of sql server because you are capturing so many events so we want to filter the data so that we just capturing the data that's of interest to us one of the things you probably want to do is to limit the data you collect to just your application you can do this using the application name filter or by the login name filter assuming that your application uses a unique login for sql server if you go to the application name root remember you need to set this application name in the connection string your application uses in dotnet this is done by including the application name parameter in the connection string and something similar can be done in other languages as well you can also limit your trace to statements that takes a longer time or use a lot of resources with columns like duration cpu or writes to set one of these just click on it and then go over here to the criteria you want to set and in this case i want queries that takes longer than 5 seconds so that's 5000 milliseconds and set the value and now this trace would capture just this long running queries if you want to get rid of this criteria which i do for the demo that i am going to run in just a second here then you just double click on the values so that you can edit the values and delete it and now that criteria will not be used anymore so now we have our trace setup and in the next video we will continue with this and go ahead and run this trace and collect some data you are watching the video module 5 capturing what your application is doing inside sql server part 3 running a sequel profile address so we got our sql profiler trace setup in the last segment and now all we have to do to run the trace is to click this button here that says run now in the background you should know i do have a program running that's generating some synthetic load against my sql server instance so we will be able to see something when we actually run this trace so i will go ahead and click this button and there we go we see this window popup and we can see the events that are being captured by the trace in this window now running the trace interactively like this is fine if you are on a local machine like i am or otherwise maybe a dev server that has a very very light load on it but you don't want to run a trace interactively like i am doing against a busy server because otherwise you could really cause some performance degradation on that server there is a way to convert our setup here to run what is called a server side trace that runs just up on the server itself and captures its data to a file and that's more efficient and i will show you how to do that in just a moment but for now know that this interactive mode is something that you only want to be doing locally or maybe you do it for just a very brief instance of time on a server to make sure you are capturing the right data and then you turn it off that way you can avoid any performance impact i am going to actually go ahead and stop this trace because i think we have captured enough data to review in the time that i have been talking and so to stop the trace we just hit this stop button here in the main pane we see the event that are captured by the stress in the order they occurred if i click one of these i will see the text of the command down here in the lower pane with the values that were submitting for this query i can also see up here in the main grid some of the performance stats about this statement like cpu reads rights and duration so what i can do is i can scroll through here and i can look for statements that were the most expensive ones and then inspect the sql for that statement and any parameters that were used in that statement what is lacking about this user interface is that there is no way to sort or filter the data once it's been taken so you can see i can go up here and click on the column header and nothing happens i am not getting the data to sort here so if you want to be able to slice and dice this data what you are going to do is to save this data to a database table and then you can query it with just some normal sql like any other table so to do this you are going to go up here to the file menu and then down to save as and then select trace table this will prompt you for a database login and what you are going to want is a place and a login that profiler is going to be able to create a table to store this trace data in if that table doesn't already exist and so what i have done on my machine here is i have created a separate trace database called trace data on my local sql server instance and that's where i am going to put this data so i will go ahead and login and then i am going to find trace data in this list for my databases i will leave the schema as dbo and finally i am going to give this table a name and now i will click ok and so that's going to do is to take this data and create that table and insert all of the data into the table and so now if i pop up over to management studio i can go and i can find the database and i will open the tables and there is our table and i will just right click so that i can grab the first thousand rows and there those rows are so obviously i could write sql queries against this table like any other table and i could filter and sort this data however i wanted to now that i have it in a table we mentioned that you could also run a trace as a serverside trace which is more efficient and therefore more appropriate if we are trying to trace access in a server environment rather than on our local machine so let's see how to do that in the next video you are watching the video module 5 capturing what your application is doing inside sql server part 4 running a trace as a serverside trace when you use sql profiler to trace your sql statements what you really want to do is to run your trace as a serverside trace rather than a interactive mode because a serverside trace will consume fewer resources in sql server so what we do is use the sql profiler tool to setup the trace meaning we select the events we want and add in any filters and then we can instruct profiler to give us a sql file of the commands that need to be run in order to run this trace on the server side so let's see how to do this i have the same trace here that we have been using throughout and we have this all set up and then all we need to do is go up here to file export and script trace definition then we will choose from sql server 2005 to 2019 and we are just going to save the sql file somewhere on our local machine so now what we need to do is we need to open that sql file in management studio and i will jump over to management studio and i actually have a version of this file that i saved earlier already opened up here in the window that we can see we see that all the sql file is a series of commands that we are going to run on sql server the only thing that we need to do is to put in a file name of where the data is going to be logged to and we see that we need to do that right here and there is nice long command telling us exactly what we need to do the file path is going to be the path that is going to be up on a sql server so that need to be a directory that exists on a machine that hosting sql server in my case i am running sql server locally so the client and server are one of the same but just remember that part is up on the server so i will go ahead and set this and then we will look through the rest of the file here just to see what else is in here and what all these commands do is they set up the events that we want to capture and any filters so sql profiler has generated all of the correct sp trace set event commands for us so we don't have to look up all of these different codes and ids so all we have to do to start the trace now is to run this script again you will need to be logged in as a user with the altered trace permission in order to run this trace otherwise you are going to get an error the user i am logged in as does have this permission so i am going to go ahead and get this trace started and so now this trace will be running up on the server and collecting data and logging that data to the output file that we specified one important thing to note here is the id number for the stress that you get back down here in the result pane because this is what we are going to need to stop the trace once we have captured all of the data that we need so let's talk next about how we manage this trace so first of all you might want a status on what traces are running on sql server or maybe you forgot that number of the trace that we have just started so let's see how we can find that information out i will go over here to this other window where i have a couple of queries and what you want to do is you want to query the system function fn trace get info and by passing a 0 to this function you will get back information on all traces that currently exist in sql server so i am going to go ahead and run this statement and you see we get this information back but this isn't a very user friendly format trace number one is the system default trace that's always running and then we see the information we have on your trace and we can convert it out our trace id the file and status information from this but it's not very user friendly so if we run this second query here this will give us a little bit friendlier view of the data so let's go ahead and run that and you see that's the same information it is just in a little bit better format for us to consume so now we have our trace that's been running for a while we have collected the information that we need and we want to stop this trace and so how do we do that well what we do is we run a stored procedure name sp trace set status and i have the various use cases of that stored procedure over in this third window so as you can see here you pass in your trace id and a value of 0 if you want to stop the trace if after some time you want to restart the trace you would pass in the trace id and a value of 1 to restart that trace so then after you have stopped the trace if you want to remove the definition of the trace from sql server you would call this procedure again with the value of 2 and what that does is it just remove the definition of the trace in sql server it doesn't go out and delete any trace file that was generated whenever sql server restarts all these definitions get removed this is just a way that we can clean up things when we are finished and keep our sql server tidy so i am going to go ahead and i am going to stop this trace and so at this point we have covered what you need to know about how to run a server side trace now once one of these traces has been taken either by you or maybe somebody on your dba team and you have this dot trc file of the trace data how do you view the data that's in this file that's pretty easy you just open them up in sql profiler so i am going to go back to sql profiler and i will select file open and then trace file and then i just find the trace file i want to open i select and say open in the dialog box and now that file will open up in profiler from here you can review the file in profiler or use profiler to load the data into a table in sql server whatever meets your needs this wraps up our discussion of sql profiler so now we are going to move on and talk about sql server extended events which for our purpose do much the same thing but is really the preferred tool on new versions of sql server for taking traces like this and also gives us the capability to trace sql statements on sql azure also so we will take a look at extended events in the next video you are watching the video of module 5 capturing what your application is doing inside sql server part 5 introduction to using extended events for sql tracing if you are using sql server 2012 or later or using sql azure then extended events is the way to go in terms of tracing what sql is running inside of your database sql server extended events use the event tracing for windows or etw framework to trace data and this framework is newer and more efficient in terms of server resources than the older tracing framework using sql profiler extended events also gives you access to many more events within sql server that can be traced and logged and many more options in terms of filtering those events down to only the ones that you want so you want to make sure that you are familiar with using extended events for tracing because this is the direction that microsoft has chosen and will be the only option available on future version of sql server to run an extended event trace the user running the trace is going to need some permission in order to setup and start the trace extended events was actually first introduced in sql server 2008 r2 and if you are on sql server 2008 r2 then your user will need the control server permission granted to it on sql server 2012 or later you will need the alter any event session permission in order to define and run an extended event trace so this is a much more targeted permission however having only alter any event session will only allow the user to create an extended event trace through sql if you want to be able to use the gui within sql server management studio in order to define your extended events capture session as we are going to do here you also need the view server state permission for your user if you will probably have this permission on your local instance of sql server that's running on your workstation and it may be possible to get these access rights in your dev and potentially your test environments in production at a lot of companies you will probably be asking your dba group to define and run these stresses for you because of security concerns around production data in servers but again these tools can be very useful in your development and application support process so we will introduce these tools here so you are familiar with what capabilities exist and you are aware of all the tools that can help you in diagnosting and solving problems within sql server in terms of setting up extended event session to capture events for the onpremises version of sql server and for sql azure about 90 percent of the steps that you need to perform are common between the two platforms there are some small differences though so what i am going to do over the next few clips show you how you would set up extended events to trace your sql on an onpremises version of sql server so let's jump in and create new extended event session to capture some of the sql inside of sql server in the next video you are watching the video of module 5 capturing what your application is doing inside sql server part 6 setting up an extended event trace session to set up an extended event session we use sql server management studio what you want to do is in your object explorer go down and open up the management folder then you will see extended events under it so expand that and finally expand sessions and you will see the existing extended event trace sessions defined on your sql server to define a new session right click on the session folder and you see you have two choices here new session wizard and new session and i suggest you select the second choice new session because this is pretty straight forward to do and there is not really a need for a wizard once we do that we will get this window to pop up and if we look on the left we have this four pages of configuration that we will go through to define this capture session so we will start out here on this general page first we want to give our session a name and it is good to make this name descriptive because this is what's going to show up in the list of management studio and it will help identify this configuration in the future so i will put in a name and then we have this drop down of template we can choose from and we can see those template and in this case i am going to choose the one named query batch tracking once you select a template you will see that sql server does gives us a good description of each one of this template all these templates do is preselect some of the events that we can choose from on the next page so they make setting up your trace a little bit faster for us as developers this query batch tracking is a really good starting point and then if we wish we can add more events on the next page finally on this screen we have some options that we can start the trace at server startup or as soon as we create this trace and i am going to leave each one of these boxes unchecked as we will just start our session manually when we are ready to the next phase that we want to look at is the event page so i will click on that and on this page the events that are currently selected to be captured are over here on the right in this list and then this main section in the middle of the page is where we can search for and select any other events of interest you can search for event with the event search box so i am actually going to do that for an event and we see the list is now filtered by my search criteria i am going to click on this event query post compilation show plan and when i do notice that these two controls on the bottom populated given me a description of what this event is and the data fields that this event will capture for this particular event if i scroll through the description i see that sql server is warning me that collecting this event can be very expensive so if i do choose to turn this event on i want to be mindful of that and only collect data for a brief period of time as not to impact the rest of my sql server so that's where the description field can be really useful because sql server lets us know if we are collecting a high resource usage event if we did want to add this or any other event to be collected as part of this trace session we just use these two buttons here to manage what events were selected once we have the list of events then we have to capture we can configure what data we want collected for each of these events and to do that we click on this configuration button that is right here that button take us to this screen where we have our selected events on the left side now and the configuration for each event is over here on the right in terms of the data that you collect for each event that is divided into two sections fields that are globally available to all events which is on this first tab here and event specific fields which are out here on the third tab so for the rpc complete events we see we are always going to collect performance data like the amount of cpu time duration and logical reads and that is good because those are fields of interest to us and also on the rpc completed event you just wanted to make sure that the statement field is checked because this will allow you to see first of all the statement and also the parameter that were passed either to the stored procedure or the parameterized query over on the global field tab there is one field you want to make sure and select and that is the sql text field the reason why you want to make sure this field is selected is because this field gives you just the sql text of the sql statement without the parameter values and we will see in a moment that allow us to group all of the various executions of a particular statement together when we start analyzing our data so you want to make sure that the sql text field is selected and we want to do this both for the rpc completed event and then also for the sql batch completed event and then you can just look through the list of the rest of the global fields and see if there are any that make sense for the trace that you are putting together for example you might want to capture fields like client host name or username to know what application ran a particular statement the last bit of configuration to do on this screen is to configure the filters for the event and we want to do this so we can focus on the data that we are really interested in and just collected the data otherwise we are going to have a lot of extra data that we are going to have to sort through and by focusing in on just the data we are interested in we can lighten the footprint of any additional load that we are putting on sql server to do this we use the filter tab and again we are going to need to do this for each event type that we are capturing we see that we already have a couple of default filters in here and what this filter will do is keep our session from capturing any internal system activity run internally by sql server since that activity is probably not of interest to us what we do want to do though is to limit our session to only capture data from our application so there are a couple of ways that we can do that you just click here to add a clause and now i will click on this middle box and you see all of the criteria that i can filter by so if i am on a shared sql server box that has many many databases i could come down here to sql server database name and now i will enter the value of students in the value field and i will only capture this event sql batch completed in the students database so that will effectively limit me to only sql statements that are run in the database there are many criterias in this list as you see so you could only capture statements that took a certain amount of time or longer or statements that took a certain amount of cpu and this could be useful if you are trying to identify what the real long running statement in your application were you could also put a filter on the sql text field and then there is like operator available so you could use this to capture all of the statements then run against a certain table in your database i will set up the filter for the other two event types offline so we can move on to the last item of configuration we are going to do and that is where we are going to store the data that we capture as part of this extended event session trace we configure that on the data storage page so i will click over there and you can see by default the storage is of type ring buffer and what this is as in memory data structure that acts like a queue so in this case you see we will keep just the last 1000 items we could use this but what you probably really want to do is to process this data you capture to a file so let's do that so i will remove this ring buffer by clicking on the remove button now i will click add and i will choose a type and what i want is an event file and now i will go down here and i will select the name and location for a file and now i am going to say i want a max file of 250 mb for my file and now this data will be captured in the file which is going to be more useful for us the last page of configuration is the advanced page but we will be in a good shape just to accept the defaults there now to create this extended event session that i have it all configured all i need to do is to hit ok button and this session will be created so in the next video what we are going to do is we are going to run this session and we are going to capture some data on our sql server you are watching the video of module 5 capturing what your application is doing inside sql server part 7 running and configuring the display settings for an extended event trace now that we have our trace defined we just right click on the extended event session and say start session and so now this trace session is running and collecting data to the file that we set up for the session to log data to if we want to stop taking data i just right click again and i would say stop session but i am not going to do that just yet we can see here on our menu that we can also have another option watch live data that we can use while the session is running and we can use this to peek at any data that the session is capturing so if we click on this this is going to bring up this window and now as the session captures data in real time those events are going to get displayed in this window we are still capturing and logging all of these events to a file we set up this just gives us a live view of things up here in the top part of the window we see a list of the captured events and if we click on one of these then we get the details of that event down here in the bottom part of the window now having to click on each and every event to inspect its results and to find the data you are looking for in not real user friendly and you are going to quickly tire off clicking through each one of these events trying to find what you are looking for so lets look next at how we can adjust this view of our data into something that is more useful so let's see how we can make our view of the data a little bit more useful here we can add columns to the upper part of this view and turn it into more of a data grid view in a couple of ways first if we have an event selected we can come down here to the data of the event and just right click on an item and select show column in table and then as we see that column now shows up in our table above the other thing we can do is right click on the table header and say choose columns and now we get this dialog where we can choose the columns that we want to see and the order that we want to see them in so i am going to choose a few columns here and i will change this order up a little bit and now when i click ok we can see that i have more of a grid view here in the top part of the pin and that makes things a lot more useful if you want to turn off this bottom part you just go up here and you click on the button here and that will toggle the bottom part of the display on and off and then once you have a column setup that you like what you want to do is you want to save that setup so you can pull it up again anytime you are viewing extended events either a file of extended events or when you are capturing live events so to do that you come up here to display settings and click save as and we will choose a file name and now i will click save and so now when i go back into management studio and i want to bring up this view up all i have to do is go up here to the display settings say open recent and there is our configuration file and we would just click on this so let's move into analyzing some of the data that we have captured in the next video you are watching the video of module 5 capturing what your application is doing inside sql server part 8 analyzing extended event trace data when an extended event session produces an output file the file produced is an xel file and you can open this file right in the management studio so you just go to the file open file again and then just find the xcl file on your hard drive that you want to open and click open and so there we go we can view our data now i am also going to get my column definitions so i am going to go up and i am going to do that real quick and now we have got a better view of our data if you watch the clips on sql profiler or you view sql profiler you know that profiler was pretty limited in terms of what we could do with the data but for this extended event files we actually can do quite a bit in management studio with this data first of all i can sort the data just by clicking on the column header i can also filter the data right here in the ui by clicking on the filter button that's going to bring up this dialog where we can add filter clauses so you just click to add a clause and then you can select the criteria the operator and the values if i use the arrow key here to cycle through some of these values you can see all the fields that we are collecting for this event we can now filter on so i am going to select cpu time as a filter then i can select greater than equal to and finally i give this filter a value of 2 lakhs because this value is in microseconds so that will be 2 seconds so this will now show us in the ui all the statements that took two seconds or more of cpu time so filter like this can be very useful in order to narrow down the data set and look at just what you need in addition to a filter on something like duration or cpu time we could also create a filter that was only for a certain time frame statements from a certain host or a filter that would look for a certain table name in the statement text for our event so we could see just the statement that run against a single table so this is pretty useful what we can do with filtering right here in management studio now and we don't have to go through the extra step of exporting our data and getting it into excel or something like that we can do all of that right here in addition if we do filter the data down and we want to share just that filter subset of data with some of our colleagues we can go up here to the extended event menu item then down to export to and you can see that we can export this filter view of the data to a separate xcl file a table in sql server or a csv file so that's really useful when you just want to share a subset of data with someone to analyze one of the other features that management studio gives us is that rather than viewing just a list of raw events like we are now we actually can group our events together and generate aggregate statistics on them so let's do that first i am going to aggregate my data together and i am going to do that by the sql text of this statement which is this column sql text remember this was the global field for our events that we made sure to capture when we set up our trace and we did that because this field contains just the sql of the statement or the stored procedure name and not the parameters so now that we can group our statements by that sql so we see that view of the data here and this basically tells us how many times of sql statement or a stored procedure executed while we are running our trace if we want to see the individual statement we just click on the little plus sign and then we see the individual statements under that grouping we can even sort these statements within the group by double clicking the column headers so of course one of the things you want to look here is if you have a statement that's being executed thousands of times more than any other statements and then ask yourself what that is for example maybe a statement is being executed inside of a loop so your application is going to the database over and over and again and that could be an indication that you want to take a look at that piece of code and refactor it the next thing that we can do is we can get some aggregate statistics for each of this statement and we do that by clicking on the aggregation button and now for all of these other fields that we are displaying we see we can get some aggregate statistics like average or sum so for this example i am going to sum up the cpu and the duration and i am going to click ok and now i see i have the summary statistics displayed along with the statements so what this allows you to do is to run a trace which is going to capture data about a specific process and then you could filter down just to that data either at capture time or after the fact in this video using the filter button as we just saw and then for that process that you have the data for you could figure out not only which statements had the highest average cpu usage or the highest duration which would indicate the longest average weight but you can also see that for all the statements that were running by this process or in this window of time that you have captured which statement has the most total cpu or resulted in the most total duration that the application was waiting so you can really zero in on what a process is doing and where the performance bottlenecks are in that process in terms of the data access so we have seen how to use extended events to capture address with the onpremises version of sql server taking the trace in azure is very similar there are just a few differences so we will cover those in the next video you are watching the video of module 5 capturing what your application is doing inside sql server part 9 using extended events in sql azure if you are using sql azure you can still capture what your application is doing inside of sql azure with extended events but there are couple of small differences that you need to account for first of all you are going to want a version of sql server management studio that fully supports sql azure what this means is that you want to get the latest version of management studio and use that to set up your extended event sessions earlier versions of management studio would allow you to connect to a sql azure database and run queries but if you try to create an extended event session you would get an error with the latest version of management studio though the ability is setup and extended event session has been extended to sql azure database as well so you will be able to use the same user interface that we have seen over the last few clips to setup your event capture session in sql azure the second thing you need to do is to set up a storage location of where you can write your extended event trace files to with sql azure being a platform as a service offering you don't have a traditional file system available where you can write files to but what you can do is to write your files to azure storage and this article on microsoft.com tells you exactly how to do that if you read through this article you will see that there are two steps step one is that you run a powershell script to create azure storage container and this part also create the storage access policy that will be used to allow your sql azure instance to be able to write to that container when you run this powershell script you will get an output like i am showing here and it is this information that you will need to feed into the second step of the process step 2 evolves running some transact sql in your sql azure database if you look at the tsql on the microsoft.com page as i am here you see that the sql creates a sample data at the top of the script and then a sample trace at the bottom what is really important though is step number two because this is what gives sql azure the credential it needs to write to the storage location so what is happening here is that you are creating some azure storage putting a security policy on that storage and then giving the credential needed to write to that storage to your sql azure instance the script covered the details of the exact command to do this but big picture that's what's going on once you have the storage setup now we can go into management studio and define an extended event session when you are looking at an azure database you will see that the extended event is under the database itself not out under a management folder like in the on premises version but otherwise we just right click on the sessions folder and say new session just like we did before the selection of the events to capture what fields you want to capture in those events and the setup of any filter is going to be like before the only real difference is that when we get the data storage tab you are going to put the url of your azure storage container in the storage url field with the file name and now this is where your extended event file is going to be created so really the process of creating a trace using extended events on sql azure is very similar to the onpremises version of sql server just make sure that you have the latest version of management studio and get a storage location setup and then you will be able to capture what your application is doing inside of a azure database just like you would be able to do with the onpremises version of sql server let's go ahead and wrap up this module in the next video you are watching the video of module 5 capturing what your application is doing inside sql server part 10 module summary there are many circumstances where we would like to be able to tell exactly what our application are doing inside of sql server and by making use of the tracing capabilities built into sql server we can do that this can really gives us a lot of insight into what our application is doing because we can capture the exact sequence that our sql is being executed in so this is really useful for debugging we can also get detailed statistics on the execution of each statement that is run so we can quickly identify which statements are using the most resources or taking the longest finally we get a good idea of how often our application is running different statements against sql server and sometimes simply knowing how often a statement is run tells you something important about how your application is executing or maybe that there's an opportunity to combine multiple sql statements into single statement if we are on sql server 2008 r2 or earlier we will use the sql profiler tool to set up this trace and then most likely we will export the definition to a trace definition in sql so we can run the trace up on the server since that consume fewer resources for sql server 2012 and beyond and for sql azure we will use sql server extended events and we can create an extended event session right in management studio in both cases we are going to select the types of events that we want to capture there are a wide variety of events that you can capture in the database but for us developers usually we are interested in tracing our sql so the sql batch complete and rpc complete events are usually the starting point of us this means that every time our application runs a sql statement or stored procedure we will be able to capture the text of what was run any parameter used for the execution and some detailed performance information like how long the statement took the number of rows written the amount of cpu and how many reads the statement performed we could also supplement these events with other events of interest to us for example we see here events around the start and completion of individual sql statements and around transactions finally we will set up some appropriate filters so we can capture only the data that we are interested in we might filter by the database name or sql server user to look at just the activity from our application but we might even filters by the text in the sql statement just to identify statements that are running against a certain table having good filters like this means there is less data to sort through and it also helps limit the load that we are putting on sql server once we run our trace we will usually end up with an output file of all the events in it that we captured and we can analyze if you are using sql profiler you can just scan through that file or you can export that data into a table in sql server to do detailed analysis if the output is from an extended event session we saw that you could do a lot of your analysis right in management studio itself what all of this gives us is the capability to see step by step what our application is doing inside of sql server so this really helps us understand what our data access layer is doing and where we can mix improvements in the next video we are going to see our last module of this course that is how we can apply common performance practices so let's get start next you are watching the video of module 6 applying common performance practices part 1 module introduction in the other modules of this course we have concentrated on what is happening inside of sql server in this module our focus is a little bit different in that we are going to go through some common performance practices that you want to apply in your application code first we will talk about why you want to make sure to use parameterize sql when you write your data access code then we will talk about if it really is faster to use stored procedures instead of including your sql directly inside of your application next we will talk about commit frequency and how that can affect the performance of your data access layer then we will talk a little bit about object relational mappers or orms first we will talk about how using an orm can make what is happening inside of sql server a little less visible and what you can do about it and finally we will wrap up by talking about the n plus 1 select problem which is commonly encountered performance problem when using orms but fortunately is easy to resolve so let's get started by talking about why it is important to parameterize the sql in your application you are watching the video of module 6 applying common performance practices part 2 use parameterize sql one practice you want to make sure to adopt to use parameterized sql statements in your data access layer what do you mean by this consider the sql statement you see on your screen the statement is using simple string concatenation to dynamically build the sql statement to be run against the database what this means is that every time this data access method is called with different values a different sql string is going to be generated and then sent to sql server to be executed contrast that without data access method that is using parameters like the one you see now on your screen these two values with the address sign in your statement signify sql parameters to sql server and then down here we attach the values that we want to use for those parameters when we submit the statement to the database so every time this method is run the same sql is sent to the sql server and it's just the parameter values that are attached outside of the sql tags that will vary in sql server we will then be able to substitute those parameters in up on the server itself there are big two advantages to this approach of using parameters first this helps prevent sql injection attacks because any value passed in this way are automatically escaped by the database so it doesn't matter if a malicious user has included some code characters or other special characters in here to try and do something inferior those characters are going to be automatically escaped in our statement and that goes a long way into feeding sql injection attacks like this the automatic escaping of these strings also means that you don't have to do anything special in your code when a user wants to legitimately include a single code character in the value they are entering like when they are typing the name o corner or orale secondly using a parameterized sql statement like this will also perform faster and use fewer resources on sql server why is this remember that every time a sql statement is sent to sql server sql server has to determine an execution plan of how to execute that statement what actually happens is that sql server looks in an area of memory known as the plan cache to see if it has already executed the same statement before and therefore already has an execution plan that can be used if yes then sql server can just use this cache execution plan if not sql server will have to do additional work to come up with an execution plan either through a process known as a simple parameterization or through generating a complete new execution plan if you are using parameterized sql statements then once that statement is run the first time every subsequent execution of that statement is going to be able to use the same cached execution plan even when different values are included with a new statement execution if you are dynamically generating your sql statements like we saw in the string concatenation example the actual sql is different each time so sql server has to perform this additional work to get an execution plan each time and that additional work will slow down your application but maybe more importantly make it less scalable the code that you saw earlier in this clip was part of the simple test application that i wrote that run the same sql statement several thousand times with different values and you see the result of the test on your screen the test took 8.5 seconds of elapsed time to run almost 7 seconds of cpu time and we use 79 mb up in sql to store all those different execution plans using parameterized sql statements all of those same sql statement executed in under 1 second and we used almost no cpu time on the database server since we are using the same plan over and over again and as you can see we just use 104 kb of memory in the database to cache our execution plan so we see that using parameterize sql not just makes our statement execute faster but also consumes significant fewer resources on our database server and this is a big deal because most database servers are pretty busy supporting many many concurrent users so being efficient about resource consumption is important if you are using an orm like entity framework entity framework core or hibernate these tools should already be parameterize your sql for you but if you are writing your data access layer using something like ado.net or jdbc or even using a micro orm where you are still writing the sql statements directly make sure that you are parameterizing your sql statements this will provide a performance boost for your application as well as help to keep you safe from sql injection attacks after watching this video you might ask that are store procedure faster than sql in application code so we are going to see that in the next video you are watching the video module 6 applying common performance practices part 3 r stored procedures faster than sql in application code if you are being doing database development for any period of time you have probably heard someone say that you should use stored procedure for your data access layer because stored procedures offers better performance than having the sql in your application code this is one of those statements that's been around a long time and it is partially correct however these statements alone does not tells the full story so let's explain what is going on here in the last section we compared the performance of dynamically generated sql statements to that of parameterize sql and we saw that parameterized sql was not just faster but also used less cpu and memory on the database server the main reason for this is that when using dynamic sql sql server has to do additional work to parse each unique sql statement and come up with an execution plan whereas in the parameterized sql case we don't need to parse the sql text each time and can use the same plan over and over again when people say using stored procedure offers a performance advantage what they are referring to is that using stored procedure will outperform dynamically generated sql from a performance standpoint stored procedure are very similar to parameterized sql statements you pass in some values to a stored procedure these are attached to a statement or statements within the stored procedure that needed them but effectively sql server has already passed the procedure and has an execution plan so you are getting the same benefit as in the parameter sql case when you compare the performance of using stored procedure to using parameterized sql statements the performance is essentially the same there is effectively no difference between the two now this doesn't mean that there aren't good reasons to use stored procedures there are especially when it comes to security and restricting excess of data but from a performance point of view parameterized sql statements and stored procedure perform about the same so the next time you hear the statement that stored procedures offer better performance you will understand this statement comes with a qualifier stored procedures do perform better than dynamic sql but as we see in the last clip we don't want to be writing dynamic sql anyway otherwise stored procession and parameter sequence statements offer the same benefits in terms of performance so as long as you choose one of these strategies you will be fine from a performance perspective in the next video we are going to take a look at commit behavior and how this can affect the performance of your data access code you are watching the video of module 6 applying common performance practices part 4 commit behavior and performance when you run an insert update or delete statement from your application against sql server an implicit commit is issued for each statement because by default the sql server driver is set with it auto commit behavior turned on this is true not just for the dotnet data provider but also for jdbc oledb and odbc as well so consider the code sample that you see on your screen what i am doing is adding all of the courses a student wants to enroll in for a semester and these environments are in a list and the typical student will have five or six courses that they want to enroll in so some pretty simple data access code we just iterate over each time in the list and run our insert command however because of the default auto commit behavior of the sql server driver when this code is run there will be an implicit data comment done each time one of this statement is executed what this means is that up on the sql server sql server has to write an entry to the transaction log to make sure there is a record of the dml operation being performed so there is a write to the transaction log incurred each time one of this statement is executed what we really intended though with this code is probably to insert all of the records at once for the student or none of the records at all so this work should be performed inside of a transaction this is a pretty common pattern to use a transaction like this another example being an ecommerce site inserting items with an order and we want all of those inserted to succeed or none of them so our database is in consistent state so to accomplish this we are going to create a transaction here then we will pass the transaction to the constructor of the sql command to attach it to the command and finally when all the items are inserted we commit to the transaction so this makes much more business sense but as it turns out it also performs better because now we are committing once per student rather than once per enrollment record so sql server doesn't have to write to the transaction log as often i have some results of when i run this program before i started recording that i can show you and you can see here that using a first method when we had to the default autonomic behavior on it took a little bit more than 30 seconds to insert one leg 65 000 rows when i used a transaction which is also more correct from a business sense it took a little more than 23 seconds so about 7 seconds faster on those same 1 lakh 65 000 rows now these numbers of course aren't absolutes and you will likely get somewhat different results on your system but the point is that we want to pay attention to our commit behavior because if we are committing after every dml statement we execute and we really don't need to we are imposing more of a load on sql server and that's going to slow things down so how often should you commit as often as your business transaction dictates and in some cases that may very well be after every statement if not though just make sure that you are explicitly using a transaction so you are controlling the commit behavior and not using the default auto commit behavior of the database driver if you are using an orm most of the time your orm should be handling this property for you however there are some other issues you want to watch out when you're using an orm so we will talk about those in the next video you are watching the video module 6 applying common performance practices part 5 object relational mapper just generates sql over the last few years object relational mappers have become very popular in the development community and it's easy to see why orms allow us to work at higher level of abstraction and they also eliminate the need to write a lot of repetitive data access code to get data into and out of our database thereby helping to increase our productivity as developers on the downside removing us from having to work directly with the database gives us less visibility and less control into how our data access code is executed and so one of the side effects of this is that if you just leave your data access up to the orm and don't think about how that data access is performed you can get very poor performance let's see what we mean by this i have a very simple application here that is using entity framework and the intention here is to search all of the current application who have a gpa over 3.5 and the math sat score of over 700 the code here is perfectly valid and will return the desired results the issue is that in this case there is not an index on any of these columns so this query will end up performing as full table scan which in this case will not result in very good performance having an iq variable we have a fluent interface that is very flexible and it becomes easy to just think about attaching the necessary expression here in code and not to think what needs to happen when the expression is converted to sql and sent to the database so we just need to keep in mind as we use our orm ultimately this expression will turn into sql and all of the rules we have talked about in this course like being properly indexed and having a query with a selective where clause still apply if you have any question about exactly what is happening you can get the actual sequel that is being generated by the orm by tracing your application as we showed in the last module or by using some of the client clientside tracing techniques that are present in most orms then once you have the sql you can generate an execution plan for the statement and apply all other techniques we have learned throughout the course to make sure the statement performs the way you need to it then once you have the sql you can generate an execution plan for the statement and apply all the other techniques we have learned throughout the course to make sure that statement performs the way you need it to in the next video we will look at another issue that frequently occurs with orm and also occurs in other scenarios and that is the n plus 1 query issue you are watching the video module 6 applying common performance practices part 6 solving the n plus 1 selects problem another performance pitfall you want to be on the lookout for especially when using an orm is the n plus 1 selects problem the problem occurs when you have a parent object loading its child data and in loading that child data the data access layer issues a separate sequence statement for each child object that needs to be loaded let's look at an example of this we have a piece of code here and what this code is trying to do is to get the cumulative gpa for a student so it needs to get all of the courses taken by the student their grades in each course and the number of credits each course is worth in order to perform that calculation so this seems pretty straightforward we load a student object and then through the navigation properties on the object we can get to all of the other data that we need to do this calculation what happens though is that initially our students object doesn't have all of the data that it needs so the data for this navigations property are lazy loaded i have an extended event session tracing all of my sql right now and i am going to actually run this program so we can see what's happening and so we see that the program just prints the name and gpa but now i am going to switch over to the trace results so we can see what sql was run i have my trace file open here and i have done a little bit of cleanup to filter out all of the sp reset connections messages that the sql server driver issues when it retrieves a connection from the connection pool so you see right away there are quite a few statements here that are executing and if we group these statements together we actually get a better view of what's going on there are some queries in here that are over headed for entity framework but we are not worried about those what we care about are these two statements that are executed 42 times each and if you expanded out the sequel that in each of those statements you would see that these two queries that are grabbing the course offerings and course records from the database what is happening is that entity framework is able to get all of the courses this student has taken with a single query from a course enrollment table but then in iterates through each of those course enrollment records and to get over to the course records where the number of credits for the class is stored it first has to get each course offering record and then each course record and so one by one it is iterating through this lazy loading each of those objects for however many courses that this student has taken and we can see here this student has taken 42 courses so that means we have 42 unique courses offering records we have issued queries for and then again 42 unique courses records that we also had to load we also had to load some grade records so we could see what each grade is worth but there is only two unique values for those so that's just two more statement in this case but you see the problem to calculate the student's gpa entity framework is having to run 87 individual queries by my count so that's going to be very chatty and not very efficient the issue here is the lazy loading pattern which fetches data just as it needed while this is convenient for many scenarios it's not the right pattern for this scenario because we are having to go back and forth to the database to get all of this database piece by piece what we can do instead is use eager loading to instruct entity framework to load all of these data upfront and then entity framework will create a single query that gets all the data we need rather than issuing all of these little queries so to do that we just use the include method up here when we fetch our student object so i will put that in now and this first line is going to tell entity framework to get all of the courses offering as well as courses for the student and the second line will get all of the grades attached to any of the course enrollments data so when we get our student object we will have all the data we need to do this calculation so i am going to save this file and rerun this program so we can get a second trace of what's happening here and now we will look at that race we can see that this time we have only one statement and in fact if i take this statement here and i look at it and i have copied that in a notepad again so we can do just that we see that this one statement is getting all of the data that we need so in this case this is going to be much more efficient because we are not having to go back and forth to the database many many times this can happen when you are lazy loading objects in an orm as we have just seen but i have also seen this problem in traditional data access code where the code was written in such a way that it was retrieving all the child data for an object in an one by one fashion the point is when you are loading child data like this you want to think about the internals of how that is happening and make sure that you are not iterating over some sort of collection and issuing a sequel statement for each object in that collection because that's going to be very chatty and that's not going to perform very well so think about this whenever you are loading data that's child data of another object and if needed we don't hesitate to use one of the various tracing facilities to make sure that you are not issuing a whole bunch of individual sql statements in order to get the data in the next video let's wrap up this last module of this course you are watching the video module 6 applying common performance practices part 7 module summary in this module we looked at some application practices that you want to make sure and implement for your application to have the best performance possible first we talk about making sure that all of the sql in your application is parameterized sql this not only protects your sql injection attacks but has a performance benefit as well as sql server can reuse the same execution plan over and over making your code run faster and saving resources within sql server we also talked about stored procedure and showed that from a performance standpoint parameters sql and stored procedures are equivalent so you can be comfortable choosing whichever approach best fits for your situation then we talk about commit frequency and how auto commit is turned on in the sql server driver by default in some cases it makes sense to commit after each dml statement but otherwise using transaction and committing however often makes logical business sense not only in more correct for your application it will also perform better as well finally we talked about orms orms are a great tool that enhance our productivity and save us from writing the same data access code over and over again but this additional level of abstraction sometimes makes it hard to understand what is happening inside of sql server so just remember that ultimately everything does have to get converted to a sql statement and then all of the normal rules about selectivity and indexing apply finally if you are loading any child object especially through lazy loading be on the lookout for the n plus 1 select problem basically each child object will load one by one from the database and this chattiness has a tendency to slow down things so be on a lookout for this and use eager loading in those situations where it makes more sense this brings us to the end of the course and i hope you have found your time well spent in learning how sql server works from a performance standpoint and what you can do to make sure your application have optimal performance we have introduced a wide variety of concepts and tools that can help you understand what sql server is doing and what you need to do in order to performance tuning your application the best course of action now is to get some handson experience with these tools by looking at an application and database that you are familiar with because this is how you really learn what to look for and what the data is telling you thank you very much for your valuable time if you have any feedback please feel free to contact me on twitter at icode mechanic and you can also send me the mail on icodemechanic gmail.com you can also comment in the comment section down here at youtube thank you very much again and good luck for your future
