With timestamps:

00:00 - Hello, everyone, and welcome to an introduction 
to data structures. My name is Steven, and over  
00:06 - the next three hours, we'll be tackling the 
topic of data structures in relation to computer  
00:10 - science. More specifically, we'll be talking 
about what they are going over some different  
00:15 - types of data structures, and discussing how we 
can use each of them effectively with examples.  
00:22 - This course will be a general overview of 
data structures, and so we won't be confining  
00:26 - ourselves to one specific programming language. 
However, this series will require you to have a  
00:31 - basic understanding of programming. If you are 
completely new to computer science, I would  
00:36 - recommend our introduction to programming series, 
which will be linked in the description below.  
00:41 - That video will give you all the information 
you need to traverse these topics comfortably.  
00:45 - Now, we're almost ready to hop into things. 
However, there are a few housekeeping items  
00:50 - that I want to go over beforehand for those 
wishing to enhance their learning experience.  
00:55 - If you're not interested in that and want to skip 
right to the content, go to the time shown on your  
01:00 - screen now. For those of you staying though there 
are just a few things I would like to mention.  
01:05 - Firstly, in the description, you will find 
timestamps for each major topic covered in this  
01:10 - video, as well as timestamps taking you to every 
smaller section contained within those topics.  
01:15 - So please feel free to skip around if you're 
already comfortable with one topic or only  
01:20 - interested in a certain data structure. 
Next, we've decided to include the script  
01:25 - and visuals used for the series also in 
the description below. That way you can  
01:29 - follow along if you like or simply read the 
script if my soothing voice isn't your style.  
01:35 - Additionally, for each segment, I'll be including 
the references and research materials used to  
01:40 - write the script for each topic. That way, if 
you ever feel as if I haven't explained a topic  
01:45 - well enough, or we're just simply like more 
information about a certain topic, you'll have  
01:50 - a readily made list of articles and websites, 
which you can use to supplement this series.  
01:56 - If you feel like you have any questions throughout 
the series about maybe something I said or a  
02:01 - certain visual, please ask your questions in the 
comments below. I'll try to be as responsive as  
02:06 - possible for the first few weeks on questions 
you guys may have about the series and such.  
02:12 - And finally, in terms of housekeeping, I just 
like to say if you're not already subscribed to  
02:16 - our channel, no pointer exception, then consider 
doing so if you enjoy this type of content. as me  
02:22 - and my co host Sean regularly post videos in this 
style. We're trying to hit 1000 subscribers before  
02:27 - the fall semester of college begins. So check 
us out if you end up enjoying the video. With  
02:33 - my shameless plug out of the way, we're finally 
ready to tackle the topic of data structures.  
02:37 - In this introduction segment, we'll simply cover 
a general overview of what data structures are,  
02:42 - and then go over what type of information will 
be covered throughout the duration of the series.  
02:47 - So the obvious question to start with 
is what exactly is a data structure?  
02:51 - Well, in computer science, a data structure is a 
way to store, organize and manage information or  
02:58 - data in a way that allows you the programmer to 
easily access or modify the values within them.  
03:05 - Essentially, it's a way for us to store 
a set of related information that we can  
03:09 - use for our programs. data structures, and 
the algorithms used to interact, modify,  
03:16 - and search through them provide the backbone for 
many of the programs that you'll end up writing,  
03:21 - I can almost guarantee that in 99% of your 
programs data structure will be involved.  
03:27 - Each of the data structures that I'll be 
talking about are designed for the sole  
03:30 - purpose of storing information and allowing the 
end user to access and manipulate that information  
03:36 - in an efficient, effective way. But each one 
differs in the manner that they accomplish this.  
03:43 - If you have a basic understanding of programming, 
you probably know about a few different data  
03:47 - structures already, such as the array and the 
array list, also known as the list in Python.  
03:53 - But if you're going to be pursuing a career 
in computer science, just knowing these two  
03:57 - is definitely not going to cut it. Well 
basic data structures such as the array  
04:01 - are used frequently by companies throughout their 
code. More advanced data structures are being put  
04:06 - to use all around you the Undo redo button 
in Google Docs, Google Maps on your phone,  
04:12 - even the autocomplete feature through your 
text messages all require the use of more  
04:17 - advanced data structures, which makes them 
extremely useful to learn and comprehend.  
04:22 - Okay, now, I can't see you per se, but I can just 
tell that you're jumping up and down in your seat  
04:27 - hooked on data structures and how useful they 
can be. But before we jump too far into things,  
04:33 - let's discuss the information that we'll be 
covering in this series. Now, before we talk  
04:37 - about any specific data structures, we're going 
to have a brief talk about efficiency. We'll  
04:43 - discuss the metrics used to judge the speed and 
efficiency of different data structures, which  
04:48 - will then give you a better understanding of why 
one data structure might be used over another one.  
04:54 - From there, we'll start by diving headfirst into 
what I believe are the basic data structures,  
04:58 - those being array. An array lists. While you may 
already have a good understanding of what these  
05:04 - are, I would highly suggest that you still watch 
these segments, because we'll be going into a  
05:08 - little bit more depth as to why they're so useful 
based on differences in how they're stored in the  
05:13 - computer's memory. After that, we'll move on to 
what I'll call the intermediate data structures.  
05:20 - These are a little bit more complicated than the 
basics and have a few special attributes which  
05:24 - make them stand out from the rest. We'll begin 
by taking a look at stacks which are the backbone  
05:30 - for all recursive processes in your computer. Then 
we'll look at the antithesis of a stack the queue.  
05:37 - Moving on from there, we'll be covering 
linked lists and their evolved form in the  
05:41 - doubly linked list. Before moving on to the 
final of our intermediate data structures,  
05:46 - the dictionary which includes 
a mini lesson on hash tables.  
05:50 - Then we'll wrap up the series talking 
about trees and tree based data structures,  
05:55 - less linear and more abstract data structures 
beginning with the tree itself. We'll then move  
06:01 - on to tries a very useful data structure used for 
a lot of word processing algorithms. And finally  
06:06 - end off the series with a discussion on heaps and 
graphs. So in total, I'll be taking you through  
06:13 - four different segments containing 12 of the most 
common data structures that are practically used,  
06:18 - as well as providing examples of where and when 
to use them. section one on efficiency, section  
06:24 - two on basic data structures, section three on 
intermediate data structures, and wrapping it up  
06:30 - with section four on tree based data structures. 
With this knowledge, you'll be able to take charge  
06:35 - of your programming career and hopefully gain a 
competitive edge by implementing them when needed.  
06:41 - Like I said, however, before we jump straight 
into the different ways to store information,  
06:46 - I'd like to have a quick discussion on how we 
score the efficiency of these data structures  
06:50 - using what is known as Big O 
notation. So let's jump right in.  
06:56 - Okay, so before we talk about all these crazy 
data structures, like maps, and heaps, we want  
07:01 - a quantifiable way to measure how efficient a 
certain data structure is at the different tasks,  
07:06 - we might ask a bit. If we're going to be 
storing extremely large amounts of data,  
07:12 - being able to search through, modify, or access 
the information within a data structure needs to  
07:18 - be fast and efficient. As we briefly mentioned 
before, the industry standard for this kind of  
07:24 - implementation is big O notation. So what exactly 
is big O notation? And how do we measure it for a  
07:31 - specific data structure? Well, that's what we'll 
be covering in this segment. For most of the basic  
07:37 - and intermediate data structures in this series, 
we're going to be spending some time talking about  
07:41 - its efficiency using big O notation. So this 
is definitely a topic you're not going to want  
07:46 - to skip. With that being said, let's begin. So 
because there are so many different ways to store  
07:52 - information, like we talked about in the previous 
segment, programmers have developed this idea  
07:57 - of big O notation as a way to basically score a 
data structure based on a few different criteria.  
08:04 - This criteria can change based on who you 
ask, but for the purposes of this video,  
08:08 - we will be using four criteria representing 
the most common functions you might want from  
08:13 - a data structure. The ability to access a 
specific element within the data structure,  
08:19 - search for a particular element 
within the data structure,  
08:22 - insert an element into the data structure and 
remove an element from the data structure.  
08:28 - Now by measuring how efficiently a certain 
data structure can do these four things,  
08:32 - we can basically create a report card of sorts, 
which measures how efficient a certain data  
08:37 - structure is. At a glance, this gives us a 
pretty good overview of what a certain data  
08:43 - structure is good at, and what it is bad at. 
And it can help us decide which one to use.  
08:48 - If we need to store data that is easily accessible 
to the end user. For example, we might choose  
08:53 - a data structure which can access elements the 
quickest, and vice versa, if accessing elements  
08:59 - isn't the most important thing to us, but we 
need a data structure which can be easily added  
09:04 - to and deleted from, we would go for one which is 
most efficient and that specific functionality.  
09:10 - By looking at a data structures 
report card, if you will,  
09:14 - we can get a quick sneak peek at what 
they're good at and what they're bad at.  
09:18 - Now, if we use big O notation to basically create 
a report card, like I said, there must be some way  
09:25 - to actually grade each of these functions. And 
there is the four criteria mentioned, accessing  
09:32 - searching, inserting and deleting are all scored 
using big O notation time complexity equations.  
09:39 - Now what is the big O notation time complexity 
equation? Well, I'm glad you asked. Besides being  
09:46 - in an annoyingly long phrase, a big O notation, 
time complexity equation, or just time complexity  
09:52 - equations, work by inserting the size of the data 
set as an integer n and returning the number of  
09:58 - operations that need to be done. indicated by 
the computer before the function can finish. The  
10:04 - integer n is simply the size or amount of elements 
contained within the data set. So for example,  
10:11 - if we have an array, one of the data structures 
we'll get into soon, with the size of 10,  
10:16 - we would place 10 into the different efficiency 
equations for accessing searching, inserting and  
10:22 - deleting that represent the array, and returned 
back to us would be the number of operations  
10:27 - that need to be conducted by the computer. Before 
completion of that function. It does get a little  
10:33 - bit more complicated than this. But for the sake 
of keeping this series simple, all you need to  
10:38 - know is that these equations help represent 
efficiency amongst different data structures.  
10:45 - Also, an important thing to note here is that we 
always use the worst case scenario when judging  
10:50 - these data structures. This is because we always 
want to prepare for the worst and know which data  
10:55 - structures are going to be able to perform under 
the worst conditions. You'll see what this means  
11:01 - in greater detail a little later on once we 
start putting this into practice. But for now,  
11:05 - just keep in mind that when judging data 
structures, we always use the worst case scenario.  
11:12 - Moving on, the reason that it's called 
Big O notation is because the syntax  
11:17 - for these particular equations includes 
a big O, and then a set of parentheses.  
11:23 - inside these parentheses will include some 
function, which will correctly return the number  
11:27 - of operations needed to be run by the computer. 
So for example, let's say we have a fake function,  
11:34 - it can really be anything, the purpose in this 
case is irrelevant. Now for this fake function,  
11:40 - let's say it's time complexity equation 
was the one shown on your screen now,  
11:45 - we would pronounce this time complexity equation 
as o of two, meaning it takes two operations  
11:51 - from the computer before our make believe 
function can finish. If the time complexity  
11:56 - equation was o with a five in the parentheses, 
instead, it would be o of five, and so on.  
12:04 - Now, these are examples of time complexity 
equations, which run in constant time, meaning no  
12:09 - matter the size of our data set, it will always 
take the same number of instructions to run.  
12:15 - This is usually not going to be the case though, 
when it comes to the four criteria we want to  
12:20 - mention for our data structures. Most of the 
time are integer n, which again contains the  
12:25 - amount of elements within the data set is going 
to have some adverse effect on how many operations  
12:31 - it takes to say search through our data 
structure, which makes sense, a larger data set  
12:36 - means it's going to take more operations from the 
computer to search through that entire data set.  
12:43 - Now we score these four functionalities in number 
of operations performed, because measuring by how  
12:49 - long the function takes to run would be silly. 
If we measured by a metrics such as time taken  
12:55 - to completion, our results would be highly 
biased by the hardware used to run the function.  
13:00 - a supercomputer used by Google is obviously going 
to be able to search through a data structure much  
13:06 - faster than a laptop. time complexity equations 
essentially level the playing field by instead  
13:12 - returning the number of operations to eliminate 
the bias in processing power that exists.  
13:18 - So to sum up everything that we've learned so far,  
13:21 - we measure the efficiency or speed of a 
data structure based on how well it can  
13:26 - perform four basic tasks accessing, searching for 
inserting and deleting elements within itself.  
13:34 - Each of these criteria is modeled by an equation 
which takes in the size of the data structure  
13:40 - in number of elements and, and returns back the 
number of operations needed to be performed by  
13:45 - the computer to complete that task. By measuring 
these four metrics, we can get a pretty good  
13:50 - understanding of what the data structure is 
good at, and what the data structure is bad.  
13:56 - Now, it's important to note that this isn't 
the end all be all for deciding on which data  
14:01 - structure to use in your program. As you'll 
see, as this video progresses, many of the  
14:06 - data structures were structured with specific 
quirks or features, which separate them from the  
14:11 - rest and provide additional functionality. Big 
O notation is incredibly useful, and something  
14:17 - that you should definitely take into consideration 
when determining which data structure to implement  
14:22 - into your program, but it should not be the only 
thing that you use. Cool. Now that we know a  
14:27 - little bit about how we measure the efficiency of 
a data structure using time complexity equations,  
14:33 - along with the four criteria actually used to 
measure them. Let's dive straight into what  
14:38 - the actual equations mean in terms of efficiency. 
That way, when we start grading data structures,  
14:44 - you'll have a good idea as to how good or 
bad each one is at that particular metric.  
14:50 - Basically, we're going to cover the six 
most common time complexity equations  
14:54 - from most efficient to least efficient. Now, the 
absolute best that a data structure can score  
15:01 - on each criteria is a time complexity equation 
of o of one. This essentially means that no  
15:07 - matter what the size of your data set is, 
the task will be completed in a single step.  
15:13 - If your data set has one element, that 
computer will finish the task in one step.  
15:18 - If your data set has 100 elements, one step 
1 million elements one step 800 quadrillion  
15:26 - elements, it does not matter, the computer 
will finish the task in a single step.  
15:32 - This is why when we look at the graph of volume 
of data versus instructions required for o of one,  
15:38 - the line remains constant at one. No matter the 
volume of data stored within the data structure,  
15:44 - that computer can complete that 
task in a single instruction.  
15:48 - Whether it be accessing searching, inserting, 
or deleting, O of one is the gold standard  
15:55 - absolute best top of its class time complexity 
equation. It is basically the Michael Jordan  
16:01 - of big O notation when it comes to time 
complexity equations. Now the next fastest  
16:07 - type of time complexity equation is O log 
n. While not as fast as instantaneous time,  
16:13 - a function having a time complexity of o log n 
will still provide you with very fast completion.  
16:19 - Now if you don't fully understand logarithms 
entirely, just know that this efficiency is  
16:24 - slower than instantaneous time of one and faster 
than the next level of efficiency known as O  
16:30 - of n. Additionally, because the volume of data 
versus time graph follows a logarithmic curve,  
16:38 - the larger the data set that you use, the more 
bang for your buck that you're going to get.  
16:43 - Basically, as the amount of data you try to 
perform one of our four criteria on increases  
16:49 - the rate of change of the amount of operations 
it takes to complete that certain task decreases.  
16:56 - So as you can see, at the beginning of the 
graph here, when we increase the volume of data,  
17:01 - our number of operations skyrockets. But 
when we do the same for larger and larger  
17:06 - sets of data, our number of operations 
increases much more slowly than before.  
17:12 - A good example of a non data structures related 
function, which has a time complexity of o  
17:17 - log n is the binary search. If you know what 
a binary search is, then you'll understand  
17:23 - how it is that when the data set gets larger, 
the number of instructions needed to find the  
17:27 - item you're searching for, doesn't increase 
at the same rate. If you don't know what a  
17:32 - binary searches, but would like to know, 
in order to better understand Oh login.  
17:38 - A link in the description below will take 
you to that point in our introduction to  
17:41 - programming series where you can learn about 
it. If not, let's move on to the next equation.  
17:48 - O of n is the next common time complexity equation 
that's going to come up frequently during this  
17:53 - lecture. The graph of volume of data versus 
instructions needed for this function is linear,  
18:00 - meaning that for every element you add to the 
data set, the amount of instructions needed to  
18:04 - complete that function will increase by the 
same amount. So to perform a function with  
18:10 - the time complexity of O of n on a data set 
with 10 elements, it will take 10 instructions  
18:16 - 50 elements will take 50 instructions, 1000 
elements, 1000 instructions, and so on O of n  
18:24 - is really the last good time complexity equation 
that exists. Anything above this is considered  
18:30 - inefficient and not very practical when it 
comes to data structures in computer science.  
18:36 - The next type of equation that will come up is o n  
18:39 - log n. This equation is the first that 
is relatively bad in terms of efficiency.  
18:45 - The graph of volume versus operations shows a 
somewhat linear but increasing graph meaning  
18:50 - unlike o log n, it won't be better in terms of 
efficiency as the size of the data set increases.  
18:57 - Instead, the slope actually 
increases as the volume of data does.  
19:02 - The last two types of equation are o 
n squared and o two to the N. These  
19:08 - are both incredibly inefficient equations, 
which should be avoided if at all possible.  
19:13 - Because they're both exponential in structure, 
which can be seen from their graphs of volume  
19:18 - versus operations. The larger the data set that 
you use, the more inefficient it will become.  
19:25 - While there are more time complexity equations 
that exist such as o n factorial, which is even  
19:31 - worse than the two I just mentioned, the data 
structures we'll be talking about, we'll never  
19:36 - have time complexity equations that exists outside 
of the five we've just covered. So we're going to  
19:41 - stop there. Now the final thing I want to say is 
just a reiteration of something I said before,  
19:48 - these time complexity equations are not the only 
metric you should be using to gauge which data  
19:54 - structure to use. As we get deeper and deeper into 
this series, you'll see that we might have some  
19:59 - data structures which don't seem that efficient 
at all based on their time complexity equations  
20:04 - for provide some other functionality or feature, 
which makes them extremely useful for programmers.  
20:11 - Okay, now that we have some knowledge 
on how we actually grade these data  
20:14 - structures in terms of efficiency, let's 
hop into our very first data structure.  
20:20 - The first data structure we're 
going to be covering is the array.  
20:25 - Now I understand the array is a pretty common 
data structure taught in most programming classes,  
20:29 - and that many of you might already know 
about the properties and uses of an array.  
20:34 - But in this segment, we'll be going into 
more detail on the array as a data structure,  
20:39 - including its time complexity equations, 
storage methods, and more. So check the  
20:44 - description for a timestamp to skip ahead to 
that section. If you already know the basics.  
20:50 - If you're not as versed in the array, 
though, or just want a general review,  
20:53 - then stick around, because right now we'll 
be discussing the basics of an array.  
21:00 - An array is fundamentally a list of similar 
values grouped together in a central location,  
21:05 - which makes sense because if you 
remember, we use data structures  
21:08 - to store sets of similar information, so 
that we can easily use that information.  
21:14 - Basically, arrays can be used to store anything, 
usernames for an app, high scores for video game  
21:21 - prices for an online shop, pretty much any 
list of values which are fundamentally similar  
21:26 - meaning of the same type. So integers, strings, 
floats, objects, the list goes on and on.  
21:34 - Any primitive or advanced data type you 
can think of can be stored within an array.  
21:40 - Every item contained within the array is 
referred to as an element of that array.  
21:45 - And we call the collective total of elements, 
the array, this is going to be true for almost  
21:51 - all data structures we talked about, by the 
way, so just keep that in mind. An array will  
21:56 - also have three attributes associated with 
it, the first being a name for the array,  
22:02 - the second a type for the array, and the third 
a size for the array. Let's start with the name.  
22:11 - The name, if you couldn't tell is just simply a 
name for the array, which can be used to reference  
22:16 - and interact with it. Say for example, we had 
an array called names, which contains the list  
22:23 - of company employees. And then we also had an 
array called salaries which contained a list of  
22:28 - salaries for those employees. If we wanted to 
search through the list of salaries, we would  
22:33 - do so by referencing the salaries array with its 
name. This way the computer knows that you want to  
22:39 - search through that specific array containing 
salaries, and not the names arraigns that  
22:45 - this works vice versa as well, of course, if we 
wanted to search through the names array instead.  
22:52 - Now another thing to note really quickly, while we 
have this up is that these two arrays are also an  
22:56 - example of parallel arrays. Parallel arrays are 
two or more arrays containing the same number  
23:03 - of elements and have the property wherein 
each element in one array is related to the  
23:08 - element in the other array, or arrays have the 
same position. So from our example, we can say  
23:16 - that the first element in our names array, john 
smith, corresponds to the first element in the  
23:22 - salary array, the integer 10,000. This would 
mean that john smith has a salary of 10,000.  
23:30 - This is really good, because as you'll see, later 
on, we can't store different types of variables  
23:36 - in the same array. So we couldn't have the salary 
integers be stored in the same place as the name  
23:42 - strings, making parallel arrays extremely useful 
for storing different types of data about the  
23:47 - same entity. Alright, tangent over, let's get back 
to the attributes of arrays. And the second one,  
23:54 - we'll be talking about the arrays type. Now an 
arrays type is simply what type of information is  
24:02 - stored or will be stored within that array. One of 
the biggest stipulations about arrays is that it  
24:08 - has to hold all the same type of information. So 
you cannot have an array containing integers and  
24:15 - strings, it would have to be either an array of 
only integers or only strings. This works for any  
24:22 - type of information you would want to store within 
the array, meaning every element or spot within  
24:27 - the array must be of the same type. The third 
and final attribute that an array has is a size.  
24:36 - This size is a set integer that is 
fixed upon the creation of the array.  
24:40 - It represents the total amount of elements 
that are able to be stored within the array.  
24:46 - An array size cannot be changed. I'll repeat 
that again because it's extremely important  
24:52 - and array size cannot be changed once created. 
Once you write the line of code which instantiates  
24:59 - an array You give it a defined size or fill 
it with elements until it has a defined size.  
25:05 - And then at that point, the array size cannot be 
changed by conventional methods. This is actually  
25:11 - pretty rare for a data structure. And it may seem 
really counterintuitive and useless. But later on  
25:17 - when we talk about an arrays efficiency, you'll 
see that this is for a specific reason, and not  
25:22 - just to be annoying to the programmer. So there 
you have it, the three attributes of an array,  
25:28 - a name to reference it a type to fill it with 
an A size to control it. Let's now shift focus  
25:35 - and talk about how we actually define an array 
ourselves how we reference and change values  
25:40 - within an array, and then dive into 
the idea of two dimensional arrays.  
25:46 - There are actually two different ways to 
instantiate an array in most languages,  
25:51 - you can either populate the array with elements 
that you want contained within it right then and  
25:56 - there. Or you can set a specific size for the 
array, and then slowly populate it later on  
26:02 - as the program runs. We'll be discussing how 
to do both, starting with the first creating  
26:08 - the array with the elements already stored. 
Now defining and filling an array as soon as  
26:14 - you create it is mainly used for when you already 
know which values are going to be held within it.  
26:20 - For example, let's say you're creating an 
array, which is going to hold the salary  
26:23 - of 10 different workers at a company. And that 
information is being read in from a text file.  
26:29 - Well, you already know the 10 salaries 
for those workers from the text file,  
26:34 - and so you're able to immediately 
populate the array when you create it.  
26:38 - This way, you'll already have the information 
available to you as the program runs.  
26:44 - The way you do this varies amongst 
different programming languages.  
26:48 - But it's shown here in three 
different languages, Java, Python,  
26:52 - and C sharp. To give you a basic gist of how this 
is accomplished. All three lines of code on your  
26:59 - screen now will create an integer array of size 
three, containing the numbers one, two, and three.  
27:07 - You can see that all of them involve 
the declaration of an array name,  
27:11 - which is then set equal to the values you want 
to include within the array encapsulated by some  
27:16 - sort of brackets or braces. This isn't 100% how 
it's going to be for all languages, but for the  
27:24 - most part, it will follow this skeletal outline. 
As you can see, Java and C sharp require you to  
27:30 - define the array type, whereas Python does not. 
But for the most part, the syntax is the same.  
27:36 - Again, that is name of array set equal to list of 
comma separated values encapsulated by brackets or  
27:43 - braces. This also develops the size of the array 
for you automatically. So since we populate the  
27:50 - array with the numbers, one, two, and three, the 
array would instinctively have a size of three,  
27:56 - as it now contains three integers. If we were to 
say go back and add in a fourth number, where we  
28:03 - define the array, the size would dynamically 
increase to for the next time we run the code.  
28:10 - Now the second way in which we can instantiate an 
array is by setting an initial size for our array,  
28:16 - but not filling it with any elements and then 
slowly populating it as the program runs.  
28:22 - This would be used in the case that you 
don't actually know what information  
28:25 - you're going to store in the array, 
but will as the program runs through.  
28:30 - The most common case in which this happens 
is in the case where the end user will enter  
28:35 - information into the program. And then that 
information will be stored inside the array.  
28:41 - Since the information varies based on which user 
runs the code and what they input during that  
28:46 - particular run, we have no way of populating 
the array as soon as it's instantiated,  
28:51 - forcing us to add the information later. 
shown on your screen now are two ways in  
28:57 - which we can do this in Java and C sharp. Now 
you'll notice that there is no Python section  
29:03 - because of the fact that creating populate 
later arrays is not conventionally possible  
29:08 - in the base version of Python, without the use 
of more advanced data structures or packages.  
29:14 - You can see However, with Java and C sharp that 
just as with the other example, there are slight  
29:20 - differences in the syntax on how we do this, but 
it generally follows some set pattern, the type  
29:26 - of the array followed by a name for the array, 
which is then set equal to a size for the array.  
29:33 - Remember, this size is final and cannot 
be changed outside of this initial call.  
29:38 - If you end up creating an array with the size 10. 
But then later in the program, find that you need  
29:43 - more space for it. You can always go back to 
where you instantiated it and change the size.  
29:49 - But after that line of code runs, there's 
no conventional way to increase it.  
29:55 - You may also be wondering what the brackets 
mean when instantiating an array and that's  
29:59 - true Way to signify to the computer that we are 
indeed trying to create an array and not just a  
30:04 - variable. Now that we know the two different ways 
to instantiate an array, we need to know how we,  
30:11 - as the programmer actually get information that is 
stored within the array so that we're able to use  
30:16 - it. And the simplest way to answer that question 
is through the means of a numerical index.  
30:23 - an index is simply an integer, which corresponds 
to an element within the array. Now, the most  
30:29 - important thing to know about indexes is that 
for a certain array, they begin at zero instead  
30:35 - of one. So if we have an array of 10 elements, 
the first index would actually be index zero,  
30:43 - the second would be index one, and so 
on all the way up to the ninth index.  
30:48 - It's a little bit weird at first, 
but trust me, you get used to it.  
30:52 - Now, to retrieve information from a certain 
position within the array, you would reference  
30:57 - it using both the arrays name, and then the 
index number of the element you wish to retrieve.  
31:04 - Say we have our array called numbers here, which 
contains the integers one through 10. To print out  
31:10 - the element at the fifth index, in this case, the 
number six, we would reference the array name, in  
31:16 - this case numbers. And then in a set of brackets, 
we would place the index we want to retrieve,  
31:22 - in this case, the number five. What this piece of 
code is basically telling the computer is to print  
31:28 - out the fifth index of the numbers array, which 
again is the integer six. Now because indexes  
31:36 - start at zero instead of one, it's very important 
to note that to get the 10th element of the array,  
31:42 - you would actually need to reference it using 
the number nine since there is no 10th index.  
31:48 - If you do end up making this mistake, it 
will result in an array out of bounds error,  
31:53 - since there is no 10th index in 
the bounds of the numbers array.  
31:58 - Referencing and arrays index number is how we 
actually replace elements within the array as  
32:03 - well. Continuing with our numbers array, let's say 
we wanted to change the last element, the integer  
32:09 - 10 to be the integer 11. Instead, what we would do 
is reference the ninth index of the numbers array,  
32:17 - the one which currently contains the 
integer 10, and set it equal to 11.  
32:23 - This tells the computer to take the element at 
index nine and replace it with the integer 11.  
32:29 - Essentially swapping 10 for 11. Now the last 
thing I want to talk about before we jump into  
32:36 - the time complexity equations for an array is 
the practice of putting arrays inside of arrays.  
32:42 - An array with an array at each element 
is known as a two dimensional array.  
32:48 - Think of a two dimensional array as a 
matrix. It's similar to any other array,  
32:54 - except for the fact that instead of a primitive 
data type like an integer how's that each element,  
32:59 - we would instead have a whole other 
array with its own size and indexes.  
33:05 - two dimensional arrays are useful for a variety 
of purposes, such as if you were programming,  
33:10 - a chess board, a bingo board, or even an image 
where each element in the two dimensional array  
33:17 - contains an RGB value, which when combined 
creates an image. Now referencing an element  
33:25 - within a two dimensional array is mostly 
the same as with one dimensional arrays,  
33:29 - except you now need two indexes, the first number 
would be the index of the column that you want to  
33:35 - reference, and the second number would be the 
index of the row that you want to reference.  
33:41 - By combining these two numbers, we can single 
out an element within the two dimensional array  
33:45 - that will then be returned to us. As an example, 
let's create a two dimensional array with 16 names  
33:52 - contained within four rows and four 
columns. Now to access the name Carl,  
33:59 - you'd first single out the column, which 
contains the name you're looking for.  
34:03 - In this case, Carl is in the third 
column, meaning it's in the second index.  
34:09 - Next, you would single out the row which contains 
the name you're looking for. In this case,  
34:15 - the name Carl is in the third row. So we would 
again use the second index to reference it.  
34:21 - Combining these two pieces of information gives us 
the index location two comma two. And if you look,  
34:28 - Carl is indeed at index location two 
comma two. Just as another example,  
34:34 - Adam is in the first column two rows down. And 
so to reference Adam, you would simply call upon  
34:41 - the element at index locations zero comma one. 
two dimensional arrays are just the beginning.  
34:48 - You can also have three dimensional arrays, four 
dimensional arrays, and so on for containing  
34:53 - large amounts of advanced relational data. But two 
dimension is where we're going to stop for today.  
35:01 - All right, that concludes the 
background information on arrays.  
35:05 - Now that we know what they are, let's actually 
talk about arrays as a data structure.  
35:10 - This is where we're finally going to be using 
our big O notation knowledge from the previous  
35:14 - segment. to judge the array as a data structure. 
We'll be going through the four criteria we talked  
35:19 - about previously, accessing elements within an 
array, searching for an element within an array,  
35:26 - inserting an element into an array, and deleting 
an element from an array, and scoring it based  
35:32 - on how effectively they can complete these 
four tasks using time complexity equations.  
35:38 - Now accessing an element within an array can 
be conducted in an instantaneous of one time.  
35:44 - This means that for any element within your 
array that you want to know the contents of,  
35:49 - you can do so immediately by simply 
calling upon its index location.  
35:54 - This has to do with the way that 
an array is stored within memory.  
35:58 - For example, sake, let's look at a 
fake portion of memory in the computer.  
36:04 - You can see we have some information near the 
beginning, maybe some integers or strings that  
36:10 - we've initialized, that's not important. What's 
important is that when we get to the part in our  
36:15 - code, which instantiates an array, we already know 
exactly how large the array is going to be. Either  
36:22 - because we've populated it with the elements 
needed, or given it a definitive final size.  
36:28 - This means we also know exactly how much space 
we need to reserve in memory for that array.  
36:34 - Combining these two pieces of knowledge means 
we can instantaneously figure out the location  
36:39 - of every single element within the array 
by simply taking the starting location of  
36:44 - the array in memory, and then adding to it the 
index of the element that we're trying to find.  
36:50 - So let's say our array contains three elements, 
each of them integers. And we want to know what  
36:56 - the element at the first index is. Well, we 
know that the array start storing data here.  
37:03 - And we also know that every element within 
the array is stored contiguously together  
37:08 - in memory. And so by taking the starting location 
for the array in memory, and adding one to it,  
37:15 - we now know that that's where the first index 
of our array is going to be, and can return  
37:20 - that stored value appropriately. This is the main 
reason why array sizes cannot be changed. All of  
37:27 - the information within an array must be stored in 
this one place. So that we're able to do this, the  
37:33 - contiguous structure of an array prevents you from 
adding space to it after it's been initialized,  
37:39 - because it literally can't without breaking up the 
data and adding in the space to store new elements  
37:44 - down the road away from the initial array. This 
would make accessing elements instantaneously  
37:51 - not possible. Hopefully now you can see why both 
accessing an element within an array is O of one,  
37:57 - as well as why array sizes are final, because 
not many other data structures allow you to have  
38:03 - instantaneous accessing power, meaning arrays have 
a huge advantage on the others in this metric.  
38:10 - Next up searching through an array is O of n.  
38:14 - This is because for the most part, you 
will be working with unsorted arrays,  
38:18 - those being arrays which are not in alphabetical 
numerical or some sort of quantifiable order,  
38:25 - meaning that to find an element within the array, 
you may have to search through each element before  
38:30 - you can find it. This is known as a linear 
search. And if you want to know more about it,  
38:36 - click the card in the top right corner, 
and it will take you to that section  
38:40 - in our introduction to programming series. A link 
will also be provided in the description below.  
38:46 - Now while there are faster ways to search 
through an array, those only work when the  
38:50 - array is sorted. And so for a basic array, in 
worst case scenario, searching through it to  
38:56 - find a particular element is going to be O of n. 
Now Sure, there are going to be cases where the  
39:02 - element you're searching for is the first element 
in the array, or even somewhere in the middle.  
39:07 - But remember that when working with big O 
notation, we always use the worst case scenario.  
39:13 - And in this case, the worst case scenario is that 
the item that you're searching for ends up being  
39:18 - the last element within the array, which for an 
array of size n would take n operations to reach.  
39:26 - Finally, inserting and deleting elements from an 
array both have a time complexity equation of O  
39:32 - of n for inserting this is because adding an 
element into the array requires you to shift  
39:39 - every element that's after the index, you want 
to insert the value at to the right one space.  
39:45 - For example, if you have an array of size 
five, currently filled with four numbers,  
39:50 - and you want to insert the number one into the 
array at index zero, you would first need to shift  
39:56 - every element to the right of the zero with index 
right One, essentially freeing up the space for  
40:02 - the new integer, while also retaining all of the 
information currently stored within the array.  
40:09 - Then, and only then can you actually insert it. 
This requires you to traverse through the whole  
40:14 - array of size n, which is why the time complexity 
equation is O of n. Now, sometimes you might not  
40:22 - have to shift the whole list, say in the case, 
you had the same array of numbers, only this  
40:27 - time the fourth index was free, and you wanted 
to insert the number five in that open spot.  
40:33 - Since you don't have to move everything to the 
right one, in order to make space for this new  
40:37 - element, does that make inserting into an array 
have a time complexity equation of o of one?  
40:43 - Well, no, because again, when we talk about a 
functions time complexity equation, we always  
40:49 - refer to it in the worst case scenario, the most 
amount of operations that we're going to have to  
40:54 - conduct before we can insert an element into an 
array is n, the size of the list, making it's time  
41:01 - complexity equation O of n. Deleting an element 
from an array follows mostly the same idea, you  
41:09 - shift every element to the right of the one, you 
want to delete down one index. And essentially,  
41:15 - you have deleted that element. Again, let's say 
we had an array of numbers one through five,  
41:21 - only this time, we want to delete 
the number one at the zeroeth index,  
41:26 - what we would do is set the zeroeth index 
to whatever is contained at the first index,  
41:31 - in this case, the integer two. And then we 
simply repeat this process until we get to  
41:37 - the end of the array, finishing it off by setting 
the last element in the array to be some no value.  
41:44 - This deletes the element from the array by 
basically replacing it with a value to the right.  
41:49 - And then this process is repeated into it 
frees up one space at the end. Essentially,  
41:55 - we're just moving the entire array down one. 
Now worst case scenario, we're going to have to  
42:01 - shift every element in the array of size n, making 
deleting from an array also have a time complexity  
42:07 - equation of O of n. So there you have it the 
four time complexity equations for an array,  
42:14 - accessing is O of one and searching, 
inserting and deleting are all O of n.  
42:21 - Arrays are a really good data structure for 
storing similar contiguous data. Its ability  
42:27 - to access any item in constant time makes it 
extremely useful for programs in which you would  
42:32 - like to be able to have instant access to the 
information contained within your data structure.  
42:38 - It's also a lot more basic than some of the other 
data structures that we'll end up talking about,  
42:43 - making it both easy to learn and reducing the 
complexity of your code. Now, some disadvantages  
42:49 - of the array are the fact that the size of the 
array cannot be changed once it's initialized.  
42:54 - Inserting and deleting from an array can take 
quite a while if you're performing the function  
42:58 - on larger data sets. And if you have an array 
which is not full of elements, you're essentially  
43:03 - wasting memory space until you fill that index 
location with a piece of data. Overall, the array  
43:10 - is a pretty reliable data structure. It has some 
flaws as well as some advantages. A program that  
43:16 - you write could use the array if need be, and 
it would work just fine. But sometimes you might  
43:22 - want some extra functionality. And that's where 
more advanced data structures come into play.  
43:28 - One of those more advanced data structures 
is what's known as the ArrayList.  
43:33 - The ArrayList fundamentally can 
be thought of as a growing array,  
43:37 - we just finished talking about the array and 
how one of its major flaws was the fact that  
43:42 - once initialized an array size could not 
be changed using conventional methods.  
43:48 - Well, in contrast, an ArrayList size expands as 
the programmer needs. If you take a look at the  
43:55 - ArrayList on your screen now full of four 
elements, and you decide to add one to it,  
44:00 - it will simply expand its size to fit five 
elements. As you can probably tell this is  
44:06 - extremely useful. Which begs the question of 
why not just always use ArrayList. I mean,  
44:13 - in comparison, the array list seems to provide 
all the functionality of an array and then some  
44:19 - Well, that's definitely a valid question, and 
one we will get to later on in this section.  
44:24 - But before we can do that, we need to cover 
the basics of an ArrayList including some of  
44:28 - the properties and methods associated with it. 
Alright, let's hop in. So as we said before,  
44:35 - an ArrayList is simply a resizeable array, 
making them extremely similar in structure.  
44:42 - This is furthered by the fact that an ArrayList 
is actually backed by an array in memory,  
44:48 - meaning that behind the scenes of your code, the 
ArrayList data structure uses an array as its  
44:53 - scaffolding system. For this series, we need not 
go further than that. But for us, it just means  
45:00 - that a lot of the functionality will be the same. 
Now, this doesn't mean everything is going to be  
45:05 - the same, which you'll see later on. But it's 
still important to make note of before we get  
45:10 - too deep into things. The next thing I want to do 
before we talk about functionality is actually go  
45:16 - over how we initialize an ArrayList. To do this, 
again, is going to vary based on which language  
45:22 - you're using. So shown on your screen now are 
two different ways to do so in Java and C sharp,  
45:29 - you may notice again, that there 
is no Python on the screen.  
45:32 - And this is because in the base version of Python, 
arrays and array lists are actually not separate  
45:38 - entities. They are frankensteined together into 
a single data structure called lists. Lists  
45:45 - take some functionality from 
arrays and some from array list.  
45:50 - It's a lot more complicated than that. But for 
this series, that's all you're going to need to  
45:54 - know as to why we are not including Python. In 
this section. We discussed initializing Python  
46:00 - lists in the previous section. So if you're 
interested, you can go back and look at that.  
46:06 - Now going back to ArrayList initializations. 
Another thing you may notice is that it  
46:11 - looks a little bit awkward in the way 
that these statements are structured.  
46:15 - And that's mainly due to the fact that the 
ArrayList is actually its own separate class  
46:19 - outside of the base version 
of these two languages.  
46:23 - Now, I'm not going to get into class hierarchy 
and object oriented programming right now,  
46:28 - because that's a whole other topic with 
big concepts and even bigger words.  
46:33 - For right now, this just means that to create a 
new ArrayList, we have to invoke the ArrayList  
46:38 - class when defining it, which as you can see is 
done at the beginning of both initializations.  
46:44 - After that, you can see that we give it a name, 
which is then set equal to new ArrayList with a  
46:50 - set of parentheses. Now in this parentheses, you 
have a few options, you can either enter in an  
46:56 - integer, which will then be used to define a size 
for the ArrayList. Or you can just leave it blank.  
47:03 - Leaving the parentheses blank like that will 
automatically define a preset size of 10 for  
47:08 - the ArrayList. Again, just as a reminder, this 
can be dynamically increased as time goes on  
47:15 - if we add enough elements, but 
it's just meant as a base size.  
47:19 - Now you may be wondering if we can actually 
populate the array with elements when initializing  
47:24 - it as we could with arrays. But array lists 
actually do not support this type of declaration.  
47:31 - Moving on, let's talk about functionality. Now 
the array list can be thought of as pretty much  
47:37 - the evolved form of an array. It's a bit beefier 
has a little bit more functionality and is overall  
47:43 - more powerful than array. That's certainly not 
to say that it's better in every case. But for  
47:49 - the most part, the ArrayList is going to be 
thought of as the older sibling amongst the two.  
47:54 - This is attributed to the fact that it belongs 
to the prebuilt ArrayList class, which we talked  
47:59 - about earlier. The fact that the ArrayList belongs 
to a class means it's going to come with pre built  
48:05 - functions that are already at our disposal from 
the moment we define and instantiate an ArrayList.  
48:11 - More specifically, the ArrayList comes with 
methods we can use to access change, add to  
48:18 - or delete from an easily if you were using an 
array, you would have to program most if not all  
48:24 - of these methods by hand. And so having them pre 
built into the ArrayList class makes it especially  
48:30 - useful. You'll see that this is the case with 
a lot of the data structures down the road,  
48:36 - we're having a data structure belonging to its 
own class cuts out a lot of programming time,  
48:41 - you have to spend making appropriate methods 
for the functionality that you might want.  
48:46 - Now the types of methods that you're 
going to get will vary based on language.  
48:50 - For example, in Java, you'll have a variety of 
methods to use, including ones to add elements  
48:56 - to the ArrayList, remove them from the 
ArrayList, clear the ArrayList entirely,  
49:01 - return it size, etc, etc. as well as 
tons of other more specific functions.  
49:07 - And another language though, such as C sharp, 
you'll have some of the same methods as the Java  
49:13 - ArrayList. But you might also have some methods so 
that the Java version does not and vice versa. The  
49:19 - C sharp version might not have some of the methods 
that the Java version does. The same is going to  
49:25 - apply for any other language you use, which 
might implement the ArrayList data structure.  
49:31 - Because of the variability surrounding the 
ArrayList amongst languages. In this series,  
49:36 - we're simply going to be covering six of the most 
common methods that are both useful and can be  
49:41 - found in 99% of the ArrayList classes. These six 
methods are the Add method, the Remove method,  
49:49 - the get and set methods, the clear method, and 
the to array method. This may look like a lot  
49:56 - but remember, all of these are pre programmed 
for you And so you don't have to make them all  
50:01 - by hand, all you have to do is call them on 
a pre made array list and you're set to go.  
50:07 - Speaking of pre made array lists, before we dive 
into each of these functions and how they work,  
50:13 - let's first create an example ArrayList, we 
can use to manipulate and show how they work.  
50:19 - Let's call it example a list and give it a 
size of four so that it's not preset to 10.  
50:24 - And we're set to go. Now, the Add method actually 
comes in two different types. One, which takes in  
50:31 - only an object to add to the end of the ArrayList. 
And one which takes in both an object to add  
50:36 - to the ArrayList, as well as an index value 
representing the index to insert the object at.  
50:43 - Let's start with the simpler of the two, one, 
which simply takes in an object. This method is  
50:48 - for more basic cases where you don't care about 
where in the ArrayList, the object you wish to  
50:54 - add is going to end up, it will simply append the 
object you pass in as an argument to the end of  
51:00 - the ArrayList. So let's take our example ArrayList 
and run the Add method on an integer of two. Now,  
51:09 - normally, ArrayList only hold objects, not 
primitive types, like the integer two that we're  
51:14 - trying to pass in. However, the computer will 
automatically convert our primitive integer into  
51:20 - an integer object with a value of two, so that 
we can still add it easily. This is known as auto  
51:27 - boxing. And it's going to be used throughout the 
rest of the series to modify data structures with  
51:31 - primitive types. So I just thought I'd mention 
it now so that you're not confused later on.  
51:37 - Okay, so when we run our code, since the 
ArrayList, is empty, and we ran the Add method,  
51:43 - which doesn't care about location, it's going 
to add the integer to at the first index,  
51:48 - index zero. Now if we run another add method, and 
this time pass in the integer five as an argument,  
51:55 - since the zero with index is already taken, 
it will be slotted in as the first available  
52:01 - open index, that being the first index. 
So as you can see, it's pretty simple.  
52:07 - It takes in an object and we'll put it 
at the first open location available.  
52:13 - Moving on to the second type of add method, one 
which takes in an object to add to the ArrayList,  
52:19 - as well as an index to place it. This 
one works very similarly to the previous  
52:24 - only makes sure that the object that you're 
adding is appended at the index provided.  
52:30 - Again, let's say now we want to add the 
number one to our example ArrayList.  
52:35 - But we want to make sure it's placed in numerical 
order. In this case, at the zero with index,  
52:41 - what we will do is call the Add method, providing 
the integer one as an argument. In addition to the  
52:47 - index zero, we want to add to the ArrayList. Once 
the code has run, the ArrayList will automatically  
52:54 - shift our integers two and five to the right 
one, in order to make space for the integer one.  
53:02 - This works for any index contained 
within the bounds of the array list.  
53:06 - So if we wanted to do it again, only this time, 
insert the number three at the second index,  
53:12 - so that the list remains in numerical order. 
We'd call example, a list dot add, and then in  
53:18 - the parentheses passing the integer three and 
the index location two. After the code has run,  
53:26 - you'll see that we have added 
the integer into our ArrayList.  
53:30 - Now it's important to note that because there 
are two integers being passed in as arguments,  
53:35 - you must know which one your computer's treating 
as the integer and which one the computer is  
53:40 - treating as the index location. mixing these up 
could cause the computer to try and insert the  
53:46 - attempted index location at a different location 
than the one you were attempting to insert at.  
53:52 - So just be careful and knowledgeable as 
to the order your methods arguments are  
53:56 - in. Now the next method that comes pre packaged in 
the ArrayList class is the Remove method. And this  
54:04 - one also comes with two different types. The first 
takes in an integer as an argument, and just as  
54:10 - the name suggests, will remove the object if there 
is one at the index location provided and return  
54:17 - it back to the user. The second takes in an object 
and will simply remove the first instance of that  
54:24 - object within the ArrayList If present, and will 
then return true or false whether an object was  
54:30 - removed. So if we wanted to remove the number five 
from our ArrayList, we would have two different  
54:36 - options. We could call example, a list dot remove, 
and inside the parentheses placed the index of the  
54:42 - value we want to remove, in this case three, and 
the program will remove the object at index three.  
54:50 - The other option would be to run another 
remove method, only this time passing an  
54:55 - integer object of five. It has to be an integer 
object because if we were to just use five,  
55:01 - the computer would try to remove the fifth index 
of the ArrayList, which doesn't even exist.  
55:06 - By creating an integer object, we 
can ensure that when the code runs,  
55:11 - the computer knows that we want to remove 
the first instance of the number five in  
55:15 - our ArrayList. And running this will return 
true and remove the integer from our list.  
55:21 - Now, if there is no integer five in the ArrayList, 
the Remove method will simply return false.  
55:26 - Now I quite like the number five, so I'm 
actually not going to permanently remove  
55:30 - it from the ArrayList just yet. Up next is 
the get method. Now the get method is pretty  
55:37 - much the same as referencing an index for an 
array, it takes in an index location, and will  
55:43 - return back to you the value at that location. So 
example a list dot get with an argument of zero  
55:50 - would return one example a list dot get with 
an argument of two would return three, and  
55:57 - so on. The next method is the set method, which 
is how we replace elements within an ArrayList.  
56:05 - Much like the name implies, it takes 
in an index and an object as arguments.  
56:10 - And we'll set the index location of the index 
you passed in to the object you also passed  
56:16 - in. So if we wanted to set the number five in our 
ArrayList, to be four instead, so that it matches  
56:23 - nicely with the other integers, what we would do 
is call example, a list dot set, and within the  
56:29 - parentheses pass in the index location of the 
element, we want to set, in this case three,  
56:35 - and then also the object we want to replace 
at that index. In this case, the integer for  
56:42 - this method call will override the element 
at position three to be four instead of five.  
56:49 - Now, you should be really careful when you're 
using this method, because you don't want to  
56:53 - accidentally override an important element within 
the ArrayList. Next up is the clear method for  
57:00 - when you simply just hate your ArrayList. This 
is perhaps the simplest of them all. It has  
57:06 - not taken any arguments, and will simply clear 
the ArrayList deleting every element entirely.  
57:13 - Calling example a list clear on our ArrayList 
would delete all the objects within it. But I  
57:19 - don't really want to do that right now, especially 
with one more method to go. So for the sake  
57:24 - of this series, let's just keep the ArrayList 
filled with the values that it currently has.  
57:31 - The final method that we'll be covering in this 
section is a little bit different from the rest.  
57:36 - And that's the to array method, which is 
used to convert an ArrayList to an array.  
57:42 - I thought I would include this one because it's  
57:44 - really useful for combining the strengths 
and weaknesses of arrays and array lists.  
57:49 - The two array method takes in no arguments, and 
will simply convert the array list into an array.  
57:56 - Now for this to work, of course, you need to set 
it to be equal to the creation of a new array,  
58:02 - like shown on your screen now. But if it is done 
correctly, you'll end up with a brand new array,  
58:07 - which contains all of the contents that used to be 
in the old array list. You may notice though, that  
58:14 - instead of an array of integers, it's actually 
an array of objects. This mostly has to do with  
58:21 - that object oriented programming stuff we talked 
about in the beginning. But for now, it won't make  
58:26 - too much of a difference, we can still treat 
it as a regular array, printing out indexes to  
58:32 - the console, placing elements within it. Typical 
array functionality. The only thing that changes  
58:39 - is that it now contains integer objects instead 
of primitive integer types. So there they are  
58:46 - the six major functions that come with any given 
version of the ArrayList class. Having these  
58:52 - at your disposal will account for much of the 
functionality, you might use an ArrayList for  
58:57 - making them extremely valuable to now Let's now 
move on to the array list as a data structure,  
59:03 - again, we're going to be looking at its four time 
complexity equations for accessing, searching,  
59:09 - inserting and deleting. Now, if you remember back 
to the beginning of this segment, we mentioned  
59:14 - that the ArrayList is backed by an array in 
memory. And this means just like the array, it too  
59:20 - will have over one accessing power. Essentially, 
this means that when we use our get method,  
59:27 - which takes in an index location, it will 
return to us the value at the index provided  
59:32 - in instantaneous time. Now you might be wondering, 
how is this possible since the data stored within  
59:38 - an ArrayList is certainly not contiguous? 
With is actually due to a really interesting  
59:44 - reason. So interesting that before scripting the 
series, I actually had no idea was the case. So  
59:51 - because it is my series, I'm going 
to take a moment to talk about it.  
59:56 - So if we pull up our example ArrayList and memory, 
you can see that it looks a little bit different.  
60:02 - Let's break down what's going 
on. instead of storing the actual  
60:06 - objects which are contained within itself, an 
ArrayList actually stores references or pointers  
60:13 - to the locations of those objects in memory. 
So the zeroeth index based on the ArrayList  
60:19 - is stored at the 87th location in memory, 
which is currently storing the integer one.  
60:25 - Checking back to our example ArrayList, you'll 
remember that that is indeed what was stored at  
60:30 - the zero with index of the example ArrayList. And 
this goes for every element within the ArrayList.  
60:36 - The first is stored at the 91st memory 
location, the second at the 100th, and so on.  
60:43 - So as you can see, while the actual data is not 
stored contiguously the references to that data  
60:50 - are. So when we run our get command, all it 
has to do is return the value that's stored  
60:56 - at whatever the index location points towards. 
It's a bit more complicated than that, especially  
61:03 - the way that these references get stored. But 
that covers the gist of it. This is the reason  
61:08 - our ArrayList can have instantaneous accessing 
power without having the data stored contiguously.  
61:15 - Alright, tangent over, let's get back to 
big O notation, time complexity equations  
61:20 - for the ArrayList. So we know accessing is going 
to be o of one. And searching is going to be O of  
61:28 - n. This is for the same reason that arrays 
were o n. If we want to find out if an object  
61:34 - is contained within our ArrayList. And that 
object is at the last index of the array list,  
61:40 - we're going to have to search through 
each and every element within it  
61:43 - of size n to make sure because remember, we 
always go based on the worst case scenario.  
61:51 - Now inserting into the ArrayList is going 
to be O of n. Because worst case scenario,  
61:56 - if we are inserting an element at the beginning 
of the ArrayList, we need to shift every element  
62:01 - after the index we're inserting to the right 
one, just like we needed to do for the array.  
62:07 - This requires a number of operations equal 
to the size of the array, making inserting  
62:12 - O of n. Deleting is O of n for the exact same 
reason. If we want to delete the first element  
62:20 - within the ArrayList, we would then have to 
shift every element down one to save space.  
62:26 - Additionally, if we want to delete an 
object contained at the last index,  
62:30 - we have to search through the whole list 
to find it. Either way, it will be O of n.  
62:36 - Alright, so there are four time complexity 
equations, accessing his old one and searching  
62:42 - for inserting and deleting are all O of n. If that 
sounds kind of familiar, that's because these are  
62:48 - the same as the arrays time complexity equations. 
See I told you they were similar. Now this does  
62:55 - bring back up the question we posed at the 
beginning of the episode. Why even use an array  
63:00 - in the first place in comparison to the ArrayList 
the array just does not seem as useful? Well,  
63:07 - let's get into that by sizing these two data 
structures against each other mano a mano.  
63:13 - Let's compare. An array is a collection with 
a fixed size, meaning it cannot be changed,  
63:19 - whereas an array list has a dynamic size, which 
can be updated to fit the needs of the programmer.  
63:25 - Arrays can store all types of data, meaning both 
primitives and advanced types, whereas array lists  
63:31 - can only store objects, not primitives. Now this 
problem is mostly solved through the auto boxing  
63:37 - situation I talked about previously, but the 
fact still stands. Moving on an array is built  
63:43 - into most languages, meaning it doesn't have any 
methods pre made for you to interact or modify it.  
63:49 - Whereas an array list is its own class, meaning it 
comes with useful methods to help you utilize it.  
63:57 - Finally, an array is very primitive in nature and 
doesn't require a lot of memory to store or use.  
64:03 - Whereas an array list is again a class, meaning 
it requires more space and time to use than an  
64:09 - array will. Hopefully now you can see that while 
the array list is more powerful, it still does  
64:16 - have some drawbacks which make using an array 
sometimes more appropriate. In general, you want  
64:22 - to use arrays for smaller tasks where you might 
not be interacting or changing the data that often  
64:28 - and array lists for more interactive programs 
where you'll be modifying the data quite a bit.  
64:35 - So that's the ArrayList. As a review, it is a 
dynamically increasing array which comes with a  
64:41 - slew of methods to help work it as the array 
list is a hefty topic in computer science.  
64:47 - If you feel I didn't do a good enough job 
explaining some of the concepts in the video.  
64:51 - The sources used to write the script for this 
video will be linked in the description below.  
64:56 - But if not, let's move on 
to our next data structure.  
65:01 - The next data structure we'll be talking about 
is the stack. Now at this point in the video,  
65:06 - we'll be diverging from what are known as random 
access data structures, ie arrays and array lists,  
65:13 - and diving into sequential access data structures. 
What's the difference? Well, if you remember back  
65:18 - to our discussion on arrays and array lists, we 
were able to access any element within the data  
65:23 - structure by calling upon its index location. This 
would then result in the computer instantaneously  
65:29 - returning the value at that location. Each element 
was independent of the one before or after it,  
65:36 - meaning obtaining a certain element did not 
rely on any of the others contained within  
65:41 - the data structure. That basically describes 
the gist of a random access data structure,  
65:46 - one where each element can be accessed 
directly and in constant time.  
65:51 - A common non computer science example of a 
random access would be a book. Getting the  
65:57 - information contained on a certain page doesn't 
depend on all the other pages within that book.  
66:03 - And getting an element contained within a 
random access data structure doesn't depend on  
66:07 - all the other elements contained within that data 
structure. In contrast, elements in a sequential  
66:13 - access data structure can only be accessed in 
a particular order. Each element within the  
66:19 - data structure is dependent on the others and may 
only be obtainable through those other elements.  
66:25 - Most of the time, this means that accessing 
a certain element won't be instantaneous.  
66:30 - A common non computer science example of 
sequential access would be a tape measure.  
66:36 - To get to the measurement of 72 inches, you would 
first have to go through inches one through 71.  
66:43 - There's no way to just instantly get to 72 inches  
66:46 - without first breaking every law of thermodynamics 
and probably the space time continuum.  
66:51 - These are also sometimes called limited access 
data structures, because unlike random access,  
66:56 - the data is limited by having to obtain it through 
a certain way. So there you have it, random access  
67:02 - data structures, which allow instantaneous 
accessing power and independent elements  
67:07 - such as the array and array list. And then 
you have sequential access data structures,  
67:13 - which only allow accessing through a 
particular order with dependent elements.  
67:18 - For the next few segments, we'll be covering a few 
of the popular sequential access data structures,  
67:23 - such as stacks, queues, linked lists, and so 
on. Beginning of course, with the stack, we've  
67:30 - already danced around the subject long enough. So 
let's finally talk about what a stack actually is.  
67:36 - Now, the stack, by definition, is a sequential 
access data structure in which we add elements  
67:42 - and remove elements according to the 
lifepo principle. The lifepo principle,  
67:48 - which stands for last in first out, means that 
whichever element we added to the stack last  
67:54 - will be the first one we retrieve. Hence, last 
in first out, think of this as a stack of books.  
68:02 - Each time you add another book to the top, 
you do so by stacking it on top of the others.  
68:08 - Then, if we want to get a book in the middle 
of that stack, we would first have to take off  
68:13 - all the books on top of it, we can't just 
grab that book from the stack and expect  
68:17 - the entire thing not to fall in on itself 
like some sort of literature ristic Jenga.  
68:23 - The same goes for the stack as a data structure, 
we add elements to the top of the stack and  
68:28 - retrieve them by taking them off the top of the 
stack. There's only one way in and one way out  
68:33 - for the data. We can't simply access or modify an 
element within the stack all willy nilly like we  
68:40 - were able to do with the array and the array list. 
This might seem like a weakness limiting the flow  
68:45 - of data to one single point. But remember what 
I said during this segment on time complexity,  
68:51 - many of these data structures have a built in 
functionality, which gives them an edge over  
68:55 - the others. And the stack with its life. 
Oh property is a prime example of that.  
69:02 - Next up, let's talk about some of the 
methods commonly associated with the stack.  
69:07 - Now the stack class will always come with two 
methods pre built into its class, those being  
69:13 - push and pop. These are the fundamental methods 
which we use to interact and modify the stack,  
69:20 - making them extremely important to comprehend. 
In addition to those two, I'm also going to cover  
69:26 - two more methods peak and contains, which are 
both useful and can be found in the stack class  
69:32 - associated with most programming languages. Now, 
just like we had an example ArrayList let's also  
69:39 - create an example stack to help us show off these 
methods. And just like that, we're ready to roll.  
69:46 - Now push is a method which pushes an object 
onto the top of the stack. All it takes in as  
69:52 - an argument is an object to add to the stack 
and will return no value. When we do this,  
69:58 - that object becomes the form front of the stack, 
and its size is dynamically increased by one.  
70:04 - Let's start running some push commands 
on a variety of different strings,  
70:08 - you'll see that the stack slowly gets 
built. Now channel this to subscribe,  
70:16 - a series of completely random words 
seamlessly pushed onto the stack.  
70:21 - As you can see, we're only adding 
information to one spot the top,  
70:26 - there's no insert method where we can just 
jam a string into the middle of the stack.  
70:32 - Moving on, the pop method is used to remove 
an element from the top of the stack,  
70:38 - it does not take in any argument and will return 
the element that is popped off the stack back to  
70:43 - the user. Once a pop method has run, the element 
that was at the top of the stack is removed and  
70:50 - returned back to the programmer. And the element 
which was second from the top becomes the new top  
70:55 - of the stack. And so on our example stack, if we 
popped off each element, you'd see that each time  
71:02 - one of these completely random strings is taken 
from the stack and returned back to the user  
71:08 - until we are left with a sad, empty stack with no 
shameless promotion. Let's add the strings back,  
71:16 - obviously, just for the sake of the 
next two methods that we need to cover,  
71:19 - and not for continuing the free advertising.  
71:22 - So push and pop are how we add and remove the 
data within our stack. So they're fundamentally  
71:27 - the backbone of the data structure. The next 
two I want to talk about peak and contains  
71:33 - are more so used to interact with the data inside 
the stack without actually changing it. Not as  
71:39 - useful for modifying data, but still extremely 
important. The first of these is the peak method,  
71:46 - the peak method allows you to get the value at 
the top of the list without actually removing.  
71:51 - We talked about before that the only way to 
access elements in a stack was through the top.  
71:56 - And this method is simply a way to look at 
what the top value is without having to pop  
72:01 - it off. It takes in no arguments and we'll just 
return back the contents have the top element.  
72:08 - In our case, if we ran it on our example stack, 
subscribe would be returned back to the user.  
72:14 - Now if we popped off the top element and ran 
it again, two would be returned, it's that  
72:20 - you get the idea. Again, let's push subscribe 
back on at the top for educational purposes.  
72:28 - Now the final method I'll be 
covering is the contains method.  
72:32 - This one is used for searching throughout the 
stack. It takes in an object as an argument  
72:38 - and will return a Boolean of whether or not that 
item is contained within the stack. Essentially,  
72:43 - this method allows us to search through the stack 
without popping off every element until we find  
72:48 - the one that we're looking for, as the contains 
method does not modify the stack in any way.  
72:54 - So for example, example stack contains with 
an argument of subscribe would return true  
73:01 - example contains with an argument 
of this would also return true.  
73:06 - But example contains with an argument of Hello 
would return false. So there they are four common  
73:15 - stack functions which are going to be vital if 
you ever want to construct a stack based program.  
73:21 - Moving on to time complexity, for accessing the 
stack has a time complexity equation of O of n.  
73:29 - This is because in order for us to reach a certain 
element within the stack, we first have to pop  
73:34 - off every element that's above. Think of it like 
this, if we had a stack of stones, and needed to  
73:41 - get to the bottom one, we'd first have to take off 
every stone on top of it. So worst case scenario,  
73:47 - if the element we want is at the bottom of 
the stack, we first have to pop off every  
73:52 - element above it. This would require a number 
of operations equal to the size of the stack,  
73:58 - making the time complexity equation Oh event. This 
is one of the major drawbacks to using stacks.  
74:05 - with arrays and array lists, we could easily 
access any element within the data structure  
74:10 - instantaneously. And with a stack, that's just not 
possible because of the way that it's structured.  
74:16 - Searching is going to be O of n for the exact same 
reason. Worst case scenario, if we're searching  
74:22 - for an element that's at the bottom of the stack, 
we have to go through the whole thing just to find  
74:27 - it. Now, inserting and deleting make up for this 
by having time complexity equations of o of one.  
74:34 - This essentially boils down to the fact 
that using our push and pop methods  
74:38 - really only requires one operation. Since the 
data only flows in and out of a single point,  
74:45 - inserting or removing an object from that point 
can be done immediately. For the push method,  
74:50 - we simply add it to the top of the stack. And 
for the pop method. We just take it off from  
74:55 - the top. It's not rocket science. Actually, it's 
computer science but that's Besides the point,  
75:01 - there's no need to rearrange data or move elements 
around like there was for the array and ArrayList,  
75:06 - because we don't have to the data that 
we're modifying is right there on top.  
75:12 - So there you go, the time 
complexity equations for the stack.  
75:16 - Now, you might be wondering if there are even 
uses for a first in first out data structure,  
75:20 - because it seems kind of out there, I mean, 
limiting your data to a single entry point.  
75:26 - But you would actually be mistaken. stacks are 
used everywhere, both in the actual writing of  
75:31 - other code as well as in real world situations. 
In fact, one of the most fundamental processes  
75:37 - of computer science uses stacks as a way of 
keeping track of active functions or subroutines.  
75:44 - Of course, I'm talking about recursion. Now, 
I won't get into recursion too much here.  
75:49 - But basically, it's the process of 
functions repeatedly calling themselves  
75:54 - when the function calls itself, that call is 
added to a stack of processes. And when that  
75:59 - stack finally reaches what's known as a base case, 
the functions are all then popped off one by one.  
76:06 - It goes much, much deeper than that. But we don't 
have time for a full blown lesson on recursion.  
76:11 - Right now. If you want that, you can click the 
card in the top right corner of your screen,  
76:17 - or the link in the description below, which will 
both take you to that part in our introduction to  
76:21 - programming series where we cover it. Either 
way, stacks are the backbone for recursion.  
76:27 - And recursion is the backbone for a plethora 
of computer science related functionality,  
76:32 - such as traversing data structures, keeping 
track of active subroutines in code, and much,  
76:37 - much more. Some examples of stack based functions 
outside of computer programming, which you use  
76:43 - every day include the Undo redo button in word 
processors and the back paging on web engines.  
76:51 - Both use stacks. Similarly, they continually 
add tasks you've completed to a stack,  
76:56 - either web pages that you've visited or words that 
you've typed. And then when you press Undo, or the  
77:02 - back button in a web browser, all we have to do is 
pop off whatever the last task was off the stack.  
77:08 - And bam, you're right back to where you were a 
second ago. It's like magic, but better in a way.  
77:15 - As you can see, the stack has a lot of real 
world applications, both on the consumer side  
77:20 - and the client side. You interact and use stacks 
every day without even realizing it. And we'll  
77:26 - use stacks frequently as you continue along your 
computer science journey. And so by learning them,  
77:31 - you're opening up a world of opportunities. That 
concludes our discussion on the stack. To review.  
77:38 - It is a sequential access data structure in which 
we use the lifepo principle to add and remove  
77:44 - elements from and again, life O stands for 
last In First Out of next we'll be talking  
77:51 - about an equally useful data structure that 
functions very differently than the stack  
77:55 - while also working very similarly. And 
that data structure is known as the queue.  
78:03 - So now that we've talked about the stack, 
a sequential access data structure,  
78:06 - which follows the life principle, we need to 
cover the opposite, which in computer science,  
78:12 - we obviously call a queue. Now by definition, 
a queue is a sequential access data structure,  
78:18 - which follows the FIFO principle, or first in 
first out, the stack and q are very much a dynamic  
78:26 - duo when it comes to the world of comp sigh. So 
you'll notice a lot of similarities between the  
78:31 - two and the way that they're structured and 
how they work. Today, and in this segment,  
78:37 - we'll cover a lot of the same topics as we did 
with the stack, only for the queue. Now timestamps  
78:43 - can be found in the description correlating to 
these topics. But if you're sticking around,  
78:47 - let's dive deeper into what the queue actually 
is. Well, the queue like the stack is a sequential  
78:53 - access data structure, meaning we can only access 
the information contained within it a certain way.  
79:00 - Now, if you remember back to stacks this certain 
way was through the lifepo methodology, or lastin.  
79:07 - First out with the last element pushed onto the 
stack would always be the first one we popped off,  
79:14 - similar to a stack of books 
that we add to and remove from.  
79:19 - Now in contrast, the queue follows what's known 
as the FIFO methodology, or first in first out,  
79:25 - where the first element added to the queue 
will always be the first one to be removed.  
79:31 - We can think of this as a line to your favorite 
amusement park ride. Mine as a kid was always  
79:36 - the Ring of Fire. So let's just use that one 
as an example. The first one to get there,  
79:42 - assuming we don't have any cutters will always 
be the first one who gets to go on the ride.  
79:47 - The later you show up, too long do you have to 
wait. This is the strategy used for cues when  
79:53 - adding and removing objects. The first element to 
be added will also be the first one to be removed.  
80:00 - Another big difference between stacks and queues 
is the location we add and remove elements from.  
80:05 - You might remember that with the stack, we added 
and removed elements from one spot the top. With  
80:12 - the queue. However, this is different. We add 
elements, the back, also known as the tail,  
80:18 - and we remove them from the front, also known 
as the head. This allows us to make sure that we  
80:24 - 100% follow the FIFO methodology. So there's your 
background information on the queue, sequential  
80:31 - access FIFO methodology, add elements the back 
and remove them from the front. Got it? Good. Now,  
80:38 - let's dive headfirst into how we can actually use 
a queue by jumping into some common queue methods.  
80:45 - So just like the stack, we're going to 
have two methods used to add and remove  
80:49 - elements from the queue. For the stack, we added 
elements with push and removed them with pop.  
80:56 - In contrast, with a queue, we add elements 
using on cue and remove them using dq. In  
81:02 - addition to these two methods, we're 
also going to cover peak and contains,  
81:07 - which if you watched the previous segment should 
look pretty familiar. Alright, let's start.  
81:13 - on cue is the first method and the one we use to 
add elements to the tail of the queue. It takes  
81:19 - in an object to put at the end of the queue, and 
simply adds that object, we'll also increasing the  
81:24 - size of the queue by one, let's pull up an example 
queue, which can see currently has a size of zero.  
81:32 - But say we call on queue on a completely random 
string, let's say the string now, that would be  
81:39 - added to the tail of the queue, and the size would 
increase by one. Now, because there's only one  
81:45 - element in the queue, at this point, the string 
now is acting as both the head and tail for us.  
81:51 - We can of course fix that by on queueing a few 
more completely random strings. If we add the  
81:56 - string video, the size goes to two, we can add 
this and it goes to three, like makes it four,  
82:03 - you get the idea. And now we have a fully 
functional queue, as you can see, like is  
82:09 - acting as the tail being that it was the last 
string added. And now is being treated as the head  
82:15 - which makes sense considering it was the first 
string to be added. Moving on. dq is the method  
82:21 - we use to actually remove elements from the head 
of our queue. It takes in no arguments and will  
82:27 - return the element that was removed from the queue 
back to the user. So if we ran a dq command on our  
82:34 - example queue, you'd see that now is both returned 
back to the user and also removed from the queue.  
82:41 - Additionally, the size of our queue 
has been dynamically decreased by one.  
82:46 - If we run it again, video is returned and removed 
from the queue and the size goes down by one yet  
82:52 - again, you get the idea. We can keep doing this 
for every element in the queue until it's empty.  
82:59 - But the next methods we're going to talk 
about need some information to work with.  
83:03 - So for now, let's refill the queue back to its 
original four elements. The next method that I'll  
83:09 - discuss is peak. Now we've actually covered peak 
just a few moments ago in our segment on stacks.  
83:15 - But if you forget or just didn't watch that 
particular part, peek returns the object that's at  
83:20 - the forefront of our queue. It doesn't take in any 
arguments and simply returns the foremost object  
83:26 - of the queue without actually removing it. The 
key word there being without this method allows  
83:33 - you to look at the head of the queue before you 
actually be queuing. There are a multitude of  
83:38 - reasons that you might want to do this, maybe 
to make sure that the element that you're about  
83:43 - to dq is the correct one, or to check to 
see if an element you need is still there,  
83:48 - etc, etc. Whatever the case is, we can 
use the peak method to fulfill our needs.  
83:55 - If we were to run it on our example queue, you'd 
see that the string now is returned. But if we  
84:01 - dq the first two elements and run it again, 
you'll see that the string This is returned,  
84:07 - pretty simple, but extremely effective. Again, 
let's add video And now back into the queue for  
84:13 - our next and final method. That method of course, 
being the contains method, the name pretty much  
84:20 - says it all. The contains method will take an 
object and will return a Boolean of whether or  
84:26 - not the queue contains that object. running it 
on our example queue with an argument of queue  
84:32 - would return false because as you can tell, 
there is no q string contained within our queue.  
84:38 - However, if we ran it on a string, such as video, 
it would return true because as you can see,  
84:44 - the string video is indeed in the queue. And 
there they are all together now on cue dq peak  
84:52 - and contains four methods which will help you 
utilize a queue to its maximum efficiency.  
84:59 - Speaking of efficient See, that takes us perfectly 
into the time complexity equations for the queue.  
85:05 - Now accessing an element within a queue is going 
to be Oh event, let's say you had a queue full  
85:10 - of three elements. If you want the object at the 
tail, you first have to dq every element off the  
85:16 - front into the one you're looking for is the head 
of the queue, Then, and only then can you actually  
85:23 - get the value contained. Since this may require 
you to go through the entire queue of size n,  
85:29 - accessing is going to be O of n. Remember, now 
queues are sequential access data structures  
85:35 - and not random access, meaning we can just 
grab an object from the middle of the queue.  
85:40 - That's just not how things work. Searching is 
going to be O of n for the exact same reason,  
85:46 - trying to find an element contained at the tail 
of a queue requires you to iterate across the  
85:51 - entirety of that queue to check for it. So in that 
scenario, we have to check every element within  
85:56 - that queue of size n, making the time complexity 
equation O of n, inserting two and deleting from  
86:04 - a queue are both going to be an instantaneous o 
of one. This is because just like with the stack,  
86:10 - we're only ever on queuing at a single point, 
and we're only ever D queuing at a single point.  
86:16 - This means that no matter how large the size 
of the queue is, it will always take the same  
86:21 - number of operations for any magnitude 
to either insert or remove an element.  
86:27 - And there they are, in all their glory, the time 
complexity equations for the queue, you may notice  
86:33 - that they're identical to the stack, which if 
you've been paying attention is the truth for  
86:37 - most of the properties about a queue. They're very 
much a yin and yang, one in the same type deal  
86:43 - different slightly in terms of the methodology 
you use to add and remove objects from them.  
86:49 - You'll often see these two data structures talked 
about together frequently, just because of how  
86:54 - similar their functionality is. The final thing 
I want to cover on the topic of queues are just  
86:59 - some common uses for them within programming. 
What are these things actually used for?  
87:05 - And the answer is quite honestly, a lot on your 
computer. Right now. queues are being used for  
87:10 - what's known as job scheduling. The process 
through which the computer determines which  
87:15 - tasks to complete for the user. And when like 
opening up a web page or a computer program.  
87:21 - It's used many times in printers to keep track 
of when multiple printers Try to print and  
87:26 - determining whose documents get printed first. 
Heck, if you're looking for real world examples,  
87:31 - Google even uses cues and their new pixel phones 
to enable what's known as zero shutter lag,  
87:37 - in which they strategically use cues to eliminate 
the time between when you take a picture  
87:42 - and what the phone actually captures. So yeah, 
in terms of functionality, the queue can be used  
87:48 - in a variety of fields. So it's good now that you 
know what they are and how to use them. This also  
87:53 - concludes our discussion on the queue. To review 
the queue is a sequential access data structure,  
87:59 - which follows the FIFO principle, were first in 
first out to add elements to the back and remove  
88:06 - elements from the front. Up next, we'll 
continue on the sequential access data  
88:10 - structures train and talk about one of 
my personal favorite data structures.  
88:16 - Next, we'll be covering the linked list, and 
it's a good one. So strap into your seats.  
88:21 - Let's just jump into things by talking about 
the elephant in the room. What exactly is a  
88:26 - linked list? Well, to answer that a linked list 
is a sequential access linear data structure  
88:32 - in which every element is a separate object 
called a node. Each node has two parts, the  
88:38 - data and the reference, also called the pointer, 
which points to the next node in the list. Wow,  
88:45 - that is a pretty big sentence with a lot of ideas 
within it. So let's break it down part by part.  
88:53 - The sequential access part of that statement means 
that the information contained within the link  
88:58 - list data structure can only be obtained through 
a certain methodology. We talked about this in  
89:03 - our segment on the stack. If you remember, during 
that segment, we compare them to a tape measure.  
89:09 - Because Similarly, you can only get measurements 
from a tape measure through a specific method.  
89:15 - The specific method for a linked list 
will be covered a little bit later  
89:18 - for moving on. The linear part of the definition 
simply means that the information or data is  
89:24 - organized in a linear fashion in which 
elements are linked one after the other.  
89:30 - Now when we state that every element is a separate 
object called a node, this means that unlike  
89:35 - an array or an ArrayList, where every element 
is just let's say a number, each element in a  
89:42 - linked list will actually be an object which 
can have multiple attributes or variables.  
89:48 - And I won't dive too far into object 
oriented programming right now.  
89:52 - If you want a good introduction to that, you 
can check out the link in the description below,  
89:56 - which will take you to our introduction 
to object oriented programming lecture.  
90:01 - For this series, we essentially mean that the 
objects or note stored within each element of  
90:06 - our linked list will hold two separate pieces 
of information. These two pieces of information  
90:12 - come together to form the node. And these 
nodes are what make up our linked list.  
90:18 - More specifically, those two pieces of information 
are the actual data or information stored within  
90:24 - that node, and the reference or pointer to the 
next node in the linked list. The data is where  
90:30 - our strings or integers or Boolean values are 
kept the actual contents of our data structure.  
90:36 - And the other piece of the puzzle is the pointer. 
This reference or pointer points to where the  
90:42 - next node in the linked list is stored in memory. 
This helps link all of the nodes in a linked list  
90:48 - together to form one long chain of information. 
Kind of like a computer science conga line,  
90:55 - you should now be able to understand what 
a linked list is. Again, it's a sequential  
91:00 - access linear data structure in which every 
element is a separate object called a node,  
91:06 - in which each node contains both data and a 
reference to the next node in the linked list.  
91:12 - This cumulatively creates a string of nodes which 
we call the length list. Feel free to rewatch this  
91:19 - explanation if you're still confused, because I 
know it can get pretty hectic pretty fast. The  
91:24 - next thing I want to do in this section is just 
visualize how one of these linked lists is set up.  
91:30 - That way, you can actually see what I mean when 
I say nodes, error, and pointers and everything  
91:36 - else, etc. So every linked list starts with 
what's known as the head node of the list.  
91:42 - This is just an arbitrary label, which represents 
a node containing some form of data, and also a  
91:49 - reference to the next node in the linked list. For 
now, let's just store the integer one in our head  
91:55 - note. Now, since this is the only node so far in 
our linked list, the head node will simply point  
92:01 - towards a no value, which is just to say it's not 
pointing anywhere. Essentially, it's pointing to  
92:07 - nowhere and just being used as a placeholder until 
we actually give it something to point towards.  
92:13 - Let's do that by adding another node to 
the link list and store the integer to  
92:18 - inside of it. Instantly, you can see that now our 
head node points to this second node instead of  
92:24 - a null value. You'll also notice that this new 
node, which I'll call the two node points to a  
92:30 - no reference value, just like the one node used to 
do, as it is now the last node in the linked list.  
92:37 - This is a pattern you'll notice as we continue 
adding nodes. The last node in the linked list,  
92:42 - also known as the tail node will always point 
towards a null value. This is our way of telling  
92:49 - the computer that we reached the end of our linked 
list and that there are no more nodes to traverse  
92:54 - towards. Anyways, let's now create our third node 
and inside store the integer three, this node now  
93:02 - becomes the tail end points towards a North value. 
And the second node that we created, the one that  
93:08 - used to point towards a null value. Now points to 
this new node which contains the integer three.  
93:15 - Essentially, we're just adding a node to the tail 
of a linked list and then setting the references  
93:20 - of the previous node to point towards that new 
node. And if you can understand that concept, you  
93:26 - pretty much comprehend the gist of how these nodes 
interact. The two main takeaways are that every  
93:32 - node has both information and a pointer. And the 
last node in a linked list points towards a null  
93:38 - value. That's pretty much the setup for a linked 
list. Definitely more abstract and fluid than,  
93:45 - say an array or ArrayList, but hopefully not too 
complicated. Next up, we're going to be discussing  
93:52 - how we add and remove these nodes from a linked 
list. Unfortunately, for us, adding and removing  
93:58 - elements from a linked list isn't going to be as 
simple as it was with the other data structures  
94:03 - such as the stack or the queue. This has to do 
with the simple fact that we are actually able to  
94:09 - insert and remove elements easily within a linked 
list at any location. With a stack or a queue,  
94:16 - we weren't actually able to do this because the 
data could only flow in and out of a few specified  
94:22 - points. This made it so we couldn't remove an 
element from the middle of the stack or jam an  
94:27 - element in the center of a queue. The way that 
they're structured just didn't allow for this.  
94:32 - Using link lists. However, we can easily 
remove elements from the middle or  
94:37 - jam elements in the center. And so today, we'll 
be covering the different methods used to do so.  
94:43 - More specifically, we'll be covering three 
different ways to both insert and remove  
94:47 - nodes from a length list. Adding to and 
removing a node from the head, the middle  
94:53 - and the tail of a linked list. These are all 
going to revolve around those pointers we talked  
94:58 - about at the beginning of the episode. Because 
whenever we change up a node in a linked list,  
95:03 - we also have to change its pointers. And that can 
get pretty complicated pretty quickly. Luckily,  
95:09 - that's why I'm here. I've set up a basic linked 
list on your screen now with three nodes that we  
95:15 - can use to play around with. Each node has a value 
of course representing the data inside the node,  
95:22 - and also a pointer which points to the next note, 
the green and red coloring on the nodes with the  
95:28 - integers one and three, simply indicate which node 
is the head node and which node is the town node.  
95:34 - Green means head node, and red means it's the 
tail node. Now in an actual link list, these  
95:40 - pointers would be locations in memory. But for 
the purpose of this series will be representing  
95:46 - the pointers visually perfect. So let's cut 
the chitchat and just jump right into it.  
95:52 - The first method we're going to be covering is 
adding and removing nodes from the head of a  
95:56 - linked list. Lucky for us, this is pretty simple. 
To add a new node to the head of a linked list,  
96:03 - literally, all we have to do is make that new 
nodes pointer point to whatever the current  
96:08 - head node of the linked list is. By doing this, 
we simply take away the title of head node from  
96:15 - the current head and bestowed upon this new 
node that we're adding, it looks a little bit  
96:20 - something like this. Let's take a node with the 
integer zero and add it to the head of the linked  
96:26 - list. All we have to do to do this is set its 
pointer to point towards the current head node.  
96:32 - Now, you'll see that none of the other nodes have 
changed in the slightest. The only thing that has  
96:37 - changed is that this new node with integer zero 
is the head node, and it now points towards  
96:43 - the old head node. extremely simple, and the 
best part is that it works in reverse as well.  
96:50 - Removing a node from the head of 
a linked list is just as simple.  
96:54 - All we have to do is set the head nodes pointer 
to some null value. Once we do, the head node  
97:00 - will get cut off from the flow of information 
essentially removed from the linked list. If we  
97:06 - did this on our example linked list, you'd see 
that once we set the zero nodes pointer to No,  
97:11 - because it no longer points to the one node and no 
node points towards its location. It has been cut  
97:17 - off from the rest of the nodes and exiled from 
the linked list. The old head node regains its  
97:23 - position. And it says if this integer zero node 
never even exists. Moving on the next methods  
97:29 - we're going to cover are inserting and deleting 
a node from the middle of the linked list.  
97:35 - These two methods are definitely the most 
difficult of the bunch. And this is because  
97:39 - we need to insert the node in such a way that the 
pointers get readjusted accordingly without losing  
97:44 - any of the information. If we accidentally set 
the pointers wrong, or do things out of order,  
97:50 - the data could get lost forever. But luckily, 
I'm here to teach you how to do it the right way.  
97:56 - We'll start with adding a node 
to the middle of a linked list.  
98:00 - Adding to the middle of a linked 
list is a two step process.  
98:03 - We first make the pointer of the new node point to 
the node after the location we want to insert at.  
98:10 - Then we set the node before the location we want 
to insert that to point towards the new note. So  
98:17 - if we wanted to insert a node with the double 
value 1.5 after the node containing the integer  
98:23 - one, but before the node containing the integer 
two, what we would first do is set the new nodes  
98:30 - pointer to point to the node containing 
the integer two, then we would make the  
98:35 - node containing the integer one point towards 
our new node which contains the double 1.5.  
98:41 - By adjusting the pointers of the nodes before 
and after the location, we want to insert that  
98:47 - we can strategically jam this node in the correct 
place without moving or modifying the entire list.  
98:54 - As you can see, now we have a linked list of 
length for where the link has not been broken  
98:59 - and is also still contiguous. Removing a node 
from the middle of a linked list is even simpler.  
99:06 - All we have to do is make the pointer of the node 
previous to the one we're removing to now point to  
99:12 - the node after the one removing. Then if we set 
the pointer of the node we want to remove equal  
99:18 - to ignore value, we again cut the node off from 
the linked list and it is removed. Simple as that.  
99:25 - So following these steps, if we now wanted to 
delete our 1.5 node, we'd make the one node  
99:31 - again point towards the two node instead of 
the 1.5 node. Then if we delete the pointer  
99:38 - of the two node, by setting it equal to null, it 
gets cut off from the flow of the linked list.  
99:44 - No changes are externally made to the rest 
of the list. Just one lonely node removed.  
99:51 - The final type of insertion and deletion we'll be 
covering is inserting to and deleting from the end  
99:56 - of a linked list. Doing this simply requires To 
modify the tail node of the length list, the one  
100:03 - which currently points towards some null value. 
for adding a node to the tail, you simply make  
100:09 - the current tails pointer, which is currently set 
to no to point towards the node you want to add.  
100:15 - So if we wanted to add a node with the integer 
four, we would just make the three node point  
100:20 - towards this new node. Then by setting the four 
nodes tail to point towards No, we've made this  
100:27 - new four node the tail of the linked list, and the 
old tail node with integer three now points to the  
100:33 - new tail. Removing the tail of a linked list is 
just as simple. If we want to remove the tail,  
100:41 - we just set the previous tail to point towards 
a null value instead of the current tail.  
100:46 - This leaves the current tail node with no 
connection to the linked list, isolating  
100:51 - it from the pack. Doing this on our list would 
look like making the three node point towards No.  
100:58 - And now because no node now points to our tail 
node continuing integer four anymore, it gets cut  
101:05 - off from the linked list and essentially removed 
making the old tail node, the one containing the  
101:10 - integer three, the current tail node once again. 
And so after a bit of pointer readjustment,  
101:18 - hopefully now you can understand the pseudocode 
behind inserting and removing elements from a  
101:23 - linked list with ease. Up Next are the time 
complexity equations for a linked list.  
101:29 - Now accessing an element within a linked list is 
going to be O of n. This is because linked lists,  
101:34 - again are sequential access data structures. 
This should be all too familiar if you watch  
101:40 - the sections on stacks and queues. But 
if not, let's just use a simple review.  
101:45 - sequential access data structures can only be 
accessed through a particular way, meaning we  
101:51 - can't get any element we want instantaneously. 
resulting from this is the stipulation that if we  
101:56 - want to get a certain element within a link list, 
we need to start at the beginning of the list  
102:01 - and cycle through every node contained within it. 
Before we can finally access the one that we want.  
102:07 - We do this by using the pointers 
as a little map for the computer.  
102:12 - First go to node one node one's pointer 
gives you the location of node two in memory.  
102:18 - And node twos pointer will take you to 
the memory location, which contains the  
102:22 - node with the information that you want. It's 
like a computer science treasure map and a way  
102:28 - for a linked list of size n. This means 
you could have to traverse the entire  
102:32 - linked list before finding your information 
making it's time complexity equation O of n  
102:39 - searching is O of n for the exact same reason, we 
check a node and if it's not the one that we want,  
102:46 - we use that nodes pointer to take us 
to the next node and check that one.  
102:50 - We do this for every node until we either find 
the node containing the value we want, or get  
102:56 - to a node which points towards no indicating 
that we've reached the end of the linked list  
103:00 - that the value that we're searching for just isn't 
contained within that particular length list.  
103:07 - Inserting and deleting from a linked list 
is a little bit complicated. Since linked  
103:12 - lists usually only store the head and sometimes 
the tail nodes location in memory permanently.  
103:18 - If we want to insert or delete an element 
at the beginning or end of a linked list,  
103:23 - we can do so instantaneously using the methodology 
we talked about previously. However, if we want  
103:30 - to insert a node within the linked list, things 
become a little bit more complicated. In order to  
103:35 - insert a node within the link list, we must first 
traverse to the location we want to insert it.  
103:41 - This means following that treasure map until 
we reach the insertion point. And then and  
103:47 - only then can we actually follow the instructions 
to insert or delete a note. So depending on how  
103:54 - and where you want to insert or delete a node at 
its time complexity equation will be either O of n  
104:00 - or o of one a little confusing, yes, but necessary 
dimension for when it inevitably comes up.  
104:09 - So in review, accessing searching, and sometimes 
inserting and deleting are all going to be O of n.  
104:16 - And other times inserting and 
deleting will be instantaneous. Cool.  
104:20 - Now let's finish up by talking about some 
real world applications of the linked list.  
104:25 - Now, something I haven't talked about before, but 
would be scolded by people in the comments for  
104:30 - not mentioning is that linked lists can actually 
be used in the backing of other data structures.  
104:36 - What do I mean by this? Well, basically, 
we can use linked lists to make stacks,  
104:41 - queues and some other data structures we haven't 
talked about. This is in the same vein as earlier  
104:47 - when I mentioned that the ArrayList uses the 
array as a backing support system in memory.  
104:52 - Now this goes a little bit above an introductory 
series, but it's one of if not the most important  
104:58 - uses of a linked list in computers. Science. 
So I thought I'd mentioned it here. Basically,  
105:03 - because of the way that it's structured. Having a 
stack or queue use the nodal methodology that we  
105:09 - talked about during this episode, which comes with 
the link list to be the back end of it structure  
105:14 - makes a lot of sense. If you're curious, I'll have 
an article linked below explaining this idea in  
105:20 - better detail. I just thought it was a little bit 
above the complexity for an introductory course.  
105:26 - Now, that's not to say link lists can't be useful 
in other ways. A queue on Spotify, for instance,  
105:32 - where each song in the queue doesn't contain 
just an integer or a string, but an entire  
105:37 - song with WAV data, a title, a length, etc. Then, 
when the track is done, it automatically points  
105:45 - to the next song in the playlist. Another 
example could be a photo viewing software  
105:50 - where each node is an image, and the pointers 
simply point to the next photo in the list.  
105:56 - Like I said, the main functionality of a linked 
list might be to back other data structures,  
106:01 - but it also has tons of uses 
in other areas of expertise.  
106:06 - In Review, a linked list is a sequential access 
linear data structure, in which every element  
106:12 - is a separate object called a node containing 
two parts, the data and the reference pointer  
106:19 - which points to the next node that comes in the 
linked list. These pointers allow us to easily  
106:25 - shift around each node, adding or removing it 
without having to move massive amounts of data  
106:30 - like we would have to do in the case of 
an array or some other data structure.  
106:35 - One thing I haven't mentioned yet about linked 
lists is that this pointer system does come with  
106:40 - one big drawback. With a normal linked list, we 
can only ever go forward with our pointers never  
106:47 - backwards from the computer's eyes. Once we follow 
a pointer to a certain nodes, location and memory,  
106:54 - there's no way to go back or undo to the previous 
one. Much like a shark, we can and only will ever  
107:01 - go forward. This problem is fixed however, 
through the use of a data structure known as  
107:06 - the doubly linked list, which coincidentally is 
the subject of the next segment in this video  
107:12 - smallworld All jokes aside, Next up, we're 
going to be covering the doubly linked list  
107:18 - so strap into your seats. A doubly linked list 
is almost exactly the same as a linked list.  
107:24 - That is to say it's a sequential access data 
structure which stores data in the form of nodes.  
107:30 - Except with doubly linked lists. There's one 
small difference. With the doubly linked list,  
107:36 - we're able to traverse both forwards to 
the next node in the list and backwards  
107:40 - to the previous node in our list. Again, 
using pointers. How? Well let me explain.  
107:46 - With regular link lists. Each element was 
a node composed of both a data section and  
107:52 - then a pointer which would take you to the 
memory location of the next node in the list.  
107:57 - Using this, we were able to traverse a 
linked list easily to search for information,  
108:02 - access data within a link list, or add and 
remove nodes from within the list with ease.  
108:08 - A doubly linked list simply builds upon this 
by also having a pointer which points to the  
108:13 - previous node location in memory. It's an undo 
button of sorts, which allows you to fluidly go  
108:18 - through the link list in either direction, instead 
of just limiting yourself to go in one direction.  
108:25 - This is great since it allows us 
to jump around the linked list and  
108:28 - have a lot more flexibility when 
it comes to modifying information.  
108:33 - Now because this can get pretty confusing 
very quickly, I want to do a visualization.  
108:38 - Before that though, let's use some lingo to help 
consolidate the terminology that I'll be using.  
108:43 - When I refer to a nodes. Next, I'm 
referring to that particular nodes  
108:48 - pointer which points to the next object in the 
list, whether that be another node or no value.  
108:54 - Similarly, when I refer to a nodes previous 
abbreviated to prevx, on the vigils,  
109:01 - I'm talking about its pointer which points 
to the previous object in the linked list,  
109:05 - again, either another node or no value. Doing 
this just helps keep things a little bit more  
109:11 - simple because as you'll see having both 
a previous and a next pointer makes things  
109:16 - a little bit more complicated. Now on to what 
these doubly linked lists actually look like.  
109:22 - Just like with a regular linked list, every doubly 
linked list is going to start with a head note.  
109:28 - Since it's the first node in the list, both its 
previous pointer and its next pointer will point  
109:34 - towards a null value. This is of course, because 
it can't point to information which isn't there.  
109:40 - We can fix this though by adding another node. 
Once we do, you can see that a few things have  
109:45 - changed. The head nodes next pointer now 
points towards this new node instead of No,  
109:51 - the new nodes previous pointer now points to the 
head node. And the new nodes next pointer now  
109:56 - points towards a null value. As you can see, it's 
Little bit more complicated than adding a node to  
110:02 - a regular link list in which we only had to worry 
about one set of pointers as opposed to two,  
110:07 - but still manageable. Let's add one 
more node for demonstration purposes.  
110:12 - And you'll see the same thing happens again. The 
second node now points to this new third node.  
110:17 - And this third node gets initialized with 
a pointer which points both to the previous  
110:22 - second node and also forward to some null 
value. You can see that with any two nodes,  
110:28 - the next pointer of the first and the previous 
pointer of the second come together to form sort  
110:34 - of a cyclical connection which ties the two nodes 
together. Then at the head and tail of the list,  
110:40 - there's a connection which ties them both 
towards some no value. Most of this should  
110:45 - be common sense, but it's still good to go over. 
doubly linked lists are a little scary at first,  
110:51 - but once you break them down, you realize that 
they're just an evolved form of linked lists.  
110:57 - of next we're going to be talking about adding 
and removing nodes from a doubly linked list.  
111:02 - Again, using the three different methods 
that we talked about in the previous segment,  
111:06 - adding and removing from the head, the middle, 
and the tail. I've also set up a basic doubly  
111:12 - linked list with three nodes containing three 
strings to help us out with this next segment.  
111:18 - Finally, just like the last segment, the green on 
top of a node signifies that it's the head node,  
111:24 - and the red signifies it's the tail node. Alright, 
with all that being said, let's get into it.  
111:31 - Now adding to the head of a doubly 
linked list is quite simple.  
111:35 - The first step is to take our new node that we 
want to insert and set its previous to no second,  
111:41 - we set the new nodes next to point towards the 
current head node of the linked list. Then all  
111:47 - we do is set the current heads previous to point 
towards this new node instead of a null value,  
111:52 - and we're set to go. Doing this rearranges the 
pointers in such a way that the new node we want  
111:58 - to add becomes the head of the doubly linked list. 
Pretty simple. So if we wanted a new node with  
112:04 - the string, aim to be the head of our doubly 
linked list, we would just follow the steps.  
112:10 - First, we set the aev nodes next to point towards 
the atom node, then we set its previous to point  
112:16 - towards the null value. And finally, by setting 
the atom nodes previous to point towards the AV  
112:22 - node, we have successfully added that node to 
the head of the list making it the head note.  
112:29 - Removing a node from the head is even simpler, we 
first set the head nodes next to point towards a  
112:35 - null value. And then by setting the second nodes 
previous to also point towards a null value,  
112:42 - we can easily remove the head node from the 
list. This is how it would look in our example,  
112:48 - we'd first set the aid nodes next to no, then we 
would set the atom nodes previous to also be no  
112:55 - and then we're done. Now because the aid 
node doesn't have anything to point towards,  
113:01 - nor does it have anything pointing towards 
it, it will be deleted from the list.  
113:07 - The Atom node will regain its position as 
the head node and the program will carry on.  
113:13 - Up next is inserting and deleting from 
the middle of a doubly linked list.  
113:17 - This is where things get really tricky, 
so make sure you're paying attention.  
113:22 - To insert a node into the middle of a doubly 
linked list, we follow a three step process.  
113:28 - The first step is to take the node we want to add 
and set its previous to point towards the node  
113:33 - previous to the position we want to insert at. 
Then we take that same node the one we want to  
113:39 - add. Instead it's next pointer to point towards 
the node after the position we want to insert.  
113:46 - Finally, we set the next on the node at the 
position before the one we're inserting, and  
113:50 - the previous on the node after the position we're 
inserting to both point towards this new node.  
113:57 - That is a lot of words, some of which might not 
even make sense to you. So let's open up the space  
114:02 - on our screen real quick and do an example with 
our list. Let's take a new node with the string  
114:09 - Chris and add it between the Adam node and 
the bed node by simply following our steps.  
114:16 - First, we want to set the new nodes previous to 
point towards the node previous to the position we  
114:21 - want to insert at. This means setting the Chris 
nodes previous to point towards the Adam node.  
114:26 - Simple enough. Second, we set the new nodes next 
to point towards the node after the position we  
114:32 - want to insert that this entails setting 
the Chris nodes next to point towards Ben.  
114:38 - So far, so good. Then the last step is to set the 
node on the next before we're inserting and the  
114:45 - previous on the node after we're inserting to both 
point towards this new node. So in this case, the  
114:52 - atom nodes next and the Ben nodes previous both 
get set to the new Chris node. This completes the  
114:59 - addition into list. Obviously, this is a little 
bit more complicated than inserting or deleting  
115:04 - from the head. But as you can see, we've now 
inserted this new node into its rightful place.  
115:10 - It may be hard to see since the pointers are a 
little bit messy, but the flow of the list has  
115:15 - remained constant without any breaks in the 
pointers, which is the most important part.  
115:21 - Removing a node from the middle of a doubly 
linked list is also a three step process.  
115:26 - First, we set the node before 
the one we want to remove  
115:29 - next to point towards the node after the one 
we want to remove. Then we set the node after  
115:35 - the one we want to remove the previous two point 
towards the node before the one we want to remove.  
115:41 - The final step is to set both pointers of the 
node we want to remove to point towards a null  
115:46 - value. Again, a little complicated, so let's 
take it to our example list and try it out.  
115:53 - Let's delete the Chris note just for 
the sake of keeping things consistent.  
115:57 - Following the steps that we've laid out, we 
would first set the next of the node before  
116:01 - the one we want to remove to point towards 
the node after the one we want to remove.  
116:06 - So in this case, we said the atom nodes 
next to point towards the Ben note,  
116:11 - essentially skipping over the crisnet. Then 
we set the previous of the node after the one  
116:16 - we want to remove to point towards the 
node before the one we want to remove.  
116:21 - So we set the bend node to point towards the 
atom node, again skipping over the Chris node.  
116:27 - Finally, we have to set the Chris nodes 
pointers to point towards null values.  
116:33 - Now that we've done this, because no 
nodes point towards it, and it doesn't  
116:37 - point towards any nodes, the Criss node 
gets deleted from the doubly linked list,  
116:42 - the list is back to its original form with no 
breaks or errors. Adding a node to the tail of  
116:48 - a doubly linked list is also a three step process. 
Step one entails setting the next pointer of the  
116:55 - current tail to point towards the new node, we 
want to become the tail. Step two is setting  
117:01 - the previous of the new node that we're adding 
to point towards the current tail of the list.  
117:06 - And step three is making the new nodes next point 
towards a null value. Again, let's do an example  
117:12 - where we add a new node containing the string 
Ethan to the tail of the doubly linked list.  
117:18 - Following our steps, we first set the next 
pointer of the current tail, the coral node  
117:24 - to point towards the Ethan node. Then we make 
the Ethan nodes previous point towards Carl,  
117:30 - and it's next to point towards an O value. And 
we're all set. Ethan has been successfully added  
117:36 - as the new tail of the list in three quick 
and easy steps. Removing from the tail of  
117:41 - the list is even easier and only requires two 
steps. We first set the tail nodes previous to  
117:47 - point towards No. And then we set the second 
to last nodes next to also point towards No.  
117:54 - On our example list, it looks like this. We first 
set the Ethan nodes previous to point towards No.  
118:00 - Then we set the coral nodes next to also point 
towards No. This isolates any pointers going to  
118:07 - or from the Ethan node and deletes it from the 
list making the coral node the tail once again.  
118:13 - adding and removing information from a doubly 
linked list might sound like a hassle. But  
118:18 - remember, we only have to use this pseudocode to 
program these functions once in whatever language  
118:23 - we're implementing them in. And then we're 
able to reuse them an infinite amount of times.  
118:28 - Now for time complexity equations, since the 
doubly linked list is really just an extension  
118:34 - of the linked list, it's time complexity 
equations are going to be exactly the same  
118:38 - as they were for the link list. And for the 
exact same reasons, O of n for all four cases  
118:44 - and sometimes over one for both inserting and 
deleting. If you didn't watch the linked list  
118:49 - segment and are wondering how we arrived at these 
equations, you can check the description for a  
118:54 - timestamp which will take you to the discussion 
we had on linked lists time complexities.  
118:59 - Finally, in terms of doubly linked lists, I just 
want to talk about some of the uses for a doubly  
119:04 - linked list because there are a ton. The back 
and forth functionality of a doubly linked list  
119:10 - lends itself to be implemented in a lot of stack 
like functionality, ie cases where you want to  
119:15 - go back and forth between information that you're 
storing for the user. Some examples of this could  
119:21 - be the browser caches which allow you to go 
back and forth between webpages, the Undo redo  
119:27 - functionality in many programs, applications which 
allow you to utilize an open recent functionality.  
119:34 - The list goes on and on. Basically any case in 
which you need to store a list of objects with  
119:39 - multiple attributes, a doubly linked list 
is going to be a safe bet to get it done.  
119:46 - linked lists and their evolved 
form and the doubly linked list  
119:50 - are a great way to store information because 
of the adaptability of the nodal structure.  
119:55 - Since we're not working with raw information 
like primitive types, and we're storing All  
120:00 - information inside of a shell, it makes it 
a lot easier to move the information around.  
120:06 - This combined with the pointer system allows for 
non contiguous but still fast operating speed,  
120:11 - making these two data structures a 
staple in computer science. Up next,  
120:16 - we'll be covering dictionaries. And with that a 
little mini lesson on hash tables. This is the  
120:21 - last of what I refer to as the intermediate data 
structures at the beginning of the lecture, and is  
120:27 - honestly personally one of my favorites. So let's 
not wait any longer and just jump into things.  
120:33 - Before we get too far, though, we should probably 
clarify that when we say dictionary, we're not  
120:38 - referencing that thick book you probably have 
lying around your house and haven't used in years.  
120:43 - Actually, dictionaries are one of the most 
abstract data structures which exist in computer  
120:47 - science and can be used for a variety of purposes. 
Another thing I'll clarify really quickly  
120:52 - is that dictionaries are also sometimes called 
both maps and associative arrays by the computer  
120:57 - science community. This moreso has to do with 
language specifics and preferences and not any  
121:04 - functional differences, since all of them work in 
almost identical ways. But for this series, we're  
121:09 - going to be referring to them as dictionaries, 
just to keep things simple and organized.  
121:15 - Now a dictionary in computer science, by 
definition is an abstract data structure, which  
121:20 - stores data in the form of key value pairs. This 
means we have two main parts to each dictionary  
121:27 - element, the key and the value. Each value within 
a dictionary has a special key associated with it.  
121:34 - And together they create a pair which is 
then stored in the dictionary data structure  
121:39 - as an element. Think of a key value 
pair like a social security number.  
121:44 - Each social security number is a key, 
which is then paired with a value,  
121:49 - that value being an individual person. These 
social security number key value pairs then  
121:55 - come together to form a dictionary of every 
human being living in the United States.  
122:01 - This is very different from many of the data 
structures we've talked about previously,  
122:05 - because we index dictionaries using 
these keys instead of a numerical index.  
122:11 - For example, with an array, we would index each 
element within the data structure according to a  
122:16 - numerical value, which started at zero and ran the 
length of the array. With a dictionary, however,  
122:23 - we index each element by using its key instead 
of some arbitrary integer and obtain information  
122:30 - through that index instead. So what exactly 
are these key value pairs going to look like?  
122:36 - Well, they can be just about anything. The keys 
and a key value pair can be any primitive data  
122:41 - type that you can think of, we can have a 
dictionary which has integers as the keys,  
122:46 - one with strings as the keys, one with doubles 
as the keys, there's really a lot of flexibility.  
122:53 - As for the values, but we have even more 
flexibility with those. We can have keys  
122:58 - and our dictionary correspond to pretty much 
anything, strings. Sure. Billions, of course,  
123:05 - another dictionary with its own set of key value 
pairs, you can even do that too. This allows us to  
123:11 - have tons of combinations in the way that we store 
our data, making dictionaries extremely powerful.  
123:18 - as powerful as they are, though, there are two 
extremely important restrictions that we have  
123:22 - to cover when it comes to dictionary. And they 
are as follows. Every key can only appear once  
123:29 - within the dictionary, and each key can only 
have one value. Let's start with the first one.  
123:35 - Each key has to be unique and cannot 
be replicated, duplicated, cloned,  
123:40 - copied or anything else that would cause there 
to be two keys of the same value in a dictionary.  
123:46 - Think of it like this, when you create a key value 
pair, the computer creates a little locked box in  
123:52 - memory to store the value, then the computer 
spends days and weeks creating a customized  
123:58 - handcrafted one of a kind key that 
corresponds directly to that locked box.  
124:04 - This key cannot be replicated in the slightest. 
And that's the same mindset you should use when  
124:09 - working with dictionaries, no two keys can 
be the same. If you were to try and make  
124:15 - a dictionary with too similar keys, you 
would be thrown in error by the computer.  
124:20 - The second stipulation is that each key can 
only have one value. Think back to our custom  
124:26 - key example. It wouldn't make sense for this one 
of a kind key to be able to open multiple boxes.  
124:32 - The same is true for our keys and computer 
science. They can only correspond to one value.  
124:39 - Now one rule that we don't have to follow is 
that there can be duplicates of values within  
124:44 - a dictionary. Meaning we can have two separate 
keys both point towards equivalent values. As long  
124:50 - as the keys are different. The computer does not 
care. Alright, now that we know what dictionaries  
124:56 - are and how they work, let's jump into the time 
complexity. For a dictionary, or at least try to,  
125:03 - let me explain. Now for a dictionary, the time 
complexity equations are a little bit funky.  
125:09 - Previously, we talked about linked lists and how 
they are sometimes used as the backing of other  
125:14 - data structures in memory. For example, a stack 
might implement the linked list structure as its  
125:20 - way of storing information. Well, the most common 
way to initialize a dictionary is to implement it  
125:26 - with what's known as a hash table. Hash tables are 
a little more advanced than the content I wanted  
125:32 - to cover in this series. But they are a huge part 
of dictionaries, especially when it comes to time  
125:37 - complexity. So we're going to give a little mini 
lesson on them today. Doing this will help you  
125:43 - better understand the time complexity equations 
for a dictionary, as well as give you some  
125:47 - background on Advanced Data structures. Alright, 
let's begin with a little thought experiment.  
125:54 - Imagine this scenario for a second. say we 
have a dictionary which contains a few key  
125:59 - value pairs like shown on your screen. Now, 
let's just assume that to store all this  
126:05 - information in the computer's memory, we use an 
array like how it is shown on your screen now,  
126:10 - with a position that a cell is in the table 
also corresponds to the key and the key value  
126:16 - pair stored in our dictionary. So the zero with 
cell would contain a key with the integer zero,  
126:22 - which leads to a value. The fourth cell 
would contain a key with the integer four,  
126:26 - which leads to another value, and so on. Any cell 
which we don't have a key for is empty, or what  
126:33 - the computer scientists refer to as nil. Why does 
this not work? Why can't we just use an array like  
126:40 - this to back our dictionary in memory, since it 
looks like actions such as accessing and searching  
126:45 - would run in constant time, since all the keys 
correspond to their table values in memory,  
126:50 - all we'd have to do to get a value is simply 
reference the key at the index we're looking for?  
126:56 - Well, this actually isn't the case, because it's 
based upon the simple assumption that the size of  
127:01 - the array is not too large. Sure, this might work 
great for this dictionary where we have 10 or so  
127:07 - elements. But let's say instead of 10 elements 
evenly spaced out, we want to have one which has  
127:12 - 10 values, which are sporadically placed from one 
to a billion. We want to keep the keys as being  
127:18 - in the same index position as their values, so we 
can know exactly where they are and reference them  
127:24 - easily. But by my count that is 999,999,990 
nil values, just taking up space in memory.  
127:35 - And that is not good whatsoever. There's 
got to be a better way to do things,  
127:39 - right? Well, this is where our hash tables come 
in. Hash tables are fundamentally a way to store  
127:46 - this information in such a way that we're able 
to cut down the amount of nil values, while also  
127:51 - allowing for the information to be stored in such 
a way that it is easily accessible. Basically,  
127:57 - by using hash table, we'd be able to store these 
10 keys in our dictionary, which range from one  
128:03 - to a billion in a table with only 10 elements, 
while also being able to keep constant time.  
128:10 - How is this possible? Well, through the use of 
a trick program is used known as hash functions.  
128:17 - A hash function will take all the keys 
for a given dictionary and strategically  
128:22 - map them to certain index locations in an array, 
so that they can eventually be retrieved easily.  
128:28 - Essentially, by giving a hash 
function both a key and a table,  
128:33 - it can determine what index location to store 
that key add for easy retrieval later. These  
128:39 - hash functions can be pretty much anything which 
takes in a value and returns an index location.  
128:45 - The goal of a good hashing function is to take in 
a key, whatever that may be, and reliably place it  
128:51 - somewhere in the table so that it can be accessed 
later by the computer. So with that being said,  
128:58 - let's just jump into an example. Because I know 
a lot of you might be confused. Let's say we have  
129:03 - our dictionary which contains keys in the form of 
integers from one to a million by a factor of 10.  
129:10 - So the keys are 110 101,000 10,000 100,000, and so 
on. A good hash function for this data set would  
129:19 - be to take the key divided by itself, and then 
multiply the result by the number of digits in  
129:25 - the key minus one. So to find out where to store 
the value corresponding to the 1 million key,  
129:32 - we would take a million divided by itself, which 
yields one and then multiply that by the number  
129:38 - of digits in that integer minus one, in this 
case, seven minus one or six. This means we'd  
129:45 - store the 1 million key at index location six. 
If we do this for every key in the dictionary,  
129:52 - we can see that each key in the key value pair 
is stored at some index from zero to nine.  
129:57 - We've consolidated the 10 keys for One to 
a billion into 10 index lots instead of a  
130:02 - billion. A pretty good improvement in my opinion, 
you might think this is a little over complicated  
130:09 - for storing 10 elements. But remember, sometimes 
we might be working with 1000s of keys at a time,  
130:15 - ranging in the billions, or even strings 
as keys, which is a whole other story.  
130:21 - Now the best part about these hash functions, 
let's say that we now want to get the value which  
130:25 - is paired with a 1 million key, all we need to 
do is put 1 million back into our hash function,  
130:32 - and it will tell us where the value tied 
to that key is stored at within the table,  
130:36 - in this case at the sixth index. Doing this 
allows us to pretty much instantaneously find  
130:42 - where any key value pair is stored at within 
the table. All of this information may be  
130:48 - pretty confusing. If you still aren't 100%, I 
would recommend watching this part over again.  
130:54 - However, in layman's terms, and for 
this series, all I want you to be able  
130:58 - to comprehend is that dictionaries are built 
upon these hash tables. And the keys in our key  
131:04 - value pairs are stored in these hash tables and 
indexes which are determined by hash functions.  
131:10 - And if you can understand that you've got a 
pretty good base for understanding hash tables.  
131:16 - Now the final thing I want to talk to 
you guys about in regards to hash tables,  
131:20 - and the reason dictionary time complexity 
equations are so funky has to do with one  
131:25 - small problem. What happens when we run two 
different dictionary keys into a hash function,  
131:31 - and the computer tells us to store them at the 
same index location. For example, let's say we  
131:38 - have two dictionary keys with the strings, Steven 
and Shawn. And when we run both of them through  
131:43 - our hash function, the computer instructs us to 
store both of them at the ninth index location,  
131:49 - this should be impossible. How are we supposed 
to put both of them at index location nine.  
131:56 - This little problem is what's known as a hash 
collision and can be solved one of two ways,  
132:01 - either open addressing or closed addressing. With 
open addressing, we just put the key in some other  
132:08 - index location separate from the one returned to 
us by the hash function. This is usually done by  
132:14 - looking for the next nil value in the table, 
ie the closest location which contains no key.  
132:21 - So with our example, the key Steven would 
get hash to the index location nine, mostly  
132:27 - because it's a better name. And the inferior 
key Shawn would get stored at index location 10,  
132:33 - because it's the closest open location available. 
This does make it harder to interact with the data  
132:39 - in our dictionary later on, and can lead to 
problems if we eventually have a value which  
132:44 - hashes to the index location 10, which is why 
computer scientists developed closed addressing.  
132:52 - Closed addressing uses linked lists to chain 
together keys that result in the same hash value.  
132:58 - So the keys Steven and Shawn would get stored 
together in a linked list and index location nine.  
133:05 - The main drawback to this is that whenever we 
want to interact with the value stored within a  
133:09 - key value pair for either Steven or Shawn, we end 
up having to look through the linked list for that  
133:15 - piece of data that we want. If there are 200 keys 
hash to one index, that's 200 keys, we might have  
133:21 - to search through to find the one that we want. 
And that's not very good. And with that concludes  
133:27 - our mini lesson on hash tables. Now we can get 
back into dictionaries, and finally talk about  
133:32 - their time complexity equations. Now remember back 
to the segment on time complexity really quickly.  
133:39 - Back then I mentioned that for 
these time complexity equations,  
133:43 - we generally measure a data structure based 
on its worst case scenario. But when it comes  
133:49 - to dictionaries, if we are to assume the worst 
case scenario, things get a little outrageous,  
133:55 - we basically end up assuming that our hash 
function makes it so that every key value pair  
134:00 - ends up in the same index, meaning that each 
of our keys gets stored in a linked list,  
134:06 - assuming that closed addressing is 
being used, then worst case scenario,  
134:10 - we have to assume that every operation functions 
how it would for accessing searching for inserting  
134:16 - or deleting from a length list, which, 
if you remember is O of n for all four.  
134:22 - This is of course preposterous and would probably 
never happen with the bare minimum decent hash  
134:28 - function. Which is why in addition to the worst 
case, scenario, time complexity equations, I'll  
134:33 - also go over the average time complexity equations 
for these four operations. Now Lucky for us,  
134:39 - they're all going to be over one. This has to do 
with these hash functions we talked about before.  
134:45 - To access search for insert or delete a key value 
pair from our dictionary. All we need to do is run  
134:52 - that key through our hash function, and it will 
tell us what index in the hash table to go to in  
134:57 - order to perform that operation. There's no time 
wasted looking through preset indexes because we,  
135:04 - the programmer generate the indexes ourselves. And 
that's the power of the hash table in the flesh.  
135:11 - Overall, dictionaries are a very useful data 
structure when it comes to computer science  
135:15 - for a few reasons. They differ from the rest 
and quite a few big ways. The fact that they  
135:21 - don't use a numerical index to retrieve values, 
rather a preset key, the notion that those keys  
135:27 - and the key value pairs can be a range of types 
from strings to integers, to chars, and so on the  
135:33 - implementation of the hash table, which allows 
for super quick utilization that is sure to  
135:38 - come in handy in any program that you write. In 
conclusion, and overall, the dictionary is just  
135:44 - a solid data structure that can be used to fill 
in the gaps in your data structures knowledge.  
135:49 - And with that, we have now completed all of 
the intermediate data structures if you will.  
135:54 - Next, we'll be moving on to trees and tree based 
data structures. This is the point in this series  
136:00 - where we will be dropping our discussion on time 
complexity equations, because if you thought that  
136:05 - the dictionary time complexity equations were 
complicated with hash tables and hash functions,  
136:10 - trees, and there are many variants only further 
complicate things to the point where I no longer  
136:15 - feel comfortable including them in an introductory 
course, we might mention that certain tree based  
136:21 - data structures are good for accessing or 
searching. But as for discussions on all four  
136:26 - time complexity equations, dictionaries are where 
we leave things. With that being said, let's move  
136:33 - on to the final few data structures as we round 
out this introduction to data structures series.  
136:40 - Next up on our list of data structures 
is the tree. No, not those big green  
136:45 - things that inhabit the outside world and 
that you can climb on and get fruit from,  
136:49 - rather the computer science data structure, 
which is way more fun in my opinion.  
136:54 - Now, before getting into trees, we need 
to talk about data structures in general.  
136:59 - Every data structure we've covered up until this 
point has been stored linearly. arrays, stacks,  
137:04 - linked lists, all of these had a definitive start 
and end to their data, and you could point them  
137:10 - out easily. Even the dictionary could be laid 
out in such a way to be represented linearly.  
137:16 - Everything was so nice and neat and easy to 
visualize. On the other hand, trees store data  
137:22 - hierarchically as opposed to linearly. What does 
that even mean? Well, besides being an extremely  
137:28 - hard word to pronounce, it's an alternative way 
to store data that we're going to be using for the  
137:33 - remainder of this series. It's a little hard to 
visualize. So I'm going to start with an example.  
137:39 - The most common real world example of 
hierarchical data would be a family tree,  
137:44 - each person would be an element of the family 
tree, and connections wouldn't be based on  
137:48 - a simple linear fashion. Rather, connections 
would be more abstract and can lead to multiple  
137:53 - paths or branches, instead of just a single data 
point. Each generation is ordered on a hierarchy,  
137:59 - which is where the name comes from. And 
as you can see, there's no definitive  
138:03 - end to the family tree. Sure, there are the ones 
at the bottom, which you could argue are the  
138:09 - ends of the family tree, but which 
one of them is the definitive end,  
138:14 - there is none. Another example could be 
a file structure system on your computer,  
138:20 - you'd have a base folder, such as your desktop. 
And then inside of that, you might have multiple  
138:24 - other folders for different types of information. 
And then inside of those, you might have more  
138:29 - folders representing more types of information. 
And then finally, you would have your documents.  
138:37 - Again, it sets up a network of those files on 
different levels, just like how we had with the  
138:41 - family tree. Trees and the trees. Many variants 
are all dependent on storing data hierarchically,  
138:49 - which of course finally begs the question of what 
exactly a tree is. Well, a trees in abstract data  
138:56 - structure which consists of a series of linked 
nodes connected together to form a hierarchical  
139:02 - representation of data. The actual information or 
data is stored in these nodes. And the collection  
139:08 - of nodes along with the connections between them 
is what's known as the tree. This is sort of like  
139:13 - a linked list only instead of each node only 
pointing to one location, it has the option of  
139:18 - pointing towards multiple, and also branching 
off on its own or pointing to no other nodes.  
139:25 - Each of the nodes in a tree can also be called 
vertices and the connections between vertices.  
139:30 - What linked two nodes together are called edges. 
Now the free flowing structure of a tree lends  
139:37 - itself to a lot of different configurations. And 
with that comes a plethora of terminology about  
139:42 - certain nodes, vertices, and just the structure 
of a tree in general. So what we're now going  
139:48 - to do is go over a lot of the terms associated 
with specific nodes based on where they are on  
139:53 - the tree and how they're connected to other nodes 
while also visualizing the general structure of  
139:58 - a tree at the same Time. Okay, let's start. Let's 
begin with the things we've already talked about.  
140:06 - A vertices is a certain node in a tree, 
and an edge is a connection between nodes.  
140:12 - The first new thing is that every tree starts with 
what's known as a root node. This is always going  
140:17 - to be the topmost node of a tree. Let's add a node 
with the integer 50 to serve as our root node.  
140:24 - Next up, let's connect the two vertices 
to our root node using two edges.  
140:29 - These two nodes we just added are what's known as 
child nodes, since they are both connected to the  
140:34 - node containing the integer 50. child nodes can 
thus be defined as a certain node which has an  
140:40 - edge connecting it to another node one level 
above itself. Vice versa, the root node, the  
140:47 - vertices containing the integer 50, is now what's 
called a parent node to these two child nodes.  
140:53 - Thus, we can define a parent node as any node 
which has one or more child nodes. Think back  
140:59 - to our family tree. If we were using people 
instead of integers, it makes perfect sense  
141:04 - that the nodes directly connected to each 
other have some sort of familial relationship.  
141:10 - Let's continue on by adding two child nodes to the 
node containing the integer 20. When we do this,  
141:16 - the 30 node becomes a parent node, and the 
two nodes we've just added become child nodes.  
141:23 - We have now branched off from the 20 node 
completely, and that the two child nodes  
141:27 - containing the integers 10 and 15, respectively, 
share no direct association with it.  
141:35 - Speaking of the 20 node, since 
it does not have any children,  
141:38 - it is what's known as a leaf node, or a node 
in a tree which doesn't have any child nodes.  
141:44 - In this context, the 10 and 15 
nodes would also be leaf nodes.  
141:49 - Finally, let's add one more node as a child of 
the 10 node and there's our tree in all its glory.  
141:56 - As a quick review, the 50 node is the root node, 
the 30 and 20 nodes are children of that root  
142:02 - node and the root node is thus the parent of 
the 30 and 20 node, then the 10 and 15 nodes  
142:10 - are children of the 30 node. And the 30 node is 
thus a parent node to both the 10 and 15 nodes.  
142:17 - The five node is a child of the 10 node and 
the 10 node is a parent to the five node.  
142:23 - And finally, the 515 and 20 nodes are all leaf 
nodes because they do not have any children.  
142:30 - As you can see, one node or vertices on 
our tree can have many different titles  
142:35 - depending on where it is in the tree and what 
other nodes are connected to it. For example,  
142:40 - the 30 node is both a parent node to the 10 
and 15 nodes, but also a child of the 50 node.  
142:47 - The 15 node is both a child of the 30 node, 
and also a leaf node as it has no children.  
142:54 - This terminology really comes in handy when 
we start talking about trees which contain  
142:58 - 1000s of vertices and edges, and the data 
becomes very complicated to order and maintain.  
143:05 - Moving on the next two pieces of 
terminology I want to go over with you guys  
143:09 - are the height and depth of a tree. The 
height is a property of a particular tree in  
143:14 - it of itself. And the depth is a property of each 
individual nodes are contained within a tree.  
143:21 - Let's start with the height. The height of a tree 
is the number of edges on the longest possible  
143:26 - path down towards the leaf. So in our tree 
example, since the longest path in our tree, which  
143:33 - leads to a leaf is from the 50 node to the five 
nodes, and there are three edges in that path,  
143:39 - the height of the tree would be three. The depth 
of a certain node is the number of edges required  
143:46 - to get from that particular node to the root 
node. For example, let's take our 30 node.  
143:52 - Since there's only one edge connecting it on the 
way up to the root node, its depth would be one.  
143:59 - For the 15 node. However, since there are two 
edges, which separate it from the root node,  
144:04 - the 15 node would have a depth to that's pretty 
much all you need to know about the terminology  
144:10 - associated with trees and computer science. As a 
review, we have the height and depth obviously.  
144:17 - Then we have vertices, edges, root 
nodes, parent nodes, and leaf nodes.  
144:24 - Now there's probably something that's 
been scratching at the edge of your mind  
144:26 - for quite a while now. And that's why the heck 
are these called trees. They look nothing like  
144:31 - trees. Regular trees are not upside down like 
this data structure would lead you to believe.  
144:36 - So who named trees and why? Well, 
there's actually a simple answer to this.  
144:41 - The tree is said to have been invented during 
the 1960s by two Russian inventors. And the  
144:46 - last time that a computer scientists got up from 
the chair and went outside to actually see a tree  
144:51 - is rumored to have happened back in 1954. Ever 
since then, it's been grinding arrayed screens  
144:57 - watching hours upon hours of their favorite youth. 
superchannel so please forgive them for their  
145:02 - confusion when it came to naming conventions, they 
must have just forgotten trees aren't upside down.  
145:08 - Now regular trees, like the ones we just created 
are great for storing hierarchical data. But their  
145:14 - power can really be heightened when you start 
messing around with how the actual data is stored  
145:18 - within them. by imposing rules and restrictions 
on what type of data is stored within a tree,  
145:25 - as well as where we can effectively use the 
tree data structure to its full potential.  
145:31 - I could talk about the different types of 
trees for a long time, so long that many of  
145:36 - them are full segments in this lecture. But for 
now, I just want to cover one popular variant.  
145:42 - This will be a good introduction to how trees can 
vary without simply diving into a huge deviation  
145:48 - from what we already know. More specifically, 
the tree variant I want to talk to you guys about  
145:54 - is the binary search tree. A binary search tree 
is a simple variation on the standard tree,  
146:01 - which has three restrictions put on it to help 
organize the data. The first is that a node can  
146:06 - have at most two children. This just helps make 
it easier to search throughout the tree as we  
146:12 - don't have to spend time looking through each 
of the eight children for a particular node.  
146:18 - Keeping it limited to two helps us do this. The 
second restriction or property is that for any  
146:24 - given parent node, the child to the left has a 
value less than or equal to itself. And the child  
146:30 - to the right has a value greater than or equal 
to itself. This might seem weird, but it comes  
146:35 - with certain advantages and disadvantages over 
using normal trees, which we'll get to in a bit.  
146:41 - The final restriction put on binary search trees 
is that no two nodes can contain the same value.  
146:48 - And this is just to prevent weird things from 
happening when we begin searching through the  
146:51 - tree. Now, how do imposing these restrictions 
on a tree actually help us? Well, the biggest  
146:58 - advantage of binary search trees is that we're 
able to search through them in logarithmic time.  
147:04 - Because there is a natural order to the way that 
the data is stored. It makes it extremely easy to  
147:09 - search for a given value. logarithmic time, if you 
remember back to the segment on time complexity  
147:15 - is the equation in which we get more bang 
for our buck the greater number of elements  
147:20 - or nodes have in our data structure. It works 
like this. All we have to do when searching  
147:27 - is tell the computer to go left if the value 
we're searching for is less than the current node,  
147:32 - and right if it's greater than the current node. 
We can then wash rinse and repeat this strategy  
147:38 - until we find our desired note. This makes 
binary search trees really popular for storing  
147:44 - large quantities of data that need to be easily 
searchable. Of course, this also translates  
147:49 - to inserting, deleting and accessing elements 
within the data structure. But for the most part,  
147:55 - searching efficiency is what really sets 
the binary search tree apart from the rest.  
148:01 - stepping away now from binary search 
trees and into trees in general,  
148:05 - let's talk about common uses for them in 
computer science. The most common uses  
148:09 - for trees in computer science includes storing 
data with a naturally hierarchical structure.  
148:15 - These are like the examples we touched 
upon at the beginning of this segment.  
148:19 - Data Sets such as file structure systems, 
family trees, a company's corporate structure,  
148:25 - all of these could be stored and implemented 
using the tree data structure very easily.  
148:31 - These are all general examples though. As I 
mentioned before, when we put restrictions on  
148:36 - trees, like in the case of the binary search 
tree, we can expand it to uses even further.  
148:43 - A trees based structure is incredibly useful, 
and it can be modified in so many ways,  
148:47 - which only add to its functionality. One of 
these ways is through what's known as a try and  
148:53 - is the next data structure on our 
agenda. So stay tuned for that.  
149:00 - Next up, we'll be talking about another one of the 
special trees with restrictions. We just finished  
149:05 - discussing the binary search tree. And with that 
we mentioned how trees usually become even more  
149:10 - useful once you start setting restrictions on 
how and where the data can be stored within them.  
149:16 - Well, a try is another one of these special trees, 
which have special restrictions put in place to  
149:21 - help store the data in an effective manner. This 
data structure is often overlooked since it's only  
149:28 - used in specific situations. But without the use 
of tries within those specific situations. Some  
149:34 - important features of your computing life would 
be a whole lot harder. So we get it Steven tries a  
149:41 - special tree with restrictions. But what are 
those restrictions and how can they help us?  
149:47 - Well, let's start with the basics. A try is 
a tree like data structure whose nodes store  
149:52 - letters of an alphabet in the form of characters. 
We can carefully construct this tree of characters  
149:59 - in such a way Which allows us to quickly retrieve 
words in the form of strings by traversing down a  
150:04 - certain path of the try. These are also sometimes 
called Digital trees or prefix trees by the  
150:11 - computer science community. We'll be calling them 
tries for today's lecture. Trees are used in the  
150:17 - retrieval of data in the form of words, which is 
actually where the name try comes from, as it's  
150:22 - smack dab in the middle of the word retrieval. 
Essentially, in layman's terms, we use tries  
150:29 - to retrieve words extremely fast by traversing 
down a tree of store characters. This is hard to  
150:35 - explain without actually showing you. So let's do 
an example of what a try might actually look like.  
150:42 - Now, just like with a tree, every try is going 
to start with a root node only in our case,  
150:46 - that root node will be empty with either some null 
value or a blank string. Now also stored within  
150:53 - this node will be a set of references, all stored 
within an array. These references are set to know  
150:59 - at the beginning of the Tris existence, but can 
be slowly filled with references to other notes.  
151:05 - For example, let's say we were creating a try 
to store words that start with the letter D.  
151:11 - The root node would contain an array which 
contains a reference to a node containing  
151:15 - the character D, signifying the start 
of our word. Now imagine for a second  
151:21 - that this D node also has an array 
containing references. Only this time,  
151:26 - it contains references that point towards all 
characters in the English alphabet, which serves  
151:31 - as the first two characters of a word in the 
English dictionary. So da serves as the first two  
151:38 - characters and a lot of English words such as dad, 
or Tao, or dad if you really want to go that far.  
151:45 - And so the array contained within the D node would 
hold a reference to an a node. Now since there  
151:51 - are no English words, which start with DB that I 
know of, we wouldn't have a reference to a b node.  
151:57 - Since we know we will never have to retrieve 
a word from our try, which starts with dB.  
152:03 - We continue this process for all letters in the 
alphabet, only including references to characters  
152:08 - which serve as the first two characters in an 
English word. But that is a lot to visualize.  
152:14 - So for now, let's just put up on the screen two 
nodes that this first D node would point towards.  
152:20 - More specifically, the node, we've already added 
the a node, as well as the E node. Now to continue  
152:27 - building our try, we would simply repeat this 
process for all of the nodes that this D node  
152:32 - points towards. So we'd store pointers in the a 
node, which would point towards characters and the  
152:38 - English alphabet that serves as the first three 
characters of a word in the English dictionary.  
152:43 - So you'd have a B, A, D, A y, and many, many more 
nodes as well. But we're just going to stick with  
152:49 - those three for now. For E, you'd have pointers 
which point towards nodes containing characters  
152:55 - like n and w. Obviously, there'd be more nodes 
than the one shown on your screen right now  
153:01 - for all levels of nodes. But what we've 
created here is a very simple try.  
153:06 - As you can see, it's exactly like I said it was 
in the beginning, a tree like data structure,  
153:11 - which stores characters that can be used 
to make words by traversing down paths.  
153:17 - As we traveled down different paths of 
our tree, we can make different words,  
153:22 - dab, Dad day down the one path, 
and then and do down the other.  
153:29 - By following the nodes downwards, we 
can easily build strings of words,  
153:34 - which you'll learn towards the end of 
the episode can be extremely useful.  
153:38 - We can even take this process further by having 
one path contain multiple strings. For example,  
153:45 - if we isolate the den path, we could continue the 
process to go on to words like dense or Denver or  
153:52 - dent, the list goes on and on. We wouldn't have 
to just stop at the word then since then, is also  
153:58 - a prefix for numerous other words, and that's what 
makes storing data and character format so useful.  
154:04 - One path down to try can represent multiple words 
depending on where you stop along the process.  
154:11 - This does provide a bit of a problem 
for computer scientists though,  
154:15 - how do we know when a word has ended? Let's take 
the Denver path as an example. If we wanted to  
154:21 - retrieve the word den from this try, how would 
we know to stop at the end node and not continue  
154:27 - along to Denver? Well, there's actually a pretty 
simple fix to this and it's known as flagging.  
154:33 - We simply mark the end of the word by having it 
also point towards a flag to let the computer know  
154:39 - that the end of a word has occurred. So in our 
Denver example, not only would the end nodes array  
154:46 - contain pointers to whichever characters follow 
it, it would also contain a pointer to a node  
154:51 - with a flag to tell the computer to stop there. 
In this case, I've just chosen a period as a flag.  
154:58 - This way we can control tries in such a 
way where an each word is marked by an  
155:03 - ending point. And the different words that 
may branch off from the prefix can continue  
155:07 - to use that word as a prefix in whichever 
retrieval program ends up getting used.  
155:14 - Okay, so now it's time to talk about 
the general use cases of a try.  
155:18 - If you remember back to the beginning of this 
segment, I said that the use cases for a try were  
155:23 - limited, but extremely effective. And now we're 
going to talk about what that statement means.  
155:29 - Now, have you ever used the autocomplete 
feature on your iPhone or spellcheck on  
155:34 - a Google Doc, because if so you have already 
experienced the overwhelming power of tries,  
155:39 - as they are used for both of 
these extremely useful features.  
155:44 - This mainly has to do with the fact that 
for big programs like iOS or Google Docs,  
155:49 - they're not just storing a try containing a 
few words, or even all the words starting with  
155:54 - a certain letter, like we tried to replicate 
their storing the entire English dictionary.  
156:00 - storing the entire dictionary in a try seems 
like a tall task. But it can and has been done.  
156:06 - Each node would simply have 26 nodes connected 
to it. And those 26 would have as many nodes  
156:11 - connected to them as needed, and so on. And so 
now I can hear you asking you in the comments.  
156:18 - How does this help us with autocomplete and 
spellcheck? Well, let's say you're typing  
156:23 - out a word. For the sake of this video, let's 
say it's the word subscribe. You start with  
156:28 - the s. At this point, that computer has already 
eliminated 95% of words that you could be typing.  
156:36 - We know it's not going to be a word which starts 
with n, or with a or o or H or l etc, etc.  
156:44 - Then you type the letter U and bam, another 
95% of possible options get deleted. With  
156:50 - each character you type, you eliminate millions of 
possible words that you could be working towards.  
156:56 - Using this fact, as well as popularity data, 
which could also be stored within the node,  
157:01 - the computer can start making educated guesses 
using that information, as well as context from  
157:07 - previous words, to make a suggestion as to what 
word you're trying to type out. The AI that works.  
157:13 - This feature is immensely complicated, but it 
is backed by the tri data structure and helps  
157:18 - autocomplete work easily on your phone. As for 
spellcheck, well when you spell a word wrong,  
157:24 - a spell checking algorithm can use the root of 
your word and compare it against try data to make  
157:30 - an educated guess as to what you were trying to 
spell. The slightly misspelled words will usually  
157:35 - have a similar path from the root of the try. If 
you accidentally type chalk a lotta instead of  
157:41 - chocolate, the computer can take the first few 
characters of the word you typed in correctly,  
157:46 - and see where you may have gone wrong. Basically, 
by comparing the misspelled word to certain paths  
157:52 - of the dictionary try, they can pretty accurately 
detect which word you were meaning to say  
157:57 - and correct you accordingly. So there you 
have it, the try. Like I said, it is often  
158:04 - underrated in the computer science world since 
it can only be used in certain situations.  
158:10 - But as I hope this segment has shown you, those 
situations are still important. Nonetheless,  
158:15 - we are nearing the end of our introduction to data 
structures series. Now, only two more left before  
158:20 - we part ways. So stay tuned, because now we're 
going to talk about heaps. Now back in our segment  
158:27 - on trees, we talked about the binary search tree, 
a special type of tree which has a few properties.  
158:33 - Each node can only have two children, the child 
to the left must have a value less than the parent  
158:38 - node, and the child to the right must have a value 
greater than the parent node. And also no two  
158:43 - nodes could contain the same value. Well, a heap 
is sort of like this, but a little bit different.  
158:50 - More specifically, by definition, a heap is a 
special tree where all parent nodes compared  
158:55 - to their children nodes in some specific way, 
by being either more extreme or less extreme,  
159:02 - ie greater than or less than. This specific way 
determines where the data is stored and is usually  
159:08 - dependent on the root nodes value. There are two 
different methodologies generally used in computer  
159:14 - science, and they are known as min heaps and Max 
heaps. In a min heap, the value at the root node  
159:21 - of the tree must be the minimum amongst all of 
its children, and this factor must be recursively.  
159:27 - The same for any and all parent nodes contained 
within the heat. So each parent node must have  
159:33 - a value lower than all of its children nodes. As 
you can see from our example, on the screen now  
159:39 - 10 is the root node and also the minimum value in 
the tree. In addition to this fact, if we pick any  
159:45 - parent node on the tree, and look at its children 
and their children and so on, the parent node will  
159:51 - have the lowest value of the mall. Take 14 for 
example, its value is less than 2631 4435. In 33,  
160:01 - this must be the case for every single 
subtree, which is contained within the heap.  
160:07 - Max heaps on the other hand are 
the exact opposite. In a max heap,  
160:11 - the value at the root node of the tree must be 
the maximum amongst all of its children. And  
160:16 - this fact must be recursively, the same for any 
and all parent nodes contained within the heap.  
160:22 - If you take a look at the example max 
heap we have on the screen now, again,  
160:26 - you'll see that this is the case 44 is the root 
node and also the largest value within the heap.  
160:32 - If you take a look at say the sub 
tree, which is parented by the 35 node,  
160:37 - you'll see that 35 is a maximum value amongst all 
nodes in that subtree greater than both 19 and 27.  
160:46 - When we store data like this, and heap, whether 
that be a min heap or a max heap, it makes it  
160:52 - extremely easy to insert or remove from. This 
lends itself to a lot of useful implementations  
160:57 - within computer science. Now to show you 
this, I'd like to build an example max heap,  
161:03 - the one which has the greatest integer stored 
in the root node. For the sake of keeping  
161:08 - things simple, let's pull up an array of seven 
elements with integers ranging from zero to 100,  
161:14 - and simply convert it into a heap one element 
by one. This can be done in a few easy steps.  
161:22 - Step one is we add the first integer in as the 
root node. So 70 would get inserted into our  
161:27 - tree as the root node, then we add another node 
to the tree in the bottom left most positions  
161:33 - available. So we would first insert a node as the 
child of the root node to the left. For our heap.  
161:40 - This means adding the integer for as a child of 
the 70. The final step is to recursively go up  
161:47 - the heap and swap nodes if necessary. Now when 
we say if necessary, we mean that if the node  
161:53 - we just added is more extreme, either greater 
than or less than the node above it, depending  
161:58 - on the type of heap that we're trying to create, 
we swap them to maintain order amongst the heap.  
162:05 - Since we're building a max heap, and 70 is greater 
than four, no swaps are necessary in this case.  
162:12 - Now we just simply repeat steps two 
and three until we've built our heap.  
162:17 - So next, we want to add the integer 90. And since 
the leftmost slot on the tree is already taken by  
162:23 - our four node, the right slot ends up being the 
leftmost location on our tree. So we insert it  
162:29 - there. And then since 90 is greater than 70, we 
actually end up swapping the 90 and 70 nodes.  
162:36 - Doing this helps keep the max heap property 
intact, where every level is greater than the  
162:42 - one below it. Let's keep going. Next, we add 45 
to the heap. Now, since we've run out of space,  
162:50 - on the second level, we move on to the third 
level and add 45 as a child of the four node,  
162:56 - then we compare it to four, which is greater 
than so we swapped the nodes. Now we have  
163:02 - to compare this node to the node above it 
again. Because remember, we recursively go  
163:07 - up the tree until we reach the root or don't 
need to swap. Now 45 is not greater than 90,  
163:14 - so it stays put where it's at. Next up, 
we add 23 as another child of the 45 node,  
163:20 - only this time on the right side, we compare and 
since it's not greater than 45, we keep it as is.  
163:28 - Moving on, we next insert 76 into 
the tree as a child of the 70 node,  
163:34 - then we compare and swap the 76 and 70 
nodes as 76 is indeed greater than 70.  
163:40 - We then compare 76 and 90. And since 90 is greater 
than 76. Keep the 76 node in place for now.  
163:49 - The next node we add is the 100 node, we compare 
it to the 76 node and see that it's greater,  
163:55 - so it gets swapped. And then we compare it 
again, this time to the 90 node. And since  
164:02 - it's also greater than that integer, it gets 
swapped yet again to become the root node,  
164:06 - signifying it is the greatest node in the heap. 
As you can see, it's a pretty simple concept, you  
164:13 - add a node and then keep trying to swap up until 
the node you've added is in its rightful place.  
164:20 - Deleting from heap is also pretty 
simple, at least in our case.  
164:24 - This is because the type of deletion I want 
to talk about is just the process of removing  
164:29 - the root node from the heap. And you'll see why 
later on. To delete the root node from a heap,  
164:35 - you follow a three step process. Step one is 
actually removing the root node from our heap.  
164:42 - So in our case, we delete the 100 node. What 
you do with it is up to the programmer, you  
164:47 - can return it back to the user stored somewhere 
else, print it out, etc, etc. Either way, then  
164:55 - step two is replacing it with a node furthest to 
the right in this case It would be the 76th note.  
165:03 - Finally, for step three, we do what's known as 
a heapify. To fix up the heap, we start with the  
165:09 - root node and compare it to its children to see 
if we need to swap any values. So for the 76 node,  
165:15 - we compare it to its two child node. And since 
76 is less than 90, we end up swapping those two.  
165:23 - Then we wash rinse, repeat for every subtree 
that we have. So on the right side, we swapped  
165:29 - 90 with 76. But since 76, remains the greatest 
integer on that particular subtree, it stays in  
165:36 - the same spot. On the left side, we didn't change 
anything, but 45 is still the greatest integer  
165:42 - amongst the three nodes in that subtree. So no 
swapping is necessary on that branch of the heap.  
165:49 - And so we've completed our heapify, and 
all heap properties are still intact.  
165:54 - That is inserting and deleting nodes in 
a nutshell. Now let's talk about how we  
165:58 - can use this to our advantage. Heaps are most 
commonly used in the implementation of heapsort.  
166:05 - heapsort is a sorting algorithm which takes 
in a list of elements, builds them into a  
166:10 - min or max heap, and then removes the root 
node continuously to make a sorted list.  
166:16 - Because heaps always start with the minimum 
or maximum value contained within them at  
166:20 - the root node, we're able to simply just 
remove the root node over and over again,  
166:26 - keep refining the data structure after every pass. 
And after every element has been removed, we are  
166:31 - left with a sorted list. On your screen, you'll 
see we have an unsorted list on the left. In the  
166:37 - middle, we've inserted all of those integers, and 
in doing so created a max heap. And then finally,  
166:42 - on the right, we have continuously removed those 
elements from the root node into a now sorted  
166:48 - list. heapsort is a really cool algorithm, 
and it will be part of our upcoming series  
166:53 - on sorting algorithms, which is kicking off very 
soon. So if you're interested in that, make sure  
166:58 - you subscribe to our channel so that you don't 
miss it, a link will be in the description below.  
167:03 - Another extremely popular use of heaps is 
through the implementation of priority queues.  
167:09 - priority queues are an advanced data structure, 
which your computer uses to designate tasks  
167:14 - and assign computing power based on how urgent a 
certain matter is. Think of it like a line at the  
167:21 - hospital. You wouldn't want your line to follow a 
regular queue methodology which implements first  
167:26 - in first out. Since then you could have patients 
with extremely urgent matters like heart attacks,  
167:32 - waiting behind people coming in for a routine 
checkup. And the same way you wouldn't want  
167:37 - your computer to update an application before it 
finishes rendering a video. Otherwise your entire  
167:43 - progress would be lost. priority queues take care 
of all the task scheduling done by your computer,  
167:49 - and the heap data structure is used as the 
backing system for it. And with that ends  
167:55 - our discussion on heaps. To review, they are a 
special tree in which each level contains nodes  
168:01 - with values more extreme either greater than 
or less than the nodes on the level above it.  
168:07 - Next up is unfortunately the last segment in the 
series on yet another tree based data structure  
168:13 - the graph. The graph is arguably the most 
dynamic data structure in our introduction  
168:19 - to data structure series and an absolute banger 
to go out on. So let's just hop right into it.  
168:25 - Before we get into the nitty gritty details, 
I first want to do a short little exercise.  
168:31 - Visualize for a second a few of your favorite 
places to eat on a map around your town. For  
168:36 - me personally, it'd be places like five guys chick 
fil a Panera, Wawa and Domino's. Now imagine for a  
168:43 - second that you are ravished absolutely starving. 
And so your plan is obviously to drive around to  
168:49 - each of your favorite places to eat in order 
an ungodly amount of food from each location.  
168:55 - Each food location has a few possible paths 
going to and from it. And so we add those to  
168:59 - the map as well. Now you can see what we now 
have looks like a network of delicious foods,  
169:05 - we can start anywhere, and all we have 
to do is make sure to hit all five.  
169:10 - You may not know it, but what we've essentially 
done here is set up a simple graph. Basically,  
169:16 - graphs are composed of pieces of information like 
the restaurants and the path set run between them.  
169:23 - Of course, this is just generally, by definition 
graphs are a nonlinear data structure consisting  
169:29 - of nodes and edges. There are a finite set of 
these nodes or vertices, which are connected by  
169:35 - edges. nodes and edges should be familiar 
to you if you watch the segment on trees.  
169:41 - The big difference between trees and graphs 
however, is that with a tree we had a specific  
169:46 - starting point. Sure, there were multiple paths 
down the tree that branched off from the initial  
169:52 - starting point. But you always had to begin at 
the root note. In contrast with a graph there  
169:59 - is no specified starting point, we can begin from 
any node and traverse to any node. Just like how  
170:06 - on our food example, we are able to start at 
any restaurant. graphs are a huge concept and  
170:12 - escaped the bounds of computer science often being 
used in many places you wouldn't even think of.  
170:17 - But before we get into anything crazy, 
though, like the difference between a  
170:21 - directed or undirected graph or a cyclic versus 
cyclical graphs, let's get down the basics.  
170:28 - Now, every graph is composed of these nodes or 
vertices and the edges that connect them. Let's  
170:34 - pull up a sample graph really quickly and 
talk about it. We represent graphs visually  
170:39 - like this a lot, because it makes it way easier 
to comprehend. But notationally wise, a graph  
170:45 - actually looks like this, which is much harder 
to comprehend. So let's break it down. First,  
170:52 - we have the vertices set, which contains a comma 
separated list of all vertices within the graph.  
170:59 - That's the simple part. Each comma separated 
value simply represents a node within the graph.  
171:06 - Then we have the edge set, which is a little bit 
more complicated. Each element of the edge set is  
171:12 - an ordered pair which describes relationships 
between nodes. For example, the first one  
171:17 - describes a relationship between the six and four 
nodes. The fifth indicates a relationship between  
171:23 - the five and two nodes, and so on. Using these 
two sets, we're able to visualize a graph pretty  
171:30 - easily by laying down both the information 
and the connections that fall between them.  
171:35 - One final thing I want to mention about the 
basics of graphs is about the relationships that  
171:40 - occur between two nodes. If we have an edge, which 
connects two different vertices, they are known as  
171:46 - adjacent to one another. So for example, 
the five node would be adjacent to the four  
171:52 - two and one nodes. Okay, now that we have 
the basics down, I now want to jump into the  
171:58 - different attributes that a particular graph might 
have. Starting with directed versus undirected.  
172:05 - an undirected graph is one in which the direction 
you traverse the nodes isn't important. This is  
172:11 - most prominently indicated by a lack of arrows 
pointing to specific nodes. Such was the case  
172:17 - with our first example graph, or even the food 
example from the beginning of the episode.  
172:22 - We can hop between nodes or even go back 
and forth between them without problem.  
172:28 - A good way to visualize undirected graphs 
is like a network of friends on Facebook,  
172:33 - where each edge indicates 
a friendship, if you will.  
172:37 - Because of the fact that when somebody accepts 
to be your friend on Facebook, you are both added  
172:41 - to each other's friends list. The friendship 
goes both ways and direction is unimportant.  
172:48 - In contrast, a directed graph is one in which the 
direction you traverse the nodes is important.  
172:54 - This is usually indicated by arrows representing 
which nodes a certain node is able to traverse to.  
173:01 - The edges could point both ways but they don't 
have to. It's very possible the edge only points  
173:08 - one way. A good way to visualize directed graphs 
is by thinking them as a network of friends on  
173:14 - Instagram. Sure, I can follow famous celebrity 
Will Smith, but the odds that he follows me  
173:20 - back fairly low. And so in that case, the 
relationship only goes one way. undirected  
173:28 - and directed graphs both have their uses. As 
we discussed with the social media example,  
173:33 - both provide different functionality which will be 
useful to you and your computer science journey.  
173:38 - Just like the next type of property a 
graph can be classified as either cyclic or  
173:45 - a cyclic acyclic graph is one which contains 
a path from at least one node back to itself.  
173:51 - So you can see by the example on your screen now 
that the four node leads to the three node which  
173:57 - leads to the two node which leads to the one node, 
which finally leads back to the four node, forming  
174:02 - a cycle, essentially, we're able to follow at 
least one path that eventually leads back to our  
174:08 - starting point. A small thing to note here is that 
all undirected graphs end up being cyclical. The  
174:16 - bi directional nature of nodes within undirected 
graphs theoretically forms a cycle between any two  
174:22 - nodes. So judging by that logic, all undirected 
graphs end up being cyclic. In a acyclic graph is  
174:31 - one which contains no path from any one node which 
leads back in on itself. This property can really  
174:37 - only apply to undirected graphs like we mentioned 
previously. Essentially, this means that for any  
174:43 - given node, there is no path which will eventually 
lead back to itself. undirected directed  
174:50 - acyclic and a cyclic are all properties we can use 
to classify types of graphs based on their nodes.  
174:56 - But the last property I want to talk about 
actually applies to the end. Have a graph instead.  
175:01 - And it's the process of waiting. Waiting the edges 
of a graph means associating a numerical value  
175:08 - with each edge, often called the cost of that 
edge. Each weight represents some property of the  
175:14 - information you're trying to convey. For example, 
again, going back to our food location scenario,  
175:22 - since the information that we're trying to convey 
is a good route, which takes us to each location,  
175:27 - a good weight for our edges, and that scenario 
could be the distance between nodes. This comes  
175:33 - in handy a lot, especially with navigation, 
such as the case with our restaurant example.  
175:39 - As we of course, always want to find the path of 
least cost or weight between the different nodes.  
175:46 - So there are the major properties of heap 
that the different nodes and edges can have  
175:51 - directed or undirected, cyclic or a cyclic and 
weighted or unweighted. There are a couple more  
175:58 - obscure ones out there, but those six are what 
we will be covering today. Combining these three  
176:03 - properties together leaves us with numerous types 
of graphs, which all have different strengths and  
176:08 - weaknesses. It would take a while to talk about 
each of these and there are implementations.  
176:14 - So for now, I'll just pick out three types of 
graphs which are used in the most popular cases.  
176:20 - Now probably the most famous 
implementation of the graph data  
176:23 - structure is through the undirected 
cyclical graph with weighted edges.  
176:28 - This one gets a lot of use, especially through its 
implementation in Dykstra shortest path algorithm.  
176:35 - This algorithm given a graph and a source 
vertex within that graph, compiles a list  
176:40 - of the shortest possible paths from that source 
vertex to all other nodes. As you might be able  
176:47 - to tell just from its description, this has a 
multitude of uses across the entire tech world.  
176:53 - Google uses this algorithm for Google Maps. 
It's used in the process of IP routing,  
176:58 - and it can even be implemented in telephone 
networks. Another type of graph, which you  
177:03 - probably use quite often is the unweighted 
cyclical graphs both undirected and directed,  
177:09 - as these make up the follower systems of a 
majority of social media websites. We already  
177:15 - talked about these in the cases of Facebook, 
which would use a cyclical representation,  
177:20 - as well as Instagram which would use an a cyclical 
representation. However, this example encapsulates  
177:26 - much more than just those to Snapchat, Twitter, 
tik tok, even all these platforms can represent  
177:33 - your follower following base through a graph 
and oftentimes do Facebook even has a graph API,  
177:40 - which you can use to interact with the models that 
they use to illustrate each user's web of friends.  
177:47 - As you can see graphs and there are many different 
forms provide a lot of the functionality that you  
177:51 - interact with in everyday life, contributing 
to almost any facet of the internet.  
177:57 - And with that concludes our discussion on graphs. 
As we review, a graph is a data structure which is  
178:04 - represented by a set of nodes and edges. These 
come together to form a visualization of data,  
178:10 - whether that data be food locations on a map or 
friends on social media. The different types of  
178:15 - graphs provide a multitude of implementations 
in computer science. And with that concludes our  
178:22 - introduction to data structure series. After 
around three hours and 12 data structures,  
178:27 - you should now have the basic knowledge to take 
with you as you continue along in your computer  
178:32 - science journey. And if you've made it this long, 
you obviously liked what you saw. So stick around  
178:37 - for an extra 30 seconds while I'll pitch to you 
why you should check out my personal channel.  
178:43 - Now I introduce myself at the beginning of the 
series, but again, my name is Steven. I'm an  
178:48 - 18 year old computer science student who runs a 
YouTube channel called nullpointerexception with  
178:53 - one of my good friend Shawn. We've been producing 
content like this seriously for about four months  
178:58 - now. And I've grown a good audience of around 800 
subscribers, which we couldn't be happier about.  
179:04 - If you'd like to join us, you can check out 
our channel linked in the description below  
179:08 - and just try out a few of our other 
videos. And if you like what you see,  
179:11 - you can subscribe. That ends my pitch and with 
that I have nothing else to say. I hope you have  
179:18 - enjoyed this series as much as I've enjoyed 
making it. Thank you so much for watching.

Cleaned transcript:

Hello, everyone, and welcome to an introduction to data structures. My name is Steven, and over the next three hours, we'll be tackling the topic of data structures in relation to computer science. More specifically, we'll be talking about what they are going over some different types of data structures, and discussing how we can use each of them effectively with examples. This course will be a general overview of data structures, and so we won't be confining ourselves to one specific programming language. However, this series will require you to have a basic understanding of programming. If you are completely new to computer science, I would recommend our introduction to programming series, which will be linked in the description below. That video will give you all the information you need to traverse these topics comfortably. Now, we're almost ready to hop into things. However, there are a few housekeeping items that I want to go over beforehand for those wishing to enhance their learning experience. If you're not interested in that and want to skip right to the content, go to the time shown on your screen now. For those of you staying though there are just a few things I would like to mention. Firstly, in the description, you will find timestamps for each major topic covered in this video, as well as timestamps taking you to every smaller section contained within those topics. So please feel free to skip around if you're already comfortable with one topic or only interested in a certain data structure. Next, we've decided to include the script and visuals used for the series also in the description below. That way you can follow along if you like or simply read the script if my soothing voice isn't your style. Additionally, for each segment, I'll be including the references and research materials used to write the script for each topic. That way, if you ever feel as if I haven't explained a topic well enough, or we're just simply like more information about a certain topic, you'll have a readily made list of articles and websites, which you can use to supplement this series. If you feel like you have any questions throughout the series about maybe something I said or a certain visual, please ask your questions in the comments below. I'll try to be as responsive as possible for the first few weeks on questions you guys may have about the series and such. And finally, in terms of housekeeping, I just like to say if you're not already subscribed to our channel, no pointer exception, then consider doing so if you enjoy this type of content. as me and my co host Sean regularly post videos in this style. We're trying to hit 1000 subscribers before the fall semester of college begins. So check us out if you end up enjoying the video. With my shameless plug out of the way, we're finally ready to tackle the topic of data structures. In this introduction segment, we'll simply cover a general overview of what data structures are, and then go over what type of information will be covered throughout the duration of the series. So the obvious question to start with is what exactly is a data structure? Well, in computer science, a data structure is a way to store, organize and manage information or data in a way that allows you the programmer to easily access or modify the values within them. Essentially, it's a way for us to store a set of related information that we can use for our programs. data structures, and the algorithms used to interact, modify, and search through them provide the backbone for many of the programs that you'll end up writing, I can almost guarantee that in 99% of your programs data structure will be involved. Each of the data structures that I'll be talking about are designed for the sole purpose of storing information and allowing the end user to access and manipulate that information in an efficient, effective way. But each one differs in the manner that they accomplish this. If you have a basic understanding of programming, you probably know about a few different data structures already, such as the array and the array list, also known as the list in Python. But if you're going to be pursuing a career in computer science, just knowing these two is definitely not going to cut it. Well basic data structures such as the array are used frequently by companies throughout their code. More advanced data structures are being put to use all around you the Undo redo button in Google Docs, Google Maps on your phone, even the autocomplete feature through your text messages all require the use of more advanced data structures, which makes them extremely useful to learn and comprehend. Okay, now, I can't see you per se, but I can just tell that you're jumping up and down in your seat hooked on data structures and how useful they can be. But before we jump too far into things, let's discuss the information that we'll be covering in this series. Now, before we talk about any specific data structures, we're going to have a brief talk about efficiency. We'll discuss the metrics used to judge the speed and efficiency of different data structures, which will then give you a better understanding of why one data structure might be used over another one. From there, we'll start by diving headfirst into what I believe are the basic data structures, those being array. An array lists. While you may already have a good understanding of what these are, I would highly suggest that you still watch these segments, because we'll be going into a little bit more depth as to why they're so useful based on differences in how they're stored in the computer's memory. After that, we'll move on to what I'll call the intermediate data structures. These are a little bit more complicated than the basics and have a few special attributes which make them stand out from the rest. We'll begin by taking a look at stacks which are the backbone for all recursive processes in your computer. Then we'll look at the antithesis of a stack the queue. Moving on from there, we'll be covering linked lists and their evolved form in the doubly linked list. Before moving on to the final of our intermediate data structures, the dictionary which includes a mini lesson on hash tables. Then we'll wrap up the series talking about trees and tree based data structures, less linear and more abstract data structures beginning with the tree itself. We'll then move on to tries a very useful data structure used for a lot of word processing algorithms. And finally end off the series with a discussion on heaps and graphs. So in total, I'll be taking you through four different segments containing 12 of the most common data structures that are practically used, as well as providing examples of where and when to use them. section one on efficiency, section two on basic data structures, section three on intermediate data structures, and wrapping it up with section four on tree based data structures. With this knowledge, you'll be able to take charge of your programming career and hopefully gain a competitive edge by implementing them when needed. Like I said, however, before we jump straight into the different ways to store information, I'd like to have a quick discussion on how we score the efficiency of these data structures using what is known as Big O notation. So let's jump right in. Okay, so before we talk about all these crazy data structures, like maps, and heaps, we want a quantifiable way to measure how efficient a certain data structure is at the different tasks, we might ask a bit. If we're going to be storing extremely large amounts of data, being able to search through, modify, or access the information within a data structure needs to be fast and efficient. As we briefly mentioned before, the industry standard for this kind of implementation is big O notation. So what exactly is big O notation? And how do we measure it for a specific data structure? Well, that's what we'll be covering in this segment. For most of the basic and intermediate data structures in this series, we're going to be spending some time talking about its efficiency using big O notation. So this is definitely a topic you're not going to want to skip. With that being said, let's begin. So because there are so many different ways to store information, like we talked about in the previous segment, programmers have developed this idea of big O notation as a way to basically score a data structure based on a few different criteria. This criteria can change based on who you ask, but for the purposes of this video, we will be using four criteria representing the most common functions you might want from a data structure. The ability to access a specific element within the data structure, search for a particular element within the data structure, insert an element into the data structure and remove an element from the data structure. Now by measuring how efficiently a certain data structure can do these four things, we can basically create a report card of sorts, which measures how efficient a certain data structure is. At a glance, this gives us a pretty good overview of what a certain data structure is good at, and what it is bad at. And it can help us decide which one to use. If we need to store data that is easily accessible to the end user. For example, we might choose a data structure which can access elements the quickest, and vice versa, if accessing elements isn't the most important thing to us, but we need a data structure which can be easily added to and deleted from, we would go for one which is most efficient and that specific functionality. By looking at a data structures report card, if you will, we can get a quick sneak peek at what they're good at and what they're bad at. Now, if we use big O notation to basically create a report card, like I said, there must be some way to actually grade each of these functions. And there is the four criteria mentioned, accessing searching, inserting and deleting are all scored using big O notation time complexity equations. Now what is the big O notation time complexity equation? Well, I'm glad you asked. Besides being in an annoyingly long phrase, a big O notation, time complexity equation, or just time complexity equations, work by inserting the size of the data set as an integer n and returning the number of operations that need to be done. indicated by the computer before the function can finish. The integer n is simply the size or amount of elements contained within the data set. So for example, if we have an array, one of the data structures we'll get into soon, with the size of 10, we would place 10 into the different efficiency equations for accessing searching, inserting and deleting that represent the array, and returned back to us would be the number of operations that need to be conducted by the computer. Before completion of that function. It does get a little bit more complicated than this. But for the sake of keeping this series simple, all you need to know is that these equations help represent efficiency amongst different data structures. Also, an important thing to note here is that we always use the worst case scenario when judging these data structures. This is because we always want to prepare for the worst and know which data structures are going to be able to perform under the worst conditions. You'll see what this means in greater detail a little later on once we start putting this into practice. But for now, just keep in mind that when judging data structures, we always use the worst case scenario. Moving on, the reason that it's called Big O notation is because the syntax for these particular equations includes a big O, and then a set of parentheses. inside these parentheses will include some function, which will correctly return the number of operations needed to be run by the computer. So for example, let's say we have a fake function, it can really be anything, the purpose in this case is irrelevant. Now for this fake function, let's say it's time complexity equation was the one shown on your screen now, we would pronounce this time complexity equation as o of two, meaning it takes two operations from the computer before our make believe function can finish. If the time complexity equation was o with a five in the parentheses, instead, it would be o of five, and so on. Now, these are examples of time complexity equations, which run in constant time, meaning no matter the size of our data set, it will always take the same number of instructions to run. This is usually not going to be the case though, when it comes to the four criteria we want to mention for our data structures. Most of the time are integer n, which again contains the amount of elements within the data set is going to have some adverse effect on how many operations it takes to say search through our data structure, which makes sense, a larger data set means it's going to take more operations from the computer to search through that entire data set. Now we score these four functionalities in number of operations performed, because measuring by how long the function takes to run would be silly. If we measured by a metrics such as time taken to completion, our results would be highly biased by the hardware used to run the function. a supercomputer used by Google is obviously going to be able to search through a data structure much faster than a laptop. time complexity equations essentially level the playing field by instead returning the number of operations to eliminate the bias in processing power that exists. So to sum up everything that we've learned so far, we measure the efficiency or speed of a data structure based on how well it can perform four basic tasks accessing, searching for inserting and deleting elements within itself. Each of these criteria is modeled by an equation which takes in the size of the data structure in number of elements and, and returns back the number of operations needed to be performed by the computer to complete that task. By measuring these four metrics, we can get a pretty good understanding of what the data structure is good at, and what the data structure is bad. Now, it's important to note that this isn't the end all be all for deciding on which data structure to use in your program. As you'll see, as this video progresses, many of the data structures were structured with specific quirks or features, which separate them from the rest and provide additional functionality. Big O notation is incredibly useful, and something that you should definitely take into consideration when determining which data structure to implement into your program, but it should not be the only thing that you use. Cool. Now that we know a little bit about how we measure the efficiency of a data structure using time complexity equations, along with the four criteria actually used to measure them. Let's dive straight into what the actual equations mean in terms of efficiency. That way, when we start grading data structures, you'll have a good idea as to how good or bad each one is at that particular metric. Basically, we're going to cover the six most common time complexity equations from most efficient to least efficient. Now, the absolute best that a data structure can score on each criteria is a time complexity equation of o of one. This essentially means that no matter what the size of your data set is, the task will be completed in a single step. If your data set has one element, that computer will finish the task in one step. If your data set has 100 elements, one step 1 million elements one step 800 quadrillion elements, it does not matter, the computer will finish the task in a single step. This is why when we look at the graph of volume of data versus instructions required for o of one, the line remains constant at one. No matter the volume of data stored within the data structure, that computer can complete that task in a single instruction. Whether it be accessing searching, inserting, or deleting, O of one is the gold standard absolute best top of its class time complexity equation. It is basically the Michael Jordan of big O notation when it comes to time complexity equations. Now the next fastest type of time complexity equation is O log n. While not as fast as instantaneous time, a function having a time complexity of o log n will still provide you with very fast completion. Now if you don't fully understand logarithms entirely, just know that this efficiency is slower than instantaneous time of one and faster than the next level of efficiency known as O of n. Additionally, because the volume of data versus time graph follows a logarithmic curve, the larger the data set that you use, the more bang for your buck that you're going to get. Basically, as the amount of data you try to perform one of our four criteria on increases the rate of change of the amount of operations it takes to complete that certain task decreases. So as you can see, at the beginning of the graph here, when we increase the volume of data, our number of operations skyrockets. But when we do the same for larger and larger sets of data, our number of operations increases much more slowly than before. A good example of a non data structures related function, which has a time complexity of o log n is the binary search. If you know what a binary search is, then you'll understand how it is that when the data set gets larger, the number of instructions needed to find the item you're searching for, doesn't increase at the same rate. If you don't know what a binary searches, but would like to know, in order to better understand Oh login. A link in the description below will take you to that point in our introduction to programming series where you can learn about it. If not, let's move on to the next equation. O of n is the next common time complexity equation that's going to come up frequently during this lecture. The graph of volume of data versus instructions needed for this function is linear, meaning that for every element you add to the data set, the amount of instructions needed to complete that function will increase by the same amount. So to perform a function with the time complexity of O of n on a data set with 10 elements, it will take 10 instructions 50 elements will take 50 instructions, 1000 elements, 1000 instructions, and so on O of n is really the last good time complexity equation that exists. Anything above this is considered inefficient and not very practical when it comes to data structures in computer science. The next type of equation that will come up is o n log n. This equation is the first that is relatively bad in terms of efficiency. The graph of volume versus operations shows a somewhat linear but increasing graph meaning unlike o log n, it won't be better in terms of efficiency as the size of the data set increases. Instead, the slope actually increases as the volume of data does. The last two types of equation are o n squared and o two to the N. These are both incredibly inefficient equations, which should be avoided if at all possible. Because they're both exponential in structure, which can be seen from their graphs of volume versus operations. The larger the data set that you use, the more inefficient it will become. While there are more time complexity equations that exist such as o n factorial, which is even worse than the two I just mentioned, the data structures we'll be talking about, we'll never have time complexity equations that exists outside of the five we've just covered. So we're going to stop there. Now the final thing I want to say is just a reiteration of something I said before, these time complexity equations are not the only metric you should be using to gauge which data structure to use. As we get deeper and deeper into this series, you'll see that we might have some data structures which don't seem that efficient at all based on their time complexity equations for provide some other functionality or feature, which makes them extremely useful for programmers. Okay, now that we have some knowledge on how we actually grade these data structures in terms of efficiency, let's hop into our very first data structure. The first data structure we're going to be covering is the array. Now I understand the array is a pretty common data structure taught in most programming classes, and that many of you might already know about the properties and uses of an array. But in this segment, we'll be going into more detail on the array as a data structure, including its time complexity equations, storage methods, and more. So check the description for a timestamp to skip ahead to that section. If you already know the basics. If you're not as versed in the array, though, or just want a general review, then stick around, because right now we'll be discussing the basics of an array. An array is fundamentally a list of similar values grouped together in a central location, which makes sense because if you remember, we use data structures to store sets of similar information, so that we can easily use that information. Basically, arrays can be used to store anything, usernames for an app, high scores for video game prices for an online shop, pretty much any list of values which are fundamentally similar meaning of the same type. So integers, strings, floats, objects, the list goes on and on. Any primitive or advanced data type you can think of can be stored within an array. Every item contained within the array is referred to as an element of that array. And we call the collective total of elements, the array, this is going to be true for almost all data structures we talked about, by the way, so just keep that in mind. An array will also have three attributes associated with it, the first being a name for the array, the second a type for the array, and the third a size for the array. Let's start with the name. The name, if you couldn't tell is just simply a name for the array, which can be used to reference and interact with it. Say for example, we had an array called names, which contains the list of company employees. And then we also had an array called salaries which contained a list of salaries for those employees. If we wanted to search through the list of salaries, we would do so by referencing the salaries array with its name. This way the computer knows that you want to search through that specific array containing salaries, and not the names arraigns that this works vice versa as well, of course, if we wanted to search through the names array instead. Now another thing to note really quickly, while we have this up is that these two arrays are also an example of parallel arrays. Parallel arrays are two or more arrays containing the same number of elements and have the property wherein each element in one array is related to the element in the other array, or arrays have the same position. So from our example, we can say that the first element in our names array, john smith, corresponds to the first element in the salary array, the integer 10,000. This would mean that john smith has a salary of 10,000. This is really good, because as you'll see, later on, we can't store different types of variables in the same array. So we couldn't have the salary integers be stored in the same place as the name strings, making parallel arrays extremely useful for storing different types of data about the same entity. Alright, tangent over, let's get back to the attributes of arrays. And the second one, we'll be talking about the arrays type. Now an arrays type is simply what type of information is stored or will be stored within that array. One of the biggest stipulations about arrays is that it has to hold all the same type of information. So you cannot have an array containing integers and strings, it would have to be either an array of only integers or only strings. This works for any type of information you would want to store within the array, meaning every element or spot within the array must be of the same type. The third and final attribute that an array has is a size. This size is a set integer that is fixed upon the creation of the array. It represents the total amount of elements that are able to be stored within the array. An array size cannot be changed. I'll repeat that again because it's extremely important and array size cannot be changed once created. Once you write the line of code which instantiates an array You give it a defined size or fill it with elements until it has a defined size. And then at that point, the array size cannot be changed by conventional methods. This is actually pretty rare for a data structure. And it may seem really counterintuitive and useless. But later on when we talk about an arrays efficiency, you'll see that this is for a specific reason, and not just to be annoying to the programmer. So there you have it, the three attributes of an array, a name to reference it a type to fill it with an A size to control it. Let's now shift focus and talk about how we actually define an array ourselves how we reference and change values within an array, and then dive into the idea of two dimensional arrays. There are actually two different ways to instantiate an array in most languages, you can either populate the array with elements that you want contained within it right then and there. Or you can set a specific size for the array, and then slowly populate it later on as the program runs. We'll be discussing how to do both, starting with the first creating the array with the elements already stored. Now defining and filling an array as soon as you create it is mainly used for when you already know which values are going to be held within it. For example, let's say you're creating an array, which is going to hold the salary of 10 different workers at a company. And that information is being read in from a text file. Well, you already know the 10 salaries for those workers from the text file, and so you're able to immediately populate the array when you create it. This way, you'll already have the information available to you as the program runs. The way you do this varies amongst different programming languages. But it's shown here in three different languages, Java, Python, and C sharp. To give you a basic gist of how this is accomplished. All three lines of code on your screen now will create an integer array of size three, containing the numbers one, two, and three. You can see that all of them involve the declaration of an array name, which is then set equal to the values you want to include within the array encapsulated by some sort of brackets or braces. This isn't 100% how it's going to be for all languages, but for the most part, it will follow this skeletal outline. As you can see, Java and C sharp require you to define the array type, whereas Python does not. But for the most part, the syntax is the same. Again, that is name of array set equal to list of comma separated values encapsulated by brackets or braces. This also develops the size of the array for you automatically. So since we populate the array with the numbers, one, two, and three, the array would instinctively have a size of three, as it now contains three integers. If we were to say go back and add in a fourth number, where we define the array, the size would dynamically increase to for the next time we run the code. Now the second way in which we can instantiate an array is by setting an initial size for our array, but not filling it with any elements and then slowly populating it as the program runs. This would be used in the case that you don't actually know what information you're going to store in the array, but will as the program runs through. The most common case in which this happens is in the case where the end user will enter information into the program. And then that information will be stored inside the array. Since the information varies based on which user runs the code and what they input during that particular run, we have no way of populating the array as soon as it's instantiated, forcing us to add the information later. shown on your screen now are two ways in which we can do this in Java and C sharp. Now you'll notice that there is no Python section because of the fact that creating populate later arrays is not conventionally possible in the base version of Python, without the use of more advanced data structures or packages. You can see However, with Java and C sharp that just as with the other example, there are slight differences in the syntax on how we do this, but it generally follows some set pattern, the type of the array followed by a name for the array, which is then set equal to a size for the array. Remember, this size is final and cannot be changed outside of this initial call. If you end up creating an array with the size 10. But then later in the program, find that you need more space for it. You can always go back to where you instantiated it and change the size. But after that line of code runs, there's no conventional way to increase it. You may also be wondering what the brackets mean when instantiating an array and that's true Way to signify to the computer that we are indeed trying to create an array and not just a variable. Now that we know the two different ways to instantiate an array, we need to know how we, as the programmer actually get information that is stored within the array so that we're able to use it. And the simplest way to answer that question is through the means of a numerical index. an index is simply an integer, which corresponds to an element within the array. Now, the most important thing to know about indexes is that for a certain array, they begin at zero instead of one. So if we have an array of 10 elements, the first index would actually be index zero, the second would be index one, and so on all the way up to the ninth index. It's a little bit weird at first, but trust me, you get used to it. Now, to retrieve information from a certain position within the array, you would reference it using both the arrays name, and then the index number of the element you wish to retrieve. Say we have our array called numbers here, which contains the integers one through 10. To print out the element at the fifth index, in this case, the number six, we would reference the array name, in this case numbers. And then in a set of brackets, we would place the index we want to retrieve, in this case, the number five. What this piece of code is basically telling the computer is to print out the fifth index of the numbers array, which again is the integer six. Now because indexes start at zero instead of one, it's very important to note that to get the 10th element of the array, you would actually need to reference it using the number nine since there is no 10th index. If you do end up making this mistake, it will result in an array out of bounds error, since there is no 10th index in the bounds of the numbers array. Referencing and arrays index number is how we actually replace elements within the array as well. Continuing with our numbers array, let's say we wanted to change the last element, the integer 10 to be the integer 11. Instead, what we would do is reference the ninth index of the numbers array, the one which currently contains the integer 10, and set it equal to 11. This tells the computer to take the element at index nine and replace it with the integer 11. Essentially swapping 10 for 11. Now the last thing I want to talk about before we jump into the time complexity equations for an array is the practice of putting arrays inside of arrays. An array with an array at each element is known as a two dimensional array. Think of a two dimensional array as a matrix. It's similar to any other array, except for the fact that instead of a primitive data type like an integer how's that each element, we would instead have a whole other array with its own size and indexes. two dimensional arrays are useful for a variety of purposes, such as if you were programming, a chess board, a bingo board, or even an image where each element in the two dimensional array contains an RGB value, which when combined creates an image. Now referencing an element within a two dimensional array is mostly the same as with one dimensional arrays, except you now need two indexes, the first number would be the index of the column that you want to reference, and the second number would be the index of the row that you want to reference. By combining these two numbers, we can single out an element within the two dimensional array that will then be returned to us. As an example, let's create a two dimensional array with 16 names contained within four rows and four columns. Now to access the name Carl, you'd first single out the column, which contains the name you're looking for. In this case, Carl is in the third column, meaning it's in the second index. Next, you would single out the row which contains the name you're looking for. In this case, the name Carl is in the third row. So we would again use the second index to reference it. Combining these two pieces of information gives us the index location two comma two. And if you look, Carl is indeed at index location two comma two. Just as another example, Adam is in the first column two rows down. And so to reference Adam, you would simply call upon the element at index locations zero comma one. two dimensional arrays are just the beginning. You can also have three dimensional arrays, four dimensional arrays, and so on for containing large amounts of advanced relational data. But two dimension is where we're going to stop for today. All right, that concludes the background information on arrays. Now that we know what they are, let's actually talk about arrays as a data structure. This is where we're finally going to be using our big O notation knowledge from the previous segment. to judge the array as a data structure. We'll be going through the four criteria we talked about previously, accessing elements within an array, searching for an element within an array, inserting an element into an array, and deleting an element from an array, and scoring it based on how effectively they can complete these four tasks using time complexity equations. Now accessing an element within an array can be conducted in an instantaneous of one time. This means that for any element within your array that you want to know the contents of, you can do so immediately by simply calling upon its index location. This has to do with the way that an array is stored within memory. For example, sake, let's look at a fake portion of memory in the computer. You can see we have some information near the beginning, maybe some integers or strings that we've initialized, that's not important. What's important is that when we get to the part in our code, which instantiates an array, we already know exactly how large the array is going to be. Either because we've populated it with the elements needed, or given it a definitive final size. This means we also know exactly how much space we need to reserve in memory for that array. Combining these two pieces of knowledge means we can instantaneously figure out the location of every single element within the array by simply taking the starting location of the array in memory, and then adding to it the index of the element that we're trying to find. So let's say our array contains three elements, each of them integers. And we want to know what the element at the first index is. Well, we know that the array start storing data here. And we also know that every element within the array is stored contiguously together in memory. And so by taking the starting location for the array in memory, and adding one to it, we now know that that's where the first index of our array is going to be, and can return that stored value appropriately. This is the main reason why array sizes cannot be changed. All of the information within an array must be stored in this one place. So that we're able to do this, the contiguous structure of an array prevents you from adding space to it after it's been initialized, because it literally can't without breaking up the data and adding in the space to store new elements down the road away from the initial array. This would make accessing elements instantaneously not possible. Hopefully now you can see why both accessing an element within an array is O of one, as well as why array sizes are final, because not many other data structures allow you to have instantaneous accessing power, meaning arrays have a huge advantage on the others in this metric. Next up searching through an array is O of n. This is because for the most part, you will be working with unsorted arrays, those being arrays which are not in alphabetical numerical or some sort of quantifiable order, meaning that to find an element within the array, you may have to search through each element before you can find it. This is known as a linear search. And if you want to know more about it, click the card in the top right corner, and it will take you to that section in our introduction to programming series. A link will also be provided in the description below. Now while there are faster ways to search through an array, those only work when the array is sorted. And so for a basic array, in worst case scenario, searching through it to find a particular element is going to be O of n. Now Sure, there are going to be cases where the element you're searching for is the first element in the array, or even somewhere in the middle. But remember that when working with big O notation, we always use the worst case scenario. And in this case, the worst case scenario is that the item that you're searching for ends up being the last element within the array, which for an array of size n would take n operations to reach. Finally, inserting and deleting elements from an array both have a time complexity equation of O of n for inserting this is because adding an element into the array requires you to shift every element that's after the index, you want to insert the value at to the right one space. For example, if you have an array of size five, currently filled with four numbers, and you want to insert the number one into the array at index zero, you would first need to shift every element to the right of the zero with index right One, essentially freeing up the space for the new integer, while also retaining all of the information currently stored within the array. Then, and only then can you actually insert it. This requires you to traverse through the whole array of size n, which is why the time complexity equation is O of n. Now, sometimes you might not have to shift the whole list, say in the case, you had the same array of numbers, only this time the fourth index was free, and you wanted to insert the number five in that open spot. Since you don't have to move everything to the right one, in order to make space for this new element, does that make inserting into an array have a time complexity equation of o of one? Well, no, because again, when we talk about a functions time complexity equation, we always refer to it in the worst case scenario, the most amount of operations that we're going to have to conduct before we can insert an element into an array is n, the size of the list, making it's time complexity equation O of n. Deleting an element from an array follows mostly the same idea, you shift every element to the right of the one, you want to delete down one index. And essentially, you have deleted that element. Again, let's say we had an array of numbers one through five, only this time, we want to delete the number one at the zeroeth index, what we would do is set the zeroeth index to whatever is contained at the first index, in this case, the integer two. And then we simply repeat this process until we get to the end of the array, finishing it off by setting the last element in the array to be some no value. This deletes the element from the array by basically replacing it with a value to the right. And then this process is repeated into it frees up one space at the end. Essentially, we're just moving the entire array down one. Now worst case scenario, we're going to have to shift every element in the array of size n, making deleting from an array also have a time complexity equation of O of n. So there you have it the four time complexity equations for an array, accessing is O of one and searching, inserting and deleting are all O of n. Arrays are a really good data structure for storing similar contiguous data. Its ability to access any item in constant time makes it extremely useful for programs in which you would like to be able to have instant access to the information contained within your data structure. It's also a lot more basic than some of the other data structures that we'll end up talking about, making it both easy to learn and reducing the complexity of your code. Now, some disadvantages of the array are the fact that the size of the array cannot be changed once it's initialized. Inserting and deleting from an array can take quite a while if you're performing the function on larger data sets. And if you have an array which is not full of elements, you're essentially wasting memory space until you fill that index location with a piece of data. Overall, the array is a pretty reliable data structure. It has some flaws as well as some advantages. A program that you write could use the array if need be, and it would work just fine. But sometimes you might want some extra functionality. And that's where more advanced data structures come into play. One of those more advanced data structures is what's known as the ArrayList. The ArrayList fundamentally can be thought of as a growing array, we just finished talking about the array and how one of its major flaws was the fact that once initialized an array size could not be changed using conventional methods. Well, in contrast, an ArrayList size expands as the programmer needs. If you take a look at the ArrayList on your screen now full of four elements, and you decide to add one to it, it will simply expand its size to fit five elements. As you can probably tell this is extremely useful. Which begs the question of why not just always use ArrayList. I mean, in comparison, the array list seems to provide all the functionality of an array and then some Well, that's definitely a valid question, and one we will get to later on in this section. But before we can do that, we need to cover the basics of an ArrayList including some of the properties and methods associated with it. Alright, let's hop in. So as we said before, an ArrayList is simply a resizeable array, making them extremely similar in structure. This is furthered by the fact that an ArrayList is actually backed by an array in memory, meaning that behind the scenes of your code, the ArrayList data structure uses an array as its scaffolding system. For this series, we need not go further than that. But for us, it just means that a lot of the functionality will be the same. Now, this doesn't mean everything is going to be the same, which you'll see later on. But it's still important to make note of before we get too deep into things. The next thing I want to do before we talk about functionality is actually go over how we initialize an ArrayList. To do this, again, is going to vary based on which language you're using. So shown on your screen now are two different ways to do so in Java and C sharp, you may notice again, that there is no Python on the screen. And this is because in the base version of Python, arrays and array lists are actually not separate entities. They are frankensteined together into a single data structure called lists. Lists take some functionality from arrays and some from array list. It's a lot more complicated than that. But for this series, that's all you're going to need to know as to why we are not including Python. In this section. We discussed initializing Python lists in the previous section. So if you're interested, you can go back and look at that. Now going back to ArrayList initializations. Another thing you may notice is that it looks a little bit awkward in the way that these statements are structured. And that's mainly due to the fact that the ArrayList is actually its own separate class outside of the base version of these two languages. Now, I'm not going to get into class hierarchy and object oriented programming right now, because that's a whole other topic with big concepts and even bigger words. For right now, this just means that to create a new ArrayList, we have to invoke the ArrayList class when defining it, which as you can see is done at the beginning of both initializations. After that, you can see that we give it a name, which is then set equal to new ArrayList with a set of parentheses. Now in this parentheses, you have a few options, you can either enter in an integer, which will then be used to define a size for the ArrayList. Or you can just leave it blank. Leaving the parentheses blank like that will automatically define a preset size of 10 for the ArrayList. Again, just as a reminder, this can be dynamically increased as time goes on if we add enough elements, but it's just meant as a base size. Now you may be wondering if we can actually populate the array with elements when initializing it as we could with arrays. But array lists actually do not support this type of declaration. Moving on, let's talk about functionality. Now the array list can be thought of as pretty much the evolved form of an array. It's a bit beefier has a little bit more functionality and is overall more powerful than array. That's certainly not to say that it's better in every case. But for the most part, the ArrayList is going to be thought of as the older sibling amongst the two. This is attributed to the fact that it belongs to the prebuilt ArrayList class, which we talked about earlier. The fact that the ArrayList belongs to a class means it's going to come with pre built functions that are already at our disposal from the moment we define and instantiate an ArrayList. More specifically, the ArrayList comes with methods we can use to access change, add to or delete from an easily if you were using an array, you would have to program most if not all of these methods by hand. And so having them pre built into the ArrayList class makes it especially useful. You'll see that this is the case with a lot of the data structures down the road, we're having a data structure belonging to its own class cuts out a lot of programming time, you have to spend making appropriate methods for the functionality that you might want. Now the types of methods that you're going to get will vary based on language. For example, in Java, you'll have a variety of methods to use, including ones to add elements to the ArrayList, remove them from the ArrayList, clear the ArrayList entirely, return it size, etc, etc. as well as tons of other more specific functions. And another language though, such as C sharp, you'll have some of the same methods as the Java ArrayList. But you might also have some methods so that the Java version does not and vice versa. The C sharp version might not have some of the methods that the Java version does. The same is going to apply for any other language you use, which might implement the ArrayList data structure. Because of the variability surrounding the ArrayList amongst languages. In this series, we're simply going to be covering six of the most common methods that are both useful and can be found in 99% of the ArrayList classes. These six methods are the Add method, the Remove method, the get and set methods, the clear method, and the to array method. This may look like a lot but remember, all of these are pre programmed for you And so you don't have to make them all by hand, all you have to do is call them on a pre made array list and you're set to go. Speaking of pre made array lists, before we dive into each of these functions and how they work, let's first create an example ArrayList, we can use to manipulate and show how they work. Let's call it example a list and give it a size of four so that it's not preset to 10. And we're set to go. Now, the Add method actually comes in two different types. One, which takes in only an object to add to the end of the ArrayList. And one which takes in both an object to add to the ArrayList, as well as an index value representing the index to insert the object at. Let's start with the simpler of the two, one, which simply takes in an object. This method is for more basic cases where you don't care about where in the ArrayList, the object you wish to add is going to end up, it will simply append the object you pass in as an argument to the end of the ArrayList. So let's take our example ArrayList and run the Add method on an integer of two. Now, normally, ArrayList only hold objects, not primitive types, like the integer two that we're trying to pass in. However, the computer will automatically convert our primitive integer into an integer object with a value of two, so that we can still add it easily. This is known as auto boxing. And it's going to be used throughout the rest of the series to modify data structures with primitive types. So I just thought I'd mention it now so that you're not confused later on. Okay, so when we run our code, since the ArrayList, is empty, and we ran the Add method, which doesn't care about location, it's going to add the integer to at the first index, index zero. Now if we run another add method, and this time pass in the integer five as an argument, since the zero with index is already taken, it will be slotted in as the first available open index, that being the first index. So as you can see, it's pretty simple. It takes in an object and we'll put it at the first open location available. Moving on to the second type of add method, one which takes in an object to add to the ArrayList, as well as an index to place it. This one works very similarly to the previous only makes sure that the object that you're adding is appended at the index provided. Again, let's say now we want to add the number one to our example ArrayList. But we want to make sure it's placed in numerical order. In this case, at the zero with index, what we will do is call the Add method, providing the integer one as an argument. In addition to the index zero, we want to add to the ArrayList. Once the code has run, the ArrayList will automatically shift our integers two and five to the right one, in order to make space for the integer one. This works for any index contained within the bounds of the array list. So if we wanted to do it again, only this time, insert the number three at the second index, so that the list remains in numerical order. We'd call example, a list dot add, and then in the parentheses passing the integer three and the index location two. After the code has run, you'll see that we have added the integer into our ArrayList. Now it's important to note that because there are two integers being passed in as arguments, you must know which one your computer's treating as the integer and which one the computer is treating as the index location. mixing these up could cause the computer to try and insert the attempted index location at a different location than the one you were attempting to insert at. So just be careful and knowledgeable as to the order your methods arguments are in. Now the next method that comes pre packaged in the ArrayList class is the Remove method. And this one also comes with two different types. The first takes in an integer as an argument, and just as the name suggests, will remove the object if there is one at the index location provided and return it back to the user. The second takes in an object and will simply remove the first instance of that object within the ArrayList If present, and will then return true or false whether an object was removed. So if we wanted to remove the number five from our ArrayList, we would have two different options. We could call example, a list dot remove, and inside the parentheses placed the index of the value we want to remove, in this case three, and the program will remove the object at index three. The other option would be to run another remove method, only this time passing an integer object of five. It has to be an integer object because if we were to just use five, the computer would try to remove the fifth index of the ArrayList, which doesn't even exist. By creating an integer object, we can ensure that when the code runs, the computer knows that we want to remove the first instance of the number five in our ArrayList. And running this will return true and remove the integer from our list. Now, if there is no integer five in the ArrayList, the Remove method will simply return false. Now I quite like the number five, so I'm actually not going to permanently remove it from the ArrayList just yet. Up next is the get method. Now the get method is pretty much the same as referencing an index for an array, it takes in an index location, and will return back to you the value at that location. So example a list dot get with an argument of zero would return one example a list dot get with an argument of two would return three, and so on. The next method is the set method, which is how we replace elements within an ArrayList. Much like the name implies, it takes in an index and an object as arguments. And we'll set the index location of the index you passed in to the object you also passed in. So if we wanted to set the number five in our ArrayList, to be four instead, so that it matches nicely with the other integers, what we would do is call example, a list dot set, and within the parentheses pass in the index location of the element, we want to set, in this case three, and then also the object we want to replace at that index. In this case, the integer for this method call will override the element at position three to be four instead of five. Now, you should be really careful when you're using this method, because you don't want to accidentally override an important element within the ArrayList. Next up is the clear method for when you simply just hate your ArrayList. This is perhaps the simplest of them all. It has not taken any arguments, and will simply clear the ArrayList deleting every element entirely. Calling example a list clear on our ArrayList would delete all the objects within it. But I don't really want to do that right now, especially with one more method to go. So for the sake of this series, let's just keep the ArrayList filled with the values that it currently has. The final method that we'll be covering in this section is a little bit different from the rest. And that's the to array method, which is used to convert an ArrayList to an array. I thought I would include this one because it's really useful for combining the strengths and weaknesses of arrays and array lists. The two array method takes in no arguments, and will simply convert the array list into an array. Now for this to work, of course, you need to set it to be equal to the creation of a new array, like shown on your screen now. But if it is done correctly, you'll end up with a brand new array, which contains all of the contents that used to be in the old array list. You may notice though, that instead of an array of integers, it's actually an array of objects. This mostly has to do with that object oriented programming stuff we talked about in the beginning. But for now, it won't make too much of a difference, we can still treat it as a regular array, printing out indexes to the console, placing elements within it. Typical array functionality. The only thing that changes is that it now contains integer objects instead of primitive integer types. So there they are the six major functions that come with any given version of the ArrayList class. Having these at your disposal will account for much of the functionality, you might use an ArrayList for making them extremely valuable to now Let's now move on to the array list as a data structure, again, we're going to be looking at its four time complexity equations for accessing, searching, inserting and deleting. Now, if you remember back to the beginning of this segment, we mentioned that the ArrayList is backed by an array in memory. And this means just like the array, it too will have over one accessing power. Essentially, this means that when we use our get method, which takes in an index location, it will return to us the value at the index provided in instantaneous time. Now you might be wondering, how is this possible since the data stored within an ArrayList is certainly not contiguous? With is actually due to a really interesting reason. So interesting that before scripting the series, I actually had no idea was the case. So because it is my series, I'm going to take a moment to talk about it. So if we pull up our example ArrayList and memory, you can see that it looks a little bit different. Let's break down what's going on. instead of storing the actual objects which are contained within itself, an ArrayList actually stores references or pointers to the locations of those objects in memory. So the zeroeth index based on the ArrayList is stored at the 87th location in memory, which is currently storing the integer one. Checking back to our example ArrayList, you'll remember that that is indeed what was stored at the zero with index of the example ArrayList. And this goes for every element within the ArrayList. The first is stored at the 91st memory location, the second at the 100th, and so on. So as you can see, while the actual data is not stored contiguously the references to that data are. So when we run our get command, all it has to do is return the value that's stored at whatever the index location points towards. It's a bit more complicated than that, especially the way that these references get stored. But that covers the gist of it. This is the reason our ArrayList can have instantaneous accessing power without having the data stored contiguously. Alright, tangent over, let's get back to big O notation, time complexity equations for the ArrayList. So we know accessing is going to be o of one. And searching is going to be O of n. This is for the same reason that arrays were o n. If we want to find out if an object is contained within our ArrayList. And that object is at the last index of the array list, we're going to have to search through each and every element within it of size n to make sure because remember, we always go based on the worst case scenario. Now inserting into the ArrayList is going to be O of n. Because worst case scenario, if we are inserting an element at the beginning of the ArrayList, we need to shift every element after the index we're inserting to the right one, just like we needed to do for the array. This requires a number of operations equal to the size of the array, making inserting O of n. Deleting is O of n for the exact same reason. If we want to delete the first element within the ArrayList, we would then have to shift every element down one to save space. Additionally, if we want to delete an object contained at the last index, we have to search through the whole list to find it. Either way, it will be O of n. Alright, so there are four time complexity equations, accessing his old one and searching for inserting and deleting are all O of n. If that sounds kind of familiar, that's because these are the same as the arrays time complexity equations. See I told you they were similar. Now this does bring back up the question we posed at the beginning of the episode. Why even use an array in the first place in comparison to the ArrayList the array just does not seem as useful? Well, let's get into that by sizing these two data structures against each other mano a mano. Let's compare. An array is a collection with a fixed size, meaning it cannot be changed, whereas an array list has a dynamic size, which can be updated to fit the needs of the programmer. Arrays can store all types of data, meaning both primitives and advanced types, whereas array lists can only store objects, not primitives. Now this problem is mostly solved through the auto boxing situation I talked about previously, but the fact still stands. Moving on an array is built into most languages, meaning it doesn't have any methods pre made for you to interact or modify it. Whereas an array list is its own class, meaning it comes with useful methods to help you utilize it. Finally, an array is very primitive in nature and doesn't require a lot of memory to store or use. Whereas an array list is again a class, meaning it requires more space and time to use than an array will. Hopefully now you can see that while the array list is more powerful, it still does have some drawbacks which make using an array sometimes more appropriate. In general, you want to use arrays for smaller tasks where you might not be interacting or changing the data that often and array lists for more interactive programs where you'll be modifying the data quite a bit. So that's the ArrayList. As a review, it is a dynamically increasing array which comes with a slew of methods to help work it as the array list is a hefty topic in computer science. If you feel I didn't do a good enough job explaining some of the concepts in the video. The sources used to write the script for this video will be linked in the description below. But if not, let's move on to our next data structure. The next data structure we'll be talking about is the stack. Now at this point in the video, we'll be diverging from what are known as random access data structures, ie arrays and array lists, and diving into sequential access data structures. What's the difference? Well, if you remember back to our discussion on arrays and array lists, we were able to access any element within the data structure by calling upon its index location. This would then result in the computer instantaneously returning the value at that location. Each element was independent of the one before or after it, meaning obtaining a certain element did not rely on any of the others contained within the data structure. That basically describes the gist of a random access data structure, one where each element can be accessed directly and in constant time. A common non computer science example of a random access would be a book. Getting the information contained on a certain page doesn't depend on all the other pages within that book. And getting an element contained within a random access data structure doesn't depend on all the other elements contained within that data structure. In contrast, elements in a sequential access data structure can only be accessed in a particular order. Each element within the data structure is dependent on the others and may only be obtainable through those other elements. Most of the time, this means that accessing a certain element won't be instantaneous. A common non computer science example of sequential access would be a tape measure. To get to the measurement of 72 inches, you would first have to go through inches one through 71. There's no way to just instantly get to 72 inches without first breaking every law of thermodynamics and probably the space time continuum. These are also sometimes called limited access data structures, because unlike random access, the data is limited by having to obtain it through a certain way. So there you have it, random access data structures, which allow instantaneous accessing power and independent elements such as the array and array list. And then you have sequential access data structures, which only allow accessing through a particular order with dependent elements. For the next few segments, we'll be covering a few of the popular sequential access data structures, such as stacks, queues, linked lists, and so on. Beginning of course, with the stack, we've already danced around the subject long enough. So let's finally talk about what a stack actually is. Now, the stack, by definition, is a sequential access data structure in which we add elements and remove elements according to the lifepo principle. The lifepo principle, which stands for last in first out, means that whichever element we added to the stack last will be the first one we retrieve. Hence, last in first out, think of this as a stack of books. Each time you add another book to the top, you do so by stacking it on top of the others. Then, if we want to get a book in the middle of that stack, we would first have to take off all the books on top of it, we can't just grab that book from the stack and expect the entire thing not to fall in on itself like some sort of literature ristic Jenga. The same goes for the stack as a data structure, we add elements to the top of the stack and retrieve them by taking them off the top of the stack. There's only one way in and one way out for the data. We can't simply access or modify an element within the stack all willy nilly like we were able to do with the array and the array list. This might seem like a weakness limiting the flow of data to one single point. But remember what I said during this segment on time complexity, many of these data structures have a built in functionality, which gives them an edge over the others. And the stack with its life. Oh property is a prime example of that. Next up, let's talk about some of the methods commonly associated with the stack. Now the stack class will always come with two methods pre built into its class, those being push and pop. These are the fundamental methods which we use to interact and modify the stack, making them extremely important to comprehend. In addition to those two, I'm also going to cover two more methods peak and contains, which are both useful and can be found in the stack class associated with most programming languages. Now, just like we had an example ArrayList let's also create an example stack to help us show off these methods. And just like that, we're ready to roll. Now push is a method which pushes an object onto the top of the stack. All it takes in as an argument is an object to add to the stack and will return no value. When we do this, that object becomes the form front of the stack, and its size is dynamically increased by one. Let's start running some push commands on a variety of different strings, you'll see that the stack slowly gets built. Now channel this to subscribe, a series of completely random words seamlessly pushed onto the stack. As you can see, we're only adding information to one spot the top, there's no insert method where we can just jam a string into the middle of the stack. Moving on, the pop method is used to remove an element from the top of the stack, it does not take in any argument and will return the element that is popped off the stack back to the user. Once a pop method has run, the element that was at the top of the stack is removed and returned back to the programmer. And the element which was second from the top becomes the new top of the stack. And so on our example stack, if we popped off each element, you'd see that each time one of these completely random strings is taken from the stack and returned back to the user until we are left with a sad, empty stack with no shameless promotion. Let's add the strings back, obviously, just for the sake of the next two methods that we need to cover, and not for continuing the free advertising. So push and pop are how we add and remove the data within our stack. So they're fundamentally the backbone of the data structure. The next two I want to talk about peak and contains are more so used to interact with the data inside the stack without actually changing it. Not as useful for modifying data, but still extremely important. The first of these is the peak method, the peak method allows you to get the value at the top of the list without actually removing. We talked about before that the only way to access elements in a stack was through the top. And this method is simply a way to look at what the top value is without having to pop it off. It takes in no arguments and we'll just return back the contents have the top element. In our case, if we ran it on our example stack, subscribe would be returned back to the user. Now if we popped off the top element and ran it again, two would be returned, it's that you get the idea. Again, let's push subscribe back on at the top for educational purposes. Now the final method I'll be covering is the contains method. This one is used for searching throughout the stack. It takes in an object as an argument and will return a Boolean of whether or not that item is contained within the stack. Essentially, this method allows us to search through the stack without popping off every element until we find the one that we're looking for, as the contains method does not modify the stack in any way. So for example, example stack contains with an argument of subscribe would return true example contains with an argument of this would also return true. But example contains with an argument of Hello would return false. So there they are four common stack functions which are going to be vital if you ever want to construct a stack based program. Moving on to time complexity, for accessing the stack has a time complexity equation of O of n. This is because in order for us to reach a certain element within the stack, we first have to pop off every element that's above. Think of it like this, if we had a stack of stones, and needed to get to the bottom one, we'd first have to take off every stone on top of it. So worst case scenario, if the element we want is at the bottom of the stack, we first have to pop off every element above it. This would require a number of operations equal to the size of the stack, making the time complexity equation Oh event. This is one of the major drawbacks to using stacks. with arrays and array lists, we could easily access any element within the data structure instantaneously. And with a stack, that's just not possible because of the way that it's structured. Searching is going to be O of n for the exact same reason. Worst case scenario, if we're searching for an element that's at the bottom of the stack, we have to go through the whole thing just to find it. Now, inserting and deleting make up for this by having time complexity equations of o of one. This essentially boils down to the fact that using our push and pop methods really only requires one operation. Since the data only flows in and out of a single point, inserting or removing an object from that point can be done immediately. For the push method, we simply add it to the top of the stack. And for the pop method. We just take it off from the top. It's not rocket science. Actually, it's computer science but that's Besides the point, there's no need to rearrange data or move elements around like there was for the array and ArrayList, because we don't have to the data that we're modifying is right there on top. So there you go, the time complexity equations for the stack. Now, you might be wondering if there are even uses for a first in first out data structure, because it seems kind of out there, I mean, limiting your data to a single entry point. But you would actually be mistaken. stacks are used everywhere, both in the actual writing of other code as well as in real world situations. In fact, one of the most fundamental processes of computer science uses stacks as a way of keeping track of active functions or subroutines. Of course, I'm talking about recursion. Now, I won't get into recursion too much here. But basically, it's the process of functions repeatedly calling themselves when the function calls itself, that call is added to a stack of processes. And when that stack finally reaches what's known as a base case, the functions are all then popped off one by one. It goes much, much deeper than that. But we don't have time for a full blown lesson on recursion. Right now. If you want that, you can click the card in the top right corner of your screen, or the link in the description below, which will both take you to that part in our introduction to programming series where we cover it. Either way, stacks are the backbone for recursion. And recursion is the backbone for a plethora of computer science related functionality, such as traversing data structures, keeping track of active subroutines in code, and much, much more. Some examples of stack based functions outside of computer programming, which you use every day include the Undo redo button in word processors and the back paging on web engines. Both use stacks. Similarly, they continually add tasks you've completed to a stack, either web pages that you've visited or words that you've typed. And then when you press Undo, or the back button in a web browser, all we have to do is pop off whatever the last task was off the stack. And bam, you're right back to where you were a second ago. It's like magic, but better in a way. As you can see, the stack has a lot of real world applications, both on the consumer side and the client side. You interact and use stacks every day without even realizing it. And we'll use stacks frequently as you continue along your computer science journey. And so by learning them, you're opening up a world of opportunities. That concludes our discussion on the stack. To review. It is a sequential access data structure in which we use the lifepo principle to add and remove elements from and again, life O stands for last In First Out of next we'll be talking about an equally useful data structure that functions very differently than the stack while also working very similarly. And that data structure is known as the queue. So now that we've talked about the stack, a sequential access data structure, which follows the life principle, we need to cover the opposite, which in computer science, we obviously call a queue. Now by definition, a queue is a sequential access data structure, which follows the FIFO principle, or first in first out, the stack and q are very much a dynamic duo when it comes to the world of comp sigh. So you'll notice a lot of similarities between the two and the way that they're structured and how they work. Today, and in this segment, we'll cover a lot of the same topics as we did with the stack, only for the queue. Now timestamps can be found in the description correlating to these topics. But if you're sticking around, let's dive deeper into what the queue actually is. Well, the queue like the stack is a sequential access data structure, meaning we can only access the information contained within it a certain way. Now, if you remember back to stacks this certain way was through the lifepo methodology, or lastin. First out with the last element pushed onto the stack would always be the first one we popped off, similar to a stack of books that we add to and remove from. Now in contrast, the queue follows what's known as the FIFO methodology, or first in first out, where the first element added to the queue will always be the first one to be removed. We can think of this as a line to your favorite amusement park ride. Mine as a kid was always the Ring of Fire. So let's just use that one as an example. The first one to get there, assuming we don't have any cutters will always be the first one who gets to go on the ride. The later you show up, too long do you have to wait. This is the strategy used for cues when adding and removing objects. The first element to be added will also be the first one to be removed. Another big difference between stacks and queues is the location we add and remove elements from. You might remember that with the stack, we added and removed elements from one spot the top. With the queue. However, this is different. We add elements, the back, also known as the tail, and we remove them from the front, also known as the head. This allows us to make sure that we 100% follow the FIFO methodology. So there's your background information on the queue, sequential access FIFO methodology, add elements the back and remove them from the front. Got it? Good. Now, let's dive headfirst into how we can actually use a queue by jumping into some common queue methods. So just like the stack, we're going to have two methods used to add and remove elements from the queue. For the stack, we added elements with push and removed them with pop. In contrast, with a queue, we add elements using on cue and remove them using dq. In addition to these two methods, we're also going to cover peak and contains, which if you watched the previous segment should look pretty familiar. Alright, let's start. on cue is the first method and the one we use to add elements to the tail of the queue. It takes in an object to put at the end of the queue, and simply adds that object, we'll also increasing the size of the queue by one, let's pull up an example queue, which can see currently has a size of zero. But say we call on queue on a completely random string, let's say the string now, that would be added to the tail of the queue, and the size would increase by one. Now, because there's only one element in the queue, at this point, the string now is acting as both the head and tail for us. We can of course fix that by on queueing a few more completely random strings. If we add the string video, the size goes to two, we can add this and it goes to three, like makes it four, you get the idea. And now we have a fully functional queue, as you can see, like is acting as the tail being that it was the last string added. And now is being treated as the head which makes sense considering it was the first string to be added. Moving on. dq is the method we use to actually remove elements from the head of our queue. It takes in no arguments and will return the element that was removed from the queue back to the user. So if we ran a dq command on our example queue, you'd see that now is both returned back to the user and also removed from the queue. Additionally, the size of our queue has been dynamically decreased by one. If we run it again, video is returned and removed from the queue and the size goes down by one yet again, you get the idea. We can keep doing this for every element in the queue until it's empty. But the next methods we're going to talk about need some information to work with. So for now, let's refill the queue back to its original four elements. The next method that I'll discuss is peak. Now we've actually covered peak just a few moments ago in our segment on stacks. But if you forget or just didn't watch that particular part, peek returns the object that's at the forefront of our queue. It doesn't take in any arguments and simply returns the foremost object of the queue without actually removing it. The key word there being without this method allows you to look at the head of the queue before you actually be queuing. There are a multitude of reasons that you might want to do this, maybe to make sure that the element that you're about to dq is the correct one, or to check to see if an element you need is still there, etc, etc. Whatever the case is, we can use the peak method to fulfill our needs. If we were to run it on our example queue, you'd see that the string now is returned. But if we dq the first two elements and run it again, you'll see that the string This is returned, pretty simple, but extremely effective. Again, let's add video And now back into the queue for our next and final method. That method of course, being the contains method, the name pretty much says it all. The contains method will take an object and will return a Boolean of whether or not the queue contains that object. running it on our example queue with an argument of queue would return false because as you can tell, there is no q string contained within our queue. However, if we ran it on a string, such as video, it would return true because as you can see, the string video is indeed in the queue. And there they are all together now on cue dq peak and contains four methods which will help you utilize a queue to its maximum efficiency. Speaking of efficient See, that takes us perfectly into the time complexity equations for the queue. Now accessing an element within a queue is going to be Oh event, let's say you had a queue full of three elements. If you want the object at the tail, you first have to dq every element off the front into the one you're looking for is the head of the queue, Then, and only then can you actually get the value contained. Since this may require you to go through the entire queue of size n, accessing is going to be O of n. Remember, now queues are sequential access data structures and not random access, meaning we can just grab an object from the middle of the queue. That's just not how things work. Searching is going to be O of n for the exact same reason, trying to find an element contained at the tail of a queue requires you to iterate across the entirety of that queue to check for it. So in that scenario, we have to check every element within that queue of size n, making the time complexity equation O of n, inserting two and deleting from a queue are both going to be an instantaneous o of one. This is because just like with the stack, we're only ever on queuing at a single point, and we're only ever D queuing at a single point. This means that no matter how large the size of the queue is, it will always take the same number of operations for any magnitude to either insert or remove an element. And there they are, in all their glory, the time complexity equations for the queue, you may notice that they're identical to the stack, which if you've been paying attention is the truth for most of the properties about a queue. They're very much a yin and yang, one in the same type deal different slightly in terms of the methodology you use to add and remove objects from them. You'll often see these two data structures talked about together frequently, just because of how similar their functionality is. The final thing I want to cover on the topic of queues are just some common uses for them within programming. What are these things actually used for? And the answer is quite honestly, a lot on your computer. Right now. queues are being used for what's known as job scheduling. The process through which the computer determines which tasks to complete for the user. And when like opening up a web page or a computer program. It's used many times in printers to keep track of when multiple printers Try to print and determining whose documents get printed first. Heck, if you're looking for real world examples, Google even uses cues and their new pixel phones to enable what's known as zero shutter lag, in which they strategically use cues to eliminate the time between when you take a picture and what the phone actually captures. So yeah, in terms of functionality, the queue can be used in a variety of fields. So it's good now that you know what they are and how to use them. This also concludes our discussion on the queue. To review the queue is a sequential access data structure, which follows the FIFO principle, were first in first out to add elements to the back and remove elements from the front. Up next, we'll continue on the sequential access data structures train and talk about one of my personal favorite data structures. Next, we'll be covering the linked list, and it's a good one. So strap into your seats. Let's just jump into things by talking about the elephant in the room. What exactly is a linked list? Well, to answer that a linked list is a sequential access linear data structure in which every element is a separate object called a node. Each node has two parts, the data and the reference, also called the pointer, which points to the next node in the list. Wow, that is a pretty big sentence with a lot of ideas within it. So let's break it down part by part. The sequential access part of that statement means that the information contained within the link list data structure can only be obtained through a certain methodology. We talked about this in our segment on the stack. If you remember, during that segment, we compare them to a tape measure. Because Similarly, you can only get measurements from a tape measure through a specific method. The specific method for a linked list will be covered a little bit later for moving on. The linear part of the definition simply means that the information or data is organized in a linear fashion in which elements are linked one after the other. Now when we state that every element is a separate object called a node, this means that unlike an array or an ArrayList, where every element is just let's say a number, each element in a linked list will actually be an object which can have multiple attributes or variables. And I won't dive too far into object oriented programming right now. If you want a good introduction to that, you can check out the link in the description below, which will take you to our introduction to object oriented programming lecture. For this series, we essentially mean that the objects or note stored within each element of our linked list will hold two separate pieces of information. These two pieces of information come together to form the node. And these nodes are what make up our linked list. More specifically, those two pieces of information are the actual data or information stored within that node, and the reference or pointer to the next node in the linked list. The data is where our strings or integers or Boolean values are kept the actual contents of our data structure. And the other piece of the puzzle is the pointer. This reference or pointer points to where the next node in the linked list is stored in memory. This helps link all of the nodes in a linked list together to form one long chain of information. Kind of like a computer science conga line, you should now be able to understand what a linked list is. Again, it's a sequential access linear data structure in which every element is a separate object called a node, in which each node contains both data and a reference to the next node in the linked list. This cumulatively creates a string of nodes which we call the length list. Feel free to rewatch this explanation if you're still confused, because I know it can get pretty hectic pretty fast. The next thing I want to do in this section is just visualize how one of these linked lists is set up. That way, you can actually see what I mean when I say nodes, error, and pointers and everything else, etc. So every linked list starts with what's known as the head node of the list. This is just an arbitrary label, which represents a node containing some form of data, and also a reference to the next node in the linked list. For now, let's just store the integer one in our head note. Now, since this is the only node so far in our linked list, the head node will simply point towards a no value, which is just to say it's not pointing anywhere. Essentially, it's pointing to nowhere and just being used as a placeholder until we actually give it something to point towards. Let's do that by adding another node to the link list and store the integer to inside of it. Instantly, you can see that now our head node points to this second node instead of a null value. You'll also notice that this new node, which I'll call the two node points to a no reference value, just like the one node used to do, as it is now the last node in the linked list. This is a pattern you'll notice as we continue adding nodes. The last node in the linked list, also known as the tail node will always point towards a null value. This is our way of telling the computer that we reached the end of our linked list and that there are no more nodes to traverse towards. Anyways, let's now create our third node and inside store the integer three, this node now becomes the tail end points towards a North value. And the second node that we created, the one that used to point towards a null value. Now points to this new node which contains the integer three. Essentially, we're just adding a node to the tail of a linked list and then setting the references of the previous node to point towards that new node. And if you can understand that concept, you pretty much comprehend the gist of how these nodes interact. The two main takeaways are that every node has both information and a pointer. And the last node in a linked list points towards a null value. That's pretty much the setup for a linked list. Definitely more abstract and fluid than, say an array or ArrayList, but hopefully not too complicated. Next up, we're going to be discussing how we add and remove these nodes from a linked list. Unfortunately, for us, adding and removing elements from a linked list isn't going to be as simple as it was with the other data structures such as the stack or the queue. This has to do with the simple fact that we are actually able to insert and remove elements easily within a linked list at any location. With a stack or a queue, we weren't actually able to do this because the data could only flow in and out of a few specified points. This made it so we couldn't remove an element from the middle of the stack or jam an element in the center of a queue. The way that they're structured just didn't allow for this. Using link lists. However, we can easily remove elements from the middle or jam elements in the center. And so today, we'll be covering the different methods used to do so. More specifically, we'll be covering three different ways to both insert and remove nodes from a length list. Adding to and removing a node from the head, the middle and the tail of a linked list. These are all going to revolve around those pointers we talked about at the beginning of the episode. Because whenever we change up a node in a linked list, we also have to change its pointers. And that can get pretty complicated pretty quickly. Luckily, that's why I'm here. I've set up a basic linked list on your screen now with three nodes that we can use to play around with. Each node has a value of course representing the data inside the node, and also a pointer which points to the next note, the green and red coloring on the nodes with the integers one and three, simply indicate which node is the head node and which node is the town node. Green means head node, and red means it's the tail node. Now in an actual link list, these pointers would be locations in memory. But for the purpose of this series will be representing the pointers visually perfect. So let's cut the chitchat and just jump right into it. The first method we're going to be covering is adding and removing nodes from the head of a linked list. Lucky for us, this is pretty simple. To add a new node to the head of a linked list, literally, all we have to do is make that new nodes pointer point to whatever the current head node of the linked list is. By doing this, we simply take away the title of head node from the current head and bestowed upon this new node that we're adding, it looks a little bit something like this. Let's take a node with the integer zero and add it to the head of the linked list. All we have to do to do this is set its pointer to point towards the current head node. Now, you'll see that none of the other nodes have changed in the slightest. The only thing that has changed is that this new node with integer zero is the head node, and it now points towards the old head node. extremely simple, and the best part is that it works in reverse as well. Removing a node from the head of a linked list is just as simple. All we have to do is set the head nodes pointer to some null value. Once we do, the head node will get cut off from the flow of information essentially removed from the linked list. If we did this on our example linked list, you'd see that once we set the zero nodes pointer to No, because it no longer points to the one node and no node points towards its location. It has been cut off from the rest of the nodes and exiled from the linked list. The old head node regains its position. And it says if this integer zero node never even exists. Moving on the next methods we're going to cover are inserting and deleting a node from the middle of the linked list. These two methods are definitely the most difficult of the bunch. And this is because we need to insert the node in such a way that the pointers get readjusted accordingly without losing any of the information. If we accidentally set the pointers wrong, or do things out of order, the data could get lost forever. But luckily, I'm here to teach you how to do it the right way. We'll start with adding a node to the middle of a linked list. Adding to the middle of a linked list is a two step process. We first make the pointer of the new node point to the node after the location we want to insert at. Then we set the node before the location we want to insert that to point towards the new note. So if we wanted to insert a node with the double value 1.5 after the node containing the integer one, but before the node containing the integer two, what we would first do is set the new nodes pointer to point to the node containing the integer two, then we would make the node containing the integer one point towards our new node which contains the double 1.5. By adjusting the pointers of the nodes before and after the location, we want to insert that we can strategically jam this node in the correct place without moving or modifying the entire list. As you can see, now we have a linked list of length for where the link has not been broken and is also still contiguous. Removing a node from the middle of a linked list is even simpler. All we have to do is make the pointer of the node previous to the one we're removing to now point to the node after the one removing. Then if we set the pointer of the node we want to remove equal to ignore value, we again cut the node off from the linked list and it is removed. Simple as that. So following these steps, if we now wanted to delete our 1.5 node, we'd make the one node again point towards the two node instead of the 1.5 node. Then if we delete the pointer of the two node, by setting it equal to null, it gets cut off from the flow of the linked list. No changes are externally made to the rest of the list. Just one lonely node removed. The final type of insertion and deletion we'll be covering is inserting to and deleting from the end of a linked list. Doing this simply requires To modify the tail node of the length list, the one which currently points towards some null value. for adding a node to the tail, you simply make the current tails pointer, which is currently set to no to point towards the node you want to add. So if we wanted to add a node with the integer four, we would just make the three node point towards this new node. Then by setting the four nodes tail to point towards No, we've made this new four node the tail of the linked list, and the old tail node with integer three now points to the new tail. Removing the tail of a linked list is just as simple. If we want to remove the tail, we just set the previous tail to point towards a null value instead of the current tail. This leaves the current tail node with no connection to the linked list, isolating it from the pack. Doing this on our list would look like making the three node point towards No. And now because no node now points to our tail node continuing integer four anymore, it gets cut off from the linked list and essentially removed making the old tail node, the one containing the integer three, the current tail node once again. And so after a bit of pointer readjustment, hopefully now you can understand the pseudocode behind inserting and removing elements from a linked list with ease. Up Next are the time complexity equations for a linked list. Now accessing an element within a linked list is going to be O of n. This is because linked lists, again are sequential access data structures. This should be all too familiar if you watch the sections on stacks and queues. But if not, let's just use a simple review. sequential access data structures can only be accessed through a particular way, meaning we can't get any element we want instantaneously. resulting from this is the stipulation that if we want to get a certain element within a link list, we need to start at the beginning of the list and cycle through every node contained within it. Before we can finally access the one that we want. We do this by using the pointers as a little map for the computer. First go to node one node one's pointer gives you the location of node two in memory. And node twos pointer will take you to the memory location, which contains the node with the information that you want. It's like a computer science treasure map and a way for a linked list of size n. This means you could have to traverse the entire linked list before finding your information making it's time complexity equation O of n searching is O of n for the exact same reason, we check a node and if it's not the one that we want, we use that nodes pointer to take us to the next node and check that one. We do this for every node until we either find the node containing the value we want, or get to a node which points towards no indicating that we've reached the end of the linked list that the value that we're searching for just isn't contained within that particular length list. Inserting and deleting from a linked list is a little bit complicated. Since linked lists usually only store the head and sometimes the tail nodes location in memory permanently. If we want to insert or delete an element at the beginning or end of a linked list, we can do so instantaneously using the methodology we talked about previously. However, if we want to insert a node within the linked list, things become a little bit more complicated. In order to insert a node within the link list, we must first traverse to the location we want to insert it. This means following that treasure map until we reach the insertion point. And then and only then can we actually follow the instructions to insert or delete a note. So depending on how and where you want to insert or delete a node at its time complexity equation will be either O of n or o of one a little confusing, yes, but necessary dimension for when it inevitably comes up. So in review, accessing searching, and sometimes inserting and deleting are all going to be O of n. And other times inserting and deleting will be instantaneous. Cool. Now let's finish up by talking about some real world applications of the linked list. Now, something I haven't talked about before, but would be scolded by people in the comments for not mentioning is that linked lists can actually be used in the backing of other data structures. What do I mean by this? Well, basically, we can use linked lists to make stacks, queues and some other data structures we haven't talked about. This is in the same vein as earlier when I mentioned that the ArrayList uses the array as a backing support system in memory. Now this goes a little bit above an introductory series, but it's one of if not the most important uses of a linked list in computers. Science. So I thought I'd mentioned it here. Basically, because of the way that it's structured. Having a stack or queue use the nodal methodology that we talked about during this episode, which comes with the link list to be the back end of it structure makes a lot of sense. If you're curious, I'll have an article linked below explaining this idea in better detail. I just thought it was a little bit above the complexity for an introductory course. Now, that's not to say link lists can't be useful in other ways. A queue on Spotify, for instance, where each song in the queue doesn't contain just an integer or a string, but an entire song with WAV data, a title, a length, etc. Then, when the track is done, it automatically points to the next song in the playlist. Another example could be a photo viewing software where each node is an image, and the pointers simply point to the next photo in the list. Like I said, the main functionality of a linked list might be to back other data structures, but it also has tons of uses in other areas of expertise. In Review, a linked list is a sequential access linear data structure, in which every element is a separate object called a node containing two parts, the data and the reference pointer which points to the next node that comes in the linked list. These pointers allow us to easily shift around each node, adding or removing it without having to move massive amounts of data like we would have to do in the case of an array or some other data structure. One thing I haven't mentioned yet about linked lists is that this pointer system does come with one big drawback. With a normal linked list, we can only ever go forward with our pointers never backwards from the computer's eyes. Once we follow a pointer to a certain nodes, location and memory, there's no way to go back or undo to the previous one. Much like a shark, we can and only will ever go forward. This problem is fixed however, through the use of a data structure known as the doubly linked list, which coincidentally is the subject of the next segment in this video smallworld All jokes aside, Next up, we're going to be covering the doubly linked list so strap into your seats. A doubly linked list is almost exactly the same as a linked list. That is to say it's a sequential access data structure which stores data in the form of nodes. Except with doubly linked lists. There's one small difference. With the doubly linked list, we're able to traverse both forwards to the next node in the list and backwards to the previous node in our list. Again, using pointers. How? Well let me explain. With regular link lists. Each element was a node composed of both a data section and then a pointer which would take you to the memory location of the next node in the list. Using this, we were able to traverse a linked list easily to search for information, access data within a link list, or add and remove nodes from within the list with ease. A doubly linked list simply builds upon this by also having a pointer which points to the previous node location in memory. It's an undo button of sorts, which allows you to fluidly go through the link list in either direction, instead of just limiting yourself to go in one direction. This is great since it allows us to jump around the linked list and have a lot more flexibility when it comes to modifying information. Now because this can get pretty confusing very quickly, I want to do a visualization. Before that though, let's use some lingo to help consolidate the terminology that I'll be using. When I refer to a nodes. Next, I'm referring to that particular nodes pointer which points to the next object in the list, whether that be another node or no value. Similarly, when I refer to a nodes previous abbreviated to prevx, on the vigils, I'm talking about its pointer which points to the previous object in the linked list, again, either another node or no value. Doing this just helps keep things a little bit more simple because as you'll see having both a previous and a next pointer makes things a little bit more complicated. Now on to what these doubly linked lists actually look like. Just like with a regular linked list, every doubly linked list is going to start with a head note. Since it's the first node in the list, both its previous pointer and its next pointer will point towards a null value. This is of course, because it can't point to information which isn't there. We can fix this though by adding another node. Once we do, you can see that a few things have changed. The head nodes next pointer now points towards this new node instead of No, the new nodes previous pointer now points to the head node. And the new nodes next pointer now points towards a null value. As you can see, it's Little bit more complicated than adding a node to a regular link list in which we only had to worry about one set of pointers as opposed to two, but still manageable. Let's add one more node for demonstration purposes. And you'll see the same thing happens again. The second node now points to this new third node. And this third node gets initialized with a pointer which points both to the previous second node and also forward to some null value. You can see that with any two nodes, the next pointer of the first and the previous pointer of the second come together to form sort of a cyclical connection which ties the two nodes together. Then at the head and tail of the list, there's a connection which ties them both towards some no value. Most of this should be common sense, but it's still good to go over. doubly linked lists are a little scary at first, but once you break them down, you realize that they're just an evolved form of linked lists. of next we're going to be talking about adding and removing nodes from a doubly linked list. Again, using the three different methods that we talked about in the previous segment, adding and removing from the head, the middle, and the tail. I've also set up a basic doubly linked list with three nodes containing three strings to help us out with this next segment. Finally, just like the last segment, the green on top of a node signifies that it's the head node, and the red signifies it's the tail node. Alright, with all that being said, let's get into it. Now adding to the head of a doubly linked list is quite simple. The first step is to take our new node that we want to insert and set its previous to no second, we set the new nodes next to point towards the current head node of the linked list. Then all we do is set the current heads previous to point towards this new node instead of a null value, and we're set to go. Doing this rearranges the pointers in such a way that the new node we want to add becomes the head of the doubly linked list. Pretty simple. So if we wanted a new node with the string, aim to be the head of our doubly linked list, we would just follow the steps. First, we set the aev nodes next to point towards the atom node, then we set its previous to point towards the null value. And finally, by setting the atom nodes previous to point towards the AV node, we have successfully added that node to the head of the list making it the head note. Removing a node from the head is even simpler, we first set the head nodes next to point towards a null value. And then by setting the second nodes previous to also point towards a null value, we can easily remove the head node from the list. This is how it would look in our example, we'd first set the aid nodes next to no, then we would set the atom nodes previous to also be no and then we're done. Now because the aid node doesn't have anything to point towards, nor does it have anything pointing towards it, it will be deleted from the list. The Atom node will regain its position as the head node and the program will carry on. Up next is inserting and deleting from the middle of a doubly linked list. This is where things get really tricky, so make sure you're paying attention. To insert a node into the middle of a doubly linked list, we follow a three step process. The first step is to take the node we want to add and set its previous to point towards the node previous to the position we want to insert at. Then we take that same node the one we want to add. Instead it's next pointer to point towards the node after the position we want to insert. Finally, we set the next on the node at the position before the one we're inserting, and the previous on the node after the position we're inserting to both point towards this new node. That is a lot of words, some of which might not even make sense to you. So let's open up the space on our screen real quick and do an example with our list. Let's take a new node with the string Chris and add it between the Adam node and the bed node by simply following our steps. First, we want to set the new nodes previous to point towards the node previous to the position we want to insert at. This means setting the Chris nodes previous to point towards the Adam node. Simple enough. Second, we set the new nodes next to point towards the node after the position we want to insert that this entails setting the Chris nodes next to point towards Ben. So far, so good. Then the last step is to set the node on the next before we're inserting and the previous on the node after we're inserting to both point towards this new node. So in this case, the atom nodes next and the Ben nodes previous both get set to the new Chris node. This completes the addition into list. Obviously, this is a little bit more complicated than inserting or deleting from the head. But as you can see, we've now inserted this new node into its rightful place. It may be hard to see since the pointers are a little bit messy, but the flow of the list has remained constant without any breaks in the pointers, which is the most important part. Removing a node from the middle of a doubly linked list is also a three step process. First, we set the node before the one we want to remove next to point towards the node after the one we want to remove. Then we set the node after the one we want to remove the previous two point towards the node before the one we want to remove. The final step is to set both pointers of the node we want to remove to point towards a null value. Again, a little complicated, so let's take it to our example list and try it out. Let's delete the Chris note just for the sake of keeping things consistent. Following the steps that we've laid out, we would first set the next of the node before the one we want to remove to point towards the node after the one we want to remove. So in this case, we said the atom nodes next to point towards the Ben note, essentially skipping over the crisnet. Then we set the previous of the node after the one we want to remove to point towards the node before the one we want to remove. So we set the bend node to point towards the atom node, again skipping over the Chris node. Finally, we have to set the Chris nodes pointers to point towards null values. Now that we've done this, because no nodes point towards it, and it doesn't point towards any nodes, the Criss node gets deleted from the doubly linked list, the list is back to its original form with no breaks or errors. Adding a node to the tail of a doubly linked list is also a three step process. Step one entails setting the next pointer of the current tail to point towards the new node, we want to become the tail. Step two is setting the previous of the new node that we're adding to point towards the current tail of the list. And step three is making the new nodes next point towards a null value. Again, let's do an example where we add a new node containing the string Ethan to the tail of the doubly linked list. Following our steps, we first set the next pointer of the current tail, the coral node to point towards the Ethan node. Then we make the Ethan nodes previous point towards Carl, and it's next to point towards an O value. And we're all set. Ethan has been successfully added as the new tail of the list in three quick and easy steps. Removing from the tail of the list is even easier and only requires two steps. We first set the tail nodes previous to point towards No. And then we set the second to last nodes next to also point towards No. On our example list, it looks like this. We first set the Ethan nodes previous to point towards No. Then we set the coral nodes next to also point towards No. This isolates any pointers going to or from the Ethan node and deletes it from the list making the coral node the tail once again. adding and removing information from a doubly linked list might sound like a hassle. But remember, we only have to use this pseudocode to program these functions once in whatever language we're implementing them in. And then we're able to reuse them an infinite amount of times. Now for time complexity equations, since the doubly linked list is really just an extension of the linked list, it's time complexity equations are going to be exactly the same as they were for the link list. And for the exact same reasons, O of n for all four cases and sometimes over one for both inserting and deleting. If you didn't watch the linked list segment and are wondering how we arrived at these equations, you can check the description for a timestamp which will take you to the discussion we had on linked lists time complexities. Finally, in terms of doubly linked lists, I just want to talk about some of the uses for a doubly linked list because there are a ton. The back and forth functionality of a doubly linked list lends itself to be implemented in a lot of stack like functionality, ie cases where you want to go back and forth between information that you're storing for the user. Some examples of this could be the browser caches which allow you to go back and forth between webpages, the Undo redo functionality in many programs, applications which allow you to utilize an open recent functionality. The list goes on and on. Basically any case in which you need to store a list of objects with multiple attributes, a doubly linked list is going to be a safe bet to get it done. linked lists and their evolved form and the doubly linked list are a great way to store information because of the adaptability of the nodal structure. Since we're not working with raw information like primitive types, and we're storing All information inside of a shell, it makes it a lot easier to move the information around. This combined with the pointer system allows for non contiguous but still fast operating speed, making these two data structures a staple in computer science. Up next, we'll be covering dictionaries. And with that a little mini lesson on hash tables. This is the last of what I refer to as the intermediate data structures at the beginning of the lecture, and is honestly personally one of my favorites. So let's not wait any longer and just jump into things. Before we get too far, though, we should probably clarify that when we say dictionary, we're not referencing that thick book you probably have lying around your house and haven't used in years. Actually, dictionaries are one of the most abstract data structures which exist in computer science and can be used for a variety of purposes. Another thing I'll clarify really quickly is that dictionaries are also sometimes called both maps and associative arrays by the computer science community. This moreso has to do with language specifics and preferences and not any functional differences, since all of them work in almost identical ways. But for this series, we're going to be referring to them as dictionaries, just to keep things simple and organized. Now a dictionary in computer science, by definition is an abstract data structure, which stores data in the form of key value pairs. This means we have two main parts to each dictionary element, the key and the value. Each value within a dictionary has a special key associated with it. And together they create a pair which is then stored in the dictionary data structure as an element. Think of a key value pair like a social security number. Each social security number is a key, which is then paired with a value, that value being an individual person. These social security number key value pairs then come together to form a dictionary of every human being living in the United States. This is very different from many of the data structures we've talked about previously, because we index dictionaries using these keys instead of a numerical index. For example, with an array, we would index each element within the data structure according to a numerical value, which started at zero and ran the length of the array. With a dictionary, however, we index each element by using its key instead of some arbitrary integer and obtain information through that index instead. So what exactly are these key value pairs going to look like? Well, they can be just about anything. The keys and a key value pair can be any primitive data type that you can think of, we can have a dictionary which has integers as the keys, one with strings as the keys, one with doubles as the keys, there's really a lot of flexibility. As for the values, but we have even more flexibility with those. We can have keys and our dictionary correspond to pretty much anything, strings. Sure. Billions, of course, another dictionary with its own set of key value pairs, you can even do that too. This allows us to have tons of combinations in the way that we store our data, making dictionaries extremely powerful. as powerful as they are, though, there are two extremely important restrictions that we have to cover when it comes to dictionary. And they are as follows. Every key can only appear once within the dictionary, and each key can only have one value. Let's start with the first one. Each key has to be unique and cannot be replicated, duplicated, cloned, copied or anything else that would cause there to be two keys of the same value in a dictionary. Think of it like this, when you create a key value pair, the computer creates a little locked box in memory to store the value, then the computer spends days and weeks creating a customized handcrafted one of a kind key that corresponds directly to that locked box. This key cannot be replicated in the slightest. And that's the same mindset you should use when working with dictionaries, no two keys can be the same. If you were to try and make a dictionary with too similar keys, you would be thrown in error by the computer. The second stipulation is that each key can only have one value. Think back to our custom key example. It wouldn't make sense for this one of a kind key to be able to open multiple boxes. The same is true for our keys and computer science. They can only correspond to one value. Now one rule that we don't have to follow is that there can be duplicates of values within a dictionary. Meaning we can have two separate keys both point towards equivalent values. As long as the keys are different. The computer does not care. Alright, now that we know what dictionaries are and how they work, let's jump into the time complexity. For a dictionary, or at least try to, let me explain. Now for a dictionary, the time complexity equations are a little bit funky. Previously, we talked about linked lists and how they are sometimes used as the backing of other data structures in memory. For example, a stack might implement the linked list structure as its way of storing information. Well, the most common way to initialize a dictionary is to implement it with what's known as a hash table. Hash tables are a little more advanced than the content I wanted to cover in this series. But they are a huge part of dictionaries, especially when it comes to time complexity. So we're going to give a little mini lesson on them today. Doing this will help you better understand the time complexity equations for a dictionary, as well as give you some background on Advanced Data structures. Alright, let's begin with a little thought experiment. Imagine this scenario for a second. say we have a dictionary which contains a few key value pairs like shown on your screen. Now, let's just assume that to store all this information in the computer's memory, we use an array like how it is shown on your screen now, with a position that a cell is in the table also corresponds to the key and the key value pair stored in our dictionary. So the zero with cell would contain a key with the integer zero, which leads to a value. The fourth cell would contain a key with the integer four, which leads to another value, and so on. Any cell which we don't have a key for is empty, or what the computer scientists refer to as nil. Why does this not work? Why can't we just use an array like this to back our dictionary in memory, since it looks like actions such as accessing and searching would run in constant time, since all the keys correspond to their table values in memory, all we'd have to do to get a value is simply reference the key at the index we're looking for? Well, this actually isn't the case, because it's based upon the simple assumption that the size of the array is not too large. Sure, this might work great for this dictionary where we have 10 or so elements. But let's say instead of 10 elements evenly spaced out, we want to have one which has 10 values, which are sporadically placed from one to a billion. We want to keep the keys as being in the same index position as their values, so we can know exactly where they are and reference them easily. But by my count that is 999,999,990 nil values, just taking up space in memory. And that is not good whatsoever. There's got to be a better way to do things, right? Well, this is where our hash tables come in. Hash tables are fundamentally a way to store this information in such a way that we're able to cut down the amount of nil values, while also allowing for the information to be stored in such a way that it is easily accessible. Basically, by using hash table, we'd be able to store these 10 keys in our dictionary, which range from one to a billion in a table with only 10 elements, while also being able to keep constant time. How is this possible? Well, through the use of a trick program is used known as hash functions. A hash function will take all the keys for a given dictionary and strategically map them to certain index locations in an array, so that they can eventually be retrieved easily. Essentially, by giving a hash function both a key and a table, it can determine what index location to store that key add for easy retrieval later. These hash functions can be pretty much anything which takes in a value and returns an index location. The goal of a good hashing function is to take in a key, whatever that may be, and reliably place it somewhere in the table so that it can be accessed later by the computer. So with that being said, let's just jump into an example. Because I know a lot of you might be confused. Let's say we have our dictionary which contains keys in the form of integers from one to a million by a factor of 10. So the keys are 110 101,000 10,000 100,000, and so on. A good hash function for this data set would be to take the key divided by itself, and then multiply the result by the number of digits in the key minus one. So to find out where to store the value corresponding to the 1 million key, we would take a million divided by itself, which yields one and then multiply that by the number of digits in that integer minus one, in this case, seven minus one or six. This means we'd store the 1 million key at index location six. If we do this for every key in the dictionary, we can see that each key in the key value pair is stored at some index from zero to nine. We've consolidated the 10 keys for One to a billion into 10 index lots instead of a billion. A pretty good improvement in my opinion, you might think this is a little over complicated for storing 10 elements. But remember, sometimes we might be working with 1000s of keys at a time, ranging in the billions, or even strings as keys, which is a whole other story. Now the best part about these hash functions, let's say that we now want to get the value which is paired with a 1 million key, all we need to do is put 1 million back into our hash function, and it will tell us where the value tied to that key is stored at within the table, in this case at the sixth index. Doing this allows us to pretty much instantaneously find where any key value pair is stored at within the table. All of this information may be pretty confusing. If you still aren't 100%, I would recommend watching this part over again. However, in layman's terms, and for this series, all I want you to be able to comprehend is that dictionaries are built upon these hash tables. And the keys in our key value pairs are stored in these hash tables and indexes which are determined by hash functions. And if you can understand that you've got a pretty good base for understanding hash tables. Now the final thing I want to talk to you guys about in regards to hash tables, and the reason dictionary time complexity equations are so funky has to do with one small problem. What happens when we run two different dictionary keys into a hash function, and the computer tells us to store them at the same index location. For example, let's say we have two dictionary keys with the strings, Steven and Shawn. And when we run both of them through our hash function, the computer instructs us to store both of them at the ninth index location, this should be impossible. How are we supposed to put both of them at index location nine. This little problem is what's known as a hash collision and can be solved one of two ways, either open addressing or closed addressing. With open addressing, we just put the key in some other index location separate from the one returned to us by the hash function. This is usually done by looking for the next nil value in the table, ie the closest location which contains no key. So with our example, the key Steven would get hash to the index location nine, mostly because it's a better name. And the inferior key Shawn would get stored at index location 10, because it's the closest open location available. This does make it harder to interact with the data in our dictionary later on, and can lead to problems if we eventually have a value which hashes to the index location 10, which is why computer scientists developed closed addressing. Closed addressing uses linked lists to chain together keys that result in the same hash value. So the keys Steven and Shawn would get stored together in a linked list and index location nine. The main drawback to this is that whenever we want to interact with the value stored within a key value pair for either Steven or Shawn, we end up having to look through the linked list for that piece of data that we want. If there are 200 keys hash to one index, that's 200 keys, we might have to search through to find the one that we want. And that's not very good. And with that concludes our mini lesson on hash tables. Now we can get back into dictionaries, and finally talk about their time complexity equations. Now remember back to the segment on time complexity really quickly. Back then I mentioned that for these time complexity equations, we generally measure a data structure based on its worst case scenario. But when it comes to dictionaries, if we are to assume the worst case scenario, things get a little outrageous, we basically end up assuming that our hash function makes it so that every key value pair ends up in the same index, meaning that each of our keys gets stored in a linked list, assuming that closed addressing is being used, then worst case scenario, we have to assume that every operation functions how it would for accessing searching for inserting or deleting from a length list, which, if you remember is O of n for all four. This is of course preposterous and would probably never happen with the bare minimum decent hash function. Which is why in addition to the worst case, scenario, time complexity equations, I'll also go over the average time complexity equations for these four operations. Now Lucky for us, they're all going to be over one. This has to do with these hash functions we talked about before. To access search for insert or delete a key value pair from our dictionary. All we need to do is run that key through our hash function, and it will tell us what index in the hash table to go to in order to perform that operation. There's no time wasted looking through preset indexes because we, the programmer generate the indexes ourselves. And that's the power of the hash table in the flesh. Overall, dictionaries are a very useful data structure when it comes to computer science for a few reasons. They differ from the rest and quite a few big ways. The fact that they don't use a numerical index to retrieve values, rather a preset key, the notion that those keys and the key value pairs can be a range of types from strings to integers, to chars, and so on the implementation of the hash table, which allows for super quick utilization that is sure to come in handy in any program that you write. In conclusion, and overall, the dictionary is just a solid data structure that can be used to fill in the gaps in your data structures knowledge. And with that, we have now completed all of the intermediate data structures if you will. Next, we'll be moving on to trees and tree based data structures. This is the point in this series where we will be dropping our discussion on time complexity equations, because if you thought that the dictionary time complexity equations were complicated with hash tables and hash functions, trees, and there are many variants only further complicate things to the point where I no longer feel comfortable including them in an introductory course, we might mention that certain tree based data structures are good for accessing or searching. But as for discussions on all four time complexity equations, dictionaries are where we leave things. With that being said, let's move on to the final few data structures as we round out this introduction to data structures series. Next up on our list of data structures is the tree. No, not those big green things that inhabit the outside world and that you can climb on and get fruit from, rather the computer science data structure, which is way more fun in my opinion. Now, before getting into trees, we need to talk about data structures in general. Every data structure we've covered up until this point has been stored linearly. arrays, stacks, linked lists, all of these had a definitive start and end to their data, and you could point them out easily. Even the dictionary could be laid out in such a way to be represented linearly. Everything was so nice and neat and easy to visualize. On the other hand, trees store data hierarchically as opposed to linearly. What does that even mean? Well, besides being an extremely hard word to pronounce, it's an alternative way to store data that we're going to be using for the remainder of this series. It's a little hard to visualize. So I'm going to start with an example. The most common real world example of hierarchical data would be a family tree, each person would be an element of the family tree, and connections wouldn't be based on a simple linear fashion. Rather, connections would be more abstract and can lead to multiple paths or branches, instead of just a single data point. Each generation is ordered on a hierarchy, which is where the name comes from. And as you can see, there's no definitive end to the family tree. Sure, there are the ones at the bottom, which you could argue are the ends of the family tree, but which one of them is the definitive end, there is none. Another example could be a file structure system on your computer, you'd have a base folder, such as your desktop. And then inside of that, you might have multiple other folders for different types of information. And then inside of those, you might have more folders representing more types of information. And then finally, you would have your documents. Again, it sets up a network of those files on different levels, just like how we had with the family tree. Trees and the trees. Many variants are all dependent on storing data hierarchically, which of course finally begs the question of what exactly a tree is. Well, a trees in abstract data structure which consists of a series of linked nodes connected together to form a hierarchical representation of data. The actual information or data is stored in these nodes. And the collection of nodes along with the connections between them is what's known as the tree. This is sort of like a linked list only instead of each node only pointing to one location, it has the option of pointing towards multiple, and also branching off on its own or pointing to no other nodes. Each of the nodes in a tree can also be called vertices and the connections between vertices. What linked two nodes together are called edges. Now the free flowing structure of a tree lends itself to a lot of different configurations. And with that comes a plethora of terminology about certain nodes, vertices, and just the structure of a tree in general. So what we're now going to do is go over a lot of the terms associated with specific nodes based on where they are on the tree and how they're connected to other nodes while also visualizing the general structure of a tree at the same Time. Okay, let's start. Let's begin with the things we've already talked about. A vertices is a certain node in a tree, and an edge is a connection between nodes. The first new thing is that every tree starts with what's known as a root node. This is always going to be the topmost node of a tree. Let's add a node with the integer 50 to serve as our root node. Next up, let's connect the two vertices to our root node using two edges. These two nodes we just added are what's known as child nodes, since they are both connected to the node containing the integer 50. child nodes can thus be defined as a certain node which has an edge connecting it to another node one level above itself. Vice versa, the root node, the vertices containing the integer 50, is now what's called a parent node to these two child nodes. Thus, we can define a parent node as any node which has one or more child nodes. Think back to our family tree. If we were using people instead of integers, it makes perfect sense that the nodes directly connected to each other have some sort of familial relationship. Let's continue on by adding two child nodes to the node containing the integer 20. When we do this, the 30 node becomes a parent node, and the two nodes we've just added become child nodes. We have now branched off from the 20 node completely, and that the two child nodes containing the integers 10 and 15, respectively, share no direct association with it. Speaking of the 20 node, since it does not have any children, it is what's known as a leaf node, or a node in a tree which doesn't have any child nodes. In this context, the 10 and 15 nodes would also be leaf nodes. Finally, let's add one more node as a child of the 10 node and there's our tree in all its glory. As a quick review, the 50 node is the root node, the 30 and 20 nodes are children of that root node and the root node is thus the parent of the 30 and 20 node, then the 10 and 15 nodes are children of the 30 node. And the 30 node is thus a parent node to both the 10 and 15 nodes. The five node is a child of the 10 node and the 10 node is a parent to the five node. And finally, the 515 and 20 nodes are all leaf nodes because they do not have any children. As you can see, one node or vertices on our tree can have many different titles depending on where it is in the tree and what other nodes are connected to it. For example, the 30 node is both a parent node to the 10 and 15 nodes, but also a child of the 50 node. The 15 node is both a child of the 30 node, and also a leaf node as it has no children. This terminology really comes in handy when we start talking about trees which contain 1000s of vertices and edges, and the data becomes very complicated to order and maintain. Moving on the next two pieces of terminology I want to go over with you guys are the height and depth of a tree. The height is a property of a particular tree in it of itself. And the depth is a property of each individual nodes are contained within a tree. Let's start with the height. The height of a tree is the number of edges on the longest possible path down towards the leaf. So in our tree example, since the longest path in our tree, which leads to a leaf is from the 50 node to the five nodes, and there are three edges in that path, the height of the tree would be three. The depth of a certain node is the number of edges required to get from that particular node to the root node. For example, let's take our 30 node. Since there's only one edge connecting it on the way up to the root node, its depth would be one. For the 15 node. However, since there are two edges, which separate it from the root node, the 15 node would have a depth to that's pretty much all you need to know about the terminology associated with trees and computer science. As a review, we have the height and depth obviously. Then we have vertices, edges, root nodes, parent nodes, and leaf nodes. Now there's probably something that's been scratching at the edge of your mind for quite a while now. And that's why the heck are these called trees. They look nothing like trees. Regular trees are not upside down like this data structure would lead you to believe. So who named trees and why? Well, there's actually a simple answer to this. The tree is said to have been invented during the 1960s by two Russian inventors. And the last time that a computer scientists got up from the chair and went outside to actually see a tree is rumored to have happened back in 1954. Ever since then, it's been grinding arrayed screens watching hours upon hours of their favorite youth. superchannel so please forgive them for their confusion when it came to naming conventions, they must have just forgotten trees aren't upside down. Now regular trees, like the ones we just created are great for storing hierarchical data. But their power can really be heightened when you start messing around with how the actual data is stored within them. by imposing rules and restrictions on what type of data is stored within a tree, as well as where we can effectively use the tree data structure to its full potential. I could talk about the different types of trees for a long time, so long that many of them are full segments in this lecture. But for now, I just want to cover one popular variant. This will be a good introduction to how trees can vary without simply diving into a huge deviation from what we already know. More specifically, the tree variant I want to talk to you guys about is the binary search tree. A binary search tree is a simple variation on the standard tree, which has three restrictions put on it to help organize the data. The first is that a node can have at most two children. This just helps make it easier to search throughout the tree as we don't have to spend time looking through each of the eight children for a particular node. Keeping it limited to two helps us do this. The second restriction or property is that for any given parent node, the child to the left has a value less than or equal to itself. And the child to the right has a value greater than or equal to itself. This might seem weird, but it comes with certain advantages and disadvantages over using normal trees, which we'll get to in a bit. The final restriction put on binary search trees is that no two nodes can contain the same value. And this is just to prevent weird things from happening when we begin searching through the tree. Now, how do imposing these restrictions on a tree actually help us? Well, the biggest advantage of binary search trees is that we're able to search through them in logarithmic time. Because there is a natural order to the way that the data is stored. It makes it extremely easy to search for a given value. logarithmic time, if you remember back to the segment on time complexity is the equation in which we get more bang for our buck the greater number of elements or nodes have in our data structure. It works like this. All we have to do when searching is tell the computer to go left if the value we're searching for is less than the current node, and right if it's greater than the current node. We can then wash rinse and repeat this strategy until we find our desired note. This makes binary search trees really popular for storing large quantities of data that need to be easily searchable. Of course, this also translates to inserting, deleting and accessing elements within the data structure. But for the most part, searching efficiency is what really sets the binary search tree apart from the rest. stepping away now from binary search trees and into trees in general, let's talk about common uses for them in computer science. The most common uses for trees in computer science includes storing data with a naturally hierarchical structure. These are like the examples we touched upon at the beginning of this segment. Data Sets such as file structure systems, family trees, a company's corporate structure, all of these could be stored and implemented using the tree data structure very easily. These are all general examples though. As I mentioned before, when we put restrictions on trees, like in the case of the binary search tree, we can expand it to uses even further. A trees based structure is incredibly useful, and it can be modified in so many ways, which only add to its functionality. One of these ways is through what's known as a try and is the next data structure on our agenda. So stay tuned for that. Next up, we'll be talking about another one of the special trees with restrictions. We just finished discussing the binary search tree. And with that we mentioned how trees usually become even more useful once you start setting restrictions on how and where the data can be stored within them. Well, a try is another one of these special trees, which have special restrictions put in place to help store the data in an effective manner. This data structure is often overlooked since it's only used in specific situations. But without the use of tries within those specific situations. Some important features of your computing life would be a whole lot harder. So we get it Steven tries a special tree with restrictions. But what are those restrictions and how can they help us? Well, let's start with the basics. A try is a tree like data structure whose nodes store letters of an alphabet in the form of characters. We can carefully construct this tree of characters in such a way Which allows us to quickly retrieve words in the form of strings by traversing down a certain path of the try. These are also sometimes called Digital trees or prefix trees by the computer science community. We'll be calling them tries for today's lecture. Trees are used in the retrieval of data in the form of words, which is actually where the name try comes from, as it's smack dab in the middle of the word retrieval. Essentially, in layman's terms, we use tries to retrieve words extremely fast by traversing down a tree of store characters. This is hard to explain without actually showing you. So let's do an example of what a try might actually look like. Now, just like with a tree, every try is going to start with a root node only in our case, that root node will be empty with either some null value or a blank string. Now also stored within this node will be a set of references, all stored within an array. These references are set to know at the beginning of the Tris existence, but can be slowly filled with references to other notes. For example, let's say we were creating a try to store words that start with the letter D. The root node would contain an array which contains a reference to a node containing the character D, signifying the start of our word. Now imagine for a second that this D node also has an array containing references. Only this time, it contains references that point towards all characters in the English alphabet, which serves as the first two characters of a word in the English dictionary. So da serves as the first two characters and a lot of English words such as dad, or Tao, or dad if you really want to go that far. And so the array contained within the D node would hold a reference to an a node. Now since there are no English words, which start with DB that I know of, we wouldn't have a reference to a b node. Since we know we will never have to retrieve a word from our try, which starts with dB. We continue this process for all letters in the alphabet, only including references to characters which serve as the first two characters in an English word. But that is a lot to visualize. So for now, let's just put up on the screen two nodes that this first D node would point towards. More specifically, the node, we've already added the a node, as well as the E node. Now to continue building our try, we would simply repeat this process for all of the nodes that this D node points towards. So we'd store pointers in the a node, which would point towards characters and the English alphabet that serves as the first three characters of a word in the English dictionary. So you'd have a B, A, D, A y, and many, many more nodes as well. But we're just going to stick with those three for now. For E, you'd have pointers which point towards nodes containing characters like n and w. Obviously, there'd be more nodes than the one shown on your screen right now for all levels of nodes. But what we've created here is a very simple try. As you can see, it's exactly like I said it was in the beginning, a tree like data structure, which stores characters that can be used to make words by traversing down paths. As we traveled down different paths of our tree, we can make different words, dab, Dad day down the one path, and then and do down the other. By following the nodes downwards, we can easily build strings of words, which you'll learn towards the end of the episode can be extremely useful. We can even take this process further by having one path contain multiple strings. For example, if we isolate the den path, we could continue the process to go on to words like dense or Denver or dent, the list goes on and on. We wouldn't have to just stop at the word then since then, is also a prefix for numerous other words, and that's what makes storing data and character format so useful. One path down to try can represent multiple words depending on where you stop along the process. This does provide a bit of a problem for computer scientists though, how do we know when a word has ended? Let's take the Denver path as an example. If we wanted to retrieve the word den from this try, how would we know to stop at the end node and not continue along to Denver? Well, there's actually a pretty simple fix to this and it's known as flagging. We simply mark the end of the word by having it also point towards a flag to let the computer know that the end of a word has occurred. So in our Denver example, not only would the end nodes array contain pointers to whichever characters follow it, it would also contain a pointer to a node with a flag to tell the computer to stop there. In this case, I've just chosen a period as a flag. This way we can control tries in such a way where an each word is marked by an ending point. And the different words that may branch off from the prefix can continue to use that word as a prefix in whichever retrieval program ends up getting used. Okay, so now it's time to talk about the general use cases of a try. If you remember back to the beginning of this segment, I said that the use cases for a try were limited, but extremely effective. And now we're going to talk about what that statement means. Now, have you ever used the autocomplete feature on your iPhone or spellcheck on a Google Doc, because if so you have already experienced the overwhelming power of tries, as they are used for both of these extremely useful features. This mainly has to do with the fact that for big programs like iOS or Google Docs, they're not just storing a try containing a few words, or even all the words starting with a certain letter, like we tried to replicate their storing the entire English dictionary. storing the entire dictionary in a try seems like a tall task. But it can and has been done. Each node would simply have 26 nodes connected to it. And those 26 would have as many nodes connected to them as needed, and so on. And so now I can hear you asking you in the comments. How does this help us with autocomplete and spellcheck? Well, let's say you're typing out a word. For the sake of this video, let's say it's the word subscribe. You start with the s. At this point, that computer has already eliminated 95% of words that you could be typing. We know it's not going to be a word which starts with n, or with a or o or H or l etc, etc. Then you type the letter U and bam, another 95% of possible options get deleted. With each character you type, you eliminate millions of possible words that you could be working towards. Using this fact, as well as popularity data, which could also be stored within the node, the computer can start making educated guesses using that information, as well as context from previous words, to make a suggestion as to what word you're trying to type out. The AI that works. This feature is immensely complicated, but it is backed by the tri data structure and helps autocomplete work easily on your phone. As for spellcheck, well when you spell a word wrong, a spell checking algorithm can use the root of your word and compare it against try data to make an educated guess as to what you were trying to spell. The slightly misspelled words will usually have a similar path from the root of the try. If you accidentally type chalk a lotta instead of chocolate, the computer can take the first few characters of the word you typed in correctly, and see where you may have gone wrong. Basically, by comparing the misspelled word to certain paths of the dictionary try, they can pretty accurately detect which word you were meaning to say and correct you accordingly. So there you have it, the try. Like I said, it is often underrated in the computer science world since it can only be used in certain situations. But as I hope this segment has shown you, those situations are still important. Nonetheless, we are nearing the end of our introduction to data structures series. Now, only two more left before we part ways. So stay tuned, because now we're going to talk about heaps. Now back in our segment on trees, we talked about the binary search tree, a special type of tree which has a few properties. Each node can only have two children, the child to the left must have a value less than the parent node, and the child to the right must have a value greater than the parent node. And also no two nodes could contain the same value. Well, a heap is sort of like this, but a little bit different. More specifically, by definition, a heap is a special tree where all parent nodes compared to their children nodes in some specific way, by being either more extreme or less extreme, ie greater than or less than. This specific way determines where the data is stored and is usually dependent on the root nodes value. There are two different methodologies generally used in computer science, and they are known as min heaps and Max heaps. In a min heap, the value at the root node of the tree must be the minimum amongst all of its children, and this factor must be recursively. The same for any and all parent nodes contained within the heat. So each parent node must have a value lower than all of its children nodes. As you can see from our example, on the screen now 10 is the root node and also the minimum value in the tree. In addition to this fact, if we pick any parent node on the tree, and look at its children and their children and so on, the parent node will have the lowest value of the mall. Take 14 for example, its value is less than 2631 4435. In 33, this must be the case for every single subtree, which is contained within the heap. Max heaps on the other hand are the exact opposite. In a max heap, the value at the root node of the tree must be the maximum amongst all of its children. And this fact must be recursively, the same for any and all parent nodes contained within the heap. If you take a look at the example max heap we have on the screen now, again, you'll see that this is the case 44 is the root node and also the largest value within the heap. If you take a look at say the sub tree, which is parented by the 35 node, you'll see that 35 is a maximum value amongst all nodes in that subtree greater than both 19 and 27. When we store data like this, and heap, whether that be a min heap or a max heap, it makes it extremely easy to insert or remove from. This lends itself to a lot of useful implementations within computer science. Now to show you this, I'd like to build an example max heap, the one which has the greatest integer stored in the root node. For the sake of keeping things simple, let's pull up an array of seven elements with integers ranging from zero to 100, and simply convert it into a heap one element by one. This can be done in a few easy steps. Step one is we add the first integer in as the root node. So 70 would get inserted into our tree as the root node, then we add another node to the tree in the bottom left most positions available. So we would first insert a node as the child of the root node to the left. For our heap. This means adding the integer for as a child of the 70. The final step is to recursively go up the heap and swap nodes if necessary. Now when we say if necessary, we mean that if the node we just added is more extreme, either greater than or less than the node above it, depending on the type of heap that we're trying to create, we swap them to maintain order amongst the heap. Since we're building a max heap, and 70 is greater than four, no swaps are necessary in this case. Now we just simply repeat steps two and three until we've built our heap. So next, we want to add the integer 90. And since the leftmost slot on the tree is already taken by our four node, the right slot ends up being the leftmost location on our tree. So we insert it there. And then since 90 is greater than 70, we actually end up swapping the 90 and 70 nodes. Doing this helps keep the max heap property intact, where every level is greater than the one below it. Let's keep going. Next, we add 45 to the heap. Now, since we've run out of space, on the second level, we move on to the third level and add 45 as a child of the four node, then we compare it to four, which is greater than so we swapped the nodes. Now we have to compare this node to the node above it again. Because remember, we recursively go up the tree until we reach the root or don't need to swap. Now 45 is not greater than 90, so it stays put where it's at. Next up, we add 23 as another child of the 45 node, only this time on the right side, we compare and since it's not greater than 45, we keep it as is. Moving on, we next insert 76 into the tree as a child of the 70 node, then we compare and swap the 76 and 70 nodes as 76 is indeed greater than 70. We then compare 76 and 90. And since 90 is greater than 76. Keep the 76 node in place for now. The next node we add is the 100 node, we compare it to the 76 node and see that it's greater, so it gets swapped. And then we compare it again, this time to the 90 node. And since it's also greater than that integer, it gets swapped yet again to become the root node, signifying it is the greatest node in the heap. As you can see, it's a pretty simple concept, you add a node and then keep trying to swap up until the node you've added is in its rightful place. Deleting from heap is also pretty simple, at least in our case. This is because the type of deletion I want to talk about is just the process of removing the root node from the heap. And you'll see why later on. To delete the root node from a heap, you follow a three step process. Step one is actually removing the root node from our heap. So in our case, we delete the 100 node. What you do with it is up to the programmer, you can return it back to the user stored somewhere else, print it out, etc, etc. Either way, then step two is replacing it with a node furthest to the right in this case It would be the 76th note. Finally, for step three, we do what's known as a heapify. To fix up the heap, we start with the root node and compare it to its children to see if we need to swap any values. So for the 76 node, we compare it to its two child node. And since 76 is less than 90, we end up swapping those two. Then we wash rinse, repeat for every subtree that we have. So on the right side, we swapped 90 with 76. But since 76, remains the greatest integer on that particular subtree, it stays in the same spot. On the left side, we didn't change anything, but 45 is still the greatest integer amongst the three nodes in that subtree. So no swapping is necessary on that branch of the heap. And so we've completed our heapify, and all heap properties are still intact. That is inserting and deleting nodes in a nutshell. Now let's talk about how we can use this to our advantage. Heaps are most commonly used in the implementation of heapsort. heapsort is a sorting algorithm which takes in a list of elements, builds them into a min or max heap, and then removes the root node continuously to make a sorted list. Because heaps always start with the minimum or maximum value contained within them at the root node, we're able to simply just remove the root node over and over again, keep refining the data structure after every pass. And after every element has been removed, we are left with a sorted list. On your screen, you'll see we have an unsorted list on the left. In the middle, we've inserted all of those integers, and in doing so created a max heap. And then finally, on the right, we have continuously removed those elements from the root node into a now sorted list. heapsort is a really cool algorithm, and it will be part of our upcoming series on sorting algorithms, which is kicking off very soon. So if you're interested in that, make sure you subscribe to our channel so that you don't miss it, a link will be in the description below. Another extremely popular use of heaps is through the implementation of priority queues. priority queues are an advanced data structure, which your computer uses to designate tasks and assign computing power based on how urgent a certain matter is. Think of it like a line at the hospital. You wouldn't want your line to follow a regular queue methodology which implements first in first out. Since then you could have patients with extremely urgent matters like heart attacks, waiting behind people coming in for a routine checkup. And the same way you wouldn't want your computer to update an application before it finishes rendering a video. Otherwise your entire progress would be lost. priority queues take care of all the task scheduling done by your computer, and the heap data structure is used as the backing system for it. And with that ends our discussion on heaps. To review, they are a special tree in which each level contains nodes with values more extreme either greater than or less than the nodes on the level above it. Next up is unfortunately the last segment in the series on yet another tree based data structure the graph. The graph is arguably the most dynamic data structure in our introduction to data structure series and an absolute banger to go out on. So let's just hop right into it. Before we get into the nitty gritty details, I first want to do a short little exercise. Visualize for a second a few of your favorite places to eat on a map around your town. For me personally, it'd be places like five guys chick fil a Panera, Wawa and Domino's. Now imagine for a second that you are ravished absolutely starving. And so your plan is obviously to drive around to each of your favorite places to eat in order an ungodly amount of food from each location. Each food location has a few possible paths going to and from it. And so we add those to the map as well. Now you can see what we now have looks like a network of delicious foods, we can start anywhere, and all we have to do is make sure to hit all five. You may not know it, but what we've essentially done here is set up a simple graph. Basically, graphs are composed of pieces of information like the restaurants and the path set run between them. Of course, this is just generally, by definition graphs are a nonlinear data structure consisting of nodes and edges. There are a finite set of these nodes or vertices, which are connected by edges. nodes and edges should be familiar to you if you watch the segment on trees. The big difference between trees and graphs however, is that with a tree we had a specific starting point. Sure, there were multiple paths down the tree that branched off from the initial starting point. But you always had to begin at the root note. In contrast with a graph there is no specified starting point, we can begin from any node and traverse to any node. Just like how on our food example, we are able to start at any restaurant. graphs are a huge concept and escaped the bounds of computer science often being used in many places you wouldn't even think of. But before we get into anything crazy, though, like the difference between a directed or undirected graph or a cyclic versus cyclical graphs, let's get down the basics. Now, every graph is composed of these nodes or vertices and the edges that connect them. Let's pull up a sample graph really quickly and talk about it. We represent graphs visually like this a lot, because it makes it way easier to comprehend. But notationally wise, a graph actually looks like this, which is much harder to comprehend. So let's break it down. First, we have the vertices set, which contains a comma separated list of all vertices within the graph. That's the simple part. Each comma separated value simply represents a node within the graph. Then we have the edge set, which is a little bit more complicated. Each element of the edge set is an ordered pair which describes relationships between nodes. For example, the first one describes a relationship between the six and four nodes. The fifth indicates a relationship between the five and two nodes, and so on. Using these two sets, we're able to visualize a graph pretty easily by laying down both the information and the connections that fall between them. One final thing I want to mention about the basics of graphs is about the relationships that occur between two nodes. If we have an edge, which connects two different vertices, they are known as adjacent to one another. So for example, the five node would be adjacent to the four two and one nodes. Okay, now that we have the basics down, I now want to jump into the different attributes that a particular graph might have. Starting with directed versus undirected. an undirected graph is one in which the direction you traverse the nodes isn't important. This is most prominently indicated by a lack of arrows pointing to specific nodes. Such was the case with our first example graph, or even the food example from the beginning of the episode. We can hop between nodes or even go back and forth between them without problem. A good way to visualize undirected graphs is like a network of friends on Facebook, where each edge indicates a friendship, if you will. Because of the fact that when somebody accepts to be your friend on Facebook, you are both added to each other's friends list. The friendship goes both ways and direction is unimportant. In contrast, a directed graph is one in which the direction you traverse the nodes is important. This is usually indicated by arrows representing which nodes a certain node is able to traverse to. The edges could point both ways but they don't have to. It's very possible the edge only points one way. A good way to visualize directed graphs is by thinking them as a network of friends on Instagram. Sure, I can follow famous celebrity Will Smith, but the odds that he follows me back fairly low. And so in that case, the relationship only goes one way. undirected and directed graphs both have their uses. As we discussed with the social media example, both provide different functionality which will be useful to you and your computer science journey. Just like the next type of property a graph can be classified as either cyclic or a cyclic acyclic graph is one which contains a path from at least one node back to itself. So you can see by the example on your screen now that the four node leads to the three node which leads to the two node which leads to the one node, which finally leads back to the four node, forming a cycle, essentially, we're able to follow at least one path that eventually leads back to our starting point. A small thing to note here is that all undirected graphs end up being cyclical. The bi directional nature of nodes within undirected graphs theoretically forms a cycle between any two nodes. So judging by that logic, all undirected graphs end up being cyclic. In a acyclic graph is one which contains no path from any one node which leads back in on itself. This property can really only apply to undirected graphs like we mentioned previously. Essentially, this means that for any given node, there is no path which will eventually lead back to itself. undirected directed acyclic and a cyclic are all properties we can use to classify types of graphs based on their nodes. But the last property I want to talk about actually applies to the end. Have a graph instead. And it's the process of waiting. Waiting the edges of a graph means associating a numerical value with each edge, often called the cost of that edge. Each weight represents some property of the information you're trying to convey. For example, again, going back to our food location scenario, since the information that we're trying to convey is a good route, which takes us to each location, a good weight for our edges, and that scenario could be the distance between nodes. This comes in handy a lot, especially with navigation, such as the case with our restaurant example. As we of course, always want to find the path of least cost or weight between the different nodes. So there are the major properties of heap that the different nodes and edges can have directed or undirected, cyclic or a cyclic and weighted or unweighted. There are a couple more obscure ones out there, but those six are what we will be covering today. Combining these three properties together leaves us with numerous types of graphs, which all have different strengths and weaknesses. It would take a while to talk about each of these and there are implementations. So for now, I'll just pick out three types of graphs which are used in the most popular cases. Now probably the most famous implementation of the graph data structure is through the undirected cyclical graph with weighted edges. This one gets a lot of use, especially through its implementation in Dykstra shortest path algorithm. This algorithm given a graph and a source vertex within that graph, compiles a list of the shortest possible paths from that source vertex to all other nodes. As you might be able to tell just from its description, this has a multitude of uses across the entire tech world. Google uses this algorithm for Google Maps. It's used in the process of IP routing, and it can even be implemented in telephone networks. Another type of graph, which you probably use quite often is the unweighted cyclical graphs both undirected and directed, as these make up the follower systems of a majority of social media websites. We already talked about these in the cases of Facebook, which would use a cyclical representation, as well as Instagram which would use an a cyclical representation. However, this example encapsulates much more than just those to Snapchat, Twitter, tik tok, even all these platforms can represent your follower following base through a graph and oftentimes do Facebook even has a graph API, which you can use to interact with the models that they use to illustrate each user's web of friends. As you can see graphs and there are many different forms provide a lot of the functionality that you interact with in everyday life, contributing to almost any facet of the internet. And with that concludes our discussion on graphs. As we review, a graph is a data structure which is represented by a set of nodes and edges. These come together to form a visualization of data, whether that data be food locations on a map or friends on social media. The different types of graphs provide a multitude of implementations in computer science. And with that concludes our introduction to data structure series. After around three hours and 12 data structures, you should now have the basic knowledge to take with you as you continue along in your computer science journey. And if you've made it this long, you obviously liked what you saw. So stick around for an extra 30 seconds while I'll pitch to you why you should check out my personal channel. Now I introduce myself at the beginning of the series, but again, my name is Steven. I'm an 18 year old computer science student who runs a YouTube channel called nullpointerexception with one of my good friend Shawn. We've been producing content like this seriously for about four months now. And I've grown a good audience of around 800 subscribers, which we couldn't be happier about. If you'd like to join us, you can check out our channel linked in the description below and just try out a few of our other videos. And if you like what you see, you can subscribe. That ends my pitch and with that I have nothing else to say. I hope you have enjoyed this series as much as I've enjoyed making it. Thank you so much for watching.
