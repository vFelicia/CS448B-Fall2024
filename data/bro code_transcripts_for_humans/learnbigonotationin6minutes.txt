With timestamps:

00:00 - hey what's going on everybody it's you
00:02 - bro hope you're doing well
00:03 - and in this video i'm going to describe
00:05 - the very basics of big o notation
00:07 - in well computer science so sit back
00:10 - relax and enjoy the show
00:14 - oh yeah big o notation so a common
00:16 - phrase used with big o notation
00:19 - is how code slows as data grows
00:22 - it describes the performance of an
00:23 - algorithm as the amount of data
00:25 - increases and really this notation is
00:28 - machine independent
00:29 - what we're really focusing on is the
00:31 - number of steps to complete an algorithm
00:33 - because some machines are going to run
00:35 - certain algorithms faster than others
00:37 - and three we tend to ignore smaller
00:40 - operations
00:41 - if we had some task that took n plus one
00:44 - we would just reduce it to just n
00:46 - because that plus one really isn't going
00:48 - to make a difference
00:49 - so here's a few examples of big o
00:51 - notation we have
00:53 - o of one o of n o of log n
00:56 - and o of n squared and n is really just
00:59 - the amount of data that we're passing in
01:01 - it's a variable like x what we're
01:03 - focusing on is
01:04 - the performance of an algorithm as the
01:06 - amount of data increases
01:08 - and n is a representation of the amount
01:10 - of data that we're passing in
01:12 - here's a more concrete example i have a
01:14 - function named addup we will add up to a
01:16 - certain number depending on what we pass
01:18 - in as an argument
01:19 - the for loop within here will iterate
01:21 - once up to whatever number that
01:23 - m is add i to sum then return it
01:26 - what if n was a large number like a
01:28 - million well it's going to take just
01:30 - above a million steps to complete this
01:32 - function
01:33 - this function is said to have a runtime
01:35 - complexity of oh then
01:37 - linear time as the amount of data
01:39 - increases
01:40 - it's going to increase the amount of
01:42 - steps linearly or proportionally
01:45 - now another way in which we could write
01:47 - the same function
01:48 - is to take sum equals n times
01:51 - n plus 1 divided by 2. we will get the
01:54 - exact same sum
01:55 - so if n was let's say a million this is
01:58 - still only going to take a couple steps
02:00 - three steps not a million steps so this
02:03 - function is going to have a runtime
02:05 - complexity of o of
02:07 - the input size doesn't matter the amount
02:09 - of data that we have really doesn't
02:10 - matter
02:11 - it's going to be completed in the same
02:13 - amount of steps
02:14 - so this function is way better than this
02:17 - previous one
02:17 - three steps is better than a million
02:20 - steps and the reason that this isn't
02:22 - o of three because this takes three
02:24 - steps is that we really don't care about
02:25 - smaller operations
02:27 - in the grand scheme of things they
02:28 - really won't make much of a difference
02:30 - so we would just shorten this and say
02:31 - that this is of one constant time the
02:34 - input size doesn't matter here's a graph
02:36 - that i made to represent the different
02:37 - runtime complexities we have data on the
02:40 - x-axis
02:41 - represented by n so data will increase
02:44 - as we go more to the right
02:45 - and we have time on the y-axis
02:48 - i did add an asterisk next to time
02:50 - because some of these times can vary
02:52 - depending on what machine you're using
02:54 - so these can vary another way to look at
02:56 - this is
02:57 - number of steps so as we increase on the
02:59 - y-axis
03:00 - these algorithms are going to perform
03:02 - with increasingly more time they're
03:04 - going to get slower
03:05 - so let's begin with of one constant time
03:08 - anything that has a runtime complexity
03:10 - of o of 1 will take the same amount of
03:12 - time regardless of the data size
03:15 - a few examples would include the random
03:17 - access of an element within an array
03:19 - and inserting at the beginning of a
03:21 - linked list so o of one is extremely
03:24 - fast
03:24 - next we have o of log n also known as
03:27 - logarithmic time
03:28 - one example is binary search we haven't
03:31 - talked about this yet
03:32 - we'll talk about a few of these
03:33 - algorithms in future videos but anything
03:35 - that has
03:36 - a runtime complexity of o of log n will
03:39 - actually take
03:40 - i don't want to say less and less time
03:42 - but increasingly
03:43 - less time to complete so as the data
03:45 - size increases
03:47 - this algorithm is going to be more and
03:49 - more efficient compared to the early
03:50 - stages with a small data set
03:52 - of n is also known as linear time as the
03:56 - amount of data increases
03:57 - the time it takes to complete something
03:59 - will increase proportionally
04:01 - linearly that would include looping
04:03 - through the elements in an array
04:05 - and searching through a linked list next
04:07 - we have o of n
04:08 - log n also known as quasi-linear time
04:11 - this would include a quick sort merge
04:13 - sort and heap sort
04:15 - we'll talk about these concepts in
04:16 - future videos so for the most part this
04:19 - is very similar to
04:20 - linear time unless we're working with a
04:22 - large data set
04:23 - then anything using o of n log n is
04:26 - going to start to slow down when we work
04:29 - with larger data sets
04:30 - hence this curve here then we have o of
04:33 - n squared also known as quadratic time
04:35 - a few examples would include insertion
04:37 - sort selection sort and bubble sort
04:40 - as the amount of data increases it's
04:42 - going to take increasingly more and more
04:44 - time to complete anything that has a
04:46 - runtime complexity of o of n squared so
04:49 - just to compare
04:50 - linear time and quadratic time with
04:52 - linear time if our data set was a
04:54 - thousand if n
04:55 - equals a thousand it's going to take a
04:57 - thousand steps they're proportional
04:59 - they're linear
05:00 - but if we were using quadratic time if n
05:03 - was a thousand
05:04 - then a thousand squared would be a
05:06 - million so if our data set was a
05:08 - thousand
05:09 - if we're using quadratic time it's going
05:10 - to take a million steps then
05:12 - way more than linear time so quadratic
05:15 - time
05:15 - is extremely slow with large data sets
05:18 - but in the case of a small data set
05:20 - it could actually be faster as you can
05:22 - see by the graph here and
05:23 - lastly we have o of n factorial also
05:26 - known as
05:26 - factorial time and one place where this
05:28 - is used is with the traveling salesman
05:31 - problem which i might discuss in a
05:32 - future video
05:34 - so this is extremely slow so if i had to
05:37 - give a letter grade
05:38 - to each of these runtime complexities
05:40 - when working with large data sets with
05:42 - constant time this would be an a like an
05:44 - a
05:44 - plus logarithmic time would be a b it's
05:47 - pretty good
05:48 - linear time is c it's okay quasi-linear
05:51 - time
05:52 - is a d it's just barely passing
05:54 - quadratic time would be an f
05:56 - and factorial time well you get expelled
05:58 - from school
05:59 - although some of these runtime
06:00 - complexities are actually faster when
06:03 - working with a smaller data set so keep
06:05 - that in mind
06:06 - well that's the very basics of big-o
06:09 - notation it's notation
06:10 - used to describe the performance of an
06:12 - algorithm as the amount of data
06:14 - increases so if you can give this video
06:17 - a thumbs up
06:18 - drop a random comment down below and
06:20 - well that's big o notation
06:22 - in computer science

Cleaned transcript:

hey what's going on everybody it's you bro hope you're doing well and in this video i'm going to describe the very basics of big o notation in well computer science so sit back relax and enjoy the show oh yeah big o notation so a common phrase used with big o notation is how code slows as data grows it describes the performance of an algorithm as the amount of data increases and really this notation is machine independent what we're really focusing on is the number of steps to complete an algorithm because some machines are going to run certain algorithms faster than others and three we tend to ignore smaller operations if we had some task that took n plus one we would just reduce it to just n because that plus one really isn't going to make a difference so here's a few examples of big o notation we have o of one o of n o of log n and o of n squared and n is really just the amount of data that we're passing in it's a variable like x what we're focusing on is the performance of an algorithm as the amount of data increases and n is a representation of the amount of data that we're passing in here's a more concrete example i have a function named addup we will add up to a certain number depending on what we pass in as an argument the for loop within here will iterate once up to whatever number that m is add i to sum then return it what if n was a large number like a million well it's going to take just above a million steps to complete this function this function is said to have a runtime complexity of oh then linear time as the amount of data increases it's going to increase the amount of steps linearly or proportionally now another way in which we could write the same function is to take sum equals n times n plus 1 divided by 2. we will get the exact same sum so if n was let's say a million this is still only going to take a couple steps three steps not a million steps so this function is going to have a runtime complexity of o of the input size doesn't matter the amount of data that we have really doesn't matter it's going to be completed in the same amount of steps so this function is way better than this previous one three steps is better than a million steps and the reason that this isn't o of three because this takes three steps is that we really don't care about smaller operations in the grand scheme of things they really won't make much of a difference so we would just shorten this and say that this is of one constant time the input size doesn't matter here's a graph that i made to represent the different runtime complexities we have data on the xaxis represented by n so data will increase as we go more to the right and we have time on the yaxis i did add an asterisk next to time because some of these times can vary depending on what machine you're using so these can vary another way to look at this is number of steps so as we increase on the yaxis these algorithms are going to perform with increasingly more time they're going to get slower so let's begin with of one constant time anything that has a runtime complexity of o of 1 will take the same amount of time regardless of the data size a few examples would include the random access of an element within an array and inserting at the beginning of a linked list so o of one is extremely fast next we have o of log n also known as logarithmic time one example is binary search we haven't talked about this yet we'll talk about a few of these algorithms in future videos but anything that has a runtime complexity of o of log n will actually take i don't want to say less and less time but increasingly less time to complete so as the data size increases this algorithm is going to be more and more efficient compared to the early stages with a small data set of n is also known as linear time as the amount of data increases the time it takes to complete something will increase proportionally linearly that would include looping through the elements in an array and searching through a linked list next we have o of n log n also known as quasilinear time this would include a quick sort merge sort and heap sort we'll talk about these concepts in future videos so for the most part this is very similar to linear time unless we're working with a large data set then anything using o of n log n is going to start to slow down when we work with larger data sets hence this curve here then we have o of n squared also known as quadratic time a few examples would include insertion sort selection sort and bubble sort as the amount of data increases it's going to take increasingly more and more time to complete anything that has a runtime complexity of o of n squared so just to compare linear time and quadratic time with linear time if our data set was a thousand if n equals a thousand it's going to take a thousand steps they're proportional they're linear but if we were using quadratic time if n was a thousand then a thousand squared would be a million so if our data set was a thousand if we're using quadratic time it's going to take a million steps then way more than linear time so quadratic time is extremely slow with large data sets but in the case of a small data set it could actually be faster as you can see by the graph here and lastly we have o of n factorial also known as factorial time and one place where this is used is with the traveling salesman problem which i might discuss in a future video so this is extremely slow so if i had to give a letter grade to each of these runtime complexities when working with large data sets with constant time this would be an a like an a plus logarithmic time would be a b it's pretty good linear time is c it's okay quasilinear time is a d it's just barely passing quadratic time would be an f and factorial time well you get expelled from school although some of these runtime complexities are actually faster when working with a smaller data set so keep that in mind well that's the very basics of bigo notation it's notation used to describe the performance of an algorithm as the amount of data increases so if you can give this video a thumbs up drop a random comment down below and well that's big o notation in computer science
