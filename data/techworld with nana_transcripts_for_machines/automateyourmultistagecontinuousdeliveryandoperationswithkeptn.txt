in this video you will get a detailed overview of the new and powerful automation tool called captain which is actually an open source cncf project captain is an sre and devops tool so if you don't know what sre is make sure to watch my recent video on sre because it will make it way easier to understand captain first we'll understand the problem captain tries to solve in the release process and then see in detail how captain works by looking at the three main use cases of captain of automated delivery automated operations and automated monitoring we will look at captain's architecture and visualize how a more efficient and automated release process can look like with captain and at this point i want to thank dinah trace for sponsoring this video so let's get started let's say we have the following release process we have an ecommerce application and we develop a really cool new feature and we want our application users to use that feature so we start the release process we commit the code changes which triggers jenkins build jenkins runs the tests builds a new docker image and pushes to docker registry and then deploys to a kubernetes cluster development environment well that's pretty cool but our goal isn't to deploy to development the goal is to deliver the change all the way to the end users because they should see the new cool feature so we want the code changes to be deployed to production but before that happens we need to test the changes extensively to really make sure that our new feature doesn't break the whole application in production because then users will see nothing and we don't want that obviously so our release will go through the stages like first we run automated tests for example functional tests on the development environment to make sure the new feature edition didn't break anything in the application now in many cases engineers can't hundred percent rely on automated tests only so manual tests are done in addition to really make sure everything works as expected now we're ready for the next stage like staging testing preprod or hardening or whatever you may call it so we deploy the feature to this next stage let's say in our case we call it staging on dev environment we ran functional tests to make sure everything functions properly here we can focus on performance of the application and run performance tests like how does application handle high load of traffic or how fast it responds to user requests and so on again automated tests may not be enough so quality engineers may do extensive manual evaluation to check application performance look at the test results and compare to the previous builds also go through checklists and some predefined release criteria to make sure that application meets all these criteria and only after that if everything is fine and the test results are good the feature will get a green light for deployment to production now even after such extensive testing there is always a little bit of risk that something will go wrong in the actual real environment in production especially in a very complex huge application with lots of dependencies so we don't just throw the new application to the production environment like deploy a new version remove the old one and switch all the traffic to it immediately no instead there are deployment strategies to do this more safely like blue green or canary deployment where we basically deploy the new version and show it to some users to test that everything is fine run it for a couple of hours maybe even days observe and monitor it closely and be ready to roll back the new release anytime if something goes wrong and eventually if everything works fine without any issues we switch 100 of the traffic to the new version and then we can get rid of the old version so as you see this whole process involves a lot of manual work of monitoring testing and evaluating on multiple stages and as you may imagine this may take days and weeks for a completed feature to finally get to production safely so a lot of human work is involved here but what's also important to mention is how many tools and technologies are involved in this whole process as a central piece of this whole release process you have a ci cd tool like jenkins that needs to integrate with various other tools like packaging tools different testing tools docker registry deployment tools for kubernetes like helm and so on so you have this complex pipeline that has lots of integrations and each integration that needs to be managed upgraded and so on plus you may even have a lot of custom logic for doing various things that none of the plugins or services help you with and usually in a company you don't have just one such pipeline you have a separate pipeline for each and every application or project team and these pipelines may be only slightly different from each other like one team may need a higher version of helm and they need a different docker registry than the rest of the teams etc so in this case it's hard to consolidate and reuse the pipelines so all the teams have to configure and maintain their own pipelines so what do we need to make this whole process faster and make the pipelines much more manageable well first of all we need automated evaluation between the stages instead of manual checks automation makes things faster so we want to replace the slow human work with fast automated process and second we want to be able to create reusable pipelines which make it much easier to integrate and plug in different tools and replace those tools for different teams if they want to and that's exactly what captain does and helps you with so how does captain help you solve all these challenges first of all captain lets you define a high level workflow for your release like how do you want to deploy your application changes like what stages what test and deployment strategy for each stage and the evaluation between the stages and it lets you define this workflow easily in a declarative way with yaml in a shipyard.yaml file now note that there is no mention of the tooling here so who is doing the deployment or who is executing all these tasks like running the tests or actually deploying to a specific stage and so on so shipyard defines what needs to be done but where do we define how all this should be executed well those tasks are executed by different tools for example helm will do the deployment to kubernetes jmeter and selenium will run the tests and so on but how does captain talk to these services how do they know that they should execute a specific task well captain uses events to communicate with various tools to send them tasks to execute so for example when it's time to execute a deploy task captain will trigger an event saying deploy this service with version 1.2 to development environment with direct deployment strategy now on the other side you have this tools configured that can execute the tasks but how do these tools know how to handle the event so for example if you use helm for deployment how does helm know to listen for this specific event and then execute deploy task well that's done through captain services which are basically small applications written for each tool that can translate the captain event into an api call to that specific tool so captain's health service will subscribe to execute deploy event read the event payload when captain sends it and then translate that event into a helm task and tell helm to deploy the application with the provided information and there are captain services for various tools that you may need in your pipeline but what if you're using a tool for which there is no captain service or if you're using a custom tool well captain actually offers a more generic way to integrate with different tools using web hooks or kubernetes jobs so you have all this flexibility as well so as you see the workflow definition and the tooling is separated that's on purpose to separate concerns so captain basically gives you a tool to orchestrate other tools using events this means instead of a code integration like with jenkins it works based on events and this makes tool integration with captain very flexible and because you have separated the release workflow and the tooling you can replace any tool without changing anything in your workflow if you want to use a new tool just set it up subscribe it to the relevant event and that's it if you want to stop using a tool unsubscribe it from the events and done another big advantage of captain's event driven approach is that you have a more efficient workflow execution for example when you start a jenkins build that runs five hour tests you will have jenkins agent blocked for five hours until the test execution is finished so other builds will have to wait for it with an eventdriven workflow you don't have to wait for the tasks to finish you make asynchronous calls instead of pulling the tool to see if it's done the tool itself responds back with an event saying i'm done i've completed my task so whether the task takes 10 minutes or five hours it doesn't matter because nothing gets blocked okay so captain helps you create more flexible pipelines that are easier to manage and easier to integrate with different tools but that's not the main advantage of captain i mentioned that it helps automate the evaluation steps between the stages and that's the cool part so captain replaces the human quality gates that do checks and evaluations based on tests to decide whether the change can pass the gate to the next stage and use these automated quality gates instead so in the workflow we would have deployment tests and evaluation and we saw that different tools can do deployment and test but which tool does the evaluation well captain actually comes packaged with a service called lighthouse service which does the evaluation so you get this functionality out of the box with captain you don't have to install and configure these services which is pretty cool so how does that work how does captain automate this extensive application evaluation process to understand that first of all let's see what goes into the evaluation process itself let's say we ran performance tests for three hours which generated traffic for the application basically simulating the actual usage of the application and its features like in a real prod environment now the application performance can be evaluated based on these test results like how quickly the application handled requests how many errors application produced was the application overloaded at some point like did we have spikes in the cpu and ram usage etc and there are monitoring services like prometheus dynatrace etc that collect such metrics so while tests are running at the same time a monitoring service like prometheus collected metrics for the application during these three hours so these metrics must be evaluated to see how application performed but what's also important they must be compared with the previous releases so basically we want to know did the application get slower compared to the previous releases did it have more errors or did the application performance actually get better so we need a history of evaluations so as you see for a proper evaluation we need these two things metrics from the monitoring tool like prometheus and we have to decide what metrics do we want to evaluate and these metrics are called slis or service level indicators because they indicate how a service is performing and we define this list of metrics for captain in a file called sli.yaml again declaratively in a yaml format and the lighthouse service that you get with captain then fetches this metrics from the monitoring tool and as i mentioned this could be performance metrics of how the application is performing but also it could be security metrics like what security vulnerabilities were identified in the application as a second step once we have the metrics or sli's we want to now evaluate them are they high low good or bad so how do we define what's good or bad we define these with what's called service level objectives or slos for example we define that a response rate below 100 milliseconds is good so the objective is to have a response rate below 100 milliseconds so that's one type of evaluation of taking the metrics and comparing it with absolute values but with slos we can also evaluate the comparison with the previous releases so for example does the application respond faster or slower than in the previous three releases for example so we can define that also as an slo in captain and for each slo evaluation captain calculates a score and we can define all these in an slo.yaml file now as i said based on that evaluation we decide whether the release can go to the next stage through the quality gate or not so the evaluation result must be a simple yes or no or pass or no pass so what does captain give us as a result of evaluation it gives us a score a total score of all the slo evaluations and based on that score we can define in the slo dot yaml file whether it's a pass or not for example if we get a total score of 90 or more the release can pass through the gate now two points i want to mention here about captain workflows and how it works first of all as you see everything is defined declaratively the shipyard workflow the slos the slis so all the configuration of captain is declarative the second is that it follows github's practices which means all the logic and definitions and configuration are stored in git repositories and every change that captain makes is updated in these repositories and stored as a single source of truth so basically captain forces you to use some of the best practices in the devops and sre field when you use it now that we know what captain actually does let's see how an example release workflow will look like with captain so we commit and push our new cool feature this triggers a jenkins build which builds and pushes a docker image to the docker registry now captain comes in to take over orchestrating the deployment all the way to the production so jenkins sends an event to captain to start the release process captain starts the release workflow defined in shipyard.yaml file so captain triggers the event for the first task like deploy to development with a direct deployment strategy for this service with image version 1.2 so all this information will be in the event a helm service that is listening for that event will say i can do that and deploys the new version to the development environment once helm completes the deployment it sends back an event to captain saying i'm done here are the results of the deployment or the status of the deployment so captain now triggers the event for the next task defined in the workflow like run functional tests on the dev environment and in this case let's say two services jmeter and litmus service are both listening for this event so they both start executing the tests and once done both send events back to captain saying the task is completed now if any of the tests have failed captain will abort the release because it means there is some issue in the changes if not it continues with the workflow so as a next step captain triggers an event start evaluation for this as i mentioned captain has its own service so you get this functionality out of the box so captains evaluation services will fetch the metrics or slis calculate the slos and the total score and send back an event to captain with that final score and if the score failed captain will abort the release process if it passed then the next task will be executed to deploy changes to the staging environment using a blue green deployment strategy and so on so this workflow continues all the way production if everything along the way all the tests and the evaluation processes are successful and if not captain can abort the release process at any stage or task so we have a completely automated multistage release process another interesting question regarding captain is where does captain actually run the control plane of captain which basically orchestrates all these tools triggers the events etc runs on kubernetes so you deploy it inside the kubernetes cluster using a helm chart or captain cli that will again use the helm chart in the background and the execution plane or services that execute tasks based on captain events can run both inside the kubernetes cluster or outside the cluster like jenkins or selenium service for example and you can even have multiple execution planes connected to the same control plane of captain in order to manage multiple environments like dev staging and prod or even multiple cloud environments now we have deployed our cool new feature all the way to the production using captain is this the end of the release cycle no because as i said no matter how much you test and evaluate there's always a slight possibility that in actual real environment things will be slightly different and then we get a problem in production let's say we released our new feature which is part of the shopping cart service to production just in time for a holiday where you expect lots of sales the application runs for seven hours and we have shifted 100 of traffic to the new version and by midnight something breaks in the shopping cart service so people can't buy stuff they can't add anything to shopping cart super bad situation because you're losing a lot of sales on a holiday well normally you would have some kind of monitoring and alerting configured in your cluster that gets triggered automatically and sends notification to the team members that there is an issue in the cluster for example the alert manager is configured to alert when response duration increases considerably or a certain service is not responding at all that means a support gets a call at midnight to investigate and fix the issue with the new feature and if they can't fix the issue to eventually roll back to the previous working version or remove the feature flag and deactivate the new feature so what are the challenges here well obviously again we have the human element which means the support engineers getting calls at night to fix major issues in production and the effort in fixing these problems and that's another use case of captain to help you with what's called automated operations so captain can be used to automate this process as well so how does that look like with captain you can define what's called a remediation sequence for production issues so you create a yaml file with steps to fix the issue so how does the workflow for this look like alert manager detects an issue and sends an alert that shopping cart service isn't working like it's super slow or it's not responding at all so captain receives this alert as an event again everything is event based with captain and based on that alert event captain starts executing a remediation sequence which you defined to fix the issue automatically for example the first remedy step in the sequence is to scale up the pots because maybe the current number of pots cannot handle the traffic so we distribute the traffic a bit among more pots this means captain will send an event scale up pods for this shopping cart service the same way as we saw before a helm service is listening for this event and will say i can do that and scale pods by changing the replica account when it's done report back to captain now this is an interesting part because we don't know yet that this remedy step actually fixed the problem right so we need to check and observe to see whether the issue was actually fixed so captain does that for us by waiting for the change to take effect and then it will evaluate the slos to see whether the metrics have improved so the same evaluation process as in the quality gate is used here and based on the evaluation score captain will know whether the problem was fixed or not if yes everything's cool problem fixed we can configure captain to send a fixed notification to the team if not captain will look at the next remedy step and execute that for example the next step could be to remove the feature flag and again captain will do the evaluation to see if this action fixed the issue if the remedy sequence didn't fix the issue it can then escalate it to the team and then someone can intervene manually but as you can imagine you could cover already a lot of error fixing scenarios with this automated remediation steps without needing a human intervention which is obviously a huge improvement of operations and finally we saw that throughout the whole release process of not only automatic deployment but also after deployment operations in production monitoring and alerting is a central core point of this whole thing because without the metrics we can't do the automatic evaluations or remediations so configuring monitoring and alerting for your applications is an important component for this whole process and captain actually also helps you with that you can use captain to automatically configure monitoring and alerting tools for you for example based on the sli.yaml files that you have defined for your services it will know which metrics you want to track so we can configure the scraping endpoints to collect these metrics data from the monitoring tool the same way based on the slo.yaml files it will know what the accepted values for these metrics are so it can automatically configure alert rules for when the values are off or not within the accepted range i hope you learned a lot in this video not only about captain but also about new concepts around sre in general definitely let me know in the comments what you think about captain and also if you want to learn more about captain they actually have few tutorials with different workflows to play around on their website or you can even check out their slack channel if you want to join the captain community and with that thank you for watching and see you in the next video