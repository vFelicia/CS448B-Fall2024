in this video we will look at fluentd which is an open source log data collector to understand what this actually means i will first explain why we actually need logs the challenges of collecting and consuming application logs how fluenty works and how it solves all those challenges and finally how to configure fluenty as a user let's say we have a microservices application deployed in a kubernetes cluster two applications in node.js a couple of python applications maybe databases a message broker and other services all these applications talk to each other and produce log data so each of these services is logging information about what the application is doing now what are some of the information these applications are logging and why do we need this log data this may be some compliance data for example like if you're required to log some specific information depending on your industry in order to be compliant it could be for your application security for example to detect suspicious requests in your application by logging all xs attempts with ip address and user id etc or log who is accessing what and when and an obvious usage for log data is debugging your application when there is an error analyzing all application logs to find the cause these are some of the examples why log data is so important now the question is how do applications log this data there are few options first one is applications right to a file which is a common way of logging in applications however as you can imagine it's difficult to analyze loads of data in raw log files so it's not really for human consumption and without user interface or visualization for this data how do you analyze logs properly especially across applications by checking each application's log file and trying to look for similar times to check across applications also logs will be in different formats coming from different applications like the timestamps and log levels etc another option could be to log directly into a log database like elastic for example to then have a visualization of this data however in this case each application developer must add a library for elastic search and configure it to connect to elastic and send those logs and also each developer must configure the proper format so again there's some challenges with this option as well now what about the thirdparty applications in your cluster like databases and message broker also in kubernetes requests go through nginx controller so what if you want to see those locks too or what about system logs you can't control how they look so how do you collect logs from all these different data sources all of these are challenges of collecting and consuming logs in complex applications with tons of useful data because you have loads of data which you can't really consume and analyze because you don't have it all in one place in a unified format to be able to visualize them properly so lots of valuable data is kind of wasted so what is a good solution to that challenge a technology that lets you collect all the data regardless of where they come from and transform in a unified format all in one place so that you can then use that data again for compliance or debugging etc and that's exactly what fluentd does and fluentd does that reliably meaning if there is a network outage or data spikes this shouldn't mess up data collection right so fluentd handles such cases as well so how does fluentd work and how does it do all this fluency gets deployed into the cluster and it starts collecting logs from all the applications it can be your own applications thirdparty applications all of it now these logs that fluently collected will be of different forms and formats right like json format nginx format some custom format maybe and so on so fluentd will process them and reformat them into a uniform way now on top of that you can enrich your data with fluentd so you can add additional information to each log entry like pod name namespace container name and so on so for example you can later group logs of the same pod or logs of the same namespace or you can even modify the data in a log so now you're streaming your logs from all the applications into one unified format through fluentd what happens to these logs after fluentd processes them well obviously in most cases the goal is to nicely visualize them right so we can do some analysis on it well fluentd can send these logs to any destination you want this could be elasticsearch mongodb s3 kafka etc now what if you want your python application logs to go to mongodb storage for data analysis and all other application logs to go to elasticsearch or what if you want that node.js logs also go to the mongodb in addition to elasticsearch you can actually very easily configure that routing in fluency which is a great thing about fluendy because it gives you such flexibility compared to alternative tools so you can send any data from any data source to any destination or storage and this flexibility also comes from the fact that fluenty is not tied to any particular back end so you have a wide choice of such destination targets without a vendor looking when using fluentd now you're probably wondering what you as a fluent user need to configure and how you can actually use fluentd first you must install fluentd in kubernetes as a daemon set daemon set is a component that runs on each kubernetes node so if you have five nodes they will all have a fluency pod running on them you can configure fluenty using a fluency configuration file now fluency configuration may be a bit complex to get started with but it's very powerful in terms of processing and reformatting your data and for that you will use fluency plugins fluently has tons of plugins for different use cases first of all you can define the data sources these are all the applications from which fluenty will start collecting the logs so first you configure which application logs you want fluenty to start collecting second you configure how these data entries will be processed line by line so you parse each log as an individual key value pair you have log level message date user id ip address etc and you do that in fluentd using parsers after that you can enrich the data using record transformers again to have even more information on that data or you can even modify the data a great use case would be if you want to anonymize personal data in the logs for data protection for example and finally you have the output where should the logs go and for each such output target there is a plugin like elasticsearch mongodb and so on and as you see here in the example configuration file fluendy has a concept of tags which you can use to group together logs or to filter logs so using these tags you can say i want all logs with tag my app to be parsed like that or i want logs with the tag my service to go to elasticsearch and so on and also using these tags you can easily filter out any unneeded logs to save resources for example in the flexible routing that i mentioned before that's why it's easy to configure because using this text you can very easily configure which logs should go where so that's basically how you can use fluency for your logs now one big advantage of fluency is its builtin reliability when fluentd collects and processes the data it saves it on hard drive until it sends that process data to the configured output destination this means that if fluency pod restarts in the middle of collecting or processing the data or the whole server restarts the data will still be there and when fluency starts again it can pick up from where it left off and it also means you don't have to configure any additional storage for fluency like a radius database and so on what can also happen is when the backend the output target is not accessible can happen that elasticsearch is down or mongodb isn't accessible in that case fluentd will handle that by automatically retrying to send logs until that endpoint becomes available again and in addition to that you can also cluster your fluency setup to make it even more performant and highly available i should mention here that this is one of the use cases of fluency which is logging in kubernetes however logging is a very important topic in iot applications too or in noncontainerized applications running on bare metal servers for example and many projects are using fluenty for those use cases as well so fluenty can be used in many different environments if you like this video subscribe for more content like this also if you're interested in behind the scenes and preview content you can follow me on instagram we'll be happy to connect with you there and with that thank you and see you in the next video