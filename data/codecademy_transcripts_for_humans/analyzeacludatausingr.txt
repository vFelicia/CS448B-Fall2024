With timestamps:

00:00 - oh hey everyone testing testing do you
00:05 - hear us okay something maybe comes up
00:09 - from the chat if your your aim I can
00:12 - hear us awesome thanks everyone for
00:17 - joining us yes yes welcome welcome we're
00:19 - streaming live from Kolkata me
00:20 - headquarters right here in New York City
00:24 - the Big Apple Big Apple
00:27 - my name is Nathan I adore the tigress
00:28 - and my name is Ian freed and we're the
00:30 - two curriculum developers behind the
00:32 - learn are launched today
00:35 - we hope you enjoyed our pirate puns as
00:37 - much as we did yes there's no feedback
00:39 - here they were a little corny we are a
00:41 - little corny a little punny bear with
00:43 - our but yeah we're super excited about
00:47 - today and this course release learn are
00:51 - the first our course I ever on code
00:54 - Academy and we're so excited about it
00:57 - yes when you know we spent the last few
01:00 - months going into our using kind of our
01:04 - backgrounds and data analysis and data
01:06 - signs and trying to think of some really
01:09 - intuitive and fun ways to teach what we
01:11 - think is a really really incredible tool
01:14 - for data analysis so a little bit of a
01:17 - high-level overview of what is our
01:19 - exactly our is a statistical programming
01:22 - language that has a huge community of
01:24 - data enthusiasts and if you sign up for
01:28 - the course you'll see that there's three
01:29 - different modules that came out today
01:30 - but there's more coming soon
01:32 - the ones that came out today the first
01:34 - one is actually about our syntax and it
01:37 - walks you through some of the basic
01:38 - coding concepts like variables
01:40 - conditionals functions and how to import
01:43 - packages because if are as powerful for
01:46 - one thing it's for all the packages that
01:47 - it has for statistics and analysis and
01:49 - if you have never coded before it's
01:53 - totally alright our this intro course is
01:57 - great for people who want to jump into
01:59 - programming and learn things from the
02:01 - basics but at the same time if you're
02:03 - someone who's maybe been doing a little
02:04 - bit of JavaScript or a little bit of
02:06 - Python and you're see this language are
02:09 - and you're interested in it this is also
02:10 - a really great
02:11 - to jump in you know move over to what
02:15 - maybe we are starting to think is the
02:18 - best is out there but once you do not
02:23 - bias but and yeah if any of you um you
02:27 - know have been using are or have heard
02:28 - about it feel free to pop in the chat
02:30 - let us know we want to keep this you
02:34 - know chat here today a conversation
02:36 - we'll be talking about some really
02:38 - important things some really interesting
02:40 - things some cool things and we want to
02:42 - share that with you yeah we want to know
02:44 - why you're excited to learn kind of what
02:46 - your preconception of the language bar
02:48 - was before you signed up for the course
02:52 - and yeah so once you get past that you
02:55 - know introduction and syntax you can in
02:58 - the second module learn a little bit
03:00 - about data frames data frames are a
03:01 - really great data structure for
03:03 - organizing data that we can import maybe
03:07 - from a CSV or Excel spreadsheet or
03:10 - Google sheets and it basically the date
03:13 - frame will allow you and are to
03:15 - manipulate that data with different
03:18 - tools in order to organize it and
03:20 - arrange it in a way that's best for your
03:22 - own analysis so if you're someone who
03:24 - for work or for school is on Excel in
03:27 - Google sheets crunching numbers we hope
03:30 - that by the end of that second module
03:32 - you'll say goodbye Excel goodbye a
03:35 - Google Schneider circle hello
03:38 - and with that we'll be diving into a
03:40 - really useful package called deep liar
03:42 - that is a really great add-on to our
03:45 - that enables you to work with data
03:47 - frames yeah
03:50 - and so I'm we're seeing some people have
03:52 - never heard of our or the heard of it
03:55 - but never use it that's totally okay
03:56 - we're here today to give you that
03:58 - introduction and we hope that you can
04:00 - get some practice with it as you follow
04:01 - along and we see Kenji baby has heard of
04:04 - our but hasn't used it
04:06 - that's perfect your perfect audience for
04:08 - this course yes and Elizabeth thanks for
04:11 - joining in and this is being recorded so
04:14 - you'll be able to access this on our
04:16 - YouTube channel and any time moving
04:18 - forward so we know we recommend you
04:20 - follow along today by watching
04:22 - the stream by also following along with
04:25 - us in the project but you know and let's
04:30 - say maybe you're a little bit newer and
04:32 - maybe you you got lost at some point
04:33 - which if you do and in time or some loss
04:35 - feel free to write in the chat but you
04:37 - know in a week from now let's say maybe
04:39 - you've gone through a few more of the
04:40 - modules and you're feeling more
04:41 - comfortable you can always come back and
04:43 - revisit the video will be there for you
04:47 - to watch so should we get into the data
04:52 - I kind of explain in the background of
04:54 - the topic of this project yeah let's
04:56 - let's let's seven okay cool so this
04:59 - project uses data that was published on
05:02 - an ACO you report which Kenya will pop
05:04 - right into the channel you can go ahead
05:06 - and read more about it but the ACO you
05:08 - report kind of is the result of a lot of
05:14 - data cleaning that the statistics team
05:16 - there did shout out to brook Watson she
05:19 - kind of inspired us taking on this data
05:21 - set thank you brother
05:23 - yeah Thank You Brooke um she has a great
05:25 - talk that kind of walks everyone through
05:27 - the process of the data cleaning and the
05:29 - legislation around family separations
05:32 - and the result of being able to organize
05:34 - data and recognize inconsistencies in it
05:37 - so that they could track down where the
05:39 - children who were separated on the
05:40 - family work and natal you want to give a
05:42 - little feedback for for our viewers on
05:45 - the situation and I kind of give like
05:47 - yeah context yeah let's start with
05:49 - context so there's a timeline kind of
05:51 - that explains the policy around family
05:52 - separation most of y'all maybe heard
05:56 - about it in the summer but a quick plug
05:57 - and just in general like a good approach
05:59 - to learning a language is to start with
06:01 - the data set that really interests you
06:03 - that's great advice from our in-house
06:05 - data scientist Katherine she kind of
06:07 - told me you know start with something
06:07 - that makes you passionate and then go
06:09 - from there
06:10 - and I'll speak a little bit to my
06:11 - personal connection to a family
06:13 - separation and just immigration policy
06:15 - in general and then I'll explain the
06:18 - context of the whole thing but just in
06:21 - general anyone should be concerned about
06:22 - this is this general citizen but I grew
06:23 - up in the border of Texas so this kind
06:25 - of hit home and it
06:26 - and was really intrigued by
06:28 - understanding the data more so it helped
06:30 - me learn ours I was teaching it and I
06:33 - guess just from my perspective of both
06:35 - of our perspectives you know we we love
06:38 - programming we love coding we love data
06:40 - analysis and data science there are
06:42 - really useful tools and in our world and
06:46 - but we also think that there's a huge
06:49 - responsibility and task that comes with
06:53 - with this knowledge and you know there
06:55 - are so many projects that you can do out
06:57 - there so many things you can do with
06:58 - data analysis data science but to us
07:01 - it's really meaningful to take these
07:02 - skill sets that you can gain you know
07:05 - practicing here in Kolkata me you're
07:07 - practicing on your own or in school and
07:09 - using them to to make an impact and this
07:12 - is an area that we're you know we
07:14 - thought that the ACU was making a huge
07:16 - impact Brook was doing that and it's
07:18 - something that we're really passionate
07:19 - about too and so I think it's really
07:21 - meaningful for us I'd be able to share
07:23 - that with you guys here today and just
07:25 - be a reminder that you know data
07:27 - analysis the data that you're working
07:29 - with there's always stories kind of
07:31 - behind the the numbers as real people
07:33 - and real people and the work that you do
07:36 - analyzing the data can have a huge
07:38 - impact on people's lives so let's start
07:42 - with the boys because actually learned a
07:43 - lot about it I don't think that
07:45 - headlines - really got into the weight
07:47 - of it but so in February 2018 the ACLU
07:52 - filed a class action lawsuit against
07:55 - immigration and customs enforcement
07:56 - branch of the government because at that
07:59 - point even though it was not a policy
08:01 - they had been separating children from
08:03 - their family some as far as two thousand
08:05 - miles away with little to no contact
08:07 - between the parent and the child the
08:09 - main case for this lawsuit actually only
08:13 - spoke to the child six times in a period
08:15 - of four months and they were separated
08:16 - 2,000 miles
08:18 - so this class-action lawsuit came about
08:21 - and in March when this was happening
08:24 - John Kelly proposed that this particular
08:29 - example could serve to help deter
08:33 - immigration on this other border
08:35 - so at that point conversation started
08:38 - around
08:38 - can get an actual policy aka making it
08:41 - legal to separate children from families
08:43 - and so inmate 2018 what became known as
08:48 - the zero tolerance policy actually went
08:50 - into place and family started being
08:52 - separated at the border and then that
08:56 - happened that was legal to do for like a
08:57 - month they were lots of protests maybe
08:59 - saw headlines about it but throughout
09:01 - that month over two thousand children
09:03 - were separated and at the end into the
09:06 - in June 2018 that's when the policy was
09:09 - stopped but at that point there was so
09:11 - much consolidation of the data that
09:13 - needed to happen so that the ice was
09:15 - equipped with how do you reunify his
09:18 - children that were separated with their
09:20 - families and the ACO you started
09:22 - analyzing data pouring in then June 2018
09:26 - and kind of it was a month long effort
09:29 - months and months of consolidating the
09:32 - inconsistency so they could hold ice
09:34 - accountable for all the children that
09:36 - were in reunify debt and so the result
09:39 - of that is this really clean data set in
09:41 - this really awesome report written off
09:44 - by the ACLU and we use that and we take
09:47 - it one step further in the project to
09:48 - gain more insight into what was
09:50 - happening this summer so yeah that's
09:53 - that's kind of the background on all of
09:54 - it if you have any questions about that
09:56 - yeah I can we can talk more about it but
09:59 - if not let's just get to the code
10:00 - definitely and just before we jump in to
10:03 - just like one thing that you know they
10:05 - did at the ACLU was clean all this data
10:07 - that immigrations and Customs
10:10 - Enforcement was providing to them and
10:12 - you know when you're working as a data
10:14 - analyst or a data scientist you want to
10:17 - kind of get right away to you're like
10:19 - looking into the data getting insights
10:20 - maybe making visualizations or you know
10:23 - making some sort of model but often one
10:26 - of the most important tasks is just like
10:28 - organizing and cleaning the data because
10:29 - you can't you can't jump into that
10:31 - analysis until until the data is in a
10:34 - proper format and one of the things that
10:36 - they did here was you know sips through
10:38 - many different Excel spreadsheet was no
10:40 - standard
10:41 - documenting like the immigration number
10:43 - per child yeah so it may be like certain
10:45 - columns they have the same information
10:46 - but different names and different column
10:50 - or different sheets or even like
10:53 - inconsistencies and data like maybe you
10:55 - know for example and their documents
10:59 - they found that there were certain
11:01 - children who their date where they were
11:03 - is reunified with their family was a
11:06 - date before they were processed into the
11:08 - country so that information doesn't make
11:09 - sense
11:10 - so you it's important that when you're
11:11 - getting kind of raw data you're sifting
11:13 - through it and making sure that it makes
11:15 - sense and so that's something they did
11:17 - and then enabled us to go ahead and
11:19 - analyze yeah
11:21 - and that's not like an endemic problem
11:22 - to this dataset that happens all the
11:24 - time
11:24 - someone once said they're like 80% of a
11:27 - data scientist job is data cleaning so I
11:29 - think we you should extrapolate kind of
11:31 - the learning that's happening with this
11:32 - particular data set but realize that it
11:34 - applies to like a lot of scenarios not
11:37 - just this particular topic yeah and
11:39 - before we jump into the code I want to
11:41 - just take a look at what some of you
11:42 - guys are saying here so yeah using
11:46 - real-world interesting data is
11:47 - definitely the way to go I would say
11:50 - when trying to get more comfortable with
11:52 - doing data analysis like Natalia said
11:54 - find those data sets they're really
11:56 - stuck to you speak to you that means
11:57 - something to you and then also having
12:00 - that background knowledge on the data is
12:02 - super important as well it's really hard
12:04 - to jump into the data let's say like
12:07 - finance data if you don't have like some
12:09 - of that knowledge about finance and so
12:11 - even if it's just like you take a day or
12:12 - two to kind of dig into it it's really
12:14 - important to get that background I think
12:18 - your analysis will be a lot better for
12:19 - that so great point Arian and then
12:23 - someone else was asking is there a
12:26 - google of datasets and so there is we
12:30 - love it yeah what's the domain so I'm
12:32 - not sure the exact domain name but
12:34 - Google itself has a dataset search tool
12:37 - we'll try and find out that exact domain
12:40 - for you and you can try and post it in
12:42 - there but are you just Google even like
12:44 - Google dataset search it's a really
12:46 - great tool for finding datasets
12:49 - accessible for you to analyze yeah a lot
12:52 - of the times they come well documented
12:53 - so they'll tell you about the data
12:54 - collection process and the different
12:57 - fields and columns for it mm-hmm
12:59 - and agree with Tim yeah katal is also a
13:01 - great place for finding datasets that
13:04 - are definitely well documented and
13:07 - especially also if you're if you're a
13:09 - newer you can kind of make me see what
13:11 - other people are doing with similar that
13:12 - kind of data again all of these links
13:18 - are posted up so if you want to learn
13:19 - more about the ACO a report or Burke
13:20 - Watson's talk you can find them yeah so
13:24 - this would be a great time if you guys
13:25 - are following along to make sure that
13:27 - you're in the the project with us and so
13:32 - you can either follow along in the the
13:36 - project that should be in the URL or if
13:38 - you're also going through the modules
13:40 - and you're at the ACLU data project you
13:42 - can follow along right in there so we
13:47 - have some background information here
13:48 - which kind of spoke at a spoke about
13:50 - we'll go ahead and get started with the
13:54 - first task you know always necessary
14:00 - always important so when you're gonna go
14:02 - ahead and do an analysis it's good to
14:04 - have access to different packages and
14:06 - what are packages packages are
14:09 - essentially collections of code that
14:13 - users have put together one of the other
14:15 - great things about our is that it's open
14:17 - source so anyone's able to go in and
14:20 - publish these packages where they can
14:23 - take all the work that they've done to
14:25 - simplify certain you know processes or
14:28 - tests that they think are really common
14:30 - and put them into a package that enables
14:33 - you to go ahead and just load that
14:34 - package and perform the task really
14:36 - nicely and simply and so we're gonna
14:43 - well I call it Diplo whatever insider
14:47 - deep fire
14:51 - and we're gonna use both of them so
14:52 - let's just go ahead and import them with
14:53 - the library function mm-hmm I will say R
14:56 - has lots of kind of fun package names
14:57 - that three hours I think it is so
15:03 - they're into the puns too okay so to go
15:07 - ahead and load a library or a package we
15:10 - use the library function and you give as
15:12 - an argument to the library function of
15:14 - the name of the package you'd like to
15:15 - load so I'm gonna load the reader I'm
15:18 - also gonna load the packages and also if
15:25 - you're unfamiliar kind of with our
15:28 - notebooks which I would say problems
15:30 - yeah you know a lot of people may be and
15:31 - we were as well so basically what we're
15:35 - doing and let me move this over to the
15:36 - side just for a second make this bigger
15:43 - so it should be a little bit bigger for
15:46 - you guys now our notebooks are a really
15:49 - cool way to code because they enable you
15:53 - to not only write code that you'll be
15:55 - able to run but you can also do some
15:58 - nice annotation with markdown markdown
16:01 - just a nice way to yeah just add some
16:04 - some annotation explanation to your code
16:06 - but what's really cool is once you run
16:09 - the notebook file it goes ahead and
16:12 - renders into a HTML page which we don't
16:18 - have any code that's gonna do anything
16:19 - exciting yet and I'm gonna try and
16:21 - expand this side as well but in a little
16:23 - bit you'll see what this rendered
16:27 - notebook looks like and you can see it
16:28 - right here on the right hand side we
16:29 - just have any code yet but it takes your
16:32 - code and it makes it look nice and
16:35 - presentable and then it will also show
16:37 - the output yeah and what I really like
16:39 - specifically about our notebooks is that
16:41 - I kind of isolates the subtasks of a
16:43 - bigger program so if you see we've
16:45 - grouped code blocks and we've given a
16:49 - comment to each one with the hashtag
16:51 - symbol that kind of explains what our
16:53 - code block will do as we go through the
16:54 - project
16:55 - but if later you need to go back and
16:57 - change part of the logic you can just go
16:58 - straight to that part and not have to
16:59 - sift through the whole file mm-hmm
17:01 - definitely I would definitely agree that
17:03 - kind of displaying and up into the
17:04 - different segments makes it really easy
17:05 - and then you can also see what the
17:07 - output looks like after each segment so
17:10 - if you're doing a data analysis where
17:11 - you're you know changing columns of a
17:13 - data frame or you're reordering things
17:15 - you can see at each step how that data
17:17 - is changing and confirm that your code
17:19 - is working as expected
17:21 - so the second step after we have
17:34 - imported our libraries we're ready to
17:35 - use it is to actually import the CSV
17:38 - file with the data so that we can use it
17:40 - inside a data frame which is a data
17:42 - structure for act angular data in our so
17:46 - let's go see what's that file called in
17:49 - our so yeah if you just click on the
17:50 - little top left there is that a file
17:53 - navigator I mean you can see the
17:55 - different files there so it's called a
17:57 - cou separations dot CSV and the way that
18:01 - you make a variable in our so that we
18:03 - could save it into a dataframe variable
18:04 - is you use this fun arrow syntax which
18:09 - is a little different than how most
18:10 - languages do it most languages use an
18:12 - equal sign but our uses this arrow
18:15 - syntax and then after that comes the
18:16 - value that you wanna identify that you
18:19 - want to assign the identifier so we made
18:22 - a variable name a cou and inside of that
18:24 - variable we want to go ahead and read
18:25 - the CSV which is a reader function and
18:30 - we want to pass in a string or character
18:32 - type that has the file name that we just
18:35 - saw which was a cou and in order to see
18:48 - if that actually loaded in as the right
18:49 - data frame we want to just get kind of
18:52 - the we want to print it so that we can
18:53 - actually see it right now when we hit
18:54 - that and the way that you do that
18:58 - there's many ways to do it you can print
19:00 - the whole thing you can print part of it
19:01 - but the way that most that is most
19:04 - common is to just print the top six rows
19:05 - of it so you can see what's in there and
19:07 - the way that you do that is with this
19:08 - function called head so you want the
19:10 - head of this data frame we just made
19:12 - called ACL you save it and here we have
19:19 - it and now you can see that on this
19:24 - right hand side this is what the
19:26 - surrendering notebook is looking like
19:27 - and so it gives us a little bit of
19:28 - feedback for some of the different code
19:30 - blocks so when we loaded the CSV file
19:33 - existed some information on the columns
19:36 - that are being brought in and then here
19:38 - when we look at the head of the data
19:40 - frame we are seeing the different
19:42 - columns so you see Baltimore big town
19:46 - Bethlehem let's just look at the column
19:48 - names really quickly together address
19:50 - maybe that's refer address and which is
19:54 - the number of children that were
19:55 - separated in that particular location
19:56 - the program City the program state the
20:01 - lawn which is the longitude and the
20:03 - latitude so it's kind of what we're
20:06 - working with here in this dataset so
20:11 - another way that's really good to kind
20:13 - of inspect data once you load it which
20:14 - is always a nice thing just because you
20:16 - might have some big CSV and you don't
20:18 - know the information that's in there and
20:20 - you wanna get an idea is to use the
20:21 - summary function and let's go ahead and
20:25 - use the summary function we'll pass in
20:26 - our data frame ACLU as an argument and
20:31 - it just provides a
20:33 - of summary statistics about the data
20:36 - frame so it will give for each different
20:39 - column if it's a numeric column so we're
20:41 - working with numbers it'll say you know
20:44 - how what's the smallest value what's the
20:46 - largest value what's the median value
20:48 - and maybe give a mean or a max and you
20:54 - know if you're not too familiar with
20:55 - different statistics like you know don't
20:57 - worry about it um we already have we
21:03 - have the courses for the mean median and
21:06 - mode coming out soon within the next two
21:09 - months so keep an eye out for those I
21:11 - did see someone and the chat was asking
21:13 - about more advanced our concepts and
21:15 - that's coming to you soon yes it's
21:17 - coming to you soon and are we also have
21:19 - some statistics and Python courses
21:21 - currently going so if you want to kind
21:23 - of jump and see things from a different
21:24 - perspective that's also a great way to
21:25 - go yeah so this gives just like nice
21:29 - kind of summary information so you have
21:32 - an idea of the data that you're working
21:34 - with so for example if we look at
21:35 - longitude I'm going to zoom in a little
21:37 - bit here so we can see we can see the
21:40 - the lowest longitude is negative 122 and
21:44 - then the let's see the max is negative
21:48 - 71 with latitude we're seeing a minimum
21:52 - of 25 and a max of 47 and so I I feel
21:57 - like all the time whenever I see lots of
21:58 - longitude I'm like which which which one
22:01 - goes which way so the lines of latitude
22:06 - they run horizontal around the world so
22:10 - those will kind of give you an
22:11 - understanding of how far north or south
22:14 - you may be so we're ranging between a
22:17 - minimum latitude of 25 and a maximum of
22:20 - 47 and so once again what it like one of
22:23 - these numbers mean all right sometimes
22:24 - it's like easy to get lost so that
22:25 - latitude is the latitude of these
22:28 - different detention centers where you
22:31 - know children who are being separated
22:33 - with their families where we're being
22:34 - taken so so I think it's always good to
22:38 - kind of just jump back and get like okay
22:39 - what what does it number mean in the
22:41 - contact
22:41 - of my project rather than it's just
22:44 - every ages from 25 to 47 yeah another
22:47 - interesting statistics just right up off
22:49 - the top with the summary function is
22:51 - that the median
22:52 - sorry the mean for children separated at
22:55 - any given location was 41 all the
22:57 - different locations we see in there
22:58 - that's what an average is up to but then
23:01 - yeah we see there's one location with a
23:03 - max of 350 50 no children separated
23:06 - there so I will see you later which one
23:10 - that is when we analyze it a little more
23:14 - for a second and and Connor I see that
23:23 - you are running into some trouble in
23:28 - your first five lines of code that
23:29 - happened to us like before we got
23:31 - anything installed in our computer we
23:33 - ran into a bunch of trouble let us know
23:35 - what's happening but also it's okay to
23:38 - be a different stage size I think like
23:39 - if you know a little bit of programming
23:41 - from another language it might be a
23:42 - little bit you might pick up are a
23:45 - little bit more quickly but that doesn't
23:46 - it's not a reflection of your ability to
23:48 - do it or like if that looks very
23:50 - differently for everyone so we
23:52 - definitely encourage everyone on the
23:53 - threat to help each other out let us
23:56 - know how we can help yeah we've all been
23:59 - there you know if your clothes not
24:00 - running and I would bet that sometime
24:04 - during the next 40 minutes our clothes
24:06 - yeah so yeah so alright we now saw kind
24:14 - of in the data let's start inspecting in
24:16 - step four says after you've inspected it
24:19 - you realized that the address column
24:21 - contains the same information that is
24:23 - contained in the program city and
24:24 - program state columns let's go check
24:27 - that out so let's say address column has
24:29 - Baltimore Maryland and then program city
24:31 - has Baltimore in programs to you
24:33 - Maryland that seems pretty repetitive
24:34 - hmm
24:36 - select all the columns from the ACLU but
24:38 - that one and save your new data to the
24:41 - ACLU variable so wait to clean what this
24:44 - is saying is like that's kind of
24:45 - repetitive remove that one let's go
24:48 - ahead and do that we're gonna reassign
24:51 - the value of the ACLU so that it changes
24:53 - with the updated data frame that does
24:55 - not include that one and the way that
24:57 - you do that is you use this pipe symbol
25:02 - the way that I describe this one to any
25:05 - student whenever I'm talking about art
25:06 - is that anything that comes after this
25:09 - pipe refers to the data frame that came
25:11 - before it and was gonna come after this
25:14 - is we're gonna stay select what's
25:15 - currently in the a cou data frame and
25:18 - give all of it back
25:20 - except for that's what the minus sign
25:22 - says the column adder which is address
25:27 - so we're saying select and program City
25:30 - program State longitude and latitude but
25:33 - please don't give us the address let's
25:36 - just make sure that works by printing it
25:37 - with our handy function head
25:42 - and so like another way to think about
25:45 - what that pipe symbol is doing is it's
25:49 - taking the ACLU data frame that's on its
25:51 - left and it's kind of pushing it or
25:54 - piping it into the first argument of
25:57 - that function select so and it's just
26:02 - another way that of kind of framing or
26:04 - writing code and are and you know as you
26:08 - dig deeper I and further into our this
26:11 - kind of formatting becomes super helpful
26:14 - and really like organizing your code and
26:18 - makes it clear for you know like what
26:21 - you are doing to a data frame yeah
26:23 - should we be writing about the pipe so
26:25 - kind of they can see yeah we can we can
26:27 - do both ways but another way to think
26:34 - about it without the pipe so that you
26:35 - can understand the cup a little better
26:36 - you could just do the same thing but
26:38 - instead of putting the pipe you would
26:40 - just put the first argument that's the
26:42 - data frame so like I was gonna look at
26:52 - this that line of code before I would
26:55 - say okay let's select from ACLU every
26:59 - column except address but now when we
27:02 - write it with the pipe you can say like
27:05 - from this data frame ACLU select
27:07 - everything but the column address yeah
27:10 - there's many way to do it we could have
27:11 - done the inverse which is listed out
27:13 - every single one but address and there's
27:20 - going to take a quick second so I see
27:22 - yeah Connor I'm glad you're not seeing
27:24 - the internal error anymore
27:26 - if they're still potentially in our in
27:31 - the our notebook the the rendered page
27:36 - might not you know appear properly so
27:40 - what I would do is try and look back at
27:43 - every code block that you know we've
27:46 - written compare it to yours and see that
27:49 - things are kind of
27:51 - similarly and then hopefully that should
27:53 - address the issue that's a ten yeah yes
28:01 - I was like filing yeah I think that
28:05 - don't happen though they might have to
28:07 - click on expand on the air-cool so let's
28:14 - go ahead and do step 5 mm-hmm so we were
28:17 - taking a look at the columns before but
28:20 - but one thing I always like to do
28:21 - especially I have a really big data set
28:23 - is just print out all the column names
28:25 - so I know I know the dinner that I'm
28:27 - working with because it's not always the
28:29 - case that you can easily click through
28:31 - and your notebook and see see all the
28:34 - columns and so the way we can do that is
28:40 - by using the call names function and we
28:43 - just pass to the call names function as
28:46 - an argument our data frame ACLU I want
28:49 - to go ahead and just put that all inside
28:52 - a print statement you don't have to put
28:55 - inside the print statement it would
28:57 - still render in the notebook but the
28:59 - output looks a bit nicer if you wrap it
29:03 - in the print statement so we'll go ahead
29:06 - and run this and we'll scroll down and
29:11 - we'll now see that were printing out the
29:14 - column names that we were seeing earlier
29:17 - now maybe maybe we can have reach out to
29:22 - you guys you guys think that these are
29:23 - helpful column names are they are they
29:26 - descriptive enough for you and they took
29:27 - my bows like what do you what do you
29:29 - think yeah I know I've got some thoughts
29:33 - and opinions on them so doesn't this
29:35 - Holly oh we're very opinionated people
29:37 - so Leah maybe maybe we can see if you
29:40 - guys have any thoughts and maybe if you
29:42 - have would like to see some changes for
29:45 - those column names maybe you can even
29:47 - without seeing what we've written I
29:50 - think of maybe some column names that we
29:52 - could change these two if you have an
29:55 - idea feel free to put it in the chat
29:56 - well we'll wait a little bit for you for
30:00 - someone to me
30:00 - put something in there just like a quick
30:03 - reminder these are the column hmm so we
30:06 - have one two three four five column
30:09 - names currently one is and it's not
30:14 - gonna Talia that's that's you know that
30:17 - says something about the column name
30:18 - it's not very descriptive mm-hmm and we
30:21 - have program city program state and then
30:24 - lon
30:25 - which actually almost looks like Ian
30:26 - also yeah and yes yeah alright so we
30:38 - let's just brainstorm together like
30:40 - given what we know about the data set
30:42 - you know we didn't all this research
30:42 - creating the ACO your report we can kind
30:46 - of speculate what these mean and then
30:47 - let's come up with names together and
30:48 - let's so we're gonna reassign our data
30:57 - form to be more descriptive so we'll do
30:59 - that same kind of pattern we've been
31:01 - doing we just say the name of the
31:02 - variable and then reassign it with this
31:04 - arrow but after we're gonna do something
31:07 - to the a cou data frame with right
31:13 - and that something is gonna be we're
31:15 - gonna rename the column so based on that
31:17 - action we're gonna use the function
31:21 - called rename that's one really sweet
31:23 - thing about our that I found you know I
31:25 - know other languages and are just has
31:28 - the best most descriptive and concise
31:30 - function names that tell me exactly what
31:32 - it's doing
31:33 - yeah rename will take in X amount of
31:38 - arguments depending on what you want to
31:40 - do with that but the way that it takes
31:41 - them is it takes the name of the new
31:44 - column and the name of the old column
31:46 - and then it reassigns that so if let's
31:50 - say we wanted to name the rename the
31:51 - first column and which is not very
31:53 - descriptive to something like number of
31:55 - children because that's way more
31:56 - descriptive the way that we would pass
31:58 - that argument and it's like this number
32:01 - underscores okay and we would follow
32:06 - that pattern with the other ones every
32:08 - one to rename them in my opinion program
32:10 - city is too verbose the column name what
32:14 - what that column is is just a city so
32:16 - I'm gonna rename it to City oh and look
32:25 - I just pattern matched myself so I
32:27 - supposed to actually be the name of a
32:29 - new column and then after it the name of
32:31 - the old column this is one that I still
32:36 - make mistakes on all the time yeah
32:38 - always do it in the wrong way I have my
32:40 - own opinions about attacked I think I
32:42 - should be like you should have old
32:44 - column name first and then anyone cuz
32:46 - that makes more sense intuitively but
32:47 - our has its reasons and if you know this
32:51 - is a like a general programming thing
32:53 - that I really recommend if you're ever
32:55 - unsure about a function what it does or
33:00 - a package and the different functions
33:01 - that are in it going online
33:05 - just maybe searching are the package
33:08 - name so this case may be like Rd plier
33:10 - and renamed and right here in
33:12 - documentation you can go to the the
33:14 - website see the documentation for this
33:19 - package and for that function and
33:20 - potentially see examples or explanations
33:23 - what the arguments are or should be and
33:26 - that's always a really helpful tool for
33:28 - for troubleshooting or for answering
33:30 - questions true let's print it out to see
33:34 - what happened and we have a great
33:37 - question from Alex does our have strings
33:41 - yes our does have strings that you
33:46 - indicate with quotes but often when we
33:50 - are referring to like a column name of a
33:53 - data frame you don't need to use the
33:58 - quotes to indicate a string let's say if
34:01 - we're using that column name as an
34:03 - argument to a function so yeah great
34:06 - question and well we're gonna work with
34:09 - strings a little bit later on yeah and
34:12 - we do so here we have it we use the
34:22 - rename function after piping it the a
34:24 - cou data frame and it renamed end number
34:28 - of children program city to city program
34:30 - state to state and latitude and
34:32 - longitude to their full flush that works
34:37 - so now we can start coding and analyzing
34:40 - and actually know what these columns
34:41 - represent mm-hmm and now that we have
34:45 - this information here you know when I
34:48 - was originally looking at this I was
34:50 - thinking okay we have this data that we
34:53 - got from the ACLU but what what else
34:56 - might I want to try and do with it right
34:59 - what else could we could we find out and
35:01 - I think this is an important question to
35:02 - ask when you're doing a data analysis
35:04 - because you might get your data from one
35:06 - source but you know that might you might
35:09 - want to dig a little further and find
35:11 - something else out that the original
35:12 - data didn't have and I was interested in
35:15 - trying to see how far some of these
35:19 - detention centers where children were
35:21 - being taken were from the border yeah
35:23 - to give a sense of you know really how
35:25 - far away some of these children
35:28 - under age 5 in many cases we're being
35:32 - taken I think it's a great question that
35:33 - you're asking with that specifically
35:35 - because a lot of the kind of headlines
35:37 - that went around with this topic didn't
35:41 - mention kind of the magnitude of the
35:43 - separation it wasn't just like they were
35:44 - kept in separate rooms a lot of the
35:45 - times they were kept in completely
35:47 - different states miles and miles from
35:49 - where the parents were kept so yeah and
35:53 - and you know when I was thinking bout
35:55 - this I said okay how how can I try and
35:57 - come up with some sort of metric to
35:59 - determine how far away these centers
36:01 - were and so since we were given the
36:03 - latitude locations at the center's I
36:07 - came up with the idea that maybe we
36:09 - should try and find the latitude for for
36:14 - part of the border between US and Mexico
36:16 - and kind of use that as a baseline to
36:18 - measure distance and so you know the
36:20 - us-mexico border is thousands of miles
36:23 - long you know it stretches from the
36:26 - western most parts of the country to the
36:30 - eastern most and it varies it in
36:34 - latitude but you know I went ahead and
36:37 - we we found this the lower latitude of
36:40 - 25.8 3m as kind of a low point and in
36:44 - order to use this in an app our analysis
36:45 - we're gonna go ahead and create a
36:47 - variable just going to be called border
36:49 - latitude yeah and that old servers our
36:52 - frame of reference and we're gonna
36:57 - assign it the value twenty five point
36:58 - eight three and yeah once again just
37:03 - just to kind of backtrack for a second
37:04 - with the arrow like assignment operator
37:07 - you know you're still feeling like maybe
37:09 - a little uncomfortable from it if you're
37:11 - coming from another language I like to
37:13 - just think of it as you're taking the
37:15 - value on the right and the arrow is
37:16 - saying put that to the left into this
37:20 - variable I'm so now board allotted to it
37:22 - is now storing this twenty five point
37:23 - eight three identifier value and so now
37:28 - that we have this border latitude we can
37:32 - go ahead and try and create a new column
37:35 - to add to our data frame that's going to
37:38 - contain the distance or the latitude
37:41 - change
37:41 - really from that individual detention
37:44 - center to the border latitude that we
37:49 - have defined here
37:50 - so once again I'm going to I want to
37:53 - store this new data frame with this new
37:55 - column of you know back into ACLU our
37:58 - different we've been working with and
38:00 - we're going to take the data from that
38:04 - we're working with ACLU we're going to
38:06 - use the pipe symbol once again and we're
38:08 - going to use a new function keep using
38:11 - the dollar sign a new function that's
38:15 - also part of D plier that is used for
38:18 - adding columns to a data frame and that
38:22 - function is called mutate and that's
38:25 - also aptly named because that is what
38:26 - we're doing to the data frame we're
38:27 - mutating it by inserting a new column in
38:30 - it mm-hm
38:32 - and the way the mutate works is we
38:36 - provide as an argument to mutate what we
38:40 - want the new column that we're creating
38:42 - to be called and we give an expression
38:45 - that defines how our calculating this
38:47 - new column so let's go ahead and we're
38:51 - going to name this new column lat change
38:54 - order just to indicate that we're saying
38:57 - this is the change in latitude from the
38:59 - Detention Center to the border and it
39:02 - will be equal to we want to find the
39:05 - difference between this border latitude
39:07 - and the latitude of the question Center
39:10 - so we have this column latitude so we're
39:12 - gonna say it's equal to latitude -
39:15 - border latitude before we present let's
39:19 - just go through so we're saying alright
39:26 - take that data frame a cou and mutate it
39:29 - by computing a new whole new column
39:32 - called lat change border and the values
39:34 - inside that column will be based on
39:36 - what's in the latitude row for the data
39:40 - frame and in the word order latitude
39:44 - value that we have
39:45 - to be 25.8 III based on the average of
39:48 - work for tourism and I made another
39:51 - little mistake I have this equal sign
39:53 - here when I really want to sit - sign -
39:55 - because we're finding the the difference
39:57 - so let's just do an example
40:00 - really quickly if we're looking at this
40:02 - detention center that in Baltimore
40:04 - Maryland will zoom in for a second and
40:08 - I'll go over and we see that has a
40:09 - latitude of thirty nine point two nine
40:12 - we're gonna say take that dirty nine
40:14 - point two nine subtract 25 from it so
40:18 - we're getting fourteen approximately you
40:23 - know a little less than fourteen changed
40:26 - latitude so once we do this we want to
40:32 - go ahead and take the head once again of
40:35 - the data frame just so we can see what
40:37 - this
40:45 - scroll down and now we are over to the
40:53 - right-hand side oh okay I'm gonna move
40:55 - my head out of the way
40:58 - and we can see we have this new column I
41:00 - keep backing up further a lot change
41:01 - border that is showing that calculation
41:06 - that we just made
41:07 - showing that changing latitude from the
41:09 - border to the detention center and that
41:12 - kind of makes sense if we look at the
41:14 - latitude for Maryland and we take that
41:17 - 25 it's about 13 in difference so it
41:20 - computed it exactly so let's go on to
41:29 - step 9 step 9 cents that's this whole
41:31 - section is actually about filtering and
41:32 - arranging gross so now that we have a
41:35 - column representing a Detention Center
41:36 - distance from the border you want to see
41:39 - which detention centers are far away
41:41 - from where the family separation
41:42 - occurred filter the rows of ACLU to find
41:46 - all of the detention centers where the
41:48 - latitude change border is greater than
41:51 - 15 save this new data frame to a new
41:55 - variable called further away and then
41:57 - let's see what's in that a different all
41:59 - right so I like this stuff because it
42:02 - has a lot to it
42:03 - we're basically now getting to the
42:05 - analysis part so we're gonna can you
42:09 - variable let's do that first
42:11 - further away we're gonna sign it and
42:17 - arranged version of what
42:20 - cou based on a condition um so let's
42:26 - pipe the ezo you and filter and like
42:39 - Natalia was saying before I think one of
42:40 - the really great things about deep liar
42:43 - which is a lot of the functions that
42:44 - we're using here today is they do
42:46 - they're doing exactly kind of what they
42:49 - say so here we want to filter out rows
42:53 - that don't meet this condition of the
42:56 - lot change water it's greater than 15 so
42:59 - the condition we're about to write is
43:01 - just gonna go inside the parentheses and
43:03 - that condition students basically give
43:05 - us anything where the value inside this
43:09 - column that change border is greater
43:21 - then after that what's printed I think I
43:25 - think it's just a formatting that's good
43:33 - further away
43:44 - great and now let's go take a look at
43:46 - this new data frame nice and so if we
43:54 - take if we go over to the right we can
43:57 - see that all of the latitudes or this
44:01 - lat change border column all those
44:02 - values are greater than 15 so we
44:07 - filtered out all the rows that you know
44:09 - don't meet that condition and are now
44:11 - just looking at these locations that are
44:13 - much further away from from the border
44:16 - now now we've done this filtering but
44:19 - there's kind of a next logical step that
44:22 - can be done once we've done the
44:23 - filtering and that is to order the rows
44:26 - of this data frame to see okay now that
44:29 - we have this group like which detention
44:31 - centers of this selection are furthest
44:34 - away
44:34 - I'm compared to which are closer and we
44:37 - can order or arrange these different
44:40 - rows using the arrange function and this
44:45 - is where we're going to go ahead and
44:48 - really use the power of the the pipe
44:51 - symbol that we've been using here and
44:55 - I'm gonna go ahead and I'm going to
44:56 - expand the code editor for a second yeah
45:06 - and so the way them I'm gonna use the
45:09 - pipe symbol now is instead of just
45:10 - making a new variable and saying okay
45:14 - let's take further away and pipe it into
45:16 - this function of rain if you want to use
45:19 - but we're gonna just take the pipe and
45:22 - stick it on to the end right here after
45:25 - our filter and then I'm gonna put a
45:36 - reins right here and to arrange we
45:42 - basically give as an argument the column
45:45 - that we want to order the rows by so if
45:48 - I go ahead and say lat change order this
45:58 - would then go ahead and arranged in
46:00 - ascending order our data frame smallest
46:04 - values and then from smallest to highest
46:07 - but we're interested in getting
46:09 - descending order so if you want to see
46:11 - the furthest ones away and then getting
46:13 - closer and closer so all we have to do
46:16 - is just put in DSC this is descending
46:21 - function and give it that change border
46:24 - as an argument so we're just going to
46:26 - wrap the column that we want to order by
46:28 - list DSC and so what what is happening
46:31 - right when we added this second pint so
46:34 - basically what this is doing is saying
46:37 - let's take all of the the code
46:40 - beforehand that we've done we've taken
46:41 - the ACLU data frame we filtered it on
46:44 - the latitude change border where the
46:47 - rows are greater than 15 or where that
46:49 - values greater than 15 and then take
46:51 - that new data frame that we have and
46:53 - then using the pipe let's arrange it or
46:57 - order the rows by descending lab to
47:00 - change order and then the result of all
47:03 - of that combined
47:05 - gonna get saved into further away so
47:09 - we'll give that a run and if we scroll
47:13 - down now and we more latitude degrees
47:28 - mm-hmm yeah so that was a great great
47:30 - like point out by Natalia we we see
47:32 - Seattle first then we get to Oregon
47:34 - Oregon sprite south of Seattle then
47:38 - we're heading New York Michigan and so
47:42 - we're only taking the head right now is
47:43 - we're only seeing be the first six rows
47:45 - but now we have the furthest away
47:47 - centers and then the closer ones I'm so
47:52 - let's take a quick break for a second
47:54 - we'll go over to the chat yeah I also
47:57 - miss type like reader but it doesn't
48:00 - have any and neither does the player
48:02 - mm-hmm
48:04 - yeah I'm gonna roll back to the top of
48:07 - our code compare and yeah we can do some
48:10 - quick comparing and contrasting one
48:12 - thing that might also be leading to
48:15 - certain issues is all the code that
48:18 - we're typing is written in between these
48:20 - tick marks so we have three tick marks
48:23 - and then we close a code block with
48:24 - three tick marks and this is essentially
48:26 - saying everything that was in that block
48:29 - of code is our code that we want to run
48:32 - so if you're typing outside of those
48:37 - three tick marks that is not gonna be
48:41 - run as our code it'll just show up
48:43 - potentially as markdown yeah so think of
48:47 - it as like anything inside this world I
48:49 - can code and it'll run are anything
48:51 - outside this code it's a different
48:53 - language and it's the markdown language
48:54 - and we we thought it was useful to split
48:57 - up code blocks for you kind of when you
48:59 - get to this page you see a bunch of
49:01 - political blocks that's the legend logic
49:03 - chunks but theoretically you could take
49:05 - all of the code that's our and put it
49:07 - inside one block and then it
49:10 - would do the same thing but we'd like to
49:12 - split up logic or clean code and
49:14 - document it code and reproducible code
49:15 - which is really important that's a data
49:17 - scientist and one of those advantages of
49:20 - the notebook is that you can add more
49:22 - kind of almost like paragraph style or
49:25 - formal writing in between the code
49:27 - blocks to really explain what what kind
49:30 - of work is being done so if you're you
49:32 - know writing reports it's a really
49:35 - useful tool for annotation yeah but if
49:42 - you have any more questions about that
49:43 - maybe that clarified some of the stuff
49:44 - with typing outside of the blocks
49:47 - if not we can move on to the number of
49:49 - children's analysis so step 11 says that
50:00 - as a concerned citizen of the world you
50:01 - want to identify the detention centers
50:03 - that held the largest number of children
50:05 - so let's order the rows of the a cou
50:08 - data frame by the number of children in
50:10 - descending order and save the new data
50:13 - frame to a new variable called order by
50:16 - children so okay let's take that step by
50:19 - step but we kind of just did something
50:20 - like it hint to this descending function
50:23 - and this arranged function we're gonna
50:25 - do something very similar to that
50:27 - actually I'll just show you like a
50:29 - normal workflow that I do which is copy
50:31 - paste and then just replace I'm gonna
50:33 - copy paste that line so I don't have to
50:34 - retype it I'm also gonna make the
50:36 - variable first cuz that's kind of limit
50:38 - my brain logic skips to variable
50:40 - creation first secret about coding copy
50:45 - and pasting the vegetables yeah from
50:49 - your own code from your own come from
50:52 - stack overflow the internet or resources
50:54 - we're not saying that you can go ahead
50:56 - and you know exceed people's code that's
51:00 - not what we're saying
51:02 - what we're saying when you write
51:03 - something you know it's really helpful
51:06 - to go ahead take the code that you've
51:07 - written yeah
51:09 - utilize it again and something we are
51:12 - saying though is that the art community
51:13 - is pretty collaborative so a lot of the
51:16 - times you'll see people give talks at
51:17 - these different conferences and then
51:19 - they'll put up they'll link to their
51:20 - codes but you can also reproduce it
51:22 - yourself
51:22 - it is a very collaborative community
51:24 - very true
51:26 - you know we recommend as you guys kind
51:28 - of dig into the our world to engage with
51:32 - that community it's something that's
51:34 - been like enjoyable for I think both of
51:37 - us
51:37 - yeah and for for people that we know who
51:39 - have been working with our for for
51:41 - longer it's a huge part of why they love
51:44 - our so much and I think this is a truly
51:48 - unique thing that that really sets our
51:51 - apart is that there is this active you
51:54 - know community that is out there if you
51:57 - go on Twitter hash check our stats yes
52:00 - super engaging community and if you ever
52:04 - have questions it's a great place to go
52:05 - mm-hmm there's also lots of meetups this
52:09 - so okay so I'm gonna pipe it and then I
52:11 - have it in my clipboard that I copy and
52:12 - paste at this line and we're trying to
52:14 - arrange the ACO au data set in order of
52:17 - number two so I'm gonna paste that in
52:22 - and what's going to change in this
52:23 - particular operation is that instead of
52:28 - the Latin change border I'm gonna sort
52:30 - it by
52:35 - and that should do it
52:37 - that should give us a new data frame
52:39 - that has everything sorted by the
52:43 - highest virtually so it's pretty it to
52:46 - make sure we actually get to verify that
52:47 - worked ordered by and what this should
52:57 - tell us or show us is the detention
53:02 - centers that have the most number of
53:06 - children that are yeah actually when I
53:11 - first saw this report and I downloaded
53:12 - the data I was shocked that the Bronx
53:15 - was number one I was not shocked at all
53:18 - that Browns what was number two just
53:19 - because that is the border of Texas is
53:20 - actually where I'm from but the Brahms I
53:23 - was surprised about because I didn't
53:24 - know that it was happening so close to
53:26 - us like I thought it was happening down
53:28 - in the order just up of the borough yeah
53:32 - and that's I think a common thing with
53:35 - when doing lots of analyses that kind of
53:38 - you're working with like real world
53:39 - human data you know lots of issues can
53:42 - seem foreign to to people it might be
53:44 - something that they can't relate to but
53:47 - I think a really powerful tool of data
53:49 - science data analysis is like showing
53:52 - how something that you know might seem
53:54 - far and it can really be close to home
53:56 - or connected to to you and and that was
54:00 - one of the things that I really like
54:02 - connected with me step 12 which now that
54:11 - we've understood a little more about
54:13 - where was happening the most well let's
54:15 - see let's zoom in on particular state
54:18 - and then let's analyze that state
54:20 - because kind of right now we're still
54:22 - doing the whole country you see we have
54:23 - New York we have Texas Arizona Florida
54:25 - let's just go in it
54:27 - and before we kind of jump into the cove
54:30 - you want to reach out to you guys and
54:31 - see if there's any states you want to
54:35 - analyze or maybe yeah what you're
54:38 - looking in or maybe you're you know
54:39 - you're logging in from outside the US
54:41 - maybe there's a state you've visited or
54:43 - maybe there you haven't been to the US
54:45 - but the state you're interested in
54:46 - visiting more than you have family and
54:48 - our friends in so if you want to want us
54:50 - to check out a certain state just type
54:53 - it in to the chat give it a look we'll
55:00 - go ahead and set up kind of the code
55:02 - that we'll use to do this analysis in
55:04 - the mean time so I'm gonna create a
55:09 - variable called chosen state that is
55:12 - gonna hold the state that we are gonna
55:14 - take a look at I'm not gonna put
55:16 - anything in this variable right now I'm
55:18 - just gonna leave it as an empty string
55:20 - and we'll fill that with the state that
55:22 - I'm hopefully someone will provide to us
55:25 - and what we want to do is we want to go
55:33 - ahead and filter the rows of our ACLU
55:36 - data frame to only show the rows for the
55:39 - state we've chosen so I'm going to say
55:44 - ACLU and then okay first let's create
55:47 - the variable that we're gonna store this
55:48 - in so we want to call that chosen state
55:51 - separations so say chosen state
55:56 - California separations yeah I think just
56:03 - for many reasons will be interesting to
56:05 - analyze that state but it is a border
56:06 - state so I'm curious to see so as you
56:10 - can see in the state column we have this
56:12 - abbreviation of two letters for every
56:14 - state so the abbreviation for California
56:16 - is CA so we'll put that into our chosen
56:19 - state variable and then we'll say chosen
56:21 - state separations let's give it a value
56:23 - of taking our ACLU data frame and we're
56:27 - going to use the pipe and I'm going to
56:30 - just expand
56:32 - our code editor for a second and we'll
56:36 - type this into filter and we're going to
56:43 - say we that we want we want to give a
56:45 - condition that we're going to filter on
56:47 - so we're gonna say we want our the state
56:48 - to be equal to let me use the double
56:52 - equal sign here in studios measure
56:54 - equality since that single equal sign
56:56 - usually means what we're giving we're
56:58 - signing a value so we use that double
57:02 - equal sign is equal to our chosen state
57:06 - now why are we just saying chosen state
57:10 - instead of just typing in California
57:12 - directly we could just write CA but you
57:16 - know what's what's the purpose so the
57:18 - idea here is that by using this variable
57:20 - we can update that state really easily
57:23 - and and we're gonna use that chosen
57:26 - state again next a little bit later on
57:29 - and we won't have to update you know all
57:32 - the different parts of our code we can
57:33 - just update that one single chosen state
57:35 - and that's it you know
57:38 - I'm Josh accordingly so let's go ahead
57:44 - and run our code but before I run it
57:46 - let's go and take the head of our new
57:51 - data frame so that we can see what it
57:54 - looks like so we're gonna take the head
57:55 - of chosen state
58:05 - okay so I'm just gonna yeah I'll be my
58:11 - code do a little refresh
58:13 - yeah always copy your coat before you
58:14 - your fresh just to be safe I'm gonna
58:19 - strength this back and we'll see okay
58:25 - all our coat is still here I'm gonna
58:26 - click Save nice that worked and down
58:40 - don't want to look up in dictionary
58:44 - there we go so we see in concord
58:47 - california there was two children in el
58:49 - cajon there was 11 fuller 10 19 laverna
58:53 - too and then in los angeles there's only
58:55 - one mmm oh yeah we're now we're not
58:57 - seeing any detention centers that were
59:00 - in states outside of California so just
59:03 - seeing this one one state and if you're
59:05 - following along at home feel free to
59:07 - change that state from California to the
59:11 - state that you want to check out and you
59:13 - can see if there are any detentions are
59:15 - children to be held in detention centers
59:17 - there so now that we've chosen the state
59:21 - of California we want to find a way to
59:25 - organize this filter data frame and one
59:31 - way that we can do that is by now using
59:33 - the arrange function again and seeing
59:37 - how we can place these detention centers
59:39 - in order by the number of children that
59:43 - are being held there ordered by children
59:50 - but now it's gonna be happening on the
59:52 - chosen state separations variable so
59:55 - once again I'm going to use that pipe
59:56 - symbol and we'll place it at the end of
59:59 - filter which is saying let's take
60:02 - everything before the pipe that filter
60:04 - data frame and let's pipe it into our
60:07 - next function which is
60:10 - to be arraigned will give to arrange the
60:14 - column we want to arrange by which will
60:16 - be number children we wanted to do that
60:25 - in descending order and send the order
60:27 - not ascending will ESC and just wrap
60:32 - that around our number children and I
60:36 - will click head so down here sorts it
60:47 - this way and we see that the most
60:51 - detentions that happened in California
60:52 - was in Fullerton and it was nineteen you
61:03 - I'm just really curious what happens if
61:05 - we change this to Texas just because I
61:06 - am from there um so I'm gonna change it
61:08 - and kind of drive home the point that
61:09 - it's really nice to say things to
61:11 - variables we saved that and we run that
61:14 - in Texas there was a lot more separation
61:17 - that happened so we actually see that in
61:20 - Brownsville there was 348 in Harlingen
61:22 - which is 30 minutes from ran school
61:23 - there 174 in San Antonio there was 163
61:27 - there's just a lot more separations
61:30 - happening there and yeah so and just you
61:34 - know maybe 30 40 minutes of coding here
61:37 - today we were able to dig into this data
61:39 - set and get an idea of how far away our
61:42 - children being taken or where children
61:44 - being taken from their families we have
61:46 - an understanding of the areas where
61:49 - these detection centers were how far
61:51 - they were from the border how many
61:53 - children were there and you know just
61:56 - looking at the CSV it would have been
61:57 - you know rather difficult to ascertain
61:59 - this information but we you know just
62:02 - from this limited amount of time have a
62:03 - better understanding and can maybe speak
62:05 - better to you know what was happening
62:08 - and then take action based on you know
62:12 - this analysis and the ACLU you know did
62:16 - some data visualization as well you can
62:18 - relate it to this data if we go over to
62:20 - you know the article that they worked on
62:22 - they came up with this great
62:25 - visualization showing where these
62:27 - descent centers are located and
62:30 - assigning every is too huge Detention
62:32 - Center seat visibly tell which ones had
62:35 - over 350 children which ones had around
62:37 - 100 and which ones had around 10 we will
62:42 - say though like a big point when I Drive
62:43 - home and in general with anything we
62:45 - teach is that you know code allows you
62:48 - to analyze data and most of the time
62:51 - depending on what kind of research
62:53 - you're doing that data is very impactful
62:56 - and so we've been talking I'm strapped
62:57 - me about these number of children but
63:01 - there it is
63:02 - a human behind that is it and there's
63:04 - statistics that represent these humans
63:06 - so never losing that component when
63:09 - you're coding is really important yeah
63:12 - definitely just always you know as
63:14 - someone working with data analysis data
63:16 - scientist there is a sense of
63:18 - responsibility that we hope that as you
63:22 - dive into this if you're if you're newer
63:24 - to programming or if you're more
63:27 - experienced and you know just checking
63:28 - this out that you just take with you as
63:30 - you go forward because yeah you have
63:33 - access to a powerful tool and we hope
63:36 - that you use it to do some good with
63:39 - powerful data comes great responsibility
63:40 - yes we recommend that if you're
63:43 - interested in any of the things that we
63:46 - did with this data set or in analyzing
63:49 - your own data's that you're interested
63:51 - in that you go find something you're
63:53 - passionate about download it and do some
63:55 - of these things to it whether it be
63:56 - arranging it by descending order or
63:58 - filtering it or really diving into the
64:01 - nitty-gritty of it we think it can be a
64:03 - great way to learn and our is made up of
64:05 - this community of a bunch of professions
64:08 - not just statisticians it's a lot of the
64:11 - humanities researches a lot of Natural
64:13 - Sciences it's just it's a very
64:16 - intersectional community so welcome you
64:21 - know if you're so just getting started
64:23 - feel free to check out our courses on
64:25 - code Academy we have three modules that
64:28 - we're really excited about on there so
64:31 - give them a look dig into the data sets
64:34 - that we provided in there we
64:35 - some fun ones I'm in addition to you
64:39 - know the ACU project you can you know
64:41 - ones about dolls yes that's one that I
64:43 - really like there is another data set
64:46 - about popular music groups and so
64:49 - hopefully some of that will interest you
64:50 - all census data yes u.s. census data so
64:58 - there's lots of exciting things there
65:00 - and then definitely like Natalia said
65:01 - after that we really encourage you to go
65:04 - on your own
65:04 - find some data that you are interested
65:08 - in and do an analysis and I'm feel free
65:10 - to leave as many comments with data sets
65:13 - you would want to see mm-hmm yes
65:15 - definitely
65:16 - you know tag us on twitter at code kata
65:20 - me also reach out to our community
65:23 - hashtag our stats get involved we also
65:27 - would love your feedback about the live
65:29 - stream so we have a link that's just
65:33 - been placed into the chat so you can
65:35 - leave your feedback there we would be
65:38 - really really appreciated everything we
65:41 - do here is to is for learners like you
65:43 - to try and yeah hear what you guys are
65:47 - thinking and give you the best
65:48 - experience possible thanks for tuning in
65:51 - thank you so much for tuning in
65:53 - enjoy your coding and see you next time
65:56 - definitely see you next time take care
65:58 - everyone

Cleaned transcript:

oh hey everyone testing testing do you hear us okay something maybe comes up from the chat if your your aim I can hear us awesome thanks everyone for joining us yes yes welcome welcome we're streaming live from Kolkata me headquarters right here in New York City the Big Apple Big Apple my name is Nathan I adore the tigress and my name is Ian freed and we're the two curriculum developers behind the learn are launched today we hope you enjoyed our pirate puns as much as we did yes there's no feedback here they were a little corny we are a little corny a little punny bear with our but yeah we're super excited about today and this course release learn are the first our course I ever on code Academy and we're so excited about it yes when you know we spent the last few months going into our using kind of our backgrounds and data analysis and data signs and trying to think of some really intuitive and fun ways to teach what we think is a really really incredible tool for data analysis so a little bit of a highlevel overview of what is our exactly our is a statistical programming language that has a huge community of data enthusiasts and if you sign up for the course you'll see that there's three different modules that came out today but there's more coming soon the ones that came out today the first one is actually about our syntax and it walks you through some of the basic coding concepts like variables conditionals functions and how to import packages because if are as powerful for one thing it's for all the packages that it has for statistics and analysis and if you have never coded before it's totally alright our this intro course is great for people who want to jump into programming and learn things from the basics but at the same time if you're someone who's maybe been doing a little bit of JavaScript or a little bit of Python and you're see this language are and you're interested in it this is also a really great to jump in you know move over to what maybe we are starting to think is the best is out there but once you do not bias but and yeah if any of you um you know have been using are or have heard about it feel free to pop in the chat let us know we want to keep this you know chat here today a conversation we'll be talking about some really important things some really interesting things some cool things and we want to share that with you yeah we want to know why you're excited to learn kind of what your preconception of the language bar was before you signed up for the course and yeah so once you get past that you know introduction and syntax you can in the second module learn a little bit about data frames data frames are a really great data structure for organizing data that we can import maybe from a CSV or Excel spreadsheet or Google sheets and it basically the date frame will allow you and are to manipulate that data with different tools in order to organize it and arrange it in a way that's best for your own analysis so if you're someone who for work or for school is on Excel in Google sheets crunching numbers we hope that by the end of that second module you'll say goodbye Excel goodbye a Google Schneider circle hello and with that we'll be diving into a really useful package called deep liar that is a really great addon to our that enables you to work with data frames yeah and so I'm we're seeing some people have never heard of our or the heard of it but never use it that's totally okay we're here today to give you that introduction and we hope that you can get some practice with it as you follow along and we see Kenji baby has heard of our but hasn't used it that's perfect your perfect audience for this course yes and Elizabeth thanks for joining in and this is being recorded so you'll be able to access this on our YouTube channel and any time moving forward so we know we recommend you follow along today by watching the stream by also following along with us in the project but you know and let's say maybe you're a little bit newer and maybe you you got lost at some point which if you do and in time or some loss feel free to write in the chat but you know in a week from now let's say maybe you've gone through a few more of the modules and you're feeling more comfortable you can always come back and revisit the video will be there for you to watch so should we get into the data I kind of explain in the background of the topic of this project yeah let's let's let's seven okay cool so this project uses data that was published on an ACO you report which Kenya will pop right into the channel you can go ahead and read more about it but the ACO you report kind of is the result of a lot of data cleaning that the statistics team there did shout out to brook Watson she kind of inspired us taking on this data set thank you brother yeah Thank You Brooke um she has a great talk that kind of walks everyone through the process of the data cleaning and the legislation around family separations and the result of being able to organize data and recognize inconsistencies in it so that they could track down where the children who were separated on the family work and natal you want to give a little feedback for for our viewers on the situation and I kind of give like yeah context yeah let's start with context so there's a timeline kind of that explains the policy around family separation most of y'all maybe heard about it in the summer but a quick plug and just in general like a good approach to learning a language is to start with the data set that really interests you that's great advice from our inhouse data scientist Katherine she kind of told me you know start with something that makes you passionate and then go from there and I'll speak a little bit to my personal connection to a family separation and just immigration policy in general and then I'll explain the context of the whole thing but just in general anyone should be concerned about this is this general citizen but I grew up in the border of Texas so this kind of hit home and it and was really intrigued by understanding the data more so it helped me learn ours I was teaching it and I guess just from my perspective of both of our perspectives you know we we love programming we love coding we love data analysis and data science there are really useful tools and in our world and but we also think that there's a huge responsibility and task that comes with with this knowledge and you know there are so many projects that you can do out there so many things you can do with data analysis data science but to us it's really meaningful to take these skill sets that you can gain you know practicing here in Kolkata me you're practicing on your own or in school and using them to to make an impact and this is an area that we're you know we thought that the ACU was making a huge impact Brook was doing that and it's something that we're really passionate about too and so I think it's really meaningful for us I'd be able to share that with you guys here today and just be a reminder that you know data analysis the data that you're working with there's always stories kind of behind the the numbers as real people and real people and the work that you do analyzing the data can have a huge impact on people's lives so let's start with the boys because actually learned a lot about it I don't think that headlines really got into the weight of it but so in February 2018 the ACLU filed a class action lawsuit against immigration and customs enforcement branch of the government because at that point even though it was not a policy they had been separating children from their family some as far as two thousand miles away with little to no contact between the parent and the child the main case for this lawsuit actually only spoke to the child six times in a period of four months and they were separated 2,000 miles so this classaction lawsuit came about and in March when this was happening John Kelly proposed that this particular example could serve to help deter immigration on this other border so at that point conversation started around can get an actual policy aka making it legal to separate children from families and so inmate 2018 what became known as the zero tolerance policy actually went into place and family started being separated at the border and then that happened that was legal to do for like a month they were lots of protests maybe saw headlines about it but throughout that month over two thousand children were separated and at the end into the in June 2018 that's when the policy was stopped but at that point there was so much consolidation of the data that needed to happen so that the ice was equipped with how do you reunify his children that were separated with their families and the ACO you started analyzing data pouring in then June 2018 and kind of it was a month long effort months and months of consolidating the inconsistency so they could hold ice accountable for all the children that were in reunify debt and so the result of that is this really clean data set in this really awesome report written off by the ACLU and we use that and we take it one step further in the project to gain more insight into what was happening this summer so yeah that's that's kind of the background on all of it if you have any questions about that yeah I can we can talk more about it but if not let's just get to the code definitely and just before we jump in to just like one thing that you know they did at the ACLU was clean all this data that immigrations and Customs Enforcement was providing to them and you know when you're working as a data analyst or a data scientist you want to kind of get right away to you're like looking into the data getting insights maybe making visualizations or you know making some sort of model but often one of the most important tasks is just like organizing and cleaning the data because you can't you can't jump into that analysis until until the data is in a proper format and one of the things that they did here was you know sips through many different Excel spreadsheet was no standard documenting like the immigration number per child yeah so it may be like certain columns they have the same information but different names and different column or different sheets or even like inconsistencies and data like maybe you know for example and their documents they found that there were certain children who their date where they were is reunified with their family was a date before they were processed into the country so that information doesn't make sense so you it's important that when you're getting kind of raw data you're sifting through it and making sure that it makes sense and so that's something they did and then enabled us to go ahead and analyze yeah and that's not like an endemic problem to this dataset that happens all the time someone once said they're like 80% of a data scientist job is data cleaning so I think we you should extrapolate kind of the learning that's happening with this particular data set but realize that it applies to like a lot of scenarios not just this particular topic yeah and before we jump into the code I want to just take a look at what some of you guys are saying here so yeah using realworld interesting data is definitely the way to go I would say when trying to get more comfortable with doing data analysis like Natalia said find those data sets they're really stuck to you speak to you that means something to you and then also having that background knowledge on the data is super important as well it's really hard to jump into the data let's say like finance data if you don't have like some of that knowledge about finance and so even if it's just like you take a day or two to kind of dig into it it's really important to get that background I think your analysis will be a lot better for that so great point Arian and then someone else was asking is there a google of datasets and so there is we love it yeah what's the domain so I'm not sure the exact domain name but Google itself has a dataset search tool we'll try and find out that exact domain for you and you can try and post it in there but are you just Google even like Google dataset search it's a really great tool for finding datasets accessible for you to analyze yeah a lot of the times they come well documented so they'll tell you about the data collection process and the different fields and columns for it mmhmm and agree with Tim yeah katal is also a great place for finding datasets that are definitely well documented and especially also if you're if you're a newer you can kind of make me see what other people are doing with similar that kind of data again all of these links are posted up so if you want to learn more about the ACO a report or Burke Watson's talk you can find them yeah so this would be a great time if you guys are following along to make sure that you're in the the project with us and so you can either follow along in the the project that should be in the URL or if you're also going through the modules and you're at the ACLU data project you can follow along right in there so we have some background information here which kind of spoke at a spoke about we'll go ahead and get started with the first task you know always necessary always important so when you're gonna go ahead and do an analysis it's good to have access to different packages and what are packages packages are essentially collections of code that users have put together one of the other great things about our is that it's open source so anyone's able to go in and publish these packages where they can take all the work that they've done to simplify certain you know processes or tests that they think are really common and put them into a package that enables you to go ahead and just load that package and perform the task really nicely and simply and so we're gonna well I call it Diplo whatever insider deep fire and we're gonna use both of them so let's just go ahead and import them with the library function mmhmm I will say R has lots of kind of fun package names that three hours I think it is so they're into the puns too okay so to go ahead and load a library or a package we use the library function and you give as an argument to the library function of the name of the package you'd like to load so I'm gonna load the reader I'm also gonna load the packages and also if you're unfamiliar kind of with our notebooks which I would say problems yeah you know a lot of people may be and we were as well so basically what we're doing and let me move this over to the side just for a second make this bigger so it should be a little bit bigger for you guys now our notebooks are a really cool way to code because they enable you to not only write code that you'll be able to run but you can also do some nice annotation with markdown markdown just a nice way to yeah just add some some annotation explanation to your code but what's really cool is once you run the notebook file it goes ahead and renders into a HTML page which we don't have any code that's gonna do anything exciting yet and I'm gonna try and expand this side as well but in a little bit you'll see what this rendered notebook looks like and you can see it right here on the right hand side we just have any code yet but it takes your code and it makes it look nice and presentable and then it will also show the output yeah and what I really like specifically about our notebooks is that I kind of isolates the subtasks of a bigger program so if you see we've grouped code blocks and we've given a comment to each one with the hashtag symbol that kind of explains what our code block will do as we go through the project but if later you need to go back and change part of the logic you can just go straight to that part and not have to sift through the whole file mmhmm definitely I would definitely agree that kind of displaying and up into the different segments makes it really easy and then you can also see what the output looks like after each segment so if you're doing a data analysis where you're you know changing columns of a data frame or you're reordering things you can see at each step how that data is changing and confirm that your code is working as expected so the second step after we have imported our libraries we're ready to use it is to actually import the CSV file with the data so that we can use it inside a data frame which is a data structure for act angular data in our so let's go see what's that file called in our so yeah if you just click on the little top left there is that a file navigator I mean you can see the different files there so it's called a cou separations dot CSV and the way that you make a variable in our so that we could save it into a dataframe variable is you use this fun arrow syntax which is a little different than how most languages do it most languages use an equal sign but our uses this arrow syntax and then after that comes the value that you wanna identify that you want to assign the identifier so we made a variable name a cou and inside of that variable we want to go ahead and read the CSV which is a reader function and we want to pass in a string or character type that has the file name that we just saw which was a cou and in order to see if that actually loaded in as the right data frame we want to just get kind of the we want to print it so that we can actually see it right now when we hit that and the way that you do that there's many ways to do it you can print the whole thing you can print part of it but the way that most that is most common is to just print the top six rows of it so you can see what's in there and the way that you do that is with this function called head so you want the head of this data frame we just made called ACL you save it and here we have it and now you can see that on this right hand side this is what the surrendering notebook is looking like and so it gives us a little bit of feedback for some of the different code blocks so when we loaded the CSV file existed some information on the columns that are being brought in and then here when we look at the head of the data frame we are seeing the different columns so you see Baltimore big town Bethlehem let's just look at the column names really quickly together address maybe that's refer address and which is the number of children that were separated in that particular location the program City the program state the lawn which is the longitude and the latitude so it's kind of what we're working with here in this dataset so another way that's really good to kind of inspect data once you load it which is always a nice thing just because you might have some big CSV and you don't know the information that's in there and you wanna get an idea is to use the summary function and let's go ahead and use the summary function we'll pass in our data frame ACLU as an argument and it just provides a of summary statistics about the data frame so it will give for each different column if it's a numeric column so we're working with numbers it'll say you know how what's the smallest value what's the largest value what's the median value and maybe give a mean or a max and you know if you're not too familiar with different statistics like you know don't worry about it um we already have we have the courses for the mean median and mode coming out soon within the next two months so keep an eye out for those I did see someone and the chat was asking about more advanced our concepts and that's coming to you soon yes it's coming to you soon and are we also have some statistics and Python courses currently going so if you want to kind of jump and see things from a different perspective that's also a great way to go yeah so this gives just like nice kind of summary information so you have an idea of the data that you're working with so for example if we look at longitude I'm going to zoom in a little bit here so we can see we can see the the lowest longitude is negative 122 and then the let's see the max is negative 71 with latitude we're seeing a minimum of 25 and a max of 47 and so I I feel like all the time whenever I see lots of longitude I'm like which which which one goes which way so the lines of latitude they run horizontal around the world so those will kind of give you an understanding of how far north or south you may be so we're ranging between a minimum latitude of 25 and a maximum of 47 and so once again what it like one of these numbers mean all right sometimes it's like easy to get lost so that latitude is the latitude of these different detention centers where you know children who are being separated with their families where we're being taken so so I think it's always good to kind of just jump back and get like okay what what does it number mean in the contact of my project rather than it's just every ages from 25 to 47 yeah another interesting statistics just right up off the top with the summary function is that the median sorry the mean for children separated at any given location was 41 all the different locations we see in there that's what an average is up to but then yeah we see there's one location with a max of 350 50 no children separated there so I will see you later which one that is when we analyze it a little more for a second and and Connor I see that you are running into some trouble in your first five lines of code that happened to us like before we got anything installed in our computer we ran into a bunch of trouble let us know what's happening but also it's okay to be a different stage size I think like if you know a little bit of programming from another language it might be a little bit you might pick up are a little bit more quickly but that doesn't it's not a reflection of your ability to do it or like if that looks very differently for everyone so we definitely encourage everyone on the threat to help each other out let us know how we can help yeah we've all been there you know if your clothes not running and I would bet that sometime during the next 40 minutes our clothes yeah so yeah so alright we now saw kind of in the data let's start inspecting in step four says after you've inspected it you realized that the address column contains the same information that is contained in the program city and program state columns let's go check that out so let's say address column has Baltimore Maryland and then program city has Baltimore in programs to you Maryland that seems pretty repetitive hmm select all the columns from the ACLU but that one and save your new data to the ACLU variable so wait to clean what this is saying is like that's kind of repetitive remove that one let's go ahead and do that we're gonna reassign the value of the ACLU so that it changes with the updated data frame that does not include that one and the way that you do that is you use this pipe symbol the way that I describe this one to any student whenever I'm talking about art is that anything that comes after this pipe refers to the data frame that came before it and was gonna come after this is we're gonna stay select what's currently in the a cou data frame and give all of it back except for that's what the minus sign says the column adder which is address so we're saying select and program City program State longitude and latitude but please don't give us the address let's just make sure that works by printing it with our handy function head and so like another way to think about what that pipe symbol is doing is it's taking the ACLU data frame that's on its left and it's kind of pushing it or piping it into the first argument of that function select so and it's just another way that of kind of framing or writing code and are and you know as you dig deeper I and further into our this kind of formatting becomes super helpful and really like organizing your code and makes it clear for you know like what you are doing to a data frame yeah should we be writing about the pipe so kind of they can see yeah we can we can do both ways but another way to think about it without the pipe so that you can understand the cup a little better you could just do the same thing but instead of putting the pipe you would just put the first argument that's the data frame so like I was gonna look at this that line of code before I would say okay let's select from ACLU every column except address but now when we write it with the pipe you can say like from this data frame ACLU select everything but the column address yeah there's many way to do it we could have done the inverse which is listed out every single one but address and there's going to take a quick second so I see yeah Connor I'm glad you're not seeing the internal error anymore if they're still potentially in our in the our notebook the the rendered page might not you know appear properly so what I would do is try and look back at every code block that you know we've written compare it to yours and see that things are kind of similarly and then hopefully that should address the issue that's a ten yeah yes I was like filing yeah I think that don't happen though they might have to click on expand on the aircool so let's go ahead and do step 5 mmhmm so we were taking a look at the columns before but but one thing I always like to do especially I have a really big data set is just print out all the column names so I know I know the dinner that I'm working with because it's not always the case that you can easily click through and your notebook and see see all the columns and so the way we can do that is by using the call names function and we just pass to the call names function as an argument our data frame ACLU I want to go ahead and just put that all inside a print statement you don't have to put inside the print statement it would still render in the notebook but the output looks a bit nicer if you wrap it in the print statement so we'll go ahead and run this and we'll scroll down and we'll now see that were printing out the column names that we were seeing earlier now maybe maybe we can have reach out to you guys you guys think that these are helpful column names are they are they descriptive enough for you and they took my bows like what do you what do you think yeah I know I've got some thoughts and opinions on them so doesn't this Holly oh we're very opinionated people so Leah maybe maybe we can see if you guys have any thoughts and maybe if you have would like to see some changes for those column names maybe you can even without seeing what we've written I think of maybe some column names that we could change these two if you have an idea feel free to put it in the chat well we'll wait a little bit for you for someone to me put something in there just like a quick reminder these are the column hmm so we have one two three four five column names currently one is and it's not gonna Talia that's that's you know that says something about the column name it's not very descriptive mmhmm and we have program city program state and then lon which actually almost looks like Ian also yeah and yes yeah alright so we let's just brainstorm together like given what we know about the data set you know we didn't all this research creating the ACO your report we can kind of speculate what these mean and then let's come up with names together and let's so we're gonna reassign our data form to be more descriptive so we'll do that same kind of pattern we've been doing we just say the name of the variable and then reassign it with this arrow but after we're gonna do something to the a cou data frame with right and that something is gonna be we're gonna rename the column so based on that action we're gonna use the function called rename that's one really sweet thing about our that I found you know I know other languages and are just has the best most descriptive and concise function names that tell me exactly what it's doing yeah rename will take in X amount of arguments depending on what you want to do with that but the way that it takes them is it takes the name of the new column and the name of the old column and then it reassigns that so if let's say we wanted to name the rename the first column and which is not very descriptive to something like number of children because that's way more descriptive the way that we would pass that argument and it's like this number underscores okay and we would follow that pattern with the other ones every one to rename them in my opinion program city is too verbose the column name what what that column is is just a city so I'm gonna rename it to City oh and look I just pattern matched myself so I supposed to actually be the name of a new column and then after it the name of the old column this is one that I still make mistakes on all the time yeah always do it in the wrong way I have my own opinions about attacked I think I should be like you should have old column name first and then anyone cuz that makes more sense intuitively but our has its reasons and if you know this is a like a general programming thing that I really recommend if you're ever unsure about a function what it does or a package and the different functions that are in it going online just maybe searching are the package name so this case may be like Rd plier and renamed and right here in documentation you can go to the the website see the documentation for this package and for that function and potentially see examples or explanations what the arguments are or should be and that's always a really helpful tool for for troubleshooting or for answering questions true let's print it out to see what happened and we have a great question from Alex does our have strings yes our does have strings that you indicate with quotes but often when we are referring to like a column name of a data frame you don't need to use the quotes to indicate a string let's say if we're using that column name as an argument to a function so yeah great question and well we're gonna work with strings a little bit later on yeah and we do so here we have it we use the rename function after piping it the a cou data frame and it renamed end number of children program city to city program state to state and latitude and longitude to their full flush that works so now we can start coding and analyzing and actually know what these columns represent mmhmm and now that we have this information here you know when I was originally looking at this I was thinking okay we have this data that we got from the ACLU but what what else might I want to try and do with it right what else could we could we find out and I think this is an important question to ask when you're doing a data analysis because you might get your data from one source but you know that might you might want to dig a little further and find something else out that the original data didn't have and I was interested in trying to see how far some of these detention centers where children were being taken were from the border yeah to give a sense of you know really how far away some of these children under age 5 in many cases we're being taken I think it's a great question that you're asking with that specifically because a lot of the kind of headlines that went around with this topic didn't mention kind of the magnitude of the separation it wasn't just like they were kept in separate rooms a lot of the times they were kept in completely different states miles and miles from where the parents were kept so yeah and and you know when I was thinking bout this I said okay how how can I try and come up with some sort of metric to determine how far away these centers were and so since we were given the latitude locations at the center's I came up with the idea that maybe we should try and find the latitude for for part of the border between US and Mexico and kind of use that as a baseline to measure distance and so you know the usmexico border is thousands of miles long you know it stretches from the western most parts of the country to the eastern most and it varies it in latitude but you know I went ahead and we we found this the lower latitude of 25.8 3m as kind of a low point and in order to use this in an app our analysis we're gonna go ahead and create a variable just going to be called border latitude yeah and that old servers our frame of reference and we're gonna assign it the value twenty five point eight three and yeah once again just just to kind of backtrack for a second with the arrow like assignment operator you know you're still feeling like maybe a little uncomfortable from it if you're coming from another language I like to just think of it as you're taking the value on the right and the arrow is saying put that to the left into this variable I'm so now board allotted to it is now storing this twenty five point eight three identifier value and so now that we have this border latitude we can go ahead and try and create a new column to add to our data frame that's going to contain the distance or the latitude change really from that individual detention center to the border latitude that we have defined here so once again I'm going to I want to store this new data frame with this new column of you know back into ACLU our different we've been working with and we're going to take the data from that we're working with ACLU we're going to use the pipe symbol once again and we're going to use a new function keep using the dollar sign a new function that's also part of D plier that is used for adding columns to a data frame and that function is called mutate and that's also aptly named because that is what we're doing to the data frame we're mutating it by inserting a new column in it mmhm and the way the mutate works is we provide as an argument to mutate what we want the new column that we're creating to be called and we give an expression that defines how our calculating this new column so let's go ahead and we're going to name this new column lat change order just to indicate that we're saying this is the change in latitude from the Detention Center to the border and it will be equal to we want to find the difference between this border latitude and the latitude of the question Center so we have this column latitude so we're gonna say it's equal to latitude border latitude before we present let's just go through so we're saying alright take that data frame a cou and mutate it by computing a new whole new column called lat change border and the values inside that column will be based on what's in the latitude row for the data frame and in the word order latitude value that we have to be 25.8 III based on the average of work for tourism and I made another little mistake I have this equal sign here when I really want to sit sign because we're finding the the difference so let's just do an example really quickly if we're looking at this detention center that in Baltimore Maryland will zoom in for a second and I'll go over and we see that has a latitude of thirty nine point two nine we're gonna say take that dirty nine point two nine subtract 25 from it so we're getting fourteen approximately you know a little less than fourteen changed latitude so once we do this we want to go ahead and take the head once again of the data frame just so we can see what this scroll down and now we are over to the righthand side oh okay I'm gonna move my head out of the way and we can see we have this new column I keep backing up further a lot change border that is showing that calculation that we just made showing that changing latitude from the border to the detention center and that kind of makes sense if we look at the latitude for Maryland and we take that 25 it's about 13 in difference so it computed it exactly so let's go on to step 9 step 9 cents that's this whole section is actually about filtering and arranging gross so now that we have a column representing a Detention Center distance from the border you want to see which detention centers are far away from where the family separation occurred filter the rows of ACLU to find all of the detention centers where the latitude change border is greater than 15 save this new data frame to a new variable called further away and then let's see what's in that a different all right so I like this stuff because it has a lot to it we're basically now getting to the analysis part so we're gonna can you variable let's do that first further away we're gonna sign it and arranged version of what cou based on a condition um so let's pipe the ezo you and filter and like Natalia was saying before I think one of the really great things about deep liar which is a lot of the functions that we're using here today is they do they're doing exactly kind of what they say so here we want to filter out rows that don't meet this condition of the lot change water it's greater than 15 so the condition we're about to write is just gonna go inside the parentheses and that condition students basically give us anything where the value inside this column that change border is greater then after that what's printed I think I think it's just a formatting that's good further away great and now let's go take a look at this new data frame nice and so if we take if we go over to the right we can see that all of the latitudes or this lat change border column all those values are greater than 15 so we filtered out all the rows that you know don't meet that condition and are now just looking at these locations that are much further away from from the border now now we've done this filtering but there's kind of a next logical step that can be done once we've done the filtering and that is to order the rows of this data frame to see okay now that we have this group like which detention centers of this selection are furthest away I'm compared to which are closer and we can order or arrange these different rows using the arrange function and this is where we're going to go ahead and really use the power of the the pipe symbol that we've been using here and I'm gonna go ahead and I'm going to expand the code editor for a second yeah and so the way them I'm gonna use the pipe symbol now is instead of just making a new variable and saying okay let's take further away and pipe it into this function of rain if you want to use but we're gonna just take the pipe and stick it on to the end right here after our filter and then I'm gonna put a reins right here and to arrange we basically give as an argument the column that we want to order the rows by so if I go ahead and say lat change order this would then go ahead and arranged in ascending order our data frame smallest values and then from smallest to highest but we're interested in getting descending order so if you want to see the furthest ones away and then getting closer and closer so all we have to do is just put in DSC this is descending function and give it that change border as an argument so we're just going to wrap the column that we want to order by list DSC and so what what is happening right when we added this second pint so basically what this is doing is saying let's take all of the the code beforehand that we've done we've taken the ACLU data frame we filtered it on the latitude change border where the rows are greater than 15 or where that values greater than 15 and then take that new data frame that we have and then using the pipe let's arrange it or order the rows by descending lab to change order and then the result of all of that combined gonna get saved into further away so we'll give that a run and if we scroll down now and we more latitude degrees mmhmm yeah so that was a great great like point out by Natalia we we see Seattle first then we get to Oregon Oregon sprite south of Seattle then we're heading New York Michigan and so we're only taking the head right now is we're only seeing be the first six rows but now we have the furthest away centers and then the closer ones I'm so let's take a quick break for a second we'll go over to the chat yeah I also miss type like reader but it doesn't have any and neither does the player mmhmm yeah I'm gonna roll back to the top of our code compare and yeah we can do some quick comparing and contrasting one thing that might also be leading to certain issues is all the code that we're typing is written in between these tick marks so we have three tick marks and then we close a code block with three tick marks and this is essentially saying everything that was in that block of code is our code that we want to run so if you're typing outside of those three tick marks that is not gonna be run as our code it'll just show up potentially as markdown yeah so think of it as like anything inside this world I can code and it'll run are anything outside this code it's a different language and it's the markdown language and we we thought it was useful to split up code blocks for you kind of when you get to this page you see a bunch of political blocks that's the legend logic chunks but theoretically you could take all of the code that's our and put it inside one block and then it would do the same thing but we'd like to split up logic or clean code and document it code and reproducible code which is really important that's a data scientist and one of those advantages of the notebook is that you can add more kind of almost like paragraph style or formal writing in between the code blocks to really explain what what kind of work is being done so if you're you know writing reports it's a really useful tool for annotation yeah but if you have any more questions about that maybe that clarified some of the stuff with typing outside of the blocks if not we can move on to the number of children's analysis so step 11 says that as a concerned citizen of the world you want to identify the detention centers that held the largest number of children so let's order the rows of the a cou data frame by the number of children in descending order and save the new data frame to a new variable called order by children so okay let's take that step by step but we kind of just did something like it hint to this descending function and this arranged function we're gonna do something very similar to that actually I'll just show you like a normal workflow that I do which is copy paste and then just replace I'm gonna copy paste that line so I don't have to retype it I'm also gonna make the variable first cuz that's kind of limit my brain logic skips to variable creation first secret about coding copy and pasting the vegetables yeah from your own code from your own come from stack overflow the internet or resources we're not saying that you can go ahead and you know exceed people's code that's not what we're saying what we're saying when you write something you know it's really helpful to go ahead take the code that you've written yeah utilize it again and something we are saying though is that the art community is pretty collaborative so a lot of the times you'll see people give talks at these different conferences and then they'll put up they'll link to their codes but you can also reproduce it yourself it is a very collaborative community very true you know we recommend as you guys kind of dig into the our world to engage with that community it's something that's been like enjoyable for I think both of us yeah and for for people that we know who have been working with our for for longer it's a huge part of why they love our so much and I think this is a truly unique thing that that really sets our apart is that there is this active you know community that is out there if you go on Twitter hash check our stats yes super engaging community and if you ever have questions it's a great place to go mmhmm there's also lots of meetups this so okay so I'm gonna pipe it and then I have it in my clipboard that I copy and paste at this line and we're trying to arrange the ACO au data set in order of number two so I'm gonna paste that in and what's going to change in this particular operation is that instead of the Latin change border I'm gonna sort it by and that should do it that should give us a new data frame that has everything sorted by the highest virtually so it's pretty it to make sure we actually get to verify that worked ordered by and what this should tell us or show us is the detention centers that have the most number of children that are yeah actually when I first saw this report and I downloaded the data I was shocked that the Bronx was number one I was not shocked at all that Browns what was number two just because that is the border of Texas is actually where I'm from but the Brahms I was surprised about because I didn't know that it was happening so close to us like I thought it was happening down in the order just up of the borough yeah and that's I think a common thing with when doing lots of analyses that kind of you're working with like real world human data you know lots of issues can seem foreign to to people it might be something that they can't relate to but I think a really powerful tool of data science data analysis is like showing how something that you know might seem far and it can really be close to home or connected to to you and and that was one of the things that I really like connected with me step 12 which now that we've understood a little more about where was happening the most well let's see let's zoom in on particular state and then let's analyze that state because kind of right now we're still doing the whole country you see we have New York we have Texas Arizona Florida let's just go in it and before we kind of jump into the cove you want to reach out to you guys and see if there's any states you want to analyze or maybe yeah what you're looking in or maybe you're you know you're logging in from outside the US maybe there's a state you've visited or maybe there you haven't been to the US but the state you're interested in visiting more than you have family and our friends in so if you want to want us to check out a certain state just type it in to the chat give it a look we'll go ahead and set up kind of the code that we'll use to do this analysis in the mean time so I'm gonna create a variable called chosen state that is gonna hold the state that we are gonna take a look at I'm not gonna put anything in this variable right now I'm just gonna leave it as an empty string and we'll fill that with the state that I'm hopefully someone will provide to us and what we want to do is we want to go ahead and filter the rows of our ACLU data frame to only show the rows for the state we've chosen so I'm going to say ACLU and then okay first let's create the variable that we're gonna store this in so we want to call that chosen state separations so say chosen state California separations yeah I think just for many reasons will be interesting to analyze that state but it is a border state so I'm curious to see so as you can see in the state column we have this abbreviation of two letters for every state so the abbreviation for California is CA so we'll put that into our chosen state variable and then we'll say chosen state separations let's give it a value of taking our ACLU data frame and we're going to use the pipe and I'm going to just expand our code editor for a second and we'll type this into filter and we're going to say we that we want we want to give a condition that we're going to filter on so we're gonna say we want our the state to be equal to let me use the double equal sign here in studios measure equality since that single equal sign usually means what we're giving we're signing a value so we use that double equal sign is equal to our chosen state now why are we just saying chosen state instead of just typing in California directly we could just write CA but you know what's what's the purpose so the idea here is that by using this variable we can update that state really easily and and we're gonna use that chosen state again next a little bit later on and we won't have to update you know all the different parts of our code we can just update that one single chosen state and that's it you know I'm Josh accordingly so let's go ahead and run our code but before I run it let's go and take the head of our new data frame so that we can see what it looks like so we're gonna take the head of chosen state okay so I'm just gonna yeah I'll be my code do a little refresh yeah always copy your coat before you your fresh just to be safe I'm gonna strength this back and we'll see okay all our coat is still here I'm gonna click Save nice that worked and down don't want to look up in dictionary there we go so we see in concord california there was two children in el cajon there was 11 fuller 10 19 laverna too and then in los angeles there's only one mmm oh yeah we're now we're not seeing any detention centers that were in states outside of California so just seeing this one one state and if you're following along at home feel free to change that state from California to the state that you want to check out and you can see if there are any detentions are children to be held in detention centers there so now that we've chosen the state of California we want to find a way to organize this filter data frame and one way that we can do that is by now using the arrange function again and seeing how we can place these detention centers in order by the number of children that are being held there ordered by children but now it's gonna be happening on the chosen state separations variable so once again I'm going to use that pipe symbol and we'll place it at the end of filter which is saying let's take everything before the pipe that filter data frame and let's pipe it into our next function which is to be arraigned will give to arrange the column we want to arrange by which will be number children we wanted to do that in descending order and send the order not ascending will ESC and just wrap that around our number children and I will click head so down here sorts it this way and we see that the most detentions that happened in California was in Fullerton and it was nineteen you I'm just really curious what happens if we change this to Texas just because I am from there um so I'm gonna change it and kind of drive home the point that it's really nice to say things to variables we saved that and we run that in Texas there was a lot more separation that happened so we actually see that in Brownsville there was 348 in Harlingen which is 30 minutes from ran school there 174 in San Antonio there was 163 there's just a lot more separations happening there and yeah so and just you know maybe 30 40 minutes of coding here today we were able to dig into this data set and get an idea of how far away our children being taken or where children being taken from their families we have an understanding of the areas where these detection centers were how far they were from the border how many children were there and you know just looking at the CSV it would have been you know rather difficult to ascertain this information but we you know just from this limited amount of time have a better understanding and can maybe speak better to you know what was happening and then take action based on you know this analysis and the ACLU you know did some data visualization as well you can relate it to this data if we go over to you know the article that they worked on they came up with this great visualization showing where these descent centers are located and assigning every is too huge Detention Center seat visibly tell which ones had over 350 children which ones had around 100 and which ones had around 10 we will say though like a big point when I Drive home and in general with anything we teach is that you know code allows you to analyze data and most of the time depending on what kind of research you're doing that data is very impactful and so we've been talking I'm strapped me about these number of children but there it is a human behind that is it and there's statistics that represent these humans so never losing that component when you're coding is really important yeah definitely just always you know as someone working with data analysis data scientist there is a sense of responsibility that we hope that as you dive into this if you're if you're newer to programming or if you're more experienced and you know just checking this out that you just take with you as you go forward because yeah you have access to a powerful tool and we hope that you use it to do some good with powerful data comes great responsibility yes we recommend that if you're interested in any of the things that we did with this data set or in analyzing your own data's that you're interested in that you go find something you're passionate about download it and do some of these things to it whether it be arranging it by descending order or filtering it or really diving into the nittygritty of it we think it can be a great way to learn and our is made up of this community of a bunch of professions not just statisticians it's a lot of the humanities researches a lot of Natural Sciences it's just it's a very intersectional community so welcome you know if you're so just getting started feel free to check out our courses on code Academy we have three modules that we're really excited about on there so give them a look dig into the data sets that we provided in there we some fun ones I'm in addition to you know the ACU project you can you know ones about dolls yes that's one that I really like there is another data set about popular music groups and so hopefully some of that will interest you all census data yes u.s. census data so there's lots of exciting things there and then definitely like Natalia said after that we really encourage you to go on your own find some data that you are interested in and do an analysis and I'm feel free to leave as many comments with data sets you would want to see mmhmm yes definitely you know tag us on twitter at code kata me also reach out to our community hashtag our stats get involved we also would love your feedback about the live stream so we have a link that's just been placed into the chat so you can leave your feedback there we would be really really appreciated everything we do here is to is for learners like you to try and yeah hear what you guys are thinking and give you the best experience possible thanks for tuning in thank you so much for tuning in enjoy your coding and see you next time definitely see you next time take care everyone
