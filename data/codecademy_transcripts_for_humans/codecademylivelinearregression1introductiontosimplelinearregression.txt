With timestamps:

00:00 - okay but we don't know for sure but yeah
00:02 - let's go ahead and get started
00:04 - okay let us know in the youtube chat if
00:08 - you can hear us
00:09 - and see us um we'll be getting started
00:12 - in just a moment i feel like
00:17 - cool we look good together
00:20 - all right well welcome to our first
00:23 - session
00:23 - of codecademy live going through the
00:27 - recent launch recently launched
00:30 - linear regression course um we're gonna
00:33 - have eight sessions over the next nine
00:35 - weeks so we're gonna skip the week week
00:37 - of fourth of july
00:38 - um that we're going to have every
00:40 - tuesday at 11 a.m
00:42 - we're going to go through some content
00:43 - together um it'll
00:45 - be alex and i for today might have some
00:47 - different
00:48 - uh c devs so curriculum developers that
00:51 - codecad me jumping on these calls
00:54 - um but we're really really excited and
00:57 - then on
00:58 - thursdays for at least the next three
01:00 - weeks we're gonna run
01:01 - kind of an open office half hour
01:04 - also at 11 a.m so if anybody has
01:08 - questions or things that they weren't
01:10 - able to ask in this
01:12 - in this session um for anyone that was
01:14 - watching it later and things came up
01:17 - or anyone really if it's a small group
01:19 - who has questions about
01:20 - any of our data science content we'll
01:23 - welcome questions we really just want
01:25 - um to get to know everyone who
01:28 - is joining us and and we want to spend
01:31 - some time with you and learn what you're
01:32 - interested in
01:33 - and what you want to get out of this
01:35 - almost like a regular classroom so
01:38 - um it's good to see some people saying
01:40 - hello in the chat
01:41 - wow we've got someone from vietnam um
01:44 - that's awesome we moved the uh
01:48 - the time of this event from some of our
01:50 - previous live streams hoping that maybe
01:52 - we could get
01:53 - uh more people from around the world uh
01:56 - in a reasonable time zone for for
01:59 - joining us so i hope that that was
02:01 - helpful
02:02 - yeah so to join that uh that
02:06 - office hour on thursday um this is our
02:08 - first time kind of really trying this
02:10 - so we hope that the tech works and it's
02:12 - all set up properly
02:13 - but there's a link in the youtube
02:15 - description to kind of an event page
02:17 - where you can register basically it's
02:18 - just you're going to be
02:19 - joining a zoom webinar um i also just
02:21 - posted it in the chat
02:23 - um so if you're interested in that um
02:25 - hopefully it all goes smoothly
02:27 - um this first week and we can we'll see
02:30 - uh
02:30 - see some of you in kind of like a
02:32 - private zoom call where you can ask us
02:33 - some questions
02:35 - yeah super awesome i guess since this is
02:37 - our first
02:38 - stream um i'll quickly introduce myself
02:41 - and alex can introduce himself as well
02:43 - um so i'm sophie i'm a curriculum
02:46 - developer at codecademy
02:47 - i've been here now almost a whole year
02:50 - which is crazy and i work on data
02:52 - science content primarily
02:54 - um and i was pretty involved in building
02:56 - the linear regression course
02:58 - um working with content contributors and
03:00 - writing some of the content myself so
03:02 - super excited to go through it
03:04 - yeah and i'm alex i'm also a curriculum
03:06 - developer
03:07 - um i used to be on the data science team
03:08 - with sophie but i've
03:10 - gone over and started doing more of the
03:12 - computer science content so
03:14 - that those are courses that are kind of
03:15 - like similar to traditional college
03:17 - courses
03:18 - um so yeah if you check out the computer
03:20 - science past we're
03:22 - building out a ton of new stuff in that
03:24 - path
03:25 - awesome cool wow i see some people from
03:28 - india
03:28 - trinidad and tobago atlanta belgium
03:32 - super awesome um so we are keeping an
03:34 - eye on the youtube chat throughout this
03:36 - i think we're streaming on a couple
03:38 - other platforms but um come over to
03:40 - youtube if you want to chat with us live
03:42 - feel free to ask questions as we go
03:44 - along um i'm going to kind of lead and
03:46 - alex is going to keep an eye on the chat
03:49 - um and so he can let me know stop me if
03:53 - anyone has questions
03:55 - cool cool i'm going to go ahead and
03:57 - share my screen
03:59 - also while i'm sharing my screen see
04:01 - someone asked if there
04:02 - are any free courses on code academy
04:04 - alex you want to take that really quick
04:06 - yeah lots of free courses on code
04:08 - academy if you go to the catalog
04:10 - kind of like the full catalog you should
04:12 - see um
04:14 - so there's a bunch of paths which are
04:15 - all pro only but if you scroll down
04:17 - towards the bottom you should see
04:18 - courses
04:19 - and uh some of those are marked as pro
04:22 - only but a lot of them
04:23 - are totally free so um yeah lots of free
04:26 - courses
04:27 - in our in our catalog yes and i believe
04:30 - is i believe this course is also free
04:34 - yeah i think so what is what is this one
04:35 - actually called sophie or if you want to
04:37 - show off
04:37 - radio regression in python yeah so
04:41 - so so if you're your fourth power yeah
04:43 - your fourth tab there looks like it's
04:45 - the um
04:45 - oh yeah on this page
04:48 - well yeah let's see
04:53 - um if you go to the catalog and look for
04:55 - a linear regression in python
04:57 - um let's see
05:01 - linear regression in python yep that is
05:03 - a free course so
05:05 - anyone may join it and and take this
05:08 - content
05:08 - on your own as well cool so
05:12 - we're going to get started uh so before
05:16 - i code anything um i just want to
05:19 - remind everyone and this is in the
05:21 - youtube description as well
05:23 - but we have a um a github page
05:26 - that has all of this uh
05:29 - all of this code that i'm gonna be
05:32 - writing
05:33 - um and it has all of the code for the
05:36 - next
05:37 - three weeks after after this so you can
05:40 - get a head start and take a look
05:41 - it's meant to complement this content um
05:44 - we're gonna be using jupiter notebooks
05:46 - which requires
05:47 - a little bit of prep work and a download
05:49 - but if you didn't have time to do it
05:51 - before today you can definitely do that
05:52 - before the next
05:53 - session um so
05:56 - a little bit of motivation so
06:00 - linear regression at its core
06:03 - is a modeling technique um
06:07 - it's considered part of machine learning
06:10 - um
06:11 - and it's really like the the base model
06:14 - that a lot of more complex models are
06:17 - built
06:18 - upon so it's really if you're interested
06:21 - in machine learning you're interested in
06:22 - data science you're interested in
06:24 - statistics
06:25 - or you even just want to be able to like
06:27 - read other
06:28 - uh analyses read read papers or
06:32 - um you know find out about
06:35 - medical research or whatever it's a
06:38 - really good thing to understand because
06:39 - it's
06:40 - really the basis for a lot of
06:43 - a lot of things that people work on
06:46 - so uh for today we're gonna stick with
06:49 - some made-up data
06:51 - um and if you download the
06:55 - the code file the jupyter notebook code
06:58 - file
06:58 - in the first session uh link from the
07:01 - github page
07:03 - you will see that data as well
07:07 - so um in my jupyter notebook
07:10 - i have downloaded a bunch of packages
07:13 - here with
07:14 - aliases if you don't understand
07:17 - for right now we're just kind of going
07:19 - to zoom through this
07:20 - but essentially there are a bunch of
07:24 - different
07:25 - libraries in python that you can
07:27 - download to use
07:29 - functionality within them and then we're
07:31 - going to
07:32 - load them as these shorthand so that we
07:34 - can access them
07:35 - a little bit more simply cool
07:39 - so for today we're going to imagine
07:43 - that we went out on the street and we
07:46 - stopped
07:47 - i think there's maybe like nine numbers
07:49 - in here it's like seven eight nine
07:51 - um we stopped nine people on the street
07:54 - and we asked them for some information
07:56 - we asked them their height
07:57 - and we asked them their weight and then
07:59 - we collected that information
08:01 - um and we we wrote it down in these
08:04 - these two python lists so here i've got
08:07 - a list of heights and here i've got a
08:10 - list of weights
08:12 - and and they kind of match up
08:15 - as for individual people at each in this
08:19 - index so for example the first person
08:22 - was uh 175 centimeters
08:26 - tall and weighed uh 64
08:29 - kilograms i believe those are the great
08:32 - correct measurements i i'm so
08:36 - yeah i'm so used to like pounds and
08:40 - inches which i know
08:41 - are nonsensical measurements
08:46 - so i gotta get used to centimeters and
08:48 - kilograms but
08:49 - yes so we clicked that information
08:52 - and i just threw it into a data frame
08:55 - here so
08:56 - you can see what that looks like for
08:58 - each person you have a height
09:02 - um for cratique
09:06 - if you're seeing saying that uh there's
09:08 - an error
09:09 - and you're not able to see anything um
09:11 - it might be
09:12 - on github it's sometimes it uh make
09:16 - has an error loading the jupiter
09:18 - notebook in github
09:19 - so uh in order to see it you need to
09:22 - download it
09:23 - and then run the code yourself
09:28 - but hopefully everyone can see my screen
09:32 - um okay so let's take a look
09:36 - at a visualization of this like what's
09:38 - the relationship between
09:40 - height and weight here so i'm going to
09:42 - go ahead and graph this
09:45 - use the actually use s and s
09:48 - scatter
09:51 - um and i'm going to
09:55 - graph the height on the x-axis
10:00 - and the weight on the y-axis
10:05 - and then we will
10:11 - show that
10:16 - okay so we see that there's this sort of
10:18 - relationship and this
10:20 - probably isn't super surprising to any
10:23 - of us
10:24 - that at least among the nine people that
10:26 - we collected data for
10:28 - um generally people who
10:31 - are taller tend to weigh more so right
10:34 - like
10:36 - the person that is around 180
10:39 - centimeters
10:40 - tall is about 70 kilograms
10:44 - uh in weight and the person who's 160
10:48 - centimeters tall is about i don't know
10:50 - 58
10:52 - uh kilograms in weight and
10:56 - this is not true for every single height
10:58 - and weight so here we see a person who
11:00 - is
11:01 - around 173 centimeters tall who weighs
11:04 - more than someone who's taller
11:06 - at 175 centimeters tall so it's not
11:08 - always true that
11:10 - every person who is taller weighs more
11:12 - but
11:13 - in general we see this kind of trend
11:15 - that there's a relationship between
11:17 - these two things
11:18 - in the data that we collected
11:21 - so how do we try to
11:25 - use this to create a model
11:29 - um a predictive model or even a model to
11:32 - kind of analyze this relationship
11:34 - understand what is the relationship
11:35 - between height and weight here how can
11:37 - we
11:37 - numerically summarize that relationship
11:42 - and you might notice right that
11:46 - you could kind of if you were going to
11:48 - try to draw what this relationship
11:50 - looks like you could kind of draw draw a
11:53 - line through these
11:54 - plots and the question is
11:58 - how do we draw that line so
12:02 - we're going to take a step away from the
12:04 - data for a second
12:05 - and we're going to go back and do a
12:07 - little refresher
12:09 - on some geometry and then we'll come
12:13 - back
12:13 - and we'll see if we can get a line onto
12:15 - this plot
12:18 - so um
12:21 - so there's a good question in the chat
12:23 - right now of if i'm
12:25 - just starting um python three or three i
12:28 - guess the the
12:29 - direct question in the chat is i'm
12:30 - taking the python three course on code
12:31 - academy
12:32 - what will i have to do next to
12:34 - understand linear regression so somebody
12:36 - that's
12:36 - coming in watching this is our first
12:38 - session right we're trying to make this
12:39 - as approachable as possible if you don't
12:41 - need any kind of like stats background
12:42 - or linear regression background in order
12:44 - to follow this
12:45 - but some of the code that you're writing
12:47 - might look a little bit intimidating
12:48 - that's like oh i'm using
12:49 - sns.scatterplot i don't know what sns is
12:52 - i don't
12:52 - you know i don't know how to graph
12:54 - something up quickly i don't even
12:55 - necessarily even know how to like run
12:56 - jupiter notebook
12:58 - like what do you think the
12:59 - pre-requirements for
13:02 - one watching this series is and then two
13:04 - for like trying to code along
13:06 - maybe after uh after the series
13:09 - totally so i think hopefully the
13:12 - pre-requirements if you have a little
13:14 - bit of experience with python
13:16 - um you understand what a list is uh
13:20 - you understand how to use a loop uh
13:23 - write a function
13:24 - we're not really going to get go further
13:28 - than that in terms of
13:29 - python skills um i don't even
13:34 - yeah we'll we'll use a little bit of
13:35 - like looping and
13:37 - indexing in the later uh in some of the
13:40 - later
13:41 - sessions um but i do think so
13:44 - we're using in in these sessions a lot
13:47 - of python libraries that are
13:48 - specifically targeted for
13:50 - data science and statistics and
13:53 - those packages are primarily
13:56 - numpy pandas
14:00 - seaborn and matplotlib uh
14:04 - for plotting so
14:08 - those are i think hopefully if you
14:10 - understand some basic python
14:13 - you can learn those packages pretty
14:15 - quickly
14:16 - um and you can kind of get a sense for
14:19 - what i'm
14:19 - doing and follow along um in terms of
14:22 - like python 3
14:24 - and jupiter notebooks it it takes a
14:27 - little bit of setup
14:28 - we have some articles on codecademy
14:31 - about setting up jupiter notebook and
14:32 - some instructions
14:33 - even in i think the youtube link and the
14:35 - event link on
14:37 - what you should download the easiest way
14:39 - is just download anaconda
14:41 - which is like a package manager and it
14:44 - will
14:45 - it also simultaneously will download
14:47 - jupiter notebooks
14:49 - um to get this code
14:52 - you need a little bit of github
14:54 - knowledge um
14:55 - but really you can just download these
14:57 - files as a zip file if you don't know
14:59 - how to clone the repository
15:01 - um so yeah it takes a little bit of
15:04 - setup
15:04 - a little bit of github a little bit of
15:07 - jupiter notebooks
15:08 - a little bit of python and then
15:12 - some knowledge about this these packages
15:15 - which hopefully
15:16 - i can help mediate as we go
15:20 - yeah i think the all these like kind of
15:21 - setup questions are definitely things
15:23 - that we can cover
15:23 - in the office hours of kind of like
15:25 - showing sophie's whole setup
15:27 - uh how to download all of these packages
15:29 - how to install them that kind of stuff
15:30 - um but yeah i really would encourage you
15:33 - even if you're not following every line
15:34 - of code i think the important thing here
15:36 - are like the concepts of um linear
15:39 - regression
15:40 - where you don't need to exactly know how
15:43 - this scatter plot function is working
15:45 - um you should be focusing on okay we're
15:46 - comparing height versus weight and
15:48 - there's some relationship between the
15:49 - two and we're trying to figure out how
15:50 - to
15:51 - draw a line between uh between all these
15:53 - points
15:57 - all right uh oh sorry i also see a note
16:00 - about
16:01 - for who is the police coming uh yeah i
16:04 - now live in new york city again i live
16:06 - across the street from a hospital
16:08 - um and it's not super super loud all the
16:12 - time but
16:12 - if you hear sirens
16:15 - you you will hear sirens and i apologize
16:19 - um nothing to be done
16:22 - so okay so let's go back to kind of
16:25 - thinking about
16:26 - what it means to draw a line through
16:29 - these points
16:30 - so what i'm going to do is i'm actually
16:32 - going to
16:34 - write out some new lists um actually i'm
16:37 - gonna i'm gonna make them numpy arrays
16:39 - so that we can
16:40 - uh work with them a little bit more
16:42 - easily
16:43 - so let's create a a numpy array
16:47 - of some some numbers i don't know alex
16:50 - come up with some numbers
16:52 - uh i'm a big fan of the tv show lost so
16:56 - i'm just going to give you the lost
16:57 - numbers of
16:57 - uh 4 8 15 16
17:01 - 23 42.
17:05 - okay so i don't know if that that's
17:08 - gonna work with our scale
17:09 - but this makes me feel anxious
17:13 - um okay so
17:17 - we've got a set of numbers and then
17:20 - let's come up with another set of
17:22 - numbers
17:23 - that are related to these numbers in
17:25 - some way so
17:26 - um we could for example take all of the
17:30 - numbers in a
17:32 - and add
17:35 - 5 right
17:39 - now actually i'm just going to print out
17:42 - b so that you can see it for a second so
17:46 - this is one of those things where
17:48 - understanding these packages a little
17:49 - bit can be helpful
17:51 - so the reason i turned this list into a
17:53 - numpy array
17:54 - is that um python math works a little
17:57 - bit differently with
17:59 - numpy arrays which are also panda series
18:03 - also true for panda series which are
18:04 - like these columns of numbers and
18:07 - a data frame in that
18:10 - when i when i say a plus 5
18:13 - it's going to add 5 to
18:16 - all of these numbers so you'll see i've
18:19 - got 9
18:20 - 13 20 21
18:24 - 28 47 9 is just four
18:27 - plus five eight is just
18:31 - eight plus or sorry thirteen is just
18:33 - eight plus five
18:34 - um twenty is fifteen plus five and so on
18:38 - yeah so it's nice to be able to just be
18:40 - able to use that plus
18:41 - operator to say do it on every element
18:43 - of my array
18:44 - where normally i think just in like
18:46 - basic python arrays
18:48 - if you tried to do an array plus five it
18:50 - would say i have no idea what you mean
18:51 - by
18:52 - you would add five to this array i think
18:54 - it would actually add the five to the
18:55 - end of it
18:56 - yeah i think like if i do this
19:01 - oh you're right you're right yeah so can
19:03 - only concatenate
19:04 - lists not into the list so that was
19:06 - thinking that it was like trying to add
19:08 - on
19:08 - this thing to the end of the array but
19:10 - it didn't know how to add on a number
19:13 - cool okay so if this is my relationship
19:17 - and then i try to plot this
19:21 - relationship the same way i did before
19:24 - but instead i'm gonna just plot
19:26 - a and b right
19:29 - you'll notice that now all these points
19:32 - form a line right
19:36 - if i did some some other uh
19:39 - some other relate if i made some other
19:41 - relationship like if i squared all of
19:43 - these numbers for example
19:46 - right and then i wrote i plotted this
19:49 - you'll notice now all of a sudden they
19:51 - don't really follow the line they follow
19:52 - kind of like a curve um and so
19:56 - when we say fit a line what we really
19:59 - mean is
20:00 - can we figure out a relationship
20:03 - between these two sets of numbers
20:07 - that most closely represents the true
20:11 - relationship between the numbers because
20:13 - of course these points don't fall
20:14 - perfectly in a line
20:16 - but can we find a set of points or can
20:19 - we find a line
20:20 - a relationship between height and weight
20:22 - that
20:23 - really closely matches what we see in
20:26 - this form
20:28 - so if you why why can't we do that for
20:31 - this example where
20:32 - we we have this non-linear relationship
20:34 - when we're when we're squaring all of
20:36 - the values in a right we can still
20:38 - draw a line that tries to meet those
20:40 - points like at what point
20:42 - is it too far away from a linear
20:44 - relationship to
20:45 - use linear regression well
20:48 - it's a good question so actually as we
20:51 - get
20:52 - past so today we're going to talk about
20:53 - simple linear regression
20:55 - which is really just the process of
20:57 - trying to fit a line to a set of data
20:59 - where you've got
21:00 - uh two quantitative variables um
21:04 - well simple linear regression could even
21:06 - be with a categorical predictor but for
21:07 - today we're going to stick
21:09 - to two quantitative variables
21:12 - and try to fit a line showing some
21:15 - relationship between them
21:17 - um once we get into more
21:20 - complex methods within still within
21:23 - linear regression
21:25 - we're going to find that there are
21:26 - actually ways to model
21:28 - more complicated relationships so
21:30 - actually like
21:31 - we can use linear regression still
21:34 - to follow this curve um
21:38 - there's a method for doing that um we
21:41 - also could try to represent this curve
21:43 - with a line
21:45 - and then acknowledge you know
21:48 - this doesn't fit our data as well but
21:51 - it's a simpler model and so we're okay
21:53 - with it
21:54 - um so there's really no there's really
21:56 - no
21:58 - specific rule or like threshold at
22:02 - after which you wouldn't use linear
22:03 - regression and actually like
22:05 - using some other methods within still
22:07 - within the world of linear regression
22:09 - you can model some complex relationships
22:12 - beyond just the line um
22:15 - but yes cool
22:18 - i see um i see a question about
22:22 - mean squared error and try to minimize
22:25 - it
22:26 - yeah so we're gonna we're gonna get to
22:29 - that in just a second
22:30 - how do we define what the best line is
22:33 - cool
22:34 - all right so what i'm gonna do is i'm
22:37 - actually gonna
22:39 - so i'll demonstrate this really quickly
22:43 - something kind of cool is that if we use
22:46 - the
22:46 - plot function from matplotlib dot pi
22:49 - plot
22:50 - it'll actually plot a line it'll just
22:54 - like connect the points
22:55 - with a line which looks a little awkward
22:57 - for this curve
22:58 - but if we come back to this like a plus
23:01 - five situation down here
23:03 - um right because all the points were in
23:05 - a line if we connect them
23:07 - we're just going to get a line right off
23:09 - the bat
23:10 - so we're going to use that trick to try
23:12 - to add some lines
23:15 - to this plot cool
23:19 - so um i guess actually before we do that
23:23 - let's come back to
23:25 - so what kinds of relationships here
23:29 - create this linear linear relationship
23:31 - so we saw one example where we get this
23:33 - line
23:34 - we saw another example where we get a
23:37 - curved
23:37 - relationship what kind what formats
23:41 - of relationships create a line
23:44 - and if you've taken an algebra class
23:48 - you've probably seen this particular
23:51 - format which is
23:53 - m y equals mx plus b um i guess i
23:56 - shouldn't have used a and b
23:58 - i'll use x and y here
24:02 - so that we can we can use that same
24:05 - shorthand
24:06 - that should be uh y equals x plus five
24:09 - right
24:12 - okay so um
24:15 - okay this should all still work great
24:17 - though just renamed everything
24:20 - so i'm going to define two more numbers
24:23 - i'm going to call them
24:24 - m and d and i'm going to say m is equal
24:27 - to
24:29 - 2 and d is equal to 5
24:33 - and then i'm going to write this
24:36 - out as m times x so really all i'm doing
24:40 - here
24:41 - um and i'll i'll print y again is i'm
24:44 - saying okay
24:45 - my x's are these numbers
24:48 - then for each value so take four as an
24:51 - example
24:52 - i'm gonna do i'm gonna take two times
24:55 - four because i set
24:56 - m equal to two two times four plus
25:00 - 5 because i set b equal to 5. so 2 times
25:03 - 4 is 8
25:05 - plus 5 is 13 and that's how we got this
25:08 - number
25:09 - and then we're just going to do that for
25:10 - each one so like 8 times 2 plus 5
25:13 - is equal to 21 and then if we plot this
25:17 - it'll still be a line okay
25:21 - just so that we can see the changes a
25:24 - little bit more easily
25:25 - i'm gonna just set the limits
25:28 - for this plot so i'm going to set it
25:30 - from 0
25:32 - to 45
25:35 - 42 if those are our x's are between
25:40 - 0 and 42.
25:43 - give ourselves a little extra room and
25:45 - then um i'll set the y
25:47 - limits to say
25:52 - let me go yeah and the reason why we're
25:53 - doing this is because it's a little bit
25:54 - hard to see that like
25:56 - these points are actually changing
25:58 - because the the axes just change on us
25:59 - automatically if we're
26:01 - if we keep redrawing this graph over and
26:02 - over again so what we're doing here is
26:04 - we're
26:05 - setting the bounds of the of the axes
26:07 - that we're drawing so
26:09 - now if we if we redraw that um
26:12 - uh that's what that line looks like but
26:15 - now if we change our
26:17 - original equation up there our y is mx
26:19 - plus b if we change it to like
26:21 - uh so if we change it to like times uh
26:24 - times three rather than yeah make n3
26:28 - all right well times
26:31 - 1.5 because i didn't make this big
26:33 - enough yet
26:35 - hold on let's do it at 2 again let's up
26:38 - this to like 200
26:42 - okay so now we've got more space so now
26:44 - if i change this to
26:47 - 3 and i rerun it
26:50 - now we've got the line changed slightly
26:53 - we can see that
26:54 - it got a little steeper yeah which would
26:57 - have been happening
26:58 - even if we didn't set the even if we
27:00 - didn't do this dot excellent.ylm
27:02 - um these two lines of code but basically
27:05 - the graph would
27:06 - look exactly the same except the axes
27:08 - would be different
27:09 - um and so by setting our axes we can
27:11 - actually see the
27:12 - line looks different rather than the
27:13 - axes looking different
27:15 - yeah exactly um and so
27:19 - it turns out that this format if you
27:21 - write
27:23 - a relationship between some set of
27:26 - numbers
27:26 - x and some set of numbers y in this way
27:30 - where y is equal is always equal to
27:33 - a number times the x value
27:37 - plus a number or minus a number
27:41 - because you could have a negative here
27:42 - you could have a negative number in
27:44 - either spot
27:45 - um if if you stick to that kind of
27:49 - a relationship then you'll always get a
27:51 - line a straight line
27:53 - and before when we were just adding five
27:55 - we were still conforming to this because
27:56 - we were kind of just
27:57 - multiplying x by one right
28:02 - okay so let's let's just like see what
28:04 - this does a little bit let's
28:06 - investigate these numbers so um
28:10 - so the first number the number that
28:11 - multiplies
28:13 - each of these numbers our x's um
28:16 - this is called the slope of the line and
28:19 - it has some effect on how steep the line
28:22 - is
28:22 - so if we make this bigger so if we make
28:25 - this
28:26 - a 5 and rerun this we'll see that that
28:30 - line got
28:31 - even steeper it's going off the page now
28:34 - if we make it a one it's going to be
28:38 - a lot shallower right a lot less
28:42 - and then if we make it negative so let's
28:45 - say i make it like
28:46 - negative three it's gonna actually
28:51 - i guess i should have done
28:56 - sorry guys
29:03 - still getting cut off but you can see
29:05 - it's going um it went from sloping up to
29:08 - sloping down
29:10 - and then the more negative we make this
29:13 - they probably should have done a smaller
29:14 - number as an example but if we made it
29:16 - even more
29:17 - um it's going to start sloping down even
29:20 - more steeply
29:22 - so that's our slope that gives us one
29:26 - one piece of the line and then the other
29:29 - piece of it the other thing that we
29:32 - might want to control
29:33 - is how high or low like where the line
29:36 - is
29:37 - or uh vertically like if it's higher or
29:39 - lower
29:40 - so that's what this is going to control
29:43 - this b
29:44 - is going to control what's called the y
29:46 - um intercept
29:48 - and so that is technically the value
29:52 - um up on this like y-axis
29:57 - where this line would hit at x equals
30:01 - zero
30:02 - so it's basically the value of y when x
30:04 - is zero
30:05 - or what we would get if we plug in zero
30:09 - into this point so in this case right
30:13 - if we plug in zero to this formula right
30:15 - now
30:16 - i'll just write it out so we can see it
30:18 - right now we basically have
30:20 - two times x plus five if we plug in
30:23 - zero for x we really have two times zero
30:26 - plus five so y
30:27 - is equal to five and you can see if we
30:29 - kind of extend this
30:30 - line out it's hitting this y-axis around
30:34 - the number five and so that that
30:37 - controls where that line falls
30:39 - if we go ahead and increase
30:42 - b let's say we make b equal to 100
30:46 - all that's going to do is raise that
30:49 - line up
30:50 - so that if we draw this back to where
30:53 - uh the zero line is it's hitting at
30:57 - 100 um and then if we make it say like
31:01 - negative 50 it'll bring it down
31:04 - so that it hits the line
31:10 - right and so your original question here
31:11 - sophie was like what kind of operations
31:13 - make a linear relationship
31:16 - right yeah and and so the answer is kind
31:19 - of
31:20 - a dish uh like addition and
31:21 - multiplication and
31:23 - um based off of that also subtraction
31:26 - and division where
31:27 - b could be a negative number which makes
31:28 - it subtraction m
31:30 - could be a fraction which would make it
31:31 - division um
31:33 - so those are the operations that will
31:35 - make this linear relationship whereas we
31:38 - saw something like
31:39 - um using an exponent if we if we squared
31:42 - all of these numbers that made
31:44 - something that wasn't a linear
31:45 - relationship exactly
31:49 - so let's now take this back
31:52 - to our original problem and let's see if
31:55 - we can add a line
31:56 - to this to this plot um
32:00 - so one thing i'm gonna do really quickly
32:02 - so we saw
32:03 - right that we can control where the line
32:06 - hits
32:07 - um at height
32:11 - equals zero so we if we extend this
32:14 - out all the way to zero we can control
32:16 - where the hot where the line would hit
32:18 - this y-axis
32:20 - and then we can control how steep the
32:22 - line is
32:23 - so let's go ahead and
32:26 - let's extend the axes of this are we
32:30 - gonna eyeball the uh the line of best
32:32 - fit and try to
32:33 - uh this this would be a great thing to
32:34 - get from from chat of like
32:36 - come up with your y equals mx plus b
32:38 - that you think fits uh
32:40 - uh gets this data the best this one's
32:43 - going to be a little tricky because
32:45 - we have to like go back a ways
32:48 - and then we've got to go from let's go
32:51 - all the way from
32:53 - zero well we'll go from
33:08 - type
33:15 - [Music]
33:21 - all right
33:27 - sorry guys um okay so why is the weight
33:30 - so
33:31 - it is and then
33:35 - excellent we wanted to go from zero to
33:41 - okay there we go so here's here's our
33:44 - points
33:45 - now we've got to imagine so we're gonna
33:48 - write out
33:50 - an equation for a line
33:53 - we're gonna see if we can figure out if
33:55 - we were to draw this
33:56 - all the way back to here
34:00 - where would this hit at height
34:03 - equals zero and then if you want to
34:07 - throw your
34:07 - guesses in the chat go ahead
34:12 - and then um and then the second piece is
34:16 - going to be the slope
34:17 - that one's a little bit harder so one
34:20 - way we can think about slope
34:22 - is rise over run
34:25 - so rise is how far
34:29 - up we have to go for some distance
34:32 - forward okay so in this case it's like
34:35 - the ratio of if we if we're drawing this
34:38 - line how far
34:39 - up do we have to go to how far
34:42 - over do we have to go to stay on that
34:45 - line
34:46 - so if the ratio is equal to one it means
34:48 - that we could
34:49 - draw this line by going up one over one
34:51 - up one over one up one over one
34:53 - and the resulting points would create
34:55 - the line if the slope is
34:57 - two it means we have to go up two and
34:59 - over one
35:00 - up two and over one to get that line i
35:03 - think there's a picture of this
35:04 - in the linear regression
35:08 - course which i think is helpful
35:11 - right so this
35:14 - line has a slope of 2 because in order
35:18 - to
35:18 - create the line we have to go
35:22 - up 4 and over 2 or
35:25 - up 2 and over 1. no matter what it's
35:29 - going to be the same ratio but
35:31 - we have to go up twice as far as we have
35:33 - to go over
35:34 - in order to draw this line so the slope
35:37 - is
35:38 - in this example 4 over 2 um
35:42 - or 2. and
35:45 - right a negative slope would be you have
35:48 - to go
35:49 - down right or over two and down
35:52 - four um okay so
35:56 - did we get any guesses in chat for the
35:58 - y-intercept
36:00 - not not yet so yeah in chat try to guess
36:03 - what the y-intercept is and then also
36:04 - kind of what sophie was just talking
36:06 - about what do you think this slope
36:08 - of best fit might be um and yeah
36:12 - i see steve is saying oh maybe you can
36:14 - remove the x lam and y limb in order to
36:16 - have it automatically kind of size the
36:18 - uh
36:19 - size of the graph that's totally what
36:21 - we'll we'll do in the long run but we're
36:22 - just trying to
36:23 - be able to we're trying to basically
36:25 - eyeball the um
36:28 - the what's the x-intercept or the
36:30 - y-intercept
36:31 - um so we wanted to expand the graph to
36:34 - just kind of see where that uh
36:36 - that intercept might be but yeah i
36:38 - imagine soon we will go back
36:40 - to uh just having it automatically
36:41 - generate the um
36:43 - the dimensions of the axes
36:51 - yeah good questions are a good comment
36:53 - of all the points won't hit
36:54 - one common line in this case we can
36:56 - assume the two points lowest and biggest
36:58 - on the plot scale would give an idea of
37:00 - where the y concept would be
37:01 - right so kind of an implied question
37:03 - there is like how do we even
37:04 - go about this right because this isn't
37:06 - going to we know this isn't going to be
37:07 - a perfect line so like how can we even
37:09 - begin to
37:10 - guess what oh what an intercept would be
37:13 - um
37:15 - thoughts on that sophie yeah totally i
37:17 - think
37:18 - i think everybody that approaches this
37:20 - problem probably approaches it a little
37:22 - bit differently
37:23 - um so right so
37:26 - do you want to just try to like take the
37:28 - smallest and the largest
37:30 - and draw a line through that do you want
37:32 - to draw the line
37:33 - sort of in the middle of all of the
37:35 - points um
37:36 - and the most commonly used form of
37:40 - linear regression is ordinary squares
37:43 - regression which i'll define in a second
37:45 - but that's one
37:47 - definition of what makes the best line
37:49 - um you can actually run
37:51 - linear regressions with different
37:53 - definitions of what makes the best line
37:55 - and it
37:55 - changes the calculation slightly um
37:58 - but yeah okay see someone guess around
38:01 - negative 10 on the y-intercept
38:04 - so um i'm gonna i'm going to type that
38:07 - in because that's the first one i see
38:09 - i think the slope is a little tricky
38:11 - don't see any um
38:13 - the ideas on slope yet i'll come up with
38:15 - an idea
38:16 - while we're waiting too in case no one
38:18 - has one but um
38:21 - but go for it if anyone there's no shame
38:24 - in in giving it a try
38:26 - yeah let's see so i think if i were to
38:29 - guess this slope it looks like it's
38:30 - going from about
38:31 - so the rise is like you know 75
38:36 - to the the the point on the furthest
38:39 - right is like at 75 maybe in the point
38:41 - on the first left
38:42 - is that like 55 so that's the change of
38:44 - 20. and then the run
38:47 - is uh like 85
38:51 - to uh what is that 60 so
38:56 - uh what is that 35 no 25 so 20 over 25
39:00 - would be my guess
39:01 - if i i remember those numbers correctly
39:06 - so let's see i guess we could see what
39:08 - that line looks like i guess
39:10 - we try that sure
39:14 - all right okay what looks
39:18 - does the slope so 20 over 25 i just want
39:21 - to print out what
39:24 - that is so 20 20 over 25 is about
39:28 - point b um so let's try
39:34 - what do you guys think we definitely
39:37 - want to move this line down a little bit
39:39 - and it also looks like the line is a
39:41 - little bit too steep
39:42 - so um maybe let's
39:46 - try to kind of mod modulate the
39:48 - steepness first
39:50 - okay so we want it to be less steep
39:54 - so we want a smaller slope
39:59 - so what do you think point six we'll see
40:02 - what it looks like maybe
40:05 - okay closer i think maybe
40:08 - even a little less 0.4
40:13 - oh that's too small maybe
40:17 - or 0.5
40:21 - i think that looks about right for these
40:23 - points
40:25 - um so we want the intercept i think to
40:29 - be a little lower
40:31 - um so maybe let's try like
40:34 - negative 20.
40:38 - okay and i think one of the points of
40:40 - this is that this is like hard to do
40:42 - right or where it's like we're kind of
40:44 - struggling to eyeball this and so that's
40:46 - that's eventually going to be the point
40:47 - of linear regression is that we're going
40:48 - to be able to
40:50 - find a line that is that we
40:53 - compute in a way that's you know a
40:56 - little bit more
40:57 - scientific than us just kind of like
40:59 - randomly guessing um i
41:01 - i we had a comment that said the slope
41:02 - looks good but the wire is up to his way
41:04 - off that was my instincts too but i
41:06 - actually think that
41:07 - it was that the slope was a little bit
41:09 - more off and the y intercept just need
41:10 - to be suited down a little bit
41:12 - yeah so i think i've gotten to a place
41:15 - where i think this is pretty good
41:17 - we can change the excellent now maybe to
41:20 - like
41:22 - oops um you can change these limits now
41:26 - to more like 150 to 200
41:29 - um 40 to 80 maybe
41:37 - that we can zoom back in um
41:40 - more like 50 to 80 and
41:44 - it looks like this line
41:47 - pretty well describes that relationship
41:50 - between
41:51 - the sets of the set of points um and so
41:54 - now
41:55 - the next question is how do we
41:57 - operationalize this
41:59 - which means like how do we define this
42:02 - so that's repeatable
42:03 - and then how do we actually calculate
42:07 - what these numbers should be this slope
42:09 - and this
42:10 - intercept right and i think a question
42:13 - is like
42:13 - is this actually good right because it's
42:15 - we could like kind of tweak forever
42:17 - where i could say oh i want this to be
42:18 - raised a little bit or
42:19 - the slope to be changed a little bit so
42:21 - like how do we know is this the best
42:23 - that we
42:23 - can possibly do how do we even define
42:25 - what is the best that we possibly can do
42:27 - um just kind of the question of how good
42:28 - is this yeah totally
42:31 - 100 agree okay so
42:35 - um i actually i think alex maybe you
42:37 - wrote this
42:38 - uh at one point this little applet
42:42 - but i'm going to go to the code cabinet
42:45 - i think yeah this one this one was not
42:46 - me but this one's good
42:47 - i like this one yeah um so
42:51 - it turns out that what we often do um or
42:54 - what we
42:54 - we do if we're running ordinary least
42:56 - squares regression
42:58 - is we set up this equation we say okay
43:02 - for each point right
43:05 - each height and each weight um so we can
43:08 - think of the y
43:09 - here as as weight and the x here is
43:11 - height for example
43:13 - um we want to define this relationship
43:15 - between the points we want to say
43:17 - somebody's weight is equal to some
43:20 - number times their height
43:21 - plus some number plus some error right
43:24 - so
43:26 - we're saying basically we'll make
43:28 - predictions along the line but we know
43:30 - that
43:31 - each individual weight is going to be
43:35 - not directly on that line so going back
43:38 - to
43:38 - this picture right none of these weights
43:41 - are exactly on the line and we can
43:43 - measure how far away they are from the
43:46 - line by drawing kind of the vertical
43:48 - line
43:48 - between them and so ordinarily squares
43:52 - regression says
43:54 - okay let's try to minimize
43:57 - these distances between the point and
43:59 - the line
44:00 - now actually finding the minimum of the
44:03 - absolute
44:04 - distances between the point and line is
44:07 - a more complicated math problem
44:09 - so because of like
44:13 - math being hard we actually do instead
44:16 - of the absolute differences
44:18 - so just taking like okay this one is um
44:21 - two units away or five units away or
44:23 - whatever and this one is
44:25 - one unit away and adding those up we
44:28 - actually just
44:29 - square these different these distances
44:31 - and add them up
44:33 - the squaring automatically makes them
44:36 - all positive right because if you square
44:38 - a number
44:39 - um a square a negative number is
44:41 - positive so by squaring it
44:43 - it means that a point below the line
44:47 - is just as that's far below the line is
44:50 - just as bad as having a point that's far
44:52 - above the line
44:53 - doesn't matter just matters how far it
44:55 - is away
44:57 - and um and so let's try to minimize
45:01 - those square distances for all of the
45:04 - points
45:05 - so we take square distance for this
45:07 - point square distance for this point
45:09 - square distance for this point add them
45:11 - all up and let's find the line
45:14 - that minimizes all of those distances
45:18 - this may be a silly question but why do
45:20 - we care about the
45:21 - um the vertical and distance and not
45:24 - like
45:25 - perpendicular to the line or horizontal
45:28 - to the line
45:29 - so actually there's like linear
45:32 - regression models that
45:33 - you can fit that minimize the horizontal
45:36 - distance
45:38 - so it actually depends on the research
45:41 - problem
45:42 - what you're trying to do um if you're
45:46 - trying to make predictions
45:48 - of the y variable so in this case like
45:51 - if your goal is to
45:54 - make weight predictions
45:57 - using height information and for example
45:59 - doctors might do this
46:01 - when you're when you're young they want
46:03 - to like
46:04 - estimate i don't know predict what
46:06 - height you're going to be or what weight
46:08 - you're going to be
46:09 - or say are you overweight or underweight
46:11 - compared to normal and so they want to
46:13 - know what this
46:14 - line looks like for on average so that
46:17 - you know um if your goal is to predict
46:21 - weight
46:21 - then minimizing these square distances
46:24 - is really like
46:25 - minimizing um
46:28 - minimizing how off you'll be in your
46:31 - prediction
46:33 - but you know if if you minimize the
46:36 - horizontal distance
46:37 - you're minimizing um how off
46:41 - you are with respect to like how much
46:44 - a difference in height um
46:48 - i don't know i honestly because because
46:50 - it's like if if we
46:52 - if we just change the axis if we just
46:54 - put
46:55 - height on the y-axis and weight on the
46:57 - x-axis
46:58 - it's the same data right
47:01 - but now the the line is going to be
47:04 - slightly different
47:05 - and the thing that we're minimizing is
47:08 - going to be
47:09 - the difference in you know in that other
47:12 - variable
47:12 - um is there is there one where you do
47:16 - perpendicular distance where you like
47:18 - just see how far away it is from the
47:19 - line oh you mean like like
47:24 - the crows distance or whatever like yeah
47:27 - exactly
47:29 - uh yeah i'm sure there is i
47:32 - have never seen it done yeah
47:35 - but that doesn't mean anything
47:41 - um so yeah cool cool
47:44 - cool so all right so i like this
47:48 - uh this little applet so in this applet
47:51 - you can
47:52 - randomize a set of points you can say
47:53 - okay let's like
47:55 - make 10 points on this graph and then
47:58 - let's
47:59 - move m and b around and see if we can
48:03 - minimize those distant those
48:06 - total squared errors um and so we can
48:09 - see this little
48:10 - bar on the side that kind of grows as
48:13 - the line gets further away
48:15 - and shrinks as it gets closer and we
48:17 - notice that there's some kind of like
48:18 - minimum here right like as i move the
48:21 - line
48:22 - as i move b up the squared error is
48:25 - getting smaller and smaller and then at
48:26 - some point
48:27 - it hits the smallest point and then
48:28 - starts getting bigger again
48:31 - so it's like we want to find this
48:34 - minimum here
48:35 - and if you have some time i very much
48:38 - recommend playing with this a little bit
48:39 - to get yourself kind of used to this
48:41 - idea of what you're
48:43 - what you're calculating when you are
48:45 - fitting these models
48:47 - cool so um
48:51 - there are a lot of different ways to
48:53 - actually calculate
48:55 - m and b for this problem for a simple
48:58 - linear regression problem where you have
49:00 - two variables you can use some calculus
49:02 - and you can actually just like get a
49:04 - formula basically for
49:06 - what those numbers are going to be um
49:09 - you need a little bit of matrix algebra
49:13 - for that and i think in session
49:15 - four maybe three or four um we'll
49:18 - actually walk through a little bit
49:19 - of that math because i think it's
49:22 - helpful in order to understand some of
49:24 - the assumptions that you're making
49:26 - and some of the errors that you can get
49:27 - it's also helpful if you want to use a
49:30 - different software
49:31 - besides python or a different package to
49:34 - fit
49:34 - a model um but i'm going to actually
49:36 - just go ahead
49:38 - and use a um
49:41 - a function within the stats models
49:44 - package
49:45 - and just quickly demonstrate to you all
49:47 - how we could fit this
49:49 - so i
49:53 - i've set this on live streams before i
49:56 - come from
49:57 - um a staff background and i learned how
50:00 - to program an
50:01 - r all of this in r first so i
50:04 - really like stats models interface more
50:08 - than
50:08 - scikit learn which is another package
50:11 - that you can use to fit these models
50:14 - there are actually many in python i
50:16 - think a lot of data scientists prefer
50:18 - scikit-learn
50:20 - because it has some other functionality
50:22 - that's really useful that
50:24 - stats models does not have um but for
50:27 - right now
50:27 - i'm going to just use stats models
50:30 - because it gives us a lot of useful
50:31 - information
50:33 - and this is the
50:39 - and this is the code so i've already
50:43 - downloaded statsmodels.api
50:46 - as sm so this is my shorthand
50:49 - for the statsmodels.api
50:53 - library and then i'm saying i want to do
50:57 - ordinary least squares
50:59 - regression which like we said is just
51:01 - minimizing those squared distances
51:04 - and i'm using the from formula function
51:08 - which allows me to kind of input a
51:11 - formula
51:12 - that i want to use to fit the model and
51:14 - so in this case
51:15 - i saved this data set as something
51:18 - called data
51:20 - it has two attributes right two columns
51:23 - white sorry weight and height um and i'm
51:26 - going to go ahead and fit the model
51:28 - to predict weight using height so this
51:32 - tilde tells me that weight is my outcome
51:36 - variable that's the first one
51:38 - height is what i'm using to predict it
51:40 - um so that's like my
51:41 - x variable here the data set i've called
51:45 - it data which is redundant
51:46 - um but anyway it's called data
51:50 - so this is telling python where to find
51:53 - these variables
51:54 - and then i'm just gonna go ahead and fit
51:56 - the model instead of doing that in two
51:58 - steps
51:58 - um and then i'm gonna go ahead and print
52:00 - a model summary so that we can all
52:02 - inspect it together so here's what this
52:06 - looks like
52:08 - um you'll notice so we do get this
52:11 - warning
52:12 - um the warning says any
52:15 - like it says something about uh kurtosis
52:19 - test is only valid for
52:20 - n greater than or equal to 20 continuing
52:24 - anyway n equals nine so basically it's
52:26 - giving me some sort of error
52:28 - around my sample size is not big enough
52:31 - i only have nine data points
52:33 - it only wants to run this test if
52:36 - there's
52:36 - uh 20 or more data points fine
52:40 - we're doing this as
52:44 - as an example we also see we see some
52:46 - other warnings in here
52:48 - um and we'll talk a little bit more
52:50 - about what these warnings mean
52:51 - basically those warnings are happening
52:53 - for a couple of reasons
52:55 - one because um all of these points
52:58 - are so far away on the x-axis from
53:01 - zero so we saw how much um
53:05 - that that intercept could change
53:08 - uh based on exactly what these points
53:12 - look like
53:13 - because it's so far away and then partly
53:15 - because we have such a small sample size
53:18 - and those are things that we can fix but
53:21 - really the important part here
53:23 - is we see these two numbers the
53:26 - intercept
53:27 - and so these are under the coefficient
53:31 - section so we see that the intercept
53:33 - which is the y-intercept or that b
53:35 - value is about negative 21.6 or
53:40 - 0.7 and then we see that the slope
53:43 - and it it tells us that it's the
53:45 - coefficient on height
53:46 - the reason it does that is that we can
53:48 - add more variables
53:50 - this later um but tells us that that
53:53 - slope
53:54 - is 0.5 um
53:58 - cool so i see a question about using
54:00 - libraries or hard
54:01 - coding simple linear regression um
54:05 - so i'm going to show you within the
54:07 - course of this
54:08 - these sessions i'll show you how to
54:11 - implement this
54:12 - in stats models i'll also show you how
54:14 - to implement it in psychic learn
54:16 - um and i'll also show you how to
54:18 - calculate it by hand
54:19 - using a formula um so we're going to do
54:22 - all three in the course
54:24 - of this this session i think the
54:26 - important thing
54:27 - is not so much understanding um
54:31 - how to do it in a particular package but
54:33 - understanding what you're doing so that
54:35 - you could do it in
54:36 - any python package or package or
54:39 - um you know some or stata
54:42 - or something else entirely sophie so if
54:46 - we go back to
54:47 - our to when we were eyeballing it and
54:49 - just
54:50 - drawing the line ourselves and change
54:52 - our
54:53 - coefficients to that negative 21 and the
54:56 - zero point
54:57 - uh or yeah 0.5 whatever so we were
55:00 - pretty close yeah
55:02 - maybe sophie knew the answer is all
55:04 - along i bet she did
55:08 - but that's okay
55:12 - yeah this basically this line is the
55:15 - model that uh whatever library we were
55:18 - using
55:19 - would generate yes
55:22 - exactly so we're getting a lot of
55:25 - information
55:26 - this information is super useful if
55:28 - you're trying to do something more
55:29 - complicated
55:31 - really all we cared about and i can just
55:33 - print them out
55:35 - is the parameters of the model which
55:38 - were
55:38 - the intercept and the slope and we can
55:42 - see
55:42 - that those look pretty good they fit
55:46 - our model pretty well cool
55:49 - um so cool we have about five minutes
55:53 - left
55:54 - one last thing that i was hoping to kind
55:55 - of get through in this
55:57 - session is the assumptions of linear
55:59 - regression
56:00 - i'm gonna compress it because we don't
56:02 - have a ton of time um
56:03 - but one thing i think it's one reason i
56:06 - think it's important to
56:08 - in any discussion of simple linear
56:10 - regression or linear regression
56:12 - or any model whatsoever to think about
56:15 - the assumptions
56:16 - is that a lot of times
56:19 - a particular method like ordinary least
56:23 - squares regression
56:24 - might not be appropriate for the kind of
56:26 - data you have
56:27 - or the kind of question you're asking um
56:30 - and so
56:30 - it's important to know when it's
56:32 - appropriate and when it's not
56:34 - um and so that's where
56:37 - the assumptions of linear regression
56:38 - come in um
56:40 - also if you're interested in applying
56:42 - for like data analysts and data
56:43 - scientist positions this is the kind of
56:45 - thing that someone might ask you
56:47 - in an interview what are like i know i
56:50 - i've heard that we ask this in our data
56:53 - science interviews um
56:55 - what are the assumptions of linear
56:56 - regression so worth
56:58 - worth knowing what they are um so the
57:01 - first
57:01 - two are pretty simple first one is that
57:04 - there is a linear relationship
57:06 - between your variables if there's not
57:09 - like i said
57:10 - you can use more complicated methods to
57:12 - model it but if you're
57:15 - modeling using a line then
57:19 - you have to believe that there
57:22 - is a linear relationship that you're
57:24 - trying to model
57:26 - the second one is that each of the
57:29 - observations are independent
57:31 - um which really just means that one row
57:34 - in this data set can't affect another
57:36 - row
57:37 - so let's say that these were actually
57:39 - the heights and weights
57:40 - of all the people all the people in a
57:44 - single family
57:45 - um then you know a mother's
57:49 - height and weight might be more similar
57:51 - to a son's height and weight or they
57:53 - might have
57:54 - some like familial thing where they're
57:56 - really short
57:57 - and or really tall and really skinny or
58:00 - whatever it is
58:01 - that isn't really true of the general
58:03 - population so if you're trying to use
58:04 - this model to describe
58:07 - a relationship in the larger population
58:10 - you shouldn't use um
58:14 - you shouldn't use that model um so we we
58:17 - assume that they're independent
58:19 - and then um and then there's two more
58:21 - assumptions related to
58:23 - uh related to the actual data so
58:26 - one is that the outcome variable
58:30 - is normally distributed and a lot of
58:32 - times we check that
58:34 - by looking to see if the residuals are
58:36 - normally distributed
58:38 - um and so the residuals are those
58:41 - error terms they're basically like how
58:43 - far away
58:44 - is each point from the line um so we can
58:48 - actually calculate those so
58:50 - really quickly um
58:54 - i just want to demonstrate for example
59:00 - we can get the predicted heights
59:04 - for each person by applying this model
59:08 - so in this case if we take or sorry the
59:11 - predicted weights
59:13 - if we take the predicted heights of each
59:16 - person
59:17 - we multiply by
59:20 - sorry we multiply by the slope
59:24 - and then we add this number
59:28 - and adding a negative number is just
59:30 - like subtracting
59:32 - then we will get
59:36 - the predictions for
59:40 - the weights of each person in our
59:43 - data set and we also have the true
59:46 - weights
59:47 - of each person in our data set
59:54 - right so the first we would predict the
59:55 - first person is
59:57 - 66 uh kilograms but they're actually 64.
60:01 - the second person is 59 kilograms
60:04 - they're actually 58.
60:05 - um and we can see that here right in
60:08 - this line
60:10 - the prediction is where they would fall
60:13 - on the line
60:15 - and then the true value is below it so
60:17 - we saw
60:18 - the first one the prediction is a little
60:20 - high
60:21 - right and i think i don't know if these
60:25 - are
60:25 - these points are in order actually but
60:28 - um
60:28 - but we can see right the prediction here
60:32 - is a little high so
60:35 - we can take all of these predicted
60:37 - weights
60:38 - and true weights and then we can
60:41 - subtract them
60:43 - um to get the residuals
60:50 - those just those um differences
60:54 - and we can plot a histogram of them to
60:56 - check for
60:57 - normality um normality just means
61:01 - that they roughly follow the histogram
61:03 - kind of follows this
61:05 - bell curve like shape um
61:08 - actually i will i'll jump over to the
61:10 - example code to
61:11 - show you at this point
61:15 - so basically this is saying that um
61:18 - some of those residuals are or some of
61:20 - those points are under the predicted
61:22 - values some are over the predicted value
61:24 - um a lot of them are you know closer to
61:28 - the predicted value than not
61:29 - um and follow that bell curve exactly
61:33 - and it's a little hard to see here
61:34 - because we only had nine points but if
61:36 - we estimate
61:37 - the the line that should follow this
61:39 - histogram we see it's like roughly
61:42 - a normal distribution um
61:46 - and like alex said most of them are
61:49 - around
61:49 - zero some are a little bit below the
61:52 - line some are a little above the line
61:54 - and then the last is an assumption of
61:56 - linear regression how do you
62:01 - how do you what do you do
62:05 - sorry sorry i think my internet broke up
62:06 - there for a second um if this is an
62:08 - assumption for
62:09 - linear regression like how do you know
62:11 - this without doing all of this
62:13 - um well so you can plot you can start by
62:16 - plotting a histogram of the outcome
62:18 - variable
62:19 - if the outcome variable is bimodal or
62:23 - something or super skewed
62:25 - then um you probably want to apply some
62:27 - transformations
62:29 - um you could also fit the model and and
62:32 - look at this directly um but
62:35 - yeah it's something that you can you can
62:37 - deal with there are some methods for
62:40 - managing it if it's not met um but it's
62:43 - actually a way that you can
62:44 - improve your model if you fit a model
62:46 - and then you check this assumption
62:48 - and you see something super skewed that
62:51 - means like you have some avenues some
62:53 - indication that you might want to fit a
62:55 - different model um you
62:57 - might want to do something a little
62:58 - differently um
63:00 - and then the last one it's called um
63:05 - it has a fancy name uh homoscedasticity
63:09 - uh which really just means that the
63:12 - variance of uh of the points
63:17 - is the same for all values of the
63:22 - uh predictor variable um
63:26 - i feel like i didn't get so basically
63:31 - what you're looking for is not this um
63:34 - so in this example
63:35 - we see that the points are really
63:37 - tightly
63:38 - wound around like the um there's not a
63:41 - lot of variance in the points like
63:42 - down here for small values of height
63:45 - this is totally made of data and not
63:47 - realistic at all we even have a person
63:48 - with negative weight which is not
63:50 - possible
63:50 - but just a picture right like so these
63:53 - points are really tightly
63:54 - bound together and then these points are
63:58 - a lot more uh have a lot more variation
64:02 - and this kind of a picture would violate
64:05 - the homoscedasticity assumption
64:07 - it would say like what you want is for
64:09 - the points to be kind of like
64:11 - have equal variance around the line for
64:14 - all
64:16 - of the predictor and you can check that
64:18 - by
64:19 - making a plot um of the
64:22 - residuals against the fitted values the
64:24 - fitted values being those predicted
64:26 - values
64:27 - and then taking a look at um
64:31 - at what that that plot looks like and if
64:33 - you see any weird patterns in it
64:35 - that's an indication that that the
64:38 - homoscedasticity assumption
64:40 - might be violated so these are also
64:43 - i'm guessing you cover all of this in
64:45 - this first lesson in the
64:48 - first lesson in the course exactly yeah
64:51 - so this is um
64:53 - right we're about five minutes over so
64:54 - i'm gonna end it there um
64:56 - we can definitely answer more questions
64:58 - about the assumptions in
64:59 - office hours um and then also highly
65:02 - recommend just taking
65:03 - this first lesson um in the course on
65:06 - codecademy
65:07 - it's um the introduction to linear
65:10 - regression lesson
65:12 - it's the first the first lesson in the
65:14 - linear regression in python course
65:16 - um and starts to introduce some of the
65:18 - topics for next week
65:21 - for office hours again i'm putting the
65:22 - link in the chat there's also a link in
65:25 - the um
65:26 - youtube description we're doing that uh
65:28 - on thursday at the exact same time
65:30 - right now we've only booked it out for a
65:32 - half hour depending on how many people
65:33 - show up there
65:34 - um but yeah again it's kind of a zoom
65:37 - call or
65:37 - it's a zoo zoom webinar technically um
65:40 - so you can join the call and then we can
65:42 - like bring you on to
65:43 - ask um to ask questions to us um
65:47 - we also got questions about setup i saw
65:49 - sophie
65:50 - in the in the github page um you talk a
65:53 - little bit about
65:55 - um let me find that uh you talk a little
65:58 - bit about
65:58 - installing anaconda um so if you want to
66:01 - go through this code definitely make
66:02 - sure to follow those instructions
66:04 - and um maybe we can put together
66:06 - something
66:07 - uh to help out um with
66:10 - all that uh installation stuff there's
66:12 - also i think
66:14 - um i think another curriculum developer
66:18 - just recently updated a bunch of our
66:20 - content on setting up jupiter notebooks
66:22 - so
66:22 - i can try to find those articles but
66:25 - also
66:26 - if you go back to the first session of
66:29 - the
66:30 - master statistics live stream which we
66:32 - have on our youtube page
66:34 - um in the first like 10 15 minutes or so
66:37 - we go through the full process of like
66:39 - opening a jupiter notebook on your
66:41 - computer
66:41 - from anaconda and like running
66:45 - code for the first time so if you want
66:47 - to
66:48 - check that out um that also exists
66:51 - cool all right well i hope some people
66:54 - show up on thursday otherwise will be
66:56 - pretty lonely
66:58 - yeah but uh but yeah that'll be fun
67:01 - thank you for um
67:02 - yeah everyone in chat for asking
67:03 - questions a lot of new faces i haven't
67:04 - seen here before a lot of good questions
67:06 - um so yeah we'll um and then we'll be
67:09 - back next week with another um another
67:11 - session
67:11 - awesome cool all right thanks sophie

Cleaned transcript:

okay but we don't know for sure but yeah let's go ahead and get started okay let us know in the youtube chat if you can hear us and see us um we'll be getting started in just a moment i feel like cool we look good together all right well welcome to our first session of codecademy live going through the recent launch recently launched linear regression course um we're gonna have eight sessions over the next nine weeks so we're gonna skip the week week of fourth of july um that we're going to have every tuesday at 11 a.m we're going to go through some content together um it'll be alex and i for today might have some different uh c devs so curriculum developers that codecad me jumping on these calls um but we're really really excited and then on thursdays for at least the next three weeks we're gonna run kind of an open office half hour also at 11 a.m so if anybody has questions or things that they weren't able to ask in this in this session um for anyone that was watching it later and things came up or anyone really if it's a small group who has questions about any of our data science content we'll welcome questions we really just want um to get to know everyone who is joining us and and we want to spend some time with you and learn what you're interested in and what you want to get out of this almost like a regular classroom so um it's good to see some people saying hello in the chat wow we've got someone from vietnam um that's awesome we moved the uh the time of this event from some of our previous live streams hoping that maybe we could get uh more people from around the world uh in a reasonable time zone for for joining us so i hope that that was helpful yeah so to join that uh that office hour on thursday um this is our first time kind of really trying this so we hope that the tech works and it's all set up properly but there's a link in the youtube description to kind of an event page where you can register basically it's just you're going to be joining a zoom webinar um i also just posted it in the chat um so if you're interested in that um hopefully it all goes smoothly um this first week and we can we'll see uh see some of you in kind of like a private zoom call where you can ask us some questions yeah super awesome i guess since this is our first stream um i'll quickly introduce myself and alex can introduce himself as well um so i'm sophie i'm a curriculum developer at codecademy i've been here now almost a whole year which is crazy and i work on data science content primarily um and i was pretty involved in building the linear regression course um working with content contributors and writing some of the content myself so super excited to go through it yeah and i'm alex i'm also a curriculum developer um i used to be on the data science team with sophie but i've gone over and started doing more of the computer science content so that those are courses that are kind of like similar to traditional college courses um so yeah if you check out the computer science past we're building out a ton of new stuff in that path awesome cool wow i see some people from india trinidad and tobago atlanta belgium super awesome um so we are keeping an eye on the youtube chat throughout this i think we're streaming on a couple other platforms but um come over to youtube if you want to chat with us live feel free to ask questions as we go along um i'm going to kind of lead and alex is going to keep an eye on the chat um and so he can let me know stop me if anyone has questions cool cool i'm going to go ahead and share my screen also while i'm sharing my screen see someone asked if there are any free courses on code academy alex you want to take that really quick yeah lots of free courses on code academy if you go to the catalog kind of like the full catalog you should see um so there's a bunch of paths which are all pro only but if you scroll down towards the bottom you should see courses and uh some of those are marked as pro only but a lot of them are totally free so um yeah lots of free courses in our in our catalog yes and i believe is i believe this course is also free yeah i think so what is what is this one actually called sophie or if you want to show off radio regression in python yeah so so so if you're your fourth power yeah your fourth tab there looks like it's the um oh yeah on this page well yeah let's see um if you go to the catalog and look for a linear regression in python um let's see linear regression in python yep that is a free course so anyone may join it and and take this content on your own as well cool so we're going to get started uh so before i code anything um i just want to remind everyone and this is in the youtube description as well but we have a um a github page that has all of this uh all of this code that i'm gonna be writing um and it has all of the code for the next three weeks after after this so you can get a head start and take a look it's meant to complement this content um we're gonna be using jupiter notebooks which requires a little bit of prep work and a download but if you didn't have time to do it before today you can definitely do that before the next session um so a little bit of motivation so linear regression at its core is a modeling technique um it's considered part of machine learning um and it's really like the the base model that a lot of more complex models are built upon so it's really if you're interested in machine learning you're interested in data science you're interested in statistics or you even just want to be able to like read other uh analyses read read papers or um you know find out about medical research or whatever it's a really good thing to understand because it's really the basis for a lot of a lot of things that people work on so uh for today we're gonna stick with some madeup data um and if you download the the code file the jupyter notebook code file in the first session uh link from the github page you will see that data as well so um in my jupyter notebook i have downloaded a bunch of packages here with aliases if you don't understand for right now we're just kind of going to zoom through this but essentially there are a bunch of different libraries in python that you can download to use functionality within them and then we're going to load them as these shorthand so that we can access them a little bit more simply cool so for today we're going to imagine that we went out on the street and we stopped i think there's maybe like nine numbers in here it's like seven eight nine um we stopped nine people on the street and we asked them for some information we asked them their height and we asked them their weight and then we collected that information um and we we wrote it down in these these two python lists so here i've got a list of heights and here i've got a list of weights and and they kind of match up as for individual people at each in this index so for example the first person was uh 175 centimeters tall and weighed uh 64 kilograms i believe those are the great correct measurements i i'm so yeah i'm so used to like pounds and inches which i know are nonsensical measurements so i gotta get used to centimeters and kilograms but yes so we clicked that information and i just threw it into a data frame here so you can see what that looks like for each person you have a height um for cratique if you're seeing saying that uh there's an error and you're not able to see anything um it might be on github it's sometimes it uh make has an error loading the jupiter notebook in github so uh in order to see it you need to download it and then run the code yourself but hopefully everyone can see my screen um okay so let's take a look at a visualization of this like what's the relationship between height and weight here so i'm going to go ahead and graph this use the actually use s and s scatter um and i'm going to graph the height on the xaxis and the weight on the yaxis and then we will show that okay so we see that there's this sort of relationship and this probably isn't super surprising to any of us that at least among the nine people that we collected data for um generally people who are taller tend to weigh more so right like the person that is around 180 centimeters tall is about 70 kilograms uh in weight and the person who's 160 centimeters tall is about i don't know 58 uh kilograms in weight and this is not true for every single height and weight so here we see a person who is around 173 centimeters tall who weighs more than someone who's taller at 175 centimeters tall so it's not always true that every person who is taller weighs more but in general we see this kind of trend that there's a relationship between these two things in the data that we collected so how do we try to use this to create a model um a predictive model or even a model to kind of analyze this relationship understand what is the relationship between height and weight here how can we numerically summarize that relationship and you might notice right that you could kind of if you were going to try to draw what this relationship looks like you could kind of draw draw a line through these plots and the question is how do we draw that line so we're going to take a step away from the data for a second and we're going to go back and do a little refresher on some geometry and then we'll come back and we'll see if we can get a line onto this plot so um so there's a good question in the chat right now of if i'm just starting um python three or three i guess the the direct question in the chat is i'm taking the python three course on code academy what will i have to do next to understand linear regression so somebody that's coming in watching this is our first session right we're trying to make this as approachable as possible if you don't need any kind of like stats background or linear regression background in order to follow this but some of the code that you're writing might look a little bit intimidating that's like oh i'm using sns.scatterplot i don't know what sns is i don't you know i don't know how to graph something up quickly i don't even necessarily even know how to like run jupiter notebook like what do you think the prerequirements for one watching this series is and then two for like trying to code along maybe after uh after the series totally so i think hopefully the prerequirements if you have a little bit of experience with python um you understand what a list is uh you understand how to use a loop uh write a function we're not really going to get go further than that in terms of python skills um i don't even yeah we'll we'll use a little bit of like looping and indexing in the later uh in some of the later sessions um but i do think so we're using in in these sessions a lot of python libraries that are specifically targeted for data science and statistics and those packages are primarily numpy pandas seaborn and matplotlib uh for plotting so those are i think hopefully if you understand some basic python you can learn those packages pretty quickly um and you can kind of get a sense for what i'm doing and follow along um in terms of like python 3 and jupiter notebooks it it takes a little bit of setup we have some articles on codecademy about setting up jupiter notebook and some instructions even in i think the youtube link and the event link on what you should download the easiest way is just download anaconda which is like a package manager and it will it also simultaneously will download jupiter notebooks um to get this code you need a little bit of github knowledge um but really you can just download these files as a zip file if you don't know how to clone the repository um so yeah it takes a little bit of setup a little bit of github a little bit of jupiter notebooks a little bit of python and then some knowledge about this these packages which hopefully i can help mediate as we go yeah i think the all these like kind of setup questions are definitely things that we can cover in the office hours of kind of like showing sophie's whole setup uh how to download all of these packages how to install them that kind of stuff um but yeah i really would encourage you even if you're not following every line of code i think the important thing here are like the concepts of um linear regression where you don't need to exactly know how this scatter plot function is working um you should be focusing on okay we're comparing height versus weight and there's some relationship between the two and we're trying to figure out how to draw a line between uh between all these points all right uh oh sorry i also see a note about for who is the police coming uh yeah i now live in new york city again i live across the street from a hospital um and it's not super super loud all the time but if you hear sirens you you will hear sirens and i apologize um nothing to be done so okay so let's go back to kind of thinking about what it means to draw a line through these points so what i'm going to do is i'm actually going to write out some new lists um actually i'm gonna i'm gonna make them numpy arrays so that we can uh work with them a little bit more easily so let's create a a numpy array of some some numbers i don't know alex come up with some numbers uh i'm a big fan of the tv show lost so i'm just going to give you the lost numbers of uh 4 8 15 16 23 42. okay so i don't know if that that's gonna work with our scale but this makes me feel anxious um okay so we've got a set of numbers and then let's come up with another set of numbers that are related to these numbers in some way so um we could for example take all of the numbers in a and add 5 right now actually i'm just going to print out b so that you can see it for a second so this is one of those things where understanding these packages a little bit can be helpful so the reason i turned this list into a numpy array is that um python math works a little bit differently with numpy arrays which are also panda series also true for panda series which are like these columns of numbers and a data frame in that when i when i say a plus 5 it's going to add 5 to all of these numbers so you'll see i've got 9 13 20 21 28 47 9 is just four plus five eight is just eight plus or sorry thirteen is just eight plus five um twenty is fifteen plus five and so on yeah so it's nice to be able to just be able to use that plus operator to say do it on every element of my array where normally i think just in like basic python arrays if you tried to do an array plus five it would say i have no idea what you mean by you would add five to this array i think it would actually add the five to the end of it yeah i think like if i do this oh you're right you're right yeah so can only concatenate lists not into the list so that was thinking that it was like trying to add on this thing to the end of the array but it didn't know how to add on a number cool okay so if this is my relationship and then i try to plot this relationship the same way i did before but instead i'm gonna just plot a and b right you'll notice that now all these points form a line right if i did some some other uh some other relate if i made some other relationship like if i squared all of these numbers for example right and then i wrote i plotted this you'll notice now all of a sudden they don't really follow the line they follow kind of like a curve um and so when we say fit a line what we really mean is can we figure out a relationship between these two sets of numbers that most closely represents the true relationship between the numbers because of course these points don't fall perfectly in a line but can we find a set of points or can we find a line a relationship between height and weight that really closely matches what we see in this form so if you why why can't we do that for this example where we we have this nonlinear relationship when we're when we're squaring all of the values in a right we can still draw a line that tries to meet those points like at what point is it too far away from a linear relationship to use linear regression well it's a good question so actually as we get past so today we're going to talk about simple linear regression which is really just the process of trying to fit a line to a set of data where you've got uh two quantitative variables um well simple linear regression could even be with a categorical predictor but for today we're going to stick to two quantitative variables and try to fit a line showing some relationship between them um once we get into more complex methods within still within linear regression we're going to find that there are actually ways to model more complicated relationships so actually like we can use linear regression still to follow this curve um there's a method for doing that um we also could try to represent this curve with a line and then acknowledge you know this doesn't fit our data as well but it's a simpler model and so we're okay with it um so there's really no there's really no specific rule or like threshold at after which you wouldn't use linear regression and actually like using some other methods within still within the world of linear regression you can model some complex relationships beyond just the line um but yes cool i see um i see a question about mean squared error and try to minimize it yeah so we're gonna we're gonna get to that in just a second how do we define what the best line is cool all right so what i'm gonna do is i'm actually gonna so i'll demonstrate this really quickly something kind of cool is that if we use the plot function from matplotlib dot pi plot it'll actually plot a line it'll just like connect the points with a line which looks a little awkward for this curve but if we come back to this like a plus five situation down here um right because all the points were in a line if we connect them we're just going to get a line right off the bat so we're going to use that trick to try to add some lines to this plot cool so um i guess actually before we do that let's come back to so what kinds of relationships here create this linear linear relationship so we saw one example where we get this line we saw another example where we get a curved relationship what kind what formats of relationships create a line and if you've taken an algebra class you've probably seen this particular format which is m y equals mx plus b um i guess i shouldn't have used a and b i'll use x and y here so that we can we can use that same shorthand that should be uh y equals x plus five right okay so um okay this should all still work great though just renamed everything so i'm going to define two more numbers i'm going to call them m and d and i'm going to say m is equal to 2 and d is equal to 5 and then i'm going to write this out as m times x so really all i'm doing here um and i'll i'll print y again is i'm saying okay my x's are these numbers then for each value so take four as an example i'm gonna do i'm gonna take two times four because i set m equal to two two times four plus 5 because i set b equal to 5. so 2 times 4 is 8 plus 5 is 13 and that's how we got this number and then we're just going to do that for each one so like 8 times 2 plus 5 is equal to 21 and then if we plot this it'll still be a line okay just so that we can see the changes a little bit more easily i'm gonna just set the limits for this plot so i'm going to set it from 0 to 45 42 if those are our x's are between 0 and 42. give ourselves a little extra room and then um i'll set the y limits to say let me go yeah and the reason why we're doing this is because it's a little bit hard to see that like these points are actually changing because the the axes just change on us automatically if we're if we keep redrawing this graph over and over again so what we're doing here is we're setting the bounds of the of the axes that we're drawing so now if we if we redraw that um uh that's what that line looks like but now if we change our original equation up there our y is mx plus b if we change it to like uh so if we change it to like times uh times three rather than yeah make n3 all right well times 1.5 because i didn't make this big enough yet hold on let's do it at 2 again let's up this to like 200 okay so now we've got more space so now if i change this to 3 and i rerun it now we've got the line changed slightly we can see that it got a little steeper yeah which would have been happening even if we didn't set the even if we didn't do this dot excellent.ylm um these two lines of code but basically the graph would look exactly the same except the axes would be different um and so by setting our axes we can actually see the line looks different rather than the axes looking different yeah exactly um and so it turns out that this format if you write a relationship between some set of numbers x and some set of numbers y in this way where y is equal is always equal to a number times the x value plus a number or minus a number because you could have a negative here you could have a negative number in either spot um if if you stick to that kind of a relationship then you'll always get a line a straight line and before when we were just adding five we were still conforming to this because we were kind of just multiplying x by one right okay so let's let's just like see what this does a little bit let's investigate these numbers so um so the first number the number that multiplies each of these numbers our x's um this is called the slope of the line and it has some effect on how steep the line is so if we make this bigger so if we make this a 5 and rerun this we'll see that that line got even steeper it's going off the page now if we make it a one it's going to be a lot shallower right a lot less and then if we make it negative so let's say i make it like negative three it's gonna actually i guess i should have done sorry guys still getting cut off but you can see it's going um it went from sloping up to sloping down and then the more negative we make this they probably should have done a smaller number as an example but if we made it even more um it's going to start sloping down even more steeply so that's our slope that gives us one one piece of the line and then the other piece of it the other thing that we might want to control is how high or low like where the line is or uh vertically like if it's higher or lower so that's what this is going to control this b is going to control what's called the y um intercept and so that is technically the value um up on this like yaxis where this line would hit at x equals zero so it's basically the value of y when x is zero or what we would get if we plug in zero into this point so in this case right if we plug in zero to this formula right now i'll just write it out so we can see it right now we basically have two times x plus five if we plug in zero for x we really have two times zero plus five so y is equal to five and you can see if we kind of extend this line out it's hitting this yaxis around the number five and so that that controls where that line falls if we go ahead and increase b let's say we make b equal to 100 all that's going to do is raise that line up so that if we draw this back to where uh the zero line is it's hitting at 100 um and then if we make it say like negative 50 it'll bring it down so that it hits the line right and so your original question here sophie was like what kind of operations make a linear relationship right yeah and and so the answer is kind of a dish uh like addition and multiplication and um based off of that also subtraction and division where b could be a negative number which makes it subtraction m could be a fraction which would make it division um so those are the operations that will make this linear relationship whereas we saw something like um using an exponent if we if we squared all of these numbers that made something that wasn't a linear relationship exactly so let's now take this back to our original problem and let's see if we can add a line to this to this plot um so one thing i'm gonna do really quickly so we saw right that we can control where the line hits um at height equals zero so we if we extend this out all the way to zero we can control where the hot where the line would hit this yaxis and then we can control how steep the line is so let's go ahead and let's extend the axes of this are we gonna eyeball the uh the line of best fit and try to uh this this would be a great thing to get from from chat of like come up with your y equals mx plus b that you think fits uh uh gets this data the best this one's going to be a little tricky because we have to like go back a ways and then we've got to go from let's go all the way from zero well we'll go from type all right sorry guys um okay so why is the weight so it is and then excellent we wanted to go from zero to okay there we go so here's here's our points now we've got to imagine so we're gonna write out an equation for a line we're gonna see if we can figure out if we were to draw this all the way back to here where would this hit at height equals zero and then if you want to throw your guesses in the chat go ahead and then um and then the second piece is going to be the slope that one's a little bit harder so one way we can think about slope is rise over run so rise is how far up we have to go for some distance forward okay so in this case it's like the ratio of if we if we're drawing this line how far up do we have to go to how far over do we have to go to stay on that line so if the ratio is equal to one it means that we could draw this line by going up one over one up one over one up one over one and the resulting points would create the line if the slope is two it means we have to go up two and over one up two and over one to get that line i think there's a picture of this in the linear regression course which i think is helpful right so this line has a slope of 2 because in order to create the line we have to go up 4 and over 2 or up 2 and over 1. no matter what it's going to be the same ratio but we have to go up twice as far as we have to go over in order to draw this line so the slope is in this example 4 over 2 um or 2. and right a negative slope would be you have to go down right or over two and down four um okay so did we get any guesses in chat for the yintercept not not yet so yeah in chat try to guess what the yintercept is and then also kind of what sophie was just talking about what do you think this slope of best fit might be um and yeah i see steve is saying oh maybe you can remove the x lam and y limb in order to have it automatically kind of size the uh size of the graph that's totally what we'll we'll do in the long run but we're just trying to be able to we're trying to basically eyeball the um the what's the xintercept or the yintercept um so we wanted to expand the graph to just kind of see where that uh that intercept might be but yeah i imagine soon we will go back to uh just having it automatically generate the um the dimensions of the axes yeah good questions are a good comment of all the points won't hit one common line in this case we can assume the two points lowest and biggest on the plot scale would give an idea of where the y concept would be right so kind of an implied question there is like how do we even go about this right because this isn't going to we know this isn't going to be a perfect line so like how can we even begin to guess what oh what an intercept would be um thoughts on that sophie yeah totally i think i think everybody that approaches this problem probably approaches it a little bit differently um so right so do you want to just try to like take the smallest and the largest and draw a line through that do you want to draw the line sort of in the middle of all of the points um and the most commonly used form of linear regression is ordinary squares regression which i'll define in a second but that's one definition of what makes the best line um you can actually run linear regressions with different definitions of what makes the best line and it changes the calculation slightly um but yeah okay see someone guess around negative 10 on the yintercept so um i'm gonna i'm going to type that in because that's the first one i see i think the slope is a little tricky don't see any um the ideas on slope yet i'll come up with an idea while we're waiting too in case no one has one but um but go for it if anyone there's no shame in in giving it a try yeah let's see so i think if i were to guess this slope it looks like it's going from about so the rise is like you know 75 to the the the point on the furthest right is like at 75 maybe in the point on the first left is that like 55 so that's the change of 20. and then the run is uh like 85 to uh what is that 60 so uh what is that 35 no 25 so 20 over 25 would be my guess if i i remember those numbers correctly so let's see i guess we could see what that line looks like i guess we try that sure all right okay what looks does the slope so 20 over 25 i just want to print out what that is so 20 20 over 25 is about point b um so let's try what do you guys think we definitely want to move this line down a little bit and it also looks like the line is a little bit too steep so um maybe let's try to kind of mod modulate the steepness first okay so we want it to be less steep so we want a smaller slope so what do you think point six we'll see what it looks like maybe okay closer i think maybe even a little less 0.4 oh that's too small maybe or 0.5 i think that looks about right for these points um so we want the intercept i think to be a little lower um so maybe let's try like negative 20. okay and i think one of the points of this is that this is like hard to do right or where it's like we're kind of struggling to eyeball this and so that's that's eventually going to be the point of linear regression is that we're going to be able to find a line that is that we compute in a way that's you know a little bit more scientific than us just kind of like randomly guessing um i i we had a comment that said the slope looks good but the wire is up to his way off that was my instincts too but i actually think that it was that the slope was a little bit more off and the y intercept just need to be suited down a little bit yeah so i think i've gotten to a place where i think this is pretty good we can change the excellent now maybe to like oops um you can change these limits now to more like 150 to 200 um 40 to 80 maybe that we can zoom back in um more like 50 to 80 and it looks like this line pretty well describes that relationship between the sets of the set of points um and so now the next question is how do we operationalize this which means like how do we define this so that's repeatable and then how do we actually calculate what these numbers should be this slope and this intercept right and i think a question is like is this actually good right because it's we could like kind of tweak forever where i could say oh i want this to be raised a little bit or the slope to be changed a little bit so like how do we know is this the best that we can possibly do how do we even define what is the best that we possibly can do um just kind of the question of how good is this yeah totally 100 agree okay so um i actually i think alex maybe you wrote this uh at one point this little applet but i'm going to go to the code cabinet i think yeah this one this one was not me but this one's good i like this one yeah um so it turns out that what we often do um or what we we do if we're running ordinary least squares regression is we set up this equation we say okay for each point right each height and each weight um so we can think of the y here as as weight and the x here is height for example um we want to define this relationship between the points we want to say somebody's weight is equal to some number times their height plus some number plus some error right so we're saying basically we'll make predictions along the line but we know that each individual weight is going to be not directly on that line so going back to this picture right none of these weights are exactly on the line and we can measure how far away they are from the line by drawing kind of the vertical line between them and so ordinarily squares regression says okay let's try to minimize these distances between the point and the line now actually finding the minimum of the absolute distances between the point and line is a more complicated math problem so because of like math being hard we actually do instead of the absolute differences so just taking like okay this one is um two units away or five units away or whatever and this one is one unit away and adding those up we actually just square these different these distances and add them up the squaring automatically makes them all positive right because if you square a number um a square a negative number is positive so by squaring it it means that a point below the line is just as that's far below the line is just as bad as having a point that's far above the line doesn't matter just matters how far it is away and um and so let's try to minimize those square distances for all of the points so we take square distance for this point square distance for this point square distance for this point add them all up and let's find the line that minimizes all of those distances this may be a silly question but why do we care about the um the vertical and distance and not like perpendicular to the line or horizontal to the line so actually there's like linear regression models that you can fit that minimize the horizontal distance so it actually depends on the research problem what you're trying to do um if you're trying to make predictions of the y variable so in this case like if your goal is to make weight predictions using height information and for example doctors might do this when you're when you're young they want to like estimate i don't know predict what height you're going to be or what weight you're going to be or say are you overweight or underweight compared to normal and so they want to know what this line looks like for on average so that you know um if your goal is to predict weight then minimizing these square distances is really like minimizing um minimizing how off you'll be in your prediction but you know if if you minimize the horizontal distance you're minimizing um how off you are with respect to like how much a difference in height um i don't know i honestly because because it's like if if we if we just change the axis if we just put height on the yaxis and weight on the xaxis it's the same data right but now the the line is going to be slightly different and the thing that we're minimizing is going to be the difference in you know in that other variable um is there is there one where you do perpendicular distance where you like just see how far away it is from the line oh you mean like like the crows distance or whatever like yeah exactly uh yeah i'm sure there is i have never seen it done yeah but that doesn't mean anything um so yeah cool cool cool so all right so i like this uh this little applet so in this applet you can randomize a set of points you can say okay let's like make 10 points on this graph and then let's move m and b around and see if we can minimize those distant those total squared errors um and so we can see this little bar on the side that kind of grows as the line gets further away and shrinks as it gets closer and we notice that there's some kind of like minimum here right like as i move the line as i move b up the squared error is getting smaller and smaller and then at some point it hits the smallest point and then starts getting bigger again so it's like we want to find this minimum here and if you have some time i very much recommend playing with this a little bit to get yourself kind of used to this idea of what you're what you're calculating when you are fitting these models cool so um there are a lot of different ways to actually calculate m and b for this problem for a simple linear regression problem where you have two variables you can use some calculus and you can actually just like get a formula basically for what those numbers are going to be um you need a little bit of matrix algebra for that and i think in session four maybe three or four um we'll actually walk through a little bit of that math because i think it's helpful in order to understand some of the assumptions that you're making and some of the errors that you can get it's also helpful if you want to use a different software besides python or a different package to fit a model um but i'm going to actually just go ahead and use a um a function within the stats models package and just quickly demonstrate to you all how we could fit this so i i've set this on live streams before i come from um a staff background and i learned how to program an r all of this in r first so i really like stats models interface more than scikit learn which is another package that you can use to fit these models there are actually many in python i think a lot of data scientists prefer scikitlearn because it has some other functionality that's really useful that stats models does not have um but for right now i'm going to just use stats models because it gives us a lot of useful information and this is the and this is the code so i've already downloaded statsmodels.api as sm so this is my shorthand for the statsmodels.api library and then i'm saying i want to do ordinary least squares regression which like we said is just minimizing those squared distances and i'm using the from formula function which allows me to kind of input a formula that i want to use to fit the model and so in this case i saved this data set as something called data it has two attributes right two columns white sorry weight and height um and i'm going to go ahead and fit the model to predict weight using height so this tilde tells me that weight is my outcome variable that's the first one height is what i'm using to predict it um so that's like my x variable here the data set i've called it data which is redundant um but anyway it's called data so this is telling python where to find these variables and then i'm just gonna go ahead and fit the model instead of doing that in two steps um and then i'm gonna go ahead and print a model summary so that we can all inspect it together so here's what this looks like um you'll notice so we do get this warning um the warning says any like it says something about uh kurtosis test is only valid for n greater than or equal to 20 continuing anyway n equals nine so basically it's giving me some sort of error around my sample size is not big enough i only have nine data points it only wants to run this test if there's uh 20 or more data points fine we're doing this as as an example we also see we see some other warnings in here um and we'll talk a little bit more about what these warnings mean basically those warnings are happening for a couple of reasons one because um all of these points are so far away on the xaxis from zero so we saw how much um that that intercept could change uh based on exactly what these points look like because it's so far away and then partly because we have such a small sample size and those are things that we can fix but really the important part here is we see these two numbers the intercept and so these are under the coefficient section so we see that the intercept which is the yintercept or that b value is about negative 21.6 or 0.7 and then we see that the slope and it it tells us that it's the coefficient on height the reason it does that is that we can add more variables this later um but tells us that that slope is 0.5 um cool so i see a question about using libraries or hard coding simple linear regression um so i'm going to show you within the course of this these sessions i'll show you how to implement this in stats models i'll also show you how to implement it in psychic learn um and i'll also show you how to calculate it by hand using a formula um so we're going to do all three in the course of this this session i think the important thing is not so much understanding um how to do it in a particular package but understanding what you're doing so that you could do it in any python package or package or um you know some or stata or something else entirely sophie so if we go back to our to when we were eyeballing it and just drawing the line ourselves and change our coefficients to that negative 21 and the zero point uh or yeah 0.5 whatever so we were pretty close yeah maybe sophie knew the answer is all along i bet she did but that's okay yeah this basically this line is the model that uh whatever library we were using would generate yes exactly so we're getting a lot of information this information is super useful if you're trying to do something more complicated really all we cared about and i can just print them out is the parameters of the model which were the intercept and the slope and we can see that those look pretty good they fit our model pretty well cool um so cool we have about five minutes left one last thing that i was hoping to kind of get through in this session is the assumptions of linear regression i'm gonna compress it because we don't have a ton of time um but one thing i think it's one reason i think it's important to in any discussion of simple linear regression or linear regression or any model whatsoever to think about the assumptions is that a lot of times a particular method like ordinary least squares regression might not be appropriate for the kind of data you have or the kind of question you're asking um and so it's important to know when it's appropriate and when it's not um and so that's where the assumptions of linear regression come in um also if you're interested in applying for like data analysts and data scientist positions this is the kind of thing that someone might ask you in an interview what are like i know i i've heard that we ask this in our data science interviews um what are the assumptions of linear regression so worth worth knowing what they are um so the first two are pretty simple first one is that there is a linear relationship between your variables if there's not like i said you can use more complicated methods to model it but if you're modeling using a line then you have to believe that there is a linear relationship that you're trying to model the second one is that each of the observations are independent um which really just means that one row in this data set can't affect another row so let's say that these were actually the heights and weights of all the people all the people in a single family um then you know a mother's height and weight might be more similar to a son's height and weight or they might have some like familial thing where they're really short and or really tall and really skinny or whatever it is that isn't really true of the general population so if you're trying to use this model to describe a relationship in the larger population you shouldn't use um you shouldn't use that model um so we we assume that they're independent and then um and then there's two more assumptions related to uh related to the actual data so one is that the outcome variable is normally distributed and a lot of times we check that by looking to see if the residuals are normally distributed um and so the residuals are those error terms they're basically like how far away is each point from the line um so we can actually calculate those so really quickly um i just want to demonstrate for example we can get the predicted heights for each person by applying this model so in this case if we take or sorry the predicted weights if we take the predicted heights of each person we multiply by sorry we multiply by the slope and then we add this number and adding a negative number is just like subtracting then we will get the predictions for the weights of each person in our data set and we also have the true weights of each person in our data set right so the first we would predict the first person is 66 uh kilograms but they're actually 64. the second person is 59 kilograms they're actually 58. um and we can see that here right in this line the prediction is where they would fall on the line and then the true value is below it so we saw the first one the prediction is a little high right and i think i don't know if these are these points are in order actually but um but we can see right the prediction here is a little high so we can take all of these predicted weights and true weights and then we can subtract them um to get the residuals those just those um differences and we can plot a histogram of them to check for normality um normality just means that they roughly follow the histogram kind of follows this bell curve like shape um actually i will i'll jump over to the example code to show you at this point so basically this is saying that um some of those residuals are or some of those points are under the predicted values some are over the predicted value um a lot of them are you know closer to the predicted value than not um and follow that bell curve exactly and it's a little hard to see here because we only had nine points but if we estimate the the line that should follow this histogram we see it's like roughly a normal distribution um and like alex said most of them are around zero some are a little bit below the line some are a little above the line and then the last is an assumption of linear regression how do you how do you what do you do sorry sorry i think my internet broke up there for a second um if this is an assumption for linear regression like how do you know this without doing all of this um well so you can plot you can start by plotting a histogram of the outcome variable if the outcome variable is bimodal or something or super skewed then um you probably want to apply some transformations um you could also fit the model and and look at this directly um but yeah it's something that you can you can deal with there are some methods for managing it if it's not met um but it's actually a way that you can improve your model if you fit a model and then you check this assumption and you see something super skewed that means like you have some avenues some indication that you might want to fit a different model um you might want to do something a little differently um and then the last one it's called um it has a fancy name uh homoscedasticity uh which really just means that the variance of uh of the points is the same for all values of the uh predictor variable um i feel like i didn't get so basically what you're looking for is not this um so in this example we see that the points are really tightly wound around like the um there's not a lot of variance in the points like down here for small values of height this is totally made of data and not realistic at all we even have a person with negative weight which is not possible but just a picture right like so these points are really tightly bound together and then these points are a lot more uh have a lot more variation and this kind of a picture would violate the homoscedasticity assumption it would say like what you want is for the points to be kind of like have equal variance around the line for all of the predictor and you can check that by making a plot um of the residuals against the fitted values the fitted values being those predicted values and then taking a look at um at what that that plot looks like and if you see any weird patterns in it that's an indication that that the homoscedasticity assumption might be violated so these are also i'm guessing you cover all of this in this first lesson in the first lesson in the course exactly yeah so this is um right we're about five minutes over so i'm gonna end it there um we can definitely answer more questions about the assumptions in office hours um and then also highly recommend just taking this first lesson um in the course on codecademy it's um the introduction to linear regression lesson it's the first the first lesson in the linear regression in python course um and starts to introduce some of the topics for next week for office hours again i'm putting the link in the chat there's also a link in the um youtube description we're doing that uh on thursday at the exact same time right now we've only booked it out for a half hour depending on how many people show up there um but yeah again it's kind of a zoom call or it's a zoo zoom webinar technically um so you can join the call and then we can like bring you on to ask um to ask questions to us um we also got questions about setup i saw sophie in the in the github page um you talk a little bit about um let me find that uh you talk a little bit about installing anaconda um so if you want to go through this code definitely make sure to follow those instructions and um maybe we can put together something uh to help out um with all that uh installation stuff there's also i think um i think another curriculum developer just recently updated a bunch of our content on setting up jupiter notebooks so i can try to find those articles but also if you go back to the first session of the master statistics live stream which we have on our youtube page um in the first like 10 15 minutes or so we go through the full process of like opening a jupiter notebook on your computer from anaconda and like running code for the first time so if you want to check that out um that also exists cool all right well i hope some people show up on thursday otherwise will be pretty lonely yeah but uh but yeah that'll be fun thank you for um yeah everyone in chat for asking questions a lot of new faces i haven't seen here before a lot of good questions um so yeah we'll um and then we'll be back next week with another um another session awesome cool all right thanks sophie
