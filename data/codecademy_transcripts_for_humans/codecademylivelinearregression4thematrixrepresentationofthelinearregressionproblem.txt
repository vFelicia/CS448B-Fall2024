With timestamps:

00:02 - oh man
00:04 - i think we are live
00:08 - hello everyone um let us know in the
00:12 - chat if you can see us
00:14 - i think uh we should
00:17 - be live by now but it's always good to
00:20 - just see some confirmation
00:25 - we're good we have confirmation um
00:28 - all right so we're gonna get started
00:30 - today and
00:31 - um talk about some of the math behind
00:34 - linear regression but before we get
00:36 - started
00:37 - i want to introduce my uh co-host for
00:40 - the day
00:40 - natya she this is her first live stream
00:43 - so
00:44 - give her a warm welcome um she also
00:48 - yeah and she also wrote the article
00:51 - on codecadme on this topic so um it was
00:54 - one of her first
00:56 - her first content items it went live on
00:58 - the site so it's really exciting that
00:59 - she's here and she can also help
01:01 - us walk through everything um
01:04 - and i am live actually from the code
01:06 - academy office and i even have
01:08 - alex kuntz sitting behind me so
01:15 - it's almost like the pandemic is
01:18 - almost over maybe in this little bubble
01:23 - um cool so we're gonna get started
01:26 - um i'll share my screen and we can jump
01:29 - right into it
01:48 - all right so um
01:53 - all of this code just so you know and
01:55 - all of the
01:56 - math stuff is written out in a file
01:59 - that is on our github so there's going
02:02 - to be lots of notes today
02:04 - don't feel like you have to write
02:05 - everything down it's all available to
02:07 - you there
02:08 - um what i'm gonna do also is just
02:11 - make this a little bigger
02:15 - cool so um
02:18 - as i said today's focus is really on
02:21 - some of the math behind
02:23 - linear regression and
02:26 - just to make sure nobody feels
02:29 - intimidated by this
02:30 - we're not going to get into super
02:33 - complicated equations or anything like
02:35 - that
02:37 - what i mean by math is i want to cover
02:41 - some of the ways that we represent the
02:43 - linear regression
02:45 - problem using mathematical
02:48 - language or notation um and the reason
02:51 - why it's important
02:52 - is because whenever you use any sort of
02:55 - software to fit
02:57 - a regression or fit any model for that
02:59 - matter there's
03:00 - that function is doing something behind
03:02 - the scenes that you're not
03:04 - seeing and it's really hard to figure
03:08 - out what's going wrong
03:09 - if things go if things do go wrong which
03:11 - they often do in the real world
03:13 - it's really hard to figure out what's
03:14 - going on if you don't have
03:16 - at least some sense of what's happening
03:18 - when you press
03:20 - run and so today what we're going to do
03:23 - is we're going to dig in a little bit
03:25 - and see a little bit of what it looks
03:28 - like behind the scenes when we run a
03:30 - linear regression
03:31 - caveat is that depending on what package
03:34 - you're using
03:35 - what function you're using the way that
03:38 - this
03:39 - these coefficients are calculated might
03:42 - vary
03:42 - and that's actually part of your
03:44 - decision making as
03:46 - a data scientist or analyst is like how
03:48 - am i going to fit a model
03:50 - but we're going to focus on the stats
03:51 - model's implementation
03:54 - and in the process we're going to talk a
03:57 - little bit about matrices
03:59 - and how we can represent this problem as
04:02 - a matrix equation so
04:06 - as an example we're going to go through
04:08 - some
04:09 - [Music]
04:10 - some data where we need some data from
04:13 - street easy again
04:14 - if you've tuned in before you've
04:15 - probably seen
04:17 - us use this data in the past
04:21 - and it's got a bunch of information
04:23 - about apartments in new york city
04:25 - with rent and some information
04:28 - about the apartments themselves so the
04:31 - number of bedrooms bathrooms
04:33 - the size minutes the subway
04:36 - building age whether or not the
04:39 - apartment has a washer and dryer
04:41 - this is a subset of the original data
04:43 - but this is what we're going to work
04:44 - with today
04:45 - so we're going to imagine that we want
04:48 - to fit a regression
04:49 - to predict rent using all of these
04:52 - predictors
04:54 - cool so
04:58 - here's where we're going to return to
04:59 - some math and i'm actually i've got my
05:01 - ipod all set up so i'm going to switch
05:03 - over to the ipad so we can write this
05:05 - out
05:06 - um and and get started
05:14 - let's see if we can get this to work
05:21 - and i want to remind everyone
05:25 - that if you have any questions feel free
05:27 - to go ahead and post them in chat
05:29 - this is going to be a pretty detailed uh
05:32 - discussion that we're having today so if
05:33 - you
05:34 - have any any questions at all or you
05:36 - want to back up and
05:37 - start something over again just let me
05:39 - know
05:40 - and also lightning um
05:44 - okay so here is our uh our data
05:47 - i've pasted it here so that we can take
05:50 - a look at it
05:52 - and this data set has
05:56 - already the makings of what we would
05:58 - call a matrix
06:00 - right so for anyone who needs a
06:02 - refresher who hasn't seen this before
06:04 - a matrix is just basically like a
06:06 - rectangular grid
06:08 - of values um and those values can be
06:12 - anything and so
06:13 - we're going to represent this using a
06:17 - matrix as well
06:19 - um so let's remind ourselves what we're
06:22 - trying to do when we fit
06:24 - a linear regression so remember that
06:28 - our initial um
06:31 - our initial equation of a line was y
06:33 - equals mx plus b
06:35 - that's what we started with and so we
06:37 - wanted to
06:38 - fit an equation that looks something
06:43 - like this
06:44 - rent equals
06:49 - m times and then
06:52 - let's pick a a predictor i guess i think
06:55 - the most obvious one
06:56 - is size square feet so let's do size
07:00 - square feet
07:04 - plus b plus error
07:08 - and remember that this is the slope
07:13 - so we want to calculate the slope
07:17 - this is the intercept
07:21 - and then this
07:25 - is our
07:28 - error how far off the true rent is
07:32 - from our predicted rent
07:36 - in in the context of our line rate this
07:39 - is
07:39 - this is all we need to plot the
07:41 - regression line
07:43 - but this error term is here because our
07:46 - regression line doesn't
07:48 - perfectly predict all of the data points
07:51 - and what this what this equation
07:54 - actually represents is a whole bunch of
07:57 - equations
07:58 - one for every row in our data set so
08:01 - we've got
08:02 - for example 3600
08:06 - and this is the first row of our data
08:08 - 3600
08:09 - equals and then the psi square feet is
08:12 - 900 so
08:13 - m times 900
08:17 - plus b plus error
08:20 - one and then we've got 3 900
08:25 - equals m times 1000
08:29 - plus b plus error two
08:32 - and we've got a bunch of equations like
08:34 - this each one with their own error term
08:38 - and that is how far off our prediction
08:41 - is based off of our model
08:45 - and our goal remember for ols regression
08:48 - is to minimize the sum of the square the
08:51 - squared errors
08:52 - so when we run a regression we want to
08:54 - try to find the value of
08:56 - m and b that works for all of these
08:59 - equations
09:00 - right so we want to find value
09:03 - of m and b
09:08 - such that
09:12 - the sum of the squared errors so error
09:15 - 1 squared plus error 2 squared
09:20 - oh my gosh can't even spell
09:24 - error 2 squared
09:27 - plus error 3 squared and so on
09:31 - all the way up to however many
09:34 - data points there are
09:38 - so that that sum is minimized
09:41 - and that's that's the whole regression
09:43 - problem
09:44 - so let's see if we can represent
09:49 - all of these equations
09:53 - using some matrices
09:57 - so i'm gonna erase everything and we're
10:00 - gonna start
10:01 - trying to build up this matrix
10:19 - all right
10:24 - so we've got our rents
10:29 - oh that is
10:37 - and we can put them in
10:40 - a single column matrix so
10:44 - 3 hundred first one thirty nine hundred
10:50 - twenty seven hundred forty nine hundred
10:54 - thirty nine hundred
10:58 - and then we want those to be equal to
11:03 - our slope
11:09 - times
11:12 - in this case all of the size square feet
11:15 - value so
11:16 - 900 1
11:19 - 900 12 16
11:26 - 1100
11:29 - now we want to add so remember in our
11:32 - equation
11:33 - right it's like 3 600 equals
11:36 - m times all of these numbers
11:40 - plus b but we want
11:44 - the b's to kind of get added to every
11:47 - single
11:48 - equation so in order to do that
11:51 - but without writing the letter b a bunch
11:53 - of times what i'm going to do is i'm
11:54 - going to write it as the letter b
11:58 - times a bunch of ones
12:05 - so this is this term right here
12:09 - is the same as just writing out
12:12 - b b b b because for matrix
12:16 - multiplication if we multiply
12:18 - by a scalar then we just multiply that
12:20 - scalar
12:21 - by every single value in the matrix
12:25 - and then we've also got our errors
12:29 - over here but it turns out right
12:39 - so this is a pretty
12:43 - ugly way to write this out
12:46 - so we can actually make it a little bit
12:48 - simpler
12:51 - by combining this matrix
12:55 - and this matrix together
12:59 - so let us
13:08 - give that a try
13:19 - sorry guys
13:20 - [Music]
13:31 - seems to be not a racing
13:39 - we can combine the ones into this matrix
13:42 - and make it a
13:45 - 5 by or in this case 5 by
13:48 - 2 matrix all at once
13:55 - so we can put the ones in here
14:04 - and then we can sorry i don't know why i
14:17 - haven't
14:17 - been able to get this
14:22 - or we're using it to work properly
14:26 - and then we can combine the m and the b
14:29 - into their own matrix as well
14:35 - the reason that this works is because of
14:38 - how
14:38 - i'm sorry b and then
14:56 - i'm doing this wrong i'm sorry
15:00 - uh nitya do you want to catch my mistake
15:05 - or do you know what i'm doing wrong yes
15:09 - um it's it's bro multiplies
15:12 - column right right um
15:15 - yeah so it has to come um
15:19 - after the first matrix yeah
15:22 - all right now we've got it
15:26 - okay the reason this works is because of
15:29 - how
15:30 - matrix multiplication works so when we
15:33 - multiply matrices
15:34 - what we do is we row we multiply each
15:38 - row
15:40 - by a by the corresponding column
15:44 - in the multi in the
15:47 - second matrix so the first thing that we
15:50 - would do if we were multiplying these
15:52 - two matrices is multiply this row
15:55 - by this column and add each
15:58 - pair of terms so we would do
16:03 - right 1 times b
16:07 - plus 900 times m
16:11 - is our first set of
16:14 - our first equation that comes out of
16:16 - this matrix multiplication
16:19 - the second pair is
16:22 - the second row by this column
16:26 - so that's going to be
16:30 - equal to 1 times b
16:34 - plus 1000 times m
16:38 - and then we just continue down the line
16:40 - so for each
16:42 - row we're going to multiply by this
16:44 - column and add up the
16:45 - add up the terms so we end up with
16:49 - b plus 900 m for the first one because
16:52 - one times b is just b by itself and for
16:55 - the second one b
16:56 - plus a thousand times m and then b plus
16:59 - 12
17:00 - 16 times m and in every case we end up
17:04 - with this mx plus b
17:05 - format although it's written in reverse
17:08 - but it doesn't matter what order we
17:09 - multiply in
17:12 - okay this gets
17:17 - slightly more complicated but not too
17:19 - much more complicated when we switch
17:21 - over
17:22 - to multiple
17:26 - linear regression so remember in
17:29 - multiple linear regression we can have
17:31 - even more predictors and we can have
17:34 - even more
17:37 - and then because we have more predictors
17:38 - we have to have even more
17:42 - coefficients so
17:45 - i'm gonna just write this out
17:49 - but if we wanted to do a multiple linear
17:51 - regression with
17:52 - bedrooms and bathrooms and all of our
17:54 - other uh
17:55 - features in there as predictors we can
17:58 - just add them to this matrix so we can
18:00 - say okay i want
18:02 - bedrooms as a predictor so i'll add
18:04 - three
18:05 - three two one zero for
18:09 - um bedrooms and then bathrooms is two
18:12 - two one one um
18:15 - and then min to subway is four four four
18:19 - six three and so on um i can add all of
18:22 - my predictors
18:23 - i won't add them all right now to write
18:25 - it out and then i can have an entire
18:32 - column here column matrix here where i
18:34 - have
18:35 - all of my thetas so my b0
18:39 - b1 b2 b3
18:42 - and b4 and these are just the
18:44 - coefficients
18:45 - on each of these predictors when we fit
18:48 - the model
18:50 - and you can see that if we
18:53 - write this all out i know this is like
18:55 - getting very
18:58 - probably very boring at this point but i
19:00 - promise we'll get back to the the fun
19:02 - part in just a moment
19:04 - right if we multiply each of these out
19:07 - we get a separate equation
19:09 - for each row of our our original data
19:13 - that follows the pattern that we wanted
19:16 - we've got in our first row
19:20 - all of these values multiplied by all
19:24 - these values and so and then
19:27 - terms are added up so we get something
19:30 - like
19:33 - 1 times b 0 which is just b 0
19:37 - plus 900 times b1
19:41 - plus 3 times b2
19:45 - plus 2 times
19:50 - b3 plus 4 times b4
19:55 - and then plus error 1
19:58 - at the end and this is our regression
20:01 - equation
20:02 - right 3600 equals
20:06 - all of this plus error is our regression
20:08 - equation for the first
20:10 - row of our data and so on um
20:14 - then we do the same thing for the second
20:16 - row
20:17 - cool hopefully i have not lost everyone
20:21 - at this point um we're gonna go back
20:24 - to to coding this
20:28 - up but i think just seeing this written
20:30 - out
20:31 - is useful so that you can build off of
20:35 - what you see
20:36 - in the code
20:39 - so when you stop did you want to add
20:43 - something
20:45 - just that b0 is the new intercept and
20:48 - like
20:48 - that the rest of the b ones too whatever
20:51 - are your new
20:52 - slopes exactly yeah so
20:56 - um right so when we talked about
20:58 - multiple linear regression
21:00 - we talked a little bit about this but
21:03 - it's worth
21:05 - coming back to because i think the
21:08 - notation can be a little bit confusing
21:10 - here
21:11 - um so yeah just like nichia said
21:14 - when we talk about multiple linear
21:15 - regression we rename the intercept
21:18 - b0 instead of b and then we
21:21 - instead of using m for our slope we use
21:24 - b1
21:25 - through b whatever because that way we
21:28 - can use the same letter to represent
21:31 - all of the coefficients and just kind of
21:33 - number them
21:34 - cool so um let's go ahead
21:38 - and take a look at what this actually
21:41 - looks like
21:41 - when we fit a model
21:44 - so i'm going to go ahead and run this
21:50 - and here's our data again
21:55 - and you might remember this from when we
21:58 - talked about categorical predictors
22:00 - uh but this is the
22:04 - this patsy.d matrices what this does is
22:07 - it takes the step
22:10 - it looks like uh the screen share is
22:13 - freezing
22:16 - yeah uh or maybe the window is just like
22:20 - really short i don't know
22:21 - i just wanted to let me reshare screen
22:23 - this happened once before when i
22:25 - switched
22:26 - from the ipad to the
22:29 - yeah it looks perfect now okay great
22:34 - um cool so
22:38 - um so essentially right the patsy d
22:41 - matrixes
22:42 - the matrices function happens inside
22:46 - stats models uh from formula ols
22:49 - function and we can use the same
22:53 - the same formulas that we use in in that
22:57 - function
22:58 - in this to see a middle step
23:01 - that happens when we run a regression so
23:05 - i'm going to do this really quickly and
23:08 - i'm going to use this uh this equation
23:11 - so
23:12 - rent is our outcome variable and then
23:15 - building age years and min to subway has
23:18 - washer dryer
23:19 - our um
23:22 - predictors for right now because i just
23:25 - don't want it to take that
23:26 - the whole space so you can see it a
23:28 - little bit more easily but
23:30 - you could include all of the predictors
23:31 - in this model
23:33 - and we're going to return this as a data
23:34 - frame so that we can just see these
23:36 - labels
23:39 - and when we print this x matrix what we
23:43 - get
23:43 - is exactly what we saw in
23:46 - that matrix that we just created
23:50 - so we've got an intercept
23:53 - and then in order to fit the intercept
23:56 - we have this column of ones
23:58 - right because that's how we're going to
24:01 - get that beta0 or b0
24:05 - or intercept whatever you want to call
24:07 - it in every single equation
24:10 - right so we can imagine that we've got
24:12 - our
24:13 - column of b0 b1 b2 over here
24:17 - and when we multiply this whole row
24:21 - by that column we're going to get a 1
24:24 - times b 0 here for the first equation
24:27 - and a 1 times b 0 for the next equation
24:30 - and so on
24:31 - so that's why we have this column of
24:32 - ones
24:34 - and then we've got each of our
24:37 - predictors as a
24:38 - separate column in this matrix we also
24:42 - have
24:43 - um as part of this this function right
24:45 - we have this
24:48 - column vector of y's
24:51 - and we'll print them out and that's just
24:53 - our outcome variable that's the run
24:56 - so these are really the x's and y's
25:01 - that are from this equation in this case
25:04 - the y represents this whole
25:08 - column vector of outcome variables
25:12 - the x capital x represents this whole
25:15 - matrix
25:16 - that has a column of ones and then a
25:18 - column for each
25:20 - predictor and then what we're trying to
25:23 - estimate
25:25 - is this column of b03
25:29 - through bm where m is the number of
25:31 - predictors
25:34 - so here's a better a
25:38 - simpler way to write it right so this is
25:40 - like
25:41 - this equation is the same as
25:44 - this equation just replacing this with a
25:47 - small
25:48 - y small x a b and an
25:51 - epsilon for the errors and so what we're
25:54 - calculating in the d matrices function
25:56 - is just this y
25:57 - and this x
26:02 - so we see that under the hood
26:06 - this regression equation is being
26:09 - represented
26:10 - essentially by a matrix and we saw that
26:14 - a little bit before
26:15 - we talked about um categorical
26:17 - predictors as well
26:20 - so now let's go ahead and let's take a
26:23 - look
26:24 - at um
26:27 - at the model summary oh
26:36 - i need to remind myself what we did
26:40 - um this was actually some very helpful
26:43 - uh
26:43 - code that nitia wrote so normally when
26:46 - we fit a model so i'm gonna just like
26:50 - jump back for a second so
26:54 - in the past we've done this like sm.ols
26:59 - got from formula
27:02 - i think it's like is that
27:06 - right you can grab the
27:10 - you can grab the code from one of the
27:12 - previous
27:15 - lessons
27:18 - yeah so we've got from formula and then
27:21 - we give
27:22 - it our formula
27:25 - and then our data
27:40 - okay and then
27:44 - when we print this out um
27:48 - so this is our that's our model
27:51 - and we're just gonna fit it on all one
27:53 - step
27:56 - and when we print out um
28:00 - model.summary we get this
28:03 - really big
28:14 - see
28:16 - did i miss
28:28 - something
28:31 - oh no
28:34 - oh i put this in quotes
28:38 - there we go okay so we get this like
28:42 - very big output that has a lot of
28:45 - information that we have to parse
28:47 - through
28:48 - and so um at the bottom of all this
28:52 - we've got so at the top we've got some
28:54 - information even about like the time
28:56 - that i
28:56 - ran this model and then some information
28:59 - about the model itself
29:01 - um and then we've got our coefficients
29:04 - which we can also get by just looking at
29:06 - the params
29:08 - attribute and then at the very bottom
29:11 - i've got some warnings
29:14 - and so we're going to look at in a in a
29:16 - moment we're going to look at
29:18 - an example where we get some more
29:19 - warnings but so far we just got standard
29:22 - errors assume that the covariance matrix
29:24 - of
29:24 - errors is correctly specified okay we'll
29:27 - come back
29:28 - um so here this is just some nice code
29:31 - nitia wrote it to print out just the
29:34 - parameters
29:35 - so just this part the coefficients
29:38 - for the intercept and each of these
29:42 - each of these predictors and then also
29:44 - print out the warning so that we can see
29:46 - it
29:49 - um just to demonstrate
29:52 - that we can do
29:55 - everything that stats models did behind
29:58 - the scenes
29:59 - um i think it's also useful
30:03 - to take a look at how we would take the
30:05 - matrix
30:06 - representation and actually solve this
30:09 - equation for the betas that or the i
30:12 - keep calling them betas but the
30:14 - the coefficients right the b0 b1 b2 etc
30:18 - so coming back
30:22 - to this equation remember that this was
30:25 - our simplified equation
30:27 - using the matrix form where y is our
30:29 - column matrix
30:30 - x is our matrix of predictors and this
30:34 - beta which corresponds to this whole
30:37 - column
30:38 - are the things that we want to we want
30:40 - to calculate
30:41 - such that the this set of
30:45 - error terms squared and added all
30:47 - together
30:48 - is minimized and we can calculate
30:52 - the the values of b0 b1 b2
30:56 - etc that minimize the errors by using a
31:00 - little bit of calculus
31:02 - and the calculus that we
31:05 - can implement and i'm not going to go
31:07 - through the math it's a little bit
31:08 - complex
31:09 - but it gives us this fancy equation
31:13 - for the betas or the b0
31:16 - b1 b2 etc and this is a lot of matrix
31:20 - algebra that you don't need to know for
31:22 - the purposes of this
31:23 - but this t represents the transpose of a
31:27 - matrix
31:28 - which is basically like flipping the
31:31 - rows and
31:31 - columns um so something that was in the
31:35 - first row
31:36 - is now in the first poll and something
31:37 - that was in the second rows now in the
31:39 - second column
31:41 - um and so off and so forth
31:44 - this is an inverse
31:47 - so we're taking like the transpose of
31:50 - our
31:50 - x matrix multiplying it by the original
31:53 - x matrix
31:54 - taking the inverse of the result and
31:57 - then
31:58 - multiplying it by the transpose of the x
32:00 - matrix and then multiplying that by
32:02 - our y matrix complicated math
32:06 - don't need to understand it for the
32:08 - purposes of this
32:10 - but it is kind of cool to see that that
32:14 - works so you can actually in um
32:17 - in numpy you can create
32:21 - so in this case this x matrix is really
32:24 - just
32:26 - it's the same thing that we got here we
32:29 - just
32:30 - got rid of all the labels and all the
32:32 - indices
32:33 - so it's just the ones the 15 896
32:37 - all of that condensed down into
32:40 - just this like list of lists it looks
32:43 - like a list of lists basically but it's
32:45 - a
32:46 - um a grid of numbers just like the
32:48 - matrix we saw before
32:50 - and we can use our um
32:53 - we can use some functions in numpy to
32:55 - take the transpose of the matrix
32:57 - find the inverse of um
33:01 - of a matrix we can perform matrix
33:03 - multiplication
33:05 - in numpy and when we do all of that we
33:08 - implement that formula on our x matrix
33:13 - and i think i did not run this
33:17 - we end up getting this matrix
33:20 - of betas and notice that the first one
33:24 - is like 369 6.17
33:28 - and that's the same as we got for the
33:31 - intercept
33:32 - then we've got negative 5.465
33:36 - um negative 5.465
33:40 - negative 25 and 719 and
33:43 - um and go back here negative 25 and 719.
33:48 - so we can actually calculate all of
33:50 - these things
33:51 - once we've written it as a set of
33:54 - matrices
33:55 - we can go back and we can actually
33:58 - perform that math or like use that
34:01 - equation
34:02 - to solve for these numbers
34:08 - okay so now let's go back and let's
34:11 - create some problems um so
34:16 - this is again some code from nitya
34:19 - going to create a new um
34:22 - a new column in our matrix or a new
34:25 - column in our data
34:27 - called seconds to subway and the way
34:29 - that we're going to get the seconds to
34:31 - the subway is we're going to
34:32 - multiply it multiply the minutes to
34:35 - subway column
34:37 - by 60. so actually let's like
34:40 - break this up into a couple of steps
34:46 - so
34:50 - i'm gonna run this and i'm gonna print
34:53 - this back out
34:56 - oh see i get so confused i don't know if
34:59 - anybody else has trouble with this
35:02 - there's all this
35:04 - uh hoopla about like how you are
35:07 - supposed to write
35:08 - onto a column of a matrix i don't know
35:11 - nitya if you have any advice i i have
35:15 - struggled with this every single time i
35:16 - thought that i managed to
35:18 - avoid the the error by doing it this way
35:22 - and clearly i have not oh man
35:25 - pandas um i wonder if we should just
35:28 - write our lambda function
35:31 - that's my usual i mean i think it works
35:34 - so like this gives me all
35:37 - of the all these errors so like yeah
35:41 - just so that everybody sees and then i
35:43 - think there it gives you this um
35:46 - this explainer which i always click on
35:48 - and then i never
35:49 - actually read through it um
35:53 - which at some point i will uh but it
35:56 - i like updated literally a year ago or
36:00 - something and it's just been hard to
36:01 - like
36:02 - keep track yeah um
36:06 - yeah like all i'm trying to do basically
36:08 - in the like simple way the old way that
36:10 - i would have done it is just said like
36:13 - take the create a new column called sex
36:16 - to subway i would just
36:19 - use the dot operator you can do it with
36:21 - like uh um
36:23 - you can do it this way as well right and
36:25 - then just
36:26 - have that set that equal to
36:30 - the min to subway column times 60
36:35 - um and yeah that gives the same error
36:39 - it still works i think at some point
36:41 - they're going to take away that
36:42 - functionality and i'm
36:44 - never going to be able to figure out how
36:45 - to do it without a lambda function
36:48 - um anyway but what we created was this
36:50 - new uh
36:52 - this new column that tells us the number
36:55 - of seconds it takes to get
36:57 - to the subway so here if something took
36:59 - four minutes
37:01 - or an apartment was four minutes away
37:02 - from the subway it's 240 seconds away
37:05 - from the subway because four
37:07 - times 60 is 240 and if it's 6 minutes
37:10 - away
37:11 - it's 360 seconds away from
37:15 - the from the subway so
37:18 - nichia do you want to take this one do
37:20 - you want to explain
37:22 - why this is a problematic column
37:26 - sure i mean i think for it's kind of
37:29 - like
37:29 - a little um apparent right that we have
37:32 - the same information
37:33 - in both of these columns which is um
37:36 - time taken to the subway
37:38 - um and not only are these two uh columns
37:41 - highly correlated they're in fact
37:43 - collinear which is
37:44 - they're literally um they literally
37:47 - differ
37:48 - from each other by a constant factor
37:51 - right um and i think i think it's sort
37:54 - of apparent that we shouldn't be fitting
37:56 - both of these columns together because
37:58 - what would the model interpret would
37:59 - they both get the same coefficient
38:02 - uh but typically what what you will see
38:04 - the model will do is it will assign a
38:06 - coefficient that's very close to zero
38:09 - um to one of the columns um to like deal
38:12 - with the redundant information
38:14 - but we're going to see that this is
38:16 - going to have
38:17 - uh some mathematical problems um
38:20 - with our matrix algebra and yeah
38:25 - yeah exactly so um
38:28 - right in the same way when we talked
38:30 - about categorical variables and we
38:32 - talked
38:33 - about how um if you have
38:37 - three categories and you want to dummy
38:40 - code those variables
38:42 - then in your model you're only going to
38:45 - end up including
38:46 - two dummy coded variables because the
38:48 - third
38:50 - the third dummy coded variable doesn't
38:52 - provide any more information like you
38:54 - can figure out the values of that
38:56 - column in that column from the other two
38:59 - if you know there are exactly three
39:00 - categories
39:02 - and and we'll show that as well in a
39:04 - moment but
39:05 - in this example the seconds to the
39:08 - subway column
39:09 - and the minutes the subway column
39:11 - provide exactly the same
39:13 - information about an apartment so
39:16 - um i think we theoretically
39:21 - can fit this model um
39:25 - so let's fit the model uh
39:28 - that has minutes to subway and seconds
39:32 - the subway
39:32 - in it along with the other predictors
39:35 - that we had before
39:37 - um and i guess actually this let me look
39:40 - nicer on here if i don't use print i
39:42 - think
39:43 - um cool
39:46 - so uh let's take a look
39:50 - it looks like exactly what nachia said
39:53 - so
39:54 - we've got the coefficient or the slope
39:56 - on min to subway
39:58 - is really really small and then
40:01 - um the coefficient on seconds of subway
40:05 - is also pretty small um
40:09 - and we can compare that actually to our
40:12 - initial model so
40:17 - in our initial one we had
40:20 - a coefficient of negative 25
40:24 - um on minutes subway
40:27 - which makes a lot more sense if our goal
40:30 - is to interpret this right like as an
40:32 - apartment gets farther away from the
40:34 - subway
40:34 - we probably are paying less money so um
40:38 - for every minute right we would
40:41 - interpret that as like
40:42 - for every minute further away it is um
40:45 - we're paying roughly
40:46 - 25 less in rent holding all other
40:49 - um all of their predictor is constant
40:53 - so now this original
40:57 - pretty sensible uh coefficient
41:02 - is now replaced by it's kind of like
41:05 - nonsensical we've got two negative
41:07 - coefficients they're like
41:09 - both much smaller and
41:12 - they're it seems like we can't really
41:14 - represent this information at all
41:16 - i wonder if the seconds to subway
41:19 - i think the second subway coefficient
41:21 - still makes sense in that i think
41:23 - there's like a factor of 60 there or
41:25 - whatever
41:25 - i guess oh yeah that sort of makes sense
41:28 - yeah
41:29 - it's still dangerous i mean that like
41:30 - yeah we shouldn't be doing this
41:32 - ideally yeah no you're totally right
41:35 - so what natia
41:38 - was saying which is which makes a lot of
41:40 - sense right is that like
41:41 - because the values in this column are
41:44 - going to be larger
41:45 - um we expect this coefficient to be
41:48 - smaller
41:49 - so if we multiply like 0.4 by 60
41:52 - we'll get like 24
41:56 - 20 like this will be roughly 25 negative
41:58 - 25
41:59 - which is similar to what um
42:02 - what our original coefficient was for
42:05 - minutes
42:06 - um so this is roughly the same
42:09 - relationship that we modeled in the
42:11 - first equation it's just that
42:13 - now we've got two things in the model
42:15 - where one of them basically has a
42:16 - coefficient of zero and all of the
42:19 - explanatory effect of uh minus the
42:22 - subway that we are representing in that
42:24 - first
42:25 - equation is now going to be attributed
42:26 - to this new column that we created
42:33 - um and then at the bottom we see there
42:35 - are some warnings so
42:37 - this warning was in our original model
42:39 - but now we get this
42:40 - additional warning that says the
42:42 - smallest eigenvalue
42:44 - is 1.18 times e to the negative 27.
42:48 - so this is really this is scientific
42:49 - notation so this number is like
42:53 - 0.0000 with
42:55 - 26 zeros and then one one eight
42:58 - um and then it says this might indicate
43:00 - that there
43:01 - are strong multi collinearity problems
43:04 - on or that the
43:05 - design matrix is singular um
43:08 - and do you wanna like talk a little bit
43:10 - about what this
43:13 - uh what their morning means um
43:16 - so actually it's interesting that um
43:20 - uh it actually when i was writing uh
43:21 - this uh article it was interesting to me
43:23 - that stats model just didn't like throw
43:25 - up an
43:25 - error and refuse to do the calculation
43:28 - because if you actually did paper and
43:30 - pen math
43:30 - um like we showed with numpy with the
43:33 - different matrices
43:34 - uh we're not going to be able to get an
43:36 - answer because
43:38 - um as it turns out when two columns or
43:41 - rows of a matrix
43:42 - um are exact multiples of each other uh
43:45 - that math doesn't work because uh it's
43:48 - what's called a singular matrix
43:50 - um it has eigenvalues that are very
43:52 - close to zero
43:54 - um and it just refuses to
43:57 - compute basically but i think the reason
44:00 - stats model still goes through with it
44:01 - is because
44:02 - because of scientific precision because
44:04 - uh with computers you can still find
44:06 - ways around this
44:08 - um but yeah so basically multi-color
44:11 - culinarity literally means that when
44:12 - it's to subway
44:13 - seconds to subway just differ by a
44:15 - factor of 60
44:17 - when a matrix has two almost identical
44:19 - columns or columns that are multiples of
44:21 - each other
44:22 - it's determinant doesn't exist or it's a
44:24 - singular matrix
44:25 - so we can't calculate those correlations
44:28 - um yeah yeah i'm trying to grab
44:32 - uh grab this code to do exactly what you
44:35 - said
44:36 - um i think actually when i was getting
44:39 - ready for
44:40 - this um
44:43 - this live stream and i was trying to
44:48 - run this with numpy i think it actually
44:51 - calculated
44:53 - it for me and numpy but it yeah
44:56 - i still think it's got to do with
44:57 - floating point precision and i think
44:59 - that's why
45:00 - um this is so important to keep in mind
45:02 - because
45:03 - i think it's not going to throw an error
45:04 - now but if this is part of your larger
45:06 - calculation and you keep to go
45:07 - keep going with it um it's going to
45:09 - cause serious problems to be able to
45:11 - catch that error is so useful
45:14 - right
45:17 - we'll give it a try so um
45:21 - if we take
45:24 - this oops
45:30 - i'm messing up this uh
45:33 - this whole thing um i think this is
45:37 - what i was
45:40 - okay so we've got now we're
45:42 - recalculating
45:44 - um our x matrix with this equation that
45:47 - has seconds to subway in it
45:50 - let's wash our dryer then we're gonna um
45:55 - we're gonna have to switch this to a
46:01 - numpy
46:04 - array so that it's a matrix and then
46:08 - so we can use numpy functions on it
46:12 - and then um
46:16 - print the betas
46:23 - call it seconds or something
46:31 - okay right we it does
46:35 - do this calculation but we get
46:38 - completely different numbers
46:40 - um even more nonsensical than before
46:43 - because all of these are multiplied
46:44 - by 10 to the like 20th or something like
46:47 - that
46:48 - so we end up with like this is
46:51 - negative 4 bazillion i have no idea what
46:55 - that would be it would be like
46:56 - four one one seven all of these numbers
47:00 - and then like
47:01 - whatever 15 more zeros or 13 more zeros
47:05 - um on the end of that
47:08 - and so we end up getting this really
47:10 - weird thing where
47:12 - uh we can't calculate like nichia said
47:15 - we can't calculate
47:17 - these coefficients if we and if we have
47:20 - um a situation where two
47:24 - of the predictors are contributing
47:26 - exactly the same
47:27 - information um i saw there's a question
47:30 - in the chat
47:31 - in theory would that cause problems when
47:32 - finding the coefficients but there
47:34 - happens to be a strong correlation
47:37 - unknown to the user between two
47:38 - predictors so that's a
47:40 - actually really good question so the
47:43 - answer is yes
47:44 - um so if you have a perfect
47:48 - collinearity between two
47:52 - basically a perfect correlation equal to
47:54 - one
47:55 - between two predictors then
47:58 - you can't even fit the model but if you
48:01 - have
48:02 - two predictors that are highly
48:04 - correlated it
48:06 - then you also are going to
48:10 - encounter some issues especially around
48:14 - interpreting coefficients because if
48:18 - two predictors are contributing mostly
48:21 - the same information even if it's not
48:23 - exactly the same information
48:26 - it won't prevent you from from
48:28 - calculating the inverse of a matrix
48:31 - so it won't prevent you from finding a
48:34 - solution
48:34 - theoretically to that matrix equation
48:37 - but it will
48:38 - mean that it kind of becomes arbitrary
48:41 - which
48:42 - predictor the coefficient gets
48:46 - linked to um so you might have a
48:49 - predictor in your model
48:50 - that has a coefficient that's really
48:53 - small
48:54 - maybe even smaller than this but it's
48:56 - actually
48:57 - a an important predictor of the outcome
49:00 - variable and
49:02 - um like right now nitia is working on
49:05 - some content on feature engineering
49:07 - which is really the process of trying to
49:09 - choose a subset of features that you
49:11 - might want to include in a model
49:13 - and one of the issues or like one of the
49:17 - things you might do right is you might
49:20 - fit a model with all the predictors and
49:21 - then try to figure out which ones are
49:23 - the most
49:23 - important ones based on these
49:25 - coefficients because they tell you
49:26 - something about
49:27 - how much um effect
49:31 - i i shy away from using the word effect
49:33 - but like how much impact on the outcome
49:35 - variable
49:36 - each predictor has um
49:39 - but if you have two highly correlated
49:42 - predictors
49:43 - then you might have one of them with a
49:46 - coefficient of zero even though it's
49:48 - really highly related
49:49 - to the outcome um also
49:53 - yeah so c of j says when features are
49:57 - linearly dependent we are in the
49:59 - situation similar to divide by zero in
50:01 - one dimension
50:02 - just my two cents and that is exactly
50:04 - correct like
50:05 - there we just can't calculate it like
50:09 - you can't divide by zero you can't
50:11 - calculate the inverse of a matrix when
50:13 - you have
50:14 - co-linearity perfect linear co-linearity
50:18 - um so this
50:21 - comes back to all comes back to another
50:24 - thing that's worthwhile to do
50:26 - before you fit these models is um
50:30 - it's it's helpful to look at the
50:32 - correlation matrix
50:34 - to see how highly correlated
50:37 - each of the predictors are so let's
50:40 - actually
50:40 - um go ahead and
50:44 - i'm gonna like create a new column
50:49 - and then i think we can do um
50:54 - did i load seaborn
51:01 - see if i can remember how to do this
51:05 - but if i do sns.key
51:10 - map of bk
51:13 - dot forum i think
51:17 - that might work no
51:21 - let's um let's get the correlation
51:24 - matrix first
51:26 - is it just one r maybe
51:30 - no it seems to be two r's
51:36 - um
51:40 - let's get this is this is fun
51:43 - um heat map function
51:56 - let's try that
52:06 - um
52:11 - do you have any chance to remember nitia
52:13 - how to do this
52:14 - i think you might need to close the i
52:17 - have a
52:18 - the the syntax the closed parenthesis
52:20 - after that i think
52:22 - uh yeah i think so oh yeah
52:26 - nice um okay so
52:29 - this just helps us visualize this a
52:32 - little bit better
52:33 - but we see the coloring is not great in
52:35 - this
52:36 - so perfect correlation would be this
52:38 - like very light color
52:40 - right and everything is perfectly
52:42 - correlated with itself
52:43 - but also min to subway and seconds to
52:46 - subway
52:47 - are perfectly correlated um but this
52:49 - gives you a sense of how highly some of
52:51 - the other features
52:53 - are correlated i think if i do like
52:55 - endnote
52:56 - equals true it'll give me the actual
53:00 - numbers on top
53:02 - um but we see right that some of these
53:04 - other ones are pretty highly correlated
53:06 - like
53:07 - uh size square feet
53:11 - and um and rent are pretty highly
53:15 - correlated in that case rent is our
53:16 - outcome variable so that's not
53:18 - actually that's actually a good thing
53:20 - means it's a really good predictor
53:22 - probably a very good predictor in this
53:24 - model for rent
53:25 - um but some of these other ones like
53:27 - size square
53:28 - feet i guess the next biggest one is
53:31 - size square feet
53:32 - and bedrooms um which makes sense like
53:36 - something that a apartment that has more
53:38 - bedrooms is probably also going to be
53:40 - bigger
53:42 - something like that has a reasonably
53:45 - large
53:46 - correlation where we might be a little
53:48 - concerned i think
53:49 - 0.68 is okay um
53:53 - but maybe like over i think once it gets
53:56 - over about like
53:58 - .75 for me is where
54:01 - i start to think like maybe these two
54:04 - predictors should
54:05 - be in the model depending on depending
54:07 - on what i'm trying to
54:08 - um trying to fit a model for
54:13 - cool um okay
54:17 - we've got about five more minutes i
54:19 - think another
54:20 - um another useful thing to just see that
54:24 - no tia also covered in that article um
54:26 - and i highly recommend that you all
54:28 - check out the article
54:29 - as well if you're interested um
54:32 - is just to demonstrate what happens
54:35 - if you instead uh if you instead
54:39 - use a categorical variable
54:42 - where you have like two
54:46 - columns or enough columns
54:49 - that there is no reference category
54:52 - so in this case we can
54:55 - create another another column
54:58 - in our data along the same lines as
55:02 - before
55:04 - but this column instead tells us whether
55:07 - there is no washer dryer so remember in
55:10 - our original data
55:11 - we had um washer
55:14 - has washer dryer as a
55:18 - as a variable as a feature and it was
55:21 - one if there is a washer and dryer and
55:24 - zero if there's not
55:25 - and if we create a separate column
55:29 - called no washer dryer um
55:32 - then we essentially
55:37 - have replicated that same information
55:40 - kind of in reverse so i'll just print
55:42 - this out really fast
55:45 - so we can see it it'll give me the
55:47 - warning but it'll do what i want anyway
55:50 - um so
55:53 - if something has a washer dryer
55:56 - or it'll be a one in this column and
55:58 - it'll be a zero otherwise
56:00 - all of these have the value zero for
56:03 - house washer dryer
56:05 - um and so it's true therefore that they
56:08 - have
56:09 - no washer dryer um and then let's see if
56:12 - we can find
56:14 - like um
56:17 - it's like print the whole thing this is
56:20 - what i like about
56:22 - uh this
56:25 - okay so we've got um
56:29 - here we've got has washer dryers equal
56:32 - to one
56:33 - because it has a washer dryer and so
56:35 - this column says
56:36 - false because it's false that there's no
56:40 - washer dryer
56:42 - um so basically right like everything
56:44 - every value in this column could be
56:46 - determined
56:47 - by the value of this column so it's
56:49 - almost like
56:51 - the same thing as before um you can't
56:53 - calculate the correlation between two
56:55 - categorical variables
56:56 - but they are not
57:00 - once again there is multi-collinearity
57:03 - in that like
57:04 - all of the information in this column is
57:07 - also contained in this column
57:09 - and again if we use that to
57:12 - fit a model so
57:16 - here we're fitting this model um
57:19 - we get some warnings we get this warning
57:23 - that the smallest eigenvalue
57:25 - is super small this might indicate
57:28 - there's
57:29 - strong collinear um that there are
57:31 - strong multicollinearity problems or
57:33 - that the design matrix is singular
57:35 - um and all we did here was we included
57:37 - both
57:38 - has washer dryer and no washer dryer
57:41 - into
57:42 - our model cool
57:45 - um cool so i hope that this like helped
57:48 - motivate a little bit why understanding
57:51 - this matrix representation
57:52 - is important um i think another thing
57:56 - to demonstrate really quickly or that i
57:58 - think is worthwhile to demonstrate
58:00 - really quickly is
58:02 - a lot of people prefer to run
58:05 - linear regressions or any sort of model
58:07 - fitting in scikit-learn
58:09 - because scikit-learn has a lot of other
58:12 - functionality that sats models does not
58:14 - although
58:15 - i find it's harder to look at the output
58:18 - and really understand what's going on um
58:21 - but
58:22 - for example if you want to fit a model
58:24 - in second learn um
58:25 - it's helpful to know that
58:29 - this like matrix representation is
58:32 - something that you need to kind of do
58:33 - yourself
58:34 - so for example um when we fit a model
58:38 - with squirn
58:39 - uh and this is the code to do it in this
58:42 - uh in this jupyter notebook that you can
58:44 - download on your own
58:45 - um there is some info
58:49 - so we're going to have to create this
58:51 - like x and y matrix
58:52 - by ourself um and so if we do that
58:56 - we end up right like taking the columns
58:59 - that we want
59:00 - out of the original data and and putting
59:03 - them in their own
59:04 - matrix and then separating out the y
59:06 - matrix as well
59:08 - in this case that's just the rent column
59:10 - and then we fit the model
59:12 - um but then scleron is
59:15 - automatically adding that um
59:19 - that column of ones behind the scenes so
59:23 - we are not adding that here
59:26 - um ourselves but there are other things
59:28 - that we do have to add so like if we
59:30 - have a
59:30 - um if we have three categories
59:34 - in a categorical variable and we want um
59:36 - to choose our reference category we're
59:38 - gonna have to like
59:40 - put those as columns in our x matrix
59:44 - and leave one out intentionally it's not
59:47 - going to be done
59:48 - automatically for us um so
59:52 - i think it's it's helpful to think about
59:54 - it in this context so that you can think
59:55 - about like
59:56 - what is happening in this
59:59 - library that's not happening or
60:02 - happening behind the scenes in this
60:04 - library
60:05 - and it can enable you therefore to learn
60:08 - a new technology like or learn how to
60:11 - fit a model in r for example and debug
60:14 - that
60:15 - so you get a little bit more insight
60:18 - into what's going on
60:21 - cool um awesome
60:25 - well we are just about out of time i
60:28 - know this was maybe a less exciting
60:30 - uh topic than usual but
60:34 - i hope that it was helpful at least to
60:36 - see a little bit of this math
60:38 - um and i promise the rest of this will
60:40 - be a little less snappy
60:41 - in um in the uh
60:45 - in the future i see there's a um
60:50 - never do math in a youtube video rule
60:53 - maybe that was
60:54 - maybe we should have followed that rule
60:55 - i don't know
60:57 - um but oh well we didn't
61:01 - so hopefully it didn't scare anyone away
61:03 - it is
61:04 - i think helpful to see um awesome
61:08 - oh this is great so there was a question
61:10 - in the chat about
61:11 - office hours this week um so
61:14 - there are going to be office hours this
61:16 - week but we're going to do them on
61:18 - discord for the first time ever so
61:22 - that is going to involve um that we're
61:25 - gonna have a
61:26 - like a chat where you can type
61:29 - and then we're gonna also have uh video
61:32 - and sound
61:32 - available and so if you have not already
61:36 - joined our discord server
61:38 - we will we'll put that information
61:41 - somewhere
61:42 - we'll put it in the youtube description
61:46 - and also the event which you can find
61:50 - at codecademy.com
61:55 - events oh there's not an event
61:59 - never mind it's not going to be there um
62:01 - we'll put it in the
62:02 - the youtube description so you can find
62:04 - it but come join us on discord
62:06 - happy to answer any questions we also
62:08 - have a chat there you can
62:09 - post questions in there at any time um
62:12 - and you can also post questions on the
62:13 - youtube video directly and we'll
62:15 - keep an eye on it and try to make sure
62:17 - that we answer things awesome
62:21 - all right thank you all i hope everyone
62:24 - has a great rest of their
62:25 - day and thank you for joining me on this
62:28 - math
62:28 - adventure thank you this was so fun

Cleaned transcript:

oh man i think we are live hello everyone um let us know in the chat if you can see us i think uh we should be live by now but it's always good to just see some confirmation we're good we have confirmation um all right so we're gonna get started today and um talk about some of the math behind linear regression but before we get started i want to introduce my uh cohost for the day natya she this is her first live stream so give her a warm welcome um she also yeah and she also wrote the article on codecadme on this topic so um it was one of her first her first content items it went live on the site so it's really exciting that she's here and she can also help us walk through everything um and i am live actually from the code academy office and i even have alex kuntz sitting behind me so it's almost like the pandemic is almost over maybe in this little bubble um cool so we're gonna get started um i'll share my screen and we can jump right into it all right so um all of this code just so you know and all of the math stuff is written out in a file that is on our github so there's going to be lots of notes today don't feel like you have to write everything down it's all available to you there um what i'm gonna do also is just make this a little bigger cool so um as i said today's focus is really on some of the math behind linear regression and just to make sure nobody feels intimidated by this we're not going to get into super complicated equations or anything like that what i mean by math is i want to cover some of the ways that we represent the linear regression problem using mathematical language or notation um and the reason why it's important is because whenever you use any sort of software to fit a regression or fit any model for that matter there's that function is doing something behind the scenes that you're not seeing and it's really hard to figure out what's going wrong if things go if things do go wrong which they often do in the real world it's really hard to figure out what's going on if you don't have at least some sense of what's happening when you press run and so today what we're going to do is we're going to dig in a little bit and see a little bit of what it looks like behind the scenes when we run a linear regression caveat is that depending on what package you're using what function you're using the way that this these coefficients are calculated might vary and that's actually part of your decision making as a data scientist or analyst is like how am i going to fit a model but we're going to focus on the stats model's implementation and in the process we're going to talk a little bit about matrices and how we can represent this problem as a matrix equation so as an example we're going to go through some some data where we need some data from street easy again if you've tuned in before you've probably seen us use this data in the past and it's got a bunch of information about apartments in new york city with rent and some information about the apartments themselves so the number of bedrooms bathrooms the size minutes the subway building age whether or not the apartment has a washer and dryer this is a subset of the original data but this is what we're going to work with today so we're going to imagine that we want to fit a regression to predict rent using all of these predictors cool so here's where we're going to return to some math and i'm actually i've got my ipod all set up so i'm going to switch over to the ipad so we can write this out um and and get started let's see if we can get this to work and i want to remind everyone that if you have any questions feel free to go ahead and post them in chat this is going to be a pretty detailed uh discussion that we're having today so if you have any any questions at all or you want to back up and start something over again just let me know and also lightning um okay so here is our uh our data i've pasted it here so that we can take a look at it and this data set has already the makings of what we would call a matrix right so for anyone who needs a refresher who hasn't seen this before a matrix is just basically like a rectangular grid of values um and those values can be anything and so we're going to represent this using a matrix as well um so let's remind ourselves what we're trying to do when we fit a linear regression so remember that our initial um our initial equation of a line was y equals mx plus b that's what we started with and so we wanted to fit an equation that looks something like this rent equals m times and then let's pick a a predictor i guess i think the most obvious one is size square feet so let's do size square feet plus b plus error and remember that this is the slope so we want to calculate the slope this is the intercept and then this is our error how far off the true rent is from our predicted rent in in the context of our line rate this is this is all we need to plot the regression line but this error term is here because our regression line doesn't perfectly predict all of the data points and what this what this equation actually represents is a whole bunch of equations one for every row in our data set so we've got for example 3600 and this is the first row of our data 3600 equals and then the psi square feet is 900 so m times 900 plus b plus error one and then we've got 3 900 equals m times 1000 plus b plus error two and we've got a bunch of equations like this each one with their own error term and that is how far off our prediction is based off of our model and our goal remember for ols regression is to minimize the sum of the square the squared errors so when we run a regression we want to try to find the value of m and b that works for all of these equations right so we want to find value of m and b such that the sum of the squared errors so error 1 squared plus error 2 squared oh my gosh can't even spell error 2 squared plus error 3 squared and so on all the way up to however many data points there are so that that sum is minimized and that's that's the whole regression problem so let's see if we can represent all of these equations using some matrices so i'm gonna erase everything and we're gonna start trying to build up this matrix all right so we've got our rents oh that is and we can put them in a single column matrix so 3 hundred first one thirty nine hundred twenty seven hundred forty nine hundred thirty nine hundred and then we want those to be equal to our slope times in this case all of the size square feet value so 900 1 900 12 16 1100 now we want to add so remember in our equation right it's like 3 600 equals m times all of these numbers plus b but we want the b's to kind of get added to every single equation so in order to do that but without writing the letter b a bunch of times what i'm going to do is i'm going to write it as the letter b times a bunch of ones so this is this term right here is the same as just writing out b b b b because for matrix multiplication if we multiply by a scalar then we just multiply that scalar by every single value in the matrix and then we've also got our errors over here but it turns out right so this is a pretty ugly way to write this out so we can actually make it a little bit simpler by combining this matrix and this matrix together so let us give that a try sorry guys seems to be not a racing we can combine the ones into this matrix and make it a 5 by or in this case 5 by 2 matrix all at once so we can put the ones in here and then we can sorry i don't know why i haven't been able to get this or we're using it to work properly and then we can combine the m and the b into their own matrix as well the reason that this works is because of how i'm sorry b and then i'm doing this wrong i'm sorry uh nitya do you want to catch my mistake or do you know what i'm doing wrong yes um it's it's bro multiplies column right right um yeah so it has to come um after the first matrix yeah all right now we've got it okay the reason this works is because of how matrix multiplication works so when we multiply matrices what we do is we row we multiply each row by a by the corresponding column in the multi in the second matrix so the first thing that we would do if we were multiplying these two matrices is multiply this row by this column and add each pair of terms so we would do right 1 times b plus 900 times m is our first set of our first equation that comes out of this matrix multiplication the second pair is the second row by this column so that's going to be equal to 1 times b plus 1000 times m and then we just continue down the line so for each row we're going to multiply by this column and add up the add up the terms so we end up with b plus 900 m for the first one because one times b is just b by itself and for the second one b plus a thousand times m and then b plus 12 16 times m and in every case we end up with this mx plus b format although it's written in reverse but it doesn't matter what order we multiply in okay this gets slightly more complicated but not too much more complicated when we switch over to multiple linear regression so remember in multiple linear regression we can have even more predictors and we can have even more and then because we have more predictors we have to have even more coefficients so i'm gonna just write this out but if we wanted to do a multiple linear regression with bedrooms and bathrooms and all of our other uh features in there as predictors we can just add them to this matrix so we can say okay i want bedrooms as a predictor so i'll add three three two one zero for um bedrooms and then bathrooms is two two one one um and then min to subway is four four four six three and so on um i can add all of my predictors i won't add them all right now to write it out and then i can have an entire column here column matrix here where i have all of my thetas so my b0 b1 b2 b3 and b4 and these are just the coefficients on each of these predictors when we fit the model and you can see that if we write this all out i know this is like getting very probably very boring at this point but i promise we'll get back to the the fun part in just a moment right if we multiply each of these out we get a separate equation for each row of our our original data that follows the pattern that we wanted we've got in our first row all of these values multiplied by all these values and so and then terms are added up so we get something like 1 times b 0 which is just b 0 plus 900 times b1 plus 3 times b2 plus 2 times b3 plus 4 times b4 and then plus error 1 at the end and this is our regression equation right 3600 equals all of this plus error is our regression equation for the first row of our data and so on um then we do the same thing for the second row cool hopefully i have not lost everyone at this point um we're gonna go back to to coding this up but i think just seeing this written out is useful so that you can build off of what you see in the code so when you stop did you want to add something just that b0 is the new intercept and like that the rest of the b ones too whatever are your new slopes exactly yeah so um right so when we talked about multiple linear regression we talked a little bit about this but it's worth coming back to because i think the notation can be a little bit confusing here um so yeah just like nichia said when we talk about multiple linear regression we rename the intercept b0 instead of b and then we instead of using m for our slope we use b1 through b whatever because that way we can use the same letter to represent all of the coefficients and just kind of number them cool so um let's go ahead and take a look at what this actually looks like when we fit a model so i'm going to go ahead and run this and here's our data again and you might remember this from when we talked about categorical predictors uh but this is the this patsy.d matrices what this does is it takes the step it looks like uh the screen share is freezing yeah uh or maybe the window is just like really short i don't know i just wanted to let me reshare screen this happened once before when i switched from the ipad to the yeah it looks perfect now okay great um cool so um so essentially right the patsy d matrixes the matrices function happens inside stats models uh from formula ols function and we can use the same the same formulas that we use in in that function in this to see a middle step that happens when we run a regression so i'm going to do this really quickly and i'm going to use this uh this equation so rent is our outcome variable and then building age years and min to subway has washer dryer our um predictors for right now because i just don't want it to take that the whole space so you can see it a little bit more easily but you could include all of the predictors in this model and we're going to return this as a data frame so that we can just see these labels and when we print this x matrix what we get is exactly what we saw in that matrix that we just created so we've got an intercept and then in order to fit the intercept we have this column of ones right because that's how we're going to get that beta0 or b0 or intercept whatever you want to call it in every single equation right so we can imagine that we've got our column of b0 b1 b2 over here and when we multiply this whole row by that column we're going to get a 1 times b 0 here for the first equation and a 1 times b 0 for the next equation and so on so that's why we have this column of ones and then we've got each of our predictors as a separate column in this matrix we also have um as part of this this function right we have this column vector of y's and we'll print them out and that's just our outcome variable that's the run so these are really the x's and y's that are from this equation in this case the y represents this whole column vector of outcome variables the x capital x represents this whole matrix that has a column of ones and then a column for each predictor and then what we're trying to estimate is this column of b03 through bm where m is the number of predictors so here's a better a simpler way to write it right so this is like this equation is the same as this equation just replacing this with a small y small x a b and an epsilon for the errors and so what we're calculating in the d matrices function is just this y and this x so we see that under the hood this regression equation is being represented essentially by a matrix and we saw that a little bit before we talked about um categorical predictors as well so now let's go ahead and let's take a look at um at the model summary oh i need to remind myself what we did um this was actually some very helpful uh code that nitia wrote so normally when we fit a model so i'm gonna just like jump back for a second so in the past we've done this like sm.ols got from formula i think it's like is that right you can grab the you can grab the code from one of the previous lessons yeah so we've got from formula and then we give it our formula and then our data okay and then when we print this out um so this is our that's our model and we're just gonna fit it on all one step and when we print out um model.summary we get this really big see did i miss something oh no oh i put this in quotes there we go okay so we get this like very big output that has a lot of information that we have to parse through and so um at the bottom of all this we've got so at the top we've got some information even about like the time that i ran this model and then some information about the model itself um and then we've got our coefficients which we can also get by just looking at the params attribute and then at the very bottom i've got some warnings and so we're going to look at in a in a moment we're going to look at an example where we get some more warnings but so far we just got standard errors assume that the covariance matrix of errors is correctly specified okay we'll come back um so here this is just some nice code nitia wrote it to print out just the parameters so just this part the coefficients for the intercept and each of these each of these predictors and then also print out the warning so that we can see it um just to demonstrate that we can do everything that stats models did behind the scenes um i think it's also useful to take a look at how we would take the matrix representation and actually solve this equation for the betas that or the i keep calling them betas but the the coefficients right the b0 b1 b2 etc so coming back to this equation remember that this was our simplified equation using the matrix form where y is our column matrix x is our matrix of predictors and this beta which corresponds to this whole column are the things that we want to we want to calculate such that the this set of error terms squared and added all together is minimized and we can calculate the the values of b0 b1 b2 etc that minimize the errors by using a little bit of calculus and the calculus that we can implement and i'm not going to go through the math it's a little bit complex but it gives us this fancy equation for the betas or the b0 b1 b2 etc and this is a lot of matrix algebra that you don't need to know for the purposes of this but this t represents the transpose of a matrix which is basically like flipping the rows and columns um so something that was in the first row is now in the first poll and something that was in the second rows now in the second column um and so off and so forth this is an inverse so we're taking like the transpose of our x matrix multiplying it by the original x matrix taking the inverse of the result and then multiplying it by the transpose of the x matrix and then multiplying that by our y matrix complicated math don't need to understand it for the purposes of this but it is kind of cool to see that that works so you can actually in um in numpy you can create so in this case this x matrix is really just it's the same thing that we got here we just got rid of all the labels and all the indices so it's just the ones the 15 896 all of that condensed down into just this like list of lists it looks like a list of lists basically but it's a um a grid of numbers just like the matrix we saw before and we can use our um we can use some functions in numpy to take the transpose of the matrix find the inverse of um of a matrix we can perform matrix multiplication in numpy and when we do all of that we implement that formula on our x matrix and i think i did not run this we end up getting this matrix of betas and notice that the first one is like 369 6.17 and that's the same as we got for the intercept then we've got negative 5.465 um negative 5.465 negative 25 and 719 and um and go back here negative 25 and 719. so we can actually calculate all of these things once we've written it as a set of matrices we can go back and we can actually perform that math or like use that equation to solve for these numbers okay so now let's go back and let's create some problems um so this is again some code from nitya going to create a new um a new column in our matrix or a new column in our data called seconds to subway and the way that we're going to get the seconds to the subway is we're going to multiply it multiply the minutes to subway column by 60. so actually let's like break this up into a couple of steps so i'm gonna run this and i'm gonna print this back out oh see i get so confused i don't know if anybody else has trouble with this there's all this uh hoopla about like how you are supposed to write onto a column of a matrix i don't know nitya if you have any advice i i have struggled with this every single time i thought that i managed to avoid the the error by doing it this way and clearly i have not oh man pandas um i wonder if we should just write our lambda function that's my usual i mean i think it works so like this gives me all of the all these errors so like yeah just so that everybody sees and then i think there it gives you this um this explainer which i always click on and then i never actually read through it um which at some point i will uh but it i like updated literally a year ago or something and it's just been hard to like keep track yeah um yeah like all i'm trying to do basically in the like simple way the old way that i would have done it is just said like take the create a new column called sex to subway i would just use the dot operator you can do it with like uh um you can do it this way as well right and then just have that set that equal to the min to subway column times 60 um and yeah that gives the same error it still works i think at some point they're going to take away that functionality and i'm never going to be able to figure out how to do it without a lambda function um anyway but what we created was this new uh this new column that tells us the number of seconds it takes to get to the subway so here if something took four minutes or an apartment was four minutes away from the subway it's 240 seconds away from the subway because four times 60 is 240 and if it's 6 minutes away it's 360 seconds away from the from the subway so nichia do you want to take this one do you want to explain why this is a problematic column sure i mean i think for it's kind of like a little um apparent right that we have the same information in both of these columns which is um time taken to the subway um and not only are these two uh columns highly correlated they're in fact collinear which is they're literally um they literally differ from each other by a constant factor right um and i think i think it's sort of apparent that we shouldn't be fitting both of these columns together because what would the model interpret would they both get the same coefficient uh but typically what what you will see the model will do is it will assign a coefficient that's very close to zero um to one of the columns um to like deal with the redundant information but we're going to see that this is going to have uh some mathematical problems um with our matrix algebra and yeah yeah exactly so um right in the same way when we talked about categorical variables and we talked about how um if you have three categories and you want to dummy code those variables then in your model you're only going to end up including two dummy coded variables because the third the third dummy coded variable doesn't provide any more information like you can figure out the values of that column in that column from the other two if you know there are exactly three categories and and we'll show that as well in a moment but in this example the seconds to the subway column and the minutes the subway column provide exactly the same information about an apartment so um i think we theoretically can fit this model um so let's fit the model uh that has minutes to subway and seconds the subway in it along with the other predictors that we had before um and i guess actually this let me look nicer on here if i don't use print i think um cool so uh let's take a look it looks like exactly what nachia said so we've got the coefficient or the slope on min to subway is really really small and then um the coefficient on seconds of subway is also pretty small um and we can compare that actually to our initial model so in our initial one we had a coefficient of negative 25 um on minutes subway which makes a lot more sense if our goal is to interpret this right like as an apartment gets farther away from the subway we probably are paying less money so um for every minute right we would interpret that as like for every minute further away it is um we're paying roughly 25 less in rent holding all other um all of their predictor is constant so now this original pretty sensible uh coefficient is now replaced by it's kind of like nonsensical we've got two negative coefficients they're like both much smaller and they're it seems like we can't really represent this information at all i wonder if the seconds to subway i think the second subway coefficient still makes sense in that i think there's like a factor of 60 there or whatever i guess oh yeah that sort of makes sense yeah it's still dangerous i mean that like yeah we shouldn't be doing this ideally yeah no you're totally right so what natia was saying which is which makes a lot of sense right is that like because the values in this column are going to be larger um we expect this coefficient to be smaller so if we multiply like 0.4 by 60 we'll get like 24 20 like this will be roughly 25 negative 25 which is similar to what um what our original coefficient was for minutes um so this is roughly the same relationship that we modeled in the first equation it's just that now we've got two things in the model where one of them basically has a coefficient of zero and all of the explanatory effect of uh minus the subway that we are representing in that first equation is now going to be attributed to this new column that we created um and then at the bottom we see there are some warnings so this warning was in our original model but now we get this additional warning that says the smallest eigenvalue is 1.18 times e to the negative 27. so this is really this is scientific notation so this number is like 0.0000 with 26 zeros and then one one eight um and then it says this might indicate that there are strong multi collinearity problems on or that the design matrix is singular um and do you wanna like talk a little bit about what this uh what their morning means um so actually it's interesting that um uh it actually when i was writing uh this uh article it was interesting to me that stats model just didn't like throw up an error and refuse to do the calculation because if you actually did paper and pen math um like we showed with numpy with the different matrices uh we're not going to be able to get an answer because um as it turns out when two columns or rows of a matrix um are exact multiples of each other uh that math doesn't work because uh it's what's called a singular matrix um it has eigenvalues that are very close to zero um and it just refuses to compute basically but i think the reason stats model still goes through with it is because because of scientific precision because uh with computers you can still find ways around this um but yeah so basically multicolor culinarity literally means that when it's to subway seconds to subway just differ by a factor of 60 when a matrix has two almost identical columns or columns that are multiples of each other it's determinant doesn't exist or it's a singular matrix so we can't calculate those correlations um yeah yeah i'm trying to grab uh grab this code to do exactly what you said um i think actually when i was getting ready for this um this live stream and i was trying to run this with numpy i think it actually calculated it for me and numpy but it yeah i still think it's got to do with floating point precision and i think that's why um this is so important to keep in mind because i think it's not going to throw an error now but if this is part of your larger calculation and you keep to go keep going with it um it's going to cause serious problems to be able to catch that error is so useful right we'll give it a try so um if we take this oops i'm messing up this uh this whole thing um i think this is what i was okay so we've got now we're recalculating um our x matrix with this equation that has seconds to subway in it let's wash our dryer then we're gonna um we're gonna have to switch this to a numpy array so that it's a matrix and then so we can use numpy functions on it and then um print the betas call it seconds or something okay right we it does do this calculation but we get completely different numbers um even more nonsensical than before because all of these are multiplied by 10 to the like 20th or something like that so we end up with like this is negative 4 bazillion i have no idea what that would be it would be like four one one seven all of these numbers and then like whatever 15 more zeros or 13 more zeros um on the end of that and so we end up getting this really weird thing where uh we can't calculate like nichia said we can't calculate these coefficients if we and if we have um a situation where two of the predictors are contributing exactly the same information um i saw there's a question in the chat in theory would that cause problems when finding the coefficients but there happens to be a strong correlation unknown to the user between two predictors so that's a actually really good question so the answer is yes um so if you have a perfect collinearity between two basically a perfect correlation equal to one between two predictors then you can't even fit the model but if you have two predictors that are highly correlated it then you also are going to encounter some issues especially around interpreting coefficients because if two predictors are contributing mostly the same information even if it's not exactly the same information it won't prevent you from from calculating the inverse of a matrix so it won't prevent you from finding a solution theoretically to that matrix equation but it will mean that it kind of becomes arbitrary which predictor the coefficient gets linked to um so you might have a predictor in your model that has a coefficient that's really small maybe even smaller than this but it's actually a an important predictor of the outcome variable and um like right now nitia is working on some content on feature engineering which is really the process of trying to choose a subset of features that you might want to include in a model and one of the issues or like one of the things you might do right is you might fit a model with all the predictors and then try to figure out which ones are the most important ones based on these coefficients because they tell you something about how much um effect i i shy away from using the word effect but like how much impact on the outcome variable each predictor has um but if you have two highly correlated predictors then you might have one of them with a coefficient of zero even though it's really highly related to the outcome um also yeah so c of j says when features are linearly dependent we are in the situation similar to divide by zero in one dimension just my two cents and that is exactly correct like there we just can't calculate it like you can't divide by zero you can't calculate the inverse of a matrix when you have colinearity perfect linear colinearity um so this comes back to all comes back to another thing that's worthwhile to do before you fit these models is um it's it's helpful to look at the correlation matrix to see how highly correlated each of the predictors are so let's actually um go ahead and i'm gonna like create a new column and then i think we can do um did i load seaborn see if i can remember how to do this but if i do sns.key map of bk dot forum i think that might work no let's um let's get the correlation matrix first is it just one r maybe no it seems to be two r's um let's get this is this is fun um heat map function let's try that um do you have any chance to remember nitia how to do this i think you might need to close the i have a the the syntax the closed parenthesis after that i think uh yeah i think so oh yeah nice um okay so this just helps us visualize this a little bit better but we see the coloring is not great in this so perfect correlation would be this like very light color right and everything is perfectly correlated with itself but also min to subway and seconds to subway are perfectly correlated um but this gives you a sense of how highly some of the other features are correlated i think if i do like endnote equals true it'll give me the actual numbers on top um but we see right that some of these other ones are pretty highly correlated like uh size square feet and um and rent are pretty highly correlated in that case rent is our outcome variable so that's not actually that's actually a good thing means it's a really good predictor probably a very good predictor in this model for rent um but some of these other ones like size square feet i guess the next biggest one is size square feet and bedrooms um which makes sense like something that a apartment that has more bedrooms is probably also going to be bigger something like that has a reasonably large correlation where we might be a little concerned i think 0.68 is okay um but maybe like over i think once it gets over about like .75 for me is where i start to think like maybe these two predictors should be in the model depending on depending on what i'm trying to um trying to fit a model for cool um okay we've got about five more minutes i think another um another useful thing to just see that no tia also covered in that article um and i highly recommend that you all check out the article as well if you're interested um is just to demonstrate what happens if you instead uh if you instead use a categorical variable where you have like two columns or enough columns that there is no reference category so in this case we can create another another column in our data along the same lines as before but this column instead tells us whether there is no washer dryer so remember in our original data we had um washer has washer dryer as a as a variable as a feature and it was one if there is a washer and dryer and zero if there's not and if we create a separate column called no washer dryer um then we essentially have replicated that same information kind of in reverse so i'll just print this out really fast so we can see it it'll give me the warning but it'll do what i want anyway um so if something has a washer dryer or it'll be a one in this column and it'll be a zero otherwise all of these have the value zero for house washer dryer um and so it's true therefore that they have no washer dryer um and then let's see if we can find like um it's like print the whole thing this is what i like about uh this okay so we've got um here we've got has washer dryers equal to one because it has a washer dryer and so this column says false because it's false that there's no washer dryer um so basically right like everything every value in this column could be determined by the value of this column so it's almost like the same thing as before um you can't calculate the correlation between two categorical variables but they are not once again there is multicollinearity in that like all of the information in this column is also contained in this column and again if we use that to fit a model so here we're fitting this model um we get some warnings we get this warning that the smallest eigenvalue is super small this might indicate there's strong collinear um that there are strong multicollinearity problems or that the design matrix is singular um and all we did here was we included both has washer dryer and no washer dryer into our model cool um cool so i hope that this like helped motivate a little bit why understanding this matrix representation is important um i think another thing to demonstrate really quickly or that i think is worthwhile to demonstrate really quickly is a lot of people prefer to run linear regressions or any sort of model fitting in scikitlearn because scikitlearn has a lot of other functionality that sats models does not although i find it's harder to look at the output and really understand what's going on um but for example if you want to fit a model in second learn um it's helpful to know that this like matrix representation is something that you need to kind of do yourself so for example um when we fit a model with squirn uh and this is the code to do it in this uh in this jupyter notebook that you can download on your own um there is some info so we're going to have to create this like x and y matrix by ourself um and so if we do that we end up right like taking the columns that we want out of the original data and and putting them in their own matrix and then separating out the y matrix as well in this case that's just the rent column and then we fit the model um but then scleron is automatically adding that um that column of ones behind the scenes so we are not adding that here um ourselves but there are other things that we do have to add so like if we have a um if we have three categories in a categorical variable and we want um to choose our reference category we're gonna have to like put those as columns in our x matrix and leave one out intentionally it's not going to be done automatically for us um so i think it's it's helpful to think about it in this context so that you can think about like what is happening in this library that's not happening or happening behind the scenes in this library and it can enable you therefore to learn a new technology like or learn how to fit a model in r for example and debug that so you get a little bit more insight into what's going on cool um awesome well we are just about out of time i know this was maybe a less exciting uh topic than usual but i hope that it was helpful at least to see a little bit of this math um and i promise the rest of this will be a little less snappy in um in the uh in the future i see there's a um never do math in a youtube video rule maybe that was maybe we should have followed that rule i don't know um but oh well we didn't so hopefully it didn't scare anyone away it is i think helpful to see um awesome oh this is great so there was a question in the chat about office hours this week um so there are going to be office hours this week but we're going to do them on discord for the first time ever so that is going to involve um that we're gonna have a like a chat where you can type and then we're gonna also have uh video and sound available and so if you have not already joined our discord server we will we'll put that information somewhere we'll put it in the youtube description and also the event which you can find at codecademy.com events oh there's not an event never mind it's not going to be there um we'll put it in the the youtube description so you can find it but come join us on discord happy to answer any questions we also have a chat there you can post questions in there at any time um and you can also post questions on the youtube video directly and we'll keep an eye on it and try to make sure that we answer things awesome all right thank you all i hope everyone has a great rest of their day and thank you for joining me on this math adventure thank you this was so fun
