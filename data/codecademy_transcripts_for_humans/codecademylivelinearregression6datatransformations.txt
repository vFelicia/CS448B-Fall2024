With timestamps:

00:00 - um she'll be live in just a moment
00:06 - okay i believe
00:10 - that we should now be live on
00:14 - youtube um but drop us a note in the
00:17 - chat
00:18 - if you can see us good morning on
00:21 - this beautiful tuesday
00:24 - um i'm sophie
00:27 - and jamie is joining me today
00:31 - to help answer some questions and keep
00:34 - me on track
00:36 - so let us know if you are there in the
00:39 - chat
00:40 - if you have any questions throughout the
00:41 - stream please just drop those questions
00:44 - in the chat
00:44 - jamie will stop me or he'll try to
00:46 - answer them if he can directly
00:49 - um but yeah we always love to know where
00:52 - people are joining from i see
00:54 - someone from greece and from alabama
00:57 - very exciting so let us know
01:00 - cool alex lackson says you're live thank
01:04 - you
01:05 - um awesome so i guess we'll get started
01:09 - um and we'll we'll kind of jump right in
01:13 - it's good good to know that youtube is
01:15 - working
01:16 - um we're back to last week we did a
01:19 - stream
01:19 - in the office this week we're back from
01:22 - home
01:23 - so uh we were really high tech last week
01:26 - a little lower tech this week but
01:28 - hopefully we'll get the office set up
01:30 - up and running uh more frequently in the
01:32 - future
01:34 - so um i'm gonna go ahead and share my
01:37 - screen
01:39 - and then we will kind of jump right in
01:47 - all right can you see that jamie
01:50 - yep okay cool
01:55 - so this is a little bit late on to the
01:58 - github page
01:59 - so if you haven't had a chance to take a
02:01 - look through these notes
02:02 - that's totally fine um i'm actually
02:05 - going to
02:06 - before we even get started with these
02:08 - notes i just want to show you
02:10 - um on codecademy where you can find
02:15 - the article that covers the topic that
02:18 - we're going over today
02:19 - um so
02:23 - if you search linear regression uh
02:25 - you'll see this linear regression in
02:27 - python course
02:28 - this is the course that we're going
02:30 - through in this series
02:32 - and right now we've kind of talked about
02:35 - simple linear regression multiple linear
02:37 - regression
02:38 - um last week we talked about
02:40 - interactions and polynomial terms
02:42 - and then this week uh there's this
02:44 - article in here on
02:45 - log transformations and more um
02:48 - this this stream is going to cover even
02:51 - more of the
02:52 - and more part of that um but
02:55 - if you take a look at this article um
02:58 - there's a bunch of helpful information
03:00 - here and
03:01 - it it explains all this with an actual
03:05 - data set
03:06 - um today i'm going to use
03:09 - some um simulated data
03:12 - and i'm actually going to kind of take
03:14 - you behind the scenes
03:16 - if if you will and show you how i'm
03:19 - simulating data
03:21 - because i actually think that this is a
03:22 - really useful skill
03:24 - as a as a data scientist as a
03:27 - statistician
03:28 - to be able to create data that follows
03:31 - some
03:32 - or that represents some situation that
03:34 - might occur in real life
03:36 - and then see what happens when you try
03:38 - to
03:40 - fit a model on that data so
03:43 - it's really useful for debugging as well
03:46 - so
03:47 - here um i'm just in this code
03:51 - uploading or
03:55 - uploading a bunch of or loading a bunch
03:57 - of libraries
03:58 - i see uh someone said sophie you did not
04:00 - finish the modules
04:02 - um i i did finish the modules i
04:06 - actually have reviewed all of them very
04:08 - very carefully um
04:10 - but but yeah it doesn't when i go
04:13 - through
04:14 - on codecademy it doesn't mark them as
04:16 - complete
04:17 - um i actually wrote some of the ones
04:20 - that it says i haven't completed which
04:22 - is funny
04:23 - um okay so anyway so we're gonna
04:26 - simulate some data so what's happening
04:28 - here
04:30 - the reason i have these random seed
04:33 - functions
04:34 - in the code is just so that if you're
04:36 - downloading this code
04:38 - on your own you get the exact same
04:40 - numbers
04:41 - this will all work without this random
04:43 - seed it'll just generate
04:44 - slightly different data every time
04:49 - now what i'm doing here is i'm just
04:51 - using
04:52 - within the numpy package or numpy
04:55 - library there's this
04:56 - random module that has a bunch
04:59 - of uh basically probability
05:02 - distributions that you can sample from
05:05 - so if i sample from a normal
05:06 - distribution i'm basically just like
05:09 - sampling from a bell curve and i can
05:12 - specify
05:13 - the mean and the standard deviation of
05:15 - that bell curve
05:16 - which i've specified as 167
05:19 - and three and then um three is the
05:23 - uh variance or the standard deviation
05:26 - and then
05:26 - 150 is how many values i'm sampling
05:30 - so i'm just gonna break this down for a
05:32 - minute so that you can see
05:34 - what i'm doing um but if i
05:40 - run this um
05:44 - and just print out the results
05:47 - i'm just getting a whole bunch of
05:49 - randomly generated numbers
05:51 - that have a mean of 167
05:54 - and a standard deviation of three so you
05:56 - can see all of these numbers are pretty
05:58 - similar to 167
06:00 - they vary um around that
06:04 - usually within two standard deviations
06:07 - or three standard deviations so like
06:10 - you're not going to see
06:11 - many numbers that are more than nine
06:13 - greater than
06:15 - 167 so you're not going to see much
06:17 - that's larger than 176
06:21 - but and then or smaller than 167 minus 9
06:26 - but basically just randomly generating
06:29 - some numbers
06:30 - and then in this next step
06:34 - what i'm doing is i'm taking um
06:39 - the numbers that i just randomly
06:40 - generated
06:42 - and i'm just like multiplying them by
06:45 - some number
06:46 - and adding some numbers so you might
06:47 - recognize this as
06:49 - basically the equation of a line i
06:51 - actually grabbed these numbers from
06:53 - a regression and then i'm adding some
06:57 - random noise
06:58 - so if i didn't add this in um and you
07:01 - can see a plot down here of the
07:04 - data that i just randomly generated this
07:06 - is the
07:07 - um height and this is weight
07:11 - right that i randomly made up
07:14 - if i didn't add this random noise
07:18 - um these would all be in a straight line
07:20 - because we would be simulating
07:23 - our weight variable to be exactly
07:26 - linear with height so i can actually
07:29 - demonstrate that
07:30 - um really fast if i do something like
07:33 - this
07:35 - i get a bunch of points in a straight
07:36 - line
07:38 - um and then
07:41 - now right i add this back in
07:47 - i've simulated some data it looks like
07:50 - this
07:50 - i'm calling it height and weight just to
07:52 - make
07:54 - this i don't know i guess a little bit
07:56 - more uh grounded and
07:58 - like a physical example but um
08:02 - but yeah that's all we're doing here i
08:05 - see
08:06 - oh yeah i see a question about are we
08:08 - doing the ols
08:10 - without an api um
08:13 - so i assume you're asking about this
08:15 - like
08:17 - oops uh
08:21 - this statsmodels.api uh
08:24 - library if you're not asking about that
08:26 - then you can clarify
08:28 - um but so we actually managed to do it
08:31 - without
08:32 - the library at all uh in the week that
08:35 - nitia was joining me i think that was
08:36 - two weeks ago
08:38 - uh we did it with some matrix algebra in
08:40 - numpy
08:41 - um and then there's definitely some code
08:44 - throughout to do this in scleron as well
08:46 - if you're interested in learning another
08:49 - library um but today we're gonna still
08:52 - stick with the statsmodels.a
08:54 - module because again it gives us some
08:57 - good information
08:58 - that's useful for for debugging models
09:04 - cool all right so
09:09 - today we're going to talk about some of
09:10 - the things
09:12 - that can go wrong when you fit a model
09:16 - and then we're going to talk about some
09:17 - of the ways of fixing them
09:20 - fixing those things um sometimes you're
09:23 - gonna notice that something went wrong
09:25 - uh because you got a warning when you
09:27 - fit the model
09:28 - sometimes you won't get a warning
09:32 - and you'll be checked you'll need to
09:34 - check the
09:35 - assumptions of the model and then you'll
09:37 - see something that surprises you
09:40 - and that will that will clue you in that
09:42 - you need to do a little bit
09:44 - more investigation
09:47 - so um so i'm gonna start by just fitting
09:50 - a model with this data that i randomly
09:53 - simulated
09:54 - and we've seen something like this
09:55 - before i'm gonna fit a model where i
09:57 - predict
09:58 - weight based off of height
10:02 - and then when i fit this model
10:07 - i see and we've seen this the same
10:10 - warning before
10:11 - um but i see this warning that says the
10:13 - condition number is large
10:16 - this might indicate that there are
10:17 - strong multicollinearity
10:20 - or other numerical problems
10:23 - okay so i think last time we saw this
10:27 - um the strong multi-collinearity
10:31 - part um we talked about a little bit
10:34 - more in depth
10:36 - and we we discussed how that has to do
10:38 - with
10:39 - having predictors in the model that are
10:41 - co-linear with each other or correlated
10:43 - with each other
10:44 - and we talked about why that might not
10:46 - be a good idea to include multiple
10:49 - predictors that are highly correlated um
10:52 - and
10:52 - but in this example we've only fit
10:55 - this model with one predictor
10:59 - so there's no chance that we have
11:01 - correlated predictors
11:02 - um so we need to think about what else
11:05 - could be wrong
11:06 - in this situation what else could be
11:07 - causing this warning
11:10 - and so um one of the things let's look
11:13 - at this picture
11:14 - that you might notice when we we first
11:17 - look at this
11:19 - this graph is that
11:23 - it's automatically in matplotlib
11:27 - is plotting this so that it fits it
11:30 - perfectly fits within the axes
11:32 - but we see that it starts at like 1
11:35 - 60-ish or a little below 160 on the
11:38 - x-axis and it starts at like
11:40 - 54. on the y-axis
11:44 - and so if we get this
11:48 - intercept of negative 23 and we talked
11:51 - about this i think
11:52 - in one of the first live streams um
11:56 - in order to visualize what that
11:58 - intercept looks like we really have to
12:00 - zoom
12:00 - out a bunch on this plot so
12:04 - i think i have yeah
12:08 - so here this plot is again i can i'll
12:11 - pull this up here so you can see it um
12:13 - all in one place
12:17 - right so if we just plot this looks like
12:20 - this
12:21 - if we plot it and we change the axis so
12:24 - that
12:24 - the x-axis starts at zero and goes up to
12:27 - 180
12:28 - because that's roughly how far up it
12:30 - goes on this
12:32 - x-axis and then on the y-axis we have it
12:36 - start at zero
12:37 - and go all the way up to 70. um
12:42 - we're gonna get this kind of like
12:43 - squished plot
12:45 - um which is to say that like the
12:48 - negative 21
12:50 - and here i can even have the uh
12:54 - the y axis go down to like negative
12:57 - or was it negative 23 yeah so we'll have
13:00 - it go to
13:01 - negative 25.
13:04 - this is coming from the fact that if i
13:07 - drew this line in
13:08 - and we can even let's draw it
13:12 - in um we can do
13:19 - equals
13:23 - dot all right
13:27 - 0 180
13:31 - and then y
13:38 - i'm doing it the the old-fashioned way
13:40 - we'll grab
13:42 - we'll even grab these so the intercept
13:45 - is negative
13:46 - 23. and then the slope
13:50 - was 0.5166
13:58 - if we add these to our plot
14:01 - in the form of a line
14:04 - right we see that so this is our
14:07 - regression line
14:08 - and the negative 23 is the y-intercept
14:12 - and the reason it's negative 23 is that
14:14 - in order to draw this
14:16 - line we've got to go all the way back to
14:18 - the y-axis
14:20 - which happens at x equals zero or height
14:23 - equals zero
14:24 - and then it hits at negative 23.
14:30 - jamie do you have any um any thoughts
14:33 - about like
14:35 - why this might be problematic or like
14:38 - what um
14:42 - what might happen if the data were to
14:44 - just change
14:45 - slightly like if we changed the the
14:48 - slope of that line just like
14:49 - ever so slightly um because
14:52 - one of we had a point down here um
14:56 - all of a sudden what would what might
14:59 - the issue be
15:00 - um i think like a main issue would be
15:03 - that like a slight
15:04 - change in the slope so like any outlier
15:06 - that like it potentially changes the
15:07 - plot
15:08 - would change the intercept like pretty
15:10 - heavily um
15:12 - so i think like that is one issue i see
15:14 - is that like the manager you're talking
15:15 - about or
15:17 - yeah yeah that is exactly
15:20 - what what i was intending so
15:23 - right so um so yeah so this is like
15:26 - a kind of unstable estimate
15:30 - for the intercept because
15:34 - and i think we talked about this a
15:35 - little bit when we started when we first
15:37 - talked about
15:38 - um the definition of
15:41 - an intercept or like the interpretation
15:44 - of an intercept
15:45 - so um
15:48 - jamie i don't know i don't remember if
15:51 - you joined this
15:52 - uh first live stream or if this is if
15:55 - you're caught up on this which
15:56 - it's totally fine if not but you
15:58 - remember how we would interpret
16:01 - this intercept it's like value of
16:04 - negative 23.
16:05 - yeah yeah so since we were talking about
16:08 - i think it's
16:09 - x-axis is height y-axis is weight or is
16:12 - it
16:12 - yeah um so this would be this
16:16 - interception is basically saying that a
16:17 - person who has a
16:19 - height of zero i guess centimeters
16:22 - has would be expected to have a weight
16:24 - of negative
16:26 - it's like 25 22 or something um
16:30 - pounds or whatever or kilograms or
16:32 - whatever it's um the scale is
16:35 - so it's probably like not super
16:38 - informative here or like not super uh
16:41 - what's it called like sometimes you'll
16:42 - have
16:43 - intercepts that like do make sense um
16:46 - like if you're doing like hours studied
16:47 - versus expected test score like
16:50 - zero hours studying expect tests where
16:52 - like that could make sense um
16:54 - i don't know anyone who who sorry who
16:57 - has a height
16:57 - of zero centimeters who has a negative
16:59 - weight um but so yeah i think
17:02 - i think this one um is potentially a
17:04 - little bit
17:06 - uh less less informative um yeah
17:09 - yeah exactly um i realized it maybe is
17:13 - more useful if i use this like sns
17:16 - scatter plot and um because then i think
17:20 - it like
17:21 - plots the uh
17:24 - or at least gives you the labels
17:28 - and we'll do that let's see if that
17:30 - works better
17:32 - yeah there we go um so
17:36 - yeah so exactly so this is the predicted
17:39 - value
17:40 - of weight when height equals zero
17:44 - but just like jamie said there's no such
17:46 - thing as having a height of zero
17:48 - right we have no data all
17:51 - in this part of the plot so drawing the
17:54 - line there
17:54 - like we can say that we the there's this
17:57 - relationship
17:58 - but really we only
18:01 - want that relationship to want to say
18:04 - we don't really only care about that
18:07 - relationship
18:08 - where the data exists which is only in
18:10 - this part of the plot
18:12 - and so we're estimating something that's
18:14 - not really
18:16 - of interest to us and it's also
18:20 - an estimate of something that's really
18:23 - easily
18:23 - changed by just like small changes in
18:26 - the data
18:27 - because it's so far out from where the
18:30 - data is
18:31 - that it's it's kind of unstable so
18:34 - there's a number of things that we can
18:36 - do to fix that
18:38 - um and so this is maybe one thing that's
18:40 - not in the article that i
18:44 - originally wanted to include in this
18:45 - course and we just didn't have time
18:47 - but but i'm going to demonstrate it now
18:54 - so
18:56 - actually before before we even get there
18:59 - um one thing i also
19:02 - actually sorry i'm changing my mind as
19:05 - we go
19:06 - uh let's yeah let's actually let's fix
19:09 - this first
19:10 - and then we'll we'll go back and talk
19:12 - about some of the other assumptions
19:15 - so um so there's a couple different ways
19:17 - to fix this
19:18 - one way to fix it is simply to
19:21 - center the height variable
19:25 - um and so i'm going to
19:28 - talk you through what centering means in
19:30 - just a second
19:31 - [Music]
19:32 - and we can actually do it together
19:38 - so to center something
19:42 - all we do center a variable all we do
19:45 - is we take the mean of that variable
19:49 - and we subtract it from every single
19:52 - value so i'm gonna
19:56 - go ahead and in my data i'm gonna find
19:59 - the mean height so i'll print that out
20:04 - first
20:06 - so the mean height in this data is
20:10 - about 167 centimeters
20:13 - and then what i'm going to do is i'm
20:15 - going to subtract
20:16 - that from every value
20:20 - in the height column
20:27 - and what this does is it just creates a
20:30 - whole list of values
20:31 - of heights um with the means subtracted
20:36 - um so jamie do you want to like talk
20:38 - through what what are
20:40 - these numbers now interpreted as like
20:43 - if we see 4.928 what does that mean
20:47 - uh so i think that means that like this
20:50 - specific
20:51 - like data for this specific data point
20:52 - um it's like 4.9
20:55 - like the height like this person's
20:58 - height is like 4.9
20:59 - um i think centimeters greater than the
21:01 - mean so it's basically like the distance
21:03 - just like from the mean exactly
21:06 - so yeah so this person is 4.23
21:10 - or sorry for 4.93 centimeters above the
21:13 - mean
21:13 - in terms of their height this person
21:17 - in the third row is very very close to
21:20 - the average height
21:21 - in the data set so if they had a value
21:24 - of zero it would mean that they were
21:25 - average height so what we can actually
21:28 - do
21:29 - is we can create
21:32 - a new
21:36 - variable we'll call it height centered
21:40 - that contains these values and then i'll
21:43 - print
21:44 - the um
21:47 - the first few rows of the data again to
21:49 - just take a look
21:50 - um and so all we did was we said okay
21:54 - we've got weight we've got height now i
21:56 - want to create another column
21:58 - that's called height centered and these
22:00 - values are now
22:01 - centered at zero right so a value of
22:04 - zero
22:04 - means average height anything above that
22:07 - means they're above the average height
22:09 - anything below that means they're um
22:13 - they are below the average height
22:17 - they do that right yeah yes
22:20 - okay so now what we can do
22:25 - is we can refit this model
22:30 - but use centered height instead
22:38 - right
22:44 - and you'll notice that that other
22:46 - warning went away
22:47 - so remember the first time we got this
22:50 - the condition number is large
22:53 - um this time that went away and we just
22:56 - get standard errors assume that the
22:57 - covariance matrix of the errors is
22:59 - correctly specified
23:01 - which i believe we get no matter what um
23:03 - that's always
23:04 - an error or a warning here
23:08 - um okay so
23:11 - that's one way and we're just kind of
23:14 - dipping our toes in
23:15 - that we can start to transform our data
23:19 - and by transform i mean take existing
23:22 - data
23:23 - and kind of do something to it so that
23:26 - we can
23:27 - fit a model that is a little bit more
23:30 - stable
23:31 - that doesn't create warnings that is
23:35 - potentially like a better fit to our
23:38 - our data um it's easier to interpret
23:42 - all of the above so
23:45 - i think one thing that is maybe useful
23:48 - before we move on
23:51 - to more complex example
23:54 - is to now look at what this picture
23:57 - looks like
23:58 - so let's go ahead and
24:03 - i'm actually going to use i like this
24:06 - lm plot function sometimes because it
24:08 - just saves us a little bit of time
24:11 - so let's re-plot
24:14 - on the x-axis now height
24:18 - center and on the y-axis
24:23 - let's re-plot weight
24:27 - and then
24:31 - the data is just called data
24:35 - and this will automatically plot the
24:37 - regression line
24:39 - show run um
24:43 - cool so now we see that the weight
24:46 - variable looks exactly the same
24:48 - but height centered right like the
24:50 - middle of these data points
24:52 - is around height centered equals zero
24:56 - which makes sense because that's the
24:58 - mean
25:00 - and and let's actually add let's add
25:02 - like a vertical line to this
25:04 - okay the code to do that somewhere um
25:07 - it's basically like this
25:12 - but ax line
25:17 - so this right here where i've drawn this
25:21 - dotted line
25:22 - this is the y-axis now
25:26 - so um so now the place where
25:30 - we cross the y-axis
25:33 - and i think uh we can estimate i
25:37 - i'll fit the model in just a second so
25:39 - we can see what it is
25:40 - but it looks like it's around like 62
25:43 - point something maybe 63.
25:46 - um that value
25:49 - jamie do you want to take a another stab
25:51 - at
25:52 - interpreting that number um so i think
25:55 - it's like
25:56 - if you have a mean
26:00 - if you have like a height that's equal
26:01 - to like the mean of the data
26:03 - then you're well
26:06 - so i guess it's like when we centered it
26:07 - we're like at zero now so like that
26:09 - is the y that's like the x equals zero
26:12 - point
26:13 - um but i i'm guessing we need to say
26:16 - like
26:18 - if you like if you're at like a height
26:20 - of
26:21 - whatever like that pre-transformation
26:23 - was and that's like what
26:25 - you would interpret it so okay it's like
26:28 - if you have like
26:28 - 62 or something or whatever the mean was
26:31 - and you have an
26:31 - expected weight of x of like
26:35 - 62 point something
26:38 - exactly so if your height centered value
26:42 - is zero
26:43 - it means that you are average height
26:47 - so the expected or the
26:50 - average weight of someone who is
26:53 - average height is 62 point whatever
26:59 - is how you would interpret that so now
27:01 - we've got a more interpret
27:03 - interpretable value
27:06 - and it's a little more stable right
27:09 - because like the
27:10 - intercept is now kind of in the middle
27:12 - of all of the data
27:14 - and so if we have another point
27:16 - somewhere
27:17 - it's going to be less likely to change
27:20 - what this value is
27:22 - um than before
27:26 - so um
27:29 - cool actually i think let's really
27:32 - quickly
27:33 - fit the model and just see what this
27:36 - looks like
27:37 - so if we go ahead and fit
27:42 - our model but now we use
27:47 - height center
27:52 - we get right an intercept of 62.7
27:57 - our um our slope on height centered
28:01 - 0.5166 is exactly
28:04 - the same as our slope on height
28:08 - so the slope of our line didn't change
28:12 - it just changed the intercept
28:15 - and now that 62.7
28:19 - is average weight of someone who's
28:21 - average height
28:22 - in the data and so we've got a situation
28:25 - where we can interpret it
28:27 - it's more stable we don't get um
28:30 - we don't get errors oh i already did
28:33 - this
28:33 - down here sorry um
28:36 - okay cool uh if there are any
28:40 - questions in the chat please let me know
28:42 - otherwise i'm gonna
28:44 - now move on to some log transform stuff
28:48 - so um i think one of the things that's
28:52 - really challenging
28:54 - about data transformations in general
28:57 - is that there are a ton of different
29:01 - ways that you can transform your data
29:03 - and it's hard to know exactly which one
29:07 - to do in any given situation
29:10 - so um one thing i want to say is like
29:16 - there are no hard and fast rules and
29:18 - there's also
29:20 - way more options than what we're going
29:22 - to cover today
29:23 - um for example we could
29:27 - instead of centering we could just
29:29 - standardize everything so we could also
29:31 - divide by the standard deviation
29:34 - um that would change that the slope
29:37 - and change our interpretation of the
29:39 - slope um
29:40 - but there are definitely situations
29:42 - where we'd want to fully standardize
29:44 - instead of just centering
29:46 - i also see a question can you explain
29:48 - the shaded area on the graph around the
29:50 - best bit line
29:51 - that is a great question um so this lm
29:54 - plot function
29:57 - it automatically creates this like
30:01 - confidence interval around the line
30:05 - which basically shows how confident
30:08 - we are that the line um
30:12 - is where where it's pictured so
30:15 - or like how like how
30:18 - sure we are that the line is here so
30:20 - you'll notice that like on
30:22 - the ends of it um this shaded area is
30:25 - wider
30:26 - here i can zoom in
30:29 - [Music]
30:32 - right the shaded area is wider on the
30:34 - ends because
30:35 - in this um
30:39 - like in this uh
30:42 - part of the the graph where
30:45 - there's not a lot of data and so we
30:48 - don't know like
30:50 - if we had one more point we don't know
30:53 - if it would be like close to this
30:55 - if it would be here if it would be up
30:57 - here um
30:58 - and that could potentially shift this
31:02 - part of the line a lot more than
31:05 - in here where the bulk of our data is uh
31:08 - we're more
31:09 - confident that the line goes through
31:12 - this point actually i think it would be
31:15 - potentially interesting
31:18 - we do like a
31:22 - lm plot on this
31:33 - what happens if i do that
31:41 - interesting so it won't even
31:44 - graph
31:48 - it outside of where the data is
31:54 - i'm assuming
31:59 - let's like let's make this like
32:02 - uh
32:10 - 130 or something
32:14 - yeah you can see it's like graphing this
32:16 - line in here it won't even
32:18 - graph it down here but you could imagine
32:21 - and it would be really nice if it did
32:23 - because i think you could visualize
32:25 - that if you see how and i know you have
32:29 - to look kind of closely
32:30 - but if you see how like the um
32:33 - confidence interval is really tight
32:35 - in here and it's it gets wider and wider
32:38 - and wider
32:38 - it would just get wider and wider and
32:40 - wider the further away from the data you
32:43 - went
32:43 - um so you could really see what i was
32:45 - talking about before where
32:47 - if you drew this line all the way down
32:49 - here that confidence interval would get
32:52 - really wide
32:53 - um around here meaning you're like very
32:56 - uncertain
32:57 - about what that intercept is that you're
32:59 - trying to
33:00 - estimate um
33:06 - oh okay i see uh alex noted
33:09 - that sns.lm plot is sns.reg plot and the
33:15 - latest version of seaborne fyi um
33:18 - yes uh the reason we have been using lm
33:21 - plot i think
33:22 - lm plot still hasn't been deprecated as
33:24 - far as i know
33:25 - um and we have we had
33:29 - uh an older version of seaborne on
33:31 - codecademy
33:32 - so we we don't have ragplot i think
33:36 - yet um but that is good to know thank
33:39 - you
33:40 - oh and then i see another
33:43 - comment wiki dd says i think they are
33:46 - still two different options with slight
33:48 - difference figure level or not not
33:52 - exactly the same arguments that is
33:54 - interesting i actually did not know that
33:56 - let's like i'm gonna just look it up
33:58 - really fast
34:02 - i think this is kind of cool
34:05 - um
34:19 - interesting
34:22 - it looks similar but it looks like it
34:24 - has some additional options
34:32 - yeah alum plot still definitely has not
34:35 - been
34:36 - deprecated
34:39 - um
34:42 - i think we used a reg plot for one of
34:44 - the off platform
34:46 - like the off platform data analyst and
34:48 - final portfolio project
34:50 - remember should we just try it
34:54 - see what happens
34:58 - oh no
35:04 - said x
35:07 - y and data
35:14 - right
35:19 - oh it's because i
35:27 - haven't recreated this yet
35:30 - so i guess in this example it basically
35:32 - looks the same
35:34 - but interesting it would definitely be
35:37 - interesting to do
35:38 - a um more careful comparison and see
35:41 - what the what the real differences are
35:46 - okay so um before we kind of move into
35:49 - the second
35:50 - half of this live stream i think it's
35:52 - useful
35:53 - to take this data which besides the fact
35:57 - that um it was
36:00 - we had this issue from before where like
36:03 - the um the intercept was kind of far
36:06 - away from the data points
36:08 - besides that that issue um this data
36:12 - is simulated such that it fits
36:16 - a linear a linear model really nicely
36:19 - in fact um i think it's kind of cool to
36:21 - just see
36:22 - um just prove to yourself that like
36:24 - things are working
36:26 - as they're supposed to um
36:29 - so when i simulated this data i used i
36:32 - used
36:32 - some numbers from a regression but i
36:35 - simulated it to have
36:37 - an intercept of around 21.7
36:40 - and a slope of about 0.5
36:45 - and then when i and but i randomly
36:48 - generated the point so it wasn't going
36:50 - to be exactly these numbers
36:51 - but this is what i kind of simulated i
36:53 - simulated an exactly linear relationship
36:58 - with some error um
37:01 - and then when i fit this model
37:05 - the first time my intercept was close to
37:08 - the negative 21 and the height and the
37:11 - slope was close to the 0.5 that i
37:13 - originally
37:14 - kind of set out to use to create the
37:16 - data
37:18 - so with that said right like
37:22 - if i create data that perfectly fits a
37:25 - linear model
37:26 - then i can see kind of what my checks of
37:29 - the assumptions of linear regression
37:31 - should look like if everything
37:34 - is kind of perfectly uh
37:37 - perfectly perfect for for
37:41 - the purposes of um
37:45 - of linear regression so
37:48 - i'm going to take a look at two
37:50 - different uh
37:51 - assumptions of linear regression so
37:55 - the first assumption that we'll talk
37:57 - about
37:58 - is homoscedasticity which is
38:01 - a very fancy word but it's pretty
38:05 - i think hopefully easy er to understand
38:08 - once
38:08 - you take a closer look so
38:11 - homoscedasticity
38:12 - essentially means that there is equal
38:15 - variation
38:18 - of the outcome variable for all values
38:22 - of the predictor
38:26 - so or predictors um if you're in
38:29 - multiple linear regression world
38:31 - so here um for height centered or height
38:36 - either one we notice that like the
38:39 - amount of variation
38:40 - on this part of the line is
38:44 - pretty similar to the amount of
38:45 - variation
38:47 - over here um and so because
38:51 - the variation is roughly the same
38:55 - for all values of height
38:58 - um the variation around the line is
39:00 - roughly the same for all
39:01 - values of height um this data follows
39:05 - the homo
39:06 - homoscedasticity assumption of linear
39:08 - regression
39:09 - um and we actually again we created the
39:13 - data to
39:14 - to do this because we added random noise
39:18 - centered at zero with the same level
39:21 - of variation for all of our values when
39:23 - we first created the data
39:27 - and so the way that we often check
39:30 - um homoscedasticity
39:33 - is we get the fitted values for our
39:36 - model
39:37 - the way we get the fitted values is we
39:39 - just predict
39:41 - um get basically like the predicted
39:43 - values for
39:44 - every for the original data itself um
39:47 - and so i will kind of
39:49 - show this i think we skipped over this
39:51 - in the first week but i think it's like
39:53 - important to kind of come back to now
39:56 - so the fitted values here
40:01 - are just the predicted weights
40:04 - for everyone in the data set
40:09 - then we can calculate the residuals
40:12 - which
40:12 - are the difference between the actual
40:15 - weights
40:16 - of every person and their
40:20 - fitted value or their predicted weight
40:24 - and then we can plot the fitted values
40:26 - against the residuals
40:30 - and we expect that
40:33 - um or sorry the residuals against the
40:36 - fitted values so we
40:38 - these are the residuals right the y-axis
40:41 - is the residuals these are the fitted
40:42 - values
40:43 - and we expect that the residuals should
40:45 - be centered at zero
40:47 - right because on average all of the
40:51 - points should be
40:54 - like equally or basically like
41:00 - some of the points are going to be below
41:01 - the line some of the points are going to
41:03 - be above the line
41:04 - but the line was defined such that
41:07 - um it's kind of going through the middle
41:10 - of all the points it's minimizing that
41:12 - vertical distance between
41:13 - all of the points in the line and so we
41:16 - expect
41:17 - there to be just as many points below
41:19 - the line as above the line
41:21 - um and we want this
41:24 - basically we want this plot to just look
41:26 - like a splatter
41:28 - we don't want to see any pattern any
41:31 - patterning
41:32 - we don't want to see that like the
41:33 - variation on this side is different from
41:35 - the variation on this side
41:37 - um and i'll show you what that would
41:39 - look like in just a second
41:41 - we also generally want to take a look at
41:47 - the distribution of the residuals
41:51 - um and we expect that distribution to be
41:55 - roughly normally distributed um so we
41:57 - don't want to have
41:58 - a lot of skew we don't have like a bunch
42:01 - of really small
42:03 - residuals and then um very few
42:06 - large residuals um so
42:11 - this looks pretty good it looks pretty
42:14 - symmetric
42:15 - there's one kind of hump around zero
42:19 - and and we feel pretty good about this
42:22 - but now let's
42:23 - go into a land where um
42:26 - where this is not the case and then take
42:29 - another look
42:30 - at how we could kind of get around that
42:34 - so going way down okay
42:39 - so here is some more simulated data
42:42 - that violates the homoscedasticity
42:46 - assumption
42:48 - and so basically i simulated this data
42:52 - so that
42:53 - the this error term
42:57 - is related to the one of
43:01 - the predictors and then um
43:05 - that means that like essentially for
43:09 - larger values of
43:13 - of age which i'm going to make my
43:15 - predictor in this
43:17 - made up data i'm adding more variation
43:21 - and for smaller values
43:22 - i have less variation
43:26 - so okay so is that kind of showing up in
43:28 - the i can't tell if i'm like seeing this
43:29 - one or not but um the shaded area for
43:31 - the alum plot
43:33 - it's like a little bit wider for like
43:34 - the data that's more
43:36 - like varied um in like the higher age
43:39 - yeah
43:40 - yeah i think it is that looks right
43:44 - um okay so uh i just
43:48 - honestly made up again some labels so
43:52 - that we would have something to kind of
43:53 - refer to
43:54 - instead of just calling it variable one
43:56 - and variable two so let's just say this
43:58 - is like
43:59 - age of person there are some very old
44:01 - people in this
44:02 - uh in this data but that's fine
44:06 - 120 yeah um and this is like how much
44:10 - they spent
44:11 - maybe should we say like on healthcare
44:14 - per year or something i don't know like
44:18 - making it up totally fake data um
44:22 - but but we see this kind of
44:25 - funneling basically where there's like
44:27 - very little um
44:28 - variation around the line for small
44:30 - values of age and for large values of
44:32 - age there's a lot of variation
44:34 - in spending like people who are 100
44:36 - years old are spending
44:38 - between 40 and 140
44:42 - versus like people who are 50 years old
44:45 - are
44:46 - really just spending between like 42
44:49 - and 47.
44:52 - um so
44:56 - we're not going to get any warnings like
44:58 - if we fit this model
45:00 - predicting spending based on age we're
45:03 - not going to get any
45:04 - warnings at all
45:08 - besides this like our typical standard
45:10 - errors assume that the covariance matrix
45:12 - is correctly specified um but
45:15 - if we were to go through and take a look
45:19 - at that plot that we
45:20 - just made for the last set of data
45:24 - we'd see that we get this kind of like
45:28 - so again this is the residuals against
45:30 - the fitted values
45:32 - we start to see this like funneling
45:35 - effect
45:36 - um i think the the like official term
45:39 - for it
45:40 - is funneling i don't know at least
45:42 - that's what i've seen
45:44 - so um this is happening remember because
45:48 - if we go back to this plot and say
45:52 - and this is the line that we're fitting
45:54 - the residuals are going to be
45:56 - really small down here like these are
45:58 - very small residuals
46:01 - um and the residuals are going to be
46:03 - really big over here like
46:04 - this point for example is going to have
46:07 - a really big
46:08 - residual um and this point is and this
46:10 - point is so we've got
46:12 - smaller residuals for smaller values of
46:15 - age
46:16 - and larger residuals for larger values
46:19 - of age
46:20 - and that's what we're seeing here
46:22 - smaller residuals
46:24 - on one side of this graph larger
46:27 - residuals or a bigger range of residuals
46:29 - on the other side and if we look
46:33 - at a histogram of the residuals they're
46:35 - still
46:36 - roughly normally distributed they're
46:38 - still in this example
46:40 - um residuals that are small and
46:44 - residuals that are large and
46:45 - um there's still more common for small
46:48 - to see small residuals than large
46:49 - residuals
46:51 - um but we're we're violating one of the
46:53 - assumptions the homogeneous to the
46:55 - assumption of linear regression
46:57 - so um i'm going to break for a second
47:00 - because i
47:01 - see that we're going to have we have
47:04 - like 12 minutes left and i'm gonna um
47:08 - try to save a little time i think by
47:11 - looking
47:12 - at this uh article really fast
47:16 - so in this article we see
47:19 - an example where there's both
47:23 - the issue of
47:26 - homoscedasticity is violated because
47:30 - you see there's like a lot more
47:31 - variation over here
47:34 - than over here but also
47:37 - the issue also in this example
47:41 - the residuals are not normally
47:43 - distributed
47:44 - um there's going to be i think a lot
47:46 - more large or
47:51 - yeah a lot more like a lot longer tale
47:54 - of larger residuals than
47:57 - small residuals um
48:01 - is there a point at which like like
48:04 - um for like it to like like hit the like
48:08 - the residuals are normally distributed
48:10 - is there like a point at which
48:13 - like you realize that they're like it
48:15 - doesn't like hit the
48:16 - like it's like not like a good amount of
48:19 - normality
48:20 - um because like sometimes like you'll
48:22 - have a graph there's like some slight
48:23 - skewness versus like
48:24 - a lot of skewness yeah that's a good
48:27 - question
48:28 - so there's actually so there are some
48:32 - more rigorous ways of checking the
48:35 - normality assumption
48:37 - um one is a qq plot
48:40 - which basically allows you to visualize
48:43 - how far off the points are from like
48:46 - what they would be expected to look like
48:49 - if it was a perfect normal distribution
48:52 - um
48:52 - but then there's also a hypothesis test
48:55 - i'm
48:56 - great i'm blanking on the name of the
48:59 - test
49:01 - um but i'll i'll look it up and i'll
49:03 - post it somewhere later
49:04 - um but there is a
49:08 - like a hypothesis test you could run for
49:11 - normality uh which is something that
49:13 - people
49:14 - sometimes do and just like set a um
49:17 - a significance threshold ahead of time
49:20 - and then
49:20 - um and then
49:24 - yeah is it the shapiro wilk yes
49:28 - yep that's it yeah i was like
49:31 - thinking that it started with a w
49:34 - but yeah yes shapiro wilkes um
49:38 - is the one that i've seen most commonly
49:40 - used
49:43 - um okay so the other thing like
49:46 - that i notice about this right off the
49:49 - bat
49:50 - is that um is that this is
49:53 - kind of this data looks kind of
49:56 - uh not linear in that um
50:00 - like if i were to draw a line through
50:02 - this set of points
50:04 - i would draw a curved line right like it
50:06 - would probably start up here
50:08 - and it would like curve around
50:12 - to here um and whenever
50:17 - um and whenever you see kind of like
50:19 - this non-linear relationship
50:21 - that also can be a
50:24 - indicator that you maybe need a
50:26 - different
50:28 - method to model this relationship
50:31 - whether it be
50:32 - adding a polynomial term or in the case
50:34 - of a log transform
50:36 - log transform can help
50:40 - manage this like homoscedasticity
50:42 - violation
50:43 - which would be called heteroskedasticity
50:47 - i believe um and then also
50:50 - help with this lack of normality among
50:54 - the residuals i also see alex said
50:57 - i think some people also check the two
50:58 - parameters skewness and kurtosis to
51:01 - assess normality
51:02 - yes um yeah so there
51:05 - are summary statistics that you can
51:08 - calculate
51:09 - called skewness and then i think
51:11 - kurtosis is like how
51:13 - um it's like peakedness
51:17 - of the um of the distribution i believe
51:21 - and you can you can calculate those
51:24 - values and then use that to
51:26 - to assess if you want like a not
51:28 - hypothesis test
51:30 - method um yeah for sure
51:34 - so um basically log transform
51:38 - generally what we would do is we would
51:42 - take a take the log of
51:46 - the outcome that variable and then fit
51:49 - that
51:50 - in the model so i'm gonna do that back
51:53 - in
51:53 - my code um
51:56 - instead of having spending i can just
51:59 - like before
52:00 - when i had centered um
52:04 - centered height i can do log
52:07 - spending and i can end up
52:10 - with therefore a new column where
52:15 - i've got the log of my original spending
52:18 - column
52:20 - and then if i plot age versus
52:24 - log spending again
52:28 - you'll see that this looks a little bit
52:31 - better
52:32 - than the original data it still got a
52:34 - little bit of fun
52:35 - funneling but it definitely like
52:38 - improved that slightly so
52:42 - again like this was the original
52:46 - where we've got quite a lot of funneling
52:48 - happening
52:49 - and then this
52:53 - is after we plot age versus log spending
52:58 - instead of age versus spending
53:03 - is there a way to like know that like
53:04 - you should try a lot transformation
53:06 - on a like y variable like is like
53:09 - funneling like a good
53:11 - time to like try a log transformation or
53:14 - yeah so there's lots of reasons why you
53:16 - might try it
53:17 - um one reason so i think
53:21 - like one reason that you might try it is
53:24 - if you see that
53:25 - funneling another reason you might try
53:28 - it
53:28 - is if um if you have
53:32 - skewed residuals so like
53:35 - a lot of times what happens with the
53:37 - skewed residuals
53:38 - is that you you have non-linear um
53:43 - you have a non-linear relationship so
53:45 - like
53:46 - you could imagine if these points were
53:50 - even
53:50 - further like above this where it was
53:52 - like
53:54 - points really following like this this
53:57 - kind of a line
53:58 - um that kind of shape you end up with
54:01 - really small residuals down here
54:03 - and then really large residuals up here
54:07 - overall and so you end up with this like
54:09 - weird distribution
54:10 - of residuals um which really comes from
54:12 - this like non-linear relationship
54:15 - that's another time to try a log
54:16 - transform um
54:18 - and also like right off the bat
54:21 - sometimes you'll just notice
54:22 - that you have a outcome variable
54:26 - that probably benefits from being
54:29 - put on a log scale so
54:33 - a lot of the time that will happen with
54:35 - measurements like
54:37 - salary or or
54:40 - cost of something rental price something
54:43 - like that where
54:44 - you end up with a really skewed
54:47 - distribution
54:48 - of your outcome variable um
54:51 - or like very large numbers down to like
54:55 - very small numbers like in this case
54:57 - we have a whole bunch of values
55:00 - that are close to zero uh in terms of
55:03 - this like phones variable
55:05 - and then we've got some values up to a
55:08 - thousand
55:09 - and so taking the log just
55:12 - helps normalize this like
55:15 - y variable like i'm pretty sure if you
55:18 - were to plot
55:19 - the distribution of phones
55:22 - um it would be skewed
55:26 - cool um all right we are
55:30 - we're running tight on time so i'm
55:32 - actually gonna kind of switch over
55:34 - to showing you well
55:37 - i can show you quickly on this example
55:39 - um
55:40 - so we took the log transform we fit our
55:43 - model with log spending
55:45 - instead of regular spending and then
55:49 - we end up with a lot less funneling in
55:52 - this plot
55:53 - there's still some funneling for sure
55:55 - like it's not perfect
55:57 - um but this definitely helped us
56:00 - um build a slightly better model
56:05 - um i think
56:10 - i forget if there's a picture in this
56:14 - article
56:20 - yeah so now um
56:24 - i think this one is maybe a better
56:26 - example i was trying right before this
56:28 - to simulate some data to kind of
56:30 - show you something that looks more like
56:32 - this but i was being slow to simulate
56:34 - data so i think it's
56:36 - it's maybe better to just show you this
56:38 - um
56:39 - but if we look at this relationship now
56:41 - between log
56:42 - phones and birth rate we see that the
56:45 - relationship between
56:46 - law the log of phones and birth rate is
56:50 - a lot more linear than the relationship
56:53 - between
56:57 - phones and birth rate and so by taking
57:01 - the log we've made this more linear
57:04 - we've addressed
57:05 - the homoscedasticity issue a little bit
57:09 - more
57:09 - there's less of this funneling in the
57:11 - original plot
57:12 - and so when we take a look
57:16 - at a histogram of the residuals and the
57:18 - residuals against the fitted values
57:21 - we end up with something that looks a
57:23 - lot more reasonable
57:25 - this looks a lot more normally
57:26 - distributed and this looks
57:28 - a lot more like a splatter of points we
57:30 - don't get that same
57:32 - funneling as we had before
57:38 - there's a whole bunch more in here that
57:40 - i was hoping to cover today but i don't
57:41 - think we're gonna have time
57:43 - about the interpretation or like how you
57:46 - would interpret
57:47 - a model where you took the log of um
57:52 - of the y variable or the outcome
57:55 - variable
57:56 - it's really just a bunch of math so what
57:59 - you're fitting
58:00 - in this case is like log of the outcome
58:04 - variable
58:05 - is equal to this
58:08 - like intercept plus slope times birth
58:11 - rate
58:12 - and so if you want to solve for just
58:15 - like
58:16 - phones um
58:19 - then you have to take you have to
58:22 - exponentiate both sides so
58:25 - i think in python if you do mp.log it's
58:28 - the natural log
58:29 - so um right
58:33 - you end up having to
58:37 - exponentiate mean like take e to the log
58:41 - of both sides and so you can kind of
58:44 - work out
58:46 - what this what this relationship is
58:49 - if you take if you exponentiate both
58:51 - sides and solve for
58:53 - this um i'll leave that to
58:56 - everybody to read the article on your
58:59 - own
58:59 - but i hope that this live stream gave at
59:02 - least
59:03 - some examples and some
59:06 - first sense of how you might want to
59:09 - transform some of your data
59:11 - so that you can more effectively model
59:14 - it using
59:14 - linear regression well
59:18 - um so with that i will
59:21 - wrap things up um we are
59:25 - we've been having office hours on
59:27 - thursdays
59:28 - on discord so you can join us there if
59:31 - you have any questions i know there was
59:33 - a really good question when i checked
59:35 - yesterday um
59:36 - and i'm going to get to that today try
59:39 - to answer that on discord but we'll
59:41 - discuss
59:42 - a little bit more um on thursday and
59:45 - feel free to join us
59:47 - if you'd like to ask anything um there's
59:49 - also a chat on there so you can
59:52 - leave us any questions um
59:55 - if you want us to like take some time to
59:57 - think about it beforehand and give you a
59:59 - more comprehensive
60:00 - answer cool
60:04 - anything to add jamie um yeah no thanks
60:07 - for letting me join you today sophie
60:08 - um it was very fun and i learned a lot
60:12 - i'm sure everyone watching did too sorry
60:14 - all right well thanks for joining me
60:17 - all right have a good day everyone

Cleaned transcript:

um she'll be live in just a moment okay i believe that we should now be live on youtube um but drop us a note in the chat if you can see us good morning on this beautiful tuesday um i'm sophie and jamie is joining me today to help answer some questions and keep me on track so let us know if you are there in the chat if you have any questions throughout the stream please just drop those questions in the chat jamie will stop me or he'll try to answer them if he can directly um but yeah we always love to know where people are joining from i see someone from greece and from alabama very exciting so let us know cool alex lackson says you're live thank you um awesome so i guess we'll get started um and we'll we'll kind of jump right in it's good good to know that youtube is working um we're back to last week we did a stream in the office this week we're back from home so uh we were really high tech last week a little lower tech this week but hopefully we'll get the office set up up and running uh more frequently in the future so um i'm gonna go ahead and share my screen and then we will kind of jump right in all right can you see that jamie yep okay cool so this is a little bit late on to the github page so if you haven't had a chance to take a look through these notes that's totally fine um i'm actually going to before we even get started with these notes i just want to show you um on codecademy where you can find the article that covers the topic that we're going over today um so if you search linear regression uh you'll see this linear regression in python course this is the course that we're going through in this series and right now we've kind of talked about simple linear regression multiple linear regression um last week we talked about interactions and polynomial terms and then this week uh there's this article in here on log transformations and more um this this stream is going to cover even more of the and more part of that um but if you take a look at this article um there's a bunch of helpful information here and it it explains all this with an actual data set um today i'm going to use some um simulated data and i'm actually going to kind of take you behind the scenes if if you will and show you how i'm simulating data because i actually think that this is a really useful skill as a as a data scientist as a statistician to be able to create data that follows some or that represents some situation that might occur in real life and then see what happens when you try to fit a model on that data so it's really useful for debugging as well so here um i'm just in this code uploading or uploading a bunch of or loading a bunch of libraries i see uh someone said sophie you did not finish the modules um i i did finish the modules i actually have reviewed all of them very very carefully um but but yeah it doesn't when i go through on codecademy it doesn't mark them as complete um i actually wrote some of the ones that it says i haven't completed which is funny um okay so anyway so we're gonna simulate some data so what's happening here the reason i have these random seed functions in the code is just so that if you're downloading this code on your own you get the exact same numbers this will all work without this random seed it'll just generate slightly different data every time now what i'm doing here is i'm just using within the numpy package or numpy library there's this random module that has a bunch of uh basically probability distributions that you can sample from so if i sample from a normal distribution i'm basically just like sampling from a bell curve and i can specify the mean and the standard deviation of that bell curve which i've specified as 167 and three and then um three is the uh variance or the standard deviation and then 150 is how many values i'm sampling so i'm just gonna break this down for a minute so that you can see what i'm doing um but if i run this um and just print out the results i'm just getting a whole bunch of randomly generated numbers that have a mean of 167 and a standard deviation of three so you can see all of these numbers are pretty similar to 167 they vary um around that usually within two standard deviations or three standard deviations so like you're not going to see many numbers that are more than nine greater than 167 so you're not going to see much that's larger than 176 but and then or smaller than 167 minus 9 but basically just randomly generating some numbers and then in this next step what i'm doing is i'm taking um the numbers that i just randomly generated and i'm just like multiplying them by some number and adding some numbers so you might recognize this as basically the equation of a line i actually grabbed these numbers from a regression and then i'm adding some random noise so if i didn't add this in um and you can see a plot down here of the data that i just randomly generated this is the um height and this is weight right that i randomly made up if i didn't add this random noise um these would all be in a straight line because we would be simulating our weight variable to be exactly linear with height so i can actually demonstrate that um really fast if i do something like this i get a bunch of points in a straight line um and then now right i add this back in i've simulated some data it looks like this i'm calling it height and weight just to make this i don't know i guess a little bit more uh grounded and like a physical example but um but yeah that's all we're doing here i see oh yeah i see a question about are we doing the ols without an api um so i assume you're asking about this like oops uh this statsmodels.api uh library if you're not asking about that then you can clarify um but so we actually managed to do it without the library at all uh in the week that nitia was joining me i think that was two weeks ago uh we did it with some matrix algebra in numpy um and then there's definitely some code throughout to do this in scleron as well if you're interested in learning another library um but today we're gonna still stick with the statsmodels.a module because again it gives us some good information that's useful for for debugging models cool all right so today we're going to talk about some of the things that can go wrong when you fit a model and then we're going to talk about some of the ways of fixing them fixing those things um sometimes you're gonna notice that something went wrong uh because you got a warning when you fit the model sometimes you won't get a warning and you'll be checked you'll need to check the assumptions of the model and then you'll see something that surprises you and that will that will clue you in that you need to do a little bit more investigation so um so i'm gonna start by just fitting a model with this data that i randomly simulated and we've seen something like this before i'm gonna fit a model where i predict weight based off of height and then when i fit this model i see and we've seen this the same warning before um but i see this warning that says the condition number is large this might indicate that there are strong multicollinearity or other numerical problems okay so i think last time we saw this um the strong multicollinearity part um we talked about a little bit more in depth and we we discussed how that has to do with having predictors in the model that are colinear with each other or correlated with each other and we talked about why that might not be a good idea to include multiple predictors that are highly correlated um and but in this example we've only fit this model with one predictor so there's no chance that we have correlated predictors um so we need to think about what else could be wrong in this situation what else could be causing this warning and so um one of the things let's look at this picture that you might notice when we we first look at this this graph is that it's automatically in matplotlib is plotting this so that it fits it perfectly fits within the axes but we see that it starts at like 1 60ish or a little below 160 on the xaxis and it starts at like 54. on the yaxis and so if we get this intercept of negative 23 and we talked about this i think in one of the first live streams um in order to visualize what that intercept looks like we really have to zoom out a bunch on this plot so i think i have yeah so here this plot is again i can i'll pull this up here so you can see it um all in one place right so if we just plot this looks like this if we plot it and we change the axis so that the xaxis starts at zero and goes up to 180 because that's roughly how far up it goes on this xaxis and then on the yaxis we have it start at zero and go all the way up to 70. um we're gonna get this kind of like squished plot um which is to say that like the negative 21 and here i can even have the uh the y axis go down to like negative or was it negative 23 yeah so we'll have it go to negative 25. this is coming from the fact that if i drew this line in and we can even let's draw it in um we can do equals dot all right 0 180 and then y i'm doing it the the oldfashioned way we'll grab we'll even grab these so the intercept is negative 23. and then the slope was 0.5166 if we add these to our plot in the form of a line right we see that so this is our regression line and the negative 23 is the yintercept and the reason it's negative 23 is that in order to draw this line we've got to go all the way back to the yaxis which happens at x equals zero or height equals zero and then it hits at negative 23. jamie do you have any um any thoughts about like why this might be problematic or like what um what might happen if the data were to just change slightly like if we changed the the slope of that line just like ever so slightly um because one of we had a point down here um all of a sudden what would what might the issue be um i think like a main issue would be that like a slight change in the slope so like any outlier that like it potentially changes the plot would change the intercept like pretty heavily um so i think like that is one issue i see is that like the manager you're talking about or yeah yeah that is exactly what what i was intending so right so um so yeah so this is like a kind of unstable estimate for the intercept because and i think we talked about this a little bit when we started when we first talked about um the definition of an intercept or like the interpretation of an intercept so um jamie i don't know i don't remember if you joined this uh first live stream or if this is if you're caught up on this which it's totally fine if not but you remember how we would interpret this intercept it's like value of negative 23. yeah yeah so since we were talking about i think it's xaxis is height yaxis is weight or is it yeah um so this would be this interception is basically saying that a person who has a height of zero i guess centimeters has would be expected to have a weight of negative it's like 25 22 or something um pounds or whatever or kilograms or whatever it's um the scale is so it's probably like not super informative here or like not super uh what's it called like sometimes you'll have intercepts that like do make sense um like if you're doing like hours studied versus expected test score like zero hours studying expect tests where like that could make sense um i don't know anyone who who sorry who has a height of zero centimeters who has a negative weight um but so yeah i think i think this one um is potentially a little bit uh less less informative um yeah yeah exactly um i realized it maybe is more useful if i use this like sns scatter plot and um because then i think it like plots the uh or at least gives you the labels and we'll do that let's see if that works better yeah there we go um so yeah so exactly so this is the predicted value of weight when height equals zero but just like jamie said there's no such thing as having a height of zero right we have no data all in this part of the plot so drawing the line there like we can say that we the there's this relationship but really we only want that relationship to want to say we don't really only care about that relationship where the data exists which is only in this part of the plot and so we're estimating something that's not really of interest to us and it's also an estimate of something that's really easily changed by just like small changes in the data because it's so far out from where the data is that it's it's kind of unstable so there's a number of things that we can do to fix that um and so this is maybe one thing that's not in the article that i originally wanted to include in this course and we just didn't have time but but i'm going to demonstrate it now so actually before before we even get there um one thing i also actually sorry i'm changing my mind as we go uh let's yeah let's actually let's fix this first and then we'll we'll go back and talk about some of the other assumptions so um so there's a couple different ways to fix this one way to fix it is simply to center the height variable um and so i'm going to talk you through what centering means in just a second and we can actually do it together so to center something all we do center a variable all we do is we take the mean of that variable and we subtract it from every single value so i'm gonna go ahead and in my data i'm gonna find the mean height so i'll print that out first so the mean height in this data is about 167 centimeters and then what i'm going to do is i'm going to subtract that from every value in the height column and what this does is it just creates a whole list of values of heights um with the means subtracted um so jamie do you want to like talk through what what are these numbers now interpreted as like if we see 4.928 what does that mean uh so i think that means that like this specific like data for this specific data point um it's like 4.9 like the height like this person's height is like 4.9 um i think centimeters greater than the mean so it's basically like the distance just like from the mean exactly so yeah so this person is 4.23 or sorry for 4.93 centimeters above the mean in terms of their height this person in the third row is very very close to the average height in the data set so if they had a value of zero it would mean that they were average height so what we can actually do is we can create a new variable we'll call it height centered that contains these values and then i'll print the um the first few rows of the data again to just take a look um and so all we did was we said okay we've got weight we've got height now i want to create another column that's called height centered and these values are now centered at zero right so a value of zero means average height anything above that means they're above the average height anything below that means they're um they are below the average height they do that right yeah yes okay so now what we can do is we can refit this model but use centered height instead right and you'll notice that that other warning went away so remember the first time we got this the condition number is large um this time that went away and we just get standard errors assume that the covariance matrix of the errors is correctly specified which i believe we get no matter what um that's always an error or a warning here um okay so that's one way and we're just kind of dipping our toes in that we can start to transform our data and by transform i mean take existing data and kind of do something to it so that we can fit a model that is a little bit more stable that doesn't create warnings that is potentially like a better fit to our our data um it's easier to interpret all of the above so i think one thing that is maybe useful before we move on to more complex example is to now look at what this picture looks like so let's go ahead and i'm actually going to use i like this lm plot function sometimes because it just saves us a little bit of time so let's replot on the xaxis now height center and on the yaxis let's replot weight and then the data is just called data and this will automatically plot the regression line show run um cool so now we see that the weight variable looks exactly the same but height centered right like the middle of these data points is around height centered equals zero which makes sense because that's the mean and and let's actually add let's add like a vertical line to this okay the code to do that somewhere um it's basically like this but ax line so this right here where i've drawn this dotted line this is the yaxis now so um so now the place where we cross the yaxis and i think uh we can estimate i i'll fit the model in just a second so we can see what it is but it looks like it's around like 62 point something maybe 63. um that value jamie do you want to take a another stab at interpreting that number um so i think it's like if you have a mean if you have like a height that's equal to like the mean of the data then you're well so i guess it's like when we centered it we're like at zero now so like that is the y that's like the x equals zero point um but i i'm guessing we need to say like if you like if you're at like a height of whatever like that pretransformation was and that's like what you would interpret it so okay it's like if you have like 62 or something or whatever the mean was and you have an expected weight of x of like 62 point something exactly so if your height centered value is zero it means that you are average height so the expected or the average weight of someone who is average height is 62 point whatever is how you would interpret that so now we've got a more interpret interpretable value and it's a little more stable right because like the intercept is now kind of in the middle of all of the data and so if we have another point somewhere it's going to be less likely to change what this value is um than before so um cool actually i think let's really quickly fit the model and just see what this looks like so if we go ahead and fit our model but now we use height center we get right an intercept of 62.7 our um our slope on height centered 0.5166 is exactly the same as our slope on height so the slope of our line didn't change it just changed the intercept and now that 62.7 is average weight of someone who's average height in the data and so we've got a situation where we can interpret it it's more stable we don't get um we don't get errors oh i already did this down here sorry um okay cool uh if there are any questions in the chat please let me know otherwise i'm gonna now move on to some log transform stuff so um i think one of the things that's really challenging about data transformations in general is that there are a ton of different ways that you can transform your data and it's hard to know exactly which one to do in any given situation so um one thing i want to say is like there are no hard and fast rules and there's also way more options than what we're going to cover today um for example we could instead of centering we could just standardize everything so we could also divide by the standard deviation um that would change that the slope and change our interpretation of the slope um but there are definitely situations where we'd want to fully standardize instead of just centering i also see a question can you explain the shaded area on the graph around the best bit line that is a great question um so this lm plot function it automatically creates this like confidence interval around the line which basically shows how confident we are that the line um is where where it's pictured so or like how like how sure we are that the line is here so you'll notice that like on the ends of it um this shaded area is wider here i can zoom in right the shaded area is wider on the ends because in this um like in this uh part of the the graph where there's not a lot of data and so we don't know like if we had one more point we don't know if it would be like close to this if it would be here if it would be up here um and that could potentially shift this part of the line a lot more than in here where the bulk of our data is uh we're more confident that the line goes through this point actually i think it would be potentially interesting we do like a lm plot on this what happens if i do that interesting so it won't even graph it outside of where the data is i'm assuming let's like let's make this like uh 130 or something yeah you can see it's like graphing this line in here it won't even graph it down here but you could imagine and it would be really nice if it did because i think you could visualize that if you see how and i know you have to look kind of closely but if you see how like the um confidence interval is really tight in here and it's it gets wider and wider and wider it would just get wider and wider and wider the further away from the data you went um so you could really see what i was talking about before where if you drew this line all the way down here that confidence interval would get really wide um around here meaning you're like very uncertain about what that intercept is that you're trying to estimate um oh okay i see uh alex noted that sns.lm plot is sns.reg plot and the latest version of seaborne fyi um yes uh the reason we have been using lm plot i think lm plot still hasn't been deprecated as far as i know um and we have we had uh an older version of seaborne on codecademy so we we don't have ragplot i think yet um but that is good to know thank you oh and then i see another comment wiki dd says i think they are still two different options with slight difference figure level or not not exactly the same arguments that is interesting i actually did not know that let's like i'm gonna just look it up really fast i think this is kind of cool um interesting it looks similar but it looks like it has some additional options yeah alum plot still definitely has not been deprecated um i think we used a reg plot for one of the off platform like the off platform data analyst and final portfolio project remember should we just try it see what happens oh no said x y and data right oh it's because i haven't recreated this yet so i guess in this example it basically looks the same but interesting it would definitely be interesting to do a um more careful comparison and see what the what the real differences are okay so um before we kind of move into the second half of this live stream i think it's useful to take this data which besides the fact that um it was we had this issue from before where like the um the intercept was kind of far away from the data points besides that that issue um this data is simulated such that it fits a linear a linear model really nicely in fact um i think it's kind of cool to just see um just prove to yourself that like things are working as they're supposed to um so when i simulated this data i used i used some numbers from a regression but i simulated it to have an intercept of around 21.7 and a slope of about 0.5 and then when i and but i randomly generated the point so it wasn't going to be exactly these numbers but this is what i kind of simulated i simulated an exactly linear relationship with some error um and then when i fit this model the first time my intercept was close to the negative 21 and the height and the slope was close to the 0.5 that i originally kind of set out to use to create the data so with that said right like if i create data that perfectly fits a linear model then i can see kind of what my checks of the assumptions of linear regression should look like if everything is kind of perfectly uh perfectly perfect for for the purposes of um of linear regression so i'm going to take a look at two different uh assumptions of linear regression so the first assumption that we'll talk about is homoscedasticity which is a very fancy word but it's pretty i think hopefully easy er to understand once you take a closer look so homoscedasticity essentially means that there is equal variation of the outcome variable for all values of the predictor so or predictors um if you're in multiple linear regression world so here um for height centered or height either one we notice that like the amount of variation on this part of the line is pretty similar to the amount of variation over here um and so because the variation is roughly the same for all values of height um the variation around the line is roughly the same for all values of height um this data follows the homo homoscedasticity assumption of linear regression um and we actually again we created the data to to do this because we added random noise centered at zero with the same level of variation for all of our values when we first created the data and so the way that we often check um homoscedasticity is we get the fitted values for our model the way we get the fitted values is we just predict um get basically like the predicted values for every for the original data itself um and so i will kind of show this i think we skipped over this in the first week but i think it's like important to kind of come back to now so the fitted values here are just the predicted weights for everyone in the data set then we can calculate the residuals which are the difference between the actual weights of every person and their fitted value or their predicted weight and then we can plot the fitted values against the residuals and we expect that um or sorry the residuals against the fitted values so we these are the residuals right the yaxis is the residuals these are the fitted values and we expect that the residuals should be centered at zero right because on average all of the points should be like equally or basically like some of the points are going to be below the line some of the points are going to be above the line but the line was defined such that um it's kind of going through the middle of all the points it's minimizing that vertical distance between all of the points in the line and so we expect there to be just as many points below the line as above the line um and we want this basically we want this plot to just look like a splatter we don't want to see any pattern any patterning we don't want to see that like the variation on this side is different from the variation on this side um and i'll show you what that would look like in just a second we also generally want to take a look at the distribution of the residuals um and we expect that distribution to be roughly normally distributed um so we don't want to have a lot of skew we don't have like a bunch of really small residuals and then um very few large residuals um so this looks pretty good it looks pretty symmetric there's one kind of hump around zero and and we feel pretty good about this but now let's go into a land where um where this is not the case and then take another look at how we could kind of get around that so going way down okay so here is some more simulated data that violates the homoscedasticity assumption and so basically i simulated this data so that the this error term is related to the one of the predictors and then um that means that like essentially for larger values of of age which i'm going to make my predictor in this made up data i'm adding more variation and for smaller values i have less variation so okay so is that kind of showing up in the i can't tell if i'm like seeing this one or not but um the shaded area for the alum plot it's like a little bit wider for like the data that's more like varied um in like the higher age yeah yeah i think it is that looks right um okay so uh i just honestly made up again some labels so that we would have something to kind of refer to instead of just calling it variable one and variable two so let's just say this is like age of person there are some very old people in this uh in this data but that's fine 120 yeah um and this is like how much they spent maybe should we say like on healthcare per year or something i don't know like making it up totally fake data um but but we see this kind of funneling basically where there's like very little um variation around the line for small values of age and for large values of age there's a lot of variation in spending like people who are 100 years old are spending between 40 and 140 versus like people who are 50 years old are really just spending between like 42 and 47. um so we're not going to get any warnings like if we fit this model predicting spending based on age we're not going to get any warnings at all besides this like our typical standard errors assume that the covariance matrix is correctly specified um but if we were to go through and take a look at that plot that we just made for the last set of data we'd see that we get this kind of like so again this is the residuals against the fitted values we start to see this like funneling effect um i think the the like official term for it is funneling i don't know at least that's what i've seen so um this is happening remember because if we go back to this plot and say and this is the line that we're fitting the residuals are going to be really small down here like these are very small residuals um and the residuals are going to be really big over here like this point for example is going to have a really big residual um and this point is and this point is so we've got smaller residuals for smaller values of age and larger residuals for larger values of age and that's what we're seeing here smaller residuals on one side of this graph larger residuals or a bigger range of residuals on the other side and if we look at a histogram of the residuals they're still roughly normally distributed they're still in this example um residuals that are small and residuals that are large and um there's still more common for small to see small residuals than large residuals um but we're we're violating one of the assumptions the homogeneous to the assumption of linear regression so um i'm going to break for a second because i see that we're going to have we have like 12 minutes left and i'm gonna um try to save a little time i think by looking at this uh article really fast so in this article we see an example where there's both the issue of homoscedasticity is violated because you see there's like a lot more variation over here than over here but also the issue also in this example the residuals are not normally distributed um there's going to be i think a lot more large or yeah a lot more like a lot longer tale of larger residuals than small residuals um is there a point at which like like um for like it to like like hit the like the residuals are normally distributed is there like a point at which like you realize that they're like it doesn't like hit the like it's like not like a good amount of normality um because like sometimes like you'll have a graph there's like some slight skewness versus like a lot of skewness yeah that's a good question so there's actually so there are some more rigorous ways of checking the normality assumption um one is a qq plot which basically allows you to visualize how far off the points are from like what they would be expected to look like if it was a perfect normal distribution um but then there's also a hypothesis test i'm great i'm blanking on the name of the test um but i'll i'll look it up and i'll post it somewhere later um but there is a like a hypothesis test you could run for normality uh which is something that people sometimes do and just like set a um a significance threshold ahead of time and then um and then yeah is it the shapiro wilk yes yep that's it yeah i was like thinking that it started with a w but yeah yes shapiro wilkes um is the one that i've seen most commonly used um okay so the other thing like that i notice about this right off the bat is that um is that this is kind of this data looks kind of uh not linear in that um like if i were to draw a line through this set of points i would draw a curved line right like it would probably start up here and it would like curve around to here um and whenever um and whenever you see kind of like this nonlinear relationship that also can be a indicator that you maybe need a different method to model this relationship whether it be adding a polynomial term or in the case of a log transform log transform can help manage this like homoscedasticity violation which would be called heteroskedasticity i believe um and then also help with this lack of normality among the residuals i also see alex said i think some people also check the two parameters skewness and kurtosis to assess normality yes um yeah so there are summary statistics that you can calculate called skewness and then i think kurtosis is like how um it's like peakedness of the um of the distribution i believe and you can you can calculate those values and then use that to to assess if you want like a not hypothesis test method um yeah for sure so um basically log transform generally what we would do is we would take a take the log of the outcome that variable and then fit that in the model so i'm gonna do that back in my code um instead of having spending i can just like before when i had centered um centered height i can do log spending and i can end up with therefore a new column where i've got the log of my original spending column and then if i plot age versus log spending again you'll see that this looks a little bit better than the original data it still got a little bit of fun funneling but it definitely like improved that slightly so again like this was the original where we've got quite a lot of funneling happening and then this is after we plot age versus log spending instead of age versus spending is there a way to like know that like you should try a lot transformation on a like y variable like is like funneling like a good time to like try a log transformation or yeah so there's lots of reasons why you might try it um one reason so i think like one reason that you might try it is if you see that funneling another reason you might try it is if um if you have skewed residuals so like a lot of times what happens with the skewed residuals is that you you have nonlinear um you have a nonlinear relationship so like you could imagine if these points were even further like above this where it was like points really following like this this kind of a line um that kind of shape you end up with really small residuals down here and then really large residuals up here overall and so you end up with this like weird distribution of residuals um which really comes from this like nonlinear relationship that's another time to try a log transform um and also like right off the bat sometimes you'll just notice that you have a outcome variable that probably benefits from being put on a log scale so a lot of the time that will happen with measurements like salary or or cost of something rental price something like that where you end up with a really skewed distribution of your outcome variable um or like very large numbers down to like very small numbers like in this case we have a whole bunch of values that are close to zero uh in terms of this like phones variable and then we've got some values up to a thousand and so taking the log just helps normalize this like y variable like i'm pretty sure if you were to plot the distribution of phones um it would be skewed cool um all right we are we're running tight on time so i'm actually gonna kind of switch over to showing you well i can show you quickly on this example um so we took the log transform we fit our model with log spending instead of regular spending and then we end up with a lot less funneling in this plot there's still some funneling for sure like it's not perfect um but this definitely helped us um build a slightly better model um i think i forget if there's a picture in this article yeah so now um i think this one is maybe a better example i was trying right before this to simulate some data to kind of show you something that looks more like this but i was being slow to simulate data so i think it's it's maybe better to just show you this um but if we look at this relationship now between log phones and birth rate we see that the relationship between law the log of phones and birth rate is a lot more linear than the relationship between phones and birth rate and so by taking the log we've made this more linear we've addressed the homoscedasticity issue a little bit more there's less of this funneling in the original plot and so when we take a look at a histogram of the residuals and the residuals against the fitted values we end up with something that looks a lot more reasonable this looks a lot more normally distributed and this looks a lot more like a splatter of points we don't get that same funneling as we had before there's a whole bunch more in here that i was hoping to cover today but i don't think we're gonna have time about the interpretation or like how you would interpret a model where you took the log of um of the y variable or the outcome variable it's really just a bunch of math so what you're fitting in this case is like log of the outcome variable is equal to this like intercept plus slope times birth rate and so if you want to solve for just like phones um then you have to take you have to exponentiate both sides so i think in python if you do mp.log it's the natural log so um right you end up having to exponentiate mean like take e to the log of both sides and so you can kind of work out what this what this relationship is if you take if you exponentiate both sides and solve for this um i'll leave that to everybody to read the article on your own but i hope that this live stream gave at least some examples and some first sense of how you might want to transform some of your data so that you can more effectively model it using linear regression well um so with that i will wrap things up um we are we've been having office hours on thursdays on discord so you can join us there if you have any questions i know there was a really good question when i checked yesterday um and i'm going to get to that today try to answer that on discord but we'll discuss a little bit more um on thursday and feel free to join us if you'd like to ask anything um there's also a chat on there so you can leave us any questions um if you want us to like take some time to think about it beforehand and give you a more comprehensive answer cool anything to add jamie um yeah no thanks for letting me join you today sophie um it was very fun and i learned a lot i'm sure everyone watching did too sorry all right well thanks for joining me all right have a good day everyone
