With timestamps:

00:00 - stream series on linear regression
00:03 - um we're gonna be kind of reviewing a
00:07 - lot of the stuff that we covered in the
00:08 - last
00:09 - seven weeks in one day and we're just
00:12 - gonna kind of play with a data set
00:14 - and really go through a full process
00:17 - from start to finish
00:18 - of how do we get some data how do we
00:21 - clean it up a little bit
00:22 - how do we fit some models how do we
00:24 - compare them
00:25 - and maybe we'll come up with we'll see
00:27 - if we can come up with the best possible
00:29 - model
00:30 - that we can for predicting rental prices
00:33 - uh by the end of this session yeah i
00:36 - think this session in particular is kind
00:38 - of
00:39 - more open-ended than our last ones
00:40 - basically our plan is to play around
00:42 - with this data set and so
00:43 - definitely if you're watching this in
00:44 - the chat as we are doing this live
00:46 - um and you want us to like take a look
00:48 - at any of these features or want us to
00:51 - experiment with this in a particular way
00:53 - we are super open to
00:55 - uh kind of uh making this as interactive
00:58 - as possible
00:58 - so um yeah exactly yeah we want this to
01:02 - be kind of a fun way to kind of
01:04 - wrap everything up but also make it
01:07 - explorational explorative
01:10 - i don't know um okay cool
01:14 - so um one thing i do want to point out
01:17 - before we get started
01:18 - is that um if you're following along
01:21 - with the linear regression course
01:23 - on our on our site and i'll i'll go
01:27 - to codecademy really quickly right now
01:30 - and pull that
01:31 - up so um if you go to this linear
01:35 - regression in python
01:36 - course and take a look at the syllabus
01:39 - um
01:40 - the data set that we're using in a
01:42 - cleaned form
01:44 - exists in this um last project
01:47 - in the in the course this craigslist
01:50 - analysis
01:51 - so if you'd like to play around with
01:54 - this and you want to
01:55 - just get the data already loaded for you
01:58 - and
01:58 - see an example solution
02:01 - this is a little bit subseted and it's a
02:04 - little bit cleaned up
02:05 - but you can do that here if you have a
02:07 - codecademy subscription
02:09 - and even after this
02:12 - this stream if you want to play with it
02:14 - yourself but you don't want to set up
02:15 - your
02:15 - local environment this is a great place
02:18 - to come and um
02:19 - and play with it cool
02:22 - so i'm just going to show you i'm not
02:24 - going to go through the full process but
02:26 - i'm just going to show you
02:27 - where i downloaded the data from and
02:31 - the reason i'm showing you this is
02:33 - really just because
02:35 - if you're working on your own computer
02:36 - you're doing your own analysis you're
02:38 - just getting started
02:40 - um you might come to a place like kaggle
02:43 - or the uci machine learning repository
02:45 - that we've shown before
02:46 - and you might download your own data set
02:49 - and so i think it's
02:50 - it's useful to kind of see the process
02:52 - from the beginning
02:53 - um so this is the data set we're going
02:56 - to be working with i found it on kaggle
03:00 - just by clicking going to kaggle.com
03:02 - clicking data sets and then searching
03:04 - around within data sets
03:06 - um i like to use data sets that have a
03:08 - high usability rating so 10.0 is
03:11 - like the highest i think that you can
03:14 - get um
03:14 - so if you're just starting out it just
03:16 - means it's really easy to
03:18 - download this data and get started with
03:19 - it it's not super super messy although
03:21 - this is a pretty big data set and has
03:24 - um some columns that you might not care
03:27 - about
03:28 - but it's super easy to use it's got um
03:31 - it's got like descriptions of everything
03:34 - um it's got like an overview it's got
03:37 - easy download all of that so
03:41 - we can take a a brief look at it even on
03:44 - kaggle we see there's
03:45 - 22 columns 10 of them are printed out
03:48 - here
03:50 - so you can get a sense for what kind of
03:51 - information might be contained in here
03:55 - and then we can also get a little bit of
03:58 - of
03:58 - context and information about how this
04:00 - data set came
04:02 - to exist on kaggle um so it gives us
04:05 - this
04:06 - the sentence craigslist is the world's
04:07 - largest collection of privately sold
04:09 - housing options yet it's very difficult
04:11 - to collect all of them in the same place
04:13 - so this person built this data set as a
04:15 - means by which to perform experimental
04:17 - analysis on united states
04:19 - um as a whole and instead of isolated
04:22 - urban housing markets
04:24 - the data is scraped so web scraped every
04:26 - few months
04:27 - so somebody else has done all the web
04:29 - scraping for us which is nice
04:31 - um it contains almost all relevant
04:33 - information that craigslist provides
04:35 - on retail sales so um this is just data
04:38 - that has been web scraped from
04:40 - craigslist
04:41 - about housing listings yeah the license
04:44 - is kind of interesting
04:45 - in if you're like making a project for
04:47 - your portfolio or something that
04:49 - you're making for your own personal
04:50 - projects you might want to look at the
04:52 - license and just kind of like dive into
04:54 - um what you're allowed to do with the
04:56 - data set
04:57 - um obviously if it's on table like this
05:00 - um you'll probably be able to like
05:01 - download it and play around with it
05:02 - yourself but again
05:04 - you might want to look at the license
05:05 - when you think about hey how should we
05:07 - be
05:07 - like publishing something related to
05:09 - this this data set
05:11 - yeah exactly um
05:14 - okay so i think we can get started so
05:17 - all i did was i pressed this download
05:19 - button
05:20 - um you have to create a kaggle account
05:22 - oh you can't see my mouse
05:24 - but you can see that it's highlighted in
05:26 - the top right
05:28 - of this right hand corner of this page
05:30 - there's this download button
05:33 - and you have to create a kaggle account
05:35 - once you press download it will download
05:37 - onto your computer
05:38 - and then all i've done is i've moved
05:41 - this housing.csv file that i that
05:45 - was downloaded when i press that button
05:47 - into the folder
05:48 - where my jupyter notebook is or it could
05:51 - be the folder where you've got
05:53 - um like your script that you're writing
05:57 - as well and then i've just opened up
06:00 - this
06:00 - code dot ipynb
06:04 - um this jupiter notebook and this allows
06:06 - me to start playing with the data
06:08 - yeah and again if you aren't super
06:10 - familiar with how to work with jupyter
06:12 - notebooks we've got a lot of articles
06:13 - that kind of
06:14 - help you set that up it's slightly
06:16 - complicated to just like open the
06:18 - jupyter notebook to begin with i'm
06:20 - assuming you did it through
06:21 - terminal or did you do it through um
06:23 - like the jupiter
06:24 - app i did it through terminal um it's
06:26 - actually
06:27 - like once you have it downloaded um it's
06:31 - relatively simple to do via the terminal
06:34 - um i think you just
06:35 - you just type jupiter notebook into the
06:37 - terminal
06:38 - um but yeah it definitely takes a little
06:41 - setup on your computer first before you
06:43 - get that running
06:45 - um and i think we we provided some
06:48 - information on how to download
06:50 - everything on the first
06:52 - stream i believe cool um
06:56 - cool so um i'm just gonna load a bunch
06:59 - of libraries i don't know which ones
07:01 - we'll end up
07:01 - using and then i'm gonna go ahead and
07:04 - load this data
07:05 - um you'll notice normally i just do
07:09 - like this right read in the csv
07:13 - um but let's just print what that looks
07:15 - like
07:17 - um
07:21 - oh actually seems like it's fine i
07:23 - pulled this from somewhere else
07:25 - um you might notice though things that
07:27 - happen when you
07:28 - when you first load a data set
07:31 - um you might see like the header for
07:34 - example is getting
07:36 - pulled in as a row instead of um
07:39 - instead of like column names and so
07:42 - that's what i was intending to do in
07:44 - that first line of code but
07:45 - it looks like it's being read in okay so
07:48 - that's fine um and i've used the pandas
07:51 - library to read it in
07:53 - and so i think a good goal for today is
07:56 - to try to
07:58 - come up with a linear model that allows
08:00 - us to predict
08:02 - price and in the process we can start
08:05 - thinking about what the relationship
08:06 - between
08:07 - price and some of these other variables
08:11 - is so
08:12 - type of apartment square feet beds baths
08:15 - um but also all this other stuff got
08:17 - like parking options
08:18 - laundry options comes furnished
08:22 - um so we got lots of lots of information
08:26 - and then we're going to see if we can
08:28 - figure out the relationship between all
08:29 - of those things in price and see if we
08:31 - can predict
08:31 - price accurately and maybe we would be
08:34 - doing this
08:34 - if we are renting an apartment ourselves
08:38 - or maybe if we are the owners of an
08:42 - apartment and we're trying to figure out
08:44 - like what is the
08:45 - the best or the appropriate market value
08:49 - for this apartment
08:51 - okay so um let's take a look at a couple
08:54 - of things
08:54 - if we're trying to build a linear model
08:57 - for price
08:58 - um are there any columns that
09:01 - you think we should get rid of alex so
09:04 - there's all this kind of like
09:06 - metadata information like the url
09:09 - the id even i don't know if we
09:11 - necessarily need that
09:13 - um i mean it looks like all of these are
09:16 - from reno
09:17 - tahoe so like region url it seems like
09:20 - it's already represented in the region
09:22 - column um so i would probably get rid of
09:24 - region url
09:26 - um that's all i would get rid of from
09:28 - what i'm seeing here there might be more
09:30 - things to the right if we keep looking
09:31 - but uh
09:32 - so let's actually we'll prac we'll print
09:34 - out the info so that we can see
09:36 - first of all how many rows and columns
09:39 - there are and see all the names
09:42 - um looks like so we've got
09:45 - around 385
09:48 - 000 rows so a fair number of rows and
09:51 - then 22 columns we saw that before
09:54 - um i agree it that it makes sense to get
09:58 - rid of
09:59 - some at least id and url
10:02 - um it looks like id for example
10:06 - is being read in as an integer so we're
10:09 - thinking of this as like
10:11 - a numerical value but like an id
10:14 - that is a higher number like this is a
10:16 - higher number than this
10:18 - i believe if i'm looking this correctly
10:20 - but
10:22 - no i have to go back further this this
10:26 - bottom one actually is a larger number
10:28 - than the one above it but
10:30 - um that doesn't really mean anything
10:33 - meaningful about this apartment with
10:35 - respect to this one doesn't mean
10:36 - like it has more of something it just
10:39 - means that
10:41 - it has a different id yeah this might
10:43 - even be a good example of something that
10:44 - are like not correlated
10:46 - at all and so would not make for a good
10:48 - uh good linear regression where if we
10:49 - mapped like price to id
10:51 - the id is just kind of like a randomly
10:53 - generated number presumably or
10:55 - um or at least that's what i'm assuming
10:56 - and so you wouldn't really see any
10:58 - relationship there
10:59 - yeah and then for url
11:02 - um i mean i don't want to do it because
11:05 - i don't want to like
11:06 - create a problem well we can do it on a
11:08 - smaller scale but
11:10 - um what would happen alex if we if we
11:12 - just like
11:13 - put url in it looks like it's being
11:16 - saved as a string
11:17 - right um if we added url to
11:21 - our linear model what do you think would
11:23 - happen i mean so i think that that would
11:25 - try to break it that would consider that
11:26 - like categorical variable right of like
11:29 - hey there are
11:30 - 700 categories and none of them share
11:34 - you know uh category one is this url
11:36 - category two is this url so it's just
11:37 - seven under categories and they're all
11:39 - different because none of them share
11:40 - the same url presumably yeah so
11:44 - let's actually do this really fast
11:45 - because i think this is kind of a funny
11:47 - exercise
11:48 - let's um let's start by just
11:52 - making a um a smaller subset of this
11:55 - housing data
11:55 - [Music]
11:57 - where we'll just take um
12:02 - i always forget the syntax here is it
12:04 - like
12:05 - if you just want the first couple of
12:07 - rows
12:09 - and then you do like i think it's like
12:11 - zero to zero colon five won't give you
12:13 - the first five i think
12:15 - yeah i think let's see
12:32 - cool um okay so let's do
12:36 - like let's do
12:41 - 50 rows and then let's fit a model
12:46 - i'm gonna actually like grab my
12:52 - i'm gonna grab some code from somewhere
12:54 - random
12:55 - um again because i never memorized these
12:59 - uh
13:02 - these things so i think uh
13:06 - breaking down uh breaking down this line
13:10 - of code real quick
13:11 - so we are using the sm module which if
13:14 - we look at our
13:14 - import statement is statsmodel.api
13:19 - and then within that we are using if you
13:21 - scroll back down to your line of code
13:23 - within that we are using the uh
13:27 - ols what does ols stand for do you know
13:29 - ordinary least squares
13:31 - okay so this is remember at the
13:34 - beginning we kind of talked about
13:36 - um ways of fitting linear regression
13:39 - model
13:40 - and the way that we've been using is
13:42 - called ordinary least squares because
13:43 - we're minimizing
13:46 - the squared distance between
13:49 - each point and the line essentially okay
13:52 - and then we're saying we're building it
13:54 - from a formula and that formula
13:56 - is what uh based on predicting price
13:59 - based on url
14:00 - yeah and i'm gonna use housing
14:03 - underscore sub
14:04 - and then i'm just gonna print model
14:07 - one dot params
14:10 - and so like how many parameters do you
14:13 - expect to see
14:14 - i expect to see what 50 parameters
14:18 - because it's uh
14:19 - yes again this is the example where
14:21 - they're using a url as a category
14:23 - and there's presumably 50 different urls
14:26 - and so
14:27 - yeah yeah i think there might be fifth
14:31 - well there's going to be 50 unique urls
14:36 - and then um minus one
14:39 - reference group right so 49
14:42 - plus a intercept so yeah you're exactly
14:45 - right there's gonna be 50 of these
14:47 - and we end up with so here's the
14:49 - intercept and then you see
14:50 - we have a slope for url true dot
14:54 - this whole thing url and then we have a
14:57 - different one for url
14:58 - true dot this whole thing um
15:02 - and then this it keeps going right so
15:05 - pretty good example if
15:06 - you just like take your data set without
15:07 - thinking about it you're going to
15:10 - um run into something like this where
15:13 - suddenly we have
15:14 - yeah 49 parameters based on these these
15:16 - random urls that's definitely not what
15:18 - we're
15:19 - what we're interested in doing yeah and
15:21 - this is actually one of the things that
15:23 - um so we can talk a little bit about
15:26 - this if we have time
15:28 - a little later today but i think um a
15:31 - lot of times
15:32 - people end up using other packages i
15:35 - think the most common one
15:36 - is scikit-learn to fit models
15:40 - and when you fit a model in scikit-learn
15:44 - you can get the you can get these like
15:48 - uh parameters but they don't come with
15:51 - labels
15:53 - automatically and it's like
15:57 - it's a library that's really built
15:59 - around like
16:00 - building the model and making
16:01 - predictions not necessarily like
16:03 - inspecting the model output and so i
16:06 - think that that one of the reasons i
16:08 - like stats models
16:10 - is that when if i were to print out like
16:12 - just the standard model summary here
16:15 - like it's gonna give me i'm gonna have
16:17 - to like
16:18 - see that something crazy happened here
16:22 - um and then that hopefully gives me a
16:25 - clue that i need to like
16:27 - be more purposeful about my model um and
16:30 - it's not always
16:31 - the diagnosis that that has to happen
16:34 - isn't always like
16:36 - it's not always really detailed and
16:38 - small sometimes it's like
16:40 - a big mistake like this where
16:43 - you just threw something into a model
16:45 - and it didn't make sense
16:46 - um but you weren't thinking about it
16:49 - necessarily before
16:52 - so yeah we have a question on the
16:53 - facebook page of how do we deal with
16:54 - those categorical variables
16:56 - um do you so one we
16:59 - we touched on this in much greater
17:01 - detail in an earlier session so take a
17:04 - look at our youtube page
17:05 - and if you look at this series you'll
17:07 - find you'll find videos about dealing
17:08 - with
17:09 - category categorical variables but do
17:11 - you have like a quick answer of like
17:12 - how would you deal with something like
17:14 - this well so you can definitely include
17:16 - categorical variables
17:17 - in your model um but you need to be a
17:21 - little bit careful about it because
17:23 - depending on how many possible values
17:25 - there are
17:26 - of a particular variable you could end
17:29 - up with
17:30 - a whole lot of parameters in your model
17:32 - which means you have to estimate
17:34 - a lot of things and you've now created
17:36 - something that's super complicated
17:38 - um that may or may not really be
17:40 - improving the model so
17:42 - in our last session we talked a little
17:43 - bit about like what's the
17:46 - trade-off between like accuracy of a
17:50 - model
17:50 - and or predictive ability of a model
17:53 - and like complexity complexity
17:57 - and i think depending on the problem
17:59 - there's maybe a different
18:00 - balance there's no right or wrong answer
18:03 - but
18:04 - i definitely think like
18:07 - there's a limit to how much complexity
18:09 - you potentially want in your model
18:12 - so we'll we'll actually we'll talk about
18:15 - that a little more today too
18:16 - yeah so i'm curious what and this is
18:18 - maybe a tangent but
18:20 - in looking at the url and then i was
18:22 - also curious about the column um
18:23 - description like we scroll over to see
18:26 - the description on some of these just to
18:27 - get a sense
18:28 - of like what those look like um
18:32 - what would you think of doing something
18:34 - like
18:35 - creating a new column that's like you
18:37 - know is the word
18:39 - um stunning in either the url or the
18:42 - description right
18:43 - could we do some like natural language
18:45 - processing of taking these
18:47 - taking this information that's in you
18:50 - know
18:50 - paragraph form or natural language form
18:53 - and trying to create new categories
18:55 - um like that or like you know definitely
18:58 - imagine
18:58 - you know washer washer dryer is already
19:01 - a column in here but let's say that
19:02 - column didn't exist
19:04 - could we look in the description and try
19:05 - to like create our own variable of like
19:07 - oh
19:07 - the description mentions a washer dryer
19:10 - totally
19:11 - yeah so that's a really good point um a
19:14 - lot of times
19:15 - in your data pre-processing stage
19:18 - you might like right now i think for
19:21 - time's sake we'll probably just get rid
19:22 - of this description column
19:24 - although we could we could probably
19:26 - relatively quickly write some code to
19:28 - pull
19:29 - something out from here um but
19:33 - but yeah for for sure there are
19:35 - definitely
19:37 - places where you can take data that
19:39 - exists in a format that
19:41 - you're not going to use like you're not
19:42 - going to throw description into your
19:44 - linear model
19:45 - and turn it into usable information
19:48 - like i like your example of if the word
19:51 - stunning is in here like maybe
19:54 - or i'm trying to think of another yeah i
19:56 - was looking i was thinking that when i
19:58 - was looking at the
19:58 - urls if you scroll down to uh or maybe
20:01 - you got rid of it at this point but um
20:03 - if you if you look at those urls
20:07 - um which were showing up in the when you
20:10 - were showing all of them
20:12 - when you were showing all of the uh
20:14 - parameters
20:15 - but the urls are like you know
20:18 - craigslist.org stunning dash bedroom
20:22 - like three dash bedroom uh yeah so i was
20:24 - thinking oh like there's information in
20:25 - those urls that even though we don't
20:27 - want to use the urls themselves as
20:28 - categorical things we could like maybe
20:30 - parse those urls to get um
20:32 - some information out of them yeah or
20:33 - maybe like ocean
20:35 - view or something like that yeah they'll
20:37 - want
20:38 - oh yeah interesting comment in the
20:39 - youtube chat from alex of uh
20:41 - you could just get the length of the
20:42 - description as a comp uh as a column
20:45 - um yeah what if i do len
20:48 - description is that going to give me the
20:50 - number of characters yeah i think so
20:52 - assuming it's just a string
20:53 - let's uh let's actually we can try that
20:56 - um let's before we do that though let's
20:59 - go back
21:00 - and let's actually like pull out some of
21:02 - these things so
21:04 - um i i feel like we can safely say the
21:06 - id column is probably not giving us any
21:08 - information
21:10 - towards uh price i'm gonna get rid of
21:13 - url as well because i figure
21:15 - anything that's in the description
21:19 - is probably in the url also i'll get rid
21:22 - of region url like you said because that
21:24 - information is also contained in this
21:26 - region variable
21:28 - um image url uh number 17 there probably
21:32 - isn't super helpful
21:33 - okay yeah um and then
21:38 - can leave we can leave lat and lawn i
21:40 - assume those are
21:41 - latitude and longitude which might be
21:43 - interesting
21:44 - so let's go ahead and start with that
21:48 - um so actually
21:52 - let's think should i use i think there's
21:55 - like a dot
21:56 - drop that allows us to easily get rid of
22:00 - columns
22:01 - or we can use subsetting but let's uh
22:05 - let's try our let's see if i can find
22:07 - this
22:08 - drop columns
22:13 - um i'm gonna just show you guys my
22:15 - googling
22:16 - as well because i feel like this is
22:20 - a useful skill indeed
22:24 - um okay so this is
22:27 - the the labels parameter gives the index
22:30 - or column labels to drop
22:32 - and it looks like i can just input like
22:35 - a list of
22:36 - labels that i want to drop so
22:39 - i'm going to go ahead and grab those so
22:42 - we've got
22:42 - id url region url
22:55 - i wonder if it doesn't in place or if
22:56 - we'll have to do housing
23:00 - you are probably right let's check and
23:03 - then was there anything else
23:04 - image url maybe
23:08 - okay was it
23:12 - img uh full image
23:16 - girl okay let's see this might just give
23:20 - us
23:25 - uh maybe it takes index of uh 0
23:28 - 1 2 instead of the column names
23:34 - i'm gonna try labels
23:37 - equals this
23:42 - and then we might need to give it an
23:48 - axis
23:50 - okay from the
23:53 - index or the column so let's give it
23:56 - axis equals one because we want to drop
23:59 - the column
24:08 - okay and then this just gave us
24:11 - this exact same data set but with those
24:14 - columns dropped
24:16 - and so now if we want to resave it we
24:19 - either need to add
24:20 - in place equals true or housing equals
24:25 - so i'll do it this way
24:29 - and then let's just see so if i take
24:33 - len of housing dot description
24:38 - is that gonna just give me a bunch of
24:39 - numbers that give me
24:42 - so uh
24:46 - interesting i bet that's like the sum
24:48 - total of
24:49 - the description i'm guessing you want to
24:51 - call them right
24:52 - right um you might need to do like an
24:56 - apply
24:57 - of like um
25:01 - or housing of description of the column
25:07 - uh let's see
25:11 - right so let's just get the column right
25:12 - housing.description of
25:14 - um i don't know that that is what uh
25:17 - that is
25:17 - that is what yeah so um
25:21 - let's try apply function
25:25 - to a column in pandas um
25:29 - i know that's the dot apply but i just
25:31 - want to pull up the
25:33 - pull this up so we can see it so um
25:36 - this allows us to apply the same
25:39 - function
25:40 - to every value in a column
25:44 - um so axis along which the function is
25:47 - apply
25:49 - zero would apply it to each column or
25:52 - sorry one apply each function apply the
25:56 - function to each row
25:57 - so if we want to apply to each row we
25:59 - would set that equal to one
26:01 - so let's try this as well
26:04 - um let's do like housing
26:10 - dot and then i see on youtube someone's
26:14 - uh pointing us to dot shape
26:16 - as well which uh which might be a uh
26:20 - a good way to do that i'll look into
26:21 - that as you uh as you work on the fly
26:24 - okay um well
26:28 - let's see this is gonna apply it to
26:31 - every column um so i wonder if i can
26:35 - just
26:36 - i wonder if i can just pull out like
26:38 - housing dot
26:40 - um description
26:43 - dot apply and then do
26:46 - like and give it wayne
26:50 - that's the function one
26:56 - do i do i would just give it len and
26:58 - then
26:59 - like then we have to figure out the axis
27:02 - stuff but
27:11 - one takes no keyword arguments um
27:16 - interesting let's see if i do like
27:19 - len of hello
27:22 - i am sophie
27:27 - it does give me the number of characters
27:36 - let's see
27:43 - so if we if we get just the column uh
27:47 - so if you do um
27:50 - if you do data frame or so if you do
27:52 - housing dot description
27:54 - dot s dot len
27:57 - dot s dot one dot string sorry dot s t
28:01 - dot str.len
28:05 - len and get rid of yeah
28:09 - and then parentheses after plan as well
28:12 - what does the str do oh wow i believe it
28:15 - was saying
28:16 - treat it as a string we can try to get
28:18 - rid of it you can try to get rid of
28:20 - str um and see what happens i think we
28:23 - had oh we had applied
28:28 - interesting okay so if we apply len to
28:32 - the entire
28:34 - into the entire series then
28:38 - there is no there's no way to calculate
28:41 - the length of
28:42 - a series object so we need to turn the
28:44 - series into a string or something
28:46 - a set of strings yeah
28:50 - super interesting was that was that your
28:53 - own googling or is that
28:54 - uh yeah so i google panda get length of
28:57 - string in column and it was the first
28:59 - result
29:00 - nice okay well i wish you guys could
29:03 - also see alex's screen
29:05 - but this is fun okay so this gives us
29:07 - the
29:08 - the length of the description let's um
29:11 - let's add this
29:12 - into our into our
29:16 - initial data set so we'll say like
29:18 - housing
29:19 - dot um description
29:22 - length equals it's gonna give me a
29:28 - warning but it's gonna work
29:32 - um let's go ahead and take a look at
29:36 - housing.head again
29:40 - yeah probably good to like verify that
29:41 - this is doing what we expect
29:43 - where um description and description
29:46 - link is probably all the way at the very
29:48 - end
29:48 - you didn't see it actually oh did you
29:51 - reset housing equals
29:59 - let's try this again
30:01 - [Music]
30:02 - but i think
30:12 - oh there it goes there it is um okay
30:16 - so now we've got a description length
30:17 - the number of characters
30:19 - um and then i'm gonna now drop
30:23 - description i don't think we're gonna
30:25 - use anything else but
30:26 - again like alex said we could we could
30:28 - do something else here we could
30:30 - figure out um we could pull out some
30:32 - word that we think is gonna
30:34 - be um associated with
30:37 - particularly um high like
30:40 - high prices like exceptional or
30:44 - special maybe like places that have us
30:48 - some sort of one free month special or
30:50 - more likely
30:51 - yeah we could even look for like free
30:53 - month in there because i think that
30:54 - that's
30:54 - that's not a data that's represented uh
30:57 - in any of
30:58 - our other columns like oh this comes
31:00 - with a free month well that like changes
31:01 - the
31:02 - you know real price of the apartment
31:04 - exactly so we could definitely
31:06 - pull some things out of here and and
31:08 - create some more columns around
31:09 - description
31:10 - but i'll leave that for now um and then
31:14 - i'll just grab this
31:17 - drop again so you can drop description
31:29 - okay and then let's take a look at it
31:32 - again
31:36 - okay i feel like we're getting closer
31:40 - um do you have any other questions
31:42 - before we try to fit a model
31:43 - like anything else you think should one
31:45 - thing i would want to do is
31:47 - because we kind of like made this
31:48 - description length thing on the fly
31:51 - i would kind of want to verify that that
31:52 - is indeed an integer rather than a
31:54 - string because like who knows
31:56 - who knows what data type uh you know
31:59 - these random functions that we were just
32:00 - googling it was throwing out so i
32:02 - probably want to look at the info again
32:03 - just to make sure that we
32:05 - know for that uh description length cool
32:08 - it's a it's a float
32:10 - that looks good um the other thing
32:14 - i mean one thing that i might look at
32:17 - before we get started is um first of all
32:20 - for some of these categorical variables
32:24 - like parking options
32:26 - and region
32:29 - i'm i'm curious how many different
32:32 - values there are
32:34 - because if there's like a thousand
32:37 - different values we're still gonna
32:39 - end up with like a lot of parameters in
32:42 - this model
32:43 - right um so one thing i might do is just
32:46 - like we can even do
32:49 - um
32:54 - is it describe
33:04 - give us a little bit more or allow us to
33:08 - um like this so in the region
33:11 - there's 404 different categories
33:15 - right so we probably don't want to
33:19 - throw that into the model willy-nilly
33:21 - because we're going to end up with 403
33:25 - slopes um on region if we put that in it
33:28 - also looks like
33:31 - oh well actually that's an and for a
33:33 - reason
33:34 - there's 12 different types of apartments
33:37 - that's a little more reasonable
33:39 - um how would you go about
33:43 - um
33:46 - states yeah um yeah that's interesting
33:50 - we could look into that maybe you know
33:52 - puerto rico or something is in there
33:54 - um how would you go about kind of
33:57 - verifying that
33:58 - all of these that there's like no errors
34:01 - in the data
34:02 - of like oh maybe laundry options maybe
34:04 - one of the five is like
34:06 - capitalized differently than the others
34:08 - or like
34:10 - you i mean i would go through if
34:13 - if we were spending more time on this
34:15 - and i want to make sure we got some time
34:17 - for modeling but
34:19 - if i was spending more time on this i
34:20 - would definitely go in
34:22 - much more detail looking at each of
34:24 - these like so for example
34:27 - um let's look at type
34:30 - for a moment because this has kind of a
34:32 - lot of categories
34:34 - to just throw it into the model but not
34:36 - so many that we
34:38 - just want to like definitely throw it
34:41 - out
34:42 - um but so one thing i might do is first
34:45 - of all
34:46 - take a look at housing
34:50 - dot type dot value counts
34:55 - and so this gives me all of the
35:00 - values in this column so we've got
35:02 - apartment house townhouse condo
35:04 - duplex manufactured
35:08 - goes on so it looks like these are all
35:12 - at least their own thing right
35:15 - um we definitely have a couple
35:16 - categories like assisted living
35:18 - and land that don't seem very highly
35:22 - represented in the data
35:23 - and in fact like even even in law
35:26 - and flat and loft and cottage cabin
35:31 - are like oh there are a lot fewer
35:34 - of those um than all of the other ones
35:38 - so like one thing i might do is this is
35:41 - a lot of extra parameters for like not
35:43 - that much variation in the data
35:46 - i might collapse this into one category
35:48 - that's like
35:49 - other interesting um to just
35:52 - kind of cut this down a little bit the
35:55 - other thing i might do
35:57 - is take a look at a box plot of all
36:00 - these things
36:01 - and price so i might do like sns.box
36:04 - plot
36:07 - x equals type
36:11 - y equals price
36:14 - data equals housing
36:17 - [Music]
36:20 - and then
36:23 - show the plot
36:29 - okay so it's like there's one massive
36:33 - outlier this is actually okay
36:36 - let's let's investigate this further so
36:38 - like
36:39 - if i it might not even be one outlier it
36:42 - might be like
36:43 - a bunch and you're going to change
36:46 - somewhere so one thing i might also look
36:49 - at here is
36:52 - um
36:54 - guess i'll use
36:57 - i always forget if we're at his plot or
36:59 - his plot
37:01 - um seabourn keeps changing
37:05 - it's uh or disc plot like
37:09 - um and then do uh
37:13 - housing.price
37:21 - [Music]
37:29 - okay so we've got this like very very
37:31 - skewed um
37:32 - distribution here where we go from zero
37:36 - all the way up to like a million or more
37:39 - than
37:40 - it's the one with nine
37:43 - yeah so yeah 2.5 with nine zeros
37:48 - um so i think
37:51 - we wanna probably restrict this a little
37:54 - bit
37:55 - let's for the moment
37:58 - i'm gonna just kind of arbitrarily cut
38:00 - this down
38:01 - um what do you think is a good like
38:04 - cut-off point
38:05 - so we're gonna be removing that rose
38:07 - where price is over
38:09 - a million yeah or like
38:15 - so how would you go about deciding this
38:17 - in like the real world of
38:18 - like when can you just throw out data
38:21 - like this
38:22 - so um i guess
38:25 - there's again no clear answer um
38:29 - i'm sorry to say but i think
38:32 - in practice you kind of play around with
38:34 - this for a little while so
38:37 - let's like for example let's just start
38:40 - without making any
38:42 - permanent changes let's actually
38:45 - we can do this in here let's subset this
38:50 - right like within the plot and say we
38:53 - only want
38:56 - um values where price is less than
39:01 - 1 million um and then
39:04 - let's like replot this
39:07 - and it still just looks like super
39:09 - skewed like we still really can't
39:12 - see this variation um
39:15 - one thing we could do at this point is
39:16 - we could try we could even try
39:18 - like taking a log and see if like
39:21 - the log transform helps us get some
39:24 - things like
39:26 - oh and then here we're we're gonna have
39:30 - an issue with taking the log of anything
39:33 - um that's zero so that gives us a clue
39:35 - that there's actually some apartments
39:36 - where the price is
39:37 - zero so um now i'm gonna say like
39:42 - and housing.price
39:47 - greater than zero and i think i need to
39:49 - put these in
39:54 - parentheses
39:58 - and now okay so now we've got like
40:02 - a little bit more usable of a
40:05 - distribution
40:07 - um so this might be a clue that we want
40:10 - a log transform
40:11 - for our price variable
40:14 - um because i threw this log in here but
40:16 - we could also
40:18 - could we go in the other direction try
40:19 - like super
40:21 - like over cutting things and just say
40:22 - like hey what does it look like if we
40:24 - cut everything that's less than um you
40:27 - know
40:27 - two thousand or everything that's
40:29 - greater than two thousand um
40:31 - dollars and see yeah let's do like
40:34 - three thousand i feel like because these
40:35 - might be rental prices
40:38 - and this is yeah so that looks pretty
40:39 - good this is another thing that we could
40:41 - do instead of instead of just taking
40:43 - log we could just cut this down and say
40:46 - okay like
40:47 - realistically we really only care about
40:50 - this
40:51 - um when we're
40:54 - looking at apartments that are three
40:56 - thousand
40:57 - price range within our lowly price range
41:01 - um yeah and then i see somebody wrote in
41:05 - the chat can we see
41:06 - performance before and after including
41:08 - the outlier
41:09 - and definitely so um this comes back to
41:11 - a question that somebody asked on
41:13 - discord
41:14 - a couple weeks ago and i thought it was
41:16 - a great question which is
41:18 - like how do you compare models and i
41:21 - think
41:23 - totally if you have a data set where
41:25 - it's not
41:26 - super costly to run or time consuming to
41:28 - run the model
41:29 - you can definitely like run the same
41:32 - model
41:33 - with a bunch of different iterations
41:35 - getting rid of um
41:37 - getting rid of an outline liar or
41:40 - getting
41:41 - um or like taking a log transform not
41:43 - taking a log transform
41:45 - you can try all those things and refit
41:47 - the model it gets a little more
41:48 - difficult if you have like a huge data
41:50 - set
41:51 - then you might need to like take a
41:53 - subset of data to do
41:54 - that exploration um but yeah
41:57 - no no need to ignore your comment
42:00 - because it looked like it definitely
42:02 - looked like an one outlier in that plot
42:04 - that we saw
42:05 - um i think it was just like a
42:09 - weird artifact of the skew um
42:12 - but it definitely looked like an outlier
42:15 - um in that first plot
42:17 - so i would say based on looking at this
42:20 - um i feel like 3000 seems like a pretty
42:25 - good cutoff point i mean we could like
42:27 - look at this tail to ten thousand or
42:31 - five
42:31 - to like ten thousand and see if
42:34 - it looks like yeah
42:37 - i mean it kind of looks like to me this
42:40 - the cutoff is maybe like
42:42 - three or four thousand so
42:45 - my very um
42:49 - precise uh
42:51 - [Music]
42:53 - method for this i'm gonna cut it to 4
42:55 - 000
42:56 - or smaller okay
42:59 - so let's let's actually do that we'll do
43:02 - housing
43:03 - dot housing equals housing
43:07 - housing price uh actually
43:11 - totally override it uh
43:14 - i like housing
43:19 - affordable um
43:24 - okay and that kind of that kind of
43:27 - distinction is like if we
43:29 - saved over housing then it would be a
43:30 - little bit trickier to say oh let's like
43:32 - plug in
43:33 - the let's plug in the model before we
43:36 - we cut out the super expensive um houses
43:39 - it'd be a little bit trickier to do that
43:40 - or we'd have to like reload the data set
43:42 - and uh yeah for sure okay
43:47 - let's do that um okay so now let's just
43:51 - take a look at this
43:52 - again
43:58 - all right so we've got region price
44:01 - and then i'm reminding myself as well we
44:05 - looked at this before we saw
44:07 - region is a little bit overwhelming of a
44:11 - variable right here
44:12 - um maybe like again i
44:15 - assume that the region is giving us like
44:18 - specific cities basically um
44:22 - maybe there's a way that we can
44:25 - do some like combining of categories
44:27 - again where we just say like
44:30 - north west yeah southwest right maybe we
44:33 - could
44:35 - oh market in the south and new york is
44:38 - in the northeast
44:38 - yeah um so there's there's lots more
44:42 - i guess it's gonna feel a little
44:44 - unsatisfying because there's a lot more
44:46 - cleaning of this data set and
44:48 - producing like not just cleaning but
44:50 - creating
44:51 - of new features so like feature
44:54 - engineering
44:55 - um that we could do before fitting a
44:57 - model that would probably improve this
44:59 - model
45:00 - because to be fair like the relationship
45:02 - between
45:03 - price and square feet is probably
45:07 - different by region right like
45:10 - the prices in jacksonville are going to
45:12 - be different than the prices in new york
45:14 - city and
45:15 - the relationships between like price and
45:18 - other variables might also be different
45:20 - in different regions so i feel like
45:22 - region is probably important in some
45:25 - sense but like
45:27 - yes i mean so ultimately we could do the
45:29 - same thing of like depending on
45:30 - you know depending on the real-world
45:32 - application of this if we're renting an
45:34 - apartment in
45:35 - denver we don't really care what rent is
45:38 - like in new york and so we could take
45:40 - this data set and just like we cut out
45:41 - all the super expensive
45:42 - apartments we could cut out all the
45:44 - apartments that are not in denver
45:46 - um that's also true yes um but what if
45:49 - we have
45:49 - apartments that we want to rent out in
45:51 - like every major city in the u.s
45:53 - yeah that's a good thing what do we do i
45:55 - don't know
45:58 - um okay so let's let's see
46:01 - what should we include in our first
46:03 - model should we include
46:06 - i mean i think the most obvious things
46:08 - are like square footage bedrooms
46:10 - yeah okay let's um let's
46:14 - let's get a a model going let me
46:17 - grab and grab the same
46:26 - code from up above
46:31 - and i'm gonna go ahead and
46:35 - start adding we're gonna do this with
46:38 - housing
46:40 - affordable and let's start grabbing
46:43 - something so
46:43 - we want should we include we'll
46:47 - include type for now um
46:50 - square feet
47:04 - and remind me of like those are
47:06 - certainly
47:07 - should be treated like a category right
47:11 - yes yes no but it looks like right now
47:14 - they're treated
47:14 - as like zero one um so
47:18 - remember that here let's actually print
47:21 - out
47:22 - the info again so
47:26 - all of these are being stored it looks
47:28 - like as integers so it's just zero
47:31 - one um and
47:35 - remember that when we when we fit
47:38 - this model with say like uh type
47:42 - what's actually happening under the hood
47:44 - is we're getting
47:46 - a new um like a new
47:50 - design matrix here that has
47:54 - 11 new columns that are ones and zeros
47:58 - um i can actually like
48:01 - print that out let's see i think it's
48:04 - like
48:09 - patsy dot
48:16 - you know what i'm gonna do i'm gonna go
48:18 - back to
48:21 - i think it was
48:25 - here
48:30 - i may have done this
48:37 - so let me try one more place
48:57 - yeah okay so
49:01 - this is a different
49:10 - different data set but let's just for
49:14 - a moment do you like rent or it was
49:17 - price
49:19 - as a function of type and this is
49:23 - um housing affordable
49:27 - and return it as a data frame and so
49:35 - yeah so this is the new
49:38 - x matrix that we get and so it'll have
49:42 - like this one for an intercept
49:44 - column of one for the intercept and then
49:46 - it'll have like type
49:47 - t assisted living and this will be ones
49:49 - and zeros type t
49:51 - condo and this will be ones and zeros
49:53 - and so it
49:54 - basically just creates a bunch of dummy
49:55 - variables with ones and zeros
49:57 - for all of the um
50:01 - for all of the possible values of that
50:03 - categorical variable
50:05 - so back up here
50:08 - this column cataloud is basically
50:12 - the same is being treated in the same
50:14 - way as this type
50:16 - column it's just we're not seeing it
50:18 - when we use this os from formula we're
50:20 - we're just putting in the string but
50:22 - in order to fit the model it's
50:24 - separating that column
50:25 - into like 11 new columns
50:29 - got it but if we
50:32 - if we had three so that that's working
50:36 - because it's binary if it's either cat's
50:37 - lab or not
50:38 - but if this were if there was a third
50:41 - category for that we wouldn't want it to
50:43 - be numerical zero one two
50:45 - we would want it to be you know string
50:48 - zero one two which would then be treated
50:50 - as a category
50:51 - or or the you know the string the actual
50:54 - names of those categories whatever they
50:56 - might be yeah exactly
50:58 - um that's a good point so again if we
51:01 - were
51:02 - being more thorough here we'd probably
51:04 - want to verify
51:05 - that the only values are zeros and ones
51:08 - um
51:09 - i'm pretty sure i've taken a look at
51:11 - this before and and verified that
51:14 - um but definitely would want to it also
51:17 - seems like these
51:18 - all of the columns where it is like a
51:20 - binary thing like cast can be allowed or
51:22 - not
51:23 - it seems like those are ones and zeros
51:25 - and anything
51:26 - where the type like type where there's
51:30 - more than
51:30 - two options it seems to be recorded as
51:33 - strings but
51:34 - that's an assumption that i'm making
51:36 - that's not like something that we have
51:38 - checked
51:39 - right now um okay
51:43 - let's go back
51:46 - so we were here
51:50 - so we had
51:54 - up to smoking aloud in the model
51:58 - let's do wheelchair
52:03 - access i'm just adding everything i
52:05 - don't know
52:09 - there might be a shortcut to add
52:11 - everything
52:12 - and then take some things out
52:17 - you know an r there is but
52:20 - um and then let's do
52:22 - [Music]
52:23 - what do you think we'll add description
52:26 - we'll add latin law
52:28 - let's add description length to start
52:30 - and then we'll we'll come back and
52:32 - add some more things um
52:35 - and then the data is housing underscore
52:37 - affordable and we're going to fit this
52:39 - model
52:40 - and then let's print out the summary
52:42 - it's going to be
52:44 - big but
52:48 - it might take a minute okay
52:53 - so and i'm actually gonna do
52:55 - [Music]
52:58 - zoom out so that we can see all of this
53:00 - this is one thing i don't like about the
53:02 - summary output i
53:03 - would love if they would output it for
53:06 - me as a data frame
53:08 - um directly but instead we get this like
53:12 - crazy thing that is subject to whatever
53:15 - weird formatting
53:17 - um jupiter notebooks decides to use
53:20 - um okay so we can actually see all a lot
53:23 - of the things we've already talked about
53:25 - we can see
53:26 - that the r squared is 0.129
53:30 - and adjusted r squared is also 0.129 so
53:34 - i would interpret this as like this
53:36 - model is
53:37 - not super good at identify
53:41 - or at predicting price because we're
53:43 - really only
53:44 - um really only explaining about like 13
53:48 - of the variation in price for all of
53:51 - these apartments
53:52 - um but we could use these numbers to
53:55 - compare
53:56 - this to a new model so let's see
54:00 - what happens and you'll notice like we
54:02 - do have
54:03 - at least the type variable like we said
54:06 - we've got like 11
54:07 - different um values here right
54:13 - let's add in let's add in lat and lawn
54:16 - and just see
54:18 - if that improves it at all because right
54:21 - now we have nothing
54:23 - cutting down the region at all so maybe
54:26 - maybe that would be useful oh yeah live
54:29 - long is actually a great way to
54:30 - get the region uh
54:35 - but that's not going to be like linear
54:36 - right yeah it's hard to
54:39 - because it's not it's not like oh higher
54:41 - latitude is going to be more expensive
54:43 - yeah maybe we need like
54:47 - well okay let's add it in
54:54 - okay well it didn't prove it i feel like
54:58 - i mean for such a small number like this
55:01 - is only
55:03 - 0.129 so we're already up to like 15
55:06 - percent
55:07 - so that's not terrible um i bet that
55:10 - that
55:11 - if we ran like a um anova comparing
55:14 - these two models i bet the
55:16 - bigger model would like that added
55:19 - complexity would
55:20 - probably be worth it um
55:23 - other ideas or questions uh question
55:25 - from the youtube chat how do we
55:26 - calculate feature importance
55:28 - oh um that is a very loaded question
55:33 - um so one thing that we can do
55:36 - is if there's something called like uh
55:42 - what are they called standardized
55:44 - coefficients um
55:46 - another another thing that we can do
55:48 - that's slightly different but related is
55:50 - we
55:50 - if we standardize all of the
55:54 - um all the quantitative variables
55:58 - then they're all on the same scale then
56:00 - they're
56:01 - then the um coefficients at least are
56:05 - all on the same scale
56:06 - so they are comparable so we could
56:08 - actually if
56:09 - everything was standardized we could
56:11 - compare coefficients at least for the
56:14 - quantitative variables
56:16 - um another thing that
56:20 - people do and i guess this is really a
56:23 - plug for
56:24 - the feature engineering um content
56:26 - that's coming out
56:27 - hopefully in a couple of months um that
56:30 - nitia is working on
56:31 - because there there are a lot of other
56:34 - mechanisms that we can use to try to
56:36 - like
56:36 - pick out features based off of
56:40 - their relative importance in terms of
56:44 - this model um or there's also ways that
56:46 - we can
56:48 - iteratively try to build this model for
56:50 - example like
56:51 - we can use like forward and or backward
56:54 - selection
56:55 - to kind of tell the computer okay like
56:58 - start with
56:58 - a model that has everything in it and
57:02 - start deleting things and testing them
57:05 - out and then
57:06 - if something helps to delete it
57:09 - then keep it missing and then like
57:12 - continue the process from there or
57:13 - the reverse we could start with nothing
57:15 - and try like adding
57:17 - predictors one at a time and then
57:20 - see which one and do it which one
57:22 - improves the model the most and then
57:24 - only keep that one and then
57:25 - iteratively do that again um we can also
57:28 - use like
57:31 - uh like ridge or lasso regression
57:35 - to try to shrink some of the
57:37 - coefficients to zero if
57:39 - they're or close to zero if they're not
57:42 - super
57:42 - relevant um
57:44 - [Music]
57:46 - i see a question i wonder what why stats
57:48 - models doesn't include in summary
57:50 - metrics such as mean
57:51 - absolute error means squared error root
57:55 - mean squared error
57:58 - is r squared better that's a interesting
58:00 - question
58:02 - um so
58:09 - r squared is related to all of those
58:11 - things um
58:13 - because it is also based off of the
58:16 - um it included in that r squared
58:19 - calculation you're calculating like
58:22 - the error as part of it and seeing
58:26 - basically saying like how much of that
58:28 - error
58:29 - are you or how much of the variation
58:33 - are you accounting for by adding that
58:35 - line in
58:36 - um but
58:39 - i don't know i don't know why they chose
58:41 - not to include
58:43 - additional metrics um
58:46 - it's always interesting um
58:50 - okay we are like running quickly out of
58:53 - time and i know we haven't gotten super
58:54 - far
58:55 - but at least we've had a chance to test
58:57 - out some models and hopefully we've
58:59 - gotten
59:00 - you've all gotten a taste of what this
59:02 - might look like on your own
59:04 - um i think the fun of today
59:07 - is kind of we didn't have a clear
59:10 - direction of where we were gonna go with
59:11 - this today um but
59:13 - it was i think i hope useful to kind of
59:16 - see
59:17 - how one might get started on an analysis
59:19 - like this
59:20 - and my hope is that if you're interested
59:23 - in learning more about linear regression
59:25 - and if you're interested in practicing
59:27 - these skills a little bit more
59:28 - that you might take this and run with it
59:31 - and like try running your own model see
59:33 - if you can
59:34 - improve this even more another thing i
59:37 - would probably do
59:38 - is start subsetting the data to
59:41 - one region and see if that allows me to
59:44 - get
59:45 - at least a better a little bit better
59:49 - um yeah but i think this kind of work in
59:51 - general just will make you more
59:52 - comfortable
59:53 - using all these tools make you more
59:54 - comfortable with like kind of solving
59:56 - arbitrary tasks like we were doing like
59:58 - how do i find the length of
60:00 - the string in this column just like the
60:02 - more and more work that you
60:03 - do working with data frames working with
60:07 - um these libraries that are making these
60:08 - models i think that
60:10 - that will just help grow your skill in
60:12 - like being able to
60:13 - or being more confident in doing these
60:15 - with when
60:16 - you need to do it in a real world
60:18 - situation yeah
60:19 - and another thing that i will say is
60:22 - once you have learned
60:23 - more methods for creating models then
60:26 - you can even like you could extend this
60:29 - even further this doesn't have to be
60:30 - just a linear regression problem this
60:32 - can be like a
60:33 - how well can i predict price problem and
60:36 - you can try even more methods
60:38 - and use some of the methods that we
60:40 - discussed here to still compare
60:43 - compare models and also some of these
60:45 - metrics that
60:47 - alex in the chat just mentioned so like
60:50 - mean absolute error is um
60:53 - is a useful one to compare different
60:56 - types of models because
60:58 - you know like you don't want to
61:01 - you don't necessarily have a way of
61:03 - calculating like aic
61:05 - for um for like every model that you
61:09 - might
61:10 - every type of model that you might
61:11 - create but mean absolute error you
61:13 - definitely could
61:16 - cool well that's it that's it for linear
61:20 - regression
61:21 - um i hope that you guys found this
61:23 - helpful and definitely let us know
61:25 - if you have any more questions we'll try
61:27 - our best to answer them
61:30 - and feel free to write those questions
61:33 - as comments
61:34 - in the youtube video or also on discord
61:37 - if you have been on discord
61:39 - yeah uh we don't have any other live
61:42 - streams currently planned but hopefully
61:43 - we will do another
61:44 - kind of series like this um in the near
61:46 - future but
61:47 - yeah might uh might take a little a
61:49 - couple weeks break
61:50 - um so we'll see what happens but uh yeah
61:54 - keep an eye on the youtube channel for
61:55 - more stuff like this and
61:57 - yeah thanks for thanks for doing this
61:58 - sophie so he led the you know eight
62:00 - straight weeks of this
62:02 - um great work sophie it's all fun thanks
62:04 - for joining me on all of these alex
62:06 - this is a wild ride all right cool all
62:09 - right

Cleaned transcript:

stream series on linear regression um we're gonna be kind of reviewing a lot of the stuff that we covered in the last seven weeks in one day and we're just gonna kind of play with a data set and really go through a full process from start to finish of how do we get some data how do we clean it up a little bit how do we fit some models how do we compare them and maybe we'll come up with we'll see if we can come up with the best possible model that we can for predicting rental prices uh by the end of this session yeah i think this session in particular is kind of more openended than our last ones basically our plan is to play around with this data set and so definitely if you're watching this in the chat as we are doing this live um and you want us to like take a look at any of these features or want us to experiment with this in a particular way we are super open to uh kind of uh making this as interactive as possible so um yeah exactly yeah we want this to be kind of a fun way to kind of wrap everything up but also make it explorational explorative i don't know um okay cool so um one thing i do want to point out before we get started is that um if you're following along with the linear regression course on our on our site and i'll i'll go to codecademy really quickly right now and pull that up so um if you go to this linear regression in python course and take a look at the syllabus um the data set that we're using in a cleaned form exists in this um last project in the in the course this craigslist analysis so if you'd like to play around with this and you want to just get the data already loaded for you and see an example solution this is a little bit subseted and it's a little bit cleaned up but you can do that here if you have a codecademy subscription and even after this this stream if you want to play with it yourself but you don't want to set up your local environment this is a great place to come and um and play with it cool so i'm just going to show you i'm not going to go through the full process but i'm just going to show you where i downloaded the data from and the reason i'm showing you this is really just because if you're working on your own computer you're doing your own analysis you're just getting started um you might come to a place like kaggle or the uci machine learning repository that we've shown before and you might download your own data set and so i think it's it's useful to kind of see the process from the beginning um so this is the data set we're going to be working with i found it on kaggle just by clicking going to kaggle.com clicking data sets and then searching around within data sets um i like to use data sets that have a high usability rating so 10.0 is like the highest i think that you can get um so if you're just starting out it just means it's really easy to download this data and get started with it it's not super super messy although this is a pretty big data set and has um some columns that you might not care about but it's super easy to use it's got um it's got like descriptions of everything um it's got like an overview it's got easy download all of that so we can take a a brief look at it even on kaggle we see there's 22 columns 10 of them are printed out here so you can get a sense for what kind of information might be contained in here and then we can also get a little bit of of context and information about how this data set came to exist on kaggle um so it gives us this the sentence craigslist is the world's largest collection of privately sold housing options yet it's very difficult to collect all of them in the same place so this person built this data set as a means by which to perform experimental analysis on united states um as a whole and instead of isolated urban housing markets the data is scraped so web scraped every few months so somebody else has done all the web scraping for us which is nice um it contains almost all relevant information that craigslist provides on retail sales so um this is just data that has been web scraped from craigslist about housing listings yeah the license is kind of interesting in if you're like making a project for your portfolio or something that you're making for your own personal projects you might want to look at the license and just kind of like dive into um what you're allowed to do with the data set um obviously if it's on table like this um you'll probably be able to like download it and play around with it yourself but again you might want to look at the license when you think about hey how should we be like publishing something related to this this data set yeah exactly um okay so i think we can get started so all i did was i pressed this download button um you have to create a kaggle account oh you can't see my mouse but you can see that it's highlighted in the top right of this right hand corner of this page there's this download button and you have to create a kaggle account once you press download it will download onto your computer and then all i've done is i've moved this housing.csv file that i that was downloaded when i press that button into the folder where my jupyter notebook is or it could be the folder where you've got um like your script that you're writing as well and then i've just opened up this code dot ipynb um this jupiter notebook and this allows me to start playing with the data yeah and again if you aren't super familiar with how to work with jupyter notebooks we've got a lot of articles that kind of help you set that up it's slightly complicated to just like open the jupyter notebook to begin with i'm assuming you did it through terminal or did you do it through um like the jupiter app i did it through terminal um it's actually like once you have it downloaded um it's relatively simple to do via the terminal um i think you just you just type jupiter notebook into the terminal um but yeah it definitely takes a little setup on your computer first before you get that running um and i think we we provided some information on how to download everything on the first stream i believe cool um cool so um i'm just gonna load a bunch of libraries i don't know which ones we'll end up using and then i'm gonna go ahead and load this data um you'll notice normally i just do like this right read in the csv um but let's just print what that looks like um oh actually seems like it's fine i pulled this from somewhere else um you might notice though things that happen when you when you first load a data set um you might see like the header for example is getting pulled in as a row instead of um instead of like column names and so that's what i was intending to do in that first line of code but it looks like it's being read in okay so that's fine um and i've used the pandas library to read it in and so i think a good goal for today is to try to come up with a linear model that allows us to predict price and in the process we can start thinking about what the relationship between price and some of these other variables is so type of apartment square feet beds baths um but also all this other stuff got like parking options laundry options comes furnished um so we got lots of lots of information and then we're going to see if we can figure out the relationship between all of those things in price and see if we can predict price accurately and maybe we would be doing this if we are renting an apartment ourselves or maybe if we are the owners of an apartment and we're trying to figure out like what is the the best or the appropriate market value for this apartment okay so um let's take a look at a couple of things if we're trying to build a linear model for price um are there any columns that you think we should get rid of alex so there's all this kind of like metadata information like the url the id even i don't know if we necessarily need that um i mean it looks like all of these are from reno tahoe so like region url it seems like it's already represented in the region column um so i would probably get rid of region url um that's all i would get rid of from what i'm seeing here there might be more things to the right if we keep looking but uh so let's actually we'll prac we'll print out the info so that we can see first of all how many rows and columns there are and see all the names um looks like so we've got around 385 000 rows so a fair number of rows and then 22 columns we saw that before um i agree it that it makes sense to get rid of some at least id and url um it looks like id for example is being read in as an integer so we're thinking of this as like a numerical value but like an id that is a higher number like this is a higher number than this i believe if i'm looking this correctly but no i have to go back further this this bottom one actually is a larger number than the one above it but um that doesn't really mean anything meaningful about this apartment with respect to this one doesn't mean like it has more of something it just means that it has a different id yeah this might even be a good example of something that are like not correlated at all and so would not make for a good uh good linear regression where if we mapped like price to id the id is just kind of like a randomly generated number presumably or um or at least that's what i'm assuming and so you wouldn't really see any relationship there yeah and then for url um i mean i don't want to do it because i don't want to like create a problem well we can do it on a smaller scale but um what would happen alex if we if we just like put url in it looks like it's being saved as a string right um if we added url to our linear model what do you think would happen i mean so i think that that would try to break it that would consider that like categorical variable right of like hey there are 700 categories and none of them share you know uh category one is this url category two is this url so it's just seven under categories and they're all different because none of them share the same url presumably yeah so let's actually do this really fast because i think this is kind of a funny exercise let's um let's start by just making a um a smaller subset of this housing data where we'll just take um i always forget the syntax here is it like if you just want the first couple of rows and then you do like i think it's like zero to zero colon five won't give you the first five i think yeah i think let's see cool um okay so let's do like let's do 50 rows and then let's fit a model i'm gonna actually like grab my i'm gonna grab some code from somewhere random um again because i never memorized these uh these things so i think uh breaking down uh breaking down this line of code real quick so we are using the sm module which if we look at our import statement is statsmodel.api and then within that we are using if you scroll back down to your line of code within that we are using the uh ols what does ols stand for do you know ordinary least squares okay so this is remember at the beginning we kind of talked about um ways of fitting linear regression model and the way that we've been using is called ordinary least squares because we're minimizing the squared distance between each point and the line essentially okay and then we're saying we're building it from a formula and that formula is what uh based on predicting price based on url yeah and i'm gonna use housing underscore sub and then i'm just gonna print model one dot params and so like how many parameters do you expect to see i expect to see what 50 parameters because it's uh yes again this is the example where they're using a url as a category and there's presumably 50 different urls and so yeah yeah i think there might be fifth well there's going to be 50 unique urls and then um minus one reference group right so 49 plus a intercept so yeah you're exactly right there's gonna be 50 of these and we end up with so here's the intercept and then you see we have a slope for url true dot this whole thing url and then we have a different one for url true dot this whole thing um and then this it keeps going right so pretty good example if you just like take your data set without thinking about it you're going to um run into something like this where suddenly we have yeah 49 parameters based on these these random urls that's definitely not what we're what we're interested in doing yeah and this is actually one of the things that um so we can talk a little bit about this if we have time a little later today but i think um a lot of times people end up using other packages i think the most common one is scikitlearn to fit models and when you fit a model in scikitlearn you can get the you can get these like uh parameters but they don't come with labels automatically and it's like it's a library that's really built around like building the model and making predictions not necessarily like inspecting the model output and so i think that that one of the reasons i like stats models is that when if i were to print out like just the standard model summary here like it's gonna give me i'm gonna have to like see that something crazy happened here um and then that hopefully gives me a clue that i need to like be more purposeful about my model um and it's not always the diagnosis that that has to happen isn't always like it's not always really detailed and small sometimes it's like a big mistake like this where you just threw something into a model and it didn't make sense um but you weren't thinking about it necessarily before so yeah we have a question on the facebook page of how do we deal with those categorical variables um do you so one we we touched on this in much greater detail in an earlier session so take a look at our youtube page and if you look at this series you'll find you'll find videos about dealing with category categorical variables but do you have like a quick answer of like how would you deal with something like this well so you can definitely include categorical variables in your model um but you need to be a little bit careful about it because depending on how many possible values there are of a particular variable you could end up with a whole lot of parameters in your model which means you have to estimate a lot of things and you've now created something that's super complicated um that may or may not really be improving the model so in our last session we talked a little bit about like what's the tradeoff between like accuracy of a model and or predictive ability of a model and like complexity complexity and i think depending on the problem there's maybe a different balance there's no right or wrong answer but i definitely think like there's a limit to how much complexity you potentially want in your model so we'll we'll actually we'll talk about that a little more today too yeah so i'm curious what and this is maybe a tangent but in looking at the url and then i was also curious about the column um description like we scroll over to see the description on some of these just to get a sense of like what those look like um what would you think of doing something like creating a new column that's like you know is the word um stunning in either the url or the description right could we do some like natural language processing of taking these taking this information that's in you know paragraph form or natural language form and trying to create new categories um like that or like you know definitely imagine you know washer washer dryer is already a column in here but let's say that column didn't exist could we look in the description and try to like create our own variable of like oh the description mentions a washer dryer totally yeah so that's a really good point um a lot of times in your data preprocessing stage you might like right now i think for time's sake we'll probably just get rid of this description column although we could we could probably relatively quickly write some code to pull something out from here um but but yeah for for sure there are definitely places where you can take data that exists in a format that you're not going to use like you're not going to throw description into your linear model and turn it into usable information like i like your example of if the word stunning is in here like maybe or i'm trying to think of another yeah i was looking i was thinking that when i was looking at the urls if you scroll down to uh or maybe you got rid of it at this point but um if you if you look at those urls um which were showing up in the when you were showing all of them when you were showing all of the uh parameters but the urls are like you know craigslist.org stunning dash bedroom like three dash bedroom uh yeah so i was thinking oh like there's information in those urls that even though we don't want to use the urls themselves as categorical things we could like maybe parse those urls to get um some information out of them yeah or maybe like ocean view or something like that yeah they'll want oh yeah interesting comment in the youtube chat from alex of uh you could just get the length of the description as a comp uh as a column um yeah what if i do len description is that going to give me the number of characters yeah i think so assuming it's just a string let's uh let's actually we can try that um let's before we do that though let's go back and let's actually like pull out some of these things so um i i feel like we can safely say the id column is probably not giving us any information towards uh price i'm gonna get rid of url as well because i figure anything that's in the description is probably in the url also i'll get rid of region url like you said because that information is also contained in this region variable um image url uh number 17 there probably isn't super helpful okay yeah um and then can leave we can leave lat and lawn i assume those are latitude and longitude which might be interesting so let's go ahead and start with that um so actually let's think should i use i think there's like a dot drop that allows us to easily get rid of columns or we can use subsetting but let's uh let's try our let's see if i can find this drop columns um i'm gonna just show you guys my googling as well because i feel like this is a useful skill indeed um okay so this is the the labels parameter gives the index or column labels to drop and it looks like i can just input like a list of labels that i want to drop so i'm going to go ahead and grab those so we've got id url region url i wonder if it doesn't in place or if we'll have to do housing you are probably right let's check and then was there anything else image url maybe okay was it img uh full image girl okay let's see this might just give us uh maybe it takes index of uh 0 1 2 instead of the column names i'm gonna try labels equals this and then we might need to give it an axis okay from the index or the column so let's give it axis equals one because we want to drop the column okay and then this just gave us this exact same data set but with those columns dropped and so now if we want to resave it we either need to add in place equals true or housing equals so i'll do it this way and then let's just see so if i take len of housing dot description is that gonna just give me a bunch of numbers that give me so uh interesting i bet that's like the sum total of the description i'm guessing you want to call them right right um you might need to do like an apply of like um or housing of description of the column uh let's see right so let's just get the column right housing.description of um i don't know that that is what uh that is that is what yeah so um let's try apply function to a column in pandas um i know that's the dot apply but i just want to pull up the pull this up so we can see it so um this allows us to apply the same function to every value in a column um so axis along which the function is apply zero would apply it to each column or sorry one apply each function apply the function to each row so if we want to apply to each row we would set that equal to one so let's try this as well um let's do like housing dot and then i see on youtube someone's uh pointing us to dot shape as well which uh which might be a uh a good way to do that i'll look into that as you uh as you work on the fly okay um well let's see this is gonna apply it to every column um so i wonder if i can just i wonder if i can just pull out like housing dot um description dot apply and then do like and give it wayne that's the function one do i do i would just give it len and then like then we have to figure out the axis stuff but one takes no keyword arguments um interesting let's see if i do like len of hello i am sophie it does give me the number of characters let's see so if we if we get just the column uh so if you do um if you do data frame or so if you do housing dot description dot s dot len dot s dot one dot string sorry dot s t dot str.len len and get rid of yeah and then parentheses after plan as well what does the str do oh wow i believe it was saying treat it as a string we can try to get rid of it you can try to get rid of str um and see what happens i think we had oh we had applied interesting okay so if we apply len to the entire into the entire series then there is no there's no way to calculate the length of a series object so we need to turn the series into a string or something a set of strings yeah super interesting was that was that your own googling or is that uh yeah so i google panda get length of string in column and it was the first result nice okay well i wish you guys could also see alex's screen but this is fun okay so this gives us the the length of the description let's um let's add this into our into our initial data set so we'll say like housing dot um description length equals it's gonna give me a warning but it's gonna work um let's go ahead and take a look at housing.head again yeah probably good to like verify that this is doing what we expect where um description and description link is probably all the way at the very end you didn't see it actually oh did you reset housing equals let's try this again but i think oh there it goes there it is um okay so now we've got a description length the number of characters um and then i'm gonna now drop description i don't think we're gonna use anything else but again like alex said we could we could do something else here we could figure out um we could pull out some word that we think is gonna be um associated with particularly um high like high prices like exceptional or special maybe like places that have us some sort of one free month special or more likely yeah we could even look for like free month in there because i think that that's that's not a data that's represented uh in any of our other columns like oh this comes with a free month well that like changes the you know real price of the apartment exactly so we could definitely pull some things out of here and and create some more columns around description but i'll leave that for now um and then i'll just grab this drop again so you can drop description okay and then let's take a look at it again okay i feel like we're getting closer um do you have any other questions before we try to fit a model like anything else you think should one thing i would want to do is because we kind of like made this description length thing on the fly i would kind of want to verify that that is indeed an integer rather than a string because like who knows who knows what data type uh you know these random functions that we were just googling it was throwing out so i probably want to look at the info again just to make sure that we know for that uh description length cool it's a it's a float that looks good um the other thing i mean one thing that i might look at before we get started is um first of all for some of these categorical variables like parking options and region i'm i'm curious how many different values there are because if there's like a thousand different values we're still gonna end up with like a lot of parameters in this model right um so one thing i might do is just like we can even do um is it describe give us a little bit more or allow us to um like this so in the region there's 404 different categories right so we probably don't want to throw that into the model willynilly because we're going to end up with 403 slopes um on region if we put that in it also looks like oh well actually that's an and for a reason there's 12 different types of apartments that's a little more reasonable um how would you go about um states yeah um yeah that's interesting we could look into that maybe you know puerto rico or something is in there um how would you go about kind of verifying that all of these that there's like no errors in the data of like oh maybe laundry options maybe one of the five is like capitalized differently than the others or like you i mean i would go through if if we were spending more time on this and i want to make sure we got some time for modeling but if i was spending more time on this i would definitely go in much more detail looking at each of these like so for example um let's look at type for a moment because this has kind of a lot of categories to just throw it into the model but not so many that we just want to like definitely throw it out um but so one thing i might do is first of all take a look at housing dot type dot value counts and so this gives me all of the values in this column so we've got apartment house townhouse condo duplex manufactured goes on so it looks like these are all at least their own thing right um we definitely have a couple categories like assisted living and land that don't seem very highly represented in the data and in fact like even even in law and flat and loft and cottage cabin are like oh there are a lot fewer of those um than all of the other ones so like one thing i might do is this is a lot of extra parameters for like not that much variation in the data i might collapse this into one category that's like other interesting um to just kind of cut this down a little bit the other thing i might do is take a look at a box plot of all these things and price so i might do like sns.box plot x equals type y equals price data equals housing and then show the plot okay so it's like there's one massive outlier this is actually okay let's let's investigate this further so like if i it might not even be one outlier it might be like a bunch and you're going to change somewhere so one thing i might also look at here is um guess i'll use i always forget if we're at his plot or his plot um seabourn keeps changing it's uh or disc plot like um and then do uh housing.price okay so we've got this like very very skewed um distribution here where we go from zero all the way up to like a million or more than it's the one with nine yeah so yeah 2.5 with nine zeros um so i think we wanna probably restrict this a little bit let's for the moment i'm gonna just kind of arbitrarily cut this down um what do you think is a good like cutoff point so we're gonna be removing that rose where price is over a million yeah or like so how would you go about deciding this in like the real world of like when can you just throw out data like this so um i guess there's again no clear answer um i'm sorry to say but i think in practice you kind of play around with this for a little while so let's like for example let's just start without making any permanent changes let's actually we can do this in here let's subset this right like within the plot and say we only want um values where price is less than 1 million um and then let's like replot this and it still just looks like super skewed like we still really can't see this variation um one thing we could do at this point is we could try we could even try like taking a log and see if like the log transform helps us get some things like oh and then here we're we're gonna have an issue with taking the log of anything um that's zero so that gives us a clue that there's actually some apartments where the price is zero so um now i'm gonna say like and housing.price greater than zero and i think i need to put these in parentheses and now okay so now we've got like a little bit more usable of a distribution um so this might be a clue that we want a log transform for our price variable um because i threw this log in here but we could also could we go in the other direction try like super like over cutting things and just say like hey what does it look like if we cut everything that's less than um you know two thousand or everything that's greater than two thousand um dollars and see yeah let's do like three thousand i feel like because these might be rental prices and this is yeah so that looks pretty good this is another thing that we could do instead of instead of just taking log we could just cut this down and say okay like realistically we really only care about this um when we're looking at apartments that are three thousand price range within our lowly price range um yeah and then i see somebody wrote in the chat can we see performance before and after including the outlier and definitely so um this comes back to a question that somebody asked on discord a couple weeks ago and i thought it was a great question which is like how do you compare models and i think totally if you have a data set where it's not super costly to run or time consuming to run the model you can definitely like run the same model with a bunch of different iterations getting rid of um getting rid of an outline liar or getting um or like taking a log transform not taking a log transform you can try all those things and refit the model it gets a little more difficult if you have like a huge data set then you might need to like take a subset of data to do that exploration um but yeah no no need to ignore your comment because it looked like it definitely looked like an one outlier in that plot that we saw um i think it was just like a weird artifact of the skew um but it definitely looked like an outlier um in that first plot so i would say based on looking at this um i feel like 3000 seems like a pretty good cutoff point i mean we could like look at this tail to ten thousand or five to like ten thousand and see if it looks like yeah i mean it kind of looks like to me this the cutoff is maybe like three or four thousand so my very um precise uh method for this i'm gonna cut it to 4 000 or smaller okay so let's let's actually do that we'll do housing dot housing equals housing housing price uh actually totally override it uh i like housing affordable um okay and that kind of that kind of distinction is like if we saved over housing then it would be a little bit trickier to say oh let's like plug in the let's plug in the model before we we cut out the super expensive um houses it'd be a little bit trickier to do that or we'd have to like reload the data set and uh yeah for sure okay let's do that um okay so now let's just take a look at this again all right so we've got region price and then i'm reminding myself as well we looked at this before we saw region is a little bit overwhelming of a variable right here um maybe like again i assume that the region is giving us like specific cities basically um maybe there's a way that we can do some like combining of categories again where we just say like north west yeah southwest right maybe we could oh market in the south and new york is in the northeast yeah um so there's there's lots more i guess it's gonna feel a little unsatisfying because there's a lot more cleaning of this data set and producing like not just cleaning but creating of new features so like feature engineering um that we could do before fitting a model that would probably improve this model because to be fair like the relationship between price and square feet is probably different by region right like the prices in jacksonville are going to be different than the prices in new york city and the relationships between like price and other variables might also be different in different regions so i feel like region is probably important in some sense but like yes i mean so ultimately we could do the same thing of like depending on you know depending on the realworld application of this if we're renting an apartment in denver we don't really care what rent is like in new york and so we could take this data set and just like we cut out all the super expensive apartments we could cut out all the apartments that are not in denver um that's also true yes um but what if we have apartments that we want to rent out in like every major city in the u.s yeah that's a good thing what do we do i don't know um okay so let's let's see what should we include in our first model should we include i mean i think the most obvious things are like square footage bedrooms yeah okay let's um let's let's get a a model going let me grab and grab the same code from up above and i'm gonna go ahead and start adding we're gonna do this with housing affordable and let's start grabbing something so we want should we include we'll include type for now um square feet and remind me of like those are certainly should be treated like a category right yes yes no but it looks like right now they're treated as like zero one um so remember that here let's actually print out the info again so all of these are being stored it looks like as integers so it's just zero one um and remember that when we when we fit this model with say like uh type what's actually happening under the hood is we're getting a new um like a new design matrix here that has 11 new columns that are ones and zeros um i can actually like print that out let's see i think it's like patsy dot you know what i'm gonna do i'm gonna go back to i think it was here i may have done this so let me try one more place yeah okay so this is a different different data set but let's just for a moment do you like rent or it was price as a function of type and this is um housing affordable and return it as a data frame and so yeah so this is the new x matrix that we get and so it'll have like this one for an intercept column of one for the intercept and then it'll have like type t assisted living and this will be ones and zeros type t condo and this will be ones and zeros and so it basically just creates a bunch of dummy variables with ones and zeros for all of the um for all of the possible values of that categorical variable so back up here this column cataloud is basically the same is being treated in the same way as this type column it's just we're not seeing it when we use this os from formula we're we're just putting in the string but in order to fit the model it's separating that column into like 11 new columns got it but if we if we had three so that that's working because it's binary if it's either cat's lab or not but if this were if there was a third category for that we wouldn't want it to be numerical zero one two we would want it to be you know string zero one two which would then be treated as a category or or the you know the string the actual names of those categories whatever they might be yeah exactly um that's a good point so again if we were being more thorough here we'd probably want to verify that the only values are zeros and ones um i'm pretty sure i've taken a look at this before and and verified that um but definitely would want to it also seems like these all of the columns where it is like a binary thing like cast can be allowed or not it seems like those are ones and zeros and anything where the type like type where there's more than two options it seems to be recorded as strings but that's an assumption that i'm making that's not like something that we have checked right now um okay let's go back so we were here so we had up to smoking aloud in the model let's do wheelchair access i'm just adding everything i don't know there might be a shortcut to add everything and then take some things out you know an r there is but um and then let's do what do you think we'll add description we'll add latin law let's add description length to start and then we'll we'll come back and add some more things um and then the data is housing underscore affordable and we're going to fit this model and then let's print out the summary it's going to be big but it might take a minute okay so and i'm actually gonna do zoom out so that we can see all of this this is one thing i don't like about the summary output i would love if they would output it for me as a data frame um directly but instead we get this like crazy thing that is subject to whatever weird formatting um jupiter notebooks decides to use um okay so we can actually see all a lot of the things we've already talked about we can see that the r squared is 0.129 and adjusted r squared is also 0.129 so i would interpret this as like this model is not super good at identify or at predicting price because we're really only um really only explaining about like 13 of the variation in price for all of these apartments um but we could use these numbers to compare this to a new model so let's see what happens and you'll notice like we do have at least the type variable like we said we've got like 11 different um values here right let's add in let's add in lat and lawn and just see if that improves it at all because right now we have nothing cutting down the region at all so maybe maybe that would be useful oh yeah live long is actually a great way to get the region uh but that's not going to be like linear right yeah it's hard to because it's not it's not like oh higher latitude is going to be more expensive yeah maybe we need like well okay let's add it in okay well it didn't prove it i feel like i mean for such a small number like this is only 0.129 so we're already up to like 15 percent so that's not terrible um i bet that that if we ran like a um anova comparing these two models i bet the bigger model would like that added complexity would probably be worth it um other ideas or questions uh question from the youtube chat how do we calculate feature importance oh um that is a very loaded question um so one thing that we can do is if there's something called like uh what are they called standardized coefficients um another another thing that we can do that's slightly different but related is we if we standardize all of the um all the quantitative variables then they're all on the same scale then they're then the um coefficients at least are all on the same scale so they are comparable so we could actually if everything was standardized we could compare coefficients at least for the quantitative variables um another thing that people do and i guess this is really a plug for the feature engineering um content that's coming out hopefully in a couple of months um that nitia is working on because there there are a lot of other mechanisms that we can use to try to like pick out features based off of their relative importance in terms of this model um or there's also ways that we can iteratively try to build this model for example like we can use like forward and or backward selection to kind of tell the computer okay like start with a model that has everything in it and start deleting things and testing them out and then if something helps to delete it then keep it missing and then like continue the process from there or the reverse we could start with nothing and try like adding predictors one at a time and then see which one and do it which one improves the model the most and then only keep that one and then iteratively do that again um we can also use like uh like ridge or lasso regression to try to shrink some of the coefficients to zero if they're or close to zero if they're not super relevant um i see a question i wonder what why stats models doesn't include in summary metrics such as mean absolute error means squared error root mean squared error is r squared better that's a interesting question um so r squared is related to all of those things um because it is also based off of the um it included in that r squared calculation you're calculating like the error as part of it and seeing basically saying like how much of that error are you or how much of the variation are you accounting for by adding that line in um but i don't know i don't know why they chose not to include additional metrics um it's always interesting um okay we are like running quickly out of time and i know we haven't gotten super far but at least we've had a chance to test out some models and hopefully we've gotten you've all gotten a taste of what this might look like on your own um i think the fun of today is kind of we didn't have a clear direction of where we were gonna go with this today um but it was i think i hope useful to kind of see how one might get started on an analysis like this and my hope is that if you're interested in learning more about linear regression and if you're interested in practicing these skills a little bit more that you might take this and run with it and like try running your own model see if you can improve this even more another thing i would probably do is start subsetting the data to one region and see if that allows me to get at least a better a little bit better um yeah but i think this kind of work in general just will make you more comfortable using all these tools make you more comfortable with like kind of solving arbitrary tasks like we were doing like how do i find the length of the string in this column just like the more and more work that you do working with data frames working with um these libraries that are making these models i think that that will just help grow your skill in like being able to or being more confident in doing these with when you need to do it in a real world situation yeah and another thing that i will say is once you have learned more methods for creating models then you can even like you could extend this even further this doesn't have to be just a linear regression problem this can be like a how well can i predict price problem and you can try even more methods and use some of the methods that we discussed here to still compare compare models and also some of these metrics that alex in the chat just mentioned so like mean absolute error is um is a useful one to compare different types of models because you know like you don't want to you don't necessarily have a way of calculating like aic for um for like every model that you might every type of model that you might create but mean absolute error you definitely could cool well that's it that's it for linear regression um i hope that you guys found this helpful and definitely let us know if you have any more questions we'll try our best to answer them and feel free to write those questions as comments in the youtube video or also on discord if you have been on discord yeah uh we don't have any other live streams currently planned but hopefully we will do another kind of series like this um in the near future but yeah might uh might take a little a couple weeks break um so we'll see what happens but uh yeah keep an eye on the youtube channel for more stuff like this and yeah thanks for thanks for doing this sophie so he led the you know eight straight weeks of this um great work sophie it's all fun thanks for joining me on all of these alex this is a wild ride all right cool all right
