With timestamps:

00:05 - and we're live
00:07 - hey hi everyone welcome back to our live
00:11 - stream uh this is the second in our
00:14 - three-part journey through codecademy's
00:16 - Evolution and today's topic is a fan
00:19 - favorite you know it you love it it's
00:22 - python for data science I'm Corey Steve
00:25 - Cook Academy's content marketing manager
00:27 - and I'm joined by fetty Garcia Lorca
00:29 - Lorca our community manager who you
00:32 - probably recognize from the forums and
00:33 - Discord as well as Dr Autumn Morse a
00:36 - senior instructional designer in data
00:38 - science who will be walking us through
00:40 - this project step by step and then over
00:43 - in the chat we have Eva savinga a senior
00:46 - curriculum developer in data science
00:47 - who's on hand to answer all of your
00:50 - questions
00:51 - uh in case you missed last week's live
00:53 - stream we're celebrating codecademy's
00:55 - birthday all month long with events and
00:57 - new course launches and lots more next
01:00 - week we're actually going to give you a
01:02 - look at our new course intro to
01:04 - generative AI so be sure to RSV for that
01:07 - live stream at the link in the YouTube
01:08 - description below and we'll see you
01:10 - there
01:11 - um today though Anna is diving into a
01:14 - project from our path Learn Python for
01:16 - data science python is the most popular
01:19 - programming language in our catalog and
01:22 - I'm actually curious how many of you
01:24 - here have taken one of our python
01:27 - courses please let us know in the chat
01:30 - um python is a general purpose language
01:32 - it's used for everything for data
01:34 - analysis to mathematical Computing to
01:36 - machine learning and web development in
01:39 - Anna's demo you'll get to see just how
01:41 - easy to read python syntax is and you'll
01:44 - also see how we work with Jupiter
01:46 - notebook a workspace for data science
01:48 - code and visualizations that's built
01:51 - right in to our data science courses and
01:53 - paths uh keep in mind that you can take
01:55 - this path whenever you want on your own
01:57 - time and it's linked in the description
01:59 - below and this live stream will be
02:01 - available to replay if you get stuck or
02:04 - need hints
02:05 - um before we get into it I want to tell
02:07 - you a little bit more about ADA Dr
02:09 - Autumn Morse has worked on courses
02:11 - across our data science catalog covering
02:13 - topics like python of course Excel data
02:17 - engineering and most recently Ai and
02:19 - prompt engineering she comes from a
02:22 - mathematics background and has a PHD
02:24 - focused on the design of self-assembling
02:27 - DNA nanostructures Ada has also worked
02:31 - as an Actuarial systems analyst she's
02:33 - taught math and statistics at Champlain
02:35 - College and she's conducted research
02:38 - supported by NASA and the Vermont space
02:40 - Grant Consortium Hi Ada
02:44 - we're so excited to do this I wanted to
02:47 - not like bumped this up and ask a
02:49 - question that a lot of people have uh
02:51 - what makes python so great for data
02:53 - science
02:56 - it's a great question
02:58 - um
02:59 - python is first of all a great language
03:02 - in general for beginners because it was
03:05 - designed to match sort of natural spoken
03:09 - English so that it sort of Narrows that
03:12 - distance between you know what you want
03:13 - to do and now how do you write the code
03:15 - to make the computer Do It
03:17 - um so that just makes it a really good
03:18 - choice for beginners in general and then
03:22 - in terms of data science
03:24 - um there's just been such a robust data
03:27 - science Community working in Python so
03:31 - um there's lots of packages of
03:33 - pre-written code so you don't need to
03:34 - sort of start from scratch with all of
03:36 - your data analytics
03:39 - um there's lots of people to ask
03:40 - questions and there's lots of courses
03:42 - available
03:44 - um and data scientists in general use
03:46 - Python so if you want to do data science
03:49 - at some point you're going to have to
03:51 - read python code and work in Python code
03:52 - so why not learn it totally all right
03:56 - take us away Ada sure
03:59 - um
04:00 - let me share my screen and here we go so
04:04 - we're going to be analyzing some
04:06 - high-speed Railway data in Python
04:10 - um light Corey said this comes
04:14 - um
04:15 - sorry
04:16 - um we're going to analyze a real data
04:18 - set from a high-speed rail network in
04:20 - China
04:21 - um so like Corey said this project is
04:23 - part of our new Learn Python for data
04:25 - science curriculum
04:26 - um and I'll show you some of what we
04:28 - just talked about in terms of how python
04:30 - is relatively easy to understand and
04:34 - Jupiter notebook is a pretty good
04:36 - workspace for doing data science
04:40 - um you might be wondering
04:44 - why we created
04:46 - um a new course about python when
04:49 - there's so many available and the reason
04:51 - is we really wanted to develop a
04:52 - curriculum specifically for data science
04:56 - um I don't know about you but I often
04:58 - want to learn something new around data
05:00 - science and then I'm stuck with you know
05:03 - weeks or months of learning sort of
05:05 - basic syntax and not getting to like
05:07 - build the thing I actually wanted to
05:08 - build so this teaches you python from
05:11 - scratch while you're actually analyzing
05:14 - real data sets about technology and
05:17 - sports and
05:19 - uh 11 or 12 others so I won't list them
05:22 - all because we want to get into the code
05:24 - so let's switch over to
05:28 - the code academy learning environment
05:31 - and take a look at this data set
05:34 - um so this is a real data set um I will
05:37 - just say as a disclaimer we made a few
05:40 - alterations to the data set structure so
05:42 - that it would make sense in the course
05:44 - and so that
05:46 - um you know you all could practice
05:47 - specific techniques with it
05:50 - um but we didn't modify any of the data
05:52 - so it's all real data about a high-speed
05:55 - rail network in China
05:57 - we can actually take a look at the raw
06:00 - data set
06:02 - um in Jupiter by clicking the Jupiter
06:05 - icon we have all our data sets in this
06:07 - data set folders you can see we have a
06:09 - whole bunch of different files
06:12 - um and they're all they all have this
06:13 - dot CSV extension which stands for comma
06:16 - separated values so if we take a look at
06:18 - what this data set looks like
06:22 - you'll see we've got a bunch of data in
06:24 - here it's not particularly easy to read
06:27 - um as as human beings but in our first
06:30 - row we've got what are column headers so
06:33 - we've got dates we've got ride IDs if we
06:36 - go down we have info like mileage and
06:39 - different delay statistics
06:42 - um and these are all separated with
06:43 - commas which is what that comma
06:45 - separated values means so we're not
06:48 - going to work with these files on their
06:49 - own because that would be a little bit
06:51 - difficult to do instead we're going to
06:54 - read them into python using a package
06:56 - called pandas and work with them there
06:59 - so let's go back to our notebook
07:03 - um so I've got the first little bit of
07:05 - code already loaded here
07:08 - um this first line import pandas as PD
07:11 - we're going to work with the data
07:13 - science package called pandas in this
07:15 - project
07:16 - um like I said at the beginning one of
07:18 - the advantages of python for data
07:19 - science is that we have all these
07:20 - packages already written and they just
07:22 - give us a bunch of code
07:24 - for standard tasks so that we don't have
07:27 - to rate them ourselves and can get right
07:29 - into Data analysis and because I don't
07:33 - like typing things I'm telling python
07:36 - here that I'm going to call pandas PD so
07:38 - I only have to type two letters instead
07:40 - of six okay
07:44 - um
07:45 - now that we have pandas imported
07:48 - what we can do is we can use
07:50 - functionality that's already been coded
07:52 - so pandas has a read CSV method so
07:55 - that's that same CSV comma separated
07:58 - values that's the file that we were
08:00 - looking at and so this is a method to
08:02 - read that data in and then structure it
08:05 - really nicely in a table called a data
08:07 - frame
08:10 - ah session Timeout on the live stream
08:14 - okay disaster averted everybody
08:18 - um
08:19 - it's just live stream luck so
08:22 - like I said it pulls in that data
08:25 - formats it nicely on a table so we can
08:27 - read it much more easily
08:29 - um and I always like after I read a data
08:32 - set in to just go through all the
08:33 - columns and make sure I understand
08:34 - what's going on so we have the date
08:36 - column
08:37 - um it looks like these are all going to
08:39 - be from January 1st 2020. if you look at
08:42 - the file name of the data set it kind of
08:44 - gives a clue that the file name of the
08:46 - data set contains that date
08:49 - um we have a ride ID
08:52 - a train number and a station order
08:55 - um the eagle-eyed might notice that the
08:58 - ride ID is a composite column so we
09:02 - start off with our train number
09:04 - then we've got our date
09:06 - and then we've got the station order and
09:10 - so what's happening here is we're
09:11 - actually tracking a train as it goes
09:13 - from Station to Station along its route
09:17 - um so each row is an individual trip
09:20 - from one station to another station with
09:22 - no breaks in between
09:25 - um and then we're following the same
09:26 - train
09:27 - g1226 as it goes from Station to Station
09:32 - um
09:33 - we have our station name data we also
09:36 - have mileage
09:37 - um which is actually in kilometers so
09:39 - maybe this column should be called
09:41 - kilometer Edge
09:43 - um and then we have delay data so
09:45 - arrival delay and departure delay
09:49 - um you might notice there's some
09:50 - negative numbers which is kind of fun so
09:52 - those are early arrivals
09:55 - um or early departures so this first
09:57 - trip it arrived on time and it departed
10:01 - early
10:03 - um so we'll dig into more of that those
10:05 - details as we go along our last column
10:08 - is major holiday this looks like it's
10:10 - just a Boolean column so Boolean means
10:12 - it's true or false and it just tells us
10:15 - was this on a major holiday
10:19 - um
10:20 - or was it not on a major holiday because
10:22 - that might impact
10:24 - um how the trains work
10:26 - all right
10:27 - um so one thing we have built into this
10:30 - for the course just in case you're
10:31 - taking the course who you know are these
10:32 - toggles where we do basically what I
10:34 - just did and go through the data set and
10:36 - let you know what's going on um I'm not
10:39 - going to look through those
10:40 - in the live stream
10:42 - so our first task whenever we're working
10:44 - with data is to read data into the file
10:48 - um
10:50 - read data into into pandas rather from
10:53 - the file and make sure it's all nice and
10:56 - clean
10:57 - um this is something that's often sort
10:59 - of missing from data science classes my
11:00 - own personal pet peeve a lot of data
11:03 - science is getting data and then
11:05 - structuring it so that we can actually
11:07 - work with it and so this project comes
11:10 - from a section of the course where we're
11:11 - teaching you how to do that so we're
11:13 - actually going to get our hands a little
11:15 - bit dirty here with the raw data and
11:18 - cleaning it up nicely so that we can
11:19 - then get to some you know cool insights
11:23 - later on in the project
11:25 - um so
11:27 - when we looked at the data sets there
11:29 - were a whole bunch of different files
11:31 - and there's one file for each day in
11:33 - January this is a pretty typical
11:35 - structure for data where you know if
11:37 - reports are being generated on a daily
11:39 - or a weekly or a monthly basis um each
11:41 - day week or month is stored in its own
11:43 - file
11:44 - now we want to read in all of those
11:47 - files right we want all of the data that
11:48 - we have and we don't want to do that by
11:51 - hand because there's
11:53 - um actually 27 files so I don't want to
11:56 - type
11:57 - the same code 27 times
11:59 - fortunately python is a robust
12:01 - programming language so we won't have to
12:03 - do that
12:04 - um so what we're going to do instead is
12:06 - we're going to create a variable file
12:08 - names
12:09 - and this is going to store a list of all
12:12 - of the files that we want to read in um
12:16 - and before I I write that just a quick
12:18 - note for anyone who's like the total
12:20 - beginner in Python it can be really easy
12:23 - to get hung up on like do I understand
12:25 - all the syntax that's going on I'll
12:27 - explain everything along the way but
12:30 - also feel free to just relax and see
12:32 - this as an opportunity of seeing like
12:33 - what python can do
12:36 - um you know I wouldn't be too worried if
12:39 - you're like I don't understand the every
12:41 - single tiny bit of bit of syntax
12:45 - um so what I want to do is I want to
12:47 - build a list of all of the file names
12:49 - um and again I don't like typing so I'm
12:51 - gonna I'm gonna try and make this as as
12:53 - short as possible for me
12:55 - so the structure of the file name is
12:59 - pretty much the same for each file
13:02 - in that what we have is we have our data
13:04 - sets folder
13:06 - and then we have the railway delays 2020
13:09 - and January and so the only thing that's
13:12 - changing for each file is the day
13:15 - um so we have data from January 1st
13:17 - through January 27th so if I wanted
13:21 - January 2nd I would change that to be a
13:23 - two
13:24 - or January 3rd I'd change that to be a
13:26 - three
13:27 - um and so what we're going to do is
13:28 - we're going to use Python to do that
13:30 - change for us automatically
13:33 - um first I just want to show you how I'm
13:35 - going to break this apart for a single
13:38 - file name so what I would do is I'd say
13:40 - okay
13:41 - all of that
13:44 - um at the beginning is going to be the
13:45 - same for every file so I'm going to
13:47 - leave that in its own string
13:49 - and then
13:51 - I'm going to separate off
13:53 - the piece that is changing so the piece
13:56 - that is changing is this number one
13:57 - that's going to change to a two to a 3
13:59 - to a four and what this plus sign is
14:02 - doing is it's telling python take all
14:04 - these individual pieces and just squish
14:06 - them together the fancy word is
14:08 - concatenate but I'm not so fancy so
14:11 - let's just say we're going to squish
14:12 - them together
14:14 - and if I run this
14:16 - um nothing will happen because I haven't
14:18 - told python to print anything out but if
14:20 - I run it
14:21 - telling Jupiter notebook actually in
14:23 - this case to display that variable at
14:25 - the end you'll see that python has done
14:28 - that sort of concatenation or squishing
14:30 - together we've got our one in there now
14:34 - I just need this to update for every
14:36 - single
14:37 - um file and so the way I do that is
14:40 - introducing another variable so instead
14:42 - of this one
14:43 - I want that to just be whatever the day
14:46 - is right whether it's the first or the
14:48 - second or the third it's going to be a
14:50 - variable that stores the day
14:52 - and then
14:54 - I need to tell python what are the
14:58 - different values that the day can be
15:02 - so
15:04 - the key word here is the word for just
15:06 - because that's what python uses so four
15:10 - day in and then there's this range
15:14 - syntax that lets me specify a range
15:16 - now the funky thing is for basic python
15:19 - is that I want to go up to January 27th
15:23 - that's the data I have the way this
15:26 - range thing works you actually give it
15:28 - the next value 28 I cannot tell you the
15:33 - number of times I've done this wrong
15:34 - just by not thinking about it
15:37 - don't stress too much
15:39 - um it's a pretty common issue to run
15:41 - into
15:43 - and so now if I run this what I just
15:46 - realized is I'm going to get an error
15:50 - let's chat about that
15:52 - um so when I run this I get an error
15:55 - called a type error
15:57 - and if I read down to the description it
15:59 - says you can only concatenate Str not
16:03 - int to Str
16:04 - a little bit
16:07 - confusing but Str stands for string
16:11 - that's like a bunch of text maybe with
16:14 - some numbers like the file name int is
16:18 - an integer and so what's happened here
16:20 - is my day is an integer and python is
16:22 - saying I'm just a computer I can't take
16:25 - a string which I think of as being a
16:27 - bunch of text and I don't know how to
16:30 - add an integer to that
16:33 - so to fix this I need to tell python
16:36 - that day should actually be a string
16:39 - again this is something that's easy to
16:41 - forget I just forgot it
16:43 - um but fortunately the error message was
16:45 - pretty helpful
16:46 - and now if we run this we've got all our
16:48 - file names in a beautiful list
16:51 - um
16:52 - I don't know even now I find it
16:53 - satisfying what can I say
16:56 - um now there are some fancy packages
16:58 - that can sort of shorten the amount of
17:00 - work you need to do to find like all the
17:02 - CSV files in a certain directory some of
17:05 - you might have encountered something
17:06 - called glob or something similar I think
17:09 - one of the cool things about python or
17:10 - any programming languages once you know
17:12 - the basics you can do a lot of things
17:15 - just by being creative
17:18 - um but certainly you can also look for
17:20 - special packages that might might help
17:22 - things out
17:23 - in the meantime we've got all our file
17:26 - names which is pretty exciting and now
17:29 - we just need to read those in
17:31 - as
17:33 - um data fribs so let's give that a shot
17:38 - so again I'm going to Define a variable
17:41 - the variable this is just a name that's
17:43 - going to store
17:44 - all of these data sets because we're
17:46 - going to have to read them in one by one
17:48 - as individual data sets
17:51 - now
17:52 - let's just grab the code from above
17:57 - um that read the data set in so this is
18:00 - the read CSV function
18:04 - now this would just read in a single
18:06 - file right but what I want to do is I
18:08 - want to read in a list
18:10 - so what I'm going to do is I'm going to
18:12 - use these square brackets again to say
18:14 - I'm reading in a list
18:16 - now
18:19 - what goes in here inside the read CSV
18:22 - function is a file name and I want that
18:25 - file name to be changing right I don't
18:27 - want it to be just this one file name so
18:30 - I'm going to do the same trick that I
18:32 - did before to generate the file names
18:34 - and I'm just going to say this is a file
18:36 - name I don't know what it is but it's a
18:38 - file name
18:40 - and once again I need to tell python
18:42 - what are the file names what are the
18:44 - different names of all of my files and
18:47 - that's why I created that file name list
18:49 - so again I can tell python what values
18:53 - to sort of
18:55 - put in for the file name using the word
18:58 - for
18:59 - file name
19:01 - in
19:03 - file names now file names here that's
19:06 - the list that we just defined that has
19:08 - all of our file names
19:10 - and then the file name on its own this
19:12 - is just the variable that's going to be
19:14 - read into all of these different
19:16 - um
19:18 - data sets or they're called Data frames
19:20 - in pandas once we've read the csvn the
19:24 - structure is called a data frame
19:26 - so
19:30 - we can't preview all of them right now
19:33 - but we can do a couple checks
19:36 - to make sure things are looking good so
19:39 - what we can do is we can print out the
19:42 - very first data frame so what I've done
19:45 - is I've taken this variable data Railway
19:47 - delays DFS this is where we're storing
19:50 - all of those data frames
19:53 - the square brackets and the zero says
19:56 - get the first one again you'll remember
19:58 - from before with 28 and 27 python starts
20:01 - counting at zero
20:03 - really fun and really annoying but zero
20:06 - says get the first one
20:08 - and then pandas has this helpful head
20:10 - method which will display the first five
20:12 - rows or so of the data frame so let's
20:16 - run that great our first data frame
20:19 - loaded correctly looks good
20:22 - um
20:23 - just to be certain that we've got 27 of
20:27 - these
20:28 - I'm going to also check the length
20:34 - so python has this function Len which
20:37 - will output the length of a list
20:41 - and beautiful we have 27. now that
20:43 - doesn't actually mean that they all were
20:45 - read incorrectly but I feel I feel
20:46 - confident enough that I can move on to
20:48 - the next step now that I know we've got
20:50 - 27 data sets probably all 27 read in
20:53 - correctly
20:56 - but we still have 27 separate data
21:00 - frames and we want one data frame we
21:03 - want one data set that contains all our
21:05 - data right if we look at our first data
21:11 - set that we pulled in here right this
21:13 - was all January 1st now what I want is I
21:16 - want to Stack this and there's a bunch
21:18 - more rows under this this is just the
21:19 - first five rows I want to Stack this
21:22 - above January seconds data above January
21:25 - thirds data to get all of them in a
21:27 - single data set
21:29 - once again the great thing about pandas
21:31 - is someone has already written this code
21:33 - for us
21:34 - so all we need to do is write the code
21:37 - to call that pandas method
21:41 - so I'm going to create a new variable to
21:45 - store our full data frame
21:48 - um I've created a typo
21:51 - fix that
21:53 - and
21:55 - I'm going to call pandas using PD and
21:58 - now the method is dot concat this is
22:01 - short for concatenate just like we were
22:03 - squishing strings together earlier now
22:05 - we're going to squish data sets together
22:08 - um and what are the data sets I'm going
22:11 - to concatenate well the ones I just
22:15 - imported using read CSV
22:19 - and now
22:22 - um what I would like to do so that works
22:24 - nicely but I would like to print out
22:28 - some information about this
22:32 - um this new new data frame to make sure
22:34 - everything worked out correctly
22:36 - um and the way I'm going to get this
22:37 - information again is the
22:39 - pandasmethod.info this is going to tell
22:41 - me things like what are all the columns
22:42 - are there any missing values all of that
22:45 - kind of exciting information
22:48 - um so the first thing I see in my output
22:51 - is I've got
22:53 - 29 000 total entries
22:58 - um
23:00 - and then in this First Column it looks
23:04 - like I've got 28 000 non-null so those
23:07 - don't quite match so it looks like there
23:09 - might be some missing data but it
23:11 - actually looks like we just have some
23:13 - other data Integrity issue going on here
23:15 - because
23:16 - these columns are all lower case and
23:21 - then I have a second set of the same
23:22 - columns that are uppercase
23:25 - and then I have this third copy of the
23:28 - station name column which is lowercase
23:31 - but is somehow different from the
23:34 - lowercase version that's above so
23:37 - I think we've got just a little data
23:40 - Integrity issue chances are at some
23:42 - point the way the data sets were encoded
23:45 - or created changed from lowercase to
23:47 - uppercase or from uppercase to lowercase
23:51 - um and for this last station name issue
23:53 - my guess would be that there's some
23:55 - sneaky white space so there's actually
23:57 - like a space before the text or after
24:01 - the text that's causing pandas to read
24:03 - it indifferently
24:05 - so in order to actually get to our
24:08 - analysis we need to fix these problems
24:12 - um
24:13 - so what we're going to do is we're going
24:15 - to go back to our list of all of our
24:19 - data frames and just quickly correct all
24:21 - of the column
24:24 - um column headers make sure that they're
24:26 - all lower case make sure that there's no
24:28 - extra white space
24:30 - the fun thing is we don't actually have
24:31 - to look
24:32 - at any of the data frames on their own
24:35 - we can just have python check and fix
24:39 - them all so what I'm going to do is I'm
24:42 - going to write what's called a for Loop
24:45 - so a for Loop we've seen this a little
24:47 - bit before as we created these file
24:49 - names this starts with the keyword for
24:52 - and it's going to tell python keep doing
24:54 - something over and over and over
24:57 - and like we did before I'm going to do
25:00 - this
25:01 - over all of our data sets so we've read
25:04 - all our data sets in
25:06 - and I'm going to look at every single
25:08 - data frame
25:11 - in
25:13 - that list of data frames DF again it's
25:17 - just sort of the uh the standard
25:19 - shorthand for data frame
25:21 - so what this is telling python is take
25:25 - every single data set that you read in
25:27 - and do something with it
25:29 - and now let's specify what we want to do
25:33 - the first thing I want to do is I want
25:36 - to lower case
25:37 - column names
25:39 - so in pandas
25:41 - we can access the columns using this dot
25:44 - columns attribute and that will return
25:47 - all of the column names
25:50 - and now I want to change
25:53 - the value of the column names right I
25:55 - want to take those columns and I want to
25:57 - lower case them so again I'm going to
26:00 - access the columns
26:02 - and now I'm going to use Str for string
26:05 - this will tell pandas I'm about to do
26:08 - something involving text right so
26:10 - uppercasing lowercasing those are all
26:12 - things that involve text
26:13 - so dot Str says hey text a method is
26:16 - about to happen
26:18 - and now it's very well named it's just
26:20 - lower
26:21 - um to lowercase them
26:23 - the other thing I want to do is I want
26:26 - to remove white space
26:28 - so this is going to be really similar
26:29 - I'm going to access the columns
26:33 - and I'm going to say
26:36 - we're going to modify this using a
26:38 - string or text method
26:41 - um and this one is called strip and
26:43 - that's just going to get rid of any
26:44 - white space that we don't want
26:47 - and now once we've done that each of our
26:51 - individual data frames should have the
26:54 - right column names but we'll double
26:57 - check by putting them all back together
26:59 - again and making sure that everything
27:02 - looks correct
27:04 - um so I'm just going to take our code
27:07 - for putting all of the data frames
27:10 - together
27:11 - and
27:14 - we are going to run that again after
27:17 - we've changed the column names
27:20 - make sure that everything looks good
27:24 - beautiful so now we've got just one set
27:28 - of column titles and we've got the right
27:32 - number of entries so we have a total of
27:36 - 29 000 entries in this data set that's
27:39 - rows in the data set and each of our
27:42 - columns has that number of values so
27:46 - that doesn't necessarily mean there's no
27:48 - missing data there could be something
27:49 - sneaky like if someone put in unknown or
27:52 - something like that as a value but at
27:53 - least there's no null data which means
27:55 - there's just nothing for that entry at
27:58 - all
27:59 - all right so I told you a lot of data
28:03 - science is data cleaning and it's true I
28:06 - promise we're going to get to some
28:07 - analytics in just a minute
28:10 - um but let's go through all of the best
28:12 - practices just to make sure that um
28:15 - we're doing things right so the last
28:18 - thing we're going to do is we're going
28:19 - to do some quick cleaning items on the
28:24 - the new full data frame
28:27 - um a couple of these are just good
28:28 - practices to follow so we're going to
28:31 - make sure that all of the text columns
28:34 - are are have the same case this is a
28:38 - really frequent thing that gets encoded
28:40 - incorrectly and we're going to strip any
28:42 - white space
28:43 - um from them because we want to make
28:45 - sure that that's not going to impact our
28:47 - analysis and those are pretty hard to
28:48 - spot
28:50 - in you know 29 000 rows so let's just
28:53 - take care of all of them
28:56 - um and then the last thing we're going
28:57 - to do is we're going to deal with
28:59 - something involving the numbers
29:03 - um the the numeric data and I'll talk
29:06 - about that in a moment
29:08 - um because
29:09 - I lost my connection I'm not sure what's
29:12 - going on there but let's work on this
29:15 - cleaning so once again I don't want to
29:19 - have to do all of this by hand so I'm
29:21 - going to write a for Loop the for Loop
29:24 - is going to tell python do this to all
29:27 - of my columns so I want to take each
29:30 - column of the data frame and I want
29:32 - python to do some cleaning on each
29:34 - column so
29:38 - remember I can access the columns using
29:42 - dot columns
29:43 - so I can say take each column
29:46 - in the list of columns and now we're
29:49 - going to do stuff with it
29:51 - so
29:52 - the first thing I want to do is I want
29:55 - to uppercase
29:58 - all text just to make sure everything's
30:01 - in the same case
30:03 - obviously not every column is a text
30:07 - column so we're going to have to deal
30:09 - with that as well but the code for
30:12 - actually uppercasing it is pretty
30:14 - straightforward right we would do the
30:16 - same thing we would access the column
30:19 - and then we're going to use that same
30:21 - Str dot in this case upper syntax
30:26 - so
30:28 - the first thing I need to do is correct
30:31 - my variable
30:35 - the actual data frame that we're working
30:36 - off of is our full data frame with all
30:39 - of the data from every single day
30:42 - and so now I'm going to take
30:46 - that data frame
30:51 - and I'm going to access the column these
30:54 - square brackets are just what we use to
30:57 - access columns or rows or specific sets
31:00 - of data within a data frame
31:04 - and then I'm going to say okay what do I
31:06 - want this column to be well I want it to
31:08 - be the same column right so I have this
31:10 - equal sign I want it to be the same
31:12 - column I just want everything to be
31:13 - uppercase now you might wonder why
31:15 - uppercase
31:17 - um
31:18 - no specific reason we just want them all
31:20 - to be the same
31:23 - and the other thing I want to do is I
31:26 - want to strip
31:28 - the text strip any white space
31:33 - so it's pretty much the same syntax so
31:36 - I'm going to copy and paste because
31:39 - that's how we get things done
31:41 - and I'm going to use dot strip now if I
31:45 - run this now
31:46 - I think we're going to get an error
31:48 - let's see
31:49 - we're gonna get an error phew
31:52 - um so the error here is an attribute
31:55 - error and what attribute is doing is
31:56 - it's referring to that str.upper and if
32:00 - you scroll all the way down it's going
32:01 - to tell you you can't use this string
32:04 - accessor so we can't use upper casing on
32:08 - things that aren't strings right on
32:10 - things that aren't text so what we're
32:12 - doing here is we're going over every
32:13 - single column which includes numeric
32:15 - columns and we are trying to upper case
32:19 - a number and python is saying I don't
32:21 - know how to uppercase a number so we
32:23 - need to fix this really quickly you know
32:24 - there's a few different ways to do this
32:26 - if you're super fancy pandas has a
32:29 - built-in like select D types method
32:32 - um for our purposes
32:34 - we can just check what the type is of
32:37 - the column before we actually call any
32:40 - of this code
32:41 - um so we do this in Python using
32:43 - something called if else syntax so this
32:46 - allows us to only execute code if we
32:49 - meet a particular requirement
32:51 - so in this case the requirement we want
32:54 - to meet is that
32:59 - the data type
33:01 - so dot d-type is the data type of the
33:04 - column is an object
33:08 - um object is how pandas stores test so
33:11 - that's the the technical name for it
33:14 - if that happens then we will perform
33:19 - this code if you're new to python the
33:22 - reason I just added a tab there is
33:25 - because python figures out what code
33:27 - belongs where by indentation so because
33:30 - I have indented here
33:33 - um that means that python now knows that
33:35 - all of this string code
33:39 - belongs to this statement and so it'll
33:41 - only try to uppercase things that are
33:44 - actually
33:46 - um
33:47 - are actually text
33:50 - we're really running into the live
33:53 - stream curse here of disconnection
33:58 - um
34:01 - all right I think that's fixed so now it
34:04 - should run and it did run and let's just
34:06 - double check
34:08 - that everything's looking good
34:13 - so I'm just going to print out the first
34:16 - five rows
34:21 - Railway delays boom
34:36 - ah I have a typo
34:39 - beautiful it's always hard to spot those
34:41 - typos I wrote Railways not Railway come
34:45 - on Otto get it together great
34:47 - everything's uppercase that's beautiful
34:50 - we haven't done anything to our numbers
34:52 - now the only other thing I notice in
34:54 - terms of cleaning is that these numbers
34:56 - have decimal points so we have like 1.0
35:00 - 2.0 these should be integers
35:03 - um and it's actually the same with the
35:04 - rest of these numbers they're showing up
35:06 - as floats
35:08 - um float is the the technical term for
35:10 - something that has a decimal but they're
35:11 - actually integers now this doesn't cause
35:13 - too many problems
35:15 - um but when you're working with a lot of
35:16 - data integers are easier to store and
35:19 - there's certain things about how python
35:20 - implements arithmetic that might be
35:22 - different
35:24 - um so we'd like to correct all of those
35:27 - um now we can do that just in the same
35:29 - code
35:31 - um by looking for only numeric data type
35:35 - columns and then changing that to
35:38 - integers
35:39 - so again in order to sort of zoom in on
35:44 - only the columns we're interested in
35:45 - what we can do is we can test the data
35:49 - type
35:51 - um so I'm going to copy and paste this
35:53 - code here so what I'm saying is take our
35:57 - data set access the column we're on
36:00 - check the data type and we're going to
36:03 - check if it's a float
36:05 - um there's some technical I should say
36:07 - there's some kind of technical things
36:08 - about testing numeric data types that
36:12 - are a little tricky uh that we don't
36:14 - have time to get to in this live stream
36:15 - for our purposes today this is this is
36:18 - good good enough
36:21 - um but
36:23 - um just something to be aware of
36:25 - so now if we've got a float which means
36:29 - we've got a decimal what I want to do is
36:32 - I want to take
36:35 - our column
36:39 - and what do I want I want it to be an
36:41 - integer so that just means it's a number
36:44 - that doesn't have that decimal point
36:45 - again this mostly has to do with
36:51 - um storage concerns and and how
36:53 - arithmetic is actually implemented than
36:55 - anything else
36:57 - now this is where it could all go wrong
36:59 - let's run this
37:01 - ah it all went right though we've got
37:04 - integers one two three four five instead
37:06 - of 1.0 2.0 3.0
37:09 - storage concerns aside it just looks
37:11 - more elegant to me so I like it
37:14 - and we're finally done getting our data
37:17 - set together I know that's
37:19 - occasionally a little bit tedious but in
37:21 - all honesty this is a huge part of any
37:23 - data science job
37:25 - um so it's really important to know how
37:27 - to do
37:29 - um and it's also in my experience a part
37:31 - of a lot of data science interviews how
37:33 - would you clean this data set
37:35 - all right so let's look at some delay
37:37 - statistics
37:39 - um I'm actually going to just in the
37:41 - interest of time
37:45 - um
37:46 - we've got some sort of exploratory
37:48 - things what we're going to do is just
37:50 - start by looking at delays on holidays
37:53 - to see does it look like there's a
37:56 - significant difference in terms of
37:58 - delays on major holidays versus
38:00 - non-major holidays you know maybe more
38:01 - people are traveling maybe less people
38:03 - are traveling and maybe that impacts how
38:05 - well the trains run on time
38:07 - so
38:10 - um let's take our
38:13 - Railway delays
38:16 - full so this is our data set
38:19 - um and what we're going to do is we're
38:21 - going to ask python to group all of this
38:24 - data so in one group we're going to have
38:26 - all the major holiday rides in another
38:28 - group we're going to have all the other
38:30 - rides and then we're going to do
38:31 - calculations on those two groups
38:34 - so the way we do this in pandas is this
38:36 - method called Group by so split into
38:39 - groups and then we're telling it what do
38:41 - we want it to split into groups using we
38:45 - want the groups to be based on major
38:47 - holiday so what's going to happen from
38:49 - here is pandas will run through the data
38:51 - set and it'll look at each row and it'll
38:53 - say oh was this a major holiday or was
38:56 - it not and separate them into those
38:58 - groups based on that
39:00 - and then we can use that to perform a
39:04 - calculation
39:05 - so the calculation we're going to
39:07 - perform is we're going to calculate the
39:09 - average of our two delay columns
39:13 - um there's a bunch of different ways to
39:15 - do this just so you know so if you know
39:17 - a different way don't worry
39:19 - um I'm going to use a method called dot
39:22 - AG that stands for Aggregate and that's
39:25 - sort of a technical term for what we're
39:27 - doing when we take a bunch of data and
39:29 - compute something like the average
39:31 - um some people call that aggregation
39:34 - and what we need to feed into this
39:37 - aggregation method are the columns that
39:41 - we want to perform calculations on and
39:43 - the calculation we want to perform
39:46 - um so this takes the form of what's
39:49 - called a dictionary if you're not
39:50 - familiar with dictionaries don't worry
39:52 - about it too much basically what I do is
39:55 - I specify my column name arrival delay
39:58 - a colon and then I tell it what I want
40:01 - it to compute I wanted to compute an
40:03 - average which is also called a mean and
40:06 - so in pandas the the word used is mean
40:09 - and then I tell it hey I also want to
40:12 - look at the departure delay
40:14 - and I want to calculate the mean
40:18 - and now provided I've remembered all my
40:20 - syntax correctly
40:22 - we get a lovely table that breaks us
40:26 - down into two groups so major holiday
40:28 - we're broken down into it's not a major
40:30 - holiday or it is a major holiday
40:32 - and then we have both our arrival delay
40:34 - and our departure delay
40:37 - um and the thing I noticed right off the
40:38 - top is that these look really similar it
40:40 - doesn't look like there's much of a
40:42 - difference
40:43 - um
40:45 - 23.3 for arrival delay if it's uh not a
40:50 - major holiday 23.8 if it is
40:53 - that's not so much of a difference
40:55 - um we could certainly test using
40:57 - statistics to see if that that actually
40:59 - is statistically significant but I don't
41:02 - think anyone would really notice that
41:03 - difference in delay just like standing
41:05 - and waiting for the train
41:06 - so I feel like we can probably say
41:08 - chances are
41:09 - there's not that much of a difference
41:11 - now if we wanted to really verify things
41:13 - we might want to dig into this and say
41:15 - hey is
41:17 - are there
41:19 - fewer rides on holidays so do we not
41:21 - have the same robust quantity of data
41:24 - for both different groups right there's
41:25 - things we could do to examine this
41:27 - further but
41:28 - it looks like holidays don't change
41:30 - things too much for the high-speed rail
41:31 - network in China which I have to say is
41:33 - different than my experience here in the
41:36 - United States
41:38 - um
41:38 - another thing we could look at is the
41:41 - distance between stations so we could
41:43 - say hey if the if it's a longer trip
41:47 - more things could go wrong so there's
41:49 - more likelihood of delay
41:52 - um so to look into this the first thing
41:56 - I'm going to do
42:01 - is I'm going to take a look at the
42:03 - mileage column
42:04 - so again I need to access my data frame
42:07 - first oops
42:09 - and then using square brackets I can I
42:12 - can access a specific column in this
42:13 - case the mileage column and I'm going to
42:16 - use a built-in method in pandas called
42:18 - describe and this is going to tell me
42:20 - some statistics about the mileage column
42:23 - what I am especially interested in is
42:26 - the average so it looks like the average
42:27 - distance between stations is about 88
42:30 - kilometers
42:32 - um and so what I'm going to do is I'm
42:33 - going to look at okay if we have above
42:36 - average versus below average length how
42:39 - does that impact the delays
42:42 - so
42:44 - it really similar to what we did with
42:47 - holidays where we said okay we want to
42:49 - group by whether it is or is it a major
42:51 - holiday we're going to do the same thing
42:53 - with whether it is or isn't a longer
42:55 - than average trip
42:58 - unfortunately in our data frame we don't
43:00 - have that data
43:04 - right if I do Railway delays full and I
43:08 - look at the first five rows with major
43:10 - holiday we had a column that told us
43:13 - this row belongs to a major holiday or
43:15 - it doesn't we don't have that in our
43:17 - data set
43:18 - um so we're going to have to add it to
43:20 - our data set
43:22 - um so this is one of the fun things
43:23 - about pandas is we can just add columns
43:26 - to our data set essentially willy-nilly
43:30 - um so I want to create a new column
43:32 - that's going to tell me is this a long
43:34 - distance versus a shorter distance trip
43:36 - now if I just ran this code now I would
43:39 - get an error because this isn't a column
43:41 - yet but I can still Define it to be a
43:45 - column
43:46 - and what I want the information I want
43:48 - is
43:50 - is the mileage value bigger than average
43:53 - or less than average so the first thing
43:56 - I need to do is I need to get that
43:57 - mileage value
44:00 - which I'll do the same way we've been
44:02 - doing it right I'll go into the data set
44:04 - and I'll access the mileage column
44:06 - and then I need to check is it greater
44:08 - than the average which is 88. and so
44:12 - what this code here will do is it will
44:14 - just check every single time is my
44:17 - mileage bigger than the average or not
44:19 - and let's just quickly preview
44:27 - to see how that works
44:31 - I swear I typed real way
44:34 - um
44:34 - amazing so we've got a long distance
44:37 - column now which I love
44:40 - um
44:41 - and all of these are false false false
44:45 - false false false which is good because
44:47 - all of these rows the mileage is below
44:49 - 88. let's actually preview a few more
44:52 - rows and see if we can grab some that
44:54 - are above 88 so I can add this sort of
44:57 - special parameter or argument to
45:01 - my preview code to get 10 rows instead
45:04 - of five
45:06 - and yes I've got a true
45:08 - here for long distance and it's awesome
45:10 - because that's 263 kilometers that's
45:13 - what I want to be happening
45:15 - um that's bigger than 88 that is a long
45:17 - distance trip so I'm very happy about
45:20 - that
45:21 - and now we can do exactly the same thing
45:23 - we did with major holiday we're going to
45:25 - split this data set into two groups is
45:27 - it long distance or short distance and
45:28 - we're going to calculate some averages
45:32 - um so I'm going to cheat and copy and
45:35 - paste the code
45:36 - because
45:39 - Y type more things that are absolutely
45:41 - necessary and it's one of the nice
45:42 - things is that a lot of this code ends
45:45 - up being the same right the only
45:46 - difference here
45:48 - is I want to group by long distance
45:51 - so I still want to calculate the
45:53 - averages of the arrival delay and the
45:55 - departure delay
45:57 - the only thing I'm changing is that I'm
45:58 - grouping by long distance
46:00 - and now let's see what happens
46:04 - so once again we've got a cool little
46:06 - table
46:07 - um that tells us was are we looking at
46:10 - the longer distance or shorter distance
46:12 - group and then what are our averages and
46:14 - here we do actually see a difference so
46:17 - we've got an average of about 16 minutes
46:21 - of delay if you are below that 88
46:25 - kilometer average
46:27 - compared to 34 minutes of delay if
46:30 - you're above it so that's about twice
46:33 - um and a similar pattern for the
46:34 - departure delay so it does look like our
46:37 - hypothesis may be correct that
46:40 - if you're going for longer distance more
46:42 - things can go wrong you know you're more
46:45 - likely to potentially arrive
46:47 - um later or depart later of course
46:51 - arrival and departure delays are a
46:52 - little bit linked because if you arrive
46:53 - late you're probably departing late
46:56 - um so it's not surprising that we'd see
46:57 - similar patterns across those two
47:01 - um again
47:02 - there's statistical techniques that we
47:04 - could use to try to verify that this is
47:06 - the case
47:08 - um but
47:09 - uh we won't dig into that today
47:11 - um because we're coming I think pretty
47:12 - close to the end of the of the live
47:15 - stream
47:17 - um let's take a real quick look though
47:20 - at the impact of weather because I think
47:22 - this one's pretty interesting
47:25 - um so what we're going to do is import
47:31 - some weather data about each ride now
47:35 - because this is really similar to stuff
47:37 - we've been doing I'm just going to add
47:40 - the code in to import this weather data
47:43 - set
47:44 - um so we have
47:45 - weather data stored again in a CSV file
47:48 - we're going to import it to a weather
47:50 - data frame
47:53 - um and so you can see for each ride we
47:56 - know some stuff about the wind we know
47:57 - some stuff about the weather and we know
47:59 - some stuff about the temperature
48:02 - just for Best Practices we'll do the
48:05 - same sort of data cleaning steps that we
48:08 - did on the other data set which is to
48:11 - uppercase all our text to make sure we
48:12 - don't have any white space handle any
48:14 - incorrect data types
48:18 - um I'm just going to run that code
48:22 - um because we went over it in in detail
48:24 - before it's the exact same code
48:26 - um so we can see now we've got all our
48:28 - text is is now uniform our temperature
48:31 - is being displayed as integers
48:33 - um and it actually is integers in the
48:35 - original data set
48:37 - and so now what we want to do is we want
48:40 - to take this weather data and we want to
48:42 - take our Railway data we want to stick
48:43 - them together
48:45 - um and the way we're going to do this is
48:47 - that both of them have this ride ID
48:50 - column and so we're just going to take
48:53 - each row from the railway data frame
48:58 - and we're going to go over to the
48:59 - weather data frame and we're going to
49:00 - find that right ID and we're going to
49:03 - grab that weather data
49:05 - um so the Syntax for this is a little
49:07 - bit uh finicky
49:09 - but
49:10 - I'm gonna do my best
49:13 - so I'm going to call this Railway delays
49:16 - join weather
49:20 - um and the method in pandas for doing
49:23 - that sort of look up for saying oh I'm
49:25 - going to take the right ID over here in
49:27 - this data set and I'm going to look for
49:28 - the same right ID in the other data set
49:30 - the Syntax for that is it's called a
49:32 - merge
49:34 - um and so what we have to do is we have
49:36 - to give our two data sets
49:38 - so the first one is our Railway I cannot
49:42 - spell delays correctly in this live
49:44 - stream I don't know why
49:46 - and then we the next data frame is
49:49 - weather
49:51 - um and then we tell it what's the column
49:53 - to do the lookups
49:55 - so we're going to be looking up ride ID
49:57 - in both to try and match that data set
49:59 - that data
50:02 - um
50:03 - and we're going to use what's called an
50:06 - inner merge that just means we're only
50:07 - going to keep things where we have data
50:09 - in both data sets which merge to use is
50:12 - kind of a it's a complex question
50:15 - um that you can learn more about in the
50:17 - course
50:18 - um so let's make sure this worked let's
50:21 - take a quick look at the
50:29 - preview oh I'm so excited
50:33 - when things work
50:35 - um so yes we've now got weather data and
50:38 - trained data all in the same data set
50:41 - and so we can produce our very last
50:43 - thing before the end of this
50:46 - uh live stream
50:48 - so what we're going to do
50:50 - is grab Railway delays join weather so
50:53 - this is our big data set that has all of
50:55 - our data
50:56 - and we're going to do the same thing
50:58 - we're going to group by what type of
51:01 - weather it is so we're going to split
51:03 - into groups is it sunny is it snowy is
51:06 - there a blizzard
51:09 - um and we're going to again calculate
51:12 - the average
51:14 - this is the one thing I'll say for for
51:16 - pandas is as you add all these methods
51:19 - on it can get a little bit finicky but
51:22 - no worry
51:24 - um
51:25 - it just gets long
51:28 - we're going to calculate our average
51:30 - arrival delay and departure delays
51:35 - and then if I just did this we would get
51:38 - an invalid syntax note ah so dot AG is
51:42 - what's called a method that means it
51:44 - takes input all of this stuff is the
51:47 - input to dot AG it tells the aggregation
51:50 - what to calculate and so that means it
51:52 - needs parentheses
51:54 - so if I do just this we've got sort of
51:58 - everything's kind of out of order right
51:59 - we don't we don't know
52:01 - um what was the biggest delay or the
52:04 - shortest DeLay So I am also going to
52:07 - sort this
52:12 - um bye
52:18 - and save myself some room by entering
52:21 - I'm going to sort this by the arrival
52:23 - delay
52:26 - and I'm going to sort it
52:29 - um descending so from biggest to delay
52:31 - to smallest delay
52:34 - and there we have it
52:36 - um it does look like weather impacts
52:38 - this
52:39 - um blizzards are the biggest delay
52:43 - um Sunny is sort of near the middle
52:44 - which is kind of interesting and then we
52:47 - have actually things like downpour
52:50 - actually have the shortest delays
52:53 - um which looks a little bit confusing
52:56 - it's not exactly what we'd expect now
52:58 - maybe there's some things some other
52:59 - things going on impacting the data one
53:02 - thing I'll show which is the very last
53:03 - thing we'll do
53:05 - is that um we can take a look at the
53:11 - number of data points for each weather
53:13 - type
53:14 - so I'm adding in I'm counting how many
53:17 - rides are in each data type
53:20 - and if I do that I can see that some of
53:23 - these
53:24 - down here have less data so the fact
53:27 - that moderate to heavy rain has very
53:30 - little delay whereas if you look all the
53:33 - way up here moderate rain has more delay
53:35 - well that might be because we only have
53:37 - 30 rides here and our data set is just
53:40 - January so that restricts the weather
53:44 - um so that's all just to show you you
53:46 - can get some pretty quick insights from
53:48 - pandas and there's also so much more
53:50 - that you could dig into
53:52 - um the whole data set is also available
53:54 - from this project
53:56 - um so that has data for all of the other
53:57 - months
53:59 - um and that's the end of the project
54:02 - um
54:04 - I guess I will turn it over to
54:07 - Tori or Fede if there are questions
54:09 - that's me
54:12 - yeah hey thank you I'm back hello
54:15 - everybody
54:16 - uh yeah thank you for stopping sharing
54:19 - the screen that way everybody can see us
54:21 - both so we are in the Q a section of the
54:25 - live stream now so I just posted in the
54:26 - chat for everybody to go ahead start
54:29 - dropping your questions for ARA
54:31 - and I'm going to start with one of my
54:34 - own while we wait for everybody to chime
54:36 - in I wanted to ask you I know that data
54:39 - science is a very bad vast feel and it
54:43 - can be very overwhelming for beginners
54:45 - that are just trying maybe are curious
54:46 - about data but really don't know much
54:48 - about it so how do you feel is a good
54:50 - way to go about getting to the field
54:52 - learning about it and doing it in a way
54:55 - that feels like at a good Pace not super
54:58 - overwhelming because there's so many
54:59 - tools there's so many ways there's so
55:01 - many things right
55:02 - yeah there's there's so much
55:05 - um it can definitely be overwhelming and
55:07 - and there's so many things you need to
55:09 - learn sort of or it might feel you need
55:11 - to learn in order right
55:12 - um before you get to say if you want to
55:14 - do machine learning or you want to do
55:16 - something else
55:17 - um I think picking sort of one
55:21 - one programming language one sort of
55:24 - stack of tools is one way to start and
55:28 - to say hey I'm going to do python pandas
55:31 - right I'm going to like that's what I'm
55:33 - going to learn I'm not going to worry
55:34 - about should I be learning R should I be
55:36 - learning Julia I'm going to pick one
55:39 - um and then the other thing I would say
55:41 - is
55:43 - to
55:45 - um
55:46 - you know focus on how
55:51 - you can
55:52 - get actionable sort of Data Insights
55:56 - from data sets without doing things that
55:58 - are too fancy so sometimes it can feel
56:00 - like oh but I'm not doing I'm not
56:02 - creating a neural network like I'm not
56:05 - creating uh you know a fancy algorithm
56:09 - um but a lot of the times the the
56:11 - day-to-day of working in data science is
56:13 - creating visualizations
56:16 - doing exploratory statistics doing that
56:18 - kind of data cleaning
56:20 - um and you can
56:21 - be pretty effective as a data analyst
56:24 - without knowing all the fancy stuff um
56:27 - so I guess don't be afraid to just
56:29 - start just jump in with data sets yeah
56:32 - just do it okay oh yeah
56:34 - just just going there work with the data
56:37 - I guess uh looking at the chat uh we
56:39 - have Richard asking how much time does
56:42 - it take on average to become very good
56:44 - as a data scientist who I mean it's a
56:48 - really tricky question because what does
56:49 - very good mean and like you know how
56:52 - much time are you devoting to it
56:55 - um you know I think
57:00 - yeah
57:01 - if you're sort of if you're working hard
57:04 - and you're focusing on it you know I
57:06 - don't know you're doing
57:08 - practicing an hour a day or something
57:12 - um you know you could get reasonably
57:14 - effective
57:15 - um if you're pretty focused in you know
57:19 - three months six months like the nice
57:21 - thing like we were talking about before
57:22 - is that there's like early milestones in
57:24 - data science so you know you could take
57:27 - a data visualization course and be able
57:28 - to produce meaningful visualizations
57:31 - within you know one or two months
57:33 - obviously becoming a very good and you
57:36 - know attaining that level is going to
57:37 - take more time
57:38 - um I don't know that I would call myself
57:40 - very good and I feel like I've been
57:41 - doing data science for a while so it's
57:42 - all kind of it's kind of a hard question
57:44 - to answer I feel like that happens to
57:46 - all the experts the more you learn about
57:48 - a field the less confident you feel
57:50 - about it like you go in very strong and
57:52 - then the more you know it's like the
57:54 - more you know that you don't know
57:57 - yeah so we have a related question in
58:00 - the chat from Matthew also asking about
58:03 - the influx of tech jobs seekers in the
58:05 - current market and wondering what are
58:07 - the things that they can do Beyond code
58:10 - academy to stand out and to kind of like
58:13 - be better or more competitive in the job
58:15 - market so what are some of maybe what
58:18 - are some of the tools like sites or
58:19 - things that people can do to go out
58:21 - there and explore more things related to
58:24 - data
58:25 - yeah so I think that
58:28 - um
58:28 - there's a couple different
58:31 - there's a couple different things that
58:33 - you know employers and other other folks
58:35 - are looking for Beyond sort of just
58:37 - knowing the code right so you're on code
58:39 - academy you learn how to do the code now
58:40 - what
58:42 - um and some of that is can you work on a
58:44 - team right
58:46 - um can you
58:49 - um you know can you not only sort of
58:51 - analyze a data set but do that extra bit
58:53 - of thinking of like oh here's what the
58:55 - business needs
58:57 - um and here's how I can go about trying
58:58 - to answer that question sort of that
59:01 - kind of work
59:02 - um unfortunately that's hard to
59:03 - demonstrate without a job I'm sure
59:05 - that's what people are thinking right
59:06 - now as I'm saying that they're like okay
59:07 - that's very nice but I don't have a data
59:09 - science job
59:11 - um you know if you can find people to do
59:14 - a group project with that's a great
59:16 - strategy
59:18 - um building up that portfolio as you
59:20 - know is always really huge
59:22 - um it's difficult to find open source
59:25 - projects on GitHub
59:27 - um but you know doing that kind of work
59:29 - to demonstrate that you can fit in with
59:34 - a group of people and contribute to a
59:36 - larger goal that's really important
59:37 - quick plug for codecademy docs where you
59:40 - can get started with GitHub
59:41 - contributions
59:43 - um right here nice another question here
59:46 - is about getting discouraged by errors
59:49 - and I think that this is probably very
59:51 - common in data especially because you
59:52 - run into a lot of trial and errors so do
59:54 - you have any advice for people that
59:57 - might you know feel like oh it's just so
59:59 - much that is always a bug something that
60:01 - doesn't run the results that don't work
60:03 - the way that you're expecting
60:04 - it's so frustrating so first of all I
60:06 - just want to say like I'm going to give
60:09 - like a very nice answer and all this
60:10 - sort of stuff but also just know that I
60:12 - have sat there with my vs code or
60:14 - Jupiter notebook open trying to tear my
60:16 - hair out um so I understand
60:19 - um you know the thing about errors is
60:21 - the the cliche is it's always an
60:23 - opportunity to learn
60:25 - um but it's really true so if there's an
60:27 - error and you don't know why it's
60:28 - happening right what that means is
60:30 - there's something you don't understand
60:31 - maybe it's something you don't
60:32 - understand about the data set right so
60:34 - maybe it's not a syntax error maybe it's
60:36 - just something's not coming out
60:37 - correctly
60:39 - um that means that's that's a sign right
60:42 - you don't know something about the data
60:43 - set or you don't know something about
60:45 - one of the methods that you're using and
60:47 - as data scientists right it's our
60:48 - responsibility to make sure that we know
60:51 - what's going on right so that when we
60:54 - get an answer it's not just like oh
60:56 - pandas told me that this was the average
60:59 - well I know how it's doing that
61:01 - computation and I know that really
61:03 - deeply so it's super frustrating but the
61:06 - errors are there to help us understand
61:08 - like oh I need to like learn a little
61:11 - bit more about this or I need to look
61:12 - into it a little bit more deeply it
61:14 - sounds like it comes to the territory
61:15 - yeah it just comes with the territory
61:17 - and like don't be embarrassed or ashamed
61:19 - like even I mean you saw I did some
61:21 - really silly stuff in this live stream
61:23 - um and like the reason I was able to
61:25 - correct those errors so quickly is that
61:27 - like
61:28 - seven years ago or however long ago when
61:31 - I was first starting out
61:33 - um I spent a lot of time being
61:34 - frustrated by those errors so it comes
61:37 - with time
61:38 - um
61:39 - yeah just be patient and uh don't be
61:42 - afraid to ask for help uh fair enough
61:44 - maybe on the Discord I'm going to yeah
61:47 - exactly go to the community so I'm going
61:49 - to combine these last two questions into
61:52 - one uh because both users are basically
61:54 - asking about the same thing and it's
61:55 - related to I guess how the field divides
61:59 - uh Specialties and things like there's
62:02 - somebody here asking you know is it
62:04 - better to go into data analytics before
62:05 - going to data science and there's
62:07 - another user asking what is the kind of
62:09 - data science specialization that you
62:11 - will you know you think that is the one
62:13 - that has the most Futures the one that
62:14 - is you know the better one to go into
62:16 - when you're trying to break into data
62:18 - science so I guess the question is you
62:20 - know out of all these you know
62:22 - Specialties within data science what uh
62:24 - how do you see beginners going into it
62:28 - yeah it's a really complicated world
62:30 - just because data science is still a
62:32 - relatively young like field right which
62:35 - statisticians by the way will be very
62:37 - mad at me for saying that because
62:38 - they're like oh it's just statistics
62:39 - we've been doing this forever but you
62:41 - know data science
62:43 - with the Advent of big data right which
62:45 - it wasn't even that long ago
62:47 - um was when data science really became a
62:49 - field and
62:51 - um you know so we're still figuring this
62:53 - out as an industry
62:55 - um I'll say that you know I think
62:56 - oftentimes people find the easiest way
62:58 - to break in is in the like business
63:01 - intelligence area
63:03 - um so that's someone who's mostly doing
63:06 - working in like SQL with databases or
63:09 - Excel or Tableau
63:11 - um you know trying to
63:13 - answer business questions and those
63:16 - sorts of things
63:17 - um often not doing as much in the form
63:20 - of you know sort of more advanced
63:21 - statistics or more advanced machine
63:23 - learning but I I feel like sometimes
63:25 - people look down on it but it's a it's
63:27 - it's a really important field and
63:29 - there's a lot of knowledge and expertise
63:30 - that goes into it but it is sometimes a
63:32 - little easier to break into because
63:35 - um sometimes you don't need to learn as
63:36 - much code
63:37 - um or as as much Advanced statistics and
63:40 - then you have the job and you can learn
63:41 - on the job
63:42 - um not as easy to find a job ever but
63:46 - um you know that's that's sort of what
63:47 - what I'm aware of in terms of where the
63:50 - field is going with Specialties
63:51 - obviously you know AI
63:54 - the AI machine learning landscape has
63:56 - been really shaken up over the last you
63:59 - know year I don't know if you've heard
64:01 - there's some you know each new products
64:02 - out there
64:04 - um I
64:05 - I
64:07 - some of it to be honest comes down to
64:09 - what you like to do I certainly think
64:10 - machine learning is going to be an Ever
64:13 - sort of growing field
64:15 - um you know it's also one that requires
64:17 - a lot of technical expertise
64:19 - um at least to do well
64:21 - um and so it is something that maybe you
64:23 - know you could learn on the job while
64:24 - doing more analytics focused stuff
64:26 - um and then there's the more statistic
64:28 - side of things
64:30 - um so you hear people talk about
64:31 - inference or causality
64:34 - um and those are people who are you know
64:36 - maybe a little bit more research focused
64:38 - a little more academic
64:40 - um trying to do
64:42 - um you know more advanced statistical
64:43 - analyzes
64:45 - um but uh a lot of data scientists will
64:48 - do a little bit of all of this right so
64:50 - many people with data science you know
64:53 - as opposed to like machine learning
64:54 - specific job titles will work with
64:56 - machine learning
64:57 - um yeah it's it's interesting how the
65:00 - landscape changes right like a few years
65:02 - ago all the rates was machine learning
65:03 - for self-driving cars everybody wanted
65:05 - to get a job and oh how to make a car
65:07 - drive itself now everybody likes large
65:09 - language models and AI so it's still
65:12 - kind of like in the same field but it
65:13 - feels like a totally different thing
65:15 - right like people don't talk about that
65:16 - they all the one too much now but they
65:18 - you know so I definitely get that uh
65:21 - that sometimes there's a big swings in
65:22 - the market it's hard to know or you know
65:24 - if I'm learning now what is going to be
65:26 - you know hot in 12 months right when I'm
65:30 - ready to apply for jobs so it's pretty
65:32 - pretty challenging thing well thank you
65:34 - so much ARA for stopping by today and
65:36 - sharing your knowledge and the project
65:37 - with everybody I think it was awesome
65:40 - everybody really enjoyed it thank you to
65:42 - everybody that attended their live
65:43 - stream today I hope that you enjoyed it
65:46 - it was a good experience for you if you
65:48 - want to get more content like this if
65:49 - you want to get into it don't forget
65:51 - that you all you need is an email
65:52 - account to have a code academy account
65:54 - getting to those courses try them for
65:57 - yourself hope that you like it we put a
65:59 - lot of work and effort into the
66:00 - curriculum that we have at Academy we
66:02 - are very proud of it so hopefully you
66:04 - enjoyed it as much as we do making it
66:07 - also please don't forget to subscribe at
66:09 - YouTube follow us social media that's
66:11 - where we post a lot of good content we
66:12 - have an incredible blog post LinkedIn
66:15 - Facebook Twitter whatever you find us
66:16 - and also don't forget that we have a
66:19 - live stream for our third part of the
66:22 - three series of the evolution of code
66:24 - academy next Thursday same time 1 p.m
66:27 - Eastern and we'll see you all in our
66:30 - next live stream thank you all again
66:31 - thank you everybody and have a good
66:33 - weekend bye

Cleaned transcript:

and we're live hey hi everyone welcome back to our live stream uh this is the second in our threepart journey through codecademy's Evolution and today's topic is a fan favorite you know it you love it it's python for data science I'm Corey Steve Cook Academy's content marketing manager and I'm joined by fetty Garcia Lorca Lorca our community manager who you probably recognize from the forums and Discord as well as Dr Autumn Morse a senior instructional designer in data science who will be walking us through this project step by step and then over in the chat we have Eva savinga a senior curriculum developer in data science who's on hand to answer all of your questions uh in case you missed last week's live stream we're celebrating codecademy's birthday all month long with events and new course launches and lots more next week we're actually going to give you a look at our new course intro to generative AI so be sure to RSV for that live stream at the link in the YouTube description below and we'll see you there um today though Anna is diving into a project from our path Learn Python for data science python is the most popular programming language in our catalog and I'm actually curious how many of you here have taken one of our python courses please let us know in the chat um python is a general purpose language it's used for everything for data analysis to mathematical Computing to machine learning and web development in Anna's demo you'll get to see just how easy to read python syntax is and you'll also see how we work with Jupiter notebook a workspace for data science code and visualizations that's built right in to our data science courses and paths uh keep in mind that you can take this path whenever you want on your own time and it's linked in the description below and this live stream will be available to replay if you get stuck or need hints um before we get into it I want to tell you a little bit more about ADA Dr Autumn Morse has worked on courses across our data science catalog covering topics like python of course Excel data engineering and most recently Ai and prompt engineering she comes from a mathematics background and has a PHD focused on the design of selfassembling DNA nanostructures Ada has also worked as an Actuarial systems analyst she's taught math and statistics at Champlain College and she's conducted research supported by NASA and the Vermont space Grant Consortium Hi Ada we're so excited to do this I wanted to not like bumped this up and ask a question that a lot of people have uh what makes python so great for data science it's a great question um python is first of all a great language in general for beginners because it was designed to match sort of natural spoken English so that it sort of Narrows that distance between you know what you want to do and now how do you write the code to make the computer Do It um so that just makes it a really good choice for beginners in general and then in terms of data science um there's just been such a robust data science Community working in Python so um there's lots of packages of prewritten code so you don't need to sort of start from scratch with all of your data analytics um there's lots of people to ask questions and there's lots of courses available um and data scientists in general use Python so if you want to do data science at some point you're going to have to read python code and work in Python code so why not learn it totally all right take us away Ada sure um let me share my screen and here we go so we're going to be analyzing some highspeed Railway data in Python um light Corey said this comes um sorry um we're going to analyze a real data set from a highspeed rail network in China um so like Corey said this project is part of our new Learn Python for data science curriculum um and I'll show you some of what we just talked about in terms of how python is relatively easy to understand and Jupiter notebook is a pretty good workspace for doing data science um you might be wondering why we created um a new course about python when there's so many available and the reason is we really wanted to develop a curriculum specifically for data science um I don't know about you but I often want to learn something new around data science and then I'm stuck with you know weeks or months of learning sort of basic syntax and not getting to like build the thing I actually wanted to build so this teaches you python from scratch while you're actually analyzing real data sets about technology and sports and uh 11 or 12 others so I won't list them all because we want to get into the code so let's switch over to the code academy learning environment and take a look at this data set um so this is a real data set um I will just say as a disclaimer we made a few alterations to the data set structure so that it would make sense in the course and so that um you know you all could practice specific techniques with it um but we didn't modify any of the data so it's all real data about a highspeed rail network in China we can actually take a look at the raw data set um in Jupiter by clicking the Jupiter icon we have all our data sets in this data set folders you can see we have a whole bunch of different files um and they're all they all have this dot CSV extension which stands for comma separated values so if we take a look at what this data set looks like you'll see we've got a bunch of data in here it's not particularly easy to read um as as human beings but in our first row we've got what are column headers so we've got dates we've got ride IDs if we go down we have info like mileage and different delay statistics um and these are all separated with commas which is what that comma separated values means so we're not going to work with these files on their own because that would be a little bit difficult to do instead we're going to read them into python using a package called pandas and work with them there so let's go back to our notebook um so I've got the first little bit of code already loaded here um this first line import pandas as PD we're going to work with the data science package called pandas in this project um like I said at the beginning one of the advantages of python for data science is that we have all these packages already written and they just give us a bunch of code for standard tasks so that we don't have to rate them ourselves and can get right into Data analysis and because I don't like typing things I'm telling python here that I'm going to call pandas PD so I only have to type two letters instead of six okay um now that we have pandas imported what we can do is we can use functionality that's already been coded so pandas has a read CSV method so that's that same CSV comma separated values that's the file that we were looking at and so this is a method to read that data in and then structure it really nicely in a table called a data frame ah session Timeout on the live stream okay disaster averted everybody um it's just live stream luck so like I said it pulls in that data formats it nicely on a table so we can read it much more easily um and I always like after I read a data set in to just go through all the columns and make sure I understand what's going on so we have the date column um it looks like these are all going to be from January 1st 2020. if you look at the file name of the data set it kind of gives a clue that the file name of the data set contains that date um we have a ride ID a train number and a station order um the eagleeyed might notice that the ride ID is a composite column so we start off with our train number then we've got our date and then we've got the station order and so what's happening here is we're actually tracking a train as it goes from Station to Station along its route um so each row is an individual trip from one station to another station with no breaks in between um and then we're following the same train g1226 as it goes from Station to Station um we have our station name data we also have mileage um which is actually in kilometers so maybe this column should be called kilometer Edge um and then we have delay data so arrival delay and departure delay um you might notice there's some negative numbers which is kind of fun so those are early arrivals um or early departures so this first trip it arrived on time and it departed early um so we'll dig into more of that those details as we go along our last column is major holiday this looks like it's just a Boolean column so Boolean means it's true or false and it just tells us was this on a major holiday um or was it not on a major holiday because that might impact um how the trains work all right um so one thing we have built into this for the course just in case you're taking the course who you know are these toggles where we do basically what I just did and go through the data set and let you know what's going on um I'm not going to look through those in the live stream so our first task whenever we're working with data is to read data into the file um read data into into pandas rather from the file and make sure it's all nice and clean um this is something that's often sort of missing from data science classes my own personal pet peeve a lot of data science is getting data and then structuring it so that we can actually work with it and so this project comes from a section of the course where we're teaching you how to do that so we're actually going to get our hands a little bit dirty here with the raw data and cleaning it up nicely so that we can then get to some you know cool insights later on in the project um so when we looked at the data sets there were a whole bunch of different files and there's one file for each day in January this is a pretty typical structure for data where you know if reports are being generated on a daily or a weekly or a monthly basis um each day week or month is stored in its own file now we want to read in all of those files right we want all of the data that we have and we don't want to do that by hand because there's um actually 27 files so I don't want to type the same code 27 times fortunately python is a robust programming language so we won't have to do that um so what we're going to do instead is we're going to create a variable file names and this is going to store a list of all of the files that we want to read in um and before I I write that just a quick note for anyone who's like the total beginner in Python it can be really easy to get hung up on like do I understand all the syntax that's going on I'll explain everything along the way but also feel free to just relax and see this as an opportunity of seeing like what python can do um you know I wouldn't be too worried if you're like I don't understand the every single tiny bit of bit of syntax um so what I want to do is I want to build a list of all of the file names um and again I don't like typing so I'm gonna I'm gonna try and make this as as short as possible for me so the structure of the file name is pretty much the same for each file in that what we have is we have our data sets folder and then we have the railway delays 2020 and January and so the only thing that's changing for each file is the day um so we have data from January 1st through January 27th so if I wanted January 2nd I would change that to be a two or January 3rd I'd change that to be a three um and so what we're going to do is we're going to use Python to do that change for us automatically um first I just want to show you how I'm going to break this apart for a single file name so what I would do is I'd say okay all of that um at the beginning is going to be the same for every file so I'm going to leave that in its own string and then I'm going to separate off the piece that is changing so the piece that is changing is this number one that's going to change to a two to a 3 to a four and what this plus sign is doing is it's telling python take all these individual pieces and just squish them together the fancy word is concatenate but I'm not so fancy so let's just say we're going to squish them together and if I run this um nothing will happen because I haven't told python to print anything out but if I run it telling Jupiter notebook actually in this case to display that variable at the end you'll see that python has done that sort of concatenation or squishing together we've got our one in there now I just need this to update for every single um file and so the way I do that is introducing another variable so instead of this one I want that to just be whatever the day is right whether it's the first or the second or the third it's going to be a variable that stores the day and then I need to tell python what are the different values that the day can be so the key word here is the word for just because that's what python uses so four day in and then there's this range syntax that lets me specify a range now the funky thing is for basic python is that I want to go up to January 27th that's the data I have the way this range thing works you actually give it the next value 28 I cannot tell you the number of times I've done this wrong just by not thinking about it don't stress too much um it's a pretty common issue to run into and so now if I run this what I just realized is I'm going to get an error let's chat about that um so when I run this I get an error called a type error and if I read down to the description it says you can only concatenate Str not int to Str a little bit confusing but Str stands for string that's like a bunch of text maybe with some numbers like the file name int is an integer and so what's happened here is my day is an integer and python is saying I'm just a computer I can't take a string which I think of as being a bunch of text and I don't know how to add an integer to that so to fix this I need to tell python that day should actually be a string again this is something that's easy to forget I just forgot it um but fortunately the error message was pretty helpful and now if we run this we've got all our file names in a beautiful list um I don't know even now I find it satisfying what can I say um now there are some fancy packages that can sort of shorten the amount of work you need to do to find like all the CSV files in a certain directory some of you might have encountered something called glob or something similar I think one of the cool things about python or any programming languages once you know the basics you can do a lot of things just by being creative um but certainly you can also look for special packages that might might help things out in the meantime we've got all our file names which is pretty exciting and now we just need to read those in as um data fribs so let's give that a shot so again I'm going to Define a variable the variable this is just a name that's going to store all of these data sets because we're going to have to read them in one by one as individual data sets now let's just grab the code from above um that read the data set in so this is the read CSV function now this would just read in a single file right but what I want to do is I want to read in a list so what I'm going to do is I'm going to use these square brackets again to say I'm reading in a list now what goes in here inside the read CSV function is a file name and I want that file name to be changing right I don't want it to be just this one file name so I'm going to do the same trick that I did before to generate the file names and I'm just going to say this is a file name I don't know what it is but it's a file name and once again I need to tell python what are the file names what are the different names of all of my files and that's why I created that file name list so again I can tell python what values to sort of put in for the file name using the word for file name in file names now file names here that's the list that we just defined that has all of our file names and then the file name on its own this is just the variable that's going to be read into all of these different um data sets or they're called Data frames in pandas once we've read the csvn the structure is called a data frame so we can't preview all of them right now but we can do a couple checks to make sure things are looking good so what we can do is we can print out the very first data frame so what I've done is I've taken this variable data Railway delays DFS this is where we're storing all of those data frames the square brackets and the zero says get the first one again you'll remember from before with 28 and 27 python starts counting at zero really fun and really annoying but zero says get the first one and then pandas has this helpful head method which will display the first five rows or so of the data frame so let's run that great our first data frame loaded correctly looks good um just to be certain that we've got 27 of these I'm going to also check the length so python has this function Len which will output the length of a list and beautiful we have 27. now that doesn't actually mean that they all were read incorrectly but I feel I feel confident enough that I can move on to the next step now that I know we've got 27 data sets probably all 27 read in correctly but we still have 27 separate data frames and we want one data frame we want one data set that contains all our data right if we look at our first data set that we pulled in here right this was all January 1st now what I want is I want to Stack this and there's a bunch more rows under this this is just the first five rows I want to Stack this above January seconds data above January thirds data to get all of them in a single data set once again the great thing about pandas is someone has already written this code for us so all we need to do is write the code to call that pandas method so I'm going to create a new variable to store our full data frame um I've created a typo fix that and I'm going to call pandas using PD and now the method is dot concat this is short for concatenate just like we were squishing strings together earlier now we're going to squish data sets together um and what are the data sets I'm going to concatenate well the ones I just imported using read CSV and now um what I would like to do so that works nicely but I would like to print out some information about this um this new new data frame to make sure everything worked out correctly um and the way I'm going to get this information again is the pandasmethod.info this is going to tell me things like what are all the columns are there any missing values all of that kind of exciting information um so the first thing I see in my output is I've got 29 000 total entries um and then in this First Column it looks like I've got 28 000 nonnull so those don't quite match so it looks like there might be some missing data but it actually looks like we just have some other data Integrity issue going on here because these columns are all lower case and then I have a second set of the same columns that are uppercase and then I have this third copy of the station name column which is lowercase but is somehow different from the lowercase version that's above so I think we've got just a little data Integrity issue chances are at some point the way the data sets were encoded or created changed from lowercase to uppercase or from uppercase to lowercase um and for this last station name issue my guess would be that there's some sneaky white space so there's actually like a space before the text or after the text that's causing pandas to read it indifferently so in order to actually get to our analysis we need to fix these problems um so what we're going to do is we're going to go back to our list of all of our data frames and just quickly correct all of the column um column headers make sure that they're all lower case make sure that there's no extra white space the fun thing is we don't actually have to look at any of the data frames on their own we can just have python check and fix them all so what I'm going to do is I'm going to write what's called a for Loop so a for Loop we've seen this a little bit before as we created these file names this starts with the keyword for and it's going to tell python keep doing something over and over and over and like we did before I'm going to do this over all of our data sets so we've read all our data sets in and I'm going to look at every single data frame in that list of data frames DF again it's just sort of the uh the standard shorthand for data frame so what this is telling python is take every single data set that you read in and do something with it and now let's specify what we want to do the first thing I want to do is I want to lower case column names so in pandas we can access the columns using this dot columns attribute and that will return all of the column names and now I want to change the value of the column names right I want to take those columns and I want to lower case them so again I'm going to access the columns and now I'm going to use Str for string this will tell pandas I'm about to do something involving text right so uppercasing lowercasing those are all things that involve text so dot Str says hey text a method is about to happen and now it's very well named it's just lower um to lowercase them the other thing I want to do is I want to remove white space so this is going to be really similar I'm going to access the columns and I'm going to say we're going to modify this using a string or text method um and this one is called strip and that's just going to get rid of any white space that we don't want and now once we've done that each of our individual data frames should have the right column names but we'll double check by putting them all back together again and making sure that everything looks correct um so I'm just going to take our code for putting all of the data frames together and we are going to run that again after we've changed the column names make sure that everything looks good beautiful so now we've got just one set of column titles and we've got the right number of entries so we have a total of 29 000 entries in this data set that's rows in the data set and each of our columns has that number of values so that doesn't necessarily mean there's no missing data there could be something sneaky like if someone put in unknown or something like that as a value but at least there's no null data which means there's just nothing for that entry at all all right so I told you a lot of data science is data cleaning and it's true I promise we're going to get to some analytics in just a minute um but let's go through all of the best practices just to make sure that um we're doing things right so the last thing we're going to do is we're going to do some quick cleaning items on the the new full data frame um a couple of these are just good practices to follow so we're going to make sure that all of the text columns are are have the same case this is a really frequent thing that gets encoded incorrectly and we're going to strip any white space um from them because we want to make sure that that's not going to impact our analysis and those are pretty hard to spot in you know 29 000 rows so let's just take care of all of them um and then the last thing we're going to do is we're going to deal with something involving the numbers um the the numeric data and I'll talk about that in a moment um because I lost my connection I'm not sure what's going on there but let's work on this cleaning so once again I don't want to have to do all of this by hand so I'm going to write a for Loop the for Loop is going to tell python do this to all of my columns so I want to take each column of the data frame and I want python to do some cleaning on each column so remember I can access the columns using dot columns so I can say take each column in the list of columns and now we're going to do stuff with it so the first thing I want to do is I want to uppercase all text just to make sure everything's in the same case obviously not every column is a text column so we're going to have to deal with that as well but the code for actually uppercasing it is pretty straightforward right we would do the same thing we would access the column and then we're going to use that same Str dot in this case upper syntax so the first thing I need to do is correct my variable the actual data frame that we're working off of is our full data frame with all of the data from every single day and so now I'm going to take that data frame and I'm going to access the column these square brackets are just what we use to access columns or rows or specific sets of data within a data frame and then I'm going to say okay what do I want this column to be well I want it to be the same column right so I have this equal sign I want it to be the same column I just want everything to be uppercase now you might wonder why uppercase um no specific reason we just want them all to be the same and the other thing I want to do is I want to strip the text strip any white space so it's pretty much the same syntax so I'm going to copy and paste because that's how we get things done and I'm going to use dot strip now if I run this now I think we're going to get an error let's see we're gonna get an error phew um so the error here is an attribute error and what attribute is doing is it's referring to that str.upper and if you scroll all the way down it's going to tell you you can't use this string accessor so we can't use upper casing on things that aren't strings right on things that aren't text so what we're doing here is we're going over every single column which includes numeric columns and we are trying to upper case a number and python is saying I don't know how to uppercase a number so we need to fix this really quickly you know there's a few different ways to do this if you're super fancy pandas has a builtin like select D types method um for our purposes we can just check what the type is of the column before we actually call any of this code um so we do this in Python using something called if else syntax so this allows us to only execute code if we meet a particular requirement so in this case the requirement we want to meet is that the data type so dot dtype is the data type of the column is an object um object is how pandas stores test so that's the the technical name for it if that happens then we will perform this code if you're new to python the reason I just added a tab there is because python figures out what code belongs where by indentation so because I have indented here um that means that python now knows that all of this string code belongs to this statement and so it'll only try to uppercase things that are actually um are actually text we're really running into the live stream curse here of disconnection um all right I think that's fixed so now it should run and it did run and let's just double check that everything's looking good so I'm just going to print out the first five rows Railway delays boom ah I have a typo beautiful it's always hard to spot those typos I wrote Railways not Railway come on Otto get it together great everything's uppercase that's beautiful we haven't done anything to our numbers now the only other thing I notice in terms of cleaning is that these numbers have decimal points so we have like 1.0 2.0 these should be integers um and it's actually the same with the rest of these numbers they're showing up as floats um float is the the technical term for something that has a decimal but they're actually integers now this doesn't cause too many problems um but when you're working with a lot of data integers are easier to store and there's certain things about how python implements arithmetic that might be different um so we'd like to correct all of those um now we can do that just in the same code um by looking for only numeric data type columns and then changing that to integers so again in order to sort of zoom in on only the columns we're interested in what we can do is we can test the data type um so I'm going to copy and paste this code here so what I'm saying is take our data set access the column we're on check the data type and we're going to check if it's a float um there's some technical I should say there's some kind of technical things about testing numeric data types that are a little tricky uh that we don't have time to get to in this live stream for our purposes today this is this is good good enough um but um just something to be aware of so now if we've got a float which means we've got a decimal what I want to do is I want to take our column and what do I want I want it to be an integer so that just means it's a number that doesn't have that decimal point again this mostly has to do with um storage concerns and and how arithmetic is actually implemented than anything else now this is where it could all go wrong let's run this ah it all went right though we've got integers one two three four five instead of 1.0 2.0 3.0 storage concerns aside it just looks more elegant to me so I like it and we're finally done getting our data set together I know that's occasionally a little bit tedious but in all honesty this is a huge part of any data science job um so it's really important to know how to do um and it's also in my experience a part of a lot of data science interviews how would you clean this data set all right so let's look at some delay statistics um I'm actually going to just in the interest of time um we've got some sort of exploratory things what we're going to do is just start by looking at delays on holidays to see does it look like there's a significant difference in terms of delays on major holidays versus nonmajor holidays you know maybe more people are traveling maybe less people are traveling and maybe that impacts how well the trains run on time so um let's take our Railway delays full so this is our data set um and what we're going to do is we're going to ask python to group all of this data so in one group we're going to have all the major holiday rides in another group we're going to have all the other rides and then we're going to do calculations on those two groups so the way we do this in pandas is this method called Group by so split into groups and then we're telling it what do we want it to split into groups using we want the groups to be based on major holiday so what's going to happen from here is pandas will run through the data set and it'll look at each row and it'll say oh was this a major holiday or was it not and separate them into those groups based on that and then we can use that to perform a calculation so the calculation we're going to perform is we're going to calculate the average of our two delay columns um there's a bunch of different ways to do this just so you know so if you know a different way don't worry um I'm going to use a method called dot AG that stands for Aggregate and that's sort of a technical term for what we're doing when we take a bunch of data and compute something like the average um some people call that aggregation and what we need to feed into this aggregation method are the columns that we want to perform calculations on and the calculation we want to perform um so this takes the form of what's called a dictionary if you're not familiar with dictionaries don't worry about it too much basically what I do is I specify my column name arrival delay a colon and then I tell it what I want it to compute I wanted to compute an average which is also called a mean and so in pandas the the word used is mean and then I tell it hey I also want to look at the departure delay and I want to calculate the mean and now provided I've remembered all my syntax correctly we get a lovely table that breaks us down into two groups so major holiday we're broken down into it's not a major holiday or it is a major holiday and then we have both our arrival delay and our departure delay um and the thing I noticed right off the top is that these look really similar it doesn't look like there's much of a difference um 23.3 for arrival delay if it's uh not a major holiday 23.8 if it is that's not so much of a difference um we could certainly test using statistics to see if that that actually is statistically significant but I don't think anyone would really notice that difference in delay just like standing and waiting for the train so I feel like we can probably say chances are there's not that much of a difference now if we wanted to really verify things we might want to dig into this and say hey is are there fewer rides on holidays so do we not have the same robust quantity of data for both different groups right there's things we could do to examine this further but it looks like holidays don't change things too much for the highspeed rail network in China which I have to say is different than my experience here in the United States um another thing we could look at is the distance between stations so we could say hey if the if it's a longer trip more things could go wrong so there's more likelihood of delay um so to look into this the first thing I'm going to do is I'm going to take a look at the mileage column so again I need to access my data frame first oops and then using square brackets I can I can access a specific column in this case the mileage column and I'm going to use a builtin method in pandas called describe and this is going to tell me some statistics about the mileage column what I am especially interested in is the average so it looks like the average distance between stations is about 88 kilometers um and so what I'm going to do is I'm going to look at okay if we have above average versus below average length how does that impact the delays so it really similar to what we did with holidays where we said okay we want to group by whether it is or is it a major holiday we're going to do the same thing with whether it is or isn't a longer than average trip unfortunately in our data frame we don't have that data right if I do Railway delays full and I look at the first five rows with major holiday we had a column that told us this row belongs to a major holiday or it doesn't we don't have that in our data set um so we're going to have to add it to our data set um so this is one of the fun things about pandas is we can just add columns to our data set essentially willynilly um so I want to create a new column that's going to tell me is this a long distance versus a shorter distance trip now if I just ran this code now I would get an error because this isn't a column yet but I can still Define it to be a column and what I want the information I want is is the mileage value bigger than average or less than average so the first thing I need to do is I need to get that mileage value which I'll do the same way we've been doing it right I'll go into the data set and I'll access the mileage column and then I need to check is it greater than the average which is 88. and so what this code here will do is it will just check every single time is my mileage bigger than the average or not and let's just quickly preview to see how that works I swear I typed real way um amazing so we've got a long distance column now which I love um and all of these are false false false false false false which is good because all of these rows the mileage is below 88. let's actually preview a few more rows and see if we can grab some that are above 88 so I can add this sort of special parameter or argument to my preview code to get 10 rows instead of five and yes I've got a true here for long distance and it's awesome because that's 263 kilometers that's what I want to be happening um that's bigger than 88 that is a long distance trip so I'm very happy about that and now we can do exactly the same thing we did with major holiday we're going to split this data set into two groups is it long distance or short distance and we're going to calculate some averages um so I'm going to cheat and copy and paste the code because Y type more things that are absolutely necessary and it's one of the nice things is that a lot of this code ends up being the same right the only difference here is I want to group by long distance so I still want to calculate the averages of the arrival delay and the departure delay the only thing I'm changing is that I'm grouping by long distance and now let's see what happens so once again we've got a cool little table um that tells us was are we looking at the longer distance or shorter distance group and then what are our averages and here we do actually see a difference so we've got an average of about 16 minutes of delay if you are below that 88 kilometer average compared to 34 minutes of delay if you're above it so that's about twice um and a similar pattern for the departure delay so it does look like our hypothesis may be correct that if you're going for longer distance more things can go wrong you know you're more likely to potentially arrive um later or depart later of course arrival and departure delays are a little bit linked because if you arrive late you're probably departing late um so it's not surprising that we'd see similar patterns across those two um again there's statistical techniques that we could use to try to verify that this is the case um but uh we won't dig into that today um because we're coming I think pretty close to the end of the of the live stream um let's take a real quick look though at the impact of weather because I think this one's pretty interesting um so what we're going to do is import some weather data about each ride now because this is really similar to stuff we've been doing I'm just going to add the code in to import this weather data set um so we have weather data stored again in a CSV file we're going to import it to a weather data frame um and so you can see for each ride we know some stuff about the wind we know some stuff about the weather and we know some stuff about the temperature just for Best Practices we'll do the same sort of data cleaning steps that we did on the other data set which is to uppercase all our text to make sure we don't have any white space handle any incorrect data types um I'm just going to run that code um because we went over it in in detail before it's the exact same code um so we can see now we've got all our text is is now uniform our temperature is being displayed as integers um and it actually is integers in the original data set and so now what we want to do is we want to take this weather data and we want to take our Railway data we want to stick them together um and the way we're going to do this is that both of them have this ride ID column and so we're just going to take each row from the railway data frame and we're going to go over to the weather data frame and we're going to find that right ID and we're going to grab that weather data um so the Syntax for this is a little bit uh finicky but I'm gonna do my best so I'm going to call this Railway delays join weather um and the method in pandas for doing that sort of look up for saying oh I'm going to take the right ID over here in this data set and I'm going to look for the same right ID in the other data set the Syntax for that is it's called a merge um and so what we have to do is we have to give our two data sets so the first one is our Railway I cannot spell delays correctly in this live stream I don't know why and then we the next data frame is weather um and then we tell it what's the column to do the lookups so we're going to be looking up ride ID in both to try and match that data set that data um and we're going to use what's called an inner merge that just means we're only going to keep things where we have data in both data sets which merge to use is kind of a it's a complex question um that you can learn more about in the course um so let's make sure this worked let's take a quick look at the preview oh I'm so excited when things work um so yes we've now got weather data and trained data all in the same data set and so we can produce our very last thing before the end of this uh live stream so what we're going to do is grab Railway delays join weather so this is our big data set that has all of our data and we're going to do the same thing we're going to group by what type of weather it is so we're going to split into groups is it sunny is it snowy is there a blizzard um and we're going to again calculate the average this is the one thing I'll say for for pandas is as you add all these methods on it can get a little bit finicky but no worry um it just gets long we're going to calculate our average arrival delay and departure delays and then if I just did this we would get an invalid syntax note ah so dot AG is what's called a method that means it takes input all of this stuff is the input to dot AG it tells the aggregation what to calculate and so that means it needs parentheses so if I do just this we've got sort of everything's kind of out of order right we don't we don't know um what was the biggest delay or the shortest DeLay So I am also going to sort this um bye and save myself some room by entering I'm going to sort this by the arrival delay and I'm going to sort it um descending so from biggest to delay to smallest delay and there we have it um it does look like weather impacts this um blizzards are the biggest delay um Sunny is sort of near the middle which is kind of interesting and then we have actually things like downpour actually have the shortest delays um which looks a little bit confusing it's not exactly what we'd expect now maybe there's some things some other things going on impacting the data one thing I'll show which is the very last thing we'll do is that um we can take a look at the number of data points for each weather type so I'm adding in I'm counting how many rides are in each data type and if I do that I can see that some of these down here have less data so the fact that moderate to heavy rain has very little delay whereas if you look all the way up here moderate rain has more delay well that might be because we only have 30 rides here and our data set is just January so that restricts the weather um so that's all just to show you you can get some pretty quick insights from pandas and there's also so much more that you could dig into um the whole data set is also available from this project um so that has data for all of the other months um and that's the end of the project um I guess I will turn it over to Tori or Fede if there are questions that's me yeah hey thank you I'm back hello everybody uh yeah thank you for stopping sharing the screen that way everybody can see us both so we are in the Q a section of the live stream now so I just posted in the chat for everybody to go ahead start dropping your questions for ARA and I'm going to start with one of my own while we wait for everybody to chime in I wanted to ask you I know that data science is a very bad vast feel and it can be very overwhelming for beginners that are just trying maybe are curious about data but really don't know much about it so how do you feel is a good way to go about getting to the field learning about it and doing it in a way that feels like at a good Pace not super overwhelming because there's so many tools there's so many ways there's so many things right yeah there's there's so much um it can definitely be overwhelming and and there's so many things you need to learn sort of or it might feel you need to learn in order right um before you get to say if you want to do machine learning or you want to do something else um I think picking sort of one one programming language one sort of stack of tools is one way to start and to say hey I'm going to do python pandas right I'm going to like that's what I'm going to learn I'm not going to worry about should I be learning R should I be learning Julia I'm going to pick one um and then the other thing I would say is to um you know focus on how you can get actionable sort of Data Insights from data sets without doing things that are too fancy so sometimes it can feel like oh but I'm not doing I'm not creating a neural network like I'm not creating uh you know a fancy algorithm um but a lot of the times the the daytoday of working in data science is creating visualizations doing exploratory statistics doing that kind of data cleaning um and you can be pretty effective as a data analyst without knowing all the fancy stuff um so I guess don't be afraid to just start just jump in with data sets yeah just do it okay oh yeah just just going there work with the data I guess uh looking at the chat uh we have Richard asking how much time does it take on average to become very good as a data scientist who I mean it's a really tricky question because what does very good mean and like you know how much time are you devoting to it um you know I think yeah if you're sort of if you're working hard and you're focusing on it you know I don't know you're doing practicing an hour a day or something um you know you could get reasonably effective um if you're pretty focused in you know three months six months like the nice thing like we were talking about before is that there's like early milestones in data science so you know you could take a data visualization course and be able to produce meaningful visualizations within you know one or two months obviously becoming a very good and you know attaining that level is going to take more time um I don't know that I would call myself very good and I feel like I've been doing data science for a while so it's all kind of it's kind of a hard question to answer I feel like that happens to all the experts the more you learn about a field the less confident you feel about it like you go in very strong and then the more you know it's like the more you know that you don't know yeah so we have a related question in the chat from Matthew also asking about the influx of tech jobs seekers in the current market and wondering what are the things that they can do Beyond code academy to stand out and to kind of like be better or more competitive in the job market so what are some of maybe what are some of the tools like sites or things that people can do to go out there and explore more things related to data yeah so I think that um there's a couple different there's a couple different things that you know employers and other other folks are looking for Beyond sort of just knowing the code right so you're on code academy you learn how to do the code now what um and some of that is can you work on a team right um can you um you know can you not only sort of analyze a data set but do that extra bit of thinking of like oh here's what the business needs um and here's how I can go about trying to answer that question sort of that kind of work um unfortunately that's hard to demonstrate without a job I'm sure that's what people are thinking right now as I'm saying that they're like okay that's very nice but I don't have a data science job um you know if you can find people to do a group project with that's a great strategy um building up that portfolio as you know is always really huge um it's difficult to find open source projects on GitHub um but you know doing that kind of work to demonstrate that you can fit in with a group of people and contribute to a larger goal that's really important quick plug for codecademy docs where you can get started with GitHub contributions um right here nice another question here is about getting discouraged by errors and I think that this is probably very common in data especially because you run into a lot of trial and errors so do you have any advice for people that might you know feel like oh it's just so much that is always a bug something that doesn't run the results that don't work the way that you're expecting it's so frustrating so first of all I just want to say like I'm going to give like a very nice answer and all this sort of stuff but also just know that I have sat there with my vs code or Jupiter notebook open trying to tear my hair out um so I understand um you know the thing about errors is the the cliche is it's always an opportunity to learn um but it's really true so if there's an error and you don't know why it's happening right what that means is there's something you don't understand maybe it's something you don't understand about the data set right so maybe it's not a syntax error maybe it's just something's not coming out correctly um that means that's that's a sign right you don't know something about the data set or you don't know something about one of the methods that you're using and as data scientists right it's our responsibility to make sure that we know what's going on right so that when we get an answer it's not just like oh pandas told me that this was the average well I know how it's doing that computation and I know that really deeply so it's super frustrating but the errors are there to help us understand like oh I need to like learn a little bit more about this or I need to look into it a little bit more deeply it sounds like it comes to the territory yeah it just comes with the territory and like don't be embarrassed or ashamed like even I mean you saw I did some really silly stuff in this live stream um and like the reason I was able to correct those errors so quickly is that like seven years ago or however long ago when I was first starting out um I spent a lot of time being frustrated by those errors so it comes with time um yeah just be patient and uh don't be afraid to ask for help uh fair enough maybe on the Discord I'm going to yeah exactly go to the community so I'm going to combine these last two questions into one uh because both users are basically asking about the same thing and it's related to I guess how the field divides uh Specialties and things like there's somebody here asking you know is it better to go into data analytics before going to data science and there's another user asking what is the kind of data science specialization that you will you know you think that is the one that has the most Futures the one that is you know the better one to go into when you're trying to break into data science so I guess the question is you know out of all these you know Specialties within data science what uh how do you see beginners going into it yeah it's a really complicated world just because data science is still a relatively young like field right which statisticians by the way will be very mad at me for saying that because they're like oh it's just statistics we've been doing this forever but you know data science with the Advent of big data right which it wasn't even that long ago um was when data science really became a field and um you know so we're still figuring this out as an industry um I'll say that you know I think oftentimes people find the easiest way to break in is in the like business intelligence area um so that's someone who's mostly doing working in like SQL with databases or Excel or Tableau um you know trying to answer business questions and those sorts of things um often not doing as much in the form of you know sort of more advanced statistics or more advanced machine learning but I I feel like sometimes people look down on it but it's a it's it's a really important field and there's a lot of knowledge and expertise that goes into it but it is sometimes a little easier to break into because um sometimes you don't need to learn as much code um or as as much Advanced statistics and then you have the job and you can learn on the job um not as easy to find a job ever but um you know that's that's sort of what what I'm aware of in terms of where the field is going with Specialties obviously you know AI the AI machine learning landscape has been really shaken up over the last you know year I don't know if you've heard there's some you know each new products out there um I I some of it to be honest comes down to what you like to do I certainly think machine learning is going to be an Ever sort of growing field um you know it's also one that requires a lot of technical expertise um at least to do well um and so it is something that maybe you know you could learn on the job while doing more analytics focused stuff um and then there's the more statistic side of things um so you hear people talk about inference or causality um and those are people who are you know maybe a little bit more research focused a little more academic um trying to do um you know more advanced statistical analyzes um but uh a lot of data scientists will do a little bit of all of this right so many people with data science you know as opposed to like machine learning specific job titles will work with machine learning um yeah it's it's interesting how the landscape changes right like a few years ago all the rates was machine learning for selfdriving cars everybody wanted to get a job and oh how to make a car drive itself now everybody likes large language models and AI so it's still kind of like in the same field but it feels like a totally different thing right like people don't talk about that they all the one too much now but they you know so I definitely get that uh that sometimes there's a big swings in the market it's hard to know or you know if I'm learning now what is going to be you know hot in 12 months right when I'm ready to apply for jobs so it's pretty pretty challenging thing well thank you so much ARA for stopping by today and sharing your knowledge and the project with everybody I think it was awesome everybody really enjoyed it thank you to everybody that attended their live stream today I hope that you enjoyed it it was a good experience for you if you want to get more content like this if you want to get into it don't forget that you all you need is an email account to have a code academy account getting to those courses try them for yourself hope that you like it we put a lot of work and effort into the curriculum that we have at Academy we are very proud of it so hopefully you enjoyed it as much as we do making it also please don't forget to subscribe at YouTube follow us social media that's where we post a lot of good content we have an incredible blog post LinkedIn Facebook Twitter whatever you find us and also don't forget that we have a live stream for our third part of the three series of the evolution of code academy next Thursday same time 1 p.m Eastern and we'll see you all in our next live stream thank you all again thank you everybody and have a good weekend bye
