With timestamps:

00:00 - um i'll read it out loud it says hi
00:03 - um what i'm wondering most is if the
00:05 - discussion here will have differences
00:06 - with the lessons on linear regression in
00:08 - codecademy
00:10 - that is a good question um there will be
00:14 - some differences just in that this is
00:17 - more of an open discussion and so if
00:18 - there's questions in the chat or there
00:20 - are specific things
00:21 - um that anybody wants to talk about uh
00:24 - we might
00:24 - go on some tangents and um and cover
00:27 - some things that are not in the lesson
00:29 - but for the most part we're gonna start
00:31 - by uh
00:32 - going through the the same content
00:36 - that's in the lesson
00:37 - yeah cool cool we are back in the office
00:41 - with uh
00:42 - our fancy streaming setup so we're
00:44 - trying out like new mics
00:45 - and cameras all that kind of stuff so
00:48 - let us know in the chat if
00:50 - anything is screwed up if you can't hear
00:52 - us if the mic sounds bad
00:53 - um but uh hopefully everything is uh
00:57 - is working fine yeah all right
01:01 - shall we get started let's do it all
01:02 - right so
01:04 - um the topic for today is going to be
01:08 - um interaction terms and polynomial
01:12 - terms which basically are ways that we
01:15 - can
01:16 - alter a linear regression a simple
01:18 - linear regression model or a multiple
01:20 - linear regression model
01:22 - to make it a little bit more flexible
01:23 - and handle some different things
01:26 - that we might see in some data
01:29 - so um i actually think this is exactly
01:32 - the same
01:32 - to the question that was asked earlier
01:34 - this is exactly the same data set
01:36 - that we give you in the lesson on
01:38 - codecademy
01:39 - and i believe it is simulated i didn't
01:42 - write this lesson
01:43 - um but the idea
01:46 - behind this this data is that we're
01:48 - imagining
01:49 - um some sort of study that measures
01:54 - self-reported levels of happiness self
01:57 - reported or maybe like measured levels
02:00 - of stress
02:01 - um whether or not a person exercises
02:05 - um i think sleep is probably
02:08 - hours of sleep make sense 13.8 it's a
02:10 - little extreme
02:13 - maybe and then maybe hours of free time
02:16 - per day i think that's the intention of
02:19 - this data set it's
02:20 - i'm pretty sure simulated though so not
02:22 - not real
02:24 - cool so um what we're gonna get started
02:28 - with is uh visualizing some of this data
02:31 - um you'll notice actually in the
02:35 - lesson on codecademy and i just grabbed
02:38 - this code directly from there
02:41 - you'll notice that we actually use this
02:43 - lm plot function
02:45 - to do a lot of the plotting in this
02:49 - in this lesson the reason for that is
02:52 - partially because
02:54 - uh we haven't updated uh
02:57 - matplotlib and seaborn in a little while
02:59 - so
03:00 - this is an older function in seabourn
03:04 - that does the same thing as the newer
03:05 - scatter plot function
03:08 - um but also it has the option to fit a
03:11 - regression line
03:12 - directly onto the plot if you wanted to
03:15 - um
03:16 - and so i'll just demo that really
03:18 - quickly
03:19 - um here actually i'll demo it in a
03:22 - separate plot
03:25 - so if i say let's look at the
03:29 - relationship
03:30 - between stress and happiness so
03:35 - is there a relationship between how
03:36 - stress someone appears to be and how
03:38 - happy they report being
03:40 - um and i say fit reg
03:43 - equals true and i think that's also the
03:46 - default
03:47 - and i run this i get
03:51 - here are the points and then it'll
03:54 - automatically draw this regression line
03:56 - for me
03:57 - which is kind of cool and it allows you
03:59 - to
04:00 - do this a little do this plotting a
04:02 - little bit more quickly
04:04 - um which is which is always fun there
04:06 - you'll notice that
04:08 - something interesting happens actually
04:10 - if i leave this is fit rug
04:11 - equals true and add q
04:15 - equals exercise right so now it's going
04:18 - to even be a little bit hard to
04:20 - visualize right because there's uh
04:24 - two variables there so three variables
04:25 - right there stress happiness
04:27 - and exercise yeah um
04:30 - it it's pretty smart actually it will do
04:32 - something here
04:35 - actually draw me two regression lines
04:38 - one for exercise equals one and
04:42 - the other for exercise equals zero
04:46 - alex do you notice anything weird about
04:48 - this plot though that's like different
04:49 - from
04:50 - what we might expect based on everything
04:53 - we've covered so far like
04:55 - do you remember i think maybe it was the
04:57 - last stream you joined me on
04:59 - where we started talking about um
05:02 - using this using a categorical variable
05:05 - in our regression
05:06 - and we we plotted um some regression
05:10 - lines
05:11 - when we included a categorical variable
05:15 - in the regression right and the the two
05:18 - lines had something
05:20 - different from what we're seeing here um
05:23 - you remember
05:24 - no i don't recall okay so
05:28 - um so actually the two lines were
05:30 - parallel
05:31 - so um there was no way previously
05:35 - or we didn't have a way previously to
05:38 - draw um to draw regression lines that
05:42 - were like
05:42 - totally just fitting the orange points
05:45 - and the blue points separately we could
05:47 - only change
05:48 - the intercept so if you go back you
05:51 - watch the
05:52 - older video you'll see that in action
05:55 - um and today we're gonna actually learn
05:57 - how to
05:58 - fit these lines um so that
06:01 - they can have different slopes yeah and
06:04 - in that graph
06:05 - is are these lines so say you took out
06:08 - all of the blue circles
06:09 - um and just fit it to the orange axes is
06:12 - that the orange line and similar
06:14 - to the other way around so they're
06:16 - completely independent of each other
06:17 - they're not um they're basically
06:19 - ignoring the other set of points
06:23 - um if you fit yeah if you fit
06:27 - just a regression for the orange line or
06:29 - orange points and just regression for
06:31 - the blue points you'd get these lines
06:33 - okay cool um so
06:36 - i'm gonna now just show you this plot uh
06:39 - without the line i
06:40 - lines i think i kind of gave away via
06:43 - the end goal before we even got started
06:45 - so we'll start back at the beginning
06:47 - and we'll motivate this by saying
06:49 - sometimes
06:50 - when we look at a plot
06:54 - of two quantitative variables and then
06:57 - we color by categorical variable
07:00 - we'll see that if we were to draw a line
07:03 - through just the blue points and if we
07:04 - were to draw a separate line through
07:06 - just the orange points
07:07 - those points would or those lines would
07:09 - probably have different slopes
07:11 - and different intercepts whereas up to
07:13 - this point
07:14 - if we just added um so if we just fit a
07:17 - model where we did like happiness
07:19 - as a function of stress and exercise
07:23 - we'd end up with a model that allows us
07:26 - to draw separate lines for these two
07:29 - sets of points but they would have to
07:30 - have the same slope they could just have
07:32 - different intercepts
07:34 - and we'll i'll show you on the ipad in
07:36 - just a second mathematically
07:38 - why that happened and then i'll show you
07:40 - how we can
07:41 - now fit a regression that visually would
07:45 - look like this
07:46 - cool cool so um
07:50 - what i'm gonna do is i'm going to
07:54 - here let's add a um
07:58 - let's add a chunk and let's fit this
08:00 - regression
08:01 - right off the bat um i'm gonna grab this
08:05 - code from over here
08:07 - and we're gonna talk through it for a
08:09 - second and then
08:11 - we'll come back to this in a minute okay
08:15 - so i'm fitting this model i believe i've
08:17 - already
08:18 - loaded yes um so i'm fitting this model
08:23 - and it's happiness as a function of
08:26 - stress
08:27 - plus exercise plus stress
08:30 - colon exercise now for a second
08:34 - let me remove this this is what we've
08:36 - done so far
08:38 - in previous um live streams so if i run
08:41 - this
08:43 - this is kind of what we've seen before
08:46 - we get an intercept
08:47 - we get a slope for stress and we get a
08:50 - slope
08:51 - for exercise same intercept so these
08:54 - lines aren't parallel
08:56 - it's same intercept and different slopes
08:59 - so this these lines actually are
09:00 - parallel so
09:02 - actually this is a good a good place to
09:04 - kind of like come back to this
09:06 - i hadn't initially planned on it but i
09:08 - think this is like a good
09:09 - reminder so um let me
09:13 - actually you can flip me over but
09:17 - um can we leave that up yes okay
09:20 - great so this is for the um
09:23 - the other one that had the stress colon
09:25 - exercise but i'm just gonna write down
09:27 - the um the values for the other
09:30 - regression for a second
09:31 - so for this one we had an intercept
09:35 - of 10 about we had
09:38 - a slope on stress
09:42 - that was equal to negative 0.7
09:46 - and then we had a slope on exercise
09:51 - that was equal to about negative 0.9
09:56 - and we fit this model it was happy
10:01 - as a function of stress
10:04 - plus exercise and what that meant
10:08 - was that we were fitting this model we
10:10 - fit something that looked like
10:12 - happy equals
10:16 - intercept which we called b0 but we'll
10:19 - replace that
10:21 - plus b1 times stress
10:26 - plus b2 times exercise
10:29 - it's gonna abbreviate
10:33 - and so plugging in b0 b1 and b2
10:37 - we got something like happy
10:40 - equals 10 plus
10:43 - well it's really uh plus negative so
10:47 - minus 0.7 times stress
10:52 - minus 0.9 times
10:55 - exercise but remember that there
10:59 - are only two possible values of exercise
11:01 - someone can exercise or not
11:03 - exercise got it so we ended up with two
11:06 - separate
11:07 - equations one when exercise
11:10 - equals zero we get happy
11:14 - equals 10 minus 0.7
11:18 - times stress minus 0.9 times 0 but 0.9
11:22 - times 0 is 0.
11:23 - so that kind of goes away and then the
11:26 - other one we got right down here
11:30 - when exercise equals 1 we get happy
11:35 - equals 10 minus 0.7
11:38 - times stress minus
11:42 - 0.9 times 1 which is just 0.9
11:45 - and so then we end up with happy equals
11:49 - um we can take the 10 and subtract the
11:52 - 0.9
11:53 - right because order of operations we can
11:55 - all of these
11:57 - addition and subtraction things can
11:59 - happen in any order so we end up with
12:01 - like
12:02 - 9.1 minus 0.7
12:06 - times stress got it so same slope
12:09 - different intercepts and so
12:10 - they both have the same slope on stress
12:13 - but the intercept is 10
12:17 - when exercise equals zero and the
12:19 - intercept is 9.1
12:21 - when exercise equals one and so that
12:23 - slope on
12:24 - exercise is really the difference in
12:26 - intercepts
12:28 - for the two lines that we would draw oh
12:30 - i'm now realizing that this is
12:31 - that we are covering the uh
12:36 - oh can't
12:39 - darn it sorry guys oh alex i don't even
12:42 - know
12:43 - it's there all right
12:47 - we'll figure it we're figuring out the
12:48 - green screen sorry guys
12:51 - um hopefully that was clear though uh
12:54 - okay
12:54 - so now let's go back to
12:58 - that second um let's can we switch back
13:01 - to the uh
13:03 - that's just okay yes um
13:06 - okay so that was when we just fit this
13:09 - where we have
13:09 - happy as a function of stress plus
13:11 - exercise
13:13 - but now we're gonna add an interaction
13:16 - term
13:16 - and the way we can do this in stats
13:19 - models
13:20 - is we can just say i want an interaction
13:23 - between
13:23 - stress and exercise
13:27 - by putting a colon between them
13:30 - so now when we run this model run it
13:34 - um now we get an intercept
13:38 - we get a slope on stress
13:41 - and then we also get a slope on exercise
13:44 - just as before
13:45 - but we get now a third slope
13:48 - slope on stress colon exercise
13:53 - so now we're going to take this
13:56 - and we'll go back to the ipad in just a
13:59 - second
14:01 - and talk through what those
14:06 - sorry i'm erasing really quickly um
14:10 - talk through what those translate to
14:13 - in terms of our model okay
14:17 - so now
14:21 - oops well that's okay um so now we see
14:24 - okay
14:25 - we had intercept stress
14:28 - exercise stress colon exercise all an
14:32 - interaction term is is an extra
14:35 - it's almost like having an extra column
14:37 - in your data that is the product
14:40 - of two other columns so
14:43 - it's almost like an extra feature that
14:45 - is just the product of two others
14:48 - so the way this looks like in terms of
14:51 - the model
14:52 - is we're fitting now happy
14:58 - screwed up let's see
15:04 - so now we're fitting happy
15:08 - equals b0
15:11 - plus b1 times stress
15:16 - plus b2 times exercise
15:21 - plus b3
15:24 - times stress times exercise
15:28 - times sign okay okay
15:32 - so in like for example
15:36 - in um scleren you have to create
15:41 - a um an interaction term yourself in
15:44 - your data frame and literally what you
15:46 - would do and we could demonstrate this
15:47 - after
15:48 - is you would just multiply everything in
15:51 - the stress column
15:52 - by everything in the exercise column and
15:54 - create a new column
15:56 - of your data frame called stress
15:58 - exercise
15:59 - and then you would just add that into
16:01 - your model
16:02 - as a predictor i think i'm a little bit
16:05 - confused of like what the overall goal
16:07 - is here
16:08 - because so in in this situation isn't
16:10 - this gonna still result in two
16:12 - parallel lines because that times one
16:15 - the time
16:15 - zero is still going to be um i can't
16:18 - even
16:18 - uh that's not pointing at anything it's
16:20 - still gonna be right when when
16:22 - exercises is equal to 0 the
16:25 - b2 times exercise and b3 times stress
16:28 - times exercise all of that is just going
16:29 - to
16:30 - go to zero so isn't this really similar
16:32 - to the last thing that we just looked at
16:34 - it's very similar but let's actually
16:36 - walk through exactly that
16:38 - so let me plug in all of these numbers
16:40 - so we've got
16:41 - happy equals
16:45 - our b0 is 12 that's still our intercept
16:49 - um then we've got b1
16:52 - it's negative so we're gonna do b1 is
16:55 - negative 0.97 so minus 0.97
16:59 - times stress and then
17:02 - again the negative sign minus 3.1
17:06 - times exercise and then we've got
17:11 - plus .36
17:15 - times stress times
17:18 - exercise okay
17:21 - now let's break this up into two
17:24 - separate
17:25 - um
17:28 - two separate equations
17:31 - feel like this eraser never works the
17:34 - way i wanted to
17:35 - um okay so let's break it up into two
17:38 - separate ones
17:39 - one where exercise equals zero and one
17:42 - where exercise equals one
17:43 - so i think you alex were doing the
17:45 - exercise equals zero
17:46 - one in your head first so um it's a
17:50 - little simpler so
17:51 - let's do that first so
17:55 - when exercise equals zero we've got
17:59 - happy equals
18:02 - 12 minus .97
18:05 - times stress and then we've got
18:08 - minus 3.1 times zero
18:13 - plus .36 times stress
18:18 - times zero now
18:21 - like you were saying right anything
18:24 - times
18:24 - zero is zero so 3.1 times 0 is 0
18:29 - but also 0.36 times stress times 0
18:33 - even though we're multiplying stress in
18:34 - there it's still like a bunch of things
18:36 - multiplied by zero so that's also zero
18:40 - okay so that's zero that's zero
18:44 - so our equation is really just
18:48 - happy equals
18:52 - 12 minus point nine
18:55 - seven times stress
19:00 - okay so we've got our line again
19:03 - for exercise equals zero now let's try
19:07 - exercise equals one
19:11 - this is where it gets a little bit more
19:12 - complicated yeah
19:14 - so we've got um and also if anybody has
19:17 - questions as
19:18 - we go i know this is like again a little
19:20 - bit of math
19:21 - um so ask us questions if they come up
19:26 - but for exercise equals 1 we've got
19:29 - happy
19:30 - equals 12 minus 0.97
19:33 - times stress just like before
19:36 - then we've got um minus
19:41 - 3.1 times 1
19:44 - and then we've got plus
19:49 - 0.36 times stress
19:54 - times one got it okay
19:57 - so multiplying by one doesn't change the
20:00 - value of anything
20:02 - right so we can do the same thing as
20:04 - before with our
20:06 - with our intercept rate so like this
20:09 - minus 3.1
20:10 - times 1 we can just add to
20:14 - the 12 again order of multiple
20:17 - order of operations since this is just
20:19 - like multiplication
20:21 - and addition we can order we can add or
20:23 - subtract these terms in any order so we
20:25 - can
20:26 - combine these two together
20:29 - so i'm going to do that first and then
20:31 - i'll write out the other set
20:33 - but some of you might already kind of
20:35 - see what that's going to be
20:37 - so we've got happy equals and then 12
20:40 - minus 3.1
20:41 - is like 8.9 right
20:46 - and then we've got minus .97 times
20:49 - stress and then we also have
20:52 - plus .36
20:55 - times stress and then i'm gonna just
20:58 - drop the one because
20:59 - anything times one is just equal to
21:01 - itself
21:03 - and now we've got we've got minus 0.97
21:06 - times stress
21:07 - plus 0.36 times stress and so
21:11 - both of these things are multiplying
21:13 - stress
21:14 - so we can actually factor stress out
21:18 - of this like piece right here
21:22 - so stress is multiplying both things
21:26 - we can factor it out and add the 0.9
21:30 - minus 0.97 and 0.36 together
21:33 - and so what we get is something that
21:35 - looks like this
21:36 - we get happy
21:40 - equals 8.9
21:43 - and then we've got um
21:48 - i guess i'll put like plus stress
21:54 - times and then we've got
21:57 - the minus 0.97
22:01 - plus 0.36 oops
22:05 - 0.36 and that
22:08 - um again like thinking about how you
22:11 - would
22:12 - calculate this or like how you would
22:14 - expand this right you would multiply
22:16 - stress by both things inside the
22:18 - parentheses so you'd get like
22:20 - you'd get back to what you had above
22:21 - you'd have the stress times minus 0.97
22:24 - that's this term from before
22:27 - and then you'd get the stress times .36
22:30 - which is this term
22:31 - so we just factored it out but we
22:33 - rewrote that same thing
22:35 - as before and then minus 0.97
22:39 - plus 0.36 uh
22:43 - 61.
22:46 - yeah um is
22:49 - yeah so we got happy equals 8.9 plus
22:56 - 0.61 or sorry minus 0.61
23:06 - i guess i shouldn't have switched the
23:07 - order but you can multiply in
23:10 - either order so um okay
23:13 - so that's our equation for
23:16 - exercise equals one that's our equation
23:19 - for exercise equals zero
23:21 - and you'll see that they have different
23:23 - intercepts
23:24 - 8.9 versus 12 and they have different
23:27 - slopes minus 0.61 versus minus 0.97
23:33 - cool cool and so right the change in
23:36 - slope
23:37 - came from this term that included both
23:40 - stress and exercise that was the thing
23:42 - that ultimately affected
23:44 - the slope when exercise was one
23:48 - exactly um
23:52 - can you briefly describe again how the
23:54 - 0.36 was calculated from the original
23:56 - data right so these are these numbers
23:57 - that are up in the top left corner
23:59 - where do where do those even come from
24:01 - oh so
24:02 - these numbers came from fitting the
24:05 - model
24:06 - um so we'll go back to the jupiter
24:07 - notebook in a second um
24:09 - but basically when we fit the model with
24:11 - that interaction term it calculated a
24:13 - slope
24:14 - on the the interaction between
24:18 - stress and exercise and that is
24:21 - basically like a multiplier right of
24:24 - stress times exercise
24:26 - um so then we just followed that out in
24:28 - the model
24:29 - i also see a really good question in
24:32 - here
24:32 - about it says would we consider the
24:35 - third variable stress times exercise
24:38 - dependent on the values of stress and
24:40 - exercise
24:41 - that you get more accurate models when
24:43 - all your feature variables were
24:44 - independent so that's a really good
24:46 - question
24:48 - i'm not sure how great of an answer i
24:52 - can give but
24:53 - um all to say so yes
24:57 - you you don't want to have
25:00 - uh co-linear very or you don't
25:03 - want to have highly correlated variables
25:06 - in
25:06 - um in a model together because
25:10 - they're both if they're both
25:13 - explaining your outcome variable
25:16 - then the model is going to have some
25:18 - trouble
25:20 - estimating the coefficients for both of
25:22 - them
25:23 - because they're both like they both have
25:25 - the same explanatory power
25:27 - in terms of your outcome variable um and
25:30 - it also
25:30 - right if they're highly dependent like
25:33 - if you just take
25:34 - um if you take one of the columns and
25:37 - multiply it by a number
25:38 - theoretically you shouldn't be able to
25:40 - fit that model at all because
25:42 - it's perfectly your new variable is
25:44 - perfectly correlated with your old
25:46 - variable
25:47 - in this case it's not perfect it's not
25:50 - like this
25:50 - new variable that we created by
25:52 - multiplying stress and exercise
25:55 - it's not perfectly correlated with
25:57 - either stress or
25:58 - exercise because we're multiplying by a
26:00 - different
26:01 - value for each so like if you think just
26:04 - about
26:04 - stress whether you multiply it by zero
26:08 - or multiply by one depends on the value
26:11 - of exercise
26:12 - so there's no way you could just take
26:16 - that column without seeing the exercise
26:18 - column
26:19 - and and get the new one right it's not
26:21 - like if you were
26:22 - just doubling stress that would be
26:24 - perfectly correlated with stress but
26:25 - there's
26:26 - um yeah it's either you're sometimes
26:29 - multiplying it and sometimes not
26:30 - depending on if exercise is zero or one
26:32 - yeah but on the flip side there are
26:36 - reasons why you might not want to
26:39 - just create like the most complicated
26:42 - model that you can
26:43 - with as many extra predictors as like
26:46 - this as
26:47 - you possibly could create because you
26:48 - could imagine like if you have
26:51 - i don't know 10 columns of a data set
26:53 - which is even in real life that's like
26:55 - pretty small um you could imagine
26:58 - you could create an interaction term for
27:00 - every possible
27:02 - pair of pair of variables basically
27:06 - pair of features and that would be a
27:09 - very large number
27:10 - that i i don't know what it is but
27:13 - yeah but that would be a very large
27:15 - number and you could create a very
27:16 - complicated model to model like every
27:18 - possible interaction
27:20 - um but that's not
27:24 - always a good idea because you end up
27:26 - what ends up happening is you end up
27:28 - over fitting and that's kind of
27:31 - actually a very important topic in a
27:33 - newer course that we're building right
27:35 - now
27:35 - on feature engineering um and like
27:38 - regularization
27:39 - um but basically if you if you fit your
27:43 - data
27:43 - too well then you're not your model is
27:45 - not going to be very good at
27:47 - predicting outcomes for new
27:50 - data so like this this model if i make
27:52 - it super complicated
27:54 - might perfectly fit my my data that i
27:57 - collected on my like 100 patients or
27:59 - whatever
27:59 - but if i go out and i find another
28:01 - patient like this might not be a good
28:04 - way of explaining the relationship
28:05 - between these things
28:06 - right because you were like so dependent
28:07 - on those original 100 patients
28:09 - exactly um can you
28:13 - describe just like the total takeaway or
28:16 - upshot of of this whole process because
28:18 - what i'm walking away from this is
28:20 - before we were able to draw two lines
28:22 - that were parallel
28:24 - that kind of stinks for some reason and
28:26 - now we can draw two lines that aren't
28:28 - parallel like
28:28 - what is what is like the thing that i
28:30 - should be taking away from
28:32 - creating this uh this new variable will
28:34 - you go back to the
28:35 - the picture or the um yeah the browser
28:38 - the browser
28:39 - yeah so that's a really good question i
28:41 - think so
28:42 - a couple of things first of all um
28:47 - it's just you know if you were to plot
28:50 - your data and you see this picture maybe
28:52 - you see something even
28:54 - more obvious like maybe you see these
28:57 - lines look completely different like
28:58 - maybe you even see
29:00 - that for um people who do
29:03 - exercise the relationship between
29:05 - happiness and stress
29:07 - is this like negative relationship like
29:10 - people who are
29:11 - less happy are more stressed but for
29:13 - people who don't exercise maybe it's
29:15 - like a positive
29:17 - relationship and so what you actually
29:19 - see sometimes and this is
29:21 - sometimes i think referred to as
29:23 - simpsons paradox and we actually have
29:25 - here i can go and then um show you
29:28 - in the lesson as a note so we're using
29:33 - new uh streaming software and your mouse
29:35 - pointer isn't showing up on the uh
29:37 - on the software so if you're ever
29:38 - gesticulating with the mouse uh that's
29:40 - not getting backed up good to know
29:42 - something we can uh play around with
29:45 - is it oh it might not be in this lesson
29:48 - which lesson is it in um
29:55 - let me see if i can find it quickly i
29:57 - think it's
29:58 - in
30:02 - this lesson
30:07 - um yes
30:12 - so this is like
30:15 - a more complicated example here let me
30:17 - like
30:18 - pull this up um but you'll see
30:22 - right like in this in this example we've
30:25 - got
30:26 - um like a positive if we were to just
30:29 - like
30:30 - ignore this um categorical variable
30:33 - and draw a line through all the points
30:35 - we would draw this black line
30:36 - which is like this positive relationship
30:39 - but then
30:40 - if you add this um
30:43 - this like categorical variable to the
30:46 - model
30:47 - and you're trying to draw the lines now
30:50 - based off of
30:51 - um you're basically trying to draw the
30:53 - lines
30:54 - through each set of points in this
30:57 - example they're parallel so like
30:59 - we're not constructing but you'll see
31:00 - right that like
31:02 - the um the lines have negative slopes
31:06 - even though the initial relation like
31:08 - the relationship
31:09 - between these two variables in among all
31:12 - the points is positive
31:14 - if you zoom in you now are getting these
31:16 - like negative relationships
31:18 - and so you can see stuff like that where
31:20 - like
31:21 - maybe right maybe one of these
31:24 - relationships is
31:25 - negative and the other one is positive
31:27 - and if you
31:28 - didn't fit anything like you didn't fit
31:31 - um
31:33 - any sort of exercise term
31:36 - like you'd end up with a flat line you'd
31:40 - think there's like no relationship
31:41 - between happiness and stress but like
31:43 - once you add this the model you suddenly
31:45 - start seeing them
31:46 - but now if you are restricting this so
31:49 - that those lines have to be
31:50 - parallel then you're like ending up with
31:54 - you're gonna end up with maybe like two
31:56 - parallel lines that are flat
31:57 - and that's like not gonna model your
32:00 - data super well so
32:01 - there's definitely situations where
32:02 - adding an interaction term
32:04 - means that you significantly improve
32:06 - your model from where like you basically
32:08 - can't
32:09 - explain anything to um oh it looks like
32:12 - i didn't we didn't switch over maybe
32:14 - um that's possible oh sorry yeah you're
32:16 - good um okay
32:17 - uh but yeah so sometimes it
32:20 - significantly improves your model
32:22 - i think like the flip side or the other
32:24 - thing
32:25 - that maybe is like the other half of the
32:27 - answer to that question
32:28 - is and maybe this gets a little bit more
32:31 - into like when we do this
32:33 - um is we do this when we think
32:36 - that the relationship between to the
32:39 - outcome variable
32:40 - and one of our features is
32:44 - moderated by or like influenced by some
32:47 - other
32:48 - factor um so in this example
32:52 - uh it looks like
32:55 - right the relationship between stress
32:58 - and happiness
32:59 - is maybe like less
33:02 - steep here i'll do it with the lines um
33:06 - is less steep for people who
33:09 - do exercise so like maybe people who are
33:12 - exercising
33:14 - are more like immune
33:17 - to stress in general and um
33:20 - and so like being more stressed
33:23 - has less of an effect on their happiness
33:26 - than
33:26 - people among people who don't exercise
33:28 - it's a steeper line
33:30 - and so we're saying that like we think
33:31 - that you know
33:34 - more stress has like more of a effect
33:37 - i say in quotes because like we're not
33:39 - really depending on how the studies run
33:41 - we don't know if it's like a causal
33:42 - relationship but
33:43 - the effect of stress on happiness is
33:46 - more significant um among people who
33:49 - don't exercise and so we think like
33:51 - there's some
33:53 - exercises moderating this relationship
33:55 - and that's why we might think like we
33:57 - want
33:57 - to we want to be able to model that in
34:00 - some way
34:01 - this is maybe a tricky question or uh
34:06 - this may be a tricky question but like
34:07 - how do you do this
34:09 - in the real world of like so you just
34:12 - described oh we have this situation
34:13 - where
34:14 - one set of data points has like a
34:16 - negative slope one has like a huge
34:18 - positive slope we want to be able to
34:19 - draw these two lines with different
34:20 - slopes
34:21 - in order to capture that and so we want
34:23 - to do something like this
34:25 - but do you know that by plotting the
34:27 - data and looking at it
34:28 - do you like just give both things a shot
34:32 - and
34:32 - like see and see what the models look
34:34 - like after
34:36 - after trying to add it or trying not to
34:38 - add it like how do you actually do this
34:39 - with
34:40 - a data set that you don't know anything
34:42 - about
34:43 - that is a great question um so you can
34:48 - certainly and i
34:49 - highly recommend in fact i'm working
34:51 - right now on some content on
34:53 - exploratory data analysis and it's
34:55 - definitely a good idea
34:57 - to before you fit a model take a look at
35:00 - some of these relationships and try to
35:02 - see
35:02 - whether maybe you have hypotheses like
35:04 - maybe you
35:05 - know something you have some prior
35:08 - research about the relationship between
35:10 - these
35:10 - all of these features and you have some
35:13 - hypotheses
35:14 - about what might be moderating what
35:16 - relationships
35:17 - and then maybe you get some data
35:20 - and you want to take a look at it before
35:22 - you try to fit the model
35:24 - and so that's one thing you would do um
35:26 - the other thing you can do which is more
35:28 - like post talk
35:29 - is you can um you can compare different
35:33 - models so you could fit it both ways
35:35 - and then you could see um you know like
35:38 - the simplest thing is you might
35:39 - calculate like a mean squared error
35:41 - like basically um or a or a
35:45 - total squared error or whatever um
35:48 - basically like how far off are all these
35:50 - points
35:51 - from their respective lines and you
35:54 - could calculate that for
35:56 - both models the one where you don't
35:58 - include the interaction and the one
35:59 - where you do
36:00 - um and then you could say like okay
36:03 - i see that when i add this term to the
36:05 - model um
36:06 - it significantly improves its ability to
36:08 - explain
36:09 - this data um and you'll actually see if
36:12 - you're following along in this linear
36:14 - regression course
36:15 - um there is a
36:19 - a section here on choosing a linear
36:21 - regression model
36:22 - that starts to cover some of those
36:24 - methods that you could use to compare
36:26 - two different models once you fit them
36:28 - um
36:29 - to get a sense for whether whether both
36:32 - of them make sense
36:33 - or whether one makes sense over the
36:35 - other then there's also additional
36:38 - additional things you can do like
36:39 - regularization which nithya is working
36:41 - on right now
36:42 - um that can even further
36:45 - like help you basically like fit the
36:48 - model at the same time as estimating
36:50 - whether like those parameters are useful
36:54 - um so cool that was
36:57 - a bit of a tangent but hopefully um
37:00 - i see a question how do you do feature
37:03 - selection
37:04 - in linear regression i've heard a
37:06 - forward and backward elimination methods
37:09 - but don't know what they do um again
37:11 - this is like a plug for
37:12 - all the content that nitia is currently
37:14 - working on
37:15 - um and that will be i think going live
37:18 - at the end of the summer early fall
37:20 - um but essentially so forward and
37:23 - backward elimination
37:25 - are essentially methods for
37:28 - like sequentially adding um terms to
37:31 - your model so like maybe you want to
37:33 - sequentially add different interaction
37:35 - terms and
37:37 - re-evaluate the model every time and
37:39 - then if
37:40 - adding the term improves the model you
37:42 - keep it if adding it
37:44 - doesn't improve the model then you don't
37:46 - keep it and you try something else
37:48 - or you start with like all of them in
37:50 - the model and you sequentially like
37:52 - delete things that are not helping you
37:55 - um that would be backwards elimination
37:58 - and then regularization is
38:00 - essentially a way that you can fit this
38:01 - model um
38:03 - with like an extra term so that
38:06 - you allow some of the coefficients to
38:08 - get really small or even go to zero
38:11 - um and that's kind of a way of like
38:15 - eliminating eliminating those uh from
38:18 - your regression
38:20 - um okay and then
38:24 - suppose i have data can i add two extra
38:26 - decimals on point five
38:30 - six
38:32 - oh you can add as many decimals as you
38:34 - want to run the linear regression
38:36 - um yeah as much i mean
38:40 - i don't know if that's that is that's
38:41 - not the question uh
38:44 - yeah the question is saying like oh if i
38:46 - have this data can i
38:48 - add can i add or the way that i'm
38:50 - interpreting this question is can i add
38:52 - points between the my existing points in
38:55 - like regular steps
38:56 - of what can i add 0.5611.56
39:01 - like can i add data that falls along
39:03 - that line i think that that's what the
39:04 - question is asking but
39:06 - if you want to clarify the question um
39:11 - so i guess like basically
39:14 - i think you can't add data if you're
39:16 - trying to answer a question
39:18 - about some data or like fit a model to
39:21 - some data then you probably don't want
39:22 - to alter your data
39:23 - before you fit the model um but
39:29 - yeah i mean if it's like something that
39:30 - you could collect
39:32 - so like if these are values of stress
39:37 - and you don't have anyone in your data
39:39 - set who reported like a value of 0.5611
39:42 - for stress but you can go out and find
39:44 - someone and you can record their
39:45 - happiness score
39:47 - then you could add that to the model if
39:49 - you don't
39:51 - have or you could add that to your data
39:52 - to fit the model if you don't
39:54 - if you can't find someone with that
39:55 - value you shouldn't just like add that
39:57 - to your data set before you hit the
39:58 - model um
40:01 - i got it and the clarification in that
40:04 - question of if
40:05 - 0.56 is occurring multiple times can i
40:07 - add extra decimals to it
40:11 - so i still don't fully understand but
40:15 - yeah if you see multiple people with
40:17 - this value of 0.56 and you are able to
40:20 - measure
40:21 - more specific values um
40:25 - then yeah like if you are able to
40:27 - measure to the fourth decimal place but
40:28 - most like measurement
40:30 - tools have some limits um
40:33 - i also see a question about
40:34 - heteroscedasticity
40:36 - um which is a fun word i think um that
40:40 - is coming in the next live stream
40:45 - so yeah we'll come back to that next
40:48 - week um
40:49 - cool so i think uh
40:53 - we've got like 20 minutes left i want to
40:54 - cover two more things if we
40:57 - if we can fit them in um so one thing
41:00 - i'll kind of go
41:01 - over a little bit more quickly but
41:04 - i will i will pull this over
41:08 - and re-run in our other notebook
41:11 - um so
41:16 - i just want to demonstrate so up to this
41:18 - point we
41:20 - have shown that we can fit this
41:23 - interaction term with a categorical
41:25 - variable
41:26 - um as one of the terms that we're
41:29 - interacting
41:30 - so in that case we ended up with just
41:33 - two lines
41:34 - because there are only two values of
41:36 - exercise so you basically get like a
41:37 - separate line for each value of exercise
41:41 - um but we can also fit interaction terms
41:44 - with two quantitative variables so
41:48 - here's another plot it shows the
41:50 - relationship between
41:52 - happy and stress and then we've colored
41:55 - by this value or this other variable
41:59 - free time which is like the number of
42:01 - hours of free time that you have
42:03 - um and this is also a quantitative
42:06 - variable
42:07 - we're just showing values uh like
42:10 - integer values
42:11 - for the the key but you could imagine
42:13 - this is like
42:14 - a scale where people that have more free
42:17 - time are darker colored dots
42:19 - and people that have less free time are
42:22 - lighter colored dots
42:23 - in this plot and you could imagine like
42:27 - i i can't do this super easily in my
42:29 - head but you can imagine if you just
42:31 - isolated all the people with
42:32 - five hours of free time you might draw
42:35 - oh i
42:36 - am remembering now that you can't see my
42:39 - my pointer but right
42:40 - so if you uh just isolated all people
42:43 - all this like the darkest purple dots
42:45 - the line that you draw through those
42:47 - might be different from the line that
42:50 - you draw through
42:51 - the lightest color dots to people with
42:52 - zero hours of free time
42:54 - and so um so we can fit this for
42:59 - uh this setup as well
43:02 - so i'm gonna go ahead and grab
43:06 - this model again and we'll
43:09 - edit it a little bit
43:14 - so this time let's edit this let's do
43:16 - stress
43:17 - uh plus free time
43:24 - plus stress interacted
43:28 - with free time and then let's fit that
43:33 - and we can fit something again we see
43:36 - we get our intercept slope on stress
43:40 - slope on free time and also a slope on
43:44 - free time colon strap or stress colon
43:47 - free time
43:48 - um and i'll just write this out super
43:51 - quick on
43:53 - our ipad again
43:56 - um
44:01 - okay cool so here we've got those
44:03 - written out
44:05 - for ourselves and let's write out what
44:06 - this model is super fast
44:08 - so the model is exactly the same as what
44:10 - we had before we've got like
44:12 - happy
44:17 - we've got happy equals b0 which is eight
44:23 - plus um and again with the minuses sorry
44:27 - minus point five five
44:30 - times stress
44:33 - plus point 0.12 times free time
44:41 - and then plus 0.04
44:45 - times stress
44:49 - times free time i'll abbreviate ft
44:53 - okay and then you can see for different
44:56 - values of free time we get different
44:58 - equations so
45:00 - for example when free time equals zero
45:04 - our equation is just like happy equals
45:07 - eight minus point five five times stress
45:11 - and these two other terms go to zero
45:14 - because point one two times zero
45:16 - is zero and .04 times stress times zero
45:19 - so zero
45:21 - and then when free time equals one
45:24 - we get happy equals eight
45:28 - minus point five five times stress
45:32 - and then plus 0.12 times 1
45:36 - which is just 0.12 and then plus
45:39 - 0.04 times stress
45:43 - times 1 which is just 0.04 times stress
45:46 - and so if we simplify this oops
45:50 - i don't know how to go back
45:55 - did that change yeah you can see it oh
45:57 - well um
46:00 - oh oh no
46:05 - this back button i'm just gonna like
46:09 - reopen it cool um
46:12 - so we've got happy
46:19 - happy equals 0.8 um
46:22 - and then we've got the minus 0.55 plus
46:25 - 0.04 so it's going to be like
46:27 - minus 0.51
46:30 - times stress and sorry i should have
46:34 - added it's 0.8 plus 0.12
46:38 - so this is going to be like 0.92 over
46:41 - here
46:46 - so it's going to be 0.92 minus 0.51
46:49 - times stress
46:50 - and then we can keep doing this for
46:52 - other values of free time so we can even
46:54 - do
46:54 - for free time equals two
46:58 - we're basically going to end up adding
47:00 - another
47:01 - .12 to this um
47:05 - to this intercept because we're going to
47:07 - end up with
47:08 - right one two times two instead of point
47:11 - one two times one in this equation so
47:15 - for free time equals two our equation is
47:17 - going to be happy
47:19 - equals point nine two plus another point
47:21 - one two which is going to be
47:24 - 1.04 and then we're going to
47:26 - also add another 0.04 onto the slope so
47:29 - it's going to be like
47:31 - minus 0.47
47:36 - right
47:40 - times stress and so we can do this for
47:43 - every value of free time
47:45 - we get a new equation every new equation
47:48 - has its own intercept and its own
47:52 - um sorry our its own
47:56 - intercept and its own slope
47:59 - and we can
48:02 - try to visualize it it's a little bit
48:06 - harder to visualize because
48:08 - now we have you know i only did integer
48:10 - values of this but like you could
48:12 - imagine
48:13 - there's a different line for every
48:14 - possible value of free time so you could
48:16 - also have like free time equals
48:17 - 1.5 theoretically you could also have
48:20 - free time equals negative
48:21 - something um but obviously that's
48:25 - not realistic um
48:28 - but here sorry i can't see you so
48:31 - sorry i could have you yeah back um
48:34 - okay so uh so we've got like
48:38 - infinitely many lines in here but we can
48:41 - visualize what some of them would look
48:43 - like i think i
48:45 - grabbed this and i will
48:48 - um i will really quickly talk through
48:51 - what this
48:52 - code is doing
48:58 - oh um i think
49:02 - in my initial
49:05 - i call this model q okay
49:08 - so here's three lines um
49:11 - those are the three lines for happiness
49:14 - or sorry for free time
49:16 - equals i think zero
49:19 - um three and six exactly um and so you
49:23 - can see what i'm doing is i'm modeling
49:25 - so for each line i'm taking the stress
49:28 - values and then i'm well i'm taking
49:31 - the intercept plus
49:34 - the um the coefficient on stress
49:38 - that's all i need for the the first line
49:42 - when um free time equals zero because
49:44 - remember those other two terms kind of
49:46 - go away
49:47 - but then uh for my line when
49:51 - uh free time equals three i'm basically
49:54 - like
49:54 - writing out this whole equation it's
49:56 - like this is my b0
49:59 - this is my b1 times the stress
50:02 - values and then this is my b2
50:07 - times three times um
50:11 - oh sorry this is my b2 times 3 which is
50:14 - the value of free time and then this is
50:15 - my
50:16 - b3 times stress times 3.
50:19 - so i'm basically plugging in a value for
50:21 - of 3 for free time
50:22 - into that equation in order to draw this
50:25 - line
50:26 - um and so that's like the second line
50:28 - and then um
50:30 - the third line for uh
50:33 - for free time equals six is this dark
50:36 - purple line and you see that they all
50:38 - have different intercepts
50:40 - and they all have different slopes
50:44 - cool um okay so that's
50:48 - uh that's it that we're going to cover
50:51 - for
50:52 - um interaction terms for right now
50:56 - um were there any questions that you
50:58 - wanted to cover
50:59 - yes i've been chatting with people in
51:00 - the chat so if you've seen me typing
51:03 - that's what i was doing uh let's see um
51:08 - uh there was one where did it go um
51:13 - what are the aic and bic criteria
51:17 - cool yeah so those we're gonna cover i
51:19 - think two live streams from now
51:21 - and like i said that that's all in this
51:24 - um
51:25 - in this course menu that's in the
51:28 - um choosing a linear regression
51:32 - model lesson you'll find
51:35 - lots of stuff about aic and bic
51:38 - uh here we go here's a exercise on it
51:41 - um so those will those will come up
51:44 - later
51:45 - i also see a question about fixed effect
51:49 - and random effect models um
51:52 - that is uh something so
51:55 - what we're doing right now is fitting
51:59 - fixed effect models um
52:02 - random effects are useful for
52:06 - different kinds of problems like i think
52:09 - the classic one is when
52:11 - you have data where the observations are
52:14 - not independent so like
52:16 - students within a school are not
52:18 - independent and so if like
52:20 - you have school as a predictor in your
52:22 - model um
52:23 - and also like individual students within
52:27 - each school
52:28 - um you want to account for those like
52:31 - that
52:32 - co-linearity or that correlation um
52:36 - i i don't think we currently have any
52:38 - content that covers that
52:39 - but um but yeah like a random effect is
52:43 - basically like
52:44 - a it's like an extra term in your model
52:47 - that can that is basically like random
52:50 - error
52:51 - um that's like us but the random errors
52:55 - are correlated within
52:57 - like say a school or something
53:01 - um lots of questions about office hours
53:03 - do you want to plug what we're doing
53:04 - there
53:04 - oh yeah so office hours we we did our
53:07 - first test run last week
53:09 - um and then we'll we'll have another
53:11 - office hours
53:12 - this week um we're doing them on discord
53:16 - and it's basically just a place where
53:19 - you can come and ask questions
53:21 - and we'll we have voice capabilities so
53:24 - we can like answer them
53:26 - speaking um you can also share our
53:27 - screen and show you stuff if
53:29 - if we need and um but it's also we also
53:32 - have a
53:33 - um like a chat in the
53:36 - on discord uh where you can post
53:39 - questions
53:40 - at any point and if you post your
53:42 - questions ahead of time
53:44 - um i think it's called yeah questions
53:45 - bank curriculum
53:47 - now um but we'll be checking that and
53:51 - keeping an eye on it
53:52 - we're not gonna respond like right away
53:54 - probably but if you have questions
53:56 - we'll look there before office hours so
53:58 - that we can make sure we get to anything
54:00 - that
54:01 - is asked ahead of time um
54:04 - cool um yeah we also have
54:07 - someone was saying in chat if we could
54:08 - do office hours directly after this
54:10 - stream that's something we weren't
54:11 - planning
54:12 - uh we weren't planning for this week but
54:14 - if folks would prefer that if you're
54:16 - here anyways watching the stream and and
54:18 - want like more time to chat and ask
54:20 - questions right away
54:21 - um we could potentially make that happen
54:23 - yeah for sure
54:24 - so that's good feedback um similarly it
54:27 - sounds like there's a cool python
54:29 - package that will let us like render
54:30 - math
54:32 - math equations quickly um so yeah more
54:34 - more feedback if uh
54:36 - like i like what sophie is doing with
54:37 - the ipad i think it's helpful to be able
54:39 - to like draw graphs and stuff but
54:41 - um if there are ideas that you have that
54:43 - uh
54:44 - would make this better um yeah
54:46 - definitely let us know cool
54:48 - all right i've got one more thing that i
54:49 - can show you guys
54:51 - since we have time um
54:55 - so last we
54:59 - another kind of flexible model that we
55:02 - can create
55:04 - is um using polynomial terms
55:07 - so i'll show this graph right here
55:11 - again a little bit made up but this is
55:14 - sleep
55:14 - versus happiness and so we see um i
55:18 - guess
55:18 - the um we'll do lm plots
55:35 - hold on i got this we got
55:38 - sleep happy
55:43 - and then what was the data called
55:47 - happiness
55:55 - all right let's try that okay so
55:58 - it's gonna fit this line
56:02 - um if we just leave it as fit rug equals
56:04 - true
56:05 - but you'll notice actually i'll add the
56:08 - like
56:08 - fit rug equals false
56:14 - re-run right you notice that if you were
56:17 - to try to like
56:18 - draw a line through these points
56:21 - you would say okay like as you get more
56:23 - sleep your happiness
56:25 - increases up to a point if you have more
56:28 - than 10 hours of sleep your happiness is
56:30 - starting to like decrease again
56:32 - and so we almost want to be able to fit
56:35 - like a curve to this
56:36 - instead of a straight line and with
56:39 - regular linear regression we don't have
56:41 - a way to do that
56:42 - so we can do it using polynomial terms
56:46 - and this is again a little bit of like a
56:49 - trick
56:49 - of almost adding another um
56:53 - another term into our model or another
56:56 - like column into our data
56:58 - that's based off of another um
57:03 - another column that already exists and
57:06 - uh and it basically allows us to
57:10 - have a model where we have like a
57:14 - squared term
57:15 - in the model or so like a higher order
57:18 - polynomial
57:19 - so i'm going to demonstrate this really
57:21 - quickly with
57:23 - this data um
57:29 - and we want so we want
57:33 - happy as a function of
57:36 - sleep this time and then
57:40 - um we're gonna add
57:43 - a i think it's like
57:49 - like this basically this is taking sleep
57:52 - to the second power let me just make
57:55 - sure
57:55 - that that's oh yeah that's right okay
57:59 - so we're just saying okay i want to add
58:01 - sleep squared
58:03 - into my model and then fit this
58:06 - and then print this out let me get
58:10 - model p for polynomial and then
58:14 - oh model yeah
58:17 - um and then you'll see that we get a
58:21 - intercept we get a slope on sleep and
58:24 - then we get a slope on
58:25 - sleep squared and i'm not going to
58:28 - switch
58:29 - to the ipad now because we're we're kind
58:31 - of running out of time but i'll i'll
58:32 - actually do this like
58:34 - mathematically i'll write it out in
58:35 - python
58:37 - so um so basically what this is doing
58:42 - is it's creating a new equation where we
58:46 - have sleep squared
58:48 - in our model um so let's like
58:51 - i guess save these as b0 b1 and b2
58:54 - so model.params
58:59 - zero so like the first thing is the
59:02 - intercept that's our b0
59:04 - b1 is the second
59:08 - thing in this model so that's at index
59:10 - one
59:12 - and then b2
59:15 - is at index 2.
59:19 - so i'm just saving i'm just saving these
59:21 - numbers as b0
59:23 - b1 and b2
59:26 - i'm gonna also just save the um the
59:29 - values of sleep
59:30 - as sleep instead of like happiness
59:34 - dot sleep because it's a column in my
59:36 - happiness data set
59:38 - i'll just save it separately as sleep
59:41 - and then i'm gonna say like predicted
59:45 - happiness based on this model
59:48 - is now equal to b zero
59:53 - plus b one times sleep
59:58 - plus b2 times
60:01 - sleep squared um so i can do that as
60:04 - like sleep
60:06 - um star star 2 that will square
60:10 - all the values and sleep i could also do
60:12 - like numpy dot power
60:15 - sleep comma two um but basically right
60:18 - this is like all the values in this
60:19 - column
60:20 - square and if i plot this
60:24 - now and the result there is going to be
60:26 - a column right because sleep is a full
60:28 - column so
60:29 - you're yeah you're doing that for every
60:31 - value in this sleep column you're doing
60:33 - that equation
60:34 - exactly in fact i can also just show
60:37 - like what this looks like
60:41 - i'm so run this so i have them saved and
60:43 - then yeah
60:44 - basically like this is every value
60:47 - in in
60:50 - sleep squared and then multiplied by b2
60:54 - i guess we can just do that
60:56 - um this is every value in the sleep
60:57 - column squared
61:00 - so okay so i'm just creating these
61:03 - predicted happiness scores
61:05 - and then what i can do is i can add it
61:08 - to this picture
61:10 - so we can see what it looks like
61:14 - and i'm going to um
61:18 - see that plot sleep and
61:21 - predicted happiness based off of this
61:23 - model
61:25 - and i'll show it
61:29 - [Music]
61:32 - i guess actually
61:38 - what is a better way to do this um
61:41 - i guess now i'm realizing this is not
61:44 - pretty because
61:45 - uh it's like drawing the line between
61:47 - all the points when we're doing it on a
61:48 - line
61:49 - all of these were like still on top of
61:51 - the line
61:52 - um so let's see we can just do like lin
61:55 - space
61:57 - um or we can do like
62:00 - sleep equals so sleep is roughly between
62:05 - 0 and 14. so let's instead of
62:09 - grabbing this actual column we can do
62:11 - like
62:14 - is it in numpy so you just want like a
62:18 - list of values from
62:20 - zero to 14. yeah
62:24 - um by 5.5
62:29 - 4.1 here let's just
62:32 - test this really fast
62:45 - thank you here actually i can
62:51 - demo this even though we're inside that
62:54 - linspace
62:55 - and then start stop
62:58 - and then
63:04 - oh so then it's just like the third
63:08 - parameter is the number of values so
63:10 - we'll do like 100 values
63:12 - and that should give me yeah okay sorry
63:15 - guys
63:16 - but hopefully this is helping you see
63:18 - like how we do this in real life
63:20 - and so here we're not using the actual
63:22 - sleep data we're making up a bunch of
63:24 - fake points just to draw what this line
63:26 - looks like
63:27 - right and then yeah
63:30 - so that's a little nicer now we see that
63:34 - that polynomial term allowed us to
63:36 - create this like kind of
63:38 - curve um in our line cool
63:41 - all right well quick question was so
63:43 - when we did um
63:44 - sleep times exercise that was the
63:46 - interaction term and if we're doing this
63:48 - sleep squared is that called the
63:49 - polynomial term it is a polynomial term
63:52 - cool yes and we could even add sleep
63:55 - cubed
63:56 - and sleep to the fourth we can add more
63:58 - and more polynomial terms but
64:00 - again like we probably don't want to if
64:03 - we added
64:04 - another if we added like a cube term
64:07 - that would allow us to get like a
64:09 - another curve in the data or in the line
64:12 - basically
64:13 - um but then i think there's
64:16 - there's some really good examples of
64:18 - this if you just like add lots and lots
64:20 - of polynomial terms you could have like
64:21 - a very squiggly line which is a good
64:24 - example um
64:26 - of overfitting in action
64:29 - cool cool um sophie you're going to be
64:32 - gone for office hours
64:34 - this week yeah i might sign on though
64:36 - okay
64:37 - if i can um i might be i might be gone
64:40 - for office hours though
64:41 - but i'll be back next week and i'll keep
64:43 - an eye on this yeah i'll i'll be in the
64:45 - office hours i
64:46 - am not the stats person that sophie is
64:48 - so i don't know if i'll be able to
64:49 - answer your
64:50 - stats questions but i could probably get
64:53 - a niche
64:54 - yeah we'll find someone yeah that knows
64:56 - what they're talking about andrea
64:57 - actually wrote
64:58 - all this this lesson okay maybe i can
65:00 - get hurt
65:01 - cool all right cool well
65:04 - awesome i'm glad that our first uh
65:07 - stream back in the office
65:10 - i love this green screen and the fact
65:12 - that we're just like
65:14 - yeah yeah i don't know if you noticed
65:15 - but i shrunk us uh in the middle of the
65:17 - stream to uh that's good yeah i like it
65:20 - like tiny people awesome
65:23 - well thank you everyone for coming and
65:25 - if you have questions ask us on discord
65:27 - or
65:27 - ask in the comments on the youtube video

Cleaned transcript:

um i'll read it out loud it says hi um what i'm wondering most is if the discussion here will have differences with the lessons on linear regression in codecademy that is a good question um there will be some differences just in that this is more of an open discussion and so if there's questions in the chat or there are specific things um that anybody wants to talk about uh we might go on some tangents and um and cover some things that are not in the lesson but for the most part we're gonna start by uh going through the the same content that's in the lesson yeah cool cool we are back in the office with uh our fancy streaming setup so we're trying out like new mics and cameras all that kind of stuff so let us know in the chat if anything is screwed up if you can't hear us if the mic sounds bad um but uh hopefully everything is uh is working fine yeah all right shall we get started let's do it all right so um the topic for today is going to be um interaction terms and polynomial terms which basically are ways that we can alter a linear regression a simple linear regression model or a multiple linear regression model to make it a little bit more flexible and handle some different things that we might see in some data so um i actually think this is exactly the same to the question that was asked earlier this is exactly the same data set that we give you in the lesson on codecademy and i believe it is simulated i didn't write this lesson um but the idea behind this this data is that we're imagining um some sort of study that measures selfreported levels of happiness self reported or maybe like measured levels of stress um whether or not a person exercises um i think sleep is probably hours of sleep make sense 13.8 it's a little extreme maybe and then maybe hours of free time per day i think that's the intention of this data set it's i'm pretty sure simulated though so not not real cool so um what we're gonna get started with is uh visualizing some of this data um you'll notice actually in the lesson on codecademy and i just grabbed this code directly from there you'll notice that we actually use this lm plot function to do a lot of the plotting in this in this lesson the reason for that is partially because uh we haven't updated uh matplotlib and seaborn in a little while so this is an older function in seabourn that does the same thing as the newer scatter plot function um but also it has the option to fit a regression line directly onto the plot if you wanted to um and so i'll just demo that really quickly um here actually i'll demo it in a separate plot so if i say let's look at the relationship between stress and happiness so is there a relationship between how stress someone appears to be and how happy they report being um and i say fit reg equals true and i think that's also the default and i run this i get here are the points and then it'll automatically draw this regression line for me which is kind of cool and it allows you to do this a little do this plotting a little bit more quickly um which is which is always fun there you'll notice that something interesting happens actually if i leave this is fit rug equals true and add q equals exercise right so now it's going to even be a little bit hard to visualize right because there's uh two variables there so three variables right there stress happiness and exercise yeah um it it's pretty smart actually it will do something here actually draw me two regression lines one for exercise equals one and the other for exercise equals zero alex do you notice anything weird about this plot though that's like different from what we might expect based on everything we've covered so far like do you remember i think maybe it was the last stream you joined me on where we started talking about um using this using a categorical variable in our regression and we we plotted um some regression lines when we included a categorical variable in the regression right and the the two lines had something different from what we're seeing here um you remember no i don't recall okay so um so actually the two lines were parallel so um there was no way previously or we didn't have a way previously to draw um to draw regression lines that were like totally just fitting the orange points and the blue points separately we could only change the intercept so if you go back you watch the older video you'll see that in action um and today we're gonna actually learn how to fit these lines um so that they can have different slopes yeah and in that graph is are these lines so say you took out all of the blue circles um and just fit it to the orange axes is that the orange line and similar to the other way around so they're completely independent of each other they're not um they're basically ignoring the other set of points um if you fit yeah if you fit just a regression for the orange line or orange points and just regression for the blue points you'd get these lines okay cool um so i'm gonna now just show you this plot uh without the line i lines i think i kind of gave away via the end goal before we even got started so we'll start back at the beginning and we'll motivate this by saying sometimes when we look at a plot of two quantitative variables and then we color by categorical variable we'll see that if we were to draw a line through just the blue points and if we were to draw a separate line through just the orange points those points would or those lines would probably have different slopes and different intercepts whereas up to this point if we just added um so if we just fit a model where we did like happiness as a function of stress and exercise we'd end up with a model that allows us to draw separate lines for these two sets of points but they would have to have the same slope they could just have different intercepts and we'll i'll show you on the ipad in just a second mathematically why that happened and then i'll show you how we can now fit a regression that visually would look like this cool cool so um what i'm gonna do is i'm going to here let's add a um let's add a chunk and let's fit this regression right off the bat um i'm gonna grab this code from over here and we're gonna talk through it for a second and then we'll come back to this in a minute okay so i'm fitting this model i believe i've already loaded yes um so i'm fitting this model and it's happiness as a function of stress plus exercise plus stress colon exercise now for a second let me remove this this is what we've done so far in previous um live streams so if i run this this is kind of what we've seen before we get an intercept we get a slope for stress and we get a slope for exercise same intercept so these lines aren't parallel it's same intercept and different slopes so this these lines actually are parallel so actually this is a good a good place to kind of like come back to this i hadn't initially planned on it but i think this is like a good reminder so um let me actually you can flip me over but um can we leave that up yes okay great so this is for the um the other one that had the stress colon exercise but i'm just gonna write down the um the values for the other regression for a second so for this one we had an intercept of 10 about we had a slope on stress that was equal to negative 0.7 and then we had a slope on exercise that was equal to about negative 0.9 and we fit this model it was happy as a function of stress plus exercise and what that meant was that we were fitting this model we fit something that looked like happy equals intercept which we called b0 but we'll replace that plus b1 times stress plus b2 times exercise it's gonna abbreviate and so plugging in b0 b1 and b2 we got something like happy equals 10 plus well it's really uh plus negative so minus 0.7 times stress minus 0.9 times exercise but remember that there are only two possible values of exercise someone can exercise or not exercise got it so we ended up with two separate equations one when exercise equals zero we get happy equals 10 minus 0.7 times stress minus 0.9 times 0 but 0.9 times 0 is 0. so that kind of goes away and then the other one we got right down here when exercise equals 1 we get happy equals 10 minus 0.7 times stress minus 0.9 times 1 which is just 0.9 and so then we end up with happy equals um we can take the 10 and subtract the 0.9 right because order of operations we can all of these addition and subtraction things can happen in any order so we end up with like 9.1 minus 0.7 times stress got it so same slope different intercepts and so they both have the same slope on stress but the intercept is 10 when exercise equals zero and the intercept is 9.1 when exercise equals one and so that slope on exercise is really the difference in intercepts for the two lines that we would draw oh i'm now realizing that this is that we are covering the uh oh can't darn it sorry guys oh alex i don't even know it's there all right we'll figure it we're figuring out the green screen sorry guys um hopefully that was clear though uh okay so now let's go back to that second um let's can we switch back to the uh that's just okay yes um okay so that was when we just fit this where we have happy as a function of stress plus exercise but now we're gonna add an interaction term and the way we can do this in stats models is we can just say i want an interaction between stress and exercise by putting a colon between them so now when we run this model run it um now we get an intercept we get a slope on stress and then we also get a slope on exercise just as before but we get now a third slope slope on stress colon exercise so now we're going to take this and we'll go back to the ipad in just a second and talk through what those sorry i'm erasing really quickly um talk through what those translate to in terms of our model okay so now oops well that's okay um so now we see okay we had intercept stress exercise stress colon exercise all an interaction term is is an extra it's almost like having an extra column in your data that is the product of two other columns so it's almost like an extra feature that is just the product of two others so the way this looks like in terms of the model is we're fitting now happy screwed up let's see so now we're fitting happy equals b0 plus b1 times stress plus b2 times exercise plus b3 times stress times exercise times sign okay okay so in like for example in um scleren you have to create a um an interaction term yourself in your data frame and literally what you would do and we could demonstrate this after is you would just multiply everything in the stress column by everything in the exercise column and create a new column of your data frame called stress exercise and then you would just add that into your model as a predictor i think i'm a little bit confused of like what the overall goal is here because so in in this situation isn't this gonna still result in two parallel lines because that times one the time zero is still going to be um i can't even uh that's not pointing at anything it's still gonna be right when when exercises is equal to 0 the b2 times exercise and b3 times stress times exercise all of that is just going to go to zero so isn't this really similar to the last thing that we just looked at it's very similar but let's actually walk through exactly that so let me plug in all of these numbers so we've got happy equals our b0 is 12 that's still our intercept um then we've got b1 it's negative so we're gonna do b1 is negative 0.97 so minus 0.97 times stress and then again the negative sign minus 3.1 times exercise and then we've got plus .36 times stress times exercise okay now let's break this up into two separate um two separate equations feel like this eraser never works the way i wanted to um okay so let's break it up into two separate ones one where exercise equals zero and one where exercise equals one so i think you alex were doing the exercise equals zero one in your head first so um it's a little simpler so let's do that first so when exercise equals zero we've got happy equals 12 minus .97 times stress and then we've got minus 3.1 times zero plus .36 times stress times zero now like you were saying right anything times zero is zero so 3.1 times 0 is 0 but also 0.36 times stress times 0 even though we're multiplying stress in there it's still like a bunch of things multiplied by zero so that's also zero okay so that's zero that's zero so our equation is really just happy equals 12 minus point nine seven times stress okay so we've got our line again for exercise equals zero now let's try exercise equals one this is where it gets a little bit more complicated yeah so we've got um and also if anybody has questions as we go i know this is like again a little bit of math um so ask us questions if they come up but for exercise equals 1 we've got happy equals 12 minus 0.97 times stress just like before then we've got um minus 3.1 times 1 and then we've got plus 0.36 times stress times one got it okay so multiplying by one doesn't change the value of anything right so we can do the same thing as before with our with our intercept rate so like this minus 3.1 times 1 we can just add to the 12 again order of multiple order of operations since this is just like multiplication and addition we can order we can add or subtract these terms in any order so we can combine these two together so i'm going to do that first and then i'll write out the other set but some of you might already kind of see what that's going to be so we've got happy equals and then 12 minus 3.1 is like 8.9 right and then we've got minus .97 times stress and then we also have plus .36 times stress and then i'm gonna just drop the one because anything times one is just equal to itself and now we've got we've got minus 0.97 times stress plus 0.36 times stress and so both of these things are multiplying stress so we can actually factor stress out of this like piece right here so stress is multiplying both things we can factor it out and add the 0.9 minus 0.97 and 0.36 together and so what we get is something that looks like this we get happy equals 8.9 and then we've got um i guess i'll put like plus stress times and then we've got the minus 0.97 plus 0.36 oops 0.36 and that um again like thinking about how you would calculate this or like how you would expand this right you would multiply stress by both things inside the parentheses so you'd get like you'd get back to what you had above you'd have the stress times minus 0.97 that's this term from before and then you'd get the stress times .36 which is this term so we just factored it out but we rewrote that same thing as before and then minus 0.97 plus 0.36 uh 61. yeah um is yeah so we got happy equals 8.9 plus 0.61 or sorry minus 0.61 i guess i shouldn't have switched the order but you can multiply in either order so um okay so that's our equation for exercise equals one that's our equation for exercise equals zero and you'll see that they have different intercepts 8.9 versus 12 and they have different slopes minus 0.61 versus minus 0.97 cool cool and so right the change in slope came from this term that included both stress and exercise that was the thing that ultimately affected the slope when exercise was one exactly um can you briefly describe again how the 0.36 was calculated from the original data right so these are these numbers that are up in the top left corner where do where do those even come from oh so these numbers came from fitting the model um so we'll go back to the jupiter notebook in a second um but basically when we fit the model with that interaction term it calculated a slope on the the interaction between stress and exercise and that is basically like a multiplier right of stress times exercise um so then we just followed that out in the model i also see a really good question in here about it says would we consider the third variable stress times exercise dependent on the values of stress and exercise that you get more accurate models when all your feature variables were independent so that's a really good question i'm not sure how great of an answer i can give but um all to say so yes you you don't want to have uh colinear very or you don't want to have highly correlated variables in um in a model together because they're both if they're both explaining your outcome variable then the model is going to have some trouble estimating the coefficients for both of them because they're both like they both have the same explanatory power in terms of your outcome variable um and it also right if they're highly dependent like if you just take um if you take one of the columns and multiply it by a number theoretically you shouldn't be able to fit that model at all because it's perfectly your new variable is perfectly correlated with your old variable in this case it's not perfect it's not like this new variable that we created by multiplying stress and exercise it's not perfectly correlated with either stress or exercise because we're multiplying by a different value for each so like if you think just about stress whether you multiply it by zero or multiply by one depends on the value of exercise so there's no way you could just take that column without seeing the exercise column and and get the new one right it's not like if you were just doubling stress that would be perfectly correlated with stress but there's um yeah it's either you're sometimes multiplying it and sometimes not depending on if exercise is zero or one yeah but on the flip side there are reasons why you might not want to just create like the most complicated model that you can with as many extra predictors as like this as you possibly could create because you could imagine like if you have i don't know 10 columns of a data set which is even in real life that's like pretty small um you could imagine you could create an interaction term for every possible pair of pair of variables basically pair of features and that would be a very large number that i i don't know what it is but yeah but that would be a very large number and you could create a very complicated model to model like every possible interaction um but that's not always a good idea because you end up what ends up happening is you end up over fitting and that's kind of actually a very important topic in a newer course that we're building right now on feature engineering um and like regularization um but basically if you if you fit your data too well then you're not your model is not going to be very good at predicting outcomes for new data so like this this model if i make it super complicated might perfectly fit my my data that i collected on my like 100 patients or whatever but if i go out and i find another patient like this might not be a good way of explaining the relationship between these things right because you were like so dependent on those original 100 patients exactly um can you describe just like the total takeaway or upshot of of this whole process because what i'm walking away from this is before we were able to draw two lines that were parallel that kind of stinks for some reason and now we can draw two lines that aren't parallel like what is what is like the thing that i should be taking away from creating this uh this new variable will you go back to the the picture or the um yeah the browser the browser yeah so that's a really good question i think so a couple of things first of all um it's just you know if you were to plot your data and you see this picture maybe you see something even more obvious like maybe you see these lines look completely different like maybe you even see that for um people who do exercise the relationship between happiness and stress is this like negative relationship like people who are less happy are more stressed but for people who don't exercise maybe it's like a positive relationship and so what you actually see sometimes and this is sometimes i think referred to as simpsons paradox and we actually have here i can go and then um show you in the lesson as a note so we're using new uh streaming software and your mouse pointer isn't showing up on the uh on the software so if you're ever gesticulating with the mouse uh that's not getting backed up good to know something we can uh play around with is it oh it might not be in this lesson which lesson is it in um let me see if i can find it quickly i think it's in this lesson um yes so this is like a more complicated example here let me like pull this up um but you'll see right like in this in this example we've got um like a positive if we were to just like ignore this um categorical variable and draw a line through all the points we would draw this black line which is like this positive relationship but then if you add this um this like categorical variable to the model and you're trying to draw the lines now based off of um you're basically trying to draw the lines through each set of points in this example they're parallel so like we're not constructing but you'll see right that like the um the lines have negative slopes even though the initial relation like the relationship between these two variables in among all the points is positive if you zoom in you now are getting these like negative relationships and so you can see stuff like that where like maybe right maybe one of these relationships is negative and the other one is positive and if you didn't fit anything like you didn't fit um any sort of exercise term like you'd end up with a flat line you'd think there's like no relationship between happiness and stress but like once you add this the model you suddenly start seeing them but now if you are restricting this so that those lines have to be parallel then you're like ending up with you're gonna end up with maybe like two parallel lines that are flat and that's like not gonna model your data super well so there's definitely situations where adding an interaction term means that you significantly improve your model from where like you basically can't explain anything to um oh it looks like i didn't we didn't switch over maybe um that's possible oh sorry yeah you're good um okay uh but yeah so sometimes it significantly improves your model i think like the flip side or the other thing that maybe is like the other half of the answer to that question is and maybe this gets a little bit more into like when we do this um is we do this when we think that the relationship between to the outcome variable and one of our features is moderated by or like influenced by some other factor um so in this example uh it looks like right the relationship between stress and happiness is maybe like less steep here i'll do it with the lines um is less steep for people who do exercise so like maybe people who are exercising are more like immune to stress in general and um and so like being more stressed has less of an effect on their happiness than people among people who don't exercise it's a steeper line and so we're saying that like we think that you know more stress has like more of a effect i say in quotes because like we're not really depending on how the studies run we don't know if it's like a causal relationship but the effect of stress on happiness is more significant um among people who don't exercise and so we think like there's some exercises moderating this relationship and that's why we might think like we want to we want to be able to model that in some way this is maybe a tricky question or uh this may be a tricky question but like how do you do this in the real world of like so you just described oh we have this situation where one set of data points has like a negative slope one has like a huge positive slope we want to be able to draw these two lines with different slopes in order to capture that and so we want to do something like this but do you know that by plotting the data and looking at it do you like just give both things a shot and like see and see what the models look like after after trying to add it or trying not to add it like how do you actually do this with a data set that you don't know anything about that is a great question um so you can certainly and i highly recommend in fact i'm working right now on some content on exploratory data analysis and it's definitely a good idea to before you fit a model take a look at some of these relationships and try to see whether maybe you have hypotheses like maybe you know something you have some prior research about the relationship between these all of these features and you have some hypotheses about what might be moderating what relationships and then maybe you get some data and you want to take a look at it before you try to fit the model and so that's one thing you would do um the other thing you can do which is more like post talk is you can um you can compare different models so you could fit it both ways and then you could see um you know like the simplest thing is you might calculate like a mean squared error like basically um or a or a total squared error or whatever um basically like how far off are all these points from their respective lines and you could calculate that for both models the one where you don't include the interaction and the one where you do um and then you could say like okay i see that when i add this term to the model um it significantly improves its ability to explain this data um and you'll actually see if you're following along in this linear regression course um there is a a section here on choosing a linear regression model that starts to cover some of those methods that you could use to compare two different models once you fit them um to get a sense for whether whether both of them make sense or whether one makes sense over the other then there's also additional additional things you can do like regularization which nithya is working on right now um that can even further like help you basically like fit the model at the same time as estimating whether like those parameters are useful um so cool that was a bit of a tangent but hopefully um i see a question how do you do feature selection in linear regression i've heard a forward and backward elimination methods but don't know what they do um again this is like a plug for all the content that nitia is currently working on um and that will be i think going live at the end of the summer early fall um but essentially so forward and backward elimination are essentially methods for like sequentially adding um terms to your model so like maybe you want to sequentially add different interaction terms and reevaluate the model every time and then if adding the term improves the model you keep it if adding it doesn't improve the model then you don't keep it and you try something else or you start with like all of them in the model and you sequentially like delete things that are not helping you um that would be backwards elimination and then regularization is essentially a way that you can fit this model um with like an extra term so that you allow some of the coefficients to get really small or even go to zero um and that's kind of a way of like eliminating eliminating those uh from your regression um okay and then suppose i have data can i add two extra decimals on point five six oh you can add as many decimals as you want to run the linear regression um yeah as much i mean i don't know if that's that is that's not the question uh yeah the question is saying like oh if i have this data can i add can i add or the way that i'm interpreting this question is can i add points between the my existing points in like regular steps of what can i add 0.5611.56 like can i add data that falls along that line i think that that's what the question is asking but if you want to clarify the question um so i guess like basically i think you can't add data if you're trying to answer a question about some data or like fit a model to some data then you probably don't want to alter your data before you fit the model um but yeah i mean if it's like something that you could collect so like if these are values of stress and you don't have anyone in your data set who reported like a value of 0.5611 for stress but you can go out and find someone and you can record their happiness score then you could add that to the model if you don't have or you could add that to your data to fit the model if you don't if you can't find someone with that value you shouldn't just like add that to your data set before you hit the model um i got it and the clarification in that question of if 0.56 is occurring multiple times can i add extra decimals to it so i still don't fully understand but yeah if you see multiple people with this value of 0.56 and you are able to measure more specific values um then yeah like if you are able to measure to the fourth decimal place but most like measurement tools have some limits um i also see a question about heteroscedasticity um which is a fun word i think um that is coming in the next live stream so yeah we'll come back to that next week um cool so i think uh we've got like 20 minutes left i want to cover two more things if we if we can fit them in um so one thing i'll kind of go over a little bit more quickly but i will i will pull this over and rerun in our other notebook um so i just want to demonstrate so up to this point we have shown that we can fit this interaction term with a categorical variable um as one of the terms that we're interacting so in that case we ended up with just two lines because there are only two values of exercise so you basically get like a separate line for each value of exercise um but we can also fit interaction terms with two quantitative variables so here's another plot it shows the relationship between happy and stress and then we've colored by this value or this other variable free time which is like the number of hours of free time that you have um and this is also a quantitative variable we're just showing values uh like integer values for the the key but you could imagine this is like a scale where people that have more free time are darker colored dots and people that have less free time are lighter colored dots in this plot and you could imagine like i i can't do this super easily in my head but you can imagine if you just isolated all the people with five hours of free time you might draw oh i am remembering now that you can't see my my pointer but right so if you uh just isolated all people all this like the darkest purple dots the line that you draw through those might be different from the line that you draw through the lightest color dots to people with zero hours of free time and so um so we can fit this for uh this setup as well so i'm gonna go ahead and grab this model again and we'll edit it a little bit so this time let's edit this let's do stress uh plus free time plus stress interacted with free time and then let's fit that and we can fit something again we see we get our intercept slope on stress slope on free time and also a slope on free time colon strap or stress colon free time um and i'll just write this out super quick on our ipad again um okay cool so here we've got those written out for ourselves and let's write out what this model is super fast so the model is exactly the same as what we had before we've got like happy we've got happy equals b0 which is eight plus um and again with the minuses sorry minus point five five times stress plus point 0.12 times free time and then plus 0.04 times stress times free time i'll abbreviate ft okay and then you can see for different values of free time we get different equations so for example when free time equals zero our equation is just like happy equals eight minus point five five times stress and these two other terms go to zero because point one two times zero is zero and .04 times stress times zero so zero and then when free time equals one we get happy equals eight minus point five five times stress and then plus 0.12 times 1 which is just 0.12 and then plus 0.04 times stress times 1 which is just 0.04 times stress and so if we simplify this oops i don't know how to go back did that change yeah you can see it oh well um oh oh no this back button i'm just gonna like reopen it cool um so we've got happy happy equals 0.8 um and then we've got the minus 0.55 plus 0.04 so it's going to be like minus 0.51 times stress and sorry i should have added it's 0.8 plus 0.12 so this is going to be like 0.92 over here so it's going to be 0.92 minus 0.51 times stress and then we can keep doing this for other values of free time so we can even do for free time equals two we're basically going to end up adding another .12 to this um to this intercept because we're going to end up with right one two times two instead of point one two times one in this equation so for free time equals two our equation is going to be happy equals point nine two plus another point one two which is going to be 1.04 and then we're going to also add another 0.04 onto the slope so it's going to be like minus 0.47 right times stress and so we can do this for every value of free time we get a new equation every new equation has its own intercept and its own um sorry our its own intercept and its own slope and we can try to visualize it it's a little bit harder to visualize because now we have you know i only did integer values of this but like you could imagine there's a different line for every possible value of free time so you could also have like free time equals 1.5 theoretically you could also have free time equals negative something um but obviously that's not realistic um but here sorry i can't see you so sorry i could have you yeah back um okay so uh so we've got like infinitely many lines in here but we can visualize what some of them would look like i think i grabbed this and i will um i will really quickly talk through what this code is doing oh um i think in my initial i call this model q okay so here's three lines um those are the three lines for happiness or sorry for free time equals i think zero um three and six exactly um and so you can see what i'm doing is i'm modeling so for each line i'm taking the stress values and then i'm well i'm taking the intercept plus the um the coefficient on stress that's all i need for the the first line when um free time equals zero because remember those other two terms kind of go away but then uh for my line when uh free time equals three i'm basically like writing out this whole equation it's like this is my b0 this is my b1 times the stress values and then this is my b2 times three times um oh sorry this is my b2 times 3 which is the value of free time and then this is my b3 times stress times 3. so i'm basically plugging in a value for of 3 for free time into that equation in order to draw this line um and so that's like the second line and then um the third line for uh for free time equals six is this dark purple line and you see that they all have different intercepts and they all have different slopes cool um okay so that's uh that's it that we're going to cover for um interaction terms for right now um were there any questions that you wanted to cover yes i've been chatting with people in the chat so if you've seen me typing that's what i was doing uh let's see um uh there was one where did it go um what are the aic and bic criteria cool yeah so those we're gonna cover i think two live streams from now and like i said that that's all in this um in this course menu that's in the um choosing a linear regression model lesson you'll find lots of stuff about aic and bic uh here we go here's a exercise on it um so those will those will come up later i also see a question about fixed effect and random effect models um that is uh something so what we're doing right now is fitting fixed effect models um random effects are useful for different kinds of problems like i think the classic one is when you have data where the observations are not independent so like students within a school are not independent and so if like you have school as a predictor in your model um and also like individual students within each school um you want to account for those like that colinearity or that correlation um i i don't think we currently have any content that covers that but um but yeah like a random effect is basically like a it's like an extra term in your model that can that is basically like random error um that's like us but the random errors are correlated within like say a school or something um lots of questions about office hours do you want to plug what we're doing there oh yeah so office hours we we did our first test run last week um and then we'll we'll have another office hours this week um we're doing them on discord and it's basically just a place where you can come and ask questions and we'll we have voice capabilities so we can like answer them speaking um you can also share our screen and show you stuff if if we need and um but it's also we also have a um like a chat in the on discord uh where you can post questions at any point and if you post your questions ahead of time um i think it's called yeah questions bank curriculum now um but we'll be checking that and keeping an eye on it we're not gonna respond like right away probably but if you have questions we'll look there before office hours so that we can make sure we get to anything that is asked ahead of time um cool um yeah we also have someone was saying in chat if we could do office hours directly after this stream that's something we weren't planning uh we weren't planning for this week but if folks would prefer that if you're here anyways watching the stream and and want like more time to chat and ask questions right away um we could potentially make that happen yeah for sure so that's good feedback um similarly it sounds like there's a cool python package that will let us like render math math equations quickly um so yeah more more feedback if uh like i like what sophie is doing with the ipad i think it's helpful to be able to like draw graphs and stuff but um if there are ideas that you have that uh would make this better um yeah definitely let us know cool all right i've got one more thing that i can show you guys since we have time um so last we another kind of flexible model that we can create is um using polynomial terms so i'll show this graph right here again a little bit made up but this is sleep versus happiness and so we see um i guess the um we'll do lm plots hold on i got this we got sleep happy and then what was the data called happiness all right let's try that okay so it's gonna fit this line um if we just leave it as fit rug equals true but you'll notice actually i'll add the like fit rug equals false rerun right you notice that if you were to try to like draw a line through these points you would say okay like as you get more sleep your happiness increases up to a point if you have more than 10 hours of sleep your happiness is starting to like decrease again and so we almost want to be able to fit like a curve to this instead of a straight line and with regular linear regression we don't have a way to do that so we can do it using polynomial terms and this is again a little bit of like a trick of almost adding another um another term into our model or another like column into our data that's based off of another um another column that already exists and uh and it basically allows us to have a model where we have like a squared term in the model or so like a higher order polynomial so i'm going to demonstrate this really quickly with this data um and we want so we want happy as a function of sleep this time and then um we're gonna add a i think it's like like this basically this is taking sleep to the second power let me just make sure that that's oh yeah that's right okay so we're just saying okay i want to add sleep squared into my model and then fit this and then print this out let me get model p for polynomial and then oh model yeah um and then you'll see that we get a intercept we get a slope on sleep and then we get a slope on sleep squared and i'm not going to switch to the ipad now because we're we're kind of running out of time but i'll i'll actually do this like mathematically i'll write it out in python so um so basically what this is doing is it's creating a new equation where we have sleep squared in our model um so let's like i guess save these as b0 b1 and b2 so model.params zero so like the first thing is the intercept that's our b0 b1 is the second thing in this model so that's at index one and then b2 is at index 2. so i'm just saving i'm just saving these numbers as b0 b1 and b2 i'm gonna also just save the um the values of sleep as sleep instead of like happiness dot sleep because it's a column in my happiness data set i'll just save it separately as sleep and then i'm gonna say like predicted happiness based on this model is now equal to b zero plus b one times sleep plus b2 times sleep squared um so i can do that as like sleep um star star 2 that will square all the values and sleep i could also do like numpy dot power sleep comma two um but basically right this is like all the values in this column square and if i plot this now and the result there is going to be a column right because sleep is a full column so you're yeah you're doing that for every value in this sleep column you're doing that equation exactly in fact i can also just show like what this looks like i'm so run this so i have them saved and then yeah basically like this is every value in in sleep squared and then multiplied by b2 i guess we can just do that um this is every value in the sleep column squared so okay so i'm just creating these predicted happiness scores and then what i can do is i can add it to this picture so we can see what it looks like and i'm going to um see that plot sleep and predicted happiness based off of this model and i'll show it i guess actually what is a better way to do this um i guess now i'm realizing this is not pretty because uh it's like drawing the line between all the points when we're doing it on a line all of these were like still on top of the line um so let's see we can just do like lin space um or we can do like sleep equals so sleep is roughly between 0 and 14. so let's instead of grabbing this actual column we can do like is it in numpy so you just want like a list of values from zero to 14. yeah um by 5.5 4.1 here let's just test this really fast thank you here actually i can demo this even though we're inside that linspace and then start stop and then oh so then it's just like the third parameter is the number of values so we'll do like 100 values and that should give me yeah okay sorry guys but hopefully this is helping you see like how we do this in real life and so here we're not using the actual sleep data we're making up a bunch of fake points just to draw what this line looks like right and then yeah so that's a little nicer now we see that that polynomial term allowed us to create this like kind of curve um in our line cool all right well quick question was so when we did um sleep times exercise that was the interaction term and if we're doing this sleep squared is that called the polynomial term it is a polynomial term cool yes and we could even add sleep cubed and sleep to the fourth we can add more and more polynomial terms but again like we probably don't want to if we added another if we added like a cube term that would allow us to get like a another curve in the data or in the line basically um but then i think there's there's some really good examples of this if you just like add lots and lots of polynomial terms you could have like a very squiggly line which is a good example um of overfitting in action cool cool um sophie you're going to be gone for office hours this week yeah i might sign on though okay if i can um i might be i might be gone for office hours though but i'll be back next week and i'll keep an eye on this yeah i'll i'll be in the office hours i am not the stats person that sophie is so i don't know if i'll be able to answer your stats questions but i could probably get a niche yeah we'll find someone yeah that knows what they're talking about andrea actually wrote all this this lesson okay maybe i can get hurt cool all right cool well awesome i'm glad that our first uh stream back in the office i love this green screen and the fact that we're just like yeah yeah i don't know if you noticed but i shrunk us uh in the middle of the stream to uh that's good yeah i like it like tiny people awesome well thank you everyone for coming and if you have questions ask us on discord or ask in the comments on the youtube video
