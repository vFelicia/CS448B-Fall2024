With timestamps:

00:23 - all right i think we're good to go
00:25 - hey hello
00:30 - um okay i think we will get started in
00:34 - just a minute
00:35 - but i see that some people are starting
00:38 - to say hello in the youtube chat
00:40 - uh thank you dr monkey uk for always
00:44 - letting us know that you can see
00:46 - us see we're getting better at the
00:48 - awkward silence at the beginning
00:50 - yeah we just really leaned into the
00:51 - awkward silence this time
00:54 - i know i have no idea until after i
00:57 - watched this later how
00:58 - long that was cool
01:02 - um so i well we say this at the
01:05 - beginning of every stream but
01:07 - we're streaming on a few different
01:08 - services but we're keeping an eye on the
01:10 - youtube chat
01:11 - so if you'd like to chat with us ask
01:14 - questions
01:15 - um please go there and and let us know
01:19 - we'd love for this to be as interactive
01:21 - as possible
01:21 - so we love your questions and would love
01:24 - to hear from you if you have any
01:26 - suggestions or ideas
01:29 - cool uh sophie do you want to give a
01:31 - little preview of maybe what we did
01:33 - last episode and then uh what we're
01:35 - doing today
01:37 - yes also thank you for the the haircut
01:39 - shout out
01:40 - my uh my boyfriend did this and i'm it's
01:43 - a little uneven
01:44 - it's pretty good actually yeah i just
01:46 - opted for no haircut in the
01:48 - 11 months um
01:52 - yeah so last week we went over the
01:55 - central limit theorem
01:56 - um and we started to get an
01:59 - understanding for
02:00 - inferential statistics which is a field
02:02 - of statistics that's focused on
02:04 - making inferences or i'm trying to think
02:07 - of a better word than inferences uh
02:10 - like making judgments uh
02:14 - about or guesses about a population
02:18 - based off of a sample that we can
02:20 - observe and that's really the basis for
02:23 - a lot of hypothesis tests
02:25 - and now today we're going to jump into
02:28 - an example of a hypothesis test
02:30 - where we're specifically going to look
02:33 - at the binomial test
02:34 - and the binomial distribution um which
02:37 - is
02:38 - maybe not the most ubiquitous or like
02:40 - most common hypothesis tests
02:42 - you probably or most people would think
02:45 - of like
02:45 - a t-test or a z-test is the most common
02:49 - um
02:50 - but it's a fun one because it's really
02:54 - easy to simulate
02:55 - um and you don't need any math in order
02:58 - to basically write a function that can
03:00 - run a binomial test and i think that
03:03 - when you're first learning something
03:06 - understanding how a function works is
03:08 - really important
03:10 - um and more important than being able to
03:13 - say in python just use a hypothesis
03:16 - testing function like there's lots of
03:18 - functions that exist in various
03:19 - libraries for
03:21 - conducting a t-test or a binomial test
03:23 - or chi-square test or any sort of
03:24 - hypothesis test that you want
03:26 - and all you need is one line of code
03:29 - but if you only ever learned how to
03:31 - implement the one line of code
03:33 - um you might not have a sense for how
03:37 - those
03:38 - functions are really working and that
03:40 - can make it harder to
03:41 - interpret things or to figure out if
03:43 - something went wrong
03:45 - and then it's also just i don't know i
03:49 - think
03:50 - a good opportunity to kind of think
03:53 - about
03:54 - inferential statistics in general what
03:56 - the goals are what the assumptions are
03:59 - and build your build your understanding
04:02 - of of the topic at hand um
04:05 - yeah someone that doesn't have a super
04:07 - strong statistics background i think
04:08 - that this is the kind of stuff that's
04:10 - really helpful for me where it's like
04:12 - i kind of know what these tests are i
04:14 - kind of know in what circumstances to
04:15 - use one
04:16 - over the other but really diving in
04:18 - trying to build this thing yourself
04:20 - seeing the um using the simulation to
04:23 - see what this uh what these functions
04:25 - are actually doing um can help
04:27 - give like a more solid understanding of
04:29 - what they're doing so then
04:30 - when you're out in the real world and
04:31 - you encounter a new situation you can
04:33 - say like okay this is exactly like
04:35 - you know what we did in this lesson
04:36 - versus another lesson
04:38 - exactly so with that
04:41 - i am going to share my screen um
04:44 - let me make sure i get the right
04:48 - screen um we're back to
04:51 - uh jupiter notebooks um these notebooks
04:55 - are in our github repo if you want to
04:58 - download them with the data
04:59 - um also just a heads up
05:03 - we added them like 15 minutes ago
05:06 - 20 minutes ago uh it's a little bit
05:09 - running a little bit late today so if
05:11 - you checked the github earlier today
05:14 - um and didn't see them they should be
05:15 - there now and uh
05:17 - if you're watching this after the fact
05:19 - you can go ahead and download those and
05:21 - run the code on your own computer
05:24 - cool so um today's
05:28 - lesson i'm going to start with the
05:31 - starting code file
05:32 - and load this
05:35 - monthly report file and i'll print it
05:37 - out
05:38 - um so this is actually directly from a
05:42 - codecademy lesson
05:44 - so if you're interested in taking that
05:45 - lesson uh i think
05:47 - maybe alex can post where it is yeah
05:50 - drop the link to the lesson
05:52 - um this this live stream series is all
05:54 - kind of a companion piece to the
05:56 - the new skill path that just launched um
05:58 - like a month or so ago so i'll drop a
06:00 - link to that
06:01 - in the uh in the chat yeah so this uh
06:04 - this is a simulated data set that alex
06:07 - and i
06:07 - actually made we had quite a lot of fun
06:10 - making it
06:11 - um but we're gonna pretend like
06:14 - uh we we work for our company
06:17 - and um and this company is collecting
06:20 - some data
06:22 - on purchases and items that were
06:25 - purchased
06:26 - and so uh we're imagining this this
06:28 - company is selling
06:30 - uh props to use to recreate your
06:34 - favorite scenes from movies
06:36 - um and so this is what alex and i spent
06:39 - probably a on a stupid amount of time on
06:42 - on a day when we were having trouble
06:44 - focusing is coming up with all of these
06:46 - props that you might buy um so you can
06:49 - take a look at the full data set if
06:50 - you're interested and see what other
06:52 - things we
06:53 - came up with there's some there's some
06:54 - funny ones in there
06:56 - um but this is this is the first few
06:58 - rows so we're imagining that
07:00 - um that there's
07:05 - there's data i think um we can get like
07:08 - monthly report dot
07:12 - info and just see a little bit more so
07:16 - there's 500
07:17 - rows um 500 non non-null values so
07:21 - non-missing values for each of these two
07:24 - columns
07:25 - um and each one
07:28 - is telling us why or yes yes or no did
07:31 - the
07:32 - did the customer who visited the website
07:35 - make a purchase and um
07:39 - and then if they did make a purchase so
07:41 - if this is a y
07:43 - where we're saying what did they buy and
07:46 - then
07:46 - the item is listed here as well
07:50 - so um basically
07:53 - we're gonna imagine that we are a um
07:58 - like a marketing team and at this
08:01 - company and we're
08:02 - trying to understand we have some idea
08:05 - for
08:05 - what the purchase rate among visitors
08:08 - should be um and we're gonna see if
08:12 - the purchase rate among this sample
08:15 - of people is lower than normal
08:19 - or higher than normal um and
08:23 - essentially the idea here is that
08:28 - maybe there was like maybe there was
08:30 - something that we were testing out like
08:31 - we were testing out a new feature
08:33 - or a new checkout system or there was a
08:36 - bug
08:36 - that some people saw and we want to know
08:39 - we want to have some understanding of
08:41 - whether
08:42 - that change had an effect on our
08:44 - purchase rate
08:45 - and that and we're thinking about this
08:48 - from the perspective that
08:51 - let's say it was a new feature like um
08:54 - a new checkout system that we that we
08:56 - randomly showed to
08:58 - some subset of our visitors
09:02 - if we did that we want to know whether
09:05 - if we show if we showed that same
09:07 - feature to
09:09 - all of our visitors would the purchase
09:11 - rate really be different from our
09:12 - expectation
09:14 - but in this case we only have a sample
09:16 - we only
09:17 - showed it to a small a small proportion
09:20 - of all of the visitors who could ever
09:22 - arrive at our site and so we don't know
09:26 - whether
09:27 - we don't know what would happen if we
09:28 - showed it to every single person
09:31 - and so with that in mind we need a
09:34 - hypothesis test to try to understand
09:37 - whether the difference that we observe
09:40 - in this sample
09:41 - is large enough that we would expect
09:45 - that difference to persist if we showed
09:47 - this thing to more people
09:48 - so so in this example we have 500 rows
09:51 - maybe that's like
09:52 - the purchase it or the people that
09:54 - visited the website on a
09:56 - on you know a tuesday or whatever the
09:58 - most recent day that we pulled this data
10:00 - we have 41 people bought something and
10:03 - so that's a percentage
10:04 - right it's 41 out of 500 that's some
10:05 - percentage and we're curious
10:07 - does that is that percentage different
10:09 - than what we expect
10:12 - right exactly and where does that that
10:14 - number that we're going to be comparing
10:16 - it to if like what we
10:17 - expect in the real world where does that
10:18 - number come from is that just like
10:20 - historical data of like oh my company
10:23 - the all-time set sales
10:25 - percentage is you know this percentage
10:27 - or that percentage
10:29 - so yeah that's a really good question um
10:33 - i think in general yes it comes from
10:35 - historical data
10:37 - we do have to be careful about thinking
10:40 - about how we use historical data
10:44 - given the sample that we have like we
10:46 - really want our sample
10:48 - in order to make this
10:51 - in order to meet the assumptions of this
10:53 - test and we'll go more into that once we
10:55 - get started
10:56 - we need the sample to be representative
10:59 - of
11:00 - the population and so
11:04 - if we're basing let's say we're basing
11:06 - our historical data off of like
11:09 - an entire year's worth of data but maybe
11:13 - like maybe this purchase rate
11:16 - fluctuates over the course of the year
11:18 - like maybe it's higher in the winter
11:20 - and when people are inside and looking
11:23 - for things to do
11:24 - and lower in the summer when everyone's
11:26 - at the pool and
11:28 - or whatever or outs outdoors um and so
11:32 - maybe you know maybe the change that
11:35 - we're
11:35 - if we're only sampling from one month or
11:38 - one day
11:39 - then um we shouldn't really be comparing
11:42 - to like
11:42 - all historical data uh so we we do have
11:45 - to be careful about our choice
11:47 - there um in real life but yeah
11:50 - it can come from a lot of different
11:51 - places like you'll also see
11:53 - these types of tests if you're if you're
11:56 - taking a sample for
11:57 - um for like a experimental study and
12:01 - you're trying to see whether your sample
12:03 - um
12:04 - is representative of the population and
12:08 - you
12:08 - have some averages or you have some
12:11 - like expectation based on like previous
12:15 - research
12:16 - um that has been done or yeah or
12:19 - sometimes it'll be like a cut off
12:21 - score like if you if
12:24 - 50 of or if you get 50 or higher on this
12:28 - test you pass or something like that so
12:30 - how about like oh if the folks that
12:31 - we're showing this new feature to
12:34 - or you know the new checkout system too
12:35 - if those are like the most recent
12:38 - subscribers or the most recent people to
12:41 - visit the site is that an issue where
12:43 - that's like the most recent people is
12:45 - not representative of
12:47 - the entire population i mean yes
12:52 - it's true that that's all that's always
12:55 - going to be the case but like at some
12:56 - point
12:57 - you do have to uh
13:00 - like kind of give up on meeting all
13:03 - assumptions in real research unless
13:06 - you're running
13:06 - like a very controlled experiment where
13:08 - you are splitting people into two groups
13:11 - and
13:12 - randomizing everything and yeah cool
13:15 - okay cool okay so we are going to
13:19 - get started then um so okay so let's
13:22 - just
13:23 - imagine um that the
13:26 - the expectation is that the purchase
13:28 - rate is usually about 10
13:30 - so like about 10 of visitors
13:34 - to this website usually make a purchase
13:37 - and we want to know if the purchase rate
13:39 - was
13:40 - significantly different from that
13:43 - so the first thing that i think it makes
13:46 - sense to do if we're going to try to
13:47 - answer this question
13:49 - is figure out what the purchase rate is
13:53 - in our data so we've got 500 visitors
13:56 - the site we want to know if the purchase
13:57 - rate was different from 10 percent
14:00 - so let's let's see if we can calculate
14:03 - the purchase rate in this sample and i'm
14:05 - actually going to divide the purchase
14:06 - rate into two numbers because it's going
14:08 - to help us with this
14:09 - like the way the binomial test is set up
14:12 - so
14:12 - the first thing i'm going to calculate
14:14 - is the the numerator
14:16 - the the top of that fraction for the
14:18 - purchase rate which is
14:19 - the number of people who made a purchase
14:23 - um so there's a few different ways to do
14:25 - that
14:27 - alex do you have i'll do it the way that
14:30 - i
14:30 - that like makes sense to me and then uh
14:34 - you can let me know if there are other
14:36 - ways that
14:37 - you've thought about doing this sure um
14:40 - but
14:41 - so monthly if i do
14:45 - mp.sum and then i take monthly report
14:50 - equal equal y
14:53 - so actually let me break this up what
14:56 - i'm first doing is
14:59 - monthly report is equal to yes
15:02 - is going to return a bunch of
15:06 - oops i should do dot purchase so we just
15:09 - get one column of this
15:12 - is going to return a bunch of trues and
15:14 - falses telling me whether or not
15:17 - that value is equal to y so it goes true
15:20 - false false true
15:21 - false because here we've got yes
15:24 - no no yes no and it turns out that
15:28 - there's this nifty thing where
15:30 - true gets a gets
15:33 - cast to a one if you try to perform
15:37 - any sort of operations on it and a false
15:40 - gets cast to a
15:41 - zero so if you take the sum of all these
15:44 - numbers it's gonna
15:45 - add a one every time there's a true and
15:47 - a zero
15:48 - every time there's a false so when i do
15:51 - this
15:53 - i will get a single number which is 41.
15:58 - yeah funny that you prompted this so
16:00 - because as you were developing the uh
16:01 - the course on codecademy this is
16:03 - something that
16:03 - i remember reviewing of like the sum
16:06 - here seems weird to me the way that i
16:07 - would have done it is i would have
16:10 - gotten all of the rows or like i would
16:12 - have extracted all the rows where
16:14 - that that column is a y and then i would
16:16 - have just
16:18 - counted the total number of rows um so i
16:21 - would have like filtered it as opposed
16:23 - to
16:24 - um summing it i guess
16:27 - so you would have done like yeah
16:30 - that yeah
16:34 - so just to repeat what alex just said
16:37 - he's taking the whole data set he's
16:40 - sub-setting it to just
16:42 - the rows where a purchase was made and
16:44 - then he's figuring out how many rows are
16:46 - in that subsetted
16:48 - data set by taking the line so
16:51 - and you'll see we got the same numbers
16:53 - that's good
16:55 - and the length of the whole data set we
16:57 - already
16:58 - saw this from the info section but
17:01 - if we say len monthly
17:04 - report which gives us the row number of
17:07 - rows
17:08 - um that was 500
17:12 - okay so the purchase rate is going to be
17:16 - 41 over 500 and i'll just print out what
17:20 - that is
17:22 - it's 0.082 okay so
17:26 - remember that we said our expectation
17:30 - was 10 percent
17:32 - so that's 0.1 here i can put this
17:35 - actually as a
17:36 - percentage because it's probably easier
17:38 - to
17:39 - read so it's an 8.2 purchase rate
17:43 - our expectation was 10 so this is a
17:46 - little bit
17:47 - above normal i also just saw somebody
17:50 - posted in the comments um you could also
17:52 - do value counts
17:55 - um with normalize equals true and that's
17:57 - totally true as well there's
17:59 - as per usual there's many many ways to
18:02 - do this
18:02 - i'll also demo that because it's kind of
18:04 - fun
18:05 - that'll give us both the number of yeses
18:08 - and no's
18:09 - so that gave us this 0.082 for the s's
18:12 - and 0.918 for
18:16 - the nose and if we took this normalize
18:18 - equals true piece out
18:20 - we would get the counts so that's cool
18:23 - um okay now and so we've seen that the
18:27 - purchase rate was lower than normal
18:29 - but that doesn't fully answer our
18:32 - question
18:32 - because like we said at the very
18:34 - beginning of this
18:36 - just because the purchase rate is 8.2
18:40 - percent
18:41 - doesn't mean that there's really a dip
18:44 - in the purchase rate if
18:46 - like ev if we showed the same feature
18:48 - that these people saw or the same
18:50 - bug that these people saw if we showed
18:53 - that same thing to
18:54 - all of our visitors because this is just
18:57 - a sample
18:57 - and there's some random chance involved
19:00 - so one of the things i like to think
19:03 - about
19:04 - and i think like this is probably the
19:06 - natural
19:08 - the natural progression or like the
19:10 - natural way that many people
19:12 - teach the concept of the binomial test
19:15 - is
19:16 - to think about flipping a fair coin
19:19 - um i like it as an example so if you
19:22 - imagine you're flipping a fair
19:24 - coin right if you
19:27 - the sorry the probability of heads is 50
19:31 - so you would expect that about half of
19:33 - your flips would be
19:35 - heads say so let's say you flip
19:38 - your coin ten times you expect that
19:41 - about five of them are going to be heads
19:43 - but because this is a stochastic process
19:46 - it's a random process
19:49 - it's not going to be exactly five every
19:51 - single time
19:52 - right it's gonna be it's gonna vary each
19:55 - time you do it if you flip a coin ten
19:57 - times you might get four heads and if
19:58 - you do it again you might get five if
20:00 - you do it again you might get
20:01 - seven um but the more you do it the
20:04 - closer you're gonna get to
20:06 - that 0.5 or that half
20:10 - level so we can actually demo this
20:14 - um in python
20:18 - i'm gonna so i'll demo actually the
20:22 - the coin flip first so let's say we do
20:26 - um numpy.random.choice
20:31 - we're gonna choose between heads
20:35 - and tails
20:39 - and we'll for right now we'll do one
20:42 - or actually let's do 10 coin flips
20:46 - and we're going to say p equals
20:51 - 0.5.5 which is to say that the
20:53 - probability of heads is 0.5 and the
20:55 - probability of tails is 0.5
20:58 - if we do this whoop
21:02 - shoot
21:05 - i think these are supposed to be one
21:07 - list
21:08 - sorry yeah there we go we just flipped a
21:11 - coin
21:12 - 10 times on a computer we got heads
21:15 - heads tails
21:16 - oh my god we got nine heads wow that's
21:19 - crazy let's try it again okay now we got
21:23 - closer
21:23 - wow it's still really funny we're
21:25 - getting like
21:27 - six heads in a row and then four tails
21:29 - actually i was reading about this
21:30 - because we made some prop bets for the
21:32 - uh
21:33 - for the super bowl that apparently it
21:35 - was like
21:37 - i think a bunch of heads or tails in a
21:41 - row it was like
21:42 - a bunch of tails in a row then a bunch
21:43 - of the last five
21:45 - six or the last seven have been tails or
21:48 - something like that
21:49 - of the super bowl like uh coin flips
21:52 - that's funny
21:53 - yeah so you know this this happens right
21:55 - you get a bunch of the same thing in
21:57 - in a row just by random chance um
22:00 - so now i'm going to up this a little bit
22:02 - because i don't want to print it out
22:04 - every time
22:05 - so let's save these as flips
22:10 - and let's just print out let's use this
22:13 - same
22:16 - code that we had up here to every time
22:19 - just print out um
22:23 - flips equals head so the number of
22:28 - heads
22:34 - and now let's up the size to like 100
22:38 - say
22:40 - okay so not 100 flips you got
22:43 - 46 heads 49
22:46 - [Music]
22:49 - and and so bringing this back to our
22:50 - example of the
22:52 - um basically this is kind of like
22:56 - okay we saw 41 purchases
23:00 - is that you know if it was actually 10
23:02 - we would have seen 50 right because we
23:04 - had 500 total
23:05 - um or we would have seen
23:10 - yes 50 sorry yeah right because 500
23:13 - people either said yes or no i'm buying
23:14 - the thing or not
23:15 - we expect it to be 10 so we're saying
23:17 - okay that would be 50 people
23:19 - we saw 41. so is the question here that
23:22 - we're asking like
23:23 - how big of a deal is that 41 is it like
23:26 - getting a 52 here or is it like
23:28 - you know if we if we keep printing this
23:31 - out we're pretty much never going to see
23:32 - a 90 or a 100 right
23:36 - um so that's kind of like the comparison
23:38 - here of like how far away are we from
23:40 - the
23:41 - expected thing that we that we expect to
23:42 - get exactly
23:44 - so right so the question here might be
23:47 - like if you're
23:48 - let's say you uh work at a casino
23:52 - and you're you're checking
23:56 - uh a coin i don't know i guess they
23:58 - don't really use coins at casinos but
24:00 - you're checking to see whether this coin
24:03 - is fair like your friend has
24:04 - decided that has offered to play some
24:08 - game where you win if the coin comes up
24:10 - heads
24:11 - and they win if the coin comes up tails
24:13 - and so you're trying to check whether
24:14 - the coin is fair
24:16 - you can't flip the coin infinitely many
24:19 - times to see
24:20 - what percent of them come up heads but
24:23 - you can
24:24 - take a sample so here we're kind of
24:26 - thinking of the sample as being like
24:28 - a sample of all the coin flips that
24:30 - could ever exist
24:32 - it's a really weird thing to think of
24:34 - like a population as being
24:36 - coin flips and a sample as being some
24:38 - subset of those coin flips but
24:40 - that's kind of how we're thinking of it
24:41 - we're thinking of it as like population
24:43 - is all the coin flips that could ever
24:44 - exist in the world
24:46 - and of all the coin flips that could
24:48 - ever exist in the world of a fair coin
24:50 - half of them would be heads
24:52 - um and this is some subset and based on
24:54 - that subset you're saying okay so we
24:56 - expect some random chance so we
24:58 - we expect that even though this should
25:00 - come out too close to 50 it's not going
25:02 - to come out to exactly 50
25:03 - every single time we might get some
25:05 - weird numbers like just by random chance
25:07 - that time we got 41 heads
25:09 - this time we got 48. but
25:12 - if we flip the coin a hundred times
25:16 - and 99 times it comes up heads we could
25:19 - be pretty convinced that it's not a fair
25:21 - coin
25:22 - um because that would be kind of outside
25:24 - the realm of
25:26 - possibility here by random chance yeah
25:28 - and i i have a question this might
25:30 - relate to something we you do in a
25:32 - second but like to me it seems like
25:33 - there's two things involved here one is
25:36 - the sample size of like okay we did it a
25:39 - hundred times and
25:40 - it's off by this amount and then there's
25:42 - also the how much is it off by so like
25:45 - are those two connected in some ways of
25:47 - like okay i flipped my coin
25:49 - a hundred times and um
25:52 - you know uh it came up heads 70 times
25:56 - versus i flipped my coin 10 times and it
25:57 - came up seven times like
25:59 - how are those two things connected the
26:02 - the sample size and then also
26:04 - how far away you are from the expected
26:06 - thing
26:07 - what a great question alex that was
26:09 - really good
26:11 - um so actually this comes exactly back
26:14 - to
26:15 - what we discussed last week so last week
26:18 - we were looking at a normal distribution
26:21 - so okay let me actually just show you
26:24 - something so
26:25 - let's say i say flip
26:29 - results equals
26:32 - empty set and we're going to do for
26:35 - i n range
26:39 - thousand um
26:42 - and then we're gonna say or
26:46 - you know what actually i'm gonna instead
26:49 - of showing you all this code i'm just
26:50 - gonna show you a picture that i already
26:52 - have i'm gonna pull it from codecademy
26:56 - um okay we're gonna
27:06 - i forgot i didn't fully share my screen
27:11 - hold on actually okay we'll do this
27:15 - sorry um okay so
27:18 - we've got uh we're gonna do this a
27:21 - thousand times
27:22 - every time i'm gonna
27:27 - flip this coin save the number of heads
27:31 - and then i'm going to
27:34 - append that to
27:40 - to my list and then i'm going to use
27:43 - i'll run this
27:45 - i think i can do it 10 000 times without
27:47 - too much of an issue
27:49 - and then i'm gonna print a histogram of
27:54 - that yeah and i think as you're as
27:56 - you're writing this out
27:57 - a thing that gets kind of confusing here
27:59 - is we're now doing
28:01 - this experiment of flipping the coin 100
28:03 - times we're doing that whole process
28:05 - 10 000 times so there's kind of like
28:07 - layers within layers here
28:09 - exactly so yeah i think i coded that
28:13 - really quickly but just to repeat what i
28:14 - did
28:15 - so by creating by putting this inside of
28:18 - a for loop
28:19 - with ten thousand iterations i said i'm
28:22 - going to
28:22 - repeat what i just did up here which is
28:24 - flipping a coin
28:26 - a hundred times and recording the number
28:28 - of heads i'm going to do that
28:30 - process 10 000 times so
28:34 - 10 000 iterations of me flipping a coin
28:37 - 100 times
28:38 - and recording the number of heads and
28:41 - then
28:41 - here i'm plotting a histogram of um
28:45 - those results and actually i'm gonna
28:47 - like
28:49 - uh run this again because this is a
28:51 - little
28:52 - well okay
28:56 - it's a little off center but if i did
28:58 - this enough times
29:00 - this would be like perfectly kind of
29:03 - centered
29:04 - around 50. and this looks a lot like
29:16 - that's a really weird one 15
29:19 - make this look nice
29:23 - yeah i should just be starting to use
29:25 - the seabourn
29:27 - the seabourn one that just automatically
29:29 - does this part for you
29:31 - finds the right number of bins um
29:34 - cool so uh
29:37 - anyway we plotted this histogram of the
29:40 - number of heads
29:41 - and we see kind of like we expect the
29:44 - most common
29:45 - outcome is that it's close the number of
29:47 - heads is close to 50
29:49 - and it looks like the number of heads is
29:51 - ranging roughly from like
29:53 - 35 to 65 with this one outlier
29:56 - like one time we did it we got 70 heads
29:59 - which was a lot but
30:01 - for the most part it's like roughly
30:03 - between 65 and 35
30:06 - and each of the farther away you get
30:09 - from 50 it's less likely
30:12 - this is called the binomial distribution
30:14 - um
30:16 - and you might think after
30:19 - if you watch last week sorry last week's
30:22 - stream you might think like well that
30:25 - looks a lot like a normal distribution
30:28 - and it actually really is
30:31 - the only difference between this
30:33 - distribution and a normal distribution
30:35 - is that this is what's called dis a
30:38 - discrete
30:39 - probability distribution or a
30:41 - probability
30:42 - i think it's called probability mass
30:45 - function
30:46 - um where the value the number of
30:50 - flips that you can have that come up
30:52 - heads has to be an integer value
30:54 - like it it can only be 35 36 37 it can't
30:58 - be like 35.5
30:59 - because you can't have 0.5 heads but
31:01 - otherwise this is really just a normal
31:03 - distribution
31:04 - and actually like the central limit
31:06 - theorem
31:08 - essentially applies here the larger the
31:10 - numbers
31:11 - you use like the higher the number of
31:13 - flips
31:14 - the closer this is to a
31:18 - normal distribution um so it approaches
31:21 - we say a normal distribution
31:22 - um and remember from the central limit
31:24 - theorem that the standard error which is
31:27 - describing how wide this thing is is
31:30 - calculated as
31:31 - the standard deviation of the population
31:34 - divided by the square root of the sample
31:36 - size and so you're dividing by
31:39 - essentially the sample size so as the
31:41 - sample size gets larger
31:44 - the variation that you expect in this
31:46 - distribution
31:47 - gets smaller and that and
31:51 - that's like sounds complicated but
31:53 - that's the whole like
31:55 - i think that's the maybe the most
31:57 - beautiful thing about all of this
31:58 - is that you end up with these really
32:00 - complicated formulas a lot of times in
32:02 - statistics and people like
32:05 - tell you give you the formulas and ask
32:07 - you to ask you to apply them
32:09 - but i feel like if you see it in this
32:10 - sense it just makes sense like
32:13 - it you don't have to think about it as
32:14 - dividing by the square root of the
32:16 - sample size
32:17 - to get the width of that distribution
32:19 - you can think about it as well like
32:21 - of course the more you the more you do
32:24 - this the more
32:24 - certain you're going to be that about
32:27 - like
32:28 - the proportion of heads so if you in
32:31 - in this case is the sample size 100
32:33 - because we're flipping 100 coins each
32:36 - exactly if we go and change that to
32:38 - flipping 10
32:39 - uh in theory we'll see the variance be a
32:42 - lot larger right so it's going to be
32:44 - yeah we're not going to see it so easily
32:47 - in this plot because
32:49 - um because we can't change like we'd
32:52 - have to set the range
32:53 - for this we're going gonna adjust the
32:55 - range of the histogram
32:57 - automatically but yes
33:00 - cool cool can uh can we
33:03 - can can i just see can can you change it
33:05 - i just want to see it and then we can
33:06 - try to remember the ring
33:08 - so you're saying like the range right
33:09 - now is going from like 35 to 65.
33:13 - yeah if you don't think it's gonna be
33:15 - instructed then i think it's
33:16 - it's it's gonna be a little weird
33:18 - because as you get closer to zero
33:21 - it gets like it gets squashed okay
33:24 - um but but we did show it we did
33:26 - demonstrate this
33:27 - yesterday or not yesterday last week
33:30 - yeah um so
33:31 - watch the other live stream and you can
33:34 - see this in
33:35 - action cool
33:38 - um all right so let's come back to our
33:41 - question and let's or let's come back to
33:45 - our data set
33:46 - and let's like basically do the exact
33:49 - same thing we just did but let's do this
33:51 - in the context
33:52 - of our research question so
33:55 - before we sorry
33:59 - we calculated that there were 41
34:01 - purchases
34:02 - out of a sample of 500 and we want to
34:04 - know if that's really different from 10
34:06 - percent
34:07 - so what we're gonna do is we're gonna
34:10 - kind of
34:11 - we're gonna simulate some more samples
34:14 - that are just like this
34:16 - so here we're going to say like
34:20 - simulated visitors
34:25 - and our simulated visitors can either
34:28 - make a purchase
34:30 - yes or not make a purchase
34:33 - no our sample had 500 visitors so we're
34:36 - going to keep that the same
34:38 - and now for this we're going to set our
34:42 - x we're going to use our expectation so
34:45 - we're going to say like we expected the
34:48 - purchase rate to be
34:49 - um 0.1 or 10
34:53 - right so we expected the probability of
34:55 - a purchase to be 0.1
34:57 - and the probability of not a purchase to
34:59 - be
35:01 - um and then we'll call this
35:05 - purchases and
35:09 - that's going to be the sum of when
35:12 - simulated visitors equals
35:14 - yes and we want to print that out
35:19 - and then oops
35:23 - and then let me just do this once so
35:25 - what i did here
35:26 - was i said okay let's imagine for a
35:29 - second
35:30 - that the real probability of a purchase
35:33 - is 10
35:35 - how much of a range am i going to see
35:38 - in the number of purchases that occur
35:40 - among 500 visitors
35:42 - if the true probability is 10 is equal
35:45 - to my expectation basically
35:48 - so i'm going to do this a few times
35:49 - remember that with a 10
35:51 - purchase rate we expected that among 500
35:54 - visitors we would have 50 purchases
35:57 - so we're seeing numbers kind of around
35:59 - 50. we had 59 then we got 45.
36:02 - yeah because in our in our real data set
36:04 - we had 41.
36:06 - in our real data set we had 41. um
36:10 - so we're definitely starting to see that
36:13 - we can get
36:13 - numbers that are kind of far away from
36:15 - 50 by random chance
36:18 - and now we're gonna basically do the
36:19 - same thing we did with our coin flips
36:22 - we're gonna say okay if i do this a
36:25 - bunch of times and then i plot the
36:26 - histogram
36:27 - of these numbers and each of those
36:31 - numbers being the number of purchases
36:33 - in a simulated sample of 500 people
36:36 - where the purchase probability was equal
36:38 - to 10
36:40 - very complicated sentence then
36:43 - what kind of distribution am i gonna see
36:46 - in terms of
36:48 - the range in the number of purchases
36:50 - that people are are making
36:51 - in this simulated
36:55 - so i'm going to copy this over
37:00 - except i'm going to replace all of so
37:03 - i'm going to call this now
37:07 - i'm gonna call this null
37:10 - purchases i'll explain why i'm calling
37:13 - it that in a second
37:15 - um and we're gonna append
37:21 - purchases to that and then i'm going to
37:23 - grab
37:25 - this code and drop that in there
37:30 - okay alex do you want to like walk
37:33 - through
37:34 - as i run this what this code is doing
37:36 - again
37:37 - right so this is this is similar to the
37:40 - flipping a coin a hundred times getting
37:43 - the
37:43 - how many heads you we had and then uh
37:46 - and then doing that process
37:48 - ten thousand times and trying a
37:50 - histogram based on based on those
37:51 - results
37:52 - this is the exact same thing except for
37:54 - now rather than flipping a coin 100
37:56 - times we're looking at
37:57 - 500 visitors and rather than the coin
37:59 - being 50 50 we're saying
38:01 - um we only expect 10 of those visitors
38:04 - to
38:05 - purchase the thing exactly
38:08 - okay so when we do that we did it
38:12 - 10 000 times simulated 10
38:15 - 000 samples of 500 in each of our
38:18 - samples of 500 each person had a 10
38:20 - probability of making a purchase um
38:24 - and we collected for each sample of 500
38:27 - how many purchases were made in our
38:28 - simulation
38:29 - and then we plotted this histogram and
38:31 - we see that the number of purchases is
38:33 - ranging from like
38:35 - 30 to 70. again
38:38 - the most our expectation which was 50 is
38:41 - the most common
38:42 - result um but we see we see this kind of
38:45 - range
38:46 - and now we can figure out where our
38:48 - observed value was
38:50 - so our observation
38:53 - and we can do
38:57 - we can l t dot a
39:01 - x v line
39:04 - um and then put that line at 41
39:07 - let me get red um
39:12 - right so here's our observed
39:15 - number of purchases and we can see that
39:19 - it's not not like uh it's not quite as
39:22 - likely as
39:23 - 50 it's not out of the question it's
39:26 - within the
39:26 - certainly within the realm of
39:28 - possibility it's not like way out here
39:30 - at
39:30 - 29 or 20 or something
39:34 - crazy or a way above like 80
39:37 - right it's it was within the realm of
39:40 - possibility but
39:41 - it's less likely than
39:45 - this kind of like middle range of
39:47 - numbers
39:50 - yeah two things that i i want to say
39:52 - here one is that i think this graph
39:54 - is potentially a little bit misleading
39:57 - if you're just looking at the graph
39:58 - without labels because it kind of looks
40:00 - like it's a percentage right it ranges
40:02 - like between 30 and 70 with 50 right in
40:05 - the middle
40:06 - so remember you know it was possible to
40:09 - have
40:10 - um 500 people buy uh
40:14 - buy the product and it would be possible
40:16 - for us to have like you know a bar
40:17 - over at 500. it just so happens that um
40:21 - because uh the expectation is that
40:24 - 10 of the people buy the buy the thing
40:27 - and we're
40:27 - sampling 500 people the middle happens
40:29 - to be 50 but this in like no way is
40:31 - connected to percentages which i think
40:33 - um could be confusing because we're
40:37 - in this example we like a lot of things
40:38 - that are like percentages coin flips 50
40:40 - 50 right and so it's a little bit
40:41 - uh uh unlucky or you know it's a little
40:45 - bit
40:45 - um confusing that the middle here
40:47 - happens to be right around 50.
40:49 - that is very helpful point thank you um
40:51 - the other thing that i wanted to ask
40:53 - i think when we bring this into the real
40:56 - world like it makes sense with
40:58 - a coin of flipping 50 50. but if we when
41:01 - we bring this into the real world of
41:02 - like
41:03 - yes my person can either buy
41:06 - buy something or no they can't a natural
41:09 - question that i had at least
41:10 - and i think i know the answer this based
41:11 - on the fact that this is called a
41:12 - binomial
41:13 - um test uh can you run this with
41:17 - three values right so is it did they buy
41:21 - like the uh using codecademy as an
41:24 - example did they buy the month-long
41:26 - version or the year-long version or do
41:28 - they pass altogether right there's three
41:29 - options there
41:30 - what do you do in that situation is this
41:32 - applicable at all would you combine
41:34 - the the two forms of yeses into one form
41:37 - of yes like what do you do if you have
41:39 - not a binary thing yeah so if you
41:42 - have not a binary thing then it depends
41:45 - on what your question is but if your
41:47 - question is
41:48 - basically is there an association
41:51 - between
41:51 - what somebody saw and what
41:54 - and their probability of making a
41:56 - purchase if that's
41:58 - what you want to answer then you need
42:00 - like basically if you
42:01 - if you don't care if if you care
42:05 - if there's any difference between those
42:07 - three or more groups
42:09 - then you're gonna need a chi-square test
42:11 - so
42:12 - um that's a different kind of hypothesis
42:14 - test we'll cover that in another week
42:16 - when we start talking about hypothesis
42:17 - tests for
42:18 - an association um
42:21 - but yes there's there is a hypothesis
42:24 - test for that
42:24 - the other thing that you could do is you
42:27 - could
42:28 - try to binarize it if you really only
42:30 - care let's say you have like three
42:31 - conditions and
42:33 - two of two of them are
42:37 - um i don't know like
42:41 - fallen like you can kind of combine two
42:44 - of them and you really only care
42:45 - about the one that's that you expect to
42:48 - be different
42:49 - then you can always turn it into a
42:52 - binary variable and use something like
42:54 - this
42:55 - yeah i'm struggling to come up with an
42:57 - example
42:58 - we have a we have a good question in the
42:59 - chat from andrew um
43:01 - i guess could you talk a little bit
43:03 - about the y-axis here and what that's
43:04 - showing where these values are a little
43:06 - bit hard to read of
43:08 - oh i just changed it because i saw that
43:10 - so yeah i just changed this to
43:12 - density equals true um as you were
43:15 - talking
43:16 - so density equals true changes this to
43:20 - a um
43:23 - basically to a probability um it's a
43:26 - little bit tricky because like
43:28 - in the case of the normal distribution
43:31 - it will it will make it such that like
43:34 - the
43:35 - area of each basically it will make it
43:37 - such that the area of
43:38 - each of these bars is a probability of
43:40 - observing the number in that
43:42 - area like or the number in that range so
43:45 - for example
43:46 - um this bar looks like it's about at 0.4
43:50 - and it runs from like 41 to
43:55 - 47 maybe or 46
43:58 - and so if you so it's maybe like a bar
44:01 - that's
44:02 - 5 units wide times 0.04 high
44:06 - so if you do 5 times 0.04 it'll give you
44:09 - the area of this bar
44:10 - and then that will tell you the
44:12 - probability of observing
44:15 - between 41 and 46 or whatever this
44:18 - is purchases by random chance if the
44:22 - true purchase rate was 10 yeah and that
44:24 - and that's compared to our owned y-axis
44:26 - if we didn't put in
44:27 - um density equals true then that's just
44:29 - like the counts of how many times we
44:32 - we saw these occur right we ran this
44:34 - experiment 10
44:35 - 000 times and 2 000 times they fell
44:37 - between 40 and 47.
44:39 - exactly that is a very good question
44:44 - um okay so now i'll come back to this
44:48 - so the reason i called this null
44:50 - purchases
44:51 - is that in the context of hypothesis
44:54 - tests
44:54 - we usually frame a hypothesis test as
44:58 - starting with a null hypothesis
45:01 - and an alternative hypothesis so
45:05 - the null hypothesis of this binomial
45:07 - test that we're running
45:09 - is that the probability of a purchase
45:12 - is 10 percent um i should write that
45:15 - down
45:17 - so null hypothesis is that the
45:20 - probability
45:21 - of a purchase is
45:24 - 10 and this is kind of in keeping
45:28 - basically the null hypothesis says that
45:30 - our expectation is true we expected
45:33 - there to be a 10 purchase rate and there
45:35 - is a
45:36 - 10 purchase rate the alternative
45:39 - hypothesis
45:40 - we have a little bit more um
45:44 - leeway to decide what we want this
45:46 - alternative hypothesis to be
45:48 - but we could for example say that the
45:50 - alternative hypothesis
45:51 - is that there's fewer that the purchase
45:54 - rate is
45:55 - below 10 we could also make the
45:58 - alternative hypothesis that the purchase
45:59 - rate is a
46:00 - above 10 or we could just say the
46:04 - purchase rate
46:04 - is not 10 it could be less than
46:08 - or greater than um we have some choice
46:11 - but let's say
46:12 - let's say for right now like we only
46:15 - really cared to measure
46:18 - whether the probability of a purchase
46:20 - was below 10
46:21 - because let's say what happened was that
46:23 - like a bug got
46:25 - thrown to a randomly selected group of
46:29 - 500 visitors base they all saw a bug and
46:32 - we wanted to know if that bug
46:34 - influenced our um our purchases
46:38 - and so we expect the purchase rate to go
46:41 - down um
46:42 - and so maybe our alternative hypothesis
46:44 - is that
46:45 - the probability of a purchase
46:48 - is less than 10 and this is this is very
46:52 - like
46:52 - pedantic but like can you do that can
46:54 - you say like of course a bug is not
46:55 - gonna result in more purchases or like
46:58 - we expect you know we expect it to go
47:00 - down and so we're only going to check to
47:02 - see if it actually went down
47:03 - um yeah i mean so basically like
47:07 - you should decide if you're running an
47:10 - experiment you should decide what your
47:11 - hype
47:11 - alternative hypothesis is ahead of time
47:14 - and there's some
47:15 - complic sort of complicated math
47:18 - associated with this but the idea is
47:21 - that
47:23 - if let's say i think it's easier to
47:26 - think about like
47:26 - if you are trying to um
47:30 - if you're trying to create a new feature
47:32 - that's going to increase the purchase
47:33 - rate
47:34 - um and look and let's say then
47:37 - like you only really care if your new
47:40 - feature
47:41 - improves your um
47:45 - your your purchase rate because if it
47:49 - doesn't improve the purchase rate then
47:52 - you're definitely not if it like if it
47:54 - decreases the purchase rate you're
47:55 - definitely not going to
47:56 - implement that feature if it if it
47:59 - doesn't change the purchase rate you're
48:00 - not going to
48:01 - then we don't care then you don't care
48:03 - so in that case
48:05 - you might want to run a one-sided a
48:08 - one-sided test which is to say you might
48:09 - want to make your
48:10 - alternative hypothesis be that the
48:12 - purchase rate is greater than 10
48:14 - because you don't care about the other
48:15 - situation um
48:18 - as you'll see in a second i hope we get
48:20 - time to do it
48:21 - like if you run if you use the
48:24 - alternative hypothesis
48:26 - where you're saying like it could be
48:28 - greater or equal
48:29 - or sorry greater or less than you're
48:32 - gonna
48:32 - be less likely i make sure i get
48:36 - my words right um you're gonna get a
48:39 - larger
48:40 - p value so you're gonna be less likely
48:42 - to
48:43 - get significant results um
48:47 - but if you do the wrong one-sided test
48:50 - like you say
48:51 - it's greater than when it's really less
48:54 - than
48:55 - then you have no it's called like pow
48:58 - your power to
48:59 - reject the null hypothesis is um like
49:02 - goes down so anyway it
49:05 - i feel like i'm getting into too
49:08 - complicated of a topic at this point but
49:11 - yes the like the bigger picture is
49:16 - you should make this decision beforehand
49:17 - and it depends on your research research
49:19 - question um
49:22 - cool okay so what i wanted to say is
49:25 - that the reason
49:26 - that i'm calling this null purchases is
49:28 - that basically the key to any hypothesis
49:31 - test
49:31 - is knowing what the null distribution
49:35 - is and the null distribution is
49:38 - basically
49:39 - the distribution of a sample statistic
49:42 - that you care about
49:43 - if the null hypothesis is true in this
49:46 - case the sample statistic that we care
49:48 - about
49:48 - is the count of the number of purchases
49:51 - and the null hypothesis is that
49:55 - the purchase rate is 10 and so our
49:58 - null distribution is the distribution
50:01 - of like imagined simulated uh
50:06 - samples of 500 with a purchase rate of
50:09 - 10
50:10 - the number of purchases among those
50:12 - simulated samples
50:14 - this is our null distribution and
50:17 - what i'll say is when you run a binomial
50:20 - test
50:21 - you're running a binomial test because
50:23 - the null distribution
50:25 - is something called a binomial
50:26 - distribution and if you run a t
50:29 - test the reason is because
50:32 - the null distribution is in the shape of
50:35 - a student t
50:36 - distribution which is essentially the
50:37 - same as a normal distribution
50:40 - um if you run a chi-square test it's
50:42 - because the null distribution is
50:43 - chi-square distributed
50:45 - so basically like the magic of
50:48 - statistics
50:49 - is knowing what this distribution is
50:52 - going to look like
50:53 - without actually having to do what we
50:55 - did which is simulate all of those
50:57 - samples
50:59 - right so this is an example where
51:02 - inherently the pro the problem that
51:04 - we're or the question that we
51:05 - have is going to end up with a simula
51:08 - with a distribution like this
51:10 - and like the shortcut that you can take
51:13 - once you learn this stuff is knowing
51:15 - that okay this problem is going to
51:16 - result in a
51:18 - binomial distribution and there
51:20 - therefore i'll run the binomial test
51:22 - exactly so okay
51:26 - so let's um let's actually calculate a
51:29 - p-value
51:30 - for our for our test
51:33 - so essentially what a p-value
51:36 - is is and i'm gonna like
51:40 - speak slowly so i get this correct is
51:44 - it's the probability of observing
51:47 - [Music]
51:48 - a range of statistics which is or a
51:51 - range of values which is defined by
51:54 - the alternative hypothesis so in this
51:56 - case if our
51:57 - alternative hypothesis is that the
51:59 - purchase rate is less than 10
52:03 - our p-value
52:06 - is going to be the probability
52:09 - of observing the number of purchases we
52:12 - got
52:13 - or fewer given that
52:17 - the null hypothesis is true
52:20 - so in this picture what this equates to
52:24 - is like the area if we
52:28 - do density equals true again
52:36 - basically is proportional to the area
52:40 - of this distribution that's to the left
52:43 - or
52:44 - smaller than this red line that we drew
52:47 - and that qualifier of like if the null
52:49 - hypothesis is true
52:51 - that's that means that's
52:54 - the experiment that we ran right that's
52:55 - the experiment of 10
52:58 - of um you know we have 500 people and 10
53:01 - of them are going to
53:02 - um buy the thing and so it's going to
53:05 - end up with a distribution like this
53:07 - yeah so basically we take the null
53:09 - distribution
53:10 - and then based on the alternative
53:12 - hypothesis we say
53:14 - what proportion uh or what's the
53:17 - probability of observing
53:19 - the alternative hypothesis or basically
53:22 - what's the probability of observing what
53:25 - we observed
53:26 - or something more extreme in the
53:29 - direction of the alternative hypothesis
53:31 - so in this case
53:31 - less than um more extreme is even less
53:35 - than 41.
53:37 - what's the probability of observing that
53:39 - if the null hypothesis is true and
53:41 - that's basically going to be
53:42 - an area of the null distribution where
53:45 - you draw the null distribution
53:47 - you draw a line at your sample statistic
53:50 - observed sample statistic and then you
53:53 - take the area
53:55 - outside of that um
53:58 - so if i were to eyeball this i would say
54:00 - that that's like maybe
54:02 - 10 that area is like maybe 10-ish
54:04 - percent of the uh
54:06 - of the total area under the curve i
54:08 - don't know if it's going to be exactly
54:09 - that but that's kind of
54:11 - what we're looking at right the area to
54:12 - the left of that red line
54:14 - is a small area compared to the rest of
54:17 - the area
54:18 - i think your your estimation cells
54:20 - skills are very good
54:21 - um okay so the p p-value is going to be
54:23 - the sum
54:25 - of null outcomes or what did i call it
54:28 - null purchases
54:29 - i'm copying this over from codecademy
54:33 - uh so this is right the top of this
54:37 - fraction is the number of
54:40 - values in num purchases the number of
54:43 - simulated purchases that are less than
54:45 - or equal to 41
54:46 - divided by the total number of
54:49 - simulations which in this case um
54:54 - is 10 000. and so if we print that
54:58 - whoop
55:01 - [Music]
55:04 - let's see oh i think you have to
55:08 - array
55:14 - okay yep so
55:17 - this is roughly and i'll do times
55:21 - tens well no i won't do that so
55:23 - basically this is like
55:24 - 10 which is exactly what
55:27 - um alex estimated and so basically we're
55:30 - saying that
55:32 - if we randomly simulated i'm just going
55:34 - to say it again because i feel like
55:36 - it helps to just keep saying it if we
55:38 - randomly simulate
55:40 - 500 visitors and we repeat that process
55:44 - 10 000 times each time recording the
55:45 - number of purchases that were made and
55:48 - in each of those simulations the
55:49 - purchase rate really is 10
55:51 - where the probability that we observe
55:55 - 41 or fewer purchases
55:58 - given that the purchase rate was 10 is
56:01 - about
56:02 - 10 um all these numbers lining up is
56:07 - probably not super helpful
56:09 - um the two-sided p-value is
56:13 - essentially going to be two times that
56:15 - because
56:16 - theoretically we would just draw another
56:19 - red line
56:20 - on the other side of this that's
56:22 - equidistant so this one's at 41 so we
56:24 - draw another line at like 59 so
56:27 - nine units away from 50 on the other
56:30 - side
56:31 - and add in this area over here
56:34 - um the reason i'm saying that so
56:37 - when i say two-sided p-value i mean if
56:39 - we said that the alternative
56:40 - hypothesis is that the probability of a
56:42 - purchase is not
56:44 - 10 then we would
56:47 - want to calculate this like
56:51 - both the area of both sides both
56:54 - extremes
56:55 - um the probability that the number of
56:59 - purchases was
57:00 - nine fewer or nine more than 50.
57:05 - um and the reason that's important is
57:07 - that
57:08 - right now because we're getting close to
57:10 - the end of time i want to just
57:11 - demonstrate
57:12 - that you can also do this with the
57:14 - built-in function
57:15 - and i want to demonstrate that you get
57:16 - the same thing but the built-in
57:18 - uh python function that
57:21 - at least all the ones i found calculate
57:24 - a two-sided p-value
57:26 - by default so um
57:31 - let's just
57:35 - uh i'm just grabbing the solution code i
57:38 - think it's a sci-pi function that i've
57:40 - been using but i just want to grab it
57:43 - um okay so let's
57:46 - do let's do this so let's
57:50 - import from scipy
57:55 - dot stats import
57:58 - binome test so now we're going to do
58:00 - this in a single line of code
58:03 - we're going to say give me the so the p
58:06 - value from this
58:11 - this function is equal to
58:15 - binome test the parameters here are
58:19 - the observed value which in this case is
58:21 - 41
58:24 - the sample size which is 500
58:28 - and then the null probability
58:32 - of a purchase which is 0.1
58:36 - if we do this and we print out this
58:38 - value it's going to be
58:40 - about two times as big as that
58:46 - but yeah so it's about
58:50 - 20 or 0.2 but if we do alternative
58:56 - equals less to tell it that we want this
59:00 - alternative hypothesis to be that the
59:02 - probability of a purchase is less than
59:04 - this 10 then we should get
59:09 - about exactly 0.1 about exactly 0.1
59:13 - so we've seen that basically
59:17 - you can use these built-in functions and
59:19 - they'll give you
59:20 - the same roughly the same value
59:23 - as this simulation that we just did to
59:27 - calculate the p-value
59:28 - the reason it's not exact is that we
59:30 - used a simulation
59:32 - um if we did more iterations like more
59:35 - than 10 000
59:36 - the more we did the closer it would get
59:38 - but
59:40 - it's pretty cool that now we can
59:43 - now we can basically write this binome
59:45 - test function
59:47 - if we wanted and actually in the lesson
59:49 - um if you're taking it on codecademy
59:51 - it'll walk you through that whole
59:53 - process of writing your own
59:55 - binome test function which i wish we had
59:58 - time to do right now but
60:00 - yeah so so i had a question about that
60:03 - that idea of like
60:04 - it's off by a little bit the actual
60:07 - binome test
60:08 - function um is using like a formula
60:11 - right we're
60:12 - basically what we just did is we like
60:13 - approximated this whole process by doing
60:16 - a simulation of running this thing 10
60:17 - 000 times and like you said if we crank
60:19 - up that number to a million
60:21 - it's going to take our computer a longer
60:22 - time to to run all those simulations but
60:24 - we're probably going to get closer to
60:26 - that number the real the like scipy
60:30 - version of this function is just
60:31 - plugging it into a formula right that's
60:33 - like derived from this process or
60:35 - similar to this process yeah so
60:38 - basically
60:39 - the real sci-fi function is instead of
60:43 - using this like simulated null
60:45 - distribution that we made
60:46 - it's using a binomial distribution
60:50 - with parameters n equal 500
60:53 - and p equals 0.1 and that's gonna
60:57 - give you like it it basically just
61:00 - describes exactly what this shape
61:02 - should look like if you did it
61:05 - a bajillion times infinity times right
61:08 - so really the issue is that our
61:09 - simulation
61:10 - didn't actually capture the binomial
61:12 - distribution perfectly because we didn't
61:14 - do it a billion
61:15 - times um but there's some definition out
61:18 - there of this is what a
61:19 - binomial function looks like with these
61:21 - parameters and we can just
61:22 - plug it into that and get the like the
61:25 - permanent p-value for this
61:26 - yep cool exactly cool
61:30 - so i think that's about it for today but
61:33 - hopefully you feel like you learned
61:35 - something
61:35 - and you understand the binomial tests a
61:38 - little bit better
61:39 - um here i'll stop sharing my screen is
61:42 - there anything
61:43 - you want to add about a second or two to
61:46 - ask any last-minute questions
61:47 - um sophie do you want to preview what
61:49 - we're doing next week
61:51 - sure i should i actually like need to
61:54 - look up what we're doing next week
61:56 - also everyone tell some people because
61:58 - sophie's you know sophie's running these
62:00 - every single week
62:01 - and prepping for them so good job sophie
62:04 - it's very impressive
62:05 - thank you i love doing this i i wish
62:08 - that more people would find them and
62:10 - come
62:10 - and ask us questions because it is very
62:14 - very fun
62:15 - um and i hope that it's helpful for
62:17 - people as well
62:19 - and hopefully people will find them if
62:20 - they're working through the master
62:21 - statistics path
62:22 - uh massachusetts statistics with python
62:25 - path
62:26 - because i i do think it would be a
62:27 - really good compliment to kind of
62:30 - see someone working on the stuff in
62:31 - action i know i feel like
62:34 - we're all in our homes right now i'm
62:36 - kind of isolated still
62:37 - the isolation fatigue and zuma fatigue
62:39 - is real and so
62:41 - it's nice sometimes i think to like just
62:43 - see some other people
62:44 - talking about stuff um
62:48 - yeah uh what is next week
62:52 - let me see i can look it up if you don't
62:53 - have it
62:56 - i've got a significance threshold and
62:59 - multiple hypothesis tests
63:01 - oh oh yeah so next equal we'll kind of
63:04 - talk about the
63:05 - um the issues that can arise if you run
63:09 - a bunch of
63:09 - hypothesis tests and has to do with the
63:12 - fact that
63:12 - probability like all of these tests are
63:16 - regarding probabilities and the more
63:18 - time you do something
63:19 - the more likely it is that even rare
63:23 - events will happen
63:24 - um so it's kind of it's
63:27 - super important because it has to do
63:29 - with the whole like reproducibility
63:32 - crisis which is the idea that like a lot
63:33 - of
63:34 - published papers are not reproducible
63:36 - with new research
63:38 - um and it's really important for anybody
63:40 - who's looking to go into data science
63:42 - and looking to uh
63:44 - or statistics and like looking to run
63:46 - these kinds of tests in their own
63:48 - research
63:49 - because if you're not aware of the issue
63:51 - then
63:52 - um then you contribute to it so
63:55 - yeah awesome well i
63:59 - i'm glad that people find it helpful
64:01 - finding this helpful
64:02 - um i don't see any other questions so
64:06 - with that i think we'll sign off but
64:09 - this was fun cool glad to have you back
64:11 - alex yeah thank you
64:13 - i'll be back next week too so see you

Cleaned transcript:

all right i think we're good to go hey hello um okay i think we will get started in just a minute but i see that some people are starting to say hello in the youtube chat uh thank you dr monkey uk for always letting us know that you can see us see we're getting better at the awkward silence at the beginning yeah we just really leaned into the awkward silence this time i know i have no idea until after i watched this later how long that was cool um so i well we say this at the beginning of every stream but we're streaming on a few different services but we're keeping an eye on the youtube chat so if you'd like to chat with us ask questions um please go there and and let us know we'd love for this to be as interactive as possible so we love your questions and would love to hear from you if you have any suggestions or ideas cool uh sophie do you want to give a little preview of maybe what we did last episode and then uh what we're doing today yes also thank you for the the haircut shout out my uh my boyfriend did this and i'm it's a little uneven it's pretty good actually yeah i just opted for no haircut in the 11 months um yeah so last week we went over the central limit theorem um and we started to get an understanding for inferential statistics which is a field of statistics that's focused on making inferences or i'm trying to think of a better word than inferences uh like making judgments uh about or guesses about a population based off of a sample that we can observe and that's really the basis for a lot of hypothesis tests and now today we're going to jump into an example of a hypothesis test where we're specifically going to look at the binomial test and the binomial distribution um which is maybe not the most ubiquitous or like most common hypothesis tests you probably or most people would think of like a ttest or a ztest is the most common um but it's a fun one because it's really easy to simulate um and you don't need any math in order to basically write a function that can run a binomial test and i think that when you're first learning something understanding how a function works is really important um and more important than being able to say in python just use a hypothesis testing function like there's lots of functions that exist in various libraries for conducting a ttest or a binomial test or chisquare test or any sort of hypothesis test that you want and all you need is one line of code but if you only ever learned how to implement the one line of code um you might not have a sense for how those functions are really working and that can make it harder to interpret things or to figure out if something went wrong and then it's also just i don't know i think a good opportunity to kind of think about inferential statistics in general what the goals are what the assumptions are and build your build your understanding of of the topic at hand um yeah someone that doesn't have a super strong statistics background i think that this is the kind of stuff that's really helpful for me where it's like i kind of know what these tests are i kind of know in what circumstances to use one over the other but really diving in trying to build this thing yourself seeing the um using the simulation to see what this uh what these functions are actually doing um can help give like a more solid understanding of what they're doing so then when you're out in the real world and you encounter a new situation you can say like okay this is exactly like you know what we did in this lesson versus another lesson exactly so with that i am going to share my screen um let me make sure i get the right screen um we're back to uh jupiter notebooks um these notebooks are in our github repo if you want to download them with the data um also just a heads up we added them like 15 minutes ago 20 minutes ago uh it's a little bit running a little bit late today so if you checked the github earlier today um and didn't see them they should be there now and uh if you're watching this after the fact you can go ahead and download those and run the code on your own computer cool so um today's lesson i'm going to start with the starting code file and load this monthly report file and i'll print it out um so this is actually directly from a codecademy lesson so if you're interested in taking that lesson uh i think maybe alex can post where it is yeah drop the link to the lesson um this this live stream series is all kind of a companion piece to the the new skill path that just launched um like a month or so ago so i'll drop a link to that in the uh in the chat yeah so this uh this is a simulated data set that alex and i actually made we had quite a lot of fun making it um but we're gonna pretend like uh we we work for our company and um and this company is collecting some data on purchases and items that were purchased and so uh we're imagining this this company is selling uh props to use to recreate your favorite scenes from movies um and so this is what alex and i spent probably a on a stupid amount of time on on a day when we were having trouble focusing is coming up with all of these props that you might buy um so you can take a look at the full data set if you're interested and see what other things we came up with there's some there's some funny ones in there um but this is this is the first few rows so we're imagining that um that there's there's data i think um we can get like monthly report dot info and just see a little bit more so there's 500 rows um 500 non nonnull values so nonmissing values for each of these two columns um and each one is telling us why or yes yes or no did the did the customer who visited the website make a purchase and um and then if they did make a purchase so if this is a y where we're saying what did they buy and then the item is listed here as well so um basically we're gonna imagine that we are a um like a marketing team and at this company and we're trying to understand we have some idea for what the purchase rate among visitors should be um and we're gonna see if the purchase rate among this sample of people is lower than normal or higher than normal um and essentially the idea here is that maybe there was like maybe there was something that we were testing out like we were testing out a new feature or a new checkout system or there was a bug that some people saw and we want to know we want to have some understanding of whether that change had an effect on our purchase rate and that and we're thinking about this from the perspective that let's say it was a new feature like um a new checkout system that we that we randomly showed to some subset of our visitors if we did that we want to know whether if we show if we showed that same feature to all of our visitors would the purchase rate really be different from our expectation but in this case we only have a sample we only showed it to a small a small proportion of all of the visitors who could ever arrive at our site and so we don't know whether we don't know what would happen if we showed it to every single person and so with that in mind we need a hypothesis test to try to understand whether the difference that we observe in this sample is large enough that we would expect that difference to persist if we showed this thing to more people so so in this example we have 500 rows maybe that's like the purchase it or the people that visited the website on a on you know a tuesday or whatever the most recent day that we pulled this data we have 41 people bought something and so that's a percentage right it's 41 out of 500 that's some percentage and we're curious does that is that percentage different than what we expect right exactly and where does that that number that we're going to be comparing it to if like what we expect in the real world where does that number come from is that just like historical data of like oh my company the alltime set sales percentage is you know this percentage or that percentage so yeah that's a really good question um i think in general yes it comes from historical data we do have to be careful about thinking about how we use historical data given the sample that we have like we really want our sample in order to make this in order to meet the assumptions of this test and we'll go more into that once we get started we need the sample to be representative of the population and so if we're basing let's say we're basing our historical data off of like an entire year's worth of data but maybe like maybe this purchase rate fluctuates over the course of the year like maybe it's higher in the winter and when people are inside and looking for things to do and lower in the summer when everyone's at the pool and or whatever or outs outdoors um and so maybe you know maybe the change that we're if we're only sampling from one month or one day then um we shouldn't really be comparing to like all historical data uh so we we do have to be careful about our choice there um in real life but yeah it can come from a lot of different places like you'll also see these types of tests if you're if you're taking a sample for um for like a experimental study and you're trying to see whether your sample um is representative of the population and you have some averages or you have some like expectation based on like previous research um that has been done or yeah or sometimes it'll be like a cut off score like if you if 50 of or if you get 50 or higher on this test you pass or something like that so how about like oh if the folks that we're showing this new feature to or you know the new checkout system too if those are like the most recent subscribers or the most recent people to visit the site is that an issue where that's like the most recent people is not representative of the entire population i mean yes it's true that that's all that's always going to be the case but like at some point you do have to uh like kind of give up on meeting all assumptions in real research unless you're running like a very controlled experiment where you are splitting people into two groups and randomizing everything and yeah cool okay cool okay so we are going to get started then um so okay so let's just imagine um that the the expectation is that the purchase rate is usually about 10 so like about 10 of visitors to this website usually make a purchase and we want to know if the purchase rate was significantly different from that so the first thing that i think it makes sense to do if we're going to try to answer this question is figure out what the purchase rate is in our data so we've got 500 visitors the site we want to know if the purchase rate was different from 10 percent so let's let's see if we can calculate the purchase rate in this sample and i'm actually going to divide the purchase rate into two numbers because it's going to help us with this like the way the binomial test is set up so the first thing i'm going to calculate is the the numerator the the top of that fraction for the purchase rate which is the number of people who made a purchase um so there's a few different ways to do that alex do you have i'll do it the way that i that like makes sense to me and then uh you can let me know if there are other ways that you've thought about doing this sure um but so monthly if i do mp.sum and then i take monthly report equal equal y so actually let me break this up what i'm first doing is monthly report is equal to yes is going to return a bunch of oops i should do dot purchase so we just get one column of this is going to return a bunch of trues and falses telling me whether or not that value is equal to y so it goes true false false true false because here we've got yes no no yes no and it turns out that there's this nifty thing where true gets a gets cast to a one if you try to perform any sort of operations on it and a false gets cast to a zero so if you take the sum of all these numbers it's gonna add a one every time there's a true and a zero every time there's a false so when i do this i will get a single number which is 41. yeah funny that you prompted this so because as you were developing the uh the course on codecademy this is something that i remember reviewing of like the sum here seems weird to me the way that i would have done it is i would have gotten all of the rows or like i would have extracted all the rows where that that column is a y and then i would have just counted the total number of rows um so i would have like filtered it as opposed to um summing it i guess so you would have done like yeah that yeah so just to repeat what alex just said he's taking the whole data set he's subsetting it to just the rows where a purchase was made and then he's figuring out how many rows are in that subsetted data set by taking the line so and you'll see we got the same numbers that's good and the length of the whole data set we already saw this from the info section but if we say len monthly report which gives us the row number of rows um that was 500 okay so the purchase rate is going to be 41 over 500 and i'll just print out what that is it's 0.082 okay so remember that we said our expectation was 10 percent so that's 0.1 here i can put this actually as a percentage because it's probably easier to read so it's an 8.2 purchase rate our expectation was 10 so this is a little bit above normal i also just saw somebody posted in the comments um you could also do value counts um with normalize equals true and that's totally true as well there's as per usual there's many many ways to do this i'll also demo that because it's kind of fun that'll give us both the number of yeses and no's so that gave us this 0.082 for the s's and 0.918 for the nose and if we took this normalize equals true piece out we would get the counts so that's cool um okay now and so we've seen that the purchase rate was lower than normal but that doesn't fully answer our question because like we said at the very beginning of this just because the purchase rate is 8.2 percent doesn't mean that there's really a dip in the purchase rate if like ev if we showed the same feature that these people saw or the same bug that these people saw if we showed that same thing to all of our visitors because this is just a sample and there's some random chance involved so one of the things i like to think about and i think like this is probably the natural the natural progression or like the natural way that many people teach the concept of the binomial test is to think about flipping a fair coin um i like it as an example so if you imagine you're flipping a fair coin right if you the sorry the probability of heads is 50 so you would expect that about half of your flips would be heads say so let's say you flip your coin ten times you expect that about five of them are going to be heads but because this is a stochastic process it's a random process it's not going to be exactly five every single time right it's gonna be it's gonna vary each time you do it if you flip a coin ten times you might get four heads and if you do it again you might get five if you do it again you might get seven um but the more you do it the closer you're gonna get to that 0.5 or that half level so we can actually demo this um in python i'm gonna so i'll demo actually the the coin flip first so let's say we do um numpy.random.choice we're gonna choose between heads and tails and we'll for right now we'll do one or actually let's do 10 coin flips and we're going to say p equals 0.5.5 which is to say that the probability of heads is 0.5 and the probability of tails is 0.5 if we do this whoop shoot i think these are supposed to be one list sorry yeah there we go we just flipped a coin 10 times on a computer we got heads heads tails oh my god we got nine heads wow that's crazy let's try it again okay now we got closer wow it's still really funny we're getting like six heads in a row and then four tails actually i was reading about this because we made some prop bets for the uh for the super bowl that apparently it was like i think a bunch of heads or tails in a row it was like a bunch of tails in a row then a bunch of the last five six or the last seven have been tails or something like that of the super bowl like uh coin flips that's funny yeah so you know this this happens right you get a bunch of the same thing in in a row just by random chance um so now i'm going to up this a little bit because i don't want to print it out every time so let's save these as flips and let's just print out let's use this same code that we had up here to every time just print out um flips equals head so the number of heads and now let's up the size to like 100 say okay so not 100 flips you got 46 heads 49 and and so bringing this back to our example of the um basically this is kind of like okay we saw 41 purchases is that you know if it was actually 10 we would have seen 50 right because we had 500 total um or we would have seen yes 50 sorry yeah right because 500 people either said yes or no i'm buying the thing or not we expect it to be 10 so we're saying okay that would be 50 people we saw 41. so is the question here that we're asking like how big of a deal is that 41 is it like getting a 52 here or is it like you know if we if we keep printing this out we're pretty much never going to see a 90 or a 100 right um so that's kind of like the comparison here of like how far away are we from the expected thing that we that we expect to get exactly so right so the question here might be like if you're let's say you uh work at a casino and you're you're checking uh a coin i don't know i guess they don't really use coins at casinos but you're checking to see whether this coin is fair like your friend has decided that has offered to play some game where you win if the coin comes up heads and they win if the coin comes up tails and so you're trying to check whether the coin is fair you can't flip the coin infinitely many times to see what percent of them come up heads but you can take a sample so here we're kind of thinking of the sample as being like a sample of all the coin flips that could ever exist it's a really weird thing to think of like a population as being coin flips and a sample as being some subset of those coin flips but that's kind of how we're thinking of it we're thinking of it as like population is all the coin flips that could ever exist in the world and of all the coin flips that could ever exist in the world of a fair coin half of them would be heads um and this is some subset and based on that subset you're saying okay so we expect some random chance so we we expect that even though this should come out too close to 50 it's not going to come out to exactly 50 every single time we might get some weird numbers like just by random chance that time we got 41 heads this time we got 48. but if we flip the coin a hundred times and 99 times it comes up heads we could be pretty convinced that it's not a fair coin um because that would be kind of outside the realm of possibility here by random chance yeah and i i have a question this might relate to something we you do in a second but like to me it seems like there's two things involved here one is the sample size of like okay we did it a hundred times and it's off by this amount and then there's also the how much is it off by so like are those two connected in some ways of like okay i flipped my coin a hundred times and um you know uh it came up heads 70 times versus i flipped my coin 10 times and it came up seven times like how are those two things connected the the sample size and then also how far away you are from the expected thing what a great question alex that was really good um so actually this comes exactly back to what we discussed last week so last week we were looking at a normal distribution so okay let me actually just show you something so let's say i say flip results equals empty set and we're going to do for i n range thousand um and then we're gonna say or you know what actually i'm gonna instead of showing you all this code i'm just gonna show you a picture that i already have i'm gonna pull it from codecademy um okay we're gonna i forgot i didn't fully share my screen hold on actually okay we'll do this sorry um okay so we've got uh we're gonna do this a thousand times every time i'm gonna flip this coin save the number of heads and then i'm going to append that to to my list and then i'm going to use i'll run this i think i can do it 10 000 times without too much of an issue and then i'm gonna print a histogram of that yeah and i think as you're as you're writing this out a thing that gets kind of confusing here is we're now doing this experiment of flipping the coin 100 times we're doing that whole process 10 000 times so there's kind of like layers within layers here exactly so yeah i think i coded that really quickly but just to repeat what i did so by creating by putting this inside of a for loop with ten thousand iterations i said i'm going to repeat what i just did up here which is flipping a coin a hundred times and recording the number of heads i'm going to do that process 10 000 times so 10 000 iterations of me flipping a coin 100 times and recording the number of heads and then here i'm plotting a histogram of um those results and actually i'm gonna like uh run this again because this is a little well okay it's a little off center but if i did this enough times this would be like perfectly kind of centered around 50. and this looks a lot like that's a really weird one 15 make this look nice yeah i should just be starting to use the seabourn the seabourn one that just automatically does this part for you finds the right number of bins um cool so uh anyway we plotted this histogram of the number of heads and we see kind of like we expect the most common outcome is that it's close the number of heads is close to 50 and it looks like the number of heads is ranging roughly from like 35 to 65 with this one outlier like one time we did it we got 70 heads which was a lot but for the most part it's like roughly between 65 and 35 and each of the farther away you get from 50 it's less likely this is called the binomial distribution um and you might think after if you watch last week sorry last week's stream you might think like well that looks a lot like a normal distribution and it actually really is the only difference between this distribution and a normal distribution is that this is what's called dis a discrete probability distribution or a probability i think it's called probability mass function um where the value the number of flips that you can have that come up heads has to be an integer value like it it can only be 35 36 37 it can't be like 35.5 because you can't have 0.5 heads but otherwise this is really just a normal distribution and actually like the central limit theorem essentially applies here the larger the numbers you use like the higher the number of flips the closer this is to a normal distribution um so it approaches we say a normal distribution um and remember from the central limit theorem that the standard error which is describing how wide this thing is is calculated as the standard deviation of the population divided by the square root of the sample size and so you're dividing by essentially the sample size so as the sample size gets larger the variation that you expect in this distribution gets smaller and that and that's like sounds complicated but that's the whole like i think that's the maybe the most beautiful thing about all of this is that you end up with these really complicated formulas a lot of times in statistics and people like tell you give you the formulas and ask you to ask you to apply them but i feel like if you see it in this sense it just makes sense like it you don't have to think about it as dividing by the square root of the sample size to get the width of that distribution you can think about it as well like of course the more you the more you do this the more certain you're going to be that about like the proportion of heads so if you in in this case is the sample size 100 because we're flipping 100 coins each exactly if we go and change that to flipping 10 uh in theory we'll see the variance be a lot larger right so it's going to be yeah we're not going to see it so easily in this plot because um because we can't change like we'd have to set the range for this we're going gonna adjust the range of the histogram automatically but yes cool cool can uh can we can can i just see can can you change it i just want to see it and then we can try to remember the ring so you're saying like the range right now is going from like 35 to 65. yeah if you don't think it's gonna be instructed then i think it's it's it's gonna be a little weird because as you get closer to zero it gets like it gets squashed okay um but but we did show it we did demonstrate this yesterday or not yesterday last week yeah um so watch the other live stream and you can see this in action cool um all right so let's come back to our question and let's or let's come back to our data set and let's like basically do the exact same thing we just did but let's do this in the context of our research question so before we sorry we calculated that there were 41 purchases out of a sample of 500 and we want to know if that's really different from 10 percent so what we're gonna do is we're gonna kind of we're gonna simulate some more samples that are just like this so here we're going to say like simulated visitors and our simulated visitors can either make a purchase yes or not make a purchase no our sample had 500 visitors so we're going to keep that the same and now for this we're going to set our x we're going to use our expectation so we're going to say like we expected the purchase rate to be um 0.1 or 10 right so we expected the probability of a purchase to be 0.1 and the probability of not a purchase to be um and then we'll call this purchases and that's going to be the sum of when simulated visitors equals yes and we want to print that out and then oops and then let me just do this once so what i did here was i said okay let's imagine for a second that the real probability of a purchase is 10 how much of a range am i going to see in the number of purchases that occur among 500 visitors if the true probability is 10 is equal to my expectation basically so i'm going to do this a few times remember that with a 10 purchase rate we expected that among 500 visitors we would have 50 purchases so we're seeing numbers kind of around 50. we had 59 then we got 45. yeah because in our in our real data set we had 41. in our real data set we had 41. um so we're definitely starting to see that we can get numbers that are kind of far away from 50 by random chance and now we're gonna basically do the same thing we did with our coin flips we're gonna say okay if i do this a bunch of times and then i plot the histogram of these numbers and each of those numbers being the number of purchases in a simulated sample of 500 people where the purchase probability was equal to 10 very complicated sentence then what kind of distribution am i gonna see in terms of the range in the number of purchases that people are are making in this simulated so i'm going to copy this over except i'm going to replace all of so i'm going to call this now i'm gonna call this null purchases i'll explain why i'm calling it that in a second um and we're gonna append purchases to that and then i'm going to grab this code and drop that in there okay alex do you want to like walk through as i run this what this code is doing again right so this is this is similar to the flipping a coin a hundred times getting the how many heads you we had and then uh and then doing that process ten thousand times and trying a histogram based on based on those results this is the exact same thing except for now rather than flipping a coin 100 times we're looking at 500 visitors and rather than the coin being 50 50 we're saying um we only expect 10 of those visitors to purchase the thing exactly okay so when we do that we did it 10 000 times simulated 10 000 samples of 500 in each of our samples of 500 each person had a 10 probability of making a purchase um and we collected for each sample of 500 how many purchases were made in our simulation and then we plotted this histogram and we see that the number of purchases is ranging from like 30 to 70. again the most our expectation which was 50 is the most common result um but we see we see this kind of range and now we can figure out where our observed value was so our observation and we can do we can l t dot a x v line um and then put that line at 41 let me get red um right so here's our observed number of purchases and we can see that it's not not like uh it's not quite as likely as 50 it's not out of the question it's within the certainly within the realm of possibility it's not like way out here at 29 or 20 or something crazy or a way above like 80 right it's it was within the realm of possibility but it's less likely than this kind of like middle range of numbers yeah two things that i i want to say here one is that i think this graph is potentially a little bit misleading if you're just looking at the graph without labels because it kind of looks like it's a percentage right it ranges like between 30 and 70 with 50 right in the middle so remember you know it was possible to have um 500 people buy uh buy the product and it would be possible for us to have like you know a bar over at 500. it just so happens that um because uh the expectation is that 10 of the people buy the buy the thing and we're sampling 500 people the middle happens to be 50 but this in like no way is connected to percentages which i think um could be confusing because we're in this example we like a lot of things that are like percentages coin flips 50 50 right and so it's a little bit uh uh unlucky or you know it's a little bit um confusing that the middle here happens to be right around 50. that is very helpful point thank you um the other thing that i wanted to ask i think when we bring this into the real world like it makes sense with a coin of flipping 50 50. but if we when we bring this into the real world of like yes my person can either buy buy something or no they can't a natural question that i had at least and i think i know the answer this based on the fact that this is called a binomial um test uh can you run this with three values right so is it did they buy like the uh using codecademy as an example did they buy the monthlong version or the yearlong version or do they pass altogether right there's three options there what do you do in that situation is this applicable at all would you combine the the two forms of yeses into one form of yes like what do you do if you have not a binary thing yeah so if you have not a binary thing then it depends on what your question is but if your question is basically is there an association between what somebody saw and what and their probability of making a purchase if that's what you want to answer then you need like basically if you if you don't care if if you care if there's any difference between those three or more groups then you're gonna need a chisquare test so um that's a different kind of hypothesis test we'll cover that in another week when we start talking about hypothesis tests for an association um but yes there's there is a hypothesis test for that the other thing that you could do is you could try to binarize it if you really only care let's say you have like three conditions and two of two of them are um i don't know like fallen like you can kind of combine two of them and you really only care about the one that's that you expect to be different then you can always turn it into a binary variable and use something like this yeah i'm struggling to come up with an example we have a we have a good question in the chat from andrew um i guess could you talk a little bit about the yaxis here and what that's showing where these values are a little bit hard to read of oh i just changed it because i saw that so yeah i just changed this to density equals true um as you were talking so density equals true changes this to a um basically to a probability um it's a little bit tricky because like in the case of the normal distribution it will it will make it such that like the area of each basically it will make it such that the area of each of these bars is a probability of observing the number in that area like or the number in that range so for example um this bar looks like it's about at 0.4 and it runs from like 41 to 47 maybe or 46 and so if you so it's maybe like a bar that's 5 units wide times 0.04 high so if you do 5 times 0.04 it'll give you the area of this bar and then that will tell you the probability of observing between 41 and 46 or whatever this is purchases by random chance if the true purchase rate was 10 yeah and that and that's compared to our owned yaxis if we didn't put in um density equals true then that's just like the counts of how many times we we saw these occur right we ran this experiment 10 000 times and 2 000 times they fell between 40 and 47. exactly that is a very good question um okay so now i'll come back to this so the reason i called this null purchases is that in the context of hypothesis tests we usually frame a hypothesis test as starting with a null hypothesis and an alternative hypothesis so the null hypothesis of this binomial test that we're running is that the probability of a purchase is 10 percent um i should write that down so null hypothesis is that the probability of a purchase is 10 and this is kind of in keeping basically the null hypothesis says that our expectation is true we expected there to be a 10 purchase rate and there is a 10 purchase rate the alternative hypothesis we have a little bit more um leeway to decide what we want this alternative hypothesis to be but we could for example say that the alternative hypothesis is that there's fewer that the purchase rate is below 10 we could also make the alternative hypothesis that the purchase rate is a above 10 or we could just say the purchase rate is not 10 it could be less than or greater than um we have some choice but let's say let's say for right now like we only really cared to measure whether the probability of a purchase was below 10 because let's say what happened was that like a bug got thrown to a randomly selected group of 500 visitors base they all saw a bug and we wanted to know if that bug influenced our um our purchases and so we expect the purchase rate to go down um and so maybe our alternative hypothesis is that the probability of a purchase is less than 10 and this is this is very like pedantic but like can you do that can you say like of course a bug is not gonna result in more purchases or like we expect you know we expect it to go down and so we're only going to check to see if it actually went down um yeah i mean so basically like you should decide if you're running an experiment you should decide what your hype alternative hypothesis is ahead of time and there's some complic sort of complicated math associated with this but the idea is that if let's say i think it's easier to think about like if you are trying to um if you're trying to create a new feature that's going to increase the purchase rate um and look and let's say then like you only really care if your new feature improves your um your your purchase rate because if it doesn't improve the purchase rate then you're definitely not if it like if it decreases the purchase rate you're definitely not going to implement that feature if it if it doesn't change the purchase rate you're not going to then we don't care then you don't care so in that case you might want to run a onesided a onesided test which is to say you might want to make your alternative hypothesis be that the purchase rate is greater than 10 because you don't care about the other situation um as you'll see in a second i hope we get time to do it like if you run if you use the alternative hypothesis where you're saying like it could be greater or equal or sorry greater or less than you're gonna be less likely i make sure i get my words right um you're gonna get a larger p value so you're gonna be less likely to get significant results um but if you do the wrong onesided test like you say it's greater than when it's really less than then you have no it's called like pow your power to reject the null hypothesis is um like goes down so anyway it i feel like i'm getting into too complicated of a topic at this point but yes the like the bigger picture is you should make this decision beforehand and it depends on your research research question um cool okay so what i wanted to say is that the reason that i'm calling this null purchases is that basically the key to any hypothesis test is knowing what the null distribution is and the null distribution is basically the distribution of a sample statistic that you care about if the null hypothesis is true in this case the sample statistic that we care about is the count of the number of purchases and the null hypothesis is that the purchase rate is 10 and so our null distribution is the distribution of like imagined simulated uh samples of 500 with a purchase rate of 10 the number of purchases among those simulated samples this is our null distribution and what i'll say is when you run a binomial test you're running a binomial test because the null distribution is something called a binomial distribution and if you run a t test the reason is because the null distribution is in the shape of a student t distribution which is essentially the same as a normal distribution um if you run a chisquare test it's because the null distribution is chisquare distributed so basically like the magic of statistics is knowing what this distribution is going to look like without actually having to do what we did which is simulate all of those samples right so this is an example where inherently the pro the problem that we're or the question that we have is going to end up with a simula with a distribution like this and like the shortcut that you can take once you learn this stuff is knowing that okay this problem is going to result in a binomial distribution and there therefore i'll run the binomial test exactly so okay so let's um let's actually calculate a pvalue for our for our test so essentially what a pvalue is is and i'm gonna like speak slowly so i get this correct is it's the probability of observing a range of statistics which is or a range of values which is defined by the alternative hypothesis so in this case if our alternative hypothesis is that the purchase rate is less than 10 our pvalue is going to be the probability of observing the number of purchases we got or fewer given that the null hypothesis is true so in this picture what this equates to is like the area if we do density equals true again basically is proportional to the area of this distribution that's to the left or smaller than this red line that we drew and that qualifier of like if the null hypothesis is true that's that means that's the experiment that we ran right that's the experiment of 10 of um you know we have 500 people and 10 of them are going to um buy the thing and so it's going to end up with a distribution like this yeah so basically we take the null distribution and then based on the alternative hypothesis we say what proportion uh or what's the probability of observing the alternative hypothesis or basically what's the probability of observing what we observed or something more extreme in the direction of the alternative hypothesis so in this case less than um more extreme is even less than 41. what's the probability of observing that if the null hypothesis is true and that's basically going to be an area of the null distribution where you draw the null distribution you draw a line at your sample statistic observed sample statistic and then you take the area outside of that um so if i were to eyeball this i would say that that's like maybe 10 that area is like maybe 10ish percent of the uh of the total area under the curve i don't know if it's going to be exactly that but that's kind of what we're looking at right the area to the left of that red line is a small area compared to the rest of the area i think your your estimation cells skills are very good um okay so the p pvalue is going to be the sum of null outcomes or what did i call it null purchases i'm copying this over from codecademy uh so this is right the top of this fraction is the number of values in num purchases the number of simulated purchases that are less than or equal to 41 divided by the total number of simulations which in this case um is 10 000. and so if we print that whoop let's see oh i think you have to array okay yep so this is roughly and i'll do times tens well no i won't do that so basically this is like 10 which is exactly what um alex estimated and so basically we're saying that if we randomly simulated i'm just going to say it again because i feel like it helps to just keep saying it if we randomly simulate 500 visitors and we repeat that process 10 000 times each time recording the number of purchases that were made and in each of those simulations the purchase rate really is 10 where the probability that we observe 41 or fewer purchases given that the purchase rate was 10 is about 10 um all these numbers lining up is probably not super helpful um the twosided pvalue is essentially going to be two times that because theoretically we would just draw another red line on the other side of this that's equidistant so this one's at 41 so we draw another line at like 59 so nine units away from 50 on the other side and add in this area over here um the reason i'm saying that so when i say twosided pvalue i mean if we said that the alternative hypothesis is that the probability of a purchase is not 10 then we would want to calculate this like both the area of both sides both extremes um the probability that the number of purchases was nine fewer or nine more than 50. um and the reason that's important is that right now because we're getting close to the end of time i want to just demonstrate that you can also do this with the builtin function and i want to demonstrate that you get the same thing but the builtin uh python function that at least all the ones i found calculate a twosided pvalue by default so um let's just uh i'm just grabbing the solution code i think it's a scipi function that i've been using but i just want to grab it um okay so let's do let's do this so let's import from scipy dot stats import binome test so now we're going to do this in a single line of code we're going to say give me the so the p value from this this function is equal to binome test the parameters here are the observed value which in this case is 41 the sample size which is 500 and then the null probability of a purchase which is 0.1 if we do this and we print out this value it's going to be about two times as big as that but yeah so it's about 20 or 0.2 but if we do alternative equals less to tell it that we want this alternative hypothesis to be that the probability of a purchase is less than this 10 then we should get about exactly 0.1 about exactly 0.1 so we've seen that basically you can use these builtin functions and they'll give you the same roughly the same value as this simulation that we just did to calculate the pvalue the reason it's not exact is that we used a simulation um if we did more iterations like more than 10 000 the more we did the closer it would get but it's pretty cool that now we can now we can basically write this binome test function if we wanted and actually in the lesson um if you're taking it on codecademy it'll walk you through that whole process of writing your own binome test function which i wish we had time to do right now but yeah so so i had a question about that that idea of like it's off by a little bit the actual binome test function um is using like a formula right we're basically what we just did is we like approximated this whole process by doing a simulation of running this thing 10 000 times and like you said if we crank up that number to a million it's going to take our computer a longer time to to run all those simulations but we're probably going to get closer to that number the real the like scipy version of this function is just plugging it into a formula right that's like derived from this process or similar to this process yeah so basically the real scifi function is instead of using this like simulated null distribution that we made it's using a binomial distribution with parameters n equal 500 and p equals 0.1 and that's gonna give you like it it basically just describes exactly what this shape should look like if you did it a bajillion times infinity times right so really the issue is that our simulation didn't actually capture the binomial distribution perfectly because we didn't do it a billion times um but there's some definition out there of this is what a binomial function looks like with these parameters and we can just plug it into that and get the like the permanent pvalue for this yep cool exactly cool so i think that's about it for today but hopefully you feel like you learned something and you understand the binomial tests a little bit better um here i'll stop sharing my screen is there anything you want to add about a second or two to ask any lastminute questions um sophie do you want to preview what we're doing next week sure i should i actually like need to look up what we're doing next week also everyone tell some people because sophie's you know sophie's running these every single week and prepping for them so good job sophie it's very impressive thank you i love doing this i i wish that more people would find them and come and ask us questions because it is very very fun um and i hope that it's helpful for people as well and hopefully people will find them if they're working through the master statistics path uh massachusetts statistics with python path because i i do think it would be a really good compliment to kind of see someone working on the stuff in action i know i feel like we're all in our homes right now i'm kind of isolated still the isolation fatigue and zuma fatigue is real and so it's nice sometimes i think to like just see some other people talking about stuff um yeah uh what is next week let me see i can look it up if you don't have it i've got a significance threshold and multiple hypothesis tests oh oh yeah so next equal we'll kind of talk about the um the issues that can arise if you run a bunch of hypothesis tests and has to do with the fact that probability like all of these tests are regarding probabilities and the more time you do something the more likely it is that even rare events will happen um so it's kind of it's super important because it has to do with the whole like reproducibility crisis which is the idea that like a lot of published papers are not reproducible with new research um and it's really important for anybody who's looking to go into data science and looking to uh or statistics and like looking to run these kinds of tests in their own research because if you're not aware of the issue then um then you contribute to it so yeah awesome well i i'm glad that people find it helpful finding this helpful um i don't see any other questions so with that i think we'll sign off but this was fun cool glad to have you back alex yeah thank you i'll be back next week too so see you
