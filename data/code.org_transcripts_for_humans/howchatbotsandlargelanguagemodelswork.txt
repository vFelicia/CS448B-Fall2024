With timestamps:

00:07 - hi I'm Mira morati I'm the chief
00:10 - technology officer at open AI the
00:12 - company that created child GPT
00:14 - I really wanted to work on AI because it
00:19 - has the potential to really improve
00:21 - almost every aspect of life and help us
00:25 - tackle really hard challenges hi I'm
00:29 - Crystal Valenzuela CEO and co-founder of
00:31 - Runway Runway is a research company that
00:35 - builds AI algorithms for storytelling
00:37 - and video creation
00:40 - chatbots like Chad gbt are based on a
00:43 - new type of AI technology that's called
00:46 - large language models so instead of a
00:49 - typical neural network which trains on a
00:52 - specific task like how to recognize
00:54 - faces or images a large language model
00:57 - is trained on the largest amount of
01:00 - information possible such as everything
01:03 - available on the internet
01:06 - it's raining to then be able to generate
01:08 - completely new information
01:10 - like to write essays or poems have
01:13 - conversations or even write code
01:16 - [Music]
01:17 - the possibilities seem endless but how
01:19 - does this work and what are its
01:22 - shortcomings let's Dive In
01:25 - while a chatbot built on a large
01:27 - language model may seem magical it works
01:30 - based on some really simple ideas in
01:33 - fact most of the magic of AI is based on
01:36 - very simple math concepts from
01:39 - statistics applied billions of times
01:42 - using fast computers the AI uses
01:44 - probabilities to predict the text that
01:46 - you wanted to produce based on all the
01:49 - previous texts that it has been trained
01:51 - on suppose that we want to train a large
01:54 - language model to read every play
01:56 - written by William Shakespeare so that
01:58 - it could write new plays in the same
02:00 - style we'd start with all the texts from
02:03 - Shakespeare's plays stored letter by
02:06 - letter in a sequence next we'd analyze
02:10 - each letter to see what letter is most
02:12 - likely to come next after an eye the
02:15 - next most likely letters that show up in
02:17 - Shakespeare plays are s or n
02:21 - after an s
02:23 - T C or h
02:26 - and so on this creates a table of
02:29 - probabilities
02:30 - with just this we can try to generate
02:33 - new writing we pick a random letter to
02:36 - start
02:38 - starting with the first letter we can
02:40 - see what's most likely to come next we
02:42 - don't always have to pick the most
02:44 - popular choice because that will lead to
02:46 - repetitive Cycles instead we pick
02:49 - randomly once we have the next letter we
02:52 - repeat the process to find the next
02:54 - letter and then the next one and so on
02:57 - okay well that doesn't look at all like
03:00 - Shakespeare it's not even English but
03:02 - it's a first step this simple system
03:04 - might not seem even remotely intelligent
03:07 - but as we build up from here you have to
03:10 - be surprised where it goes the problem
03:12 - in the last example is that at any point
03:14 - the AI only considers a single letter to
03:17 - pick what comes next
03:20 - that's not enough context and so the
03:23 - output is not helpful
03:25 - what if we could train it to consider a
03:27 - sequence of letters like sentences or
03:30 - paragraphs to give it more context to
03:33 - pick the next one to do this we don't
03:35 - use a simple table of probabilities we
03:38 - use a neural network a neural network is
03:41 - a computer system that is Loosely
03:43 - inspired by the neurons in the brain it
03:45 - is trained on a body of information and
03:48 - with enough training it it can learn to
03:51 - take in new information and give simple
03:54 - answers
03:55 - the answer is always include
03:57 - probabilities because there can be many
03:59 - options
04:01 - now let's take a neural network and
04:03 - train it on all the letter sequences in
04:06 - Shakespeare's plays to learn what letter
04:10 - is likely to come next at any point
04:13 - [Music]
04:15 - once we do this the neural network can
04:17 - take any new sequence and predict what
04:20 - could be a good next letter sometimes
04:23 - the answer is obvious but usually it's
04:25 - not
04:26 - it turns out this new approach works
04:29 - better much better by looking at a long
04:32 - enough sequence of letters the AI can
04:35 - learn complicated patterns and it uses
04:38 - those to produce all new texts it starts
04:41 - the same way with a starting letter and
04:44 - then using probabilities to pick the
04:46 - next letter and so on
04:49 - but this time the probabilities are
04:51 - based on the entire context of what came
04:54 - beforehand
04:56 - as you see this works surprisingly well
05:00 - now a system like chat GPT uses a
05:03 - similar approach but with three very
05:06 - important additions
05:08 - first instead of just training on
05:10 - Shakespeare it looks at all the
05:12 - information you can find on the internet
05:14 - including all the articles on Wikipedia
05:17 - or all the code on GitHub second instead
05:21 - of learning and predicting letters from
05:23 - just the 26 choices in the alphabet it
05:27 - looks at tokens which are either full
05:30 - words or word parts or even codes
05:35 - and third difference is that a system of
05:38 - this complexity needs a lot of human
05:41 - tuning to make sure it produces
05:43 - reasonable results in a wide variety of
05:46 - situations while also protecting against
05:49 - problems like producing highly biased or
05:53 - even dangerous content even after we do
05:56 - this tuning it's important to note that
05:58 - this system is still just using random
06:01 - probabilities to choose words
06:04 - a large language model can produce
06:06 - unbelievable results that seem like
06:09 - magic
06:10 - but because it's not actually magic it
06:13 - can often get things wrong
06:14 - and when you get things wrong people ask
06:17 - does a large language model have actual
06:20 - intelligence
06:22 - questions about AI often spark
06:25 - philosophical debates about the meaning
06:27 - of intelligence
06:28 - some argue that a neural network
06:30 - producing words using probabilities
06:33 - doesn't have real intelligence
06:36 - but what isn't under debate is that
06:39 - large language models produce amazing
06:41 - results with applications in many fields
06:45 - this technology is already been used to
06:47 - create apps and websites help produce
06:51 - movies and video games and even discover
06:54 - new drugs the rapid acceleration of AI
06:57 - will have enormous impacts on society
06:59 - and it's important for everybody to
07:03 - understand this technology what I'm
07:05 - looking forward to is the amazing things
07:07 - people will create with AI and I hope
07:10 - you dive in to learn more about how AI
07:13 - works and explore what you can build
07:15 - with it foreign
07:18 - [Music]

Cleaned transcript:

hi I'm Mira morati I'm the chief technology officer at open AI the company that created child GPT I really wanted to work on AI because it has the potential to really improve almost every aspect of life and help us tackle really hard challenges hi I'm Crystal Valenzuela CEO and cofounder of Runway Runway is a research company that builds AI algorithms for storytelling and video creation chatbots like Chad gbt are based on a new type of AI technology that's called large language models so instead of a typical neural network which trains on a specific task like how to recognize faces or images a large language model is trained on the largest amount of information possible such as everything available on the internet it's raining to then be able to generate completely new information like to write essays or poems have conversations or even write code the possibilities seem endless but how does this work and what are its shortcomings let's Dive In while a chatbot built on a large language model may seem magical it works based on some really simple ideas in fact most of the magic of AI is based on very simple math concepts from statistics applied billions of times using fast computers the AI uses probabilities to predict the text that you wanted to produce based on all the previous texts that it has been trained on suppose that we want to train a large language model to read every play written by William Shakespeare so that it could write new plays in the same style we'd start with all the texts from Shakespeare's plays stored letter by letter in a sequence next we'd analyze each letter to see what letter is most likely to come next after an eye the next most likely letters that show up in Shakespeare plays are s or n after an s T C or h and so on this creates a table of probabilities with just this we can try to generate new writing we pick a random letter to start starting with the first letter we can see what's most likely to come next we don't always have to pick the most popular choice because that will lead to repetitive Cycles instead we pick randomly once we have the next letter we repeat the process to find the next letter and then the next one and so on okay well that doesn't look at all like Shakespeare it's not even English but it's a first step this simple system might not seem even remotely intelligent but as we build up from here you have to be surprised where it goes the problem in the last example is that at any point the AI only considers a single letter to pick what comes next that's not enough context and so the output is not helpful what if we could train it to consider a sequence of letters like sentences or paragraphs to give it more context to pick the next one to do this we don't use a simple table of probabilities we use a neural network a neural network is a computer system that is Loosely inspired by the neurons in the brain it is trained on a body of information and with enough training it it can learn to take in new information and give simple answers the answer is always include probabilities because there can be many options now let's take a neural network and train it on all the letter sequences in Shakespeare's plays to learn what letter is likely to come next at any point once we do this the neural network can take any new sequence and predict what could be a good next letter sometimes the answer is obvious but usually it's not it turns out this new approach works better much better by looking at a long enough sequence of letters the AI can learn complicated patterns and it uses those to produce all new texts it starts the same way with a starting letter and then using probabilities to pick the next letter and so on but this time the probabilities are based on the entire context of what came beforehand as you see this works surprisingly well now a system like chat GPT uses a similar approach but with three very important additions first instead of just training on Shakespeare it looks at all the information you can find on the internet including all the articles on Wikipedia or all the code on GitHub second instead of learning and predicting letters from just the 26 choices in the alphabet it looks at tokens which are either full words or word parts or even codes and third difference is that a system of this complexity needs a lot of human tuning to make sure it produces reasonable results in a wide variety of situations while also protecting against problems like producing highly biased or even dangerous content even after we do this tuning it's important to note that this system is still just using random probabilities to choose words a large language model can produce unbelievable results that seem like magic but because it's not actually magic it can often get things wrong and when you get things wrong people ask does a large language model have actual intelligence questions about AI often spark philosophical debates about the meaning of intelligence some argue that a neural network producing words using probabilities doesn't have real intelligence but what isn't under debate is that large language models produce amazing results with applications in many fields this technology is already been used to create apps and websites help produce movies and video games and even discover new drugs the rapid acceleration of AI will have enormous impacts on society and it's important for everybody to understand this technology what I'm looking forward to is the amazing things people will create with AI and I hope you dive in to learn more about how AI works and explore what you can build with it foreign
