this video will cover introductory information about business analytics including a definition of business analytics the wisdom hierarchy data sources and business analytics terms applications history and uses business analytics is the process of transforming data into insights that support improve and/or automate business decisions the data can be of many types and from a variety of sources and there are many techniques and software packages that can be used for an analysis a simple way to think of business analytics is that it's the tools and processes used to find value in data to transform the raw data into information that can be acted upon a common method of understanding the relationship between data information knowledge and wisdom is by using a pyramid this graphic is called the di kW pyramid or the wisdom hierarchy and it illustrates that data is the foundation upon which decisions can be made information is defined in terms of data knowledge is defined in terms of information and wisdom is defined in terms of knowledge data are numbers or text without any context information provides meaning from data often combining multiple data points to produce a tangible idea knowledge provides context from the information making it directly applicable to a situation wisdom applies the knowledge to make a decision the original data has become useful enabling an action to be taken so where does data come from data can come from a variety of sources both internal and external internal data is collected by businesses and stored within their own servers this data can be generated in a number of ways either by physical objects such as sensors or barcodes or by using computer software such as websites while collecting and storing proprietary data sources is still very common there are now many external data sources that businesses can use public domain data sources such as government surveys or social media posts can be accessed and there are also many services that offer paid data sources like stock market or weather data these external data sources can be combined with a company's proprietary data to build a more complete picture of reality the amount of data that is being generated and collected is growing exponentially this growth is occurring due to the similar exponential trend in computing power along with a decrease in costs for digital storage because so much new data is being created and captured every year there's a corresponding growth of demand for business analysts there are many terms that are synonymous or semi synonymous to business analytics in the past few years data science has become the most common term to be used to describe this field other terms are often used interchangeably such as business intelligence big data data mining knowledge discovery and machine learning while there are many discussions about which term to use in which scenario all of the terms refer to the overall concept of using data to make better decisions or better products business analytics is an interdisciplinary subject based heavily on math and statistics it uses computer science principles and algorithms the math and computer science concepts are applied to a specific subject Business Analytics has applications in every field data on all our clicks on the Internet are used by retailers to figure out our preferences and provide targeted advertising analyzing data on past marketing campaigns provides a useful base to determine who to target for the next round supply chains in almost all industries have become far more efficient due to analytics organizations use historical data to figure out optimal numbers for staffing and hiring companies use demand data to determine optimal prices and professional sports teams use analytics such as when they look at player performance to determine who would be the best draft pick no matter which industry you work in there's room for analytics to add value although the popularity of analytics has recently peaked the field is nowhere as new as certain companies or people make it out to be since the computer was invented it was being used to process data to solve problems from decoding messages in World War 2 to generating weather forecasts in 1950 to modeling credit risk in 1958 of course these tasks involved enormous computational costs and only organizations with the most resources could attempt them toward the end of the 20th century as computing power became more affordable more organizations began collecting and storing data the types of analytical projects transitioned from being historical in nature to realtime in 1992 the first realtime credit card fraud system was introduced then the first analytically centric companies emerged companies such as Google used data to build their core product while other companies such as Amazon use analytical techniques to earn market share from competitors the rapid ascension of these tech companies has led to an arms race where all businesses have become committed to analytics businesses use analytics to gain an edge on their competitors and increase profits the three main areas in which they do this are competition to increase revenue from products or services sold efficiency to reduce the costs of resources or internal processes and customer satisfaction to improve the customer experience and encourage customer loyalty here's a case study as an example loyalty cards are used by grocery stores to uniquely identify their customers by requiring a loyalty card to obtain special discounts in the store the grocer can isolate habits of each customer and then provide customers with customized promotions to increase spending when a customer stops frequenting the store the grocer can mail coupons with aggressive offers the layout of a grocery store is constantly being changed to maximize customer spending this is why the milk section is always on the opposite side of the produce section so customers will have to traverse past every aisle to get to the two most commonly bought items each shelf is also analyzed to find the ideal arrangement more expensive items are typically placed at or around eye level while the cheaper products will be on the top or bottom shelves optimizing prices is another analytical technique used to maximize customer spending many grocery stores will have what are called loss leaders products that are very cheap to draw our customers into the store where they will inevitably spend more on other overpriced items grocers will also find the ideal times and prices to markdown expiring products preventing the product from being thrown away at a complete loss this concludes our introductory video about business analytics today we covered a definition of business analytics the wisdom hierarchy data sources and business analytics terms applications history and uses this video will cover introductory information about business analytics including the role of the business analyst what makes a business analyst successful and an overview of business analytics tools project outcomes and the analytical process there are three main reasons business analytics as an enticing career choice the first is that there's a high demand for business analysts and the relatively low supply of skilled workers means that salaries are higher in this field another reason is for the challenge of solving interesting problems analysts are typically people who are interested in solving complex puzzles finally those who are curious about how things work can use their skills in analyzing data to uncover previously unknown truths a business analyst can take many roles depending on the data and the type of project the most common roles are that of an interpreter in which the analyst uses descriptive analytics to tell the story of what happened an oracle and which analysts use predictive analytics to predict future events and a console in which the analyst uses prescriptive analytics to provide advice on the best course of action an analyst becomes successful due to a combination of hard and soft skills the hard skills are more tangible and refer to what the analyst can do with what tools while the soft skills are less flashy on a resume but equally or even more important than the hard skills analytical tools can be separated into two categories software that requires coding and software in a graphical user interface or GUI that is based on pointandclick interaction the main benefit of writing code is that it allows for more flexibility there are more features and it allows for more possibilities the drawback of coding is the extended learning curve within the last decade there have been many new GUI programs that make analytics easier to implement without the need for writing code programs such as tableau Alteryx and rapid miner have started gaining market share going along with older tools such as SAS Enterprise guide but none of these tools have yet to replace the overwhelming popularity of code based software such as SAS r Python or SQL there are many different goals that an analytics project can strive for typically these goals fit into one of two categories the first is providing information about a business such as reports and dashboards for business stakeholders reports or presentations provide onetime insights to explain events that have occurred and predict future events and dashboards are used by stakeholders for ongoing monitoring of key aspects of the business the second category is the production of analytical products in these types of projects the business's data becomes the input for a complex process that automatically produces in action this can take the form of features that offer a better experience for consumers for instance Amazon has an automated algorithm that determines products you might like to buy analytical products can also be built to make internal business processes more efficient an example of this is how credit card companies test every transaction for the probability of fraud an analytical project should start with the goals welldefined very rarely our project started to simply explore the data or find hidden truths collecting an inventory of all the relevant data sources is also an important step in the beginning of a project finally every analysis should begin with an effort to better understand the data this should be done before any analytical techniques are used simply observe the data files and write down a list of observations questions and any other ideas you have this process called creating disfluency enhances the data dictionary and helps the analyst internalize elements of the data cognitive disability nabel's students who take lecture notes by writing to retain more of the material than students who type notes even though those who type can take notes more efficiently sometimes the more work we have to do to process the information the better we can understand the information using this principle at the beginning of a project before using any advanced analytical techniques enhances the analysts capability to understand the data after the initial stages of a project there are a few more steps in the analytical process the majority of time is spent exploring and preparing data and the exploration stage the analyst learns more about the variables including distributions and frequency of values and also identifies variables with missing or null values in the preparation stage the data becomes more useful as variables are transformed and anomalous values are rectified this is referred to as data cleaning another common task during this stage is to join data into as few sources as possible perhaps one of the most valuable techniques to use during the preparation stage is called feature engineering in which the analysts transform certain variables into new variables containing slightly different data this allows hidden aspects of variables to be analyzed for example a data set with a date variable can have a new variable added to determine whether that date is on a weekday or the weekend and this new variable could add important information that was not readily available in the original data source this is one of the areas of the analytical process where creativity is needed the last two steps in the analytical process are to build models and then to put these models into production a model is a type of mathematical equation that describes relationships among variables in a dataset often for the purpose of predicting an outcome by putting a model in production an automated decision can be made when new data is observed in some cases models are not the goal of a project rather the goal is to analyze data and communicate the findings in an analytical report presentation or dashboard visualization of the data is also a key component throughout the entire analytical process as the analyst attempts to learn more about the data this concludes our introductory video about business analytics today we covered the role of the business analyst what makes a business analyst successful and an overview of tools project outcomes and the analytical process this video will cover some of the foundations of Business Analytics selecting filtering and sorting there are many synonymous terms to describe the aspects of a typical data file which can be referred to as a table spreadsheet data set or data source along the horizontal axis are the variables which can also be called fields attributes or columns along the vertical axis are the observations also referred to as records tuples or rows fields that are sparsely populated meaning that a high percentage of records is missing should not be selected fields with redundant values meaning that a high percentage of records have the same value should also not be selected techniques to identify these fields vary for now a visual inspection of the first few records can be used from this table we can remove the state column because all of its values are the same we can remove the religion column because it is sparsely populated our table now depicts only the variables of interest once the variables of interest have been selected they can be renamed if needed and assigned the proper data type there are two common variable types numeric and categorical numeric variables include discrete variables which can include whole numbers used for counting and IDs that represent a unique entity and continuous variables which are numbers with decimals used for measuring categorical variables include text which is any combination of letters numbers and symbols and strings or characters and boolean or binary variables which contain only one of two possible values variables that can be treated as either numeric or categorical include dates such as date time date or time and spatial objects which show location such as latitude and longitude here are the variable types for our table person ID is a discrete numeric variable gender and city are string variables weight is a fixed decimal variable date of birth is a date and student is boolean software packages can automatically assign fields with data types and sometimes these can be assigned incorrectly examination of these data types is necessary to ensure proper utilization further downstream sometimes data formats require advanced data transformation and extraction techniques for example dates can have varying formats that can be interpreted as strings and parts of these strings need to be separated before the software can identify as a date field for now we'll assume that you can change a fields datatype without these advanced techniques the final aspect of the Select step is to assign the proper size for each variable many software packages can do this automatically but it's still a good idea to review the sizes because the size of each field has a direct impact on the amount of storage required for a data set specifying these can save system resources and processing time the common issue is for one record that contains many more characters than the typical record to cause the size to be much larger than need be a visual inspection of the table can highlight these instances but beware that changing a variable size can truncate or cut off some of the values while selecting limits a data set size by omitting certain columns a filter limits sighs by omitting certain rows a filter is also known as a condition subset or in SQL a where clause and it's commonly used for investigative purposes it's important to be aware of the information contained within the row of a data table also commonly referred to as a record or observation a row defines the level of detail that is contained in the data set for this example each row represents one person an ID fields such as person ID in which no rows contain the same ID can be used to determine that the tables level of detail is a person if there are multiple rows for the same ID we know that the rows reflect data from the same person in this example the level of detail is more granular it shows a record of one person's weight by day finding and understanding the level of detail for a table is necessary before analyzing the data after determining the tables level of detail you should check for duplicate observations these are rows of data in which all values are exactly the same as another row in some cases duplicate observations may be legitimate but usually these are erroneous and need to be removed here's our table with the erroneous observation removed this table is filtered to show only records of people who live in Raleigh let's try another filter here's our original data set again this table is filtered to show only records of people who weigh more than 180 pounds filters are different variables can be applied together combining above examples we can filter the original table for people who live in Raleigh and weigh more than 180 this results in only one observation the next step is sorting when we sort we rearrange a table by ordering the rows according to the values of one or more fields and either ascending or descending order here's our original data set it's sorted by date of birth in ascending order here we've sorted by city in ascending order and then by weight in descending order this concludes our video on selecting filtering and sorting this video will cover two types of formulas same row and multi row a commonly used method of data preparation is using existing fields to create new variables examples include adding subtracting multiplying dividing or applying another mathematical function to numeric field extracting and transforming substrings from a text field extracting truncating and parsing date parts from a date field conditional statements using ifthen and binning to create new variables and comparing values of two different fields to create a boolean variable formulas can be used for adding subtracting multiplying or dividing two numeric fields in this table we apply a formula to create the price column which is calculated by dividing units sold by total amount formulas can also be used when applying a mathematical function to a numeric field examples of mathematical functions are average floor finding the smallest value ceiling finding the largest value square square root absolute value trigonometric functions and logarithmic functions in this table we will create an observation average column using the average formula which averaged values from the observation 1 and observation two columns formulas can also be used to extract and transform strings from a text field there are a few common functions for transforming text and they are typically named differently depending on the software package being used the most common of these are upper lower use to change the field to all upper or lowercase characters concatenate which combines two or more strings into one field substring used to extract a portion of a string trim which can be used to remove certain characters usually spaces from a field index used to find the location of a certain string within a field and length which finds how many characters are in a field these are the basic string functions that almost every software package will include and can be combined to complete almost any string transformation for the following example we'll use two functions to transform a field that holds a city and state into two fields to extract city and state from the airport location field we would need to extract all the text before the comma as the city field and everything after the comma and space as the state field to do this we need two functions index and substring the index function will help us identify the location of the comma in the text field counting from the left including spaces this is done because the comma is not always in the same location then we use the substring function to extract everything before the comma as the formula for the city field and extract everything after the comma and space for the state field to get the city and state columns back into the format of the original field we would use a concatenate function we can also use formulas for extracting truncating and parsing date parts from a date field a date or date/time field carries more information than a typical variable the time of day the day of week the day of month the month of year the year number and the difference in the date from other dates such as today extracting this and other information out of a date field is a transforming technique that can add to the value of a data set during analysis and reporting the most heavily used date functions are today used to get today's date date part used to get a part of a date for example getting the month of date will return the numeric value of the month date truncate which will return a date value at the beginning of a period specified for example truncating a date at the month level will return the first day of the month for the date date ad which will add or subtract a certain number of periods to a date and date difference used to calculate the number of periods between two dates this table contains the major holidays for 2016 for each date value we can extract other features we can add a column with a numeric value to describe the day of the week we can subtract today's date from the date column to find the number of days remaining until the holiday occurs with formulas we can also compare values of two different fields to create a boolean variable a boolean variable is typically used to capture true/false values to create a boolean variable a logical statement or condition can be used in a formula this logical statement can compare two values of the same type numeric values strings and dates this table depicts the population of States we can use a formula to create a column that displays whether the state is on the East Coast we can also create a column to display whether the state has a high population formulas can also be used with conditional statements that if then and when binning to create new variables to create a variable with multiple possible outcomes conditional statements are needed these usually take the form of if condition1 is met then outcome 1 else if condition 2 is met then outcome 2 else outcome 3 while the conditions within these statements are the same as boolean formulas these formulas offer more flexibility as the analyst can assign any value when the condition is met and can use as many conditions and outcomes as needed while there are many applications of conditional statements one of the most common is tube in or tile numeric variables a process which transforms the continuous numeric variables into categorical variables in this table we compute sales volume with ifthen logic if sales is greater than 40000 then high else if sales is greater than 20,000 then medium else low this formula begins with testing the first condition if sales is greater than 40,000 and if it is true the first outcome high will be assigned the second condition else if less than 20,000 is evaluated only if the first condition is false if the second condition is true the second outcome medium will be assigned if neither the first nor second conditions are true then the final outcome low will be assigned sometimes data from a separate row or rows needs to be used when creating variables for this a multi row formula is needed a running total is the most basic multirow formula it adds the value of field of a current row to the value of a previous row this can be done across an entire data set for a particular variable or it can be grouped by certain dimensions a lag value looks at the data in preceding rows while lead values look at data and subsequent rows and is the opposite of lag window functions provide the ability to perform calculations like sum average and rank across sets of rows that are related to the current query row this is equivalent to aggregating the data set across one or more dimensions then joining the resulting data set back to the original this should be used carefully as values will be repeated for all levels of the dimension an example of this is in the transportation industry where a row typically represents one leg of a trip after sorting the data set properly the lag function can be used to combine rows so that the data describing the round trip is displayed total price is a running total of ticket price grouped by passenger ID notice how upon reaching the first row for a passenger ID the sum starts over and total price equals ticket price total flight time is a running total of flight time grouped by passenger ID lag of destination uses the lag function which doesn't consider any variables other than destination this means it's dependent on the sort order of the data set sum of ticket price is a window function which is the sum of the ticket price partitioned or grouped by passenger ID this concludes our video on single and multi row formulas this video will cover unions and joins union is also referred to as appending concatenating or combining two tables vertically or simply adding rows the table should have the same variable names types and sizes variables that are in only one table will receive null or missing values for the rows from the tables that do not contain them in this union are two tables one with three rows and one with one row will combine to create one larger table which has 4 rows notice the null values in the storage column storage was not contained in both original tables so the rows that do not contain data for storage will have null or missing values joining is also referred to as merging very rarely as data collected and stored in a format that's ideal for analysis typically data is stored across multiple tables in a relational database schema like you see here schemas are designed to optimize for storage so values that repeat often and take up more storage such as names are reduced to representative IDs that take up much less storage in a schema the tables that contain identifying information are called dimension tables and can sometimes be referred to as lookup tables our diagram has three so for example the product dimension table contains the name manufacturer and type of product each dimension table has a primary key that is used to identify a unique record the fact table is a record of events that happen for a combination of dimensions it's comprised of two things foreign keys and measures foreign keys map to primary keys of dimension tables these are the fields that will be used to join the tables together for example in this diagram the foreign keys are person ID store ID and Product ID which each connect to their own dimension table measures can be any type of variable we've already discussed in this diagram we have two measures units sold and total amount to join these tables together we will use an inner join this will return only rows that are contained in both tables when a fact table has no lore missing values for a dimension using an inner join will remove the entire row since the row may contain useful data and other fields it's better to use a left outer join to keep all values from the left table in this case the fact table and the values from the dimension table that match on the key while there are other types of joins as shown here in and left outer joins are the most commonly used first we will join transaction fact with person dimension next we will join the results from the previous join with store dimension finally we will join our results from the previous join with product dimension notice how each step creates a progressively larger table this concludes our video on unions and joins this video will cover a definition of aggregation as well as examples of aggregation aggregation is also referred to as a PivotTable group by statement or summarize aggregation transforms data into lower dimensions using summing averaging and counting the benefits of this are to answer basic questions of data sets with many different dimensions the most basic aggregation can be done on the level of detail for the table in this case each rows a transaction and the table as a whole represents sales for phones for quarter 1 of 2015 the answers can be calculated by applying a formula to a single field using all the rows in the table how many units were sold 9 how much total revenue 3500 $90 how many transactions six how many distinct products were sold three how many customers three how many stores had sales for what was the average price 398 dollars and 89 cents what was the average number of units sold per transaction one point five what was the average amount spent per transaction 598 dollars and 33 cents what was the average amount spent per customer 1196 dollars and 67 cents pay special attention to how the average function works it uses the sum of the field divided by the number of rows so it can't be used in averages such as this where the denominator is not the number of rows the next questions are related to aggregating one dimension at a time and can be calculated in one step for each of the questions the original table is grouped by the dimension of interest either the person ID store ID product ID or date for each value of the dimension chosen calculations are performed using the measures the number of rows and the number of distinct IDs a new data table is the result of these calculations you notice that when grouping by a dimension the dimension is sorted in the output for this example we'll aggregate at the person level of detail to answer the following questions how many units to each person by how much money did each person spend for each person ID we need to calculate the sum of units sold and the sum of the total amount starting with person ID 1 we see that there were 1 plus 1 equals 2 units sold and 365 plus 425 equals seven hundred and ninety dollars spent these values are populated in the resulting table similar calculations are done for every person ID in the original table you aggregating at the store level of detail can provide basic information about store performance to find the number of transactions each store had each occurrence of a store ID needs to be counted store ID 101 has two rows in the original table indicating that there were two total transactions each store ID has its number of rows counted and the final result is the num transactions column in the newly created table aggregating at the product level of detail can answer basic questions about each product such as what was the average price of each product how many distinct customers bought each product starting with product ID one zero zero one average price is calculated as the sum of the total amount 1625 divided by the sum of units sold for the final value of 406 twenty five is populated into the resulting table again be aware that using an average function in this case would not give us the expected result distinct customers is calculated by counting the distinct number of person IDs for Product ID one zero zero one there are two person ID 1 and person ID 2 the calculations are then completed for every product ID the final example uses the month and year components of the date field to calculate all possible levels of aggregation which were shown on the previous slides how many units were sold each month how much revenue each month how many transactions each month what was the average price how many distinct products were sold each month you the final type of questions that can be asked involve combinations of multiple dimensions one of the most common combinations is to aggregate by a date dimension along with another dimension for this calculation each combination of the dimensions chosen for grouping are performed in the transaction table there are four such combinations transactions for person ID one are only in March of 2015 there's only one transaction for person ID to occurring in February 2015 and there are three transactions for person ID for two occurring in January and one occurring in February of 2015 within each of these combinations each of the calculations are performed here will show you the first row as an example you note that the more unique dimensions a data table contains the more the number of possible levels of aggregations increases in this example table with four unique dimensions there are fourteen different possible levels of aggregation each different level of aggregation can offer a unique insight but it's best to start with two or fewer levels example questions that can be answered using the example transaction data set include which customers tend to buy which products which stores two customers tend to frequent what products sell well in what stores and during what months do they sell more on average here's one final example using multiple concepts that have been covered using the transaction fact table we want to answer the question which stores had products that were on sale answering this question is a multistep process the first step is to find the average price for each product for this we aggregate by product ID and for each Product ID we calculate the average price for example product one zero zero one sold for four hundred and twenty five dollars and four hundred dollars the average of these two values is calculated and then entered into the resulting table values for subsequent IDs are calculated in a similar manner you we also need to aggregate at the store and product level this removes the customer and date level information giving us the totals for each store and product combination due to the limitations of this example the aggregation does not result in fewer rows as it normally would the resulting table is however sorted by store ID and Product ID the next step is to join the product aggregate table to the store product aggregate table using product ID as the common field between the two tables this results in a table with a similar structure as the store product aggregate table with the only difference being the new column average product price we filter the resulting table to only include rows where price is less than average product price there are only two rows where this is true finally we join this table with the store dimension and product dimension tables adding store name and product name to the table is necessary to fully answer the original question the resulting table makes it clear that the g2 went on sale at Best Buy and the s6 went on sale at h/h Greg this example is a simplified an illustration of how data preparation techniques are often used together to answer typical questions of the data this concludes our video on aggregation today we covered a definition of aggregation and examples of aggregation this video will cover cross tabs and transposing cross tabs are also called pivot tables they are used to compare one measure across two or more dimensions one dimension will have its values transformed into columns and the other dimensions will be aggregated the visual result will be a table with fewer rows but more columns for example this table shows sales by region and month the data is stored in a form similar to how it would be captured with one row for every combination of region and month a crosstab function is performed with region defined as the grouping field month defined as the header field and sales defined as the data field there can be multiple grouping fields but only one header field and one data field are allowed cross tabs are an effective way to summarize and present data as trends within the data are easier to identify and data visualization color is added to build a heatmap cross tabs are also used in statistics to build contingency tables transposing can be thought of as the opposite of cross tabs it transforms the data from a wide format into a narrow format typically transposing is required after receiving financial data from a spreadsheet as columns can be used to capture dates locations or categories once the columns known as data fields are transposed into one variable the data is ready for a more sophisticated analysis any fields that are not to be transposed are called key fields for this example we can use the result of the crosstab function from the previous section region is the only key field while January February and March are the data fields to be transposed transposing results in the original data set notice how the column names are transformed into values for the newly created month column this concludes our video on cross tabs and transposing this video will cover contingency tables first we will discuss the definition of a contingency table and then the steps for creating one finally we will discuss chisquare distributions in statistics a contingency table is a type of table in a matrix format that displays the multivariate frequency distribution of variables contingency tables are heavily used in Survey Research Business Intelligence engineering and scientific research they provide a basic picture of the inter relation between two or more variables and can help find interactions a contingency table is also referred to as a twoway frequency table here's an example given this table can you calculate the following metrics the number of males who are righthanded the percent of males who are lefthanded whether more males are lefthanded than females or the percent of lefthanded people who are females in a table such as this notice that there are no numerical values the person ID is a numerical identifier but the numbers are arbitrary so there are no obvious calculations to perform however because each row represents one person we can count the number of rows along each dimension gender and dominant hand to analyze how the dimensions are distributed to create a contingency table the first step is to aggregate the original data set along two dimensions gender and handedness and count the number of rows for each combination of values since there are two possible values for gender male and female and two possible values for dominant hand left and right there are four total possible combinations male and right male and left female and right and female and left for each of these combinations we count the number of rows that contain both values at this point we can extract information regarding the quantity of each combination allowing us to answer basic questions such as how many females are lefthanded and are there more males or females who are lefthanded next we perform a crosstab on the result moving the dominant hand to the horizontal axis this will make interpretation of the results much easier and allow us to easily calculate totals each dimension there are other questions however that are harder to answer and these deal with the proportions of each combination what proportion are percent of males are lefthanded and are a greater proportion of males lefthanded than the proportion of females for these questions the proportion of gender we need to divide the values in the cells by the totals along the gender axis which in this case is the vertical axis the resulting table called a row conditional frequency table will have 100% values in each cell in the total column the cells for each row will account for all the people within the category male and female of the gender dimension the table can be easily read eightythree percent of males are righthanded and only 8 percent of females are lefthanded but what if we want to know what percent of lefthanded people are female this is a different type of question one that can be answered by dividing the original values of the cells by the totals along the dominant hand or horizontal axis of the contingency table these are called column relative frequencies now the results can be interpreted in two sentences by focusing on the columns in the contingency table 49% of righthanded people are males 31% of lefthanded people are females etc but what if we want to answer questions about how prevalent each separate combination is among all people in this data set for instance what percent of the people are lefthanded males for that we divide the original values by the total number of rows in the data set for our example this makes for an easy calculation since there were 100 rows with this table we can answer many basic questions about the data set including what percent of the people in the data set are righthanded what percent of the people in the data set are female and what percent of the people on the data set are lefthanded females and so forth note that the language used what percent of people in the data set is different from the easier to say what percent of people this is because the data set being used is a sample of the population and to infer the trends about the population will need to use a statistical technique for a contingency table a chi square distribution can be used to make such an inference to do so however assumes that the sample you are using was acquired from the population randomly the chisquare distribution compares the actual values to the expected values to determine if the actual numbers that were observed and recorded in the data set are due to chance or if there's a difference between the two variables that cannot be explained by chance in this example we want to determine if the observed difference in the proportion of females who are lefthanded is really smaller than the proportion of males who are lefthanded or if that observation could be due to chance in other words we want to know if the dominant hand is dependent on gender to calculate the expected values for each cell multiply the relative horizontal and vertical dimension totals and divide by the number of total observations in this example to calculate the expected value for righthanded males multiply 52 the total number of males by 87 the total number of righthanded people and divide by 100 the total number of people similar calculations are done for each cell once the expected values have been computed the chisquare test can be run in Excel the chisquare test function uses the actual table and expected table as inputs to calculate the pvalue this pvalue is the probability that these results did not occur due to chance a common way to evaluate the pvalue is to compare it to 0.05 if the pvalue is less than 0.05 we say there is an association between the two variables that is statistically significant in this example our pvalue is 0.18 which is greater than 0.05 so we conclude that the dominant hand variable is independent of gender if there is an association between two variables completing the calculation for the chisquare test statistic can be used to find which values contribute the most to the Association this can be calculated using this formula for each cell subtract the expected value from the observed and square the result then divide the answer by the expected value each of these results is summed indicated by the Greek letter Sigma this value is compared to the chisquare distribution within each cell the calculation describes how far the actual value is from the expected value in this example the calculations for lefthanded people are much greater than those for righthanded people these cells have the greatest impact on the potential association between the two variables summing those values results in chisquare test statistic of 1.78 using this value in a chisquare distribution along with the degrees of freedom based on the number of values for each variable and a significance level such as 0.05 is how the pvalue is obtained this concludes our video on contingency tables today we defined contingency tables and then we discussed the steps for creating one and finally we covered chisquare distributions this video will first define distribution then we'll cover measures of distribution the mean median outliers mode minimum and maximum values and quantiles the most common method of analyzing a numeric variable is by exploring how the values are distributed the distribution of a numeric variable shows all the possible values and how often they occur a distribution provides methods in which many records of data can be summarized to provide basic information about the variable these methods can either be numerical measures or visualizations we'll explore the most popular methods using this data set of 10 rows of bank teller salary data because the sample data set is so small we can make some quick observations about the salary variable the lowest salary is twenty eight thousand six hundred and sixtyfive dollars this is referred to as the minimum value the highest salary is forty four thousand and twenty dollars the maximum value finally many of the salaries are in the low $30,000 range we'll begin by defining and calculating the salary variables measures of distribution mean median outliers mode minimum and maximum values in quantiles to find the mean or average add up all the numbers and divide by the number of rows 340 one thousand eight hundred and sixty dollars divided by 10 equals a mean of thirty four thousand 186 dollars to find the median sort the numbers and find the middle value if there are an odd number of rows there is one middle value if there are an even number of rows there are two middle values and the median will be the average of these two values here we sort the table by salary then find the average of the two middle values by adding thirty three thousand nine hundred eighty dollars to thirty four thousand eight hundred fifty dollars and dividing by 2 to get thirty four thousand one hundred and fortyfive dollars values that fall outside of the normal range of the rest of the observations are called outliers in our example the value of forty four thousand twenty dollars is an outlier from the rest of the values the mode is the most commonly occurring value in our sample data set no values occur more than once so there is no mode as we've already noticed the minimum and maximum values are twenty thousand six hundred and sixty five and forty four thousand and twenty dollars a quantile is a set of values that divide a frequency distribution in two equal groups each containing the same fraction of the total population to find quantiles divide the distribution into groups of equal size with each group containing about the same number of rows the most simple of quantiles has already been calculated the median divides the records into two groups of five the following quantiles are most often used tersh aisles three groups quartiles four groups quintiles five groups deciles ten groups and percentiles 100 groups let's look at quartiles for our example the median is commonly referred to as q2 since it is the second quartile q1 and q3 are also used often to calculate quartile 1 we can look at the middle value of the first 5 records sorted in order which is 30 $1,300 and then look at the middle value of the last 5 records for quartile 3 which is 30 $5,100 these values along with our median are our quartile values this concludes our video on measures of distribution today we define distribution and then we discuss the measures of distribution including mean median outliers mode minimum and maximum values and quantiles this video will cover variation including a definition of variation and the measures of variation range inter quartile range variance standard deviation and standardization as well as testing differences between means variation refers to how spread out the values are for a variable interpreting variation that is explaining a variables variation in reference to other variables is a foundational task in Business Analytics variables that have values that are spread out have higher variation while variables with values very close to the mean have lower variation the following measures help us understand how the values for a variable are spread out or on the mean range interquartile range variance standard deviation and standardization using an example of salaries by gender a question arises do men make more than women certainly some men make more than women but is this true for the group as a whole using the principles of variation will help us to answer this question the range is the largest number minus the smallest number to find the range we first sort from smallest to largest then we subtract the smallest value from the largest value to find the interquartile range first separate the data into quartiles and then subtract q1 from q3 to find the variance we first calculate the mean or average then we subtract the mean from each value and square the result by squaring the differences we remove the possibility of negative values cancelling out the positive values next find the average of the squared differences variance is not a very useful measure the value we got for variance is much different from the range of salaries that are variable captures to get it back into the correct scale we take the square root of the variance this results in the standard deviation this value is used relative to the mean to determine which values are within the normal variation of the salary variable subtract the standard deviation from the mean to find the lower threshold add the standard deviation to the mean to find the upper threshold using the standard deviation gives us a way to determine which values are within the normal variation and which values are either less than normal or greater than normal in other words it gives us the ability to identify outliers more easily in our example we can identify that the smallest two salaries twentyeight thousand six hundred and sixtyfive dollars and twentynine thousand five hundred dollars and the largest salary fortyfour thousand twenty dollars are outside one standard deviation from the mean mean and standard deviation are measures that describe the distribution of a set of numbers but what if we want to describe the numbers within the set to compare two numbers within an entirely different set we can do this by standardizing the values in statistics this measure is called the zscore to standardize a value subtract the mean and divide by the standard deviation for example in the first row of our data set since we've already calculated this salary mean column all we need to do is divide by 4103 to get the result of negative 1.1 for this value can be interpreted as the value of $29,500 is 1.1 for standard deviations lower than the mean looking at the rest of the values we see that the largest salary forty four thousand and twenty dollars is actually very far away from the mean this must be a high performing bank teller we can use this standardized value to compare the high performing bank teller to the high performer of another profession let's return to the question post at the beginning do men make more than women to answer this question we can use a statistical test called a ttest this test requires that we make three assumptions about our data which for now we will assume to be true the two populations have the same variance the populations are normally distributed each value is sampled independently from each other value by considering the values of salary separately we can see that females have the lowest value and the highest value so the variation must be higher calculating the mean and standard deviation for each group we see that the mean is almost the same yet the standard deviation for females is more than twice that for males using these values we can calculate the tstatistic and corresponding pvalue but in Excel we only need the original values in different columns using the ttest function we get a pvalue of 0.44 since this value is much higher than point zero five we conclude that the two distributions are not statistically significant this is mainly due to the fact that our sample size for this example is very small and also because the means are almost equivalent this concludes our video on measures of variation today we defined variation and then discussed range enter quartile range variance standard deviation standardization and testing differences between the means this video will cover distribution visualizations including buckets histograms and an introduction to area line graphs we begin with a data set that contains gender dominant hand and salary we've calculated a number of different measures including the mean median minimum maximum range interquartile range variance standard deviation and a list of zscores next we'll build a histogram which is the most common way to visualize a numeric distribution to build a distribution will group the data into buckets or bins of equal size this will effectively transform the variable into a categorical variable allowing us to count the occurrences of each bucket next we determine the size of each bucket first we locate our minimum and maximum values then we subtract the minimum from the maximum and divide by the number of buckets this gives us a value of one thousand five hundred and thirty six dollars next we need to figure out the starting value for each bucket starting with the lowest salary we add the size of the bucket to determine the starting value of the second bucket we do this until we obtain the starting values for all ten buckets we'll add a new column to the original table to assign each person into a salary bucket summarize the new variable starting value of salary bucket counting how many occurrences of each bucket are observed note that although this variable is numeric by summarizing it we're treating it as a categorical variable finally we'll make a simple bar chart to visualize this table this bar chart is known as a histogram and it's one of the most common methods of visualizing a numeric distribution although many software packages can produce a histogram very quickly as we've seen creating one involves quite a few steps if we change the visualization to an area line graph and change the vertical axis to the percent of records by dividing the count by the total number of records the result is similar to what is known as a probability density function or PDF this is commonly used in statistics to estimate the probability of a new value for now we just need to know that the shaded area under the curve adds up to 1 or 100% and that the lines are typically much smooth and we see in this example which is because most PDFs visualize more than 10 data points and use more than 10 buckets this concludes our video on distribution visualizations today we covered buckets histograms as well as a brief introduction to the use of area line graphs this video will cover normal distributions continuous distributions density functions cumulative distribution functions and the 6895 99.7 rule the single most important distribution in statistics is the normal distribution it's a continuous distribution and it's the basis of the familiar symmetric bellshaped curve the mean of the normal distribution is in the center the standard deviations are marked at equal distances from the mean any particular normal distribution is specified by its mean and standard deviation by changing the mean the normal curve shifts to the left or right by changing how spread out the standard deviations are the curve also changes standard deviations can be spread out wider or closer together therefore there are really many normal distributions not just a single one the normal distribution is a two parameter family where the two parameters are the mean and standard deviation here's a tool you can play with online that illustrates a normal distribution in real life it looks like a triangular shaped pegboard into which balls are dropped when there's an equal probability that the balls will drop either left or right their final placement forms a normal distribution however when the probability of the balls dropping left or right is unequal which is something you can experiment with using this tool the distribution changes the formulas for mean and standard deviation are very complex but you will not have to compute them because the software will with continuous variables there is a continuum of possible values such as all values between 0 and 100 or all values greater than 0 instead of assigning probabilities to each individual value in the continuum the total probability of one is spread over the continuum thus the shaded area within the bell curve will always have an area of 1 the key to the spreading is called a density function which acts like a histogram the higher the value of the density function the more likely this region of the continuum is a density function usually denoted by FX specifies the probability distribution of a continuous random variable X the higher FX is the more likely X is probabilities are found from a density function as areas under the curve so for example the shaded portion under the spell curve represents the probability of X being between 65 and 75 the cumulative distribution function or CDF is the probability that the variable takes a value less than or equal to X it's the total area under the normal curve up to X the beauty of the normal curve is that no matter what its mean and standard deviation are the area between the mean minus 1 standard deviation and the mean plus 1 standard deviation is always about 68% the area between the mean minus 2 standard deviations and the mean plus two standard deviations is always about 95% the area between the mean minus three standard deviations and the mean plus three standard deviations is always about ninetynine point seven percent that means almost all values fall within three standard deviations on either side of the mean that's true for all normal curves no matter their shape but how good is this rule for real data let's go ahead and check out an example here's our data the mean of the weight of one hundred and twenty women runners in a sample is one twenty seven point eight pounds the standard deviation is fifteen point five here's what our distribution would look like let's look a little more closely at that distribution 68% of our 120 runners is about eighty three runners according to the 68 95 99 point seven rule those runners should fall fall within one standard deviation of the mean weight of one hundred and twenty seven point eight pounds that is eightythree of our runners should fall between one hundred and twelve point three and one forty three point three pounds when we check our data we see that seventynine runners fall within one standard deviation of the mean furthermore ninetyfive percent of our group or about 114 runners should fall within two standard deviations of the mean or between ninety six point eight and one hundred and fifty eight point eight pounds the data shows that 115 runners fall within two standard deviations of the mean finally according to the rule ninetynine point seven percent of our runners or one hundred nineteen point six runners should fall within three standard deviations of the mean or within a range of eighty one point three pounds to one hundred and seventy four point three pounds according to our data all 120 runners fall within this range so it seems as if the rule is pretty accurate in this case this concludes our video on normal distributions continuous distributions density functions and cumulative distribution functions as well as the 6895 99.7 rule this video will cover kurtosis including a definition as well as positive and negative kurtosis and asymmetrical distributions including those of positive and negative skew kurtosis is the measure that describes the size of the tails in a distribution a distribution with positive kurtosis contains fewer values in the tails than a normal distribution a distribution with more values in the tails has negative kurtosis the normal distribution is a type of symmetrical distribution in which the mean is equal to the median and there is an equal probability of a value falling on either side of the mean although normal distributions and other types of symmetrical distributions are very common there are often distributions that are asymmetrical we call these types of distributions skewed for a negatively skewed distribution the left tail is longer and the mean is less than the median this is due to the occurrence of outliers at the lower end of the distribution away from the most frequently occurring values a good example of this is the height of NBA players since taller basketball players have an advantage the majority of NBA players are very tall for a positively skewed distribution the right tail is longer as there are outliers with larger numbers these outliers cause the mean to be greater than the median an example of a positively skewed distribution is in the salary of baseball players there are a few star players who make much more than the majority of players these high salaries are outliers that make the mean of the distribution increase for distributions that are skewed both positively and negatively the meeting is a better representation of central tendency than the mean as the outliers will impact the calculation of the mean the median is not affected by outliers and typically is a much better approximation for the middle of a distribution this concludes our video on kurtosis today we defined kurtosis and discussed positive and negative kurtosis we also covered asymmetrical distributions including those of positive and negative skew this video will cover sampling basics including populations and inferences selecting a sample random sampling stratified sampling and cluster sampling a population is a set of all members about which is study intends to make inferences here's a population of people we'd like to study their television watching behavior and infer how many watch a particular shows so we can decide whether to purchase advertising spots during this period of time but our population is much too large to study effectively to study their television viewing habits we'll have to survey them and it's not feasible to survey every single individual so we'll take a sample of the population to study the sample will represent the population as a whole and so will it survey results if we choose our sample correctly we will focus on selecting a sample using probability a probability sample is chosen from a population using a random mechanism there are two types of probability sampling stratified and cluster a random sample is only random if each individual has the same chance of being chosen from the population so back to our television viewing research we want to figure out how many television viewers there might be during a particular so let's say there are 30,000 viewers in our population each viewer is known as a unit in order to select a sample n of viewers from this population of 30,000 we could choose to use a simple random sample this means that there is an equal probability that each viewer could be selected for inclusion in the sample if our desired sample size was 200.final echt 200 viewers randomly and then we could send each of those viewers a questionnaire in the mail about their viewing habits but suppose various subpopulations within the total population can be identified these populations are called strata instead of taking a random sample from the entire population we might get better information by selecting a simple random sample from each stratum separately this is called stratified sampling examples of subpopulations and television viewers might include age or gender there are several advantages to stratified sampling one obvious advantage is that separate estimates can be obtained within each stratum which would not be obtained with a single random sample from the entire population for example let's say we're looking at our television viewers by age group we have three strata 18 to 24 25 to 39 and 40 plus we find that their peak times vary based on age thus we can make better decisions about which product to advertise during which time period a more important advantage of stratified sampling is that the accuracy of the resulting population estimates can be increased by using appropriately defined strata in cluster sampling the population is separated into clusters such as regions of the country and then a random sample of the clusters is selected the primary advantage of cluster sampling is sampling convenience and possibly lower cost selecting a cluster sample is straightforward the key is to define the sampling units as the clusters such as the regions of the continental US shown here this concludes our video on sampling basics today we covered populations and inferences selecting a sample random sampling stratified sampling and cluster sampling this video will cover bivariate data scatter plots and null values measures of central tendency variability and spread summarize a single variable by providing important information about its distribution often more than one variable is collected on each individual for example in large health studies of populations it's common to obtain variables such as age sex height weight blood pressure and total cholesterol in each individual economic studies may be interested in among other things personal income and years of education as a third example most university admissions committees ask for an applicant's high school grade point average in standardized admission test scores like the SAT bivariate data consists of two quantitative variables for each individual in contrast with univariate or single variable data our first interest is in summarizing such data in a way that's analogous to summarizing univariate data by way of illustration let's consider something with which we're all familiar age let's begin by asking if people tend to marry other people of about the same age our experience tells us yes but how good is the correspondence one way to address the question is to look at pairs of Ages for a sample of married couples table one shows the ages of 10 married couples going across the columns we see that yes husbands and wives tend to be of about the same age with men having a tendency to be slightly older than their wives this is no big surprise but at least the data bear out our experiences which is not always the case the pairs of Ages in table 1 are from a data set consisting of two hundred and eighty two pairs of spousal ages too many to make sense of from a table what we need is a way to summarize the two hundred and eighty two pairs of ages we know that each variable can be summarized by a histogram which is a graphical representation of a distribution a histogram partitions the variable on the x axis into various contiguous class intervals of usually equal widths the heights of the bars represent the class frequencies here we can see that each distribution is fairly skewed with a long right tail we can also summarize the variables with a mean and standard deviation from table 1 we can see that not all husband's are older than their wives and it's important to see that this fact is lost when we separate the variables that is even though we provide summary statistics on each variable the pairing within the couple is lost by separating the variables we cannot say for example based on means alone what percentage of couples has younger husbands and wives we have to count across the pair's to find this out only by maintaining the pairing can meaningful answers be found about the couples another example of information not available from the separate descriptions of husbands and wives ages is the mean age of husbands with wives of a certain age for instance what is the average age of husbands with 45 year old wives finally we don't know the relationship between the husband's age and the wife's age we can learn much more by displaying the bivariate data in a graphical form that maintains the pairing figure to shows a scatter plot of the paired ages the xaxis represents the age of the husband and the yaxis the age of the wife there are two important characteristics of the data revealed by figure two first it's clear that there's a strong relationship between the husband's age and the wife's age the older the husband the older the wife when one variable Y increases with the second variable X we say that X and y have a positive association conversely when Y decreases as x increases we say that they have a negative association second the points cluster along a straight line when this occurs the relationship is called a linear relationship figure 3 shows a scatterplot of arm strength and grip strength from 149 individuals working in physically demanding jobs including electricians construction maintenance workers and auto mechanics not surprisingly the stronger someone's grip the stronger their arm tends to be there is therefore a positive association between these variables although the points cluster along a line they're not clustered quite as closely as they are for the scatterplot of spousal age a common problem when working with realworld data is the presence of missing or null values within a data set there are three strategies to deal with the issue the first one is to omit the rows if the variable is very important to the analysis and there are not many observations with missing values it can be acceptable to filter or delete those rows the second is to treat missing as a separate category if the variable is categorical this is easy if the variable is numeric then the variable will need to be binned and a category created for the missing rows the third is to impute a value using distribution measures such as the mean or the median or other variables if values of other fields have differing distributions for the variable with missing values we can calculate separate distribution measures using these categories this concludes our video on bivariate data scatter plots and null values this video will cover uncertainty entropy and analyzing data the result of data analysis is information information resolves uncertainty the uncertainty of an event is measured by its probability of occurrence the more uncertain an event the more information is required to resolve the uncertainty of that event entropy refers to the fact that you cannot stir things apart it's a measure of information content and unpredictability here's a concrete example of entropy if you have cold water and hot water and mix them together you will have warm water you can't separate the cold and the hot after they're mixed this is what is meant by you cannot stir things apart to get an informal intuitive understanding of the connection between these terms consider the example of a pole on some political issue the outcome of the pole is relatively unpredictable and actually performing the pole and learning the results gives some new information these are just different ways of saying that the entropy of the poles results is large now let's say a second poll is performed shortly after the first poll since the result of the first poll is already known the outcome of the second poll can be predicted well and the results should not contain much new information in this case the entropy of the second poll result is small relative to the first now consider the example of a coin toss when the coin is fair that is when the probability of heads is the same as the probability of tails then the entropy of the coin toss is as high as it can be that's because there's no way to predict the outcome of the coin toss ahead of time such a coin toss has one bit of entropy since there are two possible outcomes that occur with equal probability and learning the actual outcome contains one bit of information on the contrary a coin toss with a coin that has two heads and no tails has zero entropy since the coin will always come up heads and the outcome can be predicted perfectly in this graph entropy is maximized when the probability is 50% when the probability is zero or 100% there is zero entropy by adding information we can reduce entropy and gain certainty how does entropy apply to analyzing data we can use the principles of entropy to decide what results are important and should be included in a report and what results are trivial and should not be included if there is no uncertainty for a variable then there's no information if we obtained a new observation we would already know the value of that variable typically variables with only one value are excluded from an analysis at the very beginning on the other hand if a variable has a maximum uncertainty because it contains values that are equally likely it will be difficult to guess the value for a new observation these variables are not excluded from analysis applying this concept to data analysis our goal is to reduce entropy by explaining outcomes using other variables we are reducing uncertainty and these results should be the focus of a report this concludes our video today we covered uncertainty entropy and analyzing data this video will cover the parts of an analytical report including the introduction data analysis and results in conclusion the introduction should provide a concise summary of the project including the problem faced the type of data gathered and the highlights of the solution the data section should go into detail about the data used to complete the project variables and other technical terms should be defined well an example value should be listed and interpreted also this section should mention any abnormalities in the data such as missing values and discuss the steps that were taken to clean and prepare the data for analysis in the analysis section the report should cover the thought process behind the analysis including any output and data visualizations that are pertinent methods that are used should be introduced along with a brief description for the reasons they were used and possibly including references to external sources for further study care should be taken to not include every possible analysis as this can provide information overload to the audience insignificant or less significant findings can be briefly summarized leaving the majority of content in this section to focus on the most important findings the results in conclusion section should summarize the results of the analysis and if applicable provide specific recommendations on a course of action because the analysis section covered most of the information gleaned from the data the results in conclusion should mostly just apply that information towards a goal or a further course of study combined the introduction data analysis and results in conclusion sections of an analytical report delivered succinctly and clearly by an effective business analyst can provide useful and pertinent information to drive business improvement this concludes our video on writing analytical reports today we covered the parts of a report including the introduction data analysis and results in conclusion this video will cover automation including a brief introduction to automation macros and stored procedures a report is not the only results of a business analytics project typically automation will often be a separate goal especially when a data source is often updated with new records the main benefit of automation is that it frees up the analyst time to work on different problems this is very valuable from a business perspective as analysts that are skilled in automating tasks can save the company money in terms of labor expenses and overtime can uncover more and more patterns in the data leading to greater profitability there are two main types of automation used in business analytics macros and stored procedures macros are also referred to as functions the purpose of macros are to easily replicate certain steps without having to write out those steps individually macros can make an analysis quicker and more concise for instance with a certain data set you may want to filter sort and then take the average of a field if this set of steps will be used multiple times or for different data sets you may want to transform these steps into a macro to save time the parameters of a macro are the input this can be datasets fields or values the parameters are a component that will change with each call to a macro after the steps of the macro are complete the output will be returned and like the parameters can also be of different types macros are the foundation of many analytics and software packages as many of the complex algorithms used in data science are mostly just the building of certain simpler functions by utilizing macros that have already been built an analyst can become more efficient by building customized macros and analysts can easily pass on their efforts to other analysts most companies with a data science team will build out a repository of customized macros called a code base it makes it easier to manage certain tasks such as committing changes in version control these concepts are taken from computer science best practices of developing software another concept that comes from computer science is the use of objectoriented programming this principle guides the development of macros and is based on the idea that the components of code should be compartmentalized the main benefit of this technique is that it makes large projects easier to develop and maintain instead of an analyst having to replicate an analysis manually every time new data is captured a stored procedure will execute the stored procedure is a type of algorithm that runs according to a schedule and executes a series of steps that the analysts sets up in advance usually these steps will be in the form of code but new software programs allow analysts to build stored procedures without having to write code once code has been production alized only monitoring the execution of the stored procedure is necessary when the procedure completes with an error the analyst will need to troubleshoot the code the most common reason for errors to arise are due to unforeseen data values that is variables will contain value types that were not present in the original data set for example a numeric field will contain an alphabetic character the best practice is to test for these values within the stored procedure and transform them or remove records altogether and add a warning message this will prevent the stored procedure from failing altogether logs are output from stored procedures to describe the processes that ran information that logs produce can vary but usually include the amount of time each process took the number of Records were input and any error or warning messages that were triggered analysts should add code to stored procedures to make logs more descriptive and thus easier to troubleshoot in problems arise depending on the tools being used and the amount of data being processed production Eliza an algorithm can be either a simple task for one analyst or a multiyear project involving many analysts project managers database administrators documentation writers and quality assurance specialists in these cases management methodologies such as agile software development are used to coordinate team members and progress through the project lifecycle this concludes our video on automation today we covered a brief introduction to automation macros and stored procedures this video on regression will cover simple linear regression analysis regression line fitting observed in predicted values the leastsquares coefficient estimation goodness of fit explained an unexplained variation root mean square error and the coefficient of determination significance testing and regression assumptions regression analysis is used to predict the value of one variable the dependent variable Y on the basis of other variables the independent variable X in other words if you know something about X you can use it to predict something about why we provide the independent variable X and we observe the dependent variable Y the linear regression equation is shown here the variable X is considered the independent or predictor variable the variable Y is the dependent or outcome variable we have data on both x and y we use this information to estimate the value of the intercept beta 0 and the slope beta 1 that relate to x and y since the linear relationship is not exact we include an error term in the model Epsilon why is this useful because once we have estimates of beta0 and beta1 from our regression we can use this for any value of x to predict what the value of y would be so if we have data on NBA players weight and height we can estimate how much a typical NBA player weighs based on his height then if someone wants to join the NBA and we know what is height is we can estimate what weight he should be to make it to the NBA in linear regression analysis our goal is to estimate a pattern in this case a line the best fits the data the best fit for our data will go through the core of our data and minimize error the linear relationship in algebra is when a line is represented by its slope and intercept but instead of an exact relationship regression analysis estimates the line from data since the line does not fit data points precisely there is an error term AI measuring the deviation of actual Y from estimated y using the least squares koi fish estimation we can obtain the line that best fits the data by minimizing the sum of squared errors the total variance in Y is divided into two parts that which can be explained by X using regression and that which cannot no line is perfect there's always some error in the estimation unless there's a comprehensive dependency between the predictor and response there's always some part of the response Y that can't be explained by the predictor X using the mean value of y as our reference point we can decompose the total error in measurement between the part that is explained by the regression line and the part that remains unexplained here's a look at the decomposition the sum of squares regression or SSR is the explained variation attributable to the linear relationship between X and y the sum of squares error or SSE measures the variation attributable to factors other than the linear relationship between x and y the SST is the total sum of squares the SSR and SSE together make up the SST so given that total variation SST is the sum of explained and unexplained variation we can divide through by the total sum of squares SST to get the ratios equal to one the ratio of error sum of squares / total sum of squares plus the ratio of regression sum of squares / total sum of squares equals one either of these two ratios can be used to measure our model fit error sum of squares show is called the mean square error we want to choose models with the lowest mean square error regression sum of square ratio is called R squared or the coefficient of determination we choose a model with the highest R squared because R squared + mean square error equals 1 it has to be true that R square our coefficient of determination has to lie between 0 & 1 likewise we can look at the coefficients of the intercept and slope to see if they're significantly different from zero this is done by examining the T statistic and the corresponding pvalues pvalue is less than 0.05 imply that the coefficient is significantly different from zero some key assumptions before you apply regression techniques first the variables have to have a linear relationship in this example the relationship between x and y is not linear so we cannot fit a linear regression line the variables also have to be approximately normally distributed in this example Y is not normally distributed at each value of x so it doesn't make sense to fit a linear regression line additionally the variance of y at each value of x should be the same or in other words we should have a homogeneity of variances the fourth and final assumption is that the observations are independent here one trendline is not sufficient in other words if sales today depend on sales yesterday then linear regression models will not work this concludes our video on regression today we covered simple linear regression analysis regression line fitting observed and predicted value you this lesson covers the tdistribution and how it compares to the normal distribution as well as a brief look at the student's tdistribution for large samples the normal distribution applies for small samples the standard deviation is measured in precisely and the data followed the T distribution a T distribution will approach a normal distribution for a larger n greater than or equal to 100 but it has fatter tails for a smaller n less than 100 the T distribution is very similar to the standard normal distribution it also has a bell curve but the standard deviations are computed from the sample data instead of the population suppose a simple random sample of size n is drawn from a population whose distribution can be approximated by a normal Mu Sigma model when the standard deviation is known then the sampling model for the mean X is distributed as a normal distribution with mean xbar and standard deviation Sigma divided by the square root of n when the standard deviation is estimated from the sample standard deviation s the sampling model follows a T distribution with degrees of freedom and minus 1 this is the one sample T statistic in this figure both distributions have 0 means but the variances are a bit different the T distribution has a lower peak and fatter tails this concludes our video on t distributions today we covered the T distribution and showed how it is very similar to the normal distribution we also briefly showed the students T distribution in this video we will cover logistic regression including the need for logistic regression the logistic regression model and odds ratios and prediction in many instances when you're testing hypotheses and making predictions you will have dichotomous outcomes for example in a game you can either win or lose on a website a user either clicks or does not click in an election a person votes for a candidate or does not when the outcome variable is categorical such as our game example it does not follow a normal distribution the outcome variable is a probability measured between 0 & 1 the estimates you make should be numbers in that range a linear model cannot be applied we need a nonlinear function there are many nonlinear models we can choose from to fit our data some nonlinear functions are shown here the last one shown is the logistic function that will fit the data the best because it has an upper and lower bound here's a logistic regression model in orange versus a linear regression model in blue isn't it a better fit for the data to best suit our data we want a model that predicts probabilities between 0 and 1 so it will be sshaped there are lots of sshaped curves but the logistic regression model is what we'll use in this instance the logistic function is a nonlinear function of independent variables however we can convert this nonlinear function into a linear relationship using the log of the odds ratio note that instead of modeling just zeros and ones we're modeling the probability of an event occurring with a logistic regression model instead of winning or losing we build a model for log odds of winning or losing it's a natural logarithm of the odds of the outcome P stands for the probability of the outcome while 1 minus P stands for the probability of not getting an outcome however having log of P over 1 minus P on the y axis is not very helpful we have to compute the actual odds to do that we have to use the exponential functions let's look at a very simple example of a log it function does alcohol drinking predict political party political party is the outcome variable and it is binary therefore we need a logistic regression a typical log in equation contains the log of the odds ratio is the outcome which is a linear function of the predictors X the log it model to measure the impact of drinking on voter choice is going to be set up as follows the log of the odds ratio will be measured from data on X where X here is the number of drinks per week it's really important to understand that negative 1 point 4 is measuring the log of the odds ratio in other words it is the log of the probability of being Republican divided by the probability of not being a Republican to get the actual odds ratio you have to compute the exponent which is equal to 0.25 since the odds are less than 1 it tells us that the more you drink the lower your odds of being Republican all these calculations can be done automatically in SAS but it's important to understand the math behind what SAS is doing the same model can be extended to more than one variable we just add more predictors to the equation the coefficient beta measures the impact of x on the log of the odds ratio for example in linear regression if y equals 2 plus 3x a one unit increase in X will increase Y by three units in a logistic regression log P over 1 minus P minus 2 plus 3x shows us that if x increases by one unit then the log odds of P y equals 1 increases by three units the impact on the odds ratio is represented by E exponent beta we can also compute probabilities directly to compute odds we have to use the exponent if we don't want to look at odds but the actual probabilities we apply the entire logistic function formula as shown in this probability function these are the odds ratios and log of odds ratios for various probabilities an important point to understand is how the odds ratios are tied to the probabilities note the mathematical equivalencies a 50% probability or probability of 0.5 is the same as 1 to 1 odds the log of the odds ratio at that point is equal to 0 as probability increases odds ratio increases from 0 to infinity while the log of the odds ratio can become any value this concludes our video on logistic regression today we covered the need for logistic regression logistic regression model and odds ratios and prediction this video will cover two types of statistical error type 1 or alpha and type 2 or beta all statistics derived from samples are subject to error a type 1 error rejects the null hypothesis when it is actually true a type 2 error accepts the null hypothesis when it is not true remember that a different sample can give a completely different result a sample mean is likely to fall in the confidence interval only 95% of the time so the inferences drawn from the sample may be wrong let's talk in a little more detail about the type 1 error the type 1 error occurs when a researcher thinks he or she has found a significant result but really that result is due to chance it's similar to a false positive on a drug test the type 1 error or the mistake of rejecting the true null hypothesis will happen with a frequency of alpha thus if alpha our critical value is 0.05 then a type 1 error will occur 5% of the time on the other hand a type 2 error occurs when results seem insignificant but in fact there was something significant going on type 2 errors are like a false negative on a drug test they occur when the alternative hypothesis is true but there's not enough evidence in the sample to reject the null hypothesis this type of error is traditionally considered less important than a type 1 error but it can lead to serious consequences in real situations the power of a test is 1 minus the probability of a type 2 error it is the probability of rejecting the null hypothesis when the alternative hypothesis is true in these competing sampling distributions alpha is set to point zero 5 the bottom curve assumes H a is true the top curve assumes that the null hypothesis H naught is true its right tail shows that we will reject H naught when a sample mean exceeds one eighty nine point six the probability of getting a value greater than one eighty nine point six on the bottom curve is 0.5 one six zero corresponding to the power of the test here's a table that summarizes the types of errors here's an example using a fire alarm if a fire alarm is silent and there is no fire our null hypothesis that it is working is correct but what if the assumption wrong then we've accepted the null hypothesis but we actually have a fire that's our type 1 error the opposite case may also happen if the alarm goes off and there's actually a fire there's no error but if there's no fire and the alarm goes off it's a false alarm that's the type 2 error here it is the less serious problem this concludes our video on statistical error today we discuss type 1 or alpha error and type 2 or beta error this video will cover hypothesis testing which is also called significance testing and occurs when we test a claim about a population parameter using sample evidence that confirms or rejects the claim there are four steps in the hypothesis testing process all of which will be covered in this video here's a summary of the four steps in hypothesis testing after this we'll discuss each step in detail the first step is stating the null and alternative hypotheses we have to establish what we are testing to be true once we do that we have to decide how close to true our sample statistic has to be for us to accept the truth for example we might want our estimate to be accurate with a 5% margin of error this is called locating the critical region once we know that we have to compute the test statistic the Z value or the T value finally based on our results we draw conclusions from the study the first step in the procedure is to convert the research question to a statement of the hypotheses null and alternative forms our study will be to collect and seek evidence against the null hypothesis as a way of deductively bolstering the alternative hypothesis the null hypothesis abbreviated h naught is a statement of no difference in other words the null hypothesis argues that there is no significant difference between our specified populations and that any observed difference is due to sampling or experimental error the alternative hypothesis or H a is the opposite of the null hypothesis it provides a statement of difference in our study we will seek evidence against the claim of H naught as a way of proving h a here's an example of setting up the null and alternative hypotheses in the late 1970s the weight of US men between 20 and 29 years of age had a log normal distribution with a mean of 170 pounds and a standard deviation of 40 pounds to illustrate the hypothesis testing procedure we asked if body weight in this group has changed since 1970 this is called our research question and it can be answered in one of two ways under the null hypothesis there is no difference in the mean body weight between then and now in which case Mew would still equal 170 pounds under the alternative hypothesis we assert that the mean weight has changed EMU's not equal to 170 pounds this is called a twosided test the most common form of hypothesis testing we can also do a onesided test in which we ask if weight has increased over time so the alternative hypothesis would be mu is greater than 170 pounds in step 2 we will locate the critical region once we've established the research question we have to define the level of accuracy with which we want to measure our test statistic any estimate from a sample will not be exactly the same as the population parameter so we have to decide what we think is likely versus unlikely this is called locating the critical region the critical region consists of outcomes that are very unlikely to occur if the null hypothesis is true or in other words the sample means that are almost impossible to obtain when we're estimating population parameters using a sample we have to determine the cutoff values these cutoff values are called alpha if we decide that we want to measure the mean with a 90 percent precision level then the shaded area on the left and right will be larger if we want to measure with a 1 percent precision then the area will be smaller and the range will be larger these are the locations of the critical region boundaries for three different levels of significance alpha equals 0.05 alpha equals 0.01 and alpha equals 0.01 note that boundaries get wider as the critical value Falls in most cases researchers choose an alpha of 0.05 or point zero one our rejection region should have a probability of alpha if the null hypothesis is true but some bigger probability if the alternative hypothesis is true so if the mean lies inside the cutoff value for alpha then the null hypothesis is true otherwise we fail to accept the null hypothesis the result is significant beyond the alpha level for example if alpha is 0.05 our result is significant if it's less than point zero five once we decide whether we want to measure accuracy at the 10 percent 5% or 1% level we can compute the test statistic here we will use the zscore which is a ratio comparing the obtained difference between the sample mean and the hypothesized population mean this is an example of a one sample test of a mean when the standard deviation Sigma is known in our male way example we're going to use the Z statistic because we know the population mean and the population standard deviation to compute the Z statistic we simply insert values derived from our sample into the formula if in one sample we found that the sample mean was 173 then the Z statistic would be 0.6 0 this value on the xaxis under a standard normal curve let's say we found the sample mean to be 185 putting these values into the Z stat formula we find the z stat is 3.0 this is much higher at the tail end of the xaxis on a normal distribution the final step is drawing conclusions once we've computed the Z value of our test statistic we have to look at the corresponding probability values to find out if it's reasonably close to the population mean a large value shows that the obtained mean difference is large and in the critical region the difference is significant which means we have to reject the null hypothesis that the weights have not changed over time if the mean difference is relatively small then the test statistic will have a low value in this case we conclude that the evidence from the sample is not sufficient and the decision is to fail to reject the null hypothesis the pvalue is the area under the normal curve in the tails beyond the z stat it answers the question what is the probability of the observed test statistic or one more extreme when H naught is true to convert Z statistics to pvalue we will use software in one sample with a sample mean of 173 the z statistic was 0.6 0 if we had this sample we would fail to reject the null hypothesis that the mean weights have increased over time likewise if we computed the pvalues for Z equals 3.0 we would get point zero zero 1 which means we have to reject the null hypothesis that the mean weight has remained the same over time note that when we're looking at weight change instead of weight increase all we have to do is multiply the onesided pvalue by 2 to do a twotailed test since we will be using pvalues in all our subsequent analysis it's worth emphasizing what that means pvalues ask the question what is the probability of the observed test statistic when H naught is true remember the smaller the pvalue the more likely that your null hypothesis this is not true this graphic depicts the significance of pvalues at less than one percent between one and five percent between five and ten percent and greater than ten percent these are common significance levels five percent is the most common cutoff however a note that is unwise to draw firm borders for significance as an example a pvalue of 0.27 would not be significant against H naught a pvalue of 0.01 on the other hand would be highly significant against H naught this concludes our video on hypothesis testing also called significance testing which occurs when we test a claim about a population parameter using evidence that confirms or rejects that claim today we covered the four steps in hypothesis testing state the null and alternative hypotheses locate the critical region compute the test statistic and draw conclusions this presentation will cover correlation including a definition of correlation a discussion of the need for correlation details on computing correlation including variance covariance and the correlation coefficient strength of Association linear and curvilinear relationships properties of correlation R squared the coefficient of determination and a discussion of correlation versus causation correlation is one of the most common and useful statistics it's a measure of Association a single number that describes the degree of relationship between two variables we can examine correlations between two variables heuristic ly by looking at a scatter chart in this chart our observations are very tightly centered around the line in this case we would say that the relationship between x and y is more correlated we call this a strong correlation by contrast if the observations are scattered further out we might say the relationship between x and y is less correlated or that there is a weak correlation here are some examples of questions that ask about correlation is there any association between hours of study and grades is there any association between the number of churches in a city and the murder rate when the weather gets hot what happens to sweater sales what is the strength of association between them what about the sale of icecream versus temperature what is the strength of association between them furthermore how do we quantify the association while we can guess the relationship there's a better way to do this using statistical measures the measure we use is the Pearson correlation coefficient to compute correlation we'll need information on standard deviation and covariance we know that the variance is the dispersion within a variable X or Y or the squared average deviation from the mean as shown here the covariance is the dispersion of X multiplied by the dispersion in Y it is calculated as the average of the product of deviations in individual means using the information on variance and covariance we can compute the correlation coefficient as the covariance of x and y divided by the state deviation of X multiplied by the standard deviation in Y this measure of correlation ranges from negative one to positive one a higher number is a stronger correlation and the lower number is a weaker correlation correlation coefficient are measures the strength of linear Association it measures the extent to which two variables are proportional to each other it's unit free so for example a measure of correlation between player height measured in inches and player weight measured in pounds will be meaningful even if they're measured in different units here are some examples no linear Association negligible negative Association weak positive Association moderate negative Association very strong positive Association very strong negative association in these scatter plots what is happening to Y as X is increasing an important point to remember is that correlation is a measure of linear Association if the relationship is curvilinear using the correlation measure is not appropriate if X changes and Y stays the same then the correlation is zero since the correlation measure is a measure of linear Association we cannot use correlations on categorical data it's related to sample size and it's also very sensitive to outliers the correlation measure R measures the strength of linear Association squaring the correlation coefficient gives us R squared which is the coefficient of determination it is the proportion of common variation in two variables this measures the strength or the magnitude of the relationship while we cannot use percentage to interpret R we can do so for R squared for example if R squared equals 67% then we can say that 67% of variation in X is related to variation in Y correlation does not imply causation it's easy to see that in this chart the Internet Explorer market share correlates with the murder rate in the US but that doesn't mean that one caused the other causal relationships are determined based on facts and business models we cannot determine causality from data correlation is a mathematical formula you will get a number no matter what data you feed first you need to establish a logic relation and then find the correlation variables may be correlated if they have a causal relationship for example water causes plants to grow correlation can also occur when one variable is both the cause and the effect for example coffee consumption can cause nervousness but it's possible that nervous people also drink more coffee correlation can also be high because both variables move together due to a missing third variable for example this comparison of deaths due to drowning and soft drink consumption during summer both variables are related to heat and humidity a third variable not shown here emitting such variables can be dangerous here's a look at some additional measures of correlation using scatter charts this concludes our video on correlation today we discuss the definition of correlation the need for correlation details on computing correlation including variance covariance and the correlation coefficient strength of Association linear and curva linear relationships properties of correlation R squared the coefficient of determination and correlation versus causation this video will cover binomial distributions which are a type of discrete distribution we will first compare discrete and continuous distributions of a single random variable and then we'll look at the binomial distribution specifically there are two types of random variables discrete and continuous a discrete random variable has only a finite number of possible values whereas a continuous random variable has a continuum of possible values usually a discrete distribution results from account whereas a continuous distribution results from a measurement the distinction between counts and measurements is not always clearcut a probability distribution is simply a mapping of all distinct events for a variable and their probability of occurrence such as the distribution of a coin flip experiment the form of the distribution depends on whether the variables are discrete or continuous here are some examples of discrete variables outcomes of dice rolls whether a customer likes or dislikes a product or the number of hits on a website some examples of continuous variables include the weekly change in the Dow Jones industrial average daily temperature or the time between machine failures to specify the probability distribution of event X we need to specify all of its possible values and their probabilities we assume that there are K possible values and write out our list of possible values like this a typical value is denoted like this and the probability of a typical value is denoted like this next we will discuss distributions of both discrete and continuous variables for each type of variable distributions can be characterized by three measures mean variance and standard deviation these are formulas for working with discrete distributions well we won't be computing these measures by hand you do need to be aware of the formulas the mean also called the expected value is calculated with this formula the mean is a weighted sum of all possible values weighted by their probabilities mean is denoted by the Greek letter mu the variance has a weighted sum of the squared deviations of the possible values from the mean where the weights are again the probabilities the standard deviation is simply the square root of the variance standard deviation is denoted by the Greek letter Sigma these are the formulas for working with continuous distributions a probability distribution visually summarizes the probabilities associated with all possible events for a variable we will focus on three probability distributions that are commonly used in explaining realworld events binomial and exponential distributions are used with discrete data while normal distributions are used with continuous data a binomial distribution is a discrete distribution that represents the number of successes in n independent trials each of which has the probability of success P each trial has a binary outcome for example a coin toss yields either heads or tails the probability of either observation heads or tails is the same each time we toss the coin these outcomes are generally called success and failure the probability of success is P and the probability of failure is 1 minus P the distribution Maps the outcome of all the trials each trial has to be independent and the probability of success has to be the same for each trial this is the probability mass function formula for a binomial distribution if we toss a coin 100 times what is the probability that we will get 40 heads what is the probability of getting 90 heads that probability can be computed by applying this formula we have only two possible outcomes 1 0 or success/failure in n independent trials this formula depicts the probability of exactly X successes n is the number of trials x is the number of successes out of n trials P is the probability of success and 1 minus P is the probability of failure all probability distributions are characterized by an expected value and variance if we toss a coin 100 times what would be the average number of heads we would get what about the variance these are computed using these formulas you'll often see the assumptions of normal distribution being applied to discrete outcomes this is because the binomial distribution approximates to a normal distribution for large samples so for large enough samples we can calculate probabilities using normal probability rules this concludes our video on binomial distributions today we covered discrete and continuous distributions of a single random variable and the binomial distribution this video will cover normal distributions the probability density function cumulative distribution functions the 6895 99.7 rule and standardizing z values the single most important distribution in statistics is the normal distribution it is a continuous distribution and it's the basis of the familiar symmetric bellshaped curve the mean of the normal distribution is in the center the standard deviations are marked at equal distances from the mean any particular normal distribution is specified by its mean and standard deviation by changing the mean the normal curve shifts to the left or the right by changing how spread out the standard deviations are the curve also changes standard deviations can be spread out wider or closer together therefore there are really many normal distributions not just a single one the normal distribution is a two parameter family where the two parameters are the mean and the standard deviation here's a tool you can play with online that illustrates a normal distribution in real life it looks like a triangular shaped pegboard into which balls are dropped when there's an equal probability that the balls will drop either left or right their final placement forms a normal distribution however when the probability of the balls dropping left or right is unequal which is something you can experiment with using this tool the distribution changes the formulas for mean and standard deviation are very complex but you will not have to compute them because the software will with continuous variables there is a continuum of possible values such as all values between 0 and 100 or all values greater than zero instead of assigning probabilities each individual value in the continuum the total probability of one is spread over this continuum thus the shaded area within the bell curve will always have an area of one the key to this spreading is called a density function which acts like a histogram the higher the value of the density function the more likely this region of the continuum is a density function usually denoted by FX specifies the probability distribution of a continuous random variable X the higher FX is the more likely X is probabilities are found from a density function as areas under the curve so for example the shaded portion under this bell curve represents the probability of X being between 65 and 75 the cumulative distribution function or CDF is the probability that the variable takes a value less than or equal to X it is the total area under the normal curve up to X here's an example the beauty of the normal curve is that no matter what its mean and standard deviation are the area between the mean minus one standard deviation and the mean plus one standard deviation is always about 68% the area between the mean minus two standard deviations and the mean plus two standard deviations is always about 95% and the area between the mean minus three standard deviations and the mean plus three standard deviations is always about ninetynine point seven percent that means almost all values fall within three standard deviations on either side of the mean this is true for all normal curves no matter their shape but how good is this rule for real data let's go ahead and check out an example here's our data the mean of the weight of one hundred and twenty women runners in the sample is one hundred and twenty seven point eight pounds the standard deviation is fifteen point five here's what our distribution would look like let's look a little more closely at that distribution 68% of our 120 runners is about 83 runners according to the 68 95 99.7 rule those runners should all fall within one standard deviation of the mean weight of one hundred and twenty seven point eight that is eightythree of our runners should fall between one hundred and twelve point three and one hundred and forty three point three pounds when we check our data we see that seventynine runners fall within one standard deviation of the mean furthermore ninetyfive percent of our group or about 114 runners should fall within two standard deviations of the mean or between ninety six point eight and one hundred and fifty eight point eight pounds the data shows that 115 runners fall within two standard deviations of the mean finally according to the rule ninetynine point seven percent of our runners or one hundred and nineteen point six runners should fall within three standard deviations of the mean or within a range of eighty one point three pounds to one hundred and seventy four point three pounds according to our data all 120 runners fall within this range so it seems as if the rule is pretty accurate in this case there are indefinitely many normal distributions one for each pair of standard deviation and mean one particular combination of standard deviation and mean deserves special attention and that is the standard normal distribution all normal distributions can be converted into the standard normal curve by subtracting the mean dividing by the standard deviation but all of the integrals for the standard normal distribution have been calculated and put into a table for us and we also have software to help us out so we never have to integrate the long way this diagram illustrates the conversion of X values into Z values when we convert a normal distribution to a standard normal distribution here's a practice problem if birth weights in a population are normally distributed with a mean of 100 and 9 ounces and a standard deviation of 13 ounces what is the chance of obtaining a birth weight of 141 ounces or heavier when sampling birth records at random here's how we solve this problem we subtract 109 our mean from 141 and then divide by our standard deviation 13 so Z equals two point four six then we will use the normdist function in excel to discover that our value for Z two point four six equals zero point nine nine three the chance of a baby being born heavier corresponds to the right tail of the distribution so the probability that we will get a value for Z that's greater than or equal to two point four six can be discovered by subtracting our value for Z point nine nine three from the total area of the standard distribution one this concludes our video on normal distributions continuous distributions density functions cumulative distribution functions the 6895 99.7 rule and standardizing Z values this video will cover populations and inferences sampling error and the central limit theorem a population is the set of all members about witches study intends to make inferences here's a population of people we'd like to study there television watching behavior to determine how many watch a particular show so we can decide whether to purchase advertising spots during this period of time but our population is much too large for a feasible study to study their television viewing habits we will have to survey them and it's not feasible to survey every single individual so instead we'll take a sample of the population to study choosing a representative sample we can make some inferences about the population behavior but it's unlikely that one sample can provide accurate measures of behavior for the entire population an estimate of the population parameter or the proportion watching a television show is likely to be different for different samples of the same size and is likely to be different from the population parameter this is called sampling error the sampling error is unknown but we can estimate the extent of this error by applying the central limit theorem the central limit theorem tells us that what we know about our sample can tell us about the larger population the sample came from for any results that are generated from samples we get a range of estimates of a population parameter which includes mean and standard deviation from each sample in our example it would be an estimate of the proportion watching a particular TV show these estimates have their own distribution and the central limit theorem tells us that the distribution looks like a bell curve the central limit theorem makes predicting outcomes a lot easier if the sample size is large enough then the sampling distribution of the mean is approximately normally distributed regardless of the distribution of the population if all possible random samples each of size n are taken from any population with the mean mu and a standard deviation Sigma the sampling distribution of the sample means or averages will have a mean have a standard deviation and be approximately normally distributed regardless of the shape of the parent population remember that normality improves with a larger n and it all comes back to Z note the symbols here the mean of the sample means is noted as mu of x bar the standard deviation of the sample means is written as Sigma of X bar and is also called the standard error of the sample mean that concludes our video today we covered populations and inferences sampling error and the central limit theorem in this video we will first define probability then we will cover the rule of complements the addition rule probabilistic independence conditional probability and the Bayes theorem a probability is a number between 0 and 1 that measures the likelihood that some event will occur for a random variable an event with probability 0 cannot occur whereas an event with probability 1 is certain to occur an event with probability greater than 0 and less than 1 involves uncertainty here are some examples the odds of winning a lottery the likelihood of a particular candidate winning an election or the chance of rolling a 4 on a fair die in the case of the die there are 6 sides so the odds of rolling of four are one out of six the complementary rule in probability is simply the probability of an event not occurring if a is any event the probability of a is P of a the complement of a is the event that a does not occur the probability of the complement of a is shown by this equation one minus the probability of the event occurring in our dice example the probability of getting a four was one in six so the probability of not getting a four is one minus one and six which equals five and six the addition rule of probability involves the probability that at least one of the events will occur events are exhaustive if they exhaust all possibilities one of the events must occur for example when we roll a 6sided die we will always end up with a number between 1 & 6 we say that events are mutually exclusive if at most one of them can occur for example you can't roll a 3 & a 6 on one die at the same time if you have two mutually exclusive events like our 3 & 6 then the probability of either one occurring is the sum of the two separate probabilities if two events are independent or their outcomes aren't affected by each other then the probability of both a and B occurring is simply the product of the two probabilities in the case of our die the probability of getting a six on the first roll and getting a three on the second roll is one in six times one and six which equals a 1 in 36 chance sometimes the probability of one event will affect another these are called dependent events and their probabilities are called conditional this is the formula for conditional probability the conditional probability of a conditional that B has already occurred is equal to the joint probability of both of the events occurring together divided by the probability of B occurring without regard to whether a has occurred or not the bayes theorem allows us to estimate posterior probabilities once we obtain new data with it we can measure the likelihood of event H occurring once we obtain particular pieces of evidence from data D the parts of the theorem include the independent probability of H or prior probability the independent probability of D the conditional probability of D given H or likelihood and conditional probability of H given D or posterior probability this concludes our video on basic probability today we defined probability and covered the rule of complements the addition rule probabilistic independence conditional probability and the Bayes theorem in this video we will cover variable roles including explanatory and outcome variables and variable classification including qualitative variables nominal ordinal and binary and quantitative variables discrete continuous interval and ratio any analytics project first begins with a question what is the problem you're trying to solve to address that question we need to collect data the next step in the process is to understand the data collected and only then can we move to further steps of data cleaning data analysis and solving the problem a key step in understanding the information collected is to identify all the variables in the data set we need to know what variable types we have in order to make them amenable to further analysis variables have two possible roles the first is explanatory explanatory variables are also called features or independent variables these are variables that are used as inputs to explain the variation in the outcome variable the second role a variable can take on is outcome an outcome variable is also known as a target or dependent variable these are variables that measure the output or impact that's being studied most studies have many independent variables and one dependent variable for example a person's weight could be a function of age gender and calories consumed fuel efficiency is a function of features such as car size weight and number of cylinders restaurant ratings are a function of food quality ambiance and service variables can be qualitative or quantitative qualitative data can be nominal ordinal or binary quantitative data can be discrete or continuous with either an interval or ratio level of measurement we'll start by discussing how to determine whether a variable is qualitative or quantitative the best way to decide whether a variable is qualitative or quantitative is to use the subtraction test if two experimental units such as people have different values for a particular measure then you should subtract the two values and ask yourself about the meaning of the difference for example when hair color is coded as 1 equals blonde 2 equals red 3 equals brown and 4 equals black the difference between the variables has no meaning so it fails the subtraction test which means hair color is a categorical or qualitative variable however if the difference is meaningful then it is a quantitative variable for example age in years the differences between these numbers have a meaning so the variable is quantitative we will now discuss qualitative variables in detail categorical variables are those that have only a few possible values thus assigning each value to a particular group or category for example oceans are categorical variable nominal and ordinal variables are often called labels a nominal variable has levels with arbitrary names for example car colors ordinal variables have a logical order for example exam grades a dichotomous or binary variable is a categorical variable that has only two levels or categories often the answer to a yes or no question but a variable doesn't have to be a yes/no variable to be binary it just has to have only two categories such as gender we will now discuss quantitative variables in detail quantitative variables are those for which the recorded numbers encode magnitude information based on a true quantitative scale they can be discrete or continuous a discrete variable has only whole number counts a continuous variable can take on any value on the number scale to determine whether a variable is discrete or continuous use the midway test if for every pair of values of a quantitative variable the value midway between them is a meaningful value then the variable is continuous otherwise it's discrete for example age is continuous because the difference between ages 20 and 30 is meaningful an example of a discrete variable is the number of children in a family you can see here that 2.5 does not make sense the interval level of measurement ranks data it can be either discrete or continuous with interval variables precise differences between units of measure exist but there's no meaningful 0 for example take IQ scores make sense to talk about someone having an IQ 50 points higher than another person but an IQ of 0 has no meaning ratio variables are interval variables but with the added condition that 0 of the measurement indicates that there is none of that variable true ratios exist when the same variable is measured on two different members of the population for example consider the weight of an individual it makes sense to say that a hundred and fifty pound adult weighs twice as much as a 75 pound child however it doesn't make sense to say that 70 degrees Fahrenheit is twice as hot as 35 degrees Fahrenheit so temperature is not a ratio variable this concludes our video on variables in this video we covered variable roles including explanatory and outcome variables and we also covered variable classification including qualitative variables nominal ordinal and binary and quantitative variables discrete continuous interval and ratio this video will cover basic information about coding coding systems and types of variables in coding including binary ordinal nominal and continuous coding is the process of translating the information gathered from questionnaires and other investigations into something that can be analyzed usually using a computer program coding involves assigning a value to the information given in a questionnaire and often that value is given a label coding can make the data more consistent for example if you ask the question what gender you might end up with the answers male female M F etc coding will avoid such inconsistencies a common coding system for binary variables is the following zero equals no and one equals yes where the number is the value assigned and the yes or no is the label of that value some like to use a system of ones and twos where one equals no and two equals yes this brings out an important point in coding when you assign a value to a piece of information you must also make it clear what the value means in the first example one equals yes but in the second example one equals no either way is fine as long as it's clear how the data are coded you can make it clear by creating a data dictionary as a separate file to accompany the data set a binary variable is any variable that is coded to have two levels like this example in SAS data representing gender coded as MF would be converted into a binary variable here's an example if we're asking about the number of years of education a person has with a value of 1 for each year of education that would mean anyone with more than 12 years of education has been to college and anyone with less than 12 years of education has not been to college we can recode into a binary yes/no variable by saying that if education is greater than 12 that implies that college equals one otherwise college equals zero this type of coding is useful in descriptive and predictive analytics the coding process is similar with other categorical variables for the variable education we might code as follows zero equals did not graduate from high school one equals high school graduate two equals some college or post high school education and three equals college graduate note that for this ordinal categorical variable we need to be consistent with the numbering because the value of the code assigned has significance the higher the code the more educated the respondent is in SAS we would convert years of education to education categories like this here's an example of what not to do zero equals some college or post high school education one equals high school graduate two equals college graduate and three equals did not graduate from high school can you tell what's wrong with this example the data we're trying to code has an inherent order but the coding in this example does not follow that order here's the correct way to do it for nominal categorical variables however the order makes no difference here's an example for the variable reside 1 equals northeast 2 equals South 3 equals northwest 4 equals Midwest and 5 equals southwest it doesn't matter what order we use for these categories Midwest can be coded as 4 2 or 5 because there's not an ordered value associated with each response continuous variables are usually left in the same format as they are in the original data set however be careful about missing values in Muscoda data you may also need to code responses from fill in the blank and openended questions with an openended question such as why did you choose not to see a doctor about this illness respondents will all answer differently also you may give response choices for a particular question but offer an other specify option as well where respondents can write whatever response they choose these types of openended questions can be a lot of work to analyze one way to analyze the information is to group together responses with similar themes for the question why did you choose not to see a doctor about this illness responses such as didn't feel sick enough to see a doctor symptoms stopped and the illness didn't last very long could all be grouped together as the illness was not severe you will also need to code don't know responses typically don't know is coded as 9 that concludes our video on coding with variables today we covered some basic information about coding coding systems and types of variables in coding including binary ordinal nominal and continuous this video will cover using graphs to understand data we will cover three types of graphs bar charts box plots and histograms it's important to know which graph to use if the variable is categorical look at it using a bar chart if it is continuous you should examine it using either a box and whisker plot or a histogram a bar chart translates the data from frequency tables into a pictorial representation it depicts categorical variables and shows frequency or proportion in each category this bar chart looks at the frequency distribution of patients with pulmonary embolism which occurs when one or more arteries and the lungs gets blocked by a blood clot notice that this is a binary variable with only two possible responses yes and no yes is coded as one and no is coded as zero the frequency distribution of a binary variable shows the number of patients in each group it's much easier to extract information from a bar chart than from a table this chart depicts shock index which is the ratio of heart rate to blood pressure and should lie between 0.5 and 0.8 the higher it is the greater the risk shock index is an ordinal categorical variable the vertical bars here represent the number of patients in each category the shape of a distribution describes how the data are distributed measures of shape include symmetric and skewed the first thing we're looking for in any continuous variable is the shape of distribution what are the boundaries of the data points and how are they clustered if a few small values are mixed in with a majority of values being much higher the data will have a left or negative skew likewise if we have some large values mixed in with a majority of small values the distribution will have a right skew or we say it is positively skewed if the distribution is balanced it is symmetric we can also observe skewness by inspecting the values for instance consider the numeric sequence 49 50 51 whose values are evenly distributed around a central value of 50 this produces a symmetric shape we can transform the sequence into a negatively skewed distribution by adding a value far below the mean 40 49 50 51 this produces a left skew similarly we can make this sequence positively skewed by adding a value far above the mean 49 5051 60 this produces a right skew a boxandwhisker plot provides an easy way to examine the entire distribution of a variable and it's also very useful when we want to examine relationships between two variables where one is categorical and another is continuous let's look at the Box first the bottom of the box represents the 25th percentile while the top of the box represents the 75th percentile the line in the middle represents the median the bigger the box the greater the spread of the data the whiskers in the box plot do not necessarily represent the minimum and maximum values they show the minimum and maximum only if these values are less than one and a half times the interquartile range if the values are bigger than that the whiskers represent one point five times the interquartile range or IQR values outside that range represented as dots in the example here note that there are many dots above the top whisker this is a quick and easy way to check for outliers we can look at the same shock index data with a histogram the dots in the box plot showed us that there were several large values greater than one point five times the interquartile range the same is represented in this histogram with the right skew there's no single rule of thumb for choosing bin sizes the bin sizes you choose will depend on the research question you're asking here using 100 bins shows too much detail and it's not useful likewise too few bins tells us little about the underlying shape of the distribution this example uses two bins and provides too little detail note that the box plot we looked at earlier shows the same positive or right skew that we observe in a histogram for shock index the median inside the box plot also provides information on the skewness of the distribution if the median is at the center of the box the distribution is symmetric if the data have a left skew then the median will be pulled to the right inside the box if the data have a right skew then the median will be pulled to the left in the box this concludes our video on using graphs to understand data today we covered three types of graphs bar charts box plots and histograms this video provides a quick review of the measures of central tendency including mean median and mode variation and dispersion range for tiles and interquartile range sample variance standard deviation and the normal curve as well as the 6895 99.7 rule before we begin here's a quick review of symbols we'll use in this video we will also use the abbreviation IQR for interquartile range the mean is the average or balancing point to find the mean find the sum of all the values divided by the sample size here's a simple example of calculating the mean of the age of several participants in a study the Sigma is the sum and the xbar is the sample mean after adding the values together and dividing by the number of values 8 we arrived at our mean 23 point 2 5 we can construct means of binary variables the mean of a binary variable represents the percentage of one's the mean is affected by extreme values which is why we often look at means in conjunction with medians to understand how the data are distributed in this example the mean of the values 1 2 3 4 and 5 is calculated by adding the values together to make 15 then dividing the values by 5 the mean of this group is 3 however if the values are 0 1 2 3 4 and 10 the mean shifts to 4 the median is the middle value of the data in this example we have 7 different ages to find the mean we first order them from smallest to largest and then locate the value in the center however if we have an even number of observations median is computed as the average of the two middle values the median is not impacted by outliers here our median of the five values is three if we add the value ten to the set of values our median is still three the mode is the value that occurs most frequently it is only useful when we have some values clustering together in this example the mode is 9 there may be no mode or there may be several modes there is no single measure of center that is best if the data are normally distributed then mean is used however if data are not normally distributed the median is a better measure often we use both to understand the underlying structure of the distribution there are several measures to examine the spread of the data they include range percentiles interquartile range and variance or standard deviation their range is the difference between the largest and the smallest value this histogram shows a minimum value of 15 and a maximum value of 94 the ranges 94 minus 15 equals 79 another measure of spread is the value of each quartile we take the total number of data points we have and divide them into four parts the value corresponding to the end point of each part is the quartile value the interquartile range is the difference between the value at the third quartile minus the value at the first quartile the first quartile q1 is the value for which 25% of the observations are smaller and 75% are larger the second quartile q2 is the same as the median 50% are smaller and 50% are larger only 25% of the observations are greater than the third quartile let's take the age values 1535 49 65 and 94 the first quartile is at 35 this means that 25% of the participants are below age 35 like why is 25% are above 65 years old the interquartile range is 65 minus 35 equals 30 years sample variance is calculated as the average of squared deviations of values from the mean as shown here we square the differences from the mean to provide equal weight to observations below the mean versus those above the mean because we square the difference values that are further away from the mean get higher weight than those close to the mean standard deviation is the most commonly used measure of variation it shows the variation around the mean and has the same units as the original data it is calculated by finding the square root of the variance here's an example of the standard deviation using age data note that sample standard deviation is represented by the symbol s X bar represents the sample mean the standard deviation is an extremely useful measure it tells us how close or far apart data are from the mean the higher the standard deviation the greater the spread of the data here in red is an example of a moderate standard deviation you can see that the data is spread pretty evenly the purple shows a low standard deviation in which the data is concentrated near the middle the blue example shows a high standard deviation where the data is concentrated on the outside these formulas are important to know well while software can compute these for you it's important to know how it's done using simple numbers whenever you work with data you'll have variables that have a center and spread a very useful rule to know is that no matter what the shape of the distribution 75% of values will lie within two standard deviations of the mean while 89 percent will lie three standard deviations from the mean so if someone gives you just these two pieces of information you can make some predictions on where a new data point will lie however what's even better in statistics is knowing that for large samples data are distributed symmetrically and follow the bell curve the 6895 99.7 rule states that 68% of the area of a normal curve lies within one standard deviation of the mean 95% of the area lies within two standard deviations of the mean and ninetynine point seven percent of the area lies within three standard deviations of the mean this rule works for all normal curves no matter their shape that concludes our video on measures of central tendency including mean median and mode variation and dispersion range quartiles and interquartile range sample variance standard deviation the normal curve and the 6895 99.7 rule you