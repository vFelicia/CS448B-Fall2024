the enforcement learning is somewhat special in that it receives some teaching signal but not as much as in supervised learning so it's it's somewhere between supervised and unsupervised learning the teaching signal that it gets is either whether it has done something good or bad so it's a scaler teaching signal but what's so special about the reinforcement learning is that you have an agent that has to make decisions right I that can go left or it can go right for instance once it has done that it gets either reward or punishment depending on whether it was good or bad and typically just that's just called reward can be positive reward or negative reward so I recommend to to pause here for a moment and read this very nice short description of the essence of reinforcement learning however I will go straight to the more formal description and use this didactic example for that so in reinforcement learning we have states and states are these little boxes and the agent can be in that state so X is the agent and the blue square here is the state in which the agent currently is the agent can take actions in this case and can for example go down it could also go left or could go up if it's in the middle somewhere it has four directions where it can go in this case in this particular scenario that's just one example the agent gets a negative reward of minus one for each step that it takes but it does not have the option not to move unless it is what in one of these two red squares which other absorbing states so once it arrives in this state it is allowed not to do any step anymore so and since it gets a reward of minus one for each step that it takes of course it shouldn't move as quickly as possible into one of these two absorbing states and then just rest there so it can thereby minimize its negative reward the states have a description in this case it's the coordinate of the box and they have a value minus four or minus three or whatever these values are unknown to the agent so the agent has to learn these values and it has to learn socalled policy policy services strategy how to move in that environment and these two are closely related as we will see in a moment ideally the value of the states corresponds to the expected reward given an optimal policy so each time the agent moves from one state to another each time my little agent here moves from one step from one side to the other it gets a reward and in this case the reward is R of T is minus 1 for each step that the agent takes and that is this reward and the expected reward is simply the sum of all the rewards that the agent gathers from now which is T zero to the infinite future and that of course depends on the state in which the agent currently is so this is s of T zero which is called it referred to as s0 so right now the state of the agent is in state s0 and then it takes a series of actions and for each action it receives a reward and if you sum all that up that would be the reward that it gets and if it's sort of a prediction about the future that would be the expected reward so in the example above if we assume that the agent has this policy so for each state there's an arrow that leads into another state right except for the absorbing States and that is a policy right so the agent can behave according to these arrows and if it does these are the true values of the states ie these are the expectation values for the reward for example if the agent is in this state then according to this policy it would first move to the right and then down and if you do that we get an expect we get a reward of minus 1 for this step minus 1 for this step etc so we add 1 2 3 4 5 steps which add up to a expected reward of minus 5 that's why this state here has the value of minus 5 so this is a true value of all these states right I mean these states are nearer to this absorbing state so these are the true values of the states given this policy now but obviously I mean we already see that this is not a optimal policy it would be much better to move directly to the left if the agent starts in this state these are the two values given this policy but these values are not known to the agents so the agent has to learn that so how does that work so let's look sort of a better maybe lower right corner so let's assume and this is the absorbing state right okay so the agent I mean has a map or a function that expresses the value of the different states and initially of course there are wrong values in this so let's say there's three here there's four here there's 1 here 5 1 23 yeah and ok and the agent I mean let's tell the agent that this has a value 0 because once it is there it doesn't have to move anymore and then it doesn't get any punishment anymore so let's assume the agent we put the agent somewhere right we put it here then the question is okay what should the agent who the agent can actually not plan ahead but it can see the values of the states directly around it right so put it know let's put a minus three here and a minus four so that there's very little incentive to go there and minus 5 here so let's consider this situation and sort of the with this policy how the values that are initialized to these wrong values how they can be corrected and learned to these correct values and we follow strictly this policy we will consider how to optimize a policy in a moment but right now we fix this policy and we just try to learn the value function and we do that by placing the agent randomly in different locations in different states of this environment and then see what it can do in order to adapt learn the values to correct values okay so let's put the agent in this state and then it moves by the policy it moves to the right yeah so now it sees here it has a value of five and it sees value of minus one here and it it feels that it gets a punishment of minus one or a reward of minus one on the way to this to this state now it's quite obvious that this state that the value of this state cannot be five if you move into a state with a value of minus one and you receive a punishment of minus one on the way yeah so this value needs to be corrected to a value of minus two minus two is the expectation value or expectation expected reward in this state plus a reward on the way to this state so what we do we correct this five to minus 2 now the agent is in this state it will move according to this policy right it will move down it sees a expected reward of four and receives a reward of minus one on the way and that means minus one is not a correct value right it should be three so we correct this to 3 because that's four minus one right if the expected reward is 4 in this state then it is 3 in this state if it moves down ok finally it moves down to this absorbing state and with the same argument the value 4 is incorrect it should be minus 1 and now we see at least this value is correct right that's the correct value but the others are still wrong so now we place the agent again into someone one of the random states let's say we put it here then same argument the value should not be one it should be right 31 then it moves to the right so it should not be three it should be minus two right and then we go down and that's fine the value is correct and by repeating many of these processes so we place it here let's say this gets this gets corrected 23 right then we go from here to there this gets corrected 21 then we start here again this gets corrected do two then from here to there this gets corrected 22 this is correct etc then we place it here again let's say then this gets corrected 23 and so on now if we're here gets corrected 23 if you're here and so on right so by by going through these different states and always updating the state where it comes from by the value of state where's where it goes into plus the reward that receives on the way the valleys become more and more correct yeah until it converges to this situation so this is how the correct value function can be learned if you have fixed policy and that's written in a question here so the value in the next time step of state s equals the value of the state that it goes into and that's given here by this capital s function so capital S is the Fung is the state where you get into if you are currently in state s and you take an action a that's possible in state s so a state and a particular action brings you into a new state for example if you're in this state and you take the action going to the right then you're in this state so this is the capital S state and this is a lowercase s state now and you take that value so you take this value and you add the reward that you receive if you add state s and you take the action a s all right so this formalizes exactly what I've just shown okay so now we know how to get the correct value function give me policy however we have already Mari marked that this is not an optimal policy right from this state it would be much better to move to the left rather than to the right so how can the agent learn an optimal policy give me a value function well there's not much learning actually it should simply look into the neighboring states and then move towards that state that gives a maximum reward at least in this case where the reward on the way into that state would be equal in all cases yeah so that if the agent knows what the reward is it gets it if it does step into another state and it sees the value in that state it can figure out what's sort of the optimal direction to go and that sense would be optimal to go from here to there rather than from here to there and this is expressed by this equation so the action that the agent takes in state s should be the one that gives the maximum expected reward and the maximum maximal expected reward so this is the maximum operation here over the different actions that are possible to take right a dash are the different options that the agent has and this expected reward is a combination of the value of the state where the agent ends up in after having done this action and the reward that it receives on the way into this state and that's the sum and then the agent simply figures out what's the optimal action to take based on this so there's no real learning here here involved that is something that can be done right away so here we see what happens if we iterate these two processes so we iterate deciding on a policy then learning the optimal the correct value function then based on that decide on the optimal policy because you have seen it is not optimal to go from this state to that state would be rather optimal to go to the left state so that would be the optimal policy given this value function then based on this policy the system can learn the new value function because now it realizes that going from this state to that state makes this we expect a reward minus one rather than minus five and now it turns out that going from this state to that state is better than going to the right so we adapt the policy again and then based on that we can learn the true value function and this now is the true value function and this is actually an optimal policy I mean if the agent is in this state it could also go to go to the left so it's not a unique policy optimal policy but is it is one of the optimal policies now it's not very efficient to 1st converge completely on the two value function and only then decide on a new policy it's better to do both simultaneously actually and this is this can be done by combining the two equations that we had above so the action that we take is always the optimal action in that current situation right given the value function that we currently have we take the optimal action this is how the agent decides where to go and then we use the equation from above to update the value function so this would be the most primitive simplest form of reinforcement learning so that is known as temporal difference learning although there are more refined versions of that and a lot of ways to improve on that in particular there are also ways to to deal with uncertainty right now we have argued we have a deterministic system deterministic environment but you have to use some other strategies if you have the probabilistic environment but this is sort of the the most basic version of reinforcement learning and gives you a sort of a feeling for what the principles are you standard reinforcement learning or temporal difference learning as we have learned it in the first section is rather inefficient one problem is the very slow update of the states so if we consider a very simple variant of the system that we've considered so far where we have these states and we have one absorbing state so the box in the this state in the very right is the absorbing state let's assume the boxes are all initialized with some values okay so let's put it in 3 4 1 1 3 5 5 3 1 3 and we know this has a value of 0 right now as and we place the agent in some of these states and then has the choice of going left or right so if this one is about for some time and then in the end moves from this state with a 3 in there to the last state then it learns that the that the value here should actually be minus 1 yeah but his learn nothing about the other states if then it one is bowed again and again then it updates eventually updates this stay 22 correctly at least this at minus 1 yeah so it has learned as the value of a second state and so it needs to run through this over and over again until the true value sort of gradually propagates back into the through the states now one can improve on this by not only only updating the the value of the state where which I just leave but also the previous states so let's assume we put the agent here in this box and it moves right straight to the right yeah then it learns from here to there that the value 2 is wrong it should be 4 yeah and now at that moment we don't update any other values because we have just started now if then the agent moves from here to there it learns that the value here should be minus four rather than five now the value here has changed from 5 to minus 4 so it has changed by 9 now and now that can help us to update also the value of the previous state to correct that because that used to be the correct value for the 5 that was in here but now it should be reduced by nine like this state so it is corrected along with this one so we put a it was a 4 minus 9 would be 5 now so now the system has learned in one go the two states now that has that would in principle work in such a onedimensional thing No so you could actually start here on this side and let it run once and then it would learn all the different values in one go yeah there are problems if you if you consider a two dimensional environment and for example you imagine for some reason the agent is running in a circle right then if you would really do this strict updating then it would update this point twice and then would get a wrong value so it's maybe not a good value get not a good strategy to update it by strictly the same amount by which you update the current state sort of so it might not be a good idea to update the previous and the values of the previous states by as much as you update the value of the current state so there should be maybe a decay sir the past should be corrected less in order to avoid these kinds of problems and this is what is what is being done in what's called TD lamda learning so what you do here is so this is a standard learning rule that we had above so we update the value of the current state s of T we update it so we look at the value of T plus 1 by the value that the next state has that is VT of s T plus 1 the value of the future state plus the reward and now what come what is added to this is that you also update the values of the state that have been previously visited so as T t so these because of the T these are the states that have been previously visited and you update these right so the going from VT to the T plus one by the difference of the value of the current stage this is their update of the current state and this is the value of the current value of previous states right so you simply correct the previous states by the adaptation of the current state if this prefactor would be 1 then we had the situation that we have that have just described that would allow this one here to update all states from left to right but typically one chooses a value somewhere between 0 and 1 to avoid these problems and the one takes the exponent adesh so the previous the value of the previous state is updated most and then as you go into the past they will be updated less and less if you have a system if you have an environment with no absorbing States so the agent will never finish the job then you might run into trouble because the agent moves all the time that reward will add up to infinity right if you have let's say a reward of minus one for each step that it takes it will just go to minus infinity or if you're more positive right you give the agent a reward of plus one for each successful step that it does then the expected reward will end up plus infinity so this would for instance be the case if you have a situation like Paul balancing writes imagine you have a little card here that can move left and right and you have a pole that's being balanced right so in this case right now and what the cop can do it can move left and right and it's limited right it it's limited here and the left and right so it has to balance the pole within that one meter or whatever right now it shouldn't problem move to the right so that it balances the pole the state in this situation might be described by an angle this angle and maybe the position of the cart would be two dimensional state space maybe also velocity that can vary now the job for this agent is to balance this pole and so you would give it positive reward for every second that it manages to balance the pole and if you would just add that up and if the agent is successful then this reward would add up to infinity so you have an illposed problem what you can do about this is you can discount the reward right so the expected reward would not be just the sum of all the reward that it accumulates but as you go further into the future the reward is discounted by a gamma factor now so T goes to infinity so the exponent becomes larger and larger t zero is the current time step and you can imagine that if this is a factor between zero and one then with this exponent through what in the very future would not count anymore basically right so this is a way to define to well define expected reward if you have a an environment with no absorbing States now in order to reflect this expected reward in the value function you need to discount you need to discount factor there as well so the update rule that we had above would be corrected by just adding this gamma factor here so the new value off state s is the reward reward that you get by taking action a in this state s plus the value in the future states or s of s and a is the state where the agent goes into but discounted by gamma and if you do that you end up exactly with this equation of expected reward because of course this value contains also information about the value one step ahead but that would get two gammas right gamma times gamma because it's two steps ahead and that in the end leads to this gamma exponent expression that you have up here I've mentioned already that it's also possible to deal with non determined ik deterministic transitions but you have to adapt the questions a bit by later for that so let's assume we have a transition we have transition probabilities so if the system is in state s and takes action a then it will not deterministically go into an new state s dash but there will be probable probabilities that are expressed here as P of s dash given s and a now if you had these probabilities you could update the value of the state by adding the reward that you get for taking that action in that state and a weighted average over the values of the state you might end up in or you might transition to and the probabilities are these P of s given s and a problem is these probabilities are not known so they have to be learned along with the values so one possibility would be to really just count the numbers you end up in a particular state if you are to count how often you end up in state s if you end state s and take action a so we can assume these probabilities however that's expensive in terms of of memory capacity at least so what you actually do is you learn the so called cue function that's expressed down here the Q function assigns a value to particular state action combination all right so the values are not own now not the assigned to a state but to a state action pair if you have that it's easy to calculate the value of a state by simply taking the maximum over Q over all actions right obviously the agent and would take the action that promises the highest Q value now and that's described down here so the action taken would be the one that gives the highest Q value if you run over all a possible actions now how do we update queue now if we had a deterministic situation we would update our queue for state s and action a by the reward of SNA and the value of the state we transition to write as would be the deterministically determined new state given we are in state s and take action a however in the probabilistic case where we have nondeterministic transitions this s is not clearly determined right sometimes maybe with 20% it would be you would go left and with 50% he would go down over 30% he would go right or something so you would need to average over the different states I mean if you had the probabilities up there we could actually do a similar trick like here yeah but we don't have that so what we simply do is we have a we use a so called leaky integrator so we don't replace the old value which is QT of s an a completely by this new value but if we simply replace a fraction of it by the new value which is this one here and alpha is the fraction that determines how much we replaced by the new value and we keep partly the old value so let me illustrate that so so let this be the Q value and the initial Q value should be 1 yeah and and now let's not worry about the different states etc let's just say there comes sort of two different values namely 1 and 0 and they come with 50% chance so let's just look what happens with Q if we updated according to such a rule where we just assumed the value that's that that we just get or if we update it to according this rule where we just replace 20% of the Q value by the incoming value and leave 80% at the old value so let's first do the deterministic thing so we get the values 0 and 1 with 50% probability and we start with a value of 1 now let's assume the next value is also 1 I mean then obvious obviously we take that value the next value would be 0 so we will just take the 0 value then maybe a 0 again and then 1 get 1 again 0 a 1 a 1 etc yeah so in that case so if we use this version here the value would just jump back and forth between the two possibilities and the two possibilities correspond to the 2s possible values that we here assume now if we use this rule down here it's different so we add one and we get another one so that would be of course again a 1 so we have the same value do a little cross here now if the next value is 0 and we replace 80% of this value we don't set the value exactly to 0 but we just replace 80% 20% of that by 0 then we get an open date now the next value is a 0 again so we go down another 20% the next value is a 1 so we go up 20% on the wave from this value to this value so we have a value here miss let's say then we go down again a little bit you go up a left a little bit and go up a little bit further but and what happens is that depending on whether you have zeros or 1 it's sort of zigzags back and forth always just moving 20% and it will average in the end around this 0.5 which is the theoretical prediction now so that's what you get but that's what you can achieve if you use this leaky integrator or Lucas leaky summation as it would be probably better called if alpha is large you jump back and forth more if alpha is small you jump back and forth less but you also converge slower to the true value so this is a parameter you have to think about okay I said that using this Q value is more efficient than estimating these probabilities because if you have 1,000 states and ten actions in each state then this would be ten million thousand times thousand times ten ten million probabilities you have to estimate while Q only has a variables s and a so that would be ten thousand rather than ten million and that's a big difference not so therefore this is more efficient than estimating these Peas these probabilities and then going from there now if you have a probabilistic system so if the transitions are not deterministic you have a tradeoff between exploration and exploitation now exploration means you try different things right so if you're in a state and you have three possible actions you try all of them out in order to figure out okay what will be the expected reward if you take that action exploitation means if you've learned already something about the the payoffs off the actions in that state then you should take the action that gives you the that promises the largest payoff right so that would be exploiting what you have learned already and this is a tradeoff because if you to quickly today go into the exploitation mode so you might not have learnt the the expected rewards so that your Q values might just not be right you know and I want to illustrate this with this example here so let's assume the agent always starts in state s1 and then it can assume can take the action a 1 or a 0 and from then on it just runs through this whole thing so there's just one action to take so if it takes action one it gets a reward of one here transitions too as for it gets another reward of one and transitions to as five right so quite obviously the expected reward along this line along this path would be two yeah so so Q of s 1 and a equals 1 would be 2 now let's assume it has and this is something that the agent can figure out with I mean going through there ones right but let's assume it has sort of tried four times the two actions a equals zero and a equals one and with equal probability so n is choosing this twice and this twice now if it chooses this twice and it once goes over the top path here and once it goes to take this lower path Yeah right then the system if it's if you just average over this and don't don't worry about the details of this this leaky summation right then we would get a Q of s 1 and a equals 0 off well if you go this way we have a reward of minus 2 if you go this way we have a reward of plus 4 so the reward on average averaging just over these two trials would be one now because the minus 3 and the plus 3 cancel out however if you look at this one here we see that the probability of getting a minus 2 is 0.1 while the probability of going down here is 0.9 now so this would be the estimated Q value after the two trials which happen to be one up here and one down here but the true QL value would be so maybe so this is estimated and this would be two Q of s 1 and a equals 0 so that would be 0.1 x minus 2 Plus 0.9 times 4 and that would be I mean if this were a 1 then this would be 4 but we have to reduce this by 10% so that would be 3 point 6 and then we reduce it by another 0.2 so that would be 3 point 4 so here we see that by just averaging over the two trials that we had we get a to low value by chance and if you would now decide to always choose a equals one because there we get reliably a value of 2 which is greater than this one then we would just have not done the right thing right because the true probability or the true expected reward here for equal zero would be three point four now so you have to try often enough the different options before you decide on one particular strategy or policy if you ain't a determined if here in a nondeterministic system reinforcement learning has successfully been used also for game playing for training and game playing agent now in that case you have two players so how do you deal with the situation that you have two players and you want to train reinforcement learning well that's relatively simple an illustrated in this picture you simply treat the other agent as part of environment and then it becomes obviously it becomes a probabilistic environment because the other agent might change its policy on the way now but otherwise it goes through straightforward right so let's assume we have such an environment and we haven't we have him sort of the this is a state in which the game is right now and we have a player a and a player B that alternate in their decision right so in this case player B starts first and the goal of knowledge is check yeah so the goal of and the goal of player of player a is to reach this absorbing state while the goal of player B is to reach this absorbing state so let's assume B starts it moves down then it's the turn of player a it moves left then it's a turn of player B turns down again then it's turn the turn of player a moves left again etc so that way you have this path now if you only look at the role of player a for play a it looks like it takes a decision now this here takes a decision namely left and it ends up down so this is a probabilistic decision probabilistic transition to this downfield to the lower left field due to the action left then it takes the action left again and then adds ends up here but it could also have ended up here if player beef had decided to move to the left right so the other player adds some randomness to the transitions to the transitions here for player a and in this case player has one right because player B has dum dum moves okay so the last extension I want to talk about is large is an extension for large state spaces in particular games because you have many many positions like backgammon for example that's a game that has been played by successfully trained with reinforcement learning these games have a huge state space right so there are many different ways the stones can be on the on the board much too large for the standard discrete reinforcement learning to work that we have seen so far so what you can do in these cases you can approximate the value function by a neural network let's say so you need some fun function approximator that works in this state space but has much fewer parameters than you would need to represent all the values of the different states so these were a few simple extensions to address some problems that you might run into in reinforcement learning as of course much more to it but I will stop here you