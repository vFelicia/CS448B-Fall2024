conditional and joint probabilities are concepts in probability theory that are that are quite useful when multiple events are occurring a joint probability is quite easy to describe it's the probability that an event a and an event B occur at the same time so let's take for example that event a is the probability of landing let's say that a is defined as we have to define the event first a is the that a coin flip lands as heads and let's say that B is the event that a die rolls as a 2 we can easily calculate the probability of a this is simply equal to 1/2 we can also equally calculate the probability of B this is equal to 1 and 6 right fair die now let's ask the question what's the probability of a and B occurring together this is the joint probability well the probability that you both land as ahead and roll it to 1 these are independent events one doesn't influence the other so you have to have both coin and landing as ahead an or a rolling a die these are just these two multiplied together right it's 1/2 times 1/6 which is equal to 1/12 that doesn't seem particularly insightful but things get a little more interesting when we talk about events that are not independent let's define another event event C as the die roll being even so now what's the probability of B and C occurring at the same time well if it turns out that the Dyer comes out as a 2 then we know that the roll is also even so this is just 1 over 6 because it doesn't constrain it anymore similarly the probability that probability of C all by itself right is equal to 1/2 but here we did not multiply 1/6 by 1/2 because we know that there's an interaction effect these are not independent events now let's ask a different question let's ask the likelihood let's ask for the probability of two events occurring but knowing that one of the events already occurred so let's ask what the probability of B and C occurring is right this event of rolling a dice a two and the diving even but we know that this actually occurred we know that C occurred let's then divide it by the probability that the die roll was even we know that I'm telling you that this occurred we're asking for the probability that these two events occurred at the same time but we know that I'm just we're just speculating I'm speculating but we're just defining we're saying that C occurred the probability of the die roll B even happened we know that it is even so if we know that it's even what is the probability that it lands as a well that's this expression is 1 over 3 because if we know it's even then it's either two or four or six and it's one of those three so it's 1 and 3 this is the same write as 1 over 6 which was the joint the joint probability divided by probability of C which was 1 over 2 which is equal to 2 over 6 which is equal to 1 over 3 right it's the same that's quite cool and we define this expression as the joint as the conditional probability what's the probability that B occurs knowing that C occurs and this is equal to he joins C over P see I'm just rewriting this again so in so we have it in green in fact what I'm gonna do is I'm gonna write it with a and B the probability that some event a occurred knowing that some event B occurred is equal to the joint probability of a and B divided by the probability of B now it gets a little more interesting because this idea of two events occurring together is not by any means selective for a coming in front of B two events occurring simultaneously there's two events occurring simultaneously so there's actually two ways of writing this I mean that this let's look at this if we have the probability of an A intersecting with probability to be that these two occur together in fact what I'm gonna do is I'm going to write this in the middle and show it interpret it in two ways then on one hand write this is equal to we can divide this by by B as we did before and over here show our conditional probability so this is the conditional probability of a conditioned on B and normally I would have this right in the previous in the previous slide I had this divided by B but we'll just multiply that over here times the probability of B same thing I haven't changed the equation but I needed this is the same like the probability of a and B happening together is also the same probability of B and a happening together so there's another way that we can write this on this side such that this is also the probability of B conditioned on a time's the probability of a and if this is true if this is equal on both sides that means this and this are equal and if that's the case then let's throw this B over out here and rewrite this a B by itself and we'll do that one in yellow so here I'm gonna write the probability of a conditioned on B times the probability divided and I'm dividing this on the other end that's let me set this equal to this guy here probability of B conditioned on a time's the probability of a and I'm gonna pull this guy and just divided over here divided by probability of B and this equation is bayes's theorem and it is the basis for all statistical inference the conditional probability of a conditioned on B is equal to the probability of B conditioned on a times the probability of a divided by the probability of B these ideas actually have very common names that are often thrown around this value here is the posterior this value here is the prior this value here is the likelihood this value here this term over here is the evidence you'll also see these two terms called the marginal probabilities the marginal probability is just the probability of an event by itself so these are both marginal probabilities but if you have an event that you're trying to figure out the probability of and you know some other event occurred already right you have some point of evidence about it that you know these two are related then you can condition the likelihood of a occurring based on the evidence you've seen in B and rewrite it this way because here what you're asking you're saying is well how it's the probability that the evidence that I'm seeing is what it is if I assume that a is the event of interest that I'm looking at you multiply that by how likely a event is a by itself divided by the probability that you see the evidence B that you got and you actually will get your posterior and it all comes out of this relationship that it that the that the joint probability can be written in two different ways of conditional probability the naive Bayes classifier is a very simple statistical learning technique that leverage is the relationship of bayes's theorem to perform classification task that is otherwise difficult to simply estimate when there is lots of evidence available from different different features of data then you can make an estimate here of what the likelihood of the event is interested you know of interest is again this is our posterior this is our likelihood this is our prior and this is our evidence the naive Bayes classifier was one of the very first successful classifiers that could detect for example whether or not a piece of mail with spam versus legitimate so it was actually one of the very original spam busting algorithms that was deployed in the net and the Internet in the early days now we have much more sophisticated algorithms but the naive Bayes classifier still does a decent job and the way it works in general is that you want to for example a would be write the probability if we're talking about the spam example the probability that a piece of mail is spam and B here would be the evidence that we obtained right and the evidence could be all sorts of things it could be you know the text of the email it could be where it comes from it could be the time of day it came from it could be the length oops of the email it could be the presence or absence or the presence of certain words in the email or it could just be all of the different words the most classic version of this is simply the collection of all the words that are present in the eye so this is text all words the most the most classic version of this simply looked at just that I didn't do any of this stuff just took all the words that were in there and treated every word totally independently one of the key features of the naive Bayes classifier is that it assumes every piece of evidence that you're using is fully independent if your if your evidence is not independent then that can cause problems for the accuracy of the classifier so that's called a model mismatch but the naive Bayes classifier fundamentally assumes that all of the different pieces of evidence the different features the dimensions and the features in this case every word that's present is independent that may or may not be true but that's what the assumption of this D of this classifier does so that's our evidence B and so now let's take a look at what we're doing here what we're doing is we're saying well what is the probability we're gonna dam we're gonna we're gonna look at this what's the probability that we got that evidence all right so the probability of this piece of mail is BAM given the evidence we got right the text the words in the text is equal to the probability that we got this piece of evidence conditioned on this being a piece of spam would we get these words if what's the likelihood of it what's the probability of us getting those those words together seeing those words in this email can on the fact that this we're assuming this is either a piece of spam or condition on the fact that it's not a piece of spam that would be the probability this piece of mail is not spam all right so if a is the probability that it's not a spam then if this is this is just a binary classification then not a write the inverse of a opposite of a would be the probability of this piece of mail is not spam and thus that's equal to the probability that we see this piece of evidence conditioned on the assumption that it is not right not a not a piece of spam then we multiply this by the prior which is a probability that any piece of arbitrary mail that comes in is spam and so your prior might be very very high because in most mail systems the bulk of mail that goes through is just spam so this number and the probability that your mail is spam could be very high initially for a and the probability that it's not a it could be extremely low what does this mean this means that it helps push the system to make I things as spam because if for a thousand pieces of email that come in if 998 of them are spam and only two of them are real ajith omit then your prior right knowing nothing else other than a piece of email has come in you want to just assume that you know you your chances of that being spam is extremely high and so your prior right your your non evidence driven probability of this thing being spam is already very high and so you need to have overwhelming evidence in order to convince you that it is not spam right this number the probability of seeing those were conditioned on conditioned on this being spam needs to be extremely small to counteract something with the lard bias and that's that's exactly what these distributions are designed to do and then you divide everything by the probability of seeing that slice of evidence now for classification tasks this is not something that you tend to worry about very much because even though technically that will give you the actual probability here what you are really doing a naive Bayes classifier and this is in this two version example is pairing this probability with the opposite probability the probability of the other term all right so let's draw a line here you're comparing it to the probability that it's not spam given that we saw this piece of evidence and that's equal to the probability of B conditioned on not a times the probability of not a divided by the probability of B well this is great because if this is the probability that a piece of mail is not spam given our evidence and this is the probability that is spam given our evidence we're dividing by the probability of B in both cases let's not even bother because we don't want I think about that that's just the term that is the same in both so we can just get rid of it wonderful now we just have these two terms the probability of B conditioned on a and the probability of a or the probability of a not now in the case that the probability of the events is the same guess what then this term and this term would be the same and you could cancel these out as well when would this occur this would occur in the case where if you in a thousand emails half of them are legit and half of them are spam the probability or priors right the probability that any arbitrary piece of mail is spam is half and the probability that any arbitrary piece of mail is not spam is also half and thus these two probabilities these priors would be equal and you can cancel them out of the calculation for the spam example you cannot do that because we just said in a thousand emails 998 of them are going to be spam thus these probabilities need to be kept so that you can you can properly account for that other examples may have an equal distribution of events for a and not a or all the different different type of events that it could be and thus in those cases you can cancel this out but it's not a given you have to be very careful about whatwhat this prior is telling you let's again label our terms just so we know this is posterior and I'll do this in some green here so this is the posterior this is our evidence this is our prior and this is our likelihood and so now we're task right so it's obvious that we can calculate these these probabilities for our priors they're easy right because you just look at all of the different males you get and you know for some training set whether or not they're spam or not you just count those probabilities and thus you have your priors no problem now we have to think about how do they get this likelihood this is now the challenge to model what is the probability of seeing a particular evidence distribution that we just saw conditioned on the fact that we are assuming that this is spam or what is the probability of seeing this probability distribution conditioned on the fact that it is not spam that is not something that you can calculate explicitly however it is something that can be modeled and we can model it because we have prior data telling us so in order to build and execute an EIN based classifier you need a training set you need prior data and examples with which to build this likelihood estimate you need to be able to take some training set label some of them as spam some of them as not spam manually someone has to do this and then if you are looking for evidence and your evidence is going to be this text and all of the words in the text then you need to go in and find the probabilities that any given words showed up in spam versus not spam emails and you need to have a dictionary a table of all the different probabilities for all the different words when they are spam versus not spam and so if you had for example you were building this model if you had the word for example offer right or money then these terms Knight or opportunity or virus or hack right these are often terms that are usually in if they're in an email then you know or no other term Pro terms like MoneyGram right right these terms are more often associated with spam emails than not this the probability of seeing a word like this conditioned on it being spam is higher for virus hack and offer than it would be for not spam just as examples further because the naive based classifier treats everything at all the features all the words in this case since these are features as independent you can separately multiply the probabilities for every single one these words independently because you're not modeling any correlation between them that is often a limitation because if an email has the word hack in it probably also has the word virus in it or if it has the word offer and it probably also has the word money in it and if these terms are present together that could probably even strengthen right your ability to determine whether or not this is a piece of spam or not but that's not within the scope of this classifier classifier doesn't do this then I based classifier simply assumes that each of the features are independent and thus will independently look at the probability of the whatever words are in the the email and look up in its table look up in its model what the probability of see that word is conditioned on looking up in the in the spam table versus a non spam table and give you that probability as an output and then you do that for all different words that are in the email and you have a string of multiplications one for every single word and that if you look it up from the probability of the of the spam table of the spam spam train table gives you some probability and if you look it up for the table of words that are in the non spam category it gives you a different probability of multiplicative property and then you multiply those by the prior and whichever of these two are higher you then classify that piece of email as spam or not spam accordingly it's actually a very clever and simple idea but it requires being able to build these likelihood estimates and that requires previous training data but that's really all that's going on that's the process of a naive Bayes classifier it simply looks at prior examples builds up some likelihood build some prior and for all the different conditions that a can take on in this case it's only two because it's spam versus not spam it will estimate what the probability of that event occurring given the evidence that it is seen the same evidence in all cases right we're using the same evidence in it to evaluate whether or not a piece of email a piece of email because that's what we're evaluating right the email is there it is spam or not spam and so we're smashing it through the different likelihood models for some given evidence to see whether or not this conditional probability or this conditional probability is going to be higher and if you have three different possible outcomes of a right if it's if it's a decision you're making whether you have to go forward left or right then you have three different likelihood tables you have to look up or three different likelihood models you have to consult then you end up with three different probabilities and you take the highest of those and that's what the classifier would classify that's all that's going on in our Bayes classifier very simple but also extremely powerful is if you can get this likelihood model correct if you model your evidence correctly against the different conditions then it can be extremely accurate assuming the there is a difference of abilities in across the event conditions what does this mean in you when would about naive Bayes classifier fail for this example if a night anion based classifier would feel if spam and real emails used the same words right if the same words were used for both spam and nonspam emails in exactly the same frequencies and all that kind of stuff then this decoder this classifier wouldn't work at all it would not be able to tell the difference between spam and not spam because that's all it's using to come up with these these likelihood estimates but mercifully right what can what is very salient what's very important in determining whether a piece of email is spam or not is the actual words that are used and thus the likelihood of seeing a particular set of words when something is spam are going to be very different and the likelihood of seeing those same words if that if that email is not spam and this is how a naive Bayes classifier works this is how the earliest spam detectors worked by simply pulling out these models with all of the different emails that had previously been banked labeling them manually and building these banks of likely residents