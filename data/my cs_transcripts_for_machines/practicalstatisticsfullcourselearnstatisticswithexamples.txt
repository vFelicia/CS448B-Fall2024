here we'll do a quick example where we identify the sample and the population in a survey question asks a survey asked 2 000 u.s households if they currently own at least one pet the results show that 69 of households do own at least one pet identify the sample and the population in this situation remember with a survey like this the population is the group that we're interested in knowing something about but it's usually not feasible to study the entire population so we gather data from a small subset of that group so in this case we're interested in knowing about all us households so that's the population notice that that wasn't explicitly stated but it's clear from the problem statement that that's what we're interested in knowing about because it's infeasible to study all households in the u.s we take this sample of 2000 households we gather data from them and we use that to draw inferences about the entire population here we're looking at the idea of representative samples so if we're looking to measure something about a population we want to gather a sample to measure and we want to make sure that when we do that the sample represents the population that it looks similar to the population as a whole so we'll look at a couple examples here first of all to find the average annual income of all adults in the united states suppose we sampled representatives in the congress of the united states it turns out this is not a very representative sample first of all the salary for representatives in congress is set at a fixed number and that number is relatively high compared to the average income for all adults in the united states so it's not representative because if you look at the whole population there are some people who make very little and some people who make a lot and in the congress there's a fixed value that's unlikely to be similar to the average value of all adults in the us so it's not a very good representative sample we would say no this is not representative the second example says to find out the most popular cereal among children under the age of 10 you could stand outside a large supermarket one day and pull every 20th child under the age of 10 who enters the supermarket it's not clear that there's any bias in this one this seems like a pretty good way to find an answer to this question if you pull children coming into a supermarket of the right age group you're likely to get a pretty representative sample for all children to do this now you may want to pick different areas of the country for instance there could be differences depending on where you look but without going any deeper it doesn't look like there are any obvious red flags that this would not be representative for all the answers you're looking for so this one looks fairly good and the lesson from these is just that when you're gathering a sample it's important to look for a representative one one that's likely to look similar to your population you don't want a sample that's chosen too narrowly or that's chosen with some sort of obvious bias this is a simple example that illustrates a way that a sample can be biased here a coach is interested in how many cartwheels the average college freshman can do at his university eight volunteers from the freshman class step forward after observing their performance the coach concludes the college freshman can do an average of 16 cartwheels in a row without stopping is this sample random and representative in general a good sample is random and representative a simple rule of thumb for deciding whether a sample is random or not is just to think about whether or not every member of the population is equally likely to be selected if so there's randomness involved to decide whether or not the sample is representative think about whether the sample looks similar to the population here the biggest source of bias that we observe and bias means that the results will be skewed is this voluntary response bias voluntary response bias means that rather than picking people to ask the coach asks for volunteers in this case people that are able to do more cartwheels are more likely to step forward and volunteer for the study because of that we conclude that this probably isn't a very good sample to do this study a voluntary response bias also comes into play in surveys that have questions where certain responses are more favorable than others in this example we're going to decide which type of sampling is being used in each description the first situation we have a soccer coach who selects six players from a group of boys aged eight to ten then seven players from the group of boys aged 11 to 12 and finally three players from a group of boys aged 13 to 14 to form a rec team notice the key here which is that the coach has divided the group into segments based on their ages so there's a segment from eight to ten a segment from 11 to 12 and then a segment from 13 to 14. and from each segment the coach has selected several players so after dividing into segments the sampling could be either stratified or clustered depending on what happens next but the fact that we're choosing a couple from each group makes it stratified if we chose a couple of groups all together that would look more like cluster sampling so this first one is stratified sampling now generally with stratified sampling we select the same number from each group in this case the coach didn't do that he selected six from one group seven from another and three from another but generally speaking with stratified sampling we select the same number from each group in the second part there's a pollster who interviews all human resource personnel in five different high tech companies this may be hard to see at first but the fact that this pollster selected five companies leads you to think that they looked at all the companies that were out there and thought of each company as a group and they selected all human resource personnel from a few of these groups so by dividing them into groups again we can think about either stratified or cluster sampling starting that way and then because the pollster selected everyone from a couple of groups that's cluster sampling if they had selected a few from all the groups that would be stratified sampling but the fact that they selected a few full groups makes it cluster sampling the third one a high school educational counselor interviews 50 female teachers and 50 male teachers notice again that there's a separation of categories and so the teachers have been separated into male teachers and female teachers they've been divided into groups and then from those groups some have been selected an equal number from each which again looks like stratified sampling next a medical researcher interviews every third cancer patient from a list of cancer patients at a local hospital and the key here is that term every third which is what identifies this as systematic sampling systematic sampling is where we have a list like this and we pick some step like this like three and we check every third or we could pick every fifth or every tenth whatever it is that systematic moving through the list is what makes this systematic sampling the next one a high school counselor uses a computer to generate 50 random numbers and then pick students whose names correspond to the numbers notice how there's no division into groups there's no systematic process this is the full population of students and we just select random numbers from that full group and that's what makes this simple random sampling that's kind of the simplest version where we're looking at the full population and using a random number generator to just select without any division into groups or anything else lastly a student interviews classmates in his algebra class to determine how many pairs of jeans a student at a school owns on the average notice here that this student is looking for information about the whole school but rather than looking at a full student list and selecting randomly from them or selecting every third student or even dividing them into groups the student just asks the students nearby the students that are in this class next to him which makes this a convenient sample in this example we'll see how to select a simple random sample from a population the population we're given is a set of six quiz scores and there are 10 students in this example we'll draw a dot plot and we're given data that represents the ages of 30 randomly chosen mba players the first thing we need to do is draw an axis that will cover the full range of the data so just kind of scanning through it looks like the lowest value here is around 20 and the highest value is about 36. so let's make sure our range will at least cover those values let's draw an axis here with values from 20 up to 36. now that we have our axis all we need to do is read through each of these data points and put a dot for each one so for the first value at 22 we just put a dot above the 22 then we have 28 so we'll put a dot at the 28. notice it hovers a little bit above the marker but it doesn't really matter how high we put them as long as we place them at consistent heights just so we can visualize the final result that'll make more sense as we draw more of these the next value is at 20 then at 24 then at 26 then 21 27 and then we get another 28 so the second time we've seen 28 so we won't draw the second dot at the same place as the first one but we'll put it right above it so now there's a second dot 28 then we go to 31 and 29 and just continue on entering these we hit another 24 here and again we just put this one above the first one at that location then we have another 22 another 21 then a 25 another 22 another 25. 30 another 29 another 20 and so on there's our 36 another 24 the first 23 another 36 another 24 another 29 and we'll just finish this out so there we see our dot plot where each time we run into a value we've already drawn a dot for we just draw a one a little bit higher so notice that the height of these stacks tells us how frequent an age is so the more frequent ones are over here and then the 36s there's kind of what we would call an outlier they're far out from the other data points and for instance 30 is relatively rare there's only one of those and so on so there's a lot we can tell from this plot and we'll draw other types of plots later on that fit this same kind of pattern of looking for where the data is clustered and where it's spread out but a dot plot is a very simple way to observe that at first here we're going to build a simple frequency table the problem states that 19 people were asked how many miles to the nearest mile they commute to work each day and their responses were recorded in this data set the frequency table lists each possible data value and the number of times that data value occurs so we put two columns like this one for the data values and one for their frequencies now we'll go through the data set and for each unique data value that we see we'll list that in the left hand column once we've filled in all these data values now we just need to count how many times they occur so for instance i notice that 2 appears twice in the data set so it has a frequency of 2. 3 appears only once so it has a frequency of 1 and so on and i fill in the rest of the table and it's really as simple as that once we've filled in the frequency table a quick check that we can do is add up all the frequencies and they should add up to how many data points we have and here if we add up these frequencies we do find a total of 19. in this example we'll build a frequency table for categorical data we're given a sample of mba players with their position which is a categorical variable it divides them into categories point guards shooting guards small forwards power forwards and centers and we're going to build a frequency table to go along with this so the categories will be the divisions and then we'll count the frequency in each category so our frequency table will start with two columns one for the position and one for the frequency and then we'll add a third column for relative frequency it's a good thing to include when you draw a frequency table it's not entirely necessary every time but it's a good idea to include it when you can so the positions we have point guard shooting guard small forward power forward and center as our five categories and then we just go through and count how many there are of each one so for point guards for instance we have one two three four of them so the frequency is four for shooting guards you can count one two three four five six seven eight nine for small fours we have one two three four five six seven for power forwards we have one two three four and five and then for centers we have one two three four and five and notice if we add up those frequencies we should get the total number that we have which is 30. so we should get 30 if we add those up and you can check that by adding those frequencies for the relative frequency we need to divide each frequency by the total number that we have in our sample which is again 30. so for the relative frequency of point guards we would divide 4 by 30 which we could write as a fraction or if we wanted to we could write that as a decimal which comes out to about 0.133 or 13.3 percent then for the next one we would divide nine by thirty which works out to thirty percent seven out of thirty is twenty three point three percent and then five out of thirty it's about 16.7 and that's for both the last two categories so again a frequency table is pretty easy to construct all you have to do is count how many fall into each category in this case the categories were the positions of the players here we're going to build a grouped frequency table using the data set of nba players with their points per game shown below so we have 30 values ranging from 1.4 up to about 24 and we're told to use a class width of 5 for our group frequency table so we want to start low enough that we can cover all of them and rather than starting at one let's start at zero just to get a nice round number and make it easy for ourselves so in our frequency table the first column will be points per game and our first category the first class will start at zero and we'll go up to five but remember we won't go all the way to five because we don't want to overlap our classes so the first one's gonna go up to just less than five let's use 4.9 to represent just less than five and the next one will start at 5.0 and go up to 10 but just below 10 so we'll stop at 9.9 and then we'll go from 10 to 14.9 15 to 19.9 and 20 to 24.9 we don't need any more classes because that's as high as we need to go to cover everyone in this data set so the next column will be the frequency and then lastly we'll have the relative frequency since there are 30 values here in our data set once we have the frequencies we'll just divide each one by 30 to get the relative frequency we won't show this in detail for all these classes but just for the first class from 0 to 4.9 we'll go through and select all the ones that fit into that category so between 0.0 and 4.9 in the first row we find 4.9 in the second row we find 1.4 and 2.0 and in the third row we find 3.0 4.3 and that looks like all of them so there are a total of five that fall into that range and then we can continue this on for all the others but i won't show the counting you can go through and do that yourself and i'll show you the results here so once you count all the others you should get 11 for the second class 5 for the third class 4 and then 5 for the final class then if we divide each of these by 30 to get the relative frequency 5 divided by 30 is 0.16 repeating so you can round that to 0.167 or 16.7 percent and then 11 out of 30. we could round to 36.7 percent 5 out of 30 again is 16.7 percent and then 4 out of 30. we could round to 13.3 to get the relative frequency for each so it's really just counting the only thing to keep track of for grouped frequency tables is to make sure your classes are all evenly wide and that none of them overlap that's the important piece here we'll do the histogram using the data set for the players in the mba with their points per game listed here in the last example we built a grouped frequency table and now we're going to build a histogram that matches it so we're going to take the frequency table we built earlier and just draw this histogram to represent the same picture so due to that we'll start with a grid and the x values will range from 0 up to 24.9 or up to 25 so we'll have 0 here 5 10 15 20 and 25 and each of these classes will fit between two of those values so the first class goes between 0 and 5 the next one goes between 5 and 10 and so on and of course we know that it goes just up to 5 but not including 5 and so on but we'll draw it as if it goes all the way to 5 just to make the picture as simple as possible then on the vertical axis we have the frequency so here we have the points here we have the frequency and the highest frequency we see is 11 so we need to at least go up to 11. so let's have 10 here and five and then we just draw a bar to the right height for each category in the first class the frequency is five so we'll draw a bar up to five ranging from zero to five then from five to ten the frequency was eleven so that one goes all the way up to the top of the graph then from 10 to 15 that frequency was again five from 15 to 20 it goes down to four so we'll draw it a little shorter down at the tick mark for four and the last one goes back up to five so each class each category gets a bar with the height representing the frequency it's relatively simple once you've drawn the frequency table which just consists of counting the ones that fit into each class here we'll build a bar chart for this data set which is the positions for the players in the mba sample again we have 30 observations falling into one of five categories point guard shooting guards small forwards power forwards and centers first we need to find the frequency of each category which means building the frequency table but since we've already done that in a previous example i won't go through in detail and count those rather we'll just put the results here in this frequency table we have the five positions and the frequencies there are four point guards nine shooting guards seven small forwards and five power forwards and five centers now just like with the histogram we start with our grid and this time the horizontal axis will represent again our categories so we'll have five spots for our bars to go and then the vertical axis will again represent frequency so the horizontal axis is position the vertical axis is frequency the highest frequency we see is nine so we need to make sure we go up to at least nine once again go to ten and now we're ready to draw a bar for each position so rather than having the bars connect like they would with a histogram with a bar chart since we're thinking of these as separate categories they're not ones that flow into one another we'll draw the bars with some separation so at the point guard position we'll draw a bar that goes up to four since there are four of those and then at the shooting card position we'll draw one that goes up to nine small forwards have a frequency of seven and power forwards and centers each have a frequency of five so again we're drawing a bar where the height represents the frequency of that category but unlike with a histogram these bars are separated because we're indicating that these are separate categories and there's not a flow from one to the next let's build a stemandleaf plot the question says suppose you gathered data on how long it took you to get ready in the morning for 40 days you measured the amount of time between when your alarm went off and when you left the house the results are below rounded to the nearest minute and we want to build a stemandleaf plot for this for a stemandleaf plot we divide each value into its stem and its leaf and the stems are generally the tens place although you can tweak this and make it for instance the ones place but in this case since our values are two digit values the first digit will represent the stem and the second digit will represent the leaf so if you look through you'll notice that all these numbers start with either one two or three that's the the first digit so our stems could be one two or three and now we'll go through and for each value we'll place that leaf in the correct category so the first value is 35 so we'll put a 5 under the leaves category in the third row then for 28 we'll put an 8 here then 25 we'll put a 5 here then 23 and so on we'll put another 3 then 32 means we'll put a 2 in the third column then 29 19 21 13 and so on and i'll go through and fill all these out in detail i'll go through and show the final result in just a moment now what you often find with the stem and leaf plot is that we write the leaves in the order of their value rather the order in which they appear in the data set so instead of writing 8 five three three nine one we would write the one first and then the threes and the fives and so on now let's take some extra work because we have to do some sorting but when i show the results in a second it'll be shown in that way with them sorted by value now that we have all the stems and leaves written notice how it looks it looks almost like a histogram or a bar chart turned sideways where the length of each string of leaves represents the frequency of that category so it divides it into categories by tens and we can see that the most frequent category is the range from 20 minutes to 29 minutes and yet unlike a normal grouped frequency by grouping them we haven't lost any information we could reconstruct the entire data set from this stemandleaf plot if we had to in this example we'll construct a scatter plot for some data that we're given we're given the sizes of several tvs and the price that goes along with each one now our data is separated into three categories just so the table could fit easily here but side by side you'll see a size and a price where the size is given in inches and the price is given in dollars and we want to construct a scatter plot where we compare these two variables now when we construct a scatter plot we have to pick which variable will be x and which one will be y it's not crucial that we pick it in the right order because it turns out if they get switched much of the analysis we can do is still the same but if we can it would be nice to pick x and y in a reasonable way and usually we want to think about how x determines why in other words is there one of the variables that seems to control the other one would we say that the size determines the price of the tv or would we say that the price determines the size it's probably more likely that we would say that the price depends on the size or the size determines the price so we could call x the size and y the price so let's say our xaxis represents size and our yaxis represents price again we'll try to be consistent with these where the first column is x the second column is y but it turns out that if you switch the order it doesn't change too much at least at this point so now we need to put a scale on each axis so for the sizes the sizes go up to about 60 inches so let's make sure we include at least up to 60. let's say we go up to 70 here and if we start down at zero that means we need to divide this evenly so that we get to 70. so halfway there would be 35 and then if we divide these by fives every 5 10 and so on and the prices range from 200 to about 2 800. so let's say we include all the way up to 3 000. and let's mark it in increments of 500. now for each value in our list let's take the first one for instance the size is 43 inches the price is 500 so on the horizontal axis the size axis will go up to 43 which is around here between 40 and 45 and then on the price side we'll be right at 500 so we want to find out where those two cross which is right around here so our first point will be right there at 43 inches and 500 for the next one we'll go to 55 and 900 so 55 is right here between 50 and 60. 900 is right here right under a thousand so those two look like they cross right around here which gives us our second point and then we'll continue for the rest of them without showing each one in detail i'll just show the final picture here in a moment after we've drawn all these points so there's the final result and even though it's handdrawn and imperfect we can see the general trend which is that larger tvs tend to cost more so there's this general upward trend as you scan through this picture that's really the value of a scatter plot is to look for an association or a connection between two variables to see if there's a relationship and this one looks like there's this upward trend later on we'll talk about how to draw a line or some other curve that represents this shape but for now just notice that there's that relationship between the two find the median of the salaries listed below so these are the salaries from the mba data set we have 30 values and we want to find the median remember that the median is the middle data point now helpfully these salaries have been listed for us in order from smallest to largest so notice we have the smallest value here and then they increase along this row and then down to the next row until we reach the largest value at the bottom right and that's important when you're finding the median to order them from least to greatest or from greatest to least so that when we look in the middle of that list the middle value is truly in the middle between the highest and lowest so that half of the data points fall below it and half of them fall above it so since it's listed like this for us we can look halfway through since there are six rows at the end of the third row and the beginning of the fourth row we have the middle so the middle is right between this value and this value since there are two values in the middle which will happen every time we have an even number of data points we need to find the number halfway in between those two which we do by averaging them together so find the average of those two numbers in the middle that will be the median if we had an odd number of data points like say we had 29 players we would find the middle value and that would be our median we wouldn't have to find any average but anytime there's an even number there will be two in the middle so we need to find whatever's halfway between those two so the median will be halfway between four million four hundred and sixty nine thousand one hundred sixty and 4 million 767 000. so if we add those together and divide by two what we get is four million six hundred and eighteen thousand eighty and that's the median half of the players make less than four million six hundred eighteen thousand half of the players make more so that's the middle data point and it's a good measure of the center here we'll find a weighted average using a student's score in a class so we have several assignments three tests homework project and final exam and we're given the student's score on each of these assignments as well as a weight for each assignment or a number of points for them now notice that usually we'd be given either the weight or the points in this case we're given both and if you look closely there are a total of a thousand points that they could earn if you add up all those points you should get a thousand and if you divide each of those point values by a thousand you'll get the percentages that are listed here so really the same information is given in the weight column or in the points column and you may see for some of your classes the scores are given with weights sometimes they are given with points it's really the same thing it's just written in a different way so we're going to show the calculation with both just to compare but we'll get the same answer either way for part a then let's use the weights so for the weights we can multiply each score the student got by the weight that's associated with it and once we multiply those and add them all together the answer we'll get at the end is the average score so the weights are nice because they're already scaled for us so that just by multiplying the score times the weight and adding those together we'll get the final weighted average so i have to do is take 85 percent times 20 plus 92 percent times twenty percent plus eightyseven percent and so on so if we multiply all those and add them all together the answer we get at the end is about point eight nine nine which works out to 89.9 so on a standard 10point scale this student will be very close to an a just under 90 now let's do the same thing with a point system and we're going to get the same answer of course but this time we have to do two steps where first we multiply their score so for the first category they got 85 percent times the number of points they got and then do that for all of them and once we multiply and add all those up we get 899 points so they earned a total of 899 points and then when you divide that out of the total of number of points they could have earned which was a thousand they get .899 which again works out to 89.9 so notice you're doing the same work both ways it's just that when you use the weights you've already divided by a thousand before you start when you use the points you have to divide by a thousand at the end to get the final score the same way you did with the weights so either way you do it you're using the same values and this is how you do a weighted average whether you're given the weights as percentages or as points out of a total that you could earn here we'll find the mode of a data set that's summarized for us we're already given the frequency table rather than just the raw data so most of the work is done for us the mode is the most frequent data value and the nice thing about having the data given to us as a frequency table is all we have to do is look through the frequencies and find the highest frequency notice the highest frequency is four and occurs three times there are three values which occur four times and are tied for most frequent and those are 21 22 and 24 so there are actually three modes and this can happen a lot in our case the modes are 21 22 and 24. here we'll find the range of a data set that we're given and the data set is the heights of the players given in the mba data set the range is really simple it's just the difference between the smallest and the largest so all we have to do is look through this list and find the smallest number and the largest number that occur if you scan through you should be able to pick out that the lowest number is 1.78 1.78 meters is the shortest player in the sample that we listed and the tallest player is 2.13 meters so the range is just the difference between those two if we take 2.13 minus 1.78 the range is the difference or 0.35 meters that's a really simple way of measuring how spread out the data is if the range is larger it's more spread out if the range is smaller it's more tightly clustered together so 0.35 when compared to the values in this data set is a fairly small range so they're fairly tightly clustered together all of these players are relatively tall here we'll calculate the standard deviation by hand using the formula instead of using the builtin function in the calculator just to illustrate how the formula works we have five data points and we want to calculate the standard deviation the first thing we need to do is to calculate the mean so the mean remember is the sum of the data points divided by the number of data points so we add them up and divide by 5 and we find that the mean is 10.2 now the standard deviation of this data set is the square root of the sum of the squared deviations divided by n minus 1 and all that's inside the square root so we'll need to calculate these things individually we need to find each deviation the difference between each data point and the mean square those add them up divide that answer by n minus 1 and then take the square root of that answer so this can get kind of tedious which is why we only do this with small data sets and from this point on we'll use the builtin function in the calculator but here we'll just illustrate the formula once we'll use this table to organize everything we have the data values listed and we'll calculate the deviation for each one by simply subtracting this number 20 for instance minus 10.2 and then 4 minus 10.2 and 15 and so on and we'll check the deviation for each data value once we've done that we square them all and remember the reason that we square them is because if we tried to average these deviations we would get zero because the positive ones and the negative ones would cancel each other out but by squaring them we end up with all positive numbers so when we average those they don't cancel each other then here all the squared deviations are filled in now we need to add those squared deviations divide by n minus 1 or 4 in this case and then take the square root of the answer adding these all up we get 210.8 and then if we divide that by 4 we get 52.7 but we're still not done the last step is to take the square root of that the square root of 52.7 is 7.26 so the standard deviation of this data set is 7.26 or the distance that a typical data point is from the center again this example mostly illustrates why we don't calculate the standard deviation by hand usually even for a small data set it gets pretty tedious a random sample of 11 statistics students produce the following data where x is the third exam score out of 80 and y is the final exam score out of 200 here we'll use a graphing calculator to find the equation of the least squares regression line so that we can predict the final exam performance which is why based on the third test score which is x so to use the calculator we first need to enter the data if we enter the stat menu and edit we can enter the data here which i've already done the first list we've entered the x's and in the second list we've entered the y's now if we go back under the stat menu we want to calculate so we'll scroll over to the calc menu and we want linear regression the form we've been using is this number 4 reg ax plus b so a is the slope and b is the yintercept if we select that we don't need to change anything here because we entered the x's in list 1 and the y's in list 2. so nothing needs to be changed we can just scroll down to calculate and it gives us the equation the form of it is y equals ax plus b and we get the values for a and b as well as values for r squared and r and now we have a better sense of what this r represents that's the correlation coefficient so the correlation coefficient is about 0.66 which means there's a moderate linear relationship and it's positive so there we go there's our regression line and there's the r value that goes with it here we're given a regression equation which we developed in a previous example to predict the price of a house based on its square footage so if x represents the square footage we can predict the price in thousands of dollars so if we entered a thousand for the square footage for x we could calculate a value for y that would be the predicted price for houses with that square footage and we're going to use this equation to predict the price of homes with two different square footage values so all we have to do in each case is replace x with the given value for square footage in the first case we have y hat equals 0.099 times 2700 plus 160.8 which works out to 428.1 so that corresponds to 428 thousand one hundred dollars and that technically means that the average house price we would expect for all the houses that are twenty seven hundred square feet would be four hundred and twenty eight thousand dollars approximately and then in the second case we can make a similar prediction for houses with 4 500 square feet and that works out to just over six hundred thousand then the question asks which prediction do you expect to be more reliable now for this we'd really have to go back and look at the data which we don't have in front of us but if you go back in the textbook and look at the data you'll notice that in the range of houses that we have data for 2700 square feet falls within that range and 4 500 square feet does not none of the houses we have in our data set are nearly as big as 4 500 square feet so it turns out that the first one is more likely to be reliable without external information for all we know that now it seems more likely that the first prediction would be more reliable because we've seen houses that are similar to it in our data set the first example we call interpolation where we are predicting within our data range the second one we call extrapolation where we're predicting beyond our data range and in general extrapolation is dangerous because we don't know quite what could be happening beyond the range of data we've actually looked at so in general interpolation is more reliable than extrapolation so the first prediction is more likely to be reliable even though we don't have a lot of information about it at the moment here we'll do a linear regression problem with several steps first we're given some data which is a set of quarterbacks in the nfl during the 2019 season and for each quarterback we're given their height and their weight we're going to compare these two now without any other information at the moment we're going to assume that height is going to represent x and weight will represent y just because they're ordered that way later on in the problem we might have more information that'll tell us which one should be x and which one should be y but for now we'll just assume that and change if necessary so first we want to calculate the correlation coefficient and to do this we'll use the calculator so first we need to go to the calculator and enter this data to enter the data we'll hit the stat button and hit enter to get into the edit menu and under list one we'll enter the x values so the heights that we saw the first quarterback had a height of 75 inches then 74 74 70 71 74 76 77 72 and 74. and then we can scroll over to the second list for their weight and we'll enter these as well once we have all the data entered we can go back into the stat menu and scroll over to the calc down to the linear regression option again we've entered x's in list one and y's and list two so we don't need to change anything if those were switched it wouldn't change the correlation coefficient anyway so for this first part it didn't really matter which way we entered them but later on it will be significant so we'll just hit calculate and it goes ahead and gives us the equation for the line which we don't need just yet all we're looking for at the moment is the value of r which is about 0.6 the first part of the question just asked for the value of r so now that we have that we can go back to the notes and enter that now for the second part of the question we want to know is there a strong linear relationship so based on the value of r we know first of all that there is a positive relationship since r is positive and we wouldn't necessarily say it's a strong relationship because it's less than 0.8 which again is not a magical number but it's sort of an agreed upon level for a strong relationship but 0.6 we might say there is a moderate relationship and often a moderate relationship is good enough to continue on and we'll continue to do the rest of the problem so we wouldn't necessarily say it's a strong relationship but it is a moderate linear relationship the next part of the question asks for the regression line which we've already calculated now notice the direction here the regression line is going to predict the weight from the height in other words the weight is going to depend on height which means that weight should be y and height should be x which is the way that we already set it up so it's good that we did so and the regression line that we got from the calculator earlier is the one that we'll write here so if we go back to the calculator we can see what we had there we have a is about 3.01 and b is negative 4.43 so now that we have that we can move on to the rest of the problem here we want to graph the data as well as the regression equation and i'll actually use the calculator to do this we'll graph both the data and the equation on the same window we already have the data entered so if we turn on the stat plot under second y equals we turn on this first stat plot and leave it as a scatter plot the way it is x and y are in the right order now when we graph we'll see those points as long as we're scaled to the right window and then also if we hit y equals we can graph the equation which was 3.01 x minus 4.43 now before we graph we should go to the window and make sure that our x values and y values are in the right range the x values the heights range from 70 up to 77 so we should make sure that x covers at least that range so let's say we make x min 68 and x max 79 and then the y values the weights range from 200 up to 233 so let's say we go from 190 to 240. when we graph this we see those heights as well as the line that passes through them now notice what this graph gives us first of all it points out that outlier on the lower right hand side and that's the first entry lamar jackson is on the upper end of the height scale he's at 75 inches he's one of the taller players but he's actually the lightest of all of them at 200 pounds so if we actually removed him from the dataset our r value will be much much higher because he's what's making the data points not follow a straight line since he's kind of out in space by himself that messes up some of the strength of the linear trend that would be there otherwise so sometimes there are outliers like this that will lower the value of r but if you remove them the value of r will be better of course you can't just throw away outliers because your data would look better without them but keep that in mind next we can predict the weight of a quarterback who is 73 inches tall so given a height we can predict their weight using this equation so a is 3.01 and then if we plug in 73 for x we can predict that y hat their weight for a quarterback of this height would be about 215 pounds lastly we get this question does drew brees weigh more or less than the weight predicted by the regression line based on his height so we can do the same thing we did in the last part where we predict what someone who is the height of drew brees 72 inches would tend to weigh according to this equation according to this equation that comes out to 212 pounds approximately so the question asks does he weigh more or less than the predicted value the predicted value is 212 pounds and he only weighs 209 pounds so he weighs less than what's predicted so here are the answers to all the parts of this question we found the correlation coefficient we interpreted it we found the regression line and graphed it alongside the data and then we made a couple of predictions based on that regression line the scores in a college entrance exam are normally distributed with a mean of 52 points and a standard deviation of 11 points and we're asked to find what two scores encompass 95 percent of the test takers since the data is normally distributed the empirical rule tells us that 95 of the data will be within two standard deviations of the mean since one standard deviation is 11 points two standard deviations is 22 points twice that so 52 minus that and 52 plus that will form the boundaries of this range that includes 95 percent of the data therefore we decide that 95 of the values lie between 30 and 74. iq scores are normally distributed with a mean of 100 and a standard deviation of 15. here we're asked to use the empirical rule to find the data that is within one two and three standard deviations of the mean remember the empirical rule states that sixtyeight percent of the data is within one standard deviation of the mean meaning that if we go one standard deviation below the mean and one standard deviation above the mean that range will hold 68 of the data 95 percent of the data falls within two standard deviations and almost all the data or around 99.7 percent of the data falls within three standard deviations so if we work from the mean down three standard deviations and up three standard deviations will encompass almost all the data in this case with a mean of 100 and a standard deviation of 15 one standard deviation below will be 85 and one standard deviation above will be 115 100 minus 1500 plus 15. so 68 of the data in other words 68 of people will have an iq score between 85 and 115. 95 of people will have an iq score between 70 and 130 and almost everyone will have an iq score of between 55 and 145. here we have a picture that illustrates this situation where each unit on the axis is one standard deviation suppose you know that the prices paid for cars are normally distributed with a mean of seventeen thousand and a standard deviation of five hundred we want to use the 68 95 99.7 rule or the empirical rule to find the percentage of buyers who paid in any given range the first thing to do when working with a problem like this is to draw a picture and here's the picture for this example the center is the mean and then each unit on the axis is the standard deviation so we have it centered at seventeen thousand we go up to seventeen thousand five hundred eighteen thousand eighteen thousand five hundred and we could keep going but that's all we need and then on the lower side we go down to sixteen thousand five hundred sixteen thousand and fifteen thousand five hundred here i've also filled in all the percentages for each range notice that the empirical rule tells us that 68 of the data falls within one standard deviation of the mean since everything is symmetric each half of that holds half of that 68 percent and that's where those 34 numbers came from then i know that within two standard deviations of the mean i have 95 percent of the data if i have 68 in the middle and by going out another standard deviation i get the 95 percent that means the yellow regions together must make up that 27 percent that gets us from 68 to 95 so if the two yellow regions together hold 27 percent each of them holds half of that or 13.5 percent we can repeat this process for the green regions again going out to a third standard deviation we know that that holds 99.7 percent of the data so the green regions together must hold 4.7 percent of the data since everything up through the yellow regions held 95 and then just by adding the two green regions we got 99.7 the green regions must be that 4.7 percent therefore each of them holds half of that or 2.35 percent and then outside the 99.7 is 0.3 percent of the data and again because it's symmetric each half of that or each tail contains 0.15 percent again the goal is not to memorize these percentages but just to realize how we got them and be able to rederive them at any point but now that we have them we can use them to solve the problem so part a asks what percentage of buyers paid between sixteen thousand five hundred and seventeen thousand five hundred so we find those two points on our picture and between them we add up the blocks and find that sixtyeight percent of buyers were in that area similarly for part b between seventeen thousand five hundred and eighteen thousand we locate those points and between them there's just one region with thirteen point five percent for part c we look between sixteen thousand and seventeen thousand and there's two blocks there adding them up we get forty seven point five percent for part d we're looking between sixteen thousand five hundred and eighteen thousand and adding up those three blocks we get eighty one point five percent so eighty one point five percent of people paid somewhere between sixteen thousand five hundred and eighteen thousand dollars for their car part e asks what percentage paid below sixteen thousand so that's right here and below sixteen thousand we have two blocks to take care of two point three five percent and point one five percent adding them together we get 2.5 percent so 2.5 percent of buyers paid less than 16 000 for their car lastly the final part asks what percentage paid above 18 500 and again there's only one block up there so 0.15 percent of buyers paid above that amount and here we have all the answers summarized here we're told that female adult height in some population is normally distributed with a mean of 65 inches and a standard deviation of 3.5 inches and we're asked to find the zscores of two heights remember that a zscore is the data value minus the mean divided by the standard deviation and this z score represents how many standard deviations this data point is above or below the mean if the zscore is positive it's above the mean if the zscore is negative it's below the mean for the first example then to calculate z we take 58 minus the mean 65 and divide by the standard deviation 3.5 58 minus 65 is negative 7 and that divided by 3.5 is negative 2. so what that means is that first data value is two standard deviations below the mean for the second point we do the same thing now taking 71 minus 65 and dividing by 3.5 71 minus 65 is 6 and that divided by 3.5 is 1.71 so the second data point is between 1 and 2 standard deviations above the mean scores on an iq test are normally distributed with a mean of 100 and a standard deviation of 15. we're asked to find the iq score that corresponds to each of the following zscores so here we're given z values and we're asked to work backward from them to the data values again remember though that a zscore is simply a given data value minus the mean divided by the standard deviation here we're given the zscore the mean and the standard deviation and asked to solve for the data value for the first one for example we know that z is negative 1.5 the data value is the unknown part that we're going to find so the zscore is x minus mean 100 divided by the standard deviation of 15. now to solve for x i'm going to multiply both sides by 15 and we find that negative 22.5 equals x minus 100 then add 100 to both sides to get x equals 77.5 so a zscore of negative 1.5 corresponds to an iq score of 77.5 for part b we do the same process the zscore equals the data value minus the mean divided by the standard deviation so we multiply both sides by 15 and find that 30.75 equals x minus 100 and then adding 100 to both sides x equals 130.75 so a z score of 2.05 or just over two standard deviations above the mean corresponds to an iq score of around 130. this question asks what is the margin of error on a poll with a sample size of a thousand people with the simplifying assumptions that we've made the margin of error is one divided by the square root of the sample size as a percentage so this will give us a decimal and then we'll write it as a percentage to do that some people write that this times a hundred percent is the margin of error and that times 100 percent just converts that decimal into a percentage again there's some simplifying assumptions that lie behind this but for our purposes this is good enough in this example n is a thousand so the margin of error is one divided by the square root of a thousand times a hundred percent one divided by the square root of a thousand is point zero three one six etcetera and by multiplying by a hundred percent or converting to a percentage gives us three point one six percent approximately it's fairly common for a pole to have a margin of error of around three percent and when you see that you can tell the sample size is around a thousand people in this example we're going to find a sample size that corresponds to a given margin of error so if we want a poll to have a margin of error of 2 percent or less what's the minimum sample size needed to make that happen remember the margin of error looks like 1 over the square root of n as a percentage so we can write times percent just to indicate that we're making it a percentage so if we want this to equal two percent then really what we want is we want one over the square root of n to equal zero point zero two so as a decimal we want one over the square root of n to equal point zero two or as a percentage we want it to equal two percent now we need to solve for n and it takes a little bit of algebra to do so there are a few ways to observe this one if we want to get n by itself we could move it out of the denominator by multiplying it on both sides so if we multiply it over there we get 1 equals 0.02 times the square root of n and then again we're trying to get this part by itself so we'll divide both sides by .02 which gives us 1 over 0.02 equals the square root of n now there's a shortcut to this at this step you could say we have 1 over the square root of n equals 0.02 you can actually flip both sides of that equation upside down and get square root of n over 1 equals 1 over .02 which gets you to the same place that we already did just by a shortcut don't get too lost in that step if that doesn't make sense but there are shortcuts to some of this algebra once we get to this point though we just have one step left which is that we need to square both sides now before i do that i'm going to simplify 1 divided by 0.02 and that simplifies to 50 but now to get rid of that square root we just need to square both sides and the square and the square root will cancel each other when we square 50 we get 2500 equals n so that's our answer that if we sampled at least 2500 people we would be guaranteed to have a margin of error of 2 or less and if we sampled more than 2500 our margin of error would be less than 2 percent but as long as we sample 2500 will be guaranteed to have a margin of error of 2 percent