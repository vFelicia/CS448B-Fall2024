what is sqlite it's different from most other database engines it's it's a library it's not a system it's not a process if you've done if you've done a lot of database if you've worked with any other database it's probably a system it's running in the data center somewhere sql is a library that just links into your application it's a very different thing it's what it's delivered as a single file of ansi c code now this is a large file and we don't actually develop by editing that one big file we have hundreds of files and then part of our make process kind of concatenates them all together in the right order but that makes it very easy to deploy it compiles down to roughly 500 kilobytes it's very small and compact small footprint low dependencies it's designed to run on embedded systems i've i've got here uh the minimum set of dependencies i mean just standard library things that's all it is you don't have to import a bunch of other libraries to get this thing to work so it can work on embedded systems a complete database file is stored a complete database is stored in on disk as a single file which is an interesting thing most other database systems store the content in a directory full of files and in a lot of systems that directory is known only to the system administrator it's some weird place that nobody entered where is but with sqlite the entire database is just an ordinary file and that means you can take that file and put it on a flash drive or email it to a colleague it's really easy to move around it's a full featured sql implementation with you know things like um common table expressions partial indices full text search archery indices it features power safe serializable transactions by power safe i mean you can be in the middle of a transaction or in the middle of doing a commit and the system will lose power and you know under the assumption that the hardware behaves as advertised which is not always an accurate assumption but assuming that hardware does what it's supposed to do you won't lose any work very simple api easy to program to it's designed to be able to drop in it's designed for for application developers to be able to take this code drop it into their application and have a full featured database with no other work um and the source code is in the public domain so you can grab the source code and do whatever you want with it you can grab a copy of the source code relabel it andydb and go with it i mean that's whatever you want to do it's called sqlite people think oh it's just a little tiny toy database it does have its limits but they're pretty big we support multiple uh concurrent riders and one reader or what multiple concurrent readers and one rider all the same time now you know that's not huge but that but that's usually enough for your embedded device uh we take strings and blobs up to a gigabyte in size which is actually more than a lot of largescale databases we'll do a single database can be up to 140 terabytes we've never actually tested that limit because we've never actually come up with a file system that could give us a 140 terabyte file but that's the theoretical limit 64way joins 2000 columns per table there aren't any real arbitrary limits other than these it it's actually a fullfeatured database engine so what is the forcing function for 140 terabytes what limits it uh the the limiting factor on the size of the database file 140 terabytes is that it's it the the we use a 32bit integer to count uh the pages actually it's a signed integer so we have to leave the top bit off and so we have 31 bits and the maximum page size is 64k so you do the math so um how did this get started this was um i'm not you know i didn't start out as a database person i was doing some application development uh solving some hard problems and i was doing these client programs that were doing a really interesting theoretical calculation and but i had to get my data from a database engine and the client that i was working for provided the database engine and it was informix and that hey it worked great when it worked but sometimes they would power cycle the database or power cycle the machine and the database engine wouldn't come back up and when that would happen that would mean that my application couldn't do its job because it couldn't read the data because it had to talk to the database engine and so my application would bring up that dialog box and that's what an error dialog box actually looked like in the late 1990s that's an actual screenshot they look better now don't they yeah a little bit grainy yeah yeah but you know back in you know back in 1998 we thought this was so cool this was so state of the art you know do you remember that um so i had this idea well look uh this particular application it's not doing a lot of heavy transaction stuff in fact it's read only and it's not doing any elaborate joins or queries why can't i just read this data directly from the disk why do i need this server sitting in between me and my data which is just another potential point of failure so after that project was finished i you know i had a couple months off and i wrote sqlite and that was in that started in 2000 and in may of 2000 was first code and the first release was a few months later so that's how it got started and that sort of shows you the motivation behind it so it's a little bit different from what you're kind of used to seeing it has different use cases sqlite is not trying to to replace all these other databases that you're more familiar with it sqlite ends up being very useful in embedded devices in the internet of things it's in your cell phone it's in you know the you know the smart thermostats it's in your microwave oven it's in your tv set these sorts of things don't need to be doing a bazillion transactions how many such devices is it uh it's hard to count because it is open because it's public domain a lot of people don't tell us so for and you know until a few minutes ago i did not know that sqlite was flying on the space station you missed the intro no sorry you missed the internet so um so yeah aol was one an early adopter do you remember back in early in the early part of this millennium where you know you get the aol install cds you know ten dollars a month or something do you remember those i never was the aol sqlite was always on those very few people so um but sqlite is also good as an application file format if you're doing a traditional desktop application of some sort and instead of doing file save and writing out a bunch of xml make it a database you get all this powerful query language and and and transactions uh it's great for uh what i call the lingua franca of a federation of programs you got a bunch of graduate students working on a problem you know and you're writing in python and you over here you're doing c plus plus and you're doing ruby and this guy over here is doing php oh you're doing lisp you know all these guys are doing different programs because that's what they want to do but they all got to talk to each other why not use an sqlite database file as your means of your common mode of communication um it's often used as a local cache for an enterprise database so you're on a device you want to download data that's relevant to the device so that the device can continue to operate while you're off network you're you're on your phone you're going through a tunnel and you've lost reception you're off network for a while and it works well it's also good as an interchange format so for example um you know the uh the the the program guide on your cable tv and it comes down to the set top box in a lot of cases it's being being bound from the satellite to your setup box as an sqlite database and then you know and then the little the window that shows you what that's that's just an archery query so uh here's your decision checklist about when to use or what data storage engine to use for your project is the data remote if the data is on a different device from the one that your program is application is running on use one of the traditional client server databases if it's big data if it's more data than you're comfortable putting in a single file use one of the traditional client server database you've got concurrent writers if you're trying to do a gazillion transactions per second use a client server database that's what they're for and these are all very important problems and these client server databases are all very good at solving them but there's a lot of problems that don't fit any of those categories and for all those other cases just use sqlite and where people mess this up is that they they do the first part of that checklist right and they get down to the bottom says oh well i don't have any of these problems i'm just going to open a file and write a bunch of json into it or some binary format that i made up and this happens a lot and that's the use case for sqlite sqlite is not really competing against these other database engines that you study all the time it's competing with f open that's it's that's its goal um so sqlite is found in lots of different things as andy was saying it's in all the phones it's in your mac it's in your windows 10 machine it's in the web browser it's in lots of applications it's built into a lot of programming languages and it's just appearing in devices all over we we're pretty sure we can't measure this because it is public domain but we're pretty sure it is the most widely used database engine in the world i think that it is probably the number two most used piece of software in the world i'm thinking that number one is the zlide compression library it's probably more deployments than sqlite but other than that i'm not aware of anything that does more okay i've got one marketing slot actually two this is the first of two mark i got a question in the back what percentage of the code base deals with the sql part i've got a slide coming up so hold the question okay so this is my i've got two marketing slides i just want to point you this is a this is a graph of apple's stock price from there when they originally went public in 1981 up until about 2012 and from there it's gone up uh uh this is before the split they had an 81 split but adjusted for the split it would be now at about a thousand and you can notice how the stock stayed around 10 or 15 dollars per share for over 20 years and then it suddenly started this rise up to a thousand okay absolutely true sqlite was introduced into the mac platform there okay i'm just saying i'm just saying but did you buy a bunch of apple stock then unfortunately no um so so what i'm going to talk about here is i'm going to go over how sqlite is implemented because you're all database people i assume and even if you're not even if you're working on you know one of these systems that does a gazillion transactions per second and that's really not what you're into it's an important database and you need to understand it and so i'm going to talk i'm going to give you an overview of the implementation so that you understand what it's doing and so that if you want to go and look at the code you've got kind of a road map the code is available online here's how you get it there's two places that you can get it and it's readable code we put a lot of comments in the code and the readable comments we get we see tweets about how oh you should reask your light code it's really easy so you can actually study this my goal is to give you a road map so that when you just pull a random bit of code out you you have some inkling of what it's trying to do um so any database engine i like to think of them in terms of a compiler and a virtual machine so you've got sql that comes in and you've got a part of the program that's going to compile part of the database engine that's going to compile the sql into a a prepared statement this is um uh i think of think of every statement of sql as a miniature program and the prepared statement is the executable and so this is just like this is like gcc and then you get a prepared statement and then you run the prepared statement that's like just executing the binary that's the way we like to think of it this is the stack of sqlite there's a parser code generator virtual machine b3 layer pager and an os interface i'm going to talk about each of these in turn uh at the top there's a parser it's a standard kind of parser we did the the tokenizer is written by hand it's only a couple hundred lines of code you know when you're when you're studying compiler construction everybody you have these big chapters about you know lex and lectures and stuff and they use a lot i've never understood that because it is so so easy to write a tokenizer and a couple hundred lines of c code that is at least two orders of magnitude faster than anything flex will ever generate so i i don't know why they do that but never heard anybody writing their own parser by hand that's crazy no no the the this just the tokenizer this is just splitting it up into tokens now the parser is a traditional la lr1 parser it doesn't use yak i wrote my own partial generator when i was a graduate student okay and and it has the advantage of react that it generates a partial that is reentrant and thread safe and back back when i was doing this bison and yak parsers wouldn't were neither of these things they may have fixed that now i don't know i haven't kept up with it so but it's a traditional parser these are the files where you can find this stuff if you if you want to look at the code the the source code to the lemon parser generator is included in the source tree and documentation is included in the source tree so you can learn about lemon and then the the structures that define the asset abstract syntax tree are in that header file and the tokenizers in that file up there so moving on down further we have the code generator which does semantic analysis of the parsed code transforms the parse tree and ways to try to make things more efficient does query planning and then generates byte code the output of this is the prepared statement so these two things implement the compiler think of these two steps as gcc they take raw program text and turn it into something the machine can understand and of course the rest of the stack is going to be implementing the machine um so the virtual machine in sqlite is it's a it's a bytecode interpreter a lot of other database engines they will um they just walk the parse tree and that's how they execute but i wanted to do a bytecode interpreter the original bytecode interpreter was a stack base where you push things onto a stack and then operate on the stack just like jvm and all the others seems like every virtual machine always starts as a stack based machine but we changed it to a three address machine because that actually turns out to be more efficient and much easier to write optimal code for so it's really simple it's a big for loop program counter equals zero you know program counter plus plus and inside the floor there's a switch and switches on the op code and there's a case for every op code uh and part of the virtual machine i also include the implementation of the builtin sql functions so they're included there talk more about that later so here's an example of what the byte code looks like i won't walk you through this but you can you can look at the bytecode for any sql statement that sql lite can generate by just putting explain up front like this and the the documentation for the op codes is available online if you want to want to try and decode that this is doing a full table scan so this one's pretty simple and i could fit on one slide for a join with sub queries and lots of conditions this might go on for hundreds or even thousands of instructions but a simple table scan it pretty quick if you want to to study this you want to look at what the bytecode sqlite generates is let me tell you how to do that you get you need you need to do a custom custom build and so to get the tar i'll get the source code do configure and i like to do disabled shared because auto comp if you don't do that it does all this freaky shell script stuff to do shared libraries and and you can do that if you want to it'll work but i find finding confusing so i always disable it and when you before you do make there's an extra c pre processor to find that you need to give it and then that's going to um add these comments i'm going to go back a slide over here on the far right we've got comments that help explain what each op code is doing and and by default those are not generated because they take up space and it takes cpu cycles to generate them so in a production environment we don't want to do that but if you're debugging they're very useful to help you quickly see what's going on and so you'll probably want to include those so once you get that thing compiled what you've compiled then is a command line shell it's just a simple program that reads sql statements and then sends them to sqlite to be executed and this this command line shell sqlite3 uh if you if you give it a line that starts with a dot period that's special the shell doesn't send that to sqlite it does some special processing and the dot explain sets up the output formatting so that you have nice neat columns and it automatically indents loops so that you can spot the loops more easily and so that's a nice thing you want to do so you want to be sure and type that and then you just type in explain and then the rest of your query and you'll get to see the bytecode so moving on down the stack this is kind of the boundary between to the storage engine and you know a lot of places that go and i talk to people and about databases and when i say databases to them the storage engine is the database this is their focus how can i get as many writes to disk as possible my view is a little different i think that this whole stack is the database and and the bottom part is just the storage engine and if you've got just a key value pair type thing you only have half a database that's my opinion really in my view the interesting stuff is happening up above all the query planning and the the the analysis of the declarative programming language that's where the interesting parts are the bottom is just a storage engine now there is a a reasonably clean interface for this in sqlite and what a lot of people have done including some people who will be speaking to you in the in this lecture series is um they have taken the default sqlite storage engine out just stripped it out and plugged in their own so then they've got an sql system on top and then they put their own whizbang storage system underneath because you see in a minute my storage engine is is not like cool and has the latest algorithms it's it's it's old tech it's old school so um the bee tree layer uh we support b trees and b plus trees there's multiple b trees per disk file one b tree for table for each table one b tree for each index variable length records uses a very efficient coding mechanism um it's accessed via cursor so when you're working with b trees you open a cursor into a into a b tree and then you can seek the cursor and advance the cursor backwards and forwards and it allows for multiple cursors to be open on the same b tree at the same time and including cursors that are changing the b tree and and it takes care automatically that people don't write things out from underneath each other i'm going to go back over this is just the i don't know if i made clear this is just sort of the 30 000 foot view i'm going to go back over all of this in more depth after i finish the quick overview next on down is the pager layer and this is the part that implements your transactions your atomic commit and your rollback the pager views the database as a bunch of pages then they're numbered starting with one because page zero is like our null pointer and the page size is a power of two between 512 and 64k the pager has no idea what what the content of the page is it doesn't care it's just managing the pages handing them to the b tree and then dealing with transactions it also provides concurrency control because with sqlite you can have multiple processes talking to the same database file at the same time and there's not a server controlling this they're all peers and and so there's got to be a mechanism to make sure they don't step on each other and and that's handled by the pager layer and then at the bottom we have the operating system interface this is the portability layer this is how we allow sqlite to operate on windows on mac on a various embedded database or various embedded operating systems including some custom operating systems uh this is you can plug in new os interfaces at runtime which is an interesting feature which i'll talk about in the next slide and you know i say an os interface uh people have substituted we have an example of this an os interface that talks directly to hardware bypasses the operating system completely so you can actually buy commercial off the shelf devices little gadgets that are using sqlite and they plug in their own os interface that talks directly to the flash controller and they use sqlite as their file system they have no file system on the device that you only have the database interesting concept now because it's runtime plugable we have this concept above shim where you can you can plug in your own os interface that doesn't really do a complete os interface it just uh maybe changes the the calls around a little bit and then passes it on down to the real os interface and this could do things like you could add encryption or compression you could do logging we use this a lot for testing because we can plug in a shim that can simulate hardware failures and that we can we can prove that sqlite is going to be able to recover gracefully from a hardware failure and there's some examples uh implementations of this sort of thing if you want to play around with it so uh there was a question in the back of what the percentage was here is the graph here is the chart uh the parser is the little green this is source code uh if you looked at compiled binary the ratios are going to be a little bit different but it's this is roughly the same um and in particular the parts are going to grow a little bit because uh the la lr1 language is a very compact notation but um not a whole lot and then the the code generator is the bulk of it coach rendering and the virtual machine and the partial together are over half of it the b tree layer is this thin little slice right here it's really not that much um so that is that is the 30 000 foot view of sqlite now what i'm going to do now my plan is to go back through this whole thing again but this time from the bottom up and get into a little bit more detail about how things work so that you better understand what sqlite is doing behind the scenes so let's start with the pager again this is what handles power save transactions and currency control this is the thing that makes sure that you can roll back your transactions or that if you crash in the middle of a transaction your database is consistent that transactions are atomic across power failures it also handles current control and provides an inmemory cache for the disk controller so when you start out you're getting ready to read the database here's a little diagram i've got i guess uh it's labeled disk now it means nand flash right because i don't think i even own a computer that has spinning rust anymore if they're all but but you know what i mean so all the data is on disk there's an os cache an operating system cache but it's empty right now there's no content there the cache is cold and you want to read from the database and so the first thing you have to do is get a shared lock and that's drawn on the in ram because you know you think about these locks they don't really persist if the system crashes all the locks go away so you get a shared lock and that prevents other processes from from changing the data out from under you while you're trying to read it and then you read a few pages that you need in order to do your work and everybody's happy now suppose you want to make some changes you want to change the data but you want to write you want to insert some data the first thing first thing it does is it gets a reserve lock on the database file which says you know what i'm getting ready to write nobody and we can only have one writer at a time i i call dibs it's not writing yet other people can continue to read but this guy has dibs he's he's claiming the right to the the reservation to make a right and once he does that then he stores the um original unchanged content of the database files who's going to database pages is going to change in a rollback journal and this is a file in the same directory as the original database but with the name dash journal appended it's a rollback journal this should be familiar to all of you i'm just giving you details and then after he's done that he's free to make changes to the individual pages in user space nobody else can see this yet other processes continue to read the old original data now we're ready to commit so the first thing we have to do is fsync or flush the rollback journal to disk this is important because if you lose power that stuff's got to be there in order to recover um sometimes people don't care about recovering after power failure in which case you can turn that step off and you know what to a first approximation this step of forcing it out to disk is what takes all of the time everything else is free this is what costs time so you can turn that off and you'll make the things run a whole lot faster many many many times faster but if you lose power and and the rights to hardware occurred out of order it could corrupt your database so anyway it flushes it out to memory then it gets an exclusive lock on the database file which prevents anybody from being able to read the database file because we're getting we're getting ready to write on the database and we can't write to it while somebody else is reading because that would read it out from underneath them so then we write to the we do the right system calls and then we have to flush that out to disk with another fsync and then the the moment of commit we delete the rollback journal or maybe we we truncated or set it to zero or what somehow we make the rollback journal unusable and that's what causes the commit to occur before this point if we lose power the rollback journal is always sitting there so here we here we've lost power and in the middle of writing to the dis database file we didn't get the complete write done we've lost power and now power's been restored we're coming back up and somebody gets ready to read and they get the shared lock and they immediately notice oh we've got a hot journal over here it's a journal that didn't get processed correctly so it immediately goes to an exclusive lock and rolls the whole thing back restoring the database to its original condition so this will always happen until you delete the rollback journal so the delete is when the transaction commits that's rollback mode that's the default that's the most reliable but it's it's kind of slow it only it you can have multiple readers or one rider but you can't have a reader and a writer going at the same time so we have another way of doing this called um right ahead log mode this is not the default and i'll get i'll explain why this is not the default in a second but with the righthand log mode it starts at the same you get a shared lock on the database file you read it into user space but you go ahead it's okay to go ahead and make changes in user space right away you don't have to log the old original content and you don't have to upgrade your lot from shared other people can continue to read and then when you want to commit you just write the changes out to a write ahead log which is the name of the database file with minus wall appended and you're done you didn't have to fsync you're finished this is not durable i'll get to that but it is but it is atomic and i've got a little dot here in the last record of course all of these pages have a header which says which page number it is and there's a check sum and some other things and one of them is marked oh this is the last record of the commit so now i know the transaction's finished so that's great and then another process can come along and it wants to read too and it's reading some of the pages of some fresh pages off the database that the other guy didn't touch but it also wants to read one of the pages that the the first user has changed and it has to read that change out of the writing head log you see so this this page came out of the write ahead log so that it reflects the change whereas these others were unchanged as it read them directly from the database file so you can see that we have some readers that were reading completely out of the database they're looking at a snapshot in history whereas this guy's looking at what the current version is and and then this guy might want to make some changes and then he just appends to the log as well and so you can have multiple readers going at the same time looking at different reading from different points of the right head log in the database file and have snapshot isolation um the the reason that we don't do this by default is that when you're trying to get a page you have to know if if um that page is first in the right ahead log before you read it from the database and the way we do that is that we have a little hash table that's in shared memory but if the hash table's in shared memory and you have multiple processes trying to access the database and it's on a network file system and on their different computers that's not going to work and so for the other scheme works fine if you've got processes on separate computers accessing it over network files this one does not it also does not work on some operating systems that have dodgy memory mapping because we use memory mapping for the shared memory so um uh not on tape it's once that you've heard of actually um they they claim to have fixed it we've reported the bugs and i think they may have fixed it but whatever uh so so this is an option a lot of people use it and so and if you're using firefox if you're using an iphone if you're using android they normally enable this because this works a lot better but it's just an option oh wait but you know but this wall file can grow without bound at some point you've got to get this data back into the the um the original database file so we have a checkpoint operation that runs automatically or you can you can set up a separate thread or separate process to run checkpoints but if you don't do that it'll do it automatically when the wall file gets big enough and to do a checkpoint the first thing we have to do is make sure the wall gets persisted to disk this gives you durability and you can also set it up so that it automatically does this fsync after every transaction if durability is important to you turns out most applications if you lose power uh and you and and you lose if you lost power when you come back up if you miss the last three transactions most people don't care they'd rather have the performance but if you if you really have to have the durability you can set it up that it flushes the disk after every transaction so then periodically we will take the content of the wall file roll it back onto the disk and that is um and then we truncate the wall file and start over and that's a checkpoint operation question in the back of the room in the situation described with multiple computers reading over nfs or something like that um how do you do it you have to make sure you have to think a lot today the question is how does how do we handle multiple computers reading the same database file over like nfs yeah the whatever your network file system has to support posix advisory locking or the windows equivalent and if it doesn't and a lot of them don't you run the risk of some corruption put into the database file we no the the locking is posix advisory actually we have multiple you know we got the plugable os layer you can you can there's a different os layer that will substitute creating dot files in place of posix advisory locking for cases where but but that's a lot heavier slower and if you crash in the middle the the lock doesn't automatically go away so you've got to go back and clean it up so either way i'm running behind i'm running badly behind so i'm going to go faster so that's kind of the the discussion of the pager we didn't talk about nested transactions pluggable page cache or how we test this thing for crashes how do you know that this really works how do you know that this is really going to recover on a power failure that's an interesting problem i haven't talked about it uh v3 layer is next multiple yes about the granularity so so it's like all the transactions are in units that are the full page size yes is that a problem for any of your users that they say oh i'm only changing a little little uh you know so the question is uh when when we're logging in either the rollback journal or in the right ahead log we're logging complete pages rather than just the change and i know that some other like berkeley b just does the change and uh we benchmarked it it's not a big performance hit and it sure makes things a whole lot simpler and more reliable if you've got the complete page there rather than just the change so i'm you know i the berkeley db people will be here in a few weeks and you can ask them they might have a different they might have a different opinion of this i don't know um so in the b tree layer we do there's multiple b trees per file we use b plus trees for tables with a 64bit integer key and regular b trees for indexes a table that looks like this it's a it's a key with arbitrary data the format of the data the b3 doesn't know what what that data is it's just binary to it the format is actually interpreted by the next layer up and of course you've got a root page and it points to child pages by pointer i mean it's just a page number it's a 32bit page number and all the data and it's a bplus tree so all the data is in the leaves of the tree the keys can appear more than once in a b plus tree but um because they're small integers it's not a problem and we because our b tree is used for the tables it's optimized for a pending rather than for arbitrary inserts oh uh i did want to point it amazing fan out uh you know uh because i'm only showing a fan out of three in this simple diagram but you're really paying out on the order of a thousand so it works rest we use variable length integers and this is the variable length integer encoding where um it just it reads bytes that have zero in the high order bit and uses the seven lower bits or it uses the entire ninth byte in order to construct your integers and this was a mistake and i give you this this is this is a failure so if you're ever doing something like this and you need variable length integers don't do them this way instead do them like this where the first byte tells you you know the magnitude of the integer somehow or another so here's here's an example where if the first byte is less 240 or less than that's just the value if it's between 241 and 248 there's some little formula that gives you larger values and so forth if the first body is 255 then it just takes the next eight bytes and that's your your value and the reason for doing it this way is when the first body determines the size of the variable linked integer which is very important for efficiency and parsing and the other thing is that you can actually compare two integers using mem compare without having to decode them okay so this was a mistake always do it the way i did it was a mistake always do it this way the other thing i want to point out is how the pages are laid out there's a header on each page and then i have a section in here which are two byte offsets to each row within that page and then down over here i have all the the rows i did this backwards as well if you ever are doing a b tree let me suggest that you flip it around the other way put the header at the end and the pointers to the offset before that and then the content here and the reason for this is is you've got variable length rows in here and if you're having to parse this stuff out and and it could potentially have been handed a corrupt database file because remember sqlite is used to pass information around on sticks and stuff somebody could have deliberately given you a corrupted database file in attempting to crash your system so when we're parsing this and we're doing these variable length fields we don't want to do a buffer over run and the way i've got it now because the content area goes all the way to the end of the page i have to be very very precise in making sure i don't overrun the buffer whereas if i had done it the other way around and put this header and this other stuff at the beginning i've got kind of an overrun area so a little bit of slop and it you could save a lot of performance that way just some hints okay um it's not so much for performance it's for safety right it's it's it's the question um it's for performance in an environment where we want to guarantee safety because sqlite right now has to spend a lot of cpu cycles making sure that we never overrun that buffer when in fact in practice you never do unless somebody's trying to break into your program so that's those are essentially wasted cycles okay so um indexes uh are stored as regular b trees and we think we treat an index as just the key there's no data on an index it's just a key and it's binary data the b tree doesn't know how to sort these things because it's binary but the the next layer up hands at a point or a comparison function that allows it to sort these and we'll talk about in a minute b trees there's no there's no um uh the the data is distributed throughout the tree or the keys are distributed throughout the key the key is the data but the keys are not duplicated remember in a b plus three the keys are duplicated here it's not duplicated there's only one key one instance of the key in the table for each one which is important because now the keys are much larger yup but you have reduced fan out so searching takes longer because the keys are larger fewer of them fit on one page it doesn't fan out as fast search takes a little bit longer we've got sqlite set up so that there's always a minimum of four keys on every page so i'm going to skip that slide in the interest of time now we've got a bunch of b trees in the same file and these these individual pages can be interleaved all through the file and the only thing that you need to know is what the root page of each b tree is um if you want to see where the pages are you can download the the source code do configure do make show db show dv is a little utility that we wrote that kind of decodes the file format and they do show db database page index and it will actually show you what each page of the database file is used for and we can see here that the first page is is both a root and a leaf so that that particular table fit on a single page and in both the and it all fit there and you can see a bunch of other tables down here at the bottom i want to point out we've got overflow pages because i mentioned earlier that sqlite handles up to a gigabyte of content in a row but the pages are like 1k how do we do that well if it doesn't fit on one page it puts a little bit on the on the original page and then puts a pointer to another an overflow chain and this is just a linked list and when i was designing sqlite i looked around at all the sql databases i could find and i didn't find any that really had large blobs or strings and so i thought you know i'm going to support this but it's rarely used it doesn't have to be efficient i didn't try to make this fast but amazingly enough it turns out to work very well even for megabyte size strings and blobs um adobe discovered this for us uh the and the lightroom the adobe lightroom product um they uh they they have to store a lot of uh thumbnails of images and they were one and they use sqlite as their application file format and they're wondering do i store thumbnails directly in the database or do i just store the file name and then write the the image out to a separate file and they they ran it and it turns out for for blobs less than about 50 or 100k it's actually faster to write it into the database than it is to write it to the file system i believe this is because if you write it to the file system you have to open and close and it's the overhead of the open and closed system calls that slows you down whereas the database file is continuously open so we're faster with that all right the beep tree print is again it's you access the b trees by cursor you open a cursor you seek on the cursor forward and back ask for the date or the key close the cursor um how do we find out where the root pages are for each of the b trees in the file so there's a a special table in every sqlite database called the sqlite master table and the schema looks like this it's there by default you can't change it and it has the the type which is table index view or trigger the name of the thing the sql that originally created the original sql text and it also has the root page so and this particular table always has a root at page one so we can go to page one there's a b tree there which is this thing we can read this b tree find the root page of every other table in the file and here's just an example of how you can actually see that table in action page the first page in your file like you just lose everything right so this means that if the like the first page gets clobbered it's going to be really hard to recover much from that database yes because you've lost the schema the scheme is stored on the first page or you know of course in overflow pages as well um so that's the b tree we didn't talk about freelance management auto vacuuming shared cache i'm looking at this clock here and i'm running way over so i'm just going to slip through this the virtual machine is it defines the format of the of the records and i'm going to very quickly go over this the the records sqlite has this interesting property that is is that it kind of ignores column types uh you can put anything you want any column you don't have to declare the have to clear type one column it it sqlite derives from the scripting language community from tickle anybody heard of the tickle programming language you used it sqlite is in fact a tickle extension that escaped into the wild this is the truth so you know kind of a typeless python type thing so we have to store the data type for everything and so we've got a a bunch of variable length integers that define the data type and then we have the actual data and so here's how the energy code like an integer of zero means that it's a null um you know integer six means an eight byte signed integer uh and then then for strings and blobs it's these values here and you've got a little formula that gives you the length so most of the time these type codes are a single byte here's an example in coding create table abc notice i didn't put any type information there just three columns abc and you can do that in sqlite and i'm inserting 177 and null and hello and so here's the header in these four bytes and then here's the the two byte integer for 177 and the null doesn't take any data at all and then there's the string hello so that's the encoding uh the code generator it's in these files in the interest of time i'm going to skip over this really quick if you're going to work with the code generator i suggest you enhance your shell by downloading the tarball doing configure and then adding these extra sharp defines to the make file and then doing the build and now you get some extra command line tools that allow you to for example print out a parse tree in kind of ascii art the clock is just spinning around so fast so i'm just going to i'm going to flip slip right over this and you can go and read this at your leisure if you want to do this we've got really cool tools that if you're in the debugging you're single stepping and you want to look at a parse tree you can actually call some routines and it'll print them out for you you've also got lots of extra pragmas that allow you to trace it prints out each virtual machine opcode as it runs it so i want to get on into how the query actually runs so here's here's an example of how the data is stored in sqlite you've got the row id and then all the data for a simple little table here and if you want to query this table uh you know give me the price for peaches of course one way to do that would be to seek to the beginning of the table step through each row pull out the fruit field see if it's equal to peach and if it is output the price it's a full table scan that always works and of course but you know if there were like seven million rows that would be really slow so well you could also ask for it by row id and then it'll do a binary search and that's very fast but you know if you're if you're running a grocer you don't want to have to remember that the secret code for peach is four that's crazy you really want to ask for peach so for that we have an index and the way an index works is it just uh it creates this key over here that contains the the value being indexed and the row id and because the row id is unique this guarantees that each row in the index will also be unique so and in in cases where um uh the actual fruit is not unique you know the the row will this the row id will disambiguate it and so once you have that uh to query for the for the peach you can do a binary search for the entry that has peach you read off the row id use that to do a binary search in the original table that gives you the whole record and then you can pull out the price and that's great and if you do the same thing for orange and it goes to the first one into the orange gets the right and gets it second entry and and gets that one as well and this is just by stepping through the the bee tree but if i said um orange in california you know it has to do orange uh it gets one it gets one then after it does the look of row one it has to check to see well is it in the state of california no it's in florida i have to reject that row i did that i did that beach research that binary search for not and we we hate that you know that's extra work we like to avoid that and so you think well maybe i'll i'll do an index on state but that doesn't really help either because then you could look for california but then you'd have to check for for grape and and and then you'd miss one there too it's the same amount of work so what you need there is a two column index where you have both the fruit and the state in the index and where the fruits are tied the state breaks the tie it's the same type of thing but now you can look for orange and california get the 23 and immediately look it up and that's a lot faster that's a lot better to do that sort of thing um you could do even better than this though if you built an index that contains all the columns in there the fruit the state and the price and then when you do select price from table where fruit is orange you could do the do a binary search to find the first entry where it says orange california and the price is already there in the index and you just read it straight out the index you don't even have to look at the original table this is called a covering index um if it's an or of course you can always do a full table skin just stop start the beginning of the table read always read read column like row by row checking the the condition and the where clause to see if it's true and then outputting the price but we'd really like to use an index and in this case it will take two separate indexes one for fruit one for state do the lookup for the row ids take the union of those and then pull the prices out that way another thing it can do if you've got a two column index on state and fruit but you want to look at by fruit you think well i can't really do this because i can't do a binary search for the second field of an index but sqlite will do this and the way it does that is that it recognizes there are not many states there there's just not many values so it will iterate through all the possible values for state and then within each state look for the fruit orange we call this there's no official name to it for this as far as i'm aware we call it skip scan but it will try and do this if it knows that the cart that um there aren't many distinct values or just a few distinct values for for the first column so we can sort always do a sort of sort is the most expensive thing you can probably do in a database engine um if you if you if you give an order by clause in sqlite and it it knows that it's going to come out in the right order anyway it just throws the order by clause away it doesn't force you to sort so be generous with your order by clauses if you give an order by fruit and you've got an index on fruit it will walk the index and then do uh you know pull out the rods and then do the search and pull out the rows in the correct order and you think well you know that's still in log in i haven't saved anything well actually you have because it uses a lot less memory and also if you're you're looping through this and you get only a couple entries through and say okay i'm going to quit it didn't have to do all those other lookups before it did the search so this is a much more efficient way of doing it in practice and of course if you have a covering index it just spins right down the covering index if you have a covering index that's almost there but not quite uh here you know we want to sort by fruit and price but we've got this pesky state row in the middle which kind of messes up our sword order it will read down as much as it can uh it'll read all the all the unique values for fruit and then gather them and sort them separately so it does lots of little sorts which is more efficient because it can start outputting rows immediately before it scanned the whole table and the sorts are smaller if you've got something like a union with two order bodies it will actually break this up into two separate queries run them as coroutines and then take the union and the output yes who decides are you doing it blindly for all of these potential choices or there is some decision making right yeah so this requires a lot of memory a lot of maintenance uh with simple updates now it's not only just worrying about it well i mean the indices are maintained automatically you the programmer i'm skipping over some slides here because we are out of time you the programmer have to come up with the indices sqlite is not going to do that itself but you know this is the key advantage of having a query language and i want this is an important slide it's probably the last one i have um because if you've got a query language like this you can code you can design your application you can build it and spend weeks coding it up you get down to the end and you've got a performance problem oftentimes you can fix that performance problem just by doing create index and suddenly you've got completely different algorithms that are being used and you can do this the night before you release your product whereas if you if you if you're using a key value store or something else that doesn't have a query language like this and you get down to the end and and it's not working it's not performing well then um you've got you've got to spend some serious time recoding you can't do that on the day before you release you've got to recode and retest that's the beauty that's why it's so important that's what this top half the part above the storage engine is so important because it gives you that flexibility do you combine those in order to help you to identify next into the tune so the question is do i would provide utilities to help programmers identify bottlenecks and do the tuning that's true no that is a frequent request it's on our todo list i mean there are things there we provide mechanism but it's not an and especially intuitive thing to do you have to kind of know what you're doing but we need an automatic tool that looks at the schema looks at your query and say hey you should consider this index we don't have that yet that's on the todo list so code generator we didn't talk about joint order selection i also skipped some slides there's a lot of other cool things in here this query planner stability guarantee this is an important part of an embedded database is that you know there's all these different algorithms that it can choose but in in a commercial uh database in a data center you want it to adjust its algorithms as the data changes to to select the best algorithm for the current state of the data if you're doing an embedded product and shipping millions of these things you don't want the query planner changing its mind for some small percentage of the users out in the field because usually it will do a good job but sometimes it might choose a bad plan and then you're going to get bug reports you want the query planner to always do the same thing and sqlite can be set up in fact in this default situation it's always going to choose the same query plan for a given schema it won't change that unless you run analyze or change the schema around no it's actually a costbased system but the costs are fixed they don't automatically recompute do you maintain when you run analyze the analyze command so if you rerun analyze it's going to get different statistics and then it might choose different plans so don't do that if you're concerned about query plans changing out front of you okay other topics i haven't talked about virtual tables i haven't talked about the full text search engine and this is a really cool thing because it actually implements lsm trees on top of b plus trees and it's a really cool idea and it's actually very efficient it's faster than scene our trees memory management how we test this thing it's we got a really impressive test suite and that's very important to us um that is i'm i'm slightly over time and and i'm happy to talk about any of this this is the this is a slightly we've gone to a slightly lower orbit and i've given you sort of an overview of how the system works there's a lot of details here love to hear your questions um and your feedback and uh and you can also go to the mailing list and visit us there thank you very much for your attention i'm sorry for running five minutes over if people have to leave you're welcome to step away otherwise if are there questions i'll be happy to take them now all right thanks richard i replied yes ma'am it means how minimal or how long and do you think that you have reached the optimal points of our big variables our internet of things there is a all right so if i understand the question you're asking um yeah there's tradeoffs in any system because uh you know on a fullscale data center oriented database you've got a lot more power and it's doing lots of fancy things and we don't do that and and we do and and so the tradeoff is because we're using less power and it's easier to maintain and we want it consistent across millions of devices have we found that right balance well we're constantly adjusting that balance we take a lot of input from people who are actually using this stuff and we try and adjust it i talked about the query planner stability guarantee and how it doesn't recompute plans based on evolving data there's a compile time option that will make it do that and so if you if if you have a very specific need where you want it to violate that guarantee but but work more like a a mainframe database you can make it do that and so we provide a lot of options that way have we found the best blend i i think so because uh there used to be a lot of these embedded sql databases and now there's just sqlite and maybe sql anywhere and uh the rest of them have kind of gone away so um and the java ones oh yeah the java ones does anybody ever use those really yeah um so uh yeah i think the market seems to be saying that we're doing well maybe we're just lucky i don't know but uh i think we're it is you're but you're right there's a balance here you got to find the right mixture of features and capabilities and i think we've got a niche and we're trying to do that yes sir