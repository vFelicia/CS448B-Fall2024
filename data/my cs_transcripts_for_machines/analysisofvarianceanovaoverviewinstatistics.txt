this video is an introduction to oneway anova and twoway anova so this is like the basic idea and you really need to understand this video before you move on to other videos otherwise the equations just won't make any sense all right so first oneway anova the basic idea here is that you have at least three populations so three or more populations and you're trying to compare the mean of each of those populations to see if any of those differ so our null hypothesis is all the means from all of the populations all m populations are equal and then the alternative hypothesis is at least one of these means differs all right so what we're going to do to do this test we're going to compare the variability within each group against the variability between different groups so if we have m populations we'll take a sample from each of those m populations and then we're going to maybe make a plot to visually see the variability within each group and the variability between groups alright so imagine we take a sample here's our data set here's our box plots here we can see that yes there is a little bit of difference in these sample means but there's a lot of variability within each group so here we see a lot of variability in this group that's why the box plot is so long a lot of variability in this group that's why the box plot is so long and so even though there is variability between groups it's not very much compared to the variability within each group so here if we saw box plots like this it could just mean that the difference in sample means is simply due to random chance all right in the second data set here we see very little variability within each group so a very tight box plot here another very tight box another very tight box and so we see very little variability within each group compared to the variability between groups so if we saw a box plot like this assuming the sample size is large enough we'd probably be inclined to reject the null hypothesis and say that at least one of these means differs all right so again the null hypothesis is all the means are equal alternative hypothesis is at least one of these means differs you'll notice that if we end up rejecting the alternative hypothesis we don't yet know which mean is different all we know is that at least one of the means differs all right so that's the um big idea for one way anova for twoway anova we're changing a little bit so in oneway anova we just had one categorical variable now in twoway anova we're going to have two categorical variables so we still have a quantitative response in both of these here in twoway nova we have two categorical variables okay so it's easiest if we think about an example so imagine that we're doing an experiment to look at the temperature of water used when you're washing our clothes and the type of detergent you're using when you're washing your clothes so imagine you're using two detergents tide versus kirkland and then your water temperature on your washing machine is like cold warm or hot okay so you want to see if you make the clothes equally dirty in each of these experiments is one of these things going to release more dirt get rid of more dirt from your clothing so we're trying to measure as our response the amount of dirt that's removed from the clothing in one wash cycle all right so there's our response the amount of dirt removed from one wash cycle uh predictors like we said the type of detergent used and then the water temperature okay so we can use twoway anova to figure out a whole bunch of things so one of the things we can figure out is whether one of the detergents removes more dirt so in other words if we like look at the mean for tide versus the mean for kirkland do these differ all right next question we can answer is whether one of the water temperatures more effectively removes dirt so our null hypothesis is going to be the mean for cold water the mean for warm water and the mean for hot water those are all equal and then the alternative is one of those means is not equal okay and then finally the thing that we can figure out is whether some combination of detergent and water temperature is more or less effective at removing dirt so in other words we're trying to see is there some good combination that will remove more dirt or in other words we could say do some detergents work better or worse in certain water temperatures so it might be for example that like kirkland works the best in hot water but for tide it doesn't really matter or something like that all right you'll notice that there's question one and this question two here those are things that we could answer by doing a oneway anova for each one of those but the advantage of twoway anova is this last question that we get to answer we get to figure out whether some combination is working so in other words we're trying to see is the interaction term significant in this video we started digging into the math of the oneway anova all right so first thing we need to do is inform you how this all is working and set up the notation all right so what we can imagine is that we have m groups so that means we have m different means mu 1 mu 2 up to mu m so what we do is we take a popular take a sample of size ni from the ith population all right so like you go to population one you take a sample of size n1 then you go to population two take a sample of size n2 and so on in other words all these sample sizes don't have to be the same across the different populations all right so once you have that sample from each one of those populations we can get our total sample size n by just adding up the sample sizes of each of those uh samples right so n equals just the sum of the ni all right so then we need to define our random sample so the sample that we take is denoted by x i n i so this is a random sample of size n i from the ice population and we're going to assume that these populations have a normal distribution with mean mu i for the population and then a shared variance sigma squared so this is an assumption we're making that the variance is shared across the different populations all right so we have our samples x i and i from each of those i populations we can think about the mean for each of those populations so like what's the mean for population one what's the mean for population two and so on all right so our sample mean for the eighth population we just add up our sample and then divide by the sample size so we took a sample of size ni from the eyes population so we're dividing by that i populations that ice groups sample size and then here this is just saying we're adding up all of the measurements that we took from that ice population all right so we denote this by x bar and then in the subscript we have i and then a dot so that dot just means that we're um averaging across that second subscript there that which which is shown as j here all right so this is how we're going to denote the sample mean taken from population i all right so then how are we going to denote our overall mean we're going to denote it with x bar with two dots in the subscript because we're averaging over both of those subscripts all right so we divide by our overall sample size and we need to add up all of the observations we need to add up all of our measurements so the way that we can think about doing this is go to each one of these different groups so like let's imagine right now we're at population one so then i would be equal to one and then we're going to add up all of the measurements we took from population i so we're going to add up xi1 xi2 xi3 and so on so we add up all of those measurements from the um population i equals one and then we add up all the measurements from population i equals two and so on until we have added up all of these different measurements so we've added up all the different measurements and just divided by the sample size that's our overall mean all right so we have our group means and our overall means now we can start actually talking about variability so remember in one way anova we're interested in measuring the variability within groups and the variability between groups because we want to compare those to see if the variability between groups is really big compared to the variability within groups then that probably means that we can reject our null hypothesis in favor of our alternative all right so how do we measure the variability between groups well let's denote this by sst so this t is like if we're thinking about doing an experiment we have different treatments so the t here means the treatment for the um the treatment here and ss indicates like sum of squares so that'll make sense because we're adding up a bunch of things that are squared all right so what is the sum of squares for the treatment well let's think about this intuitively for a second we're measuring the variability between groups so in other words we want to look at all these different sample means and see how much variability there is so the way that we're going to do that is we're going to compare each sample mean against the overall mean so we take each one of these sample means and subtract off the overall mean so that says how far apart those two things are and then we square it and multiply by the sample size for that ice population okay so this is going to give us our treatment sum of squares so again this is measuring the variability between the different sample means all right and then later we'll use this notation mst is the sum of squares for t divided by m minus 1. so remember m is our number of populations our number of groups so what we're doing here is we're taking our total sum of our treatment sum of squares and then dividing by the number of groups minus one so we can think of this kind of like as the mean sum of squares for the treatment all right next thing we need to look at is measuring the variability within groups okay so what does it mean to have variability within groups well we could look at each group's mean and then see how much the sample varies for that ice population so we're say we're talking about population one we look at the sample mean for population one and then we see how much variability is there in the measurements from population one how much variability is there around that sample mean from population one okay so say let's work with i equals one for now um we go and we take each one of those measurements from sample one and we compare it to the sample mean for sample one so we're finding all of these differences squaring them and adding them up all right so now we've done this part for i equals 1. now we need to do it for i equals 2 i equal 3 all the way to i equals m so we're adding up all of those squares and that will be our sum of squared sum of squares for the error okay so we can think about it kind of like as error because um we're looking at the variability around some sample mean and then the mean squared error is going to be the sum of squares for the error and then we're going to divide by the sample size minus the number of groups so we'll talk a little bit more about why that's the denominator later all right so we have the variability between groups the variability within groups now we need to talk about what is the total variability and very conveniently the total variability is just the variability within groups plus the variability between groups okay so we're going to denote the total variability by ssto and what we're going to do is just compare each observation against that overall mean all right so this differs from like this one because here we are measuring the variability compared to each group's sample mean here we're comparing it to the overall mean so take every observation find the distance between that and the overall mean square it add all of those up so that's the total sum of squares okay so intuitively if we are looking at the total variability it can really only come from two sources it can either arise because of error in other words variability within groups or it can arise because the treatments are different so in other words the variability between groups so that means that the total sum of squares is equal to the treatment sum of squares plus sse and then mathematically if you want to think about it that way if you write this out and then expand it you'll see that the cross term is actually zero and they go through that in the book if you would like to check that out in the last anova video we set the notation and now we can actually get into the theory behind the oneway anova all right so um remember that we just left off saying that the total sum of squares equals the treatment sum of squares plus sse so if we want to we can divide by a constant all the way across and that's just fine so we divide across by sigma squared all right so if you look at your book theorem 931 then it says that the total sorry the treatment sum of squares and sse are independent okay so if we look at this first piece and break that down we're going to see that it's a chisquared random variable with n minus 1 degrees of freedom similarly if we look at sse that that's also sorry sse divided by sigma squared that's also a chisquared random variable with m minus 1 degrees of freedom all right so this is a chisquared random variable this is a chisquared random variable and this and this are independent of each other so if we remember back in our mgf days from last semester if we sorry that's moment generating function if we have a chisquared random variable equals something plus a chisquared random variable then that's going to mean that this is also a chisquared chisquared random variable and remember the way that these work is that if we have chi squared with however many degrees of freedom plus chi squared with m minus 1 degrees of freedom that equals chi squared with n minus 1 degrees of freedom so in other words the degrees of freedom just add so if this has n minus 1 degrees of freedom and this has m minus 1 degrees of freedom and this is chi squared and it's not a mystery how many degrees of freedom it has we know that it's going to be n minus m degrees of freedom because we need n minus m plus m minus 1 to equal n minus 1. all right so under the null hypothesis then this must be a chisquared random variable with n minus m degrees of freedom all right so remember that this whole oneway anova stuff we're looking at comparing the variability within groups to the variability between groups so let's start thinking about that variability within groups and the variability between groups all right so under the null hypothesis the treatment sum of squares divided by sigma squared we just said is chi squared distributed with minus one degrees of freedom remember if we have a chisquared random variable and we want to get its mean that's just going to be equal to the number of degrees of freedom that that chi squared has okay so we have n minus 1 degrees of freedom so that means this random variable sst divided by sigma squared that means that this has expectation m minus 1. all right n minus 1 that's a constant sigma squared that's a constant so there's no harm done in moving these around so we could have the expectation of sst divided by m minus 1 and that will equal sigma squared under the null hypothesis so you might be tempted to say okay if we want to estimate sigma squared let's just take the treatment sum of squares and divide by m minus 1. but this is not a good way to estimate our sigma squared because this all relies on the null hypothesis being true so if it's not true we're actually going to be overestimating sigma squared all right so now we know something about how this is distributed let's think now about the null hypothesis whether it's true or not doesn't matter sse divided by sigma squared is going to have a chisquared random variable um is going to be a chisquared random variable with n minus m degrees of freedom so same sort of thing if we take the expectation of this it's going to have n minus m as its expectation all right so since this does not rely on the null hypothesis being true we can use this mse to estimate sigma squared and it will be an unbiased estimator all right so we know we have a chi squared and a chi squared and that they're independent so it says our theorem in the book and so if we take a chisquared random variable and divide by another chisquared random variable as long as they're independent then we're going to end up with an f random variable all right so that's exactly what we're going to do so we said that mst has a chisquared random variable mse has a chisquared random variable they're independent so this which will be our test statistic will actually have an f distribution with m minus 1 degrees of freedom and n minus m degrees of freedom under the null hypothesis so this all relies on being under the null hypothesis because we need sst over sigma squared to have a chisquared distribution and the only way that's going to happen is if the null hypothesis is true so under the null hypothesis here's our test statistic it's going to have this distribution f with m minus 1 degrees of freedom and n minus m degrees of freedom all right so let's think about this logically we said that if the variability between groups is much larger than the variability within groups then we're going to want to reject the null hypothesis so what's the variability between groups that's that mst and then the variability within groups that's that mse so here we're looking at the variability between groups divided by the variability within groups so if this quantity is a lot bigger than one and a lot will be defined by our pvalue if that quantity that test statistic is a lot bigger than 1 then we're going to want to reject the null hypothesis all right so in other words the bigger that test statistic gets the more we're going to want to reject the null hypothesis so our pvalue is going to be the right tail all right so here's our f distribution with m minus 1 degrees of freedom and n minus m degrees of freedom here's our test statistic mst over mse and then the pvalue is going to be the area to the right of that test statistic because like we said as our test statistic gets larger and larger we're going to want to reject the null hypothesis more and more and so as we move further to the right with our test statistic our pvalue is going to be getting smaller and smaller thus letting us have more and more evidence against the null hypothesis so in the previous video we went through all the math of one way anova and now we can talk about how we summarize it so lots of times we'll summarize it in what's called an anova table here and the way it works is we have a column for the source of variability a column for the sum of squares a column for the degrees of freedom a column for the mean squares and then finally our test statistic all right so remember that we have the treatment sum of squares and the sse because we have two sources of error two sources of variability the treatment and the error all right so then we in the next column have our degrees of freedom so this is the number of populations or number of groups minus one and then this is the overall sample size minus the number of groups all right so when we look at the total this total will add so remember the total sum of squares is equal to sst plus sse similarly the total degrees of freedom equals n minus 1 which is m minus 1 plus and minus m all right so f sum of squares and degrees of freedom add and once we get to the next column these no longer add so here in this next column we have the mean squares so mst is just sst divided by n minus 1. so we just take this column divide by that column similarly for mse we take this column divide by that column so it's pretty easy as long as we organize it this way it's super easy to remember what how to calculate mean squares and then finally when we get to our test statistic remember that's just mst divided by mse so again that's pretty easy we just look at this column take this value divide by that value and that's our test statistic okay so remember under the null hypothesis that test statistic has an f distribution with m minus 1 degrees of freedom and and minus m degrees of freedom and remember the larger the test that is the more evidence we have against the null hypothesis so here i've drawn our f distribution with m minus one and n minus n degrees of freedom and then here marked is our test statistic mst over mse and then the pvalue is just the area to the right of that in a previous video we very briefly introduced twoway anova and now we're going to get into more of the details more of the notation all right so remember that in twoway anova we have two categorical variables so every measurement has two categorical variables associated with it so like if our categorical variables are the water temperature and the brand of detergent then every measurement that we take every experiment that we run we will have some detergent that we're using whatever brand kirkland tied whatever and then the water temperature so cold warm or hot all right so each measurement has two attributes all right so if we want we could look at a bunch of different things we could look at the average dirt removed by each detergent by averaging over the different water temperatures or we could average over the different detergents to look at the average amount of dirt removed by each water temperature and then finally if we have more than one observation for each one of those water temp detergent brand combos then we can take the average for each one of those combos so that we can say like okay the average amount of dirt removed by tide in warm water is this much but we can only take an average rate if we have more than one observation for each one of those combos and it'll make it a little bit easier on the notation at first we look at one observation per combo so we actually won't be able to answer this third question here quite yet we'll get to that in a little bit though so for now we're going to set things up so that we can look at the average dirt removed by each detergent and the average dirt removed by each water temperature okay so let's generalize this more so let's say that we have two factors not just water temp and the brand of detergent let's say that we have factor a and factor b factor a has eight categories or levels factor b has b categories or levels all right so we're going to denote each observation by x i j so x i j we're going to assume has a normal distribution with mean mu i j and variance sigma squared and again just like in the oneway anova we're going to assume that sigma squared is a shared variance all right so we're going to have one observation per combo so that means we'll have i equals 1 through a and j equals one through b so in other words this first subscript the i that's for factor a the second subscript b that's for factor the second subscript is j and that's for factor b okay so since we have one observation per combo then if we take the number of levels in a and the number of levels in b that's going to be our overall sample size all right so we said that we could look at the average dirt removed by each detergent and the average dirt removed by each water temperature so in other words let's find the mean for each level of factor a and the mean for each level of factor b all right so if we're looking at the ice level of factor a then we're looking for the mean for the i level and we're going to average over all of the different j's so this would be like we're finding the average dirt removed by each detergent and we're averaging over all the different water temperatures okay so this x bar i that is 1 divided by b which is the number of levels in factor b and we're going to add up all the temperatures sorry all the observations x i j where this i and that i are the same so we're adding up over all the j's okay similar thing for our factor b if we're going to look for the mean for the j level of b we'll denote that by x bar dot j so we'll add up all the observations for that jth level and all the different levels for the first factor so we add all those up divide by the sample size which is a which is the number of levels in factor a and then our overall mean we're just going to add up all of our observations for all of the all of the levels in b and all of the levels in a and then we're going to divide by our sample size which is 1 over a times b all right so we've got most of our notation down so now we can define like the total sum of squares and all that stuff so the total sum of squares remember is going to be we're going to take each one of these observations and find the distance between it and the mean that overall grand mean so we find that distance square it add all of those up and that's our total sum of squares so this is pretty much a very direct extension of the oneway anova all right so if we manipulate this a little bit we'll end up getting that this total sum of squares is equal to b times this sum which is saying let's look at the variability for the different levels in a so we'll look at okay the first level in a and take that mean and subtract off the grand mean find that distance there square it add all those up so we're going to find we're going to be looking at the variability among the means for factor a here we're going to be looking at the variability for the means for factor b so that's a similar thing and then here we're going to be looking at x i j minus the mean for the i level of a minus the mean for the jth level of b minus that overall grand mean okay so this is um this part here is like what we would predict x i j to be and this is what it actually is so it's again same thing as usual like observed minus predicted so we take that observed subtract off the group mean square it and then add all of those up all right so we're going to call this piece ss a this piece is ssb then that final piece is sse so this is the sum of squares for factor a this is the sum of squares for factor b and then this is the sum of squares due to error all right so we're going to use this notation to move forward and actually like get our test statistics and distributions and all that sort of stuff in the next video now that we've set up some of the notation for this twoway novel we can actually get into some of the mathematical properties statistical properties of these different things that we've set up all right so we just left off by saying that the total sum of squares equals the sum of squares from a plus the sum of squares from b plus the sum of squares due to error all right so that's what we set up and then now if we think back to how we did things in one way anova we set up the o sum of squares and then we figured out okay what is the distribution of each of these components so that's exactly what we're going to do here and just like in one way anova we're going to see that each of these components is chisquared distributed all right so we want to show that sum of squares for a over sigma squared sum of squares for b over sigma squared and sum of squares due to error over sigma squared are each independent chisquared random variables as long as some hypotheses are true so this is just like we did for the oneway anova we said we're going to show that these are chisquared distributed as long as the null is true and oneway anova so in twoway anova we're testing the means for a and the means for b so we need to say two sets of hypotheses so the null for a is that the means among a's different levels are equal and the null hypothesis for b is the means among these different levels are equal so if we want to think of an example then for the null hypothesis for a that might be saying like the different detergents remove an equal amount of dirt on average and then the null hypothesis for b would be saying that the different water temperatures remove an equal amount of dirt on average all right so those are the hypotheses we're going to work under to show that these different components are chisquared distributed all right so let's go ahead and start from there we'll assume that h a and h b are true so if you remember from one way anova this total sum of squares is chisquared distributed and its degrees of freedom is just the sample size minus one so remember we're working in the situation here where we have one observation per combo of detergent and water or in other words one observation per combo of each of these levels and each of these levels so the number of levels is a times b that's the total number of different combos we could have so that's the sample size n is a times b and so our degrees of freedom for ss total over sigma squared is a b minus one all right so if we use that theorem that we talked about back in one way anova then we know that ssa over sigma squared is going to be chi squared distributed with a minus 1 degrees of freedom so remember the degrees of freedom is the number of levels in a minus one and then same story for ssb over sigma squared again degrees of freedom is number of levels in b minus one all right so that means that um this piece over sigma squared is chisquared distributed this piece over sigma squared is chisquared distributed and this piece over sigma squared is chisquared distributed also from the theorem we used back in one way nova we know that ssa over sigma squared and s b over sigma squared are independent so what that tells us is that since this is chi squared and this over sigma squared is chi squared this over sigma squared is chi squared therefore this plus this over sigma squared is also chi squared distributed all right so where that puts us is that this over sigma squared is chi squared distributed and this piece is also chisquare distributed so therefore if we have chi squared plus something is a chisquared that must mean that then this is also chisquared distributed so if we think back to mgfs that can help you understand that all right so sse over sigma squared must be chi squared distributed but what about the degrees of freedom well we know that the degrees of freedom should add right so if we know the degrees of freedom for ss total over sigma squared and we know the degrees of freedom for this chunk over sigma squared then we know that we can just add these to get there so the degrees of freedom for ss total over sigma squared is a b minus one and then we figured out that the degrees of freedom for this piece over sigma squared is a minus one plus b minus one so then that leaves the degrees of freedom for ss due to error over sigma squared and so it must be then the degrees of freedom here is equal to a minus 1 times b minus 1. all right so now we figured out that ss total over sigma squared is chisquare distributed and ssa over sigma squared's chisquared distributed ssb over sigma squared is chisquared distributed these two components are independent of each other and then finally sse over sigma squared is also chisquared distributed and we have all the degrees of freedom here so that will help us set up the test statistics in the next video in the previous video we figured out a few things so if the means among a's different levels and if the means among these different levels are equal then we said that ss total over sigma squared is chisquared distributed with a b minus one degrees of freedom ssa over sigma squared is also chisquared distributed with a minus one degrees of freedom similar story for ssb over sigma squared and then ss e over sigma squared is chi squared distributed with a minus 1 times b minus 1 degrees of freedom and finally ssa over sigma squared ssb over sigma squared and sse over sigma squared are all independent random variables all right so now we can work with these facts work with these findings to derive our test statistics to test whether the means among a's different levels are truly equal all right so say we want to do just that so let's work first with the null hypothesis for a and then we could do the exact same thing for the null hypothesis for b so we want to test whether the means among a's different levels are equal so remember from oneway anova what we did is we compared the variability between groups the variability within groups so that's exactly what we're going to do here again so we're going to test set up the test at the exact same way so we have the variability between groups up in the numerator and the variability within groups in the denominator so this is the test at for testing whether a's different means are equal all right so if the null hypothesis actually were true then this should be pretty small or maybe about one but if the means are actually different then we're going to see sse sorry ssa is going to be bigger than expected so we're going to compare this to a statistic against its sampling distribution so what's its sampling distribution well we know that ssa over sigma squared is chi squared distributed with a minus 1 degrees of freedom we know sse over sigma squared is chi squared distributive of a minus 1 times b minus 1 degrees of freedom therefore this divided by this is going to have an f distribution so we're going to compare this test statistic against an f distribution df 1 is going to be a minus 1 df 2 is a minus 1 times b minus 1. all right so when our test statistic is large compared to this distribution then we're going to reject the null hypothesis that the means amongst a's different levels are equal so here's our little picture we have our f distribution with a minus 1 and a minus 1 times b minus 1 degrees of freedom we've marked off our test at and we've shaded the area to the right because that represents larger more extreme test statistics so that area to the right that's our p value so our p value is the probability that this f distribution is greater than our test statistic all right one more thing we probably want to have a good unbiased estimator for sigma squared and so we can do just that so if the null hypothesis for a were true we could do something with ssa and if the null hypothesis for b were true we could do something with ssb to figure out a good estimator for sigma squared but we can't rely on those being true right we can't rely on the means amongst a's different groups being true we can't rely on the means amongst b's different groups being true so what we're going to rely on is sse over sigma squared having a chisquared distribution with a minus 1 times b minus 1 degrees of freedom because whether the null for a and the null for b whether those are true or false it doesn't matter this finding number 5 here that's still going to be true so sse over sigma squared has a chisquare distribution with a minus 1 times b minus 1 degrees of freedom therefore a good unbiased estimator for sigma squared is going to be sse divided by a minus one times b minus one in our previous work with two factor anova we had two factors but we only had one observation for each combo of levels in a and levels in b so in other words we only had like one observation for tied with cold water but now we want to move from just having one observation for tied with cold water to having more than one observation all right so when we only had like one observation for each combo of detergent and water temperature we were restricted to testing just two hypotheses so one of the hypotheses was are the means among a's different levels equal and are the means among b's different levels equal but we were not able to test when we only had one observation per combo we were not able to test is there some combo of a and b that has a different mean from the other means from the other combos of a and b all right so we want to test for this interaction between a and b so what we're going to do is increase our number of observations from 1 to some larger number c all right so we're going to denote each observation now that we have multiple observations for each combo we're going to need three subscripts all right so each observation is going to be called x i j k so i is for the level of a j is for the level of b and then k is for um which like iteration we've done so like if we're on the second trial for this combo of a and b then k is going to be equal to two so if we're having c different observations for each combo then k is going to go from 1 to c all right so i goes from 1 to a because i is for the different levels of a j goes from 1 to b because j is for the different levels of b and then k tells us which number experiment we are on for that exact combo of a and b all right so since we have a different levels of a b different levels of b and c different runs for each combo then that means that our sample size is a times b times c all right so that's our overall sample size all right so just like we did before we need to define some notation so let's go ahead and do that so again when we have a mean and then we have a dot that means we're going to be averaging over that subscript so if we have x bar i j dot that means we're going to be averaging over the k subscript which means all right for this combo of level of a and level of b let's take the mean so this is like the mean amount of dirt removed if we use this type of detergent and whatever temperature of water and then x bar i dot dot so we're averaging over the last two subscripts so that means that this might be like the mean amount of dirt removed by one of the detergents like the ice level might be tied and then x bar dot j dot that means that we're averaging to find the mean amount of dirt removed by whatever water temperature we're looking at whatever water temperature j is and then finally that overall grand mean is x bar dot dot dot okay so if we're trying to find um the mean for the ife level of a then that means we have to add up all the different water temperatures and all the different trials that we've run so we add over j and k and then divide by the sample size which is b times c similar story for x bar dot j dot our sample size our number of thingies that we're adding up is one over a times c and what are the thingies that we're adding up well we're adding over the i subscript and the k subscript so that's what we're doing here we're adding from i equals 1 to a and from k equals 1 to c and then finally for this last one we're adding everything up because we're looking for that grand mean so we add them all up we're adding over all three of these indices and dividing by the overall sample size all right so that's our notation now we need to go ahead and write down our total sum of squares so as you can notice this is kind of following exactly what we did for oneway anova and twoway anova we're just making it more complicated so remember our total sum of squares that's going to be just our variability between each observation and the grand mean so we have one observation minus the grand mean square it and do that for all of our data points so we're adding over i equals one to a j equals one to b and k equals one to c all right so if we maneuver this just like we did in the oneway anova stuff we can rewrite this as this total sum of squares as b times c times the variability amongst a's different levels plus a times c times the variability amongst b these different levels and then here's the variability amongst the different combos of a and b and then finally we have our error term which is describing how much variability is there within one combo of level of a and level of b so remember x bar i j that's the mean for some combo and then x i j k is one observation so we're saying okay let's compare each observation within some combo of ambi to what its mean is square that add them all up all right so what that tells us is that this is our sum of squares for a this piece is our sum of squares for b the middle piece here is our sum of squares for the interaction and then finally this last piece here is the sum of squares due to error all right so in the next video we're going to talk about the distributions of each of these and then find our test stats now that we have set up the notation for our twoway anova with multiple observations for each combo of a and b we can actually get into some of the statistical properties all right so just like in all our previous anova situations the total sum of squares divided by sigma squared has a chisquared distribution with n minus 1 degrees of freedom but what is n n is a times b times c so ss total over sigma squared has a chisquared distribution with a times b times c minus 1 degrees of freedom all right so if we use our same line of reasoning as in our previous anovas then we know that s a over sigma squared ssb over sigma squared assets a b over sigma squared and sse over sigma squared are all independent and we're going to see that each one of these has a chisquared distribution all right so ssa over sigma squared has a chisquared distribution with a minus 1 degrees of freedom as long as our null hypothesis for a is true in other words as long as the means among a's different levels are all equal then ssa over sigma squared has a chisquare distribution with a minus 1 degrees of freedom similarly when the means amongst b's different levels are all equal then ssb over sigma squared has a chisquare distribution with b minus 1 degrees of freedom similarly when the means amongst all combos of a and b are equal in other words when there's no interaction term then ss a b over sigma squared has a chisquared distribution with a minus 1 times b minus 1 degrees of freedom all right so if we have chi squared equals chi squared plus chi squared plus pi squared plus something then we know that that last thing e has to be a chisquared random variable as well so that's exactly what we have going on just like we've been seeing all along so s e over sigma squared must be a chisquared random variable and to find this degrees of freedom we would just use the fact that degrees of freedom add so we know that a minus 1 plus b minus 1 plus a minus 1 times b minus 1 plus whatever the degrees of freedom are for sse over sigma squared must equal n minus 1 which is a times b times c minus 1. so if we just do a little bit of arithmetic we can find that the degrees of freedom for sse over a sigma squared is a times b times c minus one all right so now we have all those chisquared random variables set up we can now do some of our test stats so say that we want to test whether there is an interaction term so we're trying to figure out is there some combo of detergent and water temperature that will more effectively or less effectively remove dirt from clothing so we're going to use this test statistic here we're going to look at the variability among the different interaction terms so we're looking at ssab and we're comparing it to the variability within the interactions in other words we're looking at the variability due to error so the variability amongst all our different runs of the same type of detergent and the same temperature of water so our test stat is we can call it fab the sum of squares for a b divided by the degrees of freedom which is a minus 1 times b minus 1. and then in the denominator we have sse divided by a times b times c minus 1. all right so that's our test statistic we know that the numerator is a chisquare the denominator is a chisquared and they're independent therefore this test statistic must be an f random variable and its degrees of freedom are this is the first degree of degrees of freedom and this is the second degrees of freedom so it's an f with a minus 1 times b minus 1 degrees of freedom for the first degrees of freedom and the second degrees of freedom is a times b times c minus 1. all right so this test at has this f distribution under the null hypothesis so as long as in reality there's no interaction term then this will be true so in other words as long as actually there's no combo of water temperature and detergent that does better or worse than the other ones then this distribution this uh test at has this sampling distribution here all right so then we can find our p value by looking at as always our right tail so here's our f distribution with a minus one times b minus one degrees of freedom for the first degrees of freedom and then a times b times c minus one for the second degrees of freedom we mark off our test stat here f a b and we look at the area to the right and that is our p value so if we have a small p value then we can say actually there must be some interaction that has a different mean than the others in other words there's some combo of detergent and water temperature that more effectively or less effectively removes dirt from clothing and if our pvalue is too big then we would say there is no combo of water temperature and detergent that more or less effectively removes dirt from clothing all right so that's how we would test our interaction term now let's talk about these main effects so i'll show you just for a but b is the exact same story so to test whether any of a's levels have a different mean we're going to use this test statistic here so again we're going to look at the variability amongst a's different levels and compared to the variability within groups okay so we have ss a divided by number of levels in a minus 1 in the numerator and then our denominator is sse divided by a times b times c minus 1. all right so that's our test statistic if all the means for the levels of a are actually equal then that test statistic will have an f distribution with a minus 1 times a times b times c minus 1 degrees of freedom and our p value again we're going to draw out our f distribution mark off our test at and the pvalue is the area to the right so our pvalue is the probability that an fdistribution with a minus one and a times b times cminus one degrees of freedom is greater than our test statistic finally a good thing to know again would be an unbiased estimator for sigma squared so that's going to be similar to all the previous anovas that we've looked at sse divided by a times b times quantity c minus 1. all right so that wraps up all the anova stuff it would be very useful to look at the anova tables in the book just to help you kind of organize everything as you are preparing to work on this in class